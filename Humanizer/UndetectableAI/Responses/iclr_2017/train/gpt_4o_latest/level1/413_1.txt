The article discusses Sigma Delta Networks as a method to enhance the efficiency of deep neural networks when dealing with temporally redundant data like videos. It introduces a framework where neurons share the changes in their activations in a discretized manner rather than their exact values to lower computation requirements. The authors suggest an optimization technique to transform existing networks into Sigma Delta networks and show that this strategy can notably cut down computational expenses by as much, as tenfold without compromising accuracy. The study also delves into balancing resources and precision by adjusting a scaling factor and presents findings from tests conducted using both artificial and real life datasets such, as Temporal MNIST and video recordings. 
Decision to approve.
The paper has a motivation, behind it and introduces a fresh and significant idea while showcasing promising outcomes that make it deserving of acceptance reasons. 
In the realm of learning the Sigma Delta method tackles a crucial issue of computational efficiency when dealing with time based repetitive data by utilizing sparsity and redundancy in a structured manner that is noteworthy and impactful. 
The practical testing clearly proves that the suggested approach is effective by highlighting reductions in computational resources with only a minor decrease, in accuracy. 
Reasons, for Support 
The paper is nicely positioned within the existing literature on spiking networks and quantized networks and their relevance, to event based sensors. 
The theoretical basis is solid and the experiments are meticulously planned to support the arguments made The incorporation of Temporal MNIST and video data adds credibility to the practical evaluation. 
The technique could have significance in settings with limited resources, like edge devices and distributed systems and might spark more exploration in asynchronous and sparse neural computation. 
Ways to make things better
The paper would benefit from a thorough examination of how feasible it is to implement Sigma Delta networks on existing hardware beyond the brief discussion of hardware considerations, like GPUs or IBM TrueNorth that the authors touched upon. 
The surprising discovery that high level characteristics in video data are not as temporally consistent as expected requires research to be done by the authors to examine whether starting training from scratch, with temporal data or adjusting the structure could help overcome this issue. 
Scalability with models remains an area of interest based off the promising yet preliminary tests conducted using VGG19 architecture calling for a more thorough assessment involving top notch architectures and datasets to bolster confidence, in the scalability of the method. 
Inquiries, for the Writers
How does the cost of running the optimization process for transforming trained networks into Sigma Delta networks stack up against the savings realized during inference? 
Could incorporating noise during the training process impact how well the network can generalize its performance, in real world scenarios and have you tested this out yet? 
Have you thought about expanding the Sigma Delta framework to incorporate neural networks or transformers that are often employed for analyzing sequential data? 
Overall the paper introduces an original method, for improving neural computation efficiency backed by thorough theoretical and practical research. Implementing the recommended enhancements could amplify its influence and lucidity. 