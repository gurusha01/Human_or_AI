
Overview of Contributions
The study introduces a hierarchical reinforcement learning (HRL) model designed to tackle issues related to limited rewards and long term objectives. This model involves two phases; (1) developing a range of skills through unsupervised training using Stochastic Neural Networks (SNNs) incorporating a proxy reward system and an information theoretic regularizer; and (2) utilizing these skills in subsequent tasks through training a top level policy that chooses from the pre established skills. The implementation of SNN technology allows for the development of strategies in handling various tasks effectively by incorporating mutual information bonuses that promote a wide array of skills and abilities, among individuals or agents involved in the learning process. 
Decision approved.
The paper is well thought out. Makes a valuable contribution, to the field of hierarchical reinforcement learning while offering compelling empirical evidence to back up its assertions. 
Innovation and Significance; The combination of SNN models with MI regularization for acquiring skills along with a framework for subsequent tasks presents an intriguing and inventive strategy, for addressing challenges related to sparse rewards. 
The experiments conducted are comprehensive. Showcase the frameworks efficiency, in various tasks and settings consistently surpassing baseline performance levels; thus highlighting the practical value of the suggested approach. 
Points, in favor 
The research paper tackles an issue, in reinforcement learning – dealing with sparse rewards and extended time horizons – by blending intrinsic motivation and hierarchical approaches effectively showcased in existing literature while presenting a comprehensive solution that demands only basic domain understanding. 
The decision of using SNN for learning skills is well supported since they naturally represent policies in ways.The addition of the MI bonus is a way of promoting diversity, in skills and the hierarchical structure effectively harnesses these skills for subsequent tasks. 
Experimental Validation Review; The experiments conducted in this study are thorough and extensive as they encompass both training stages and subsequent tasks for evaluation purposes. The incorporation of settings such as mazes and tasks involving object collection serves to showcase the resilience and adaptability of the method being tested. Moreover the ablation studies provide support for the assertions made by dissecting the impacts of specific elements, like the mutual information bonus and bilinear integration in a more isolated manner. 
Tips, for Enhancing Your Skills
Switch of Skills in Robots; This study highlights the difficulties faced in skill transitioning for robots such, as the Ant robot. Further research could investigate switching methods or comprehensive training to address this challenge. 
The paper touches on hyperparameter sensitivity by discussing factors, like MI bonus and switch time; however delving deeper into how these variables affect performance could enhance reproducibility and practical application. 
The paper should provide a detailed comparison with current high level reinforcement learning methods, like Option Critic to enhance the assessment in terms of effectiveness and efficiency in using samples. 
Scalability, for handling tasks is a key aspect to consider in this study as the tasks investigated are somewhat basic when compared to real world scenarios; it would be interesting to delve into how well the framework can adapt to more intricate and dynamic environments involving multiple agents in future research endeavors. 
Queries to the Writers
How much does the framework rely on the reward chosen during pre training sessions and would a badly structured proxy reward notably lower performance levels? 
Have you thought about using structures in the advanced policy to make better use of timing details when choosing skills? 
Is it possible to expand the framework to accommodate tasks that involve changes, in the environment over time and require a greater ability to switch between skills adaptively? 
The paper significantly adds value to the reinforcement learning domain and lays a sturdy groundwork for upcoming studies. The new approach is creative and feasible as it tackles obstacles, in scenarios with scarce rewards. 