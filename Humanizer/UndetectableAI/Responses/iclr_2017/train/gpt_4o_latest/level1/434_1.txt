Reviewing the document titled "Batch Normalized LSTM, for Recurring Neural Networks."
Key Points of Contribution
This article presents a way to modify Long Short Term Memory (LSTM) networks by integrating batch normalization into the transitions from input to hidden and hidden to hidden layers according to the authors proposal reducing internal covariate shift, in recurrent neural networks (RNNs). This change aims to enhance speed of convergence and boost generalization performance. The article discusses the significance of setup for batch normalization parameters and showcases how the Batch Normalized LSTM (BN LSTM) can be beneficial in different types of tasks like sequence classification and language modeling as well, as question answering tasks.The practical findings reveal enhancements compared to standard LSTM models and even outperform them on various evaluation metrics. 
Outcome Determined to be Approved 
Top factors contributing to approval; 
The research paper questions beliefs regarding the constraints of batch normalization in RNN models and showcases its effective utilization in hidden to hidden connections – a notable advancement, in the field. 
The writers present a range of experimental findings on various tasks that demonstrate steady enhancements in both the speed of convergence and overall performance, in generalization. 
Reasons, for support 
The paper is firmly grounded in existing research by focusing on a recognized drawback of batch normalization in RNN models. The authors offer a theoretical rationale, for their methodological approach highlight the significance of correctly initializing batch normalization parameters. 
The tests encompass tasks such as sequential MNIST analysis and character level language prediction on datasets like Penn Treebank and text 7; additionally including a demanding question and answer assessment benchmark for thorough evaluation purposes which consistently proves the BN LSTMs advantage, over traditional LSTM benchmarks. 
The authors demonstrate rigor by conducting thorough analyses on topics like the influence of initialization, on gradient flow and how well their model applies to longer sequences; these findings enhance the credibility of their arguments. 
Ways to enhance Things
Ensuring clarity during initialization is crucial as the paper stresses the significance of setting the batch normalization parameter γ correctly; however it would be beneficial to offer specific guidance or rules of thumb for practitioners to apply this initialization method consistently across various tasks. 
Further research could enhance the paper by conducting ablation studies to isolate the impacts of various components like separately normalizing input, to hidden and hidden to hidden transitions. 
The authors could improve the paper by providing a thorough comparison with other normalization techniques for RNN models like Layer Normalization rather than just mentioning related works such, as Ba et al., 2016. 
Scalability is an aspect here as the tests mainly target smaller tasks so it would be beneficial to test the approach on bigger datasets or more intricate architectures, like Transformer based models to gauge its scalability better. 
Queries, for the Writers
How does the BN LSTM approach stack up against normalization methods, like Layer Normalization in terms of computational resources and effectiveness? 
Could the writers provide details about the difficulties involved in applying the suggested approach to different types of recurrent structures, like GRUs or bidirectional RNN models? 
How much does the performance of BN LSTM depend on the selection of hyperparameters, like the initialization of γ and β? 
In summary this study provides an addition to the realm of recurrent neural networks by showcasing the practicality and advantages of implementing batch normalization in transitions between hidden layers. The BN LSTM model put forward is supported by reasoning, thoroughly assessed and could serve as a catalyst for future exploration in this domain. Given some enhancements, in terms of clarity and more comparative analyses the paper could further solidify its findings. 