The research paper presents a technique for trimming down the weights of networks while they are being trained to reach sparse outcomes.To test this method out in practice it was implemented on a system using RNN technology. The process was done on a speech recognition dataset.The findings indicate that there are decreases in computational requirements at test time with little effect on task performance.Surprisingly in situations this method has even led to enhancements, in evaluation results. 
The researchers conducted experiments using a RNN system with a strong experimental approach in place which is commendable! I find the analysis of effects for large networks and the resulting computational savings quite significant in their findings. However I do feel a bit let down that all experiments were based on a private dataset. Even if they used training data it would have been more valuable to see evaluations done on a widely recognized test set like HUB5, for conversational speech. Furthermore it would have been beneficial to compare the proposed method with pruning techniques and highlight its similarities to the work of Han et al.[2], in order to demonstrate the advantages of this approach more clearly. Although the simplicity of a single stage training process may seem appealing at glance it may not actually save significant time in real world scenarios if extensive experimentation is needed to adjust hyperparameters for the threshold adaptation scheme. Moreover the strong foundation would have been more convincing if it included methods, for reducing the model size like learning from the output of a network. 
The paper is nicely. Easy to understand overall! Although the descriptions, for the tables and figures could use a bit detail to enhance clarity of information provided in them; they still get the point across well enough! The part that discusses ways to speed up processes and save memory in sparse recurrent neural networks is quite interesting; even if it doesn't directly relate to the pruning method proposed here specifically! One thing that could be improved is explaining methodological choices better. Like why the threshold needs to increase after a set period of time! If this decision was based on results or observations made earlier in the process should be explicitly stated in the paper! 
Extensive research has been done on neural networks especially within the realm of recurrent neural networks (for example echo state networks commonly used sparse recurrent weight matrices). The method suggested here is quite similar, to the approach taken by Han and colleagues in their work that focuses on trimming weights after training and then retraining the ones that remain. Although the idea of condensing this three step process into a training session sounds intriguing in theory the suggested plan actually includes various stages; initial training without pruning then pruning at two distinct rates and finally another round of training without additional pruning. The main innovation here is implementing this approach, with RNNs which're typically trickier to train compared to feedforward networks. 
Boostin​g scalability plays a role in advancing research on neural networks​. Though the paper doesn't introduce groundbreaking concepts or scientific revelations​ it efficiently showcases that trimming weight can be implemented in extensive RNN setups without significant decline, in performance​. The noteworthy outcome here is that this feat can be accomplished through a rule of thumb​.
I'm sorry. I cannot provide a response, without the original text that needs to be paraphrased. Please provide the input for me to rewrite in a like manner.
Advantages; 
The technique successfully decreases the amount of parameters, in RNN models while maintaining performance at a level. 
The tests are carried out using a cutting edge platform, for real world use. 
Drawbacks; 
The approach bears striking resemblances to research and seems to lack significant originality. 
Alternative pruning methods cannot be compared to any others. 
The utilization of information hinders the ability to replicate the findings. 
I'm sorry. I cannot provide a paraphrased response without having the original input you provided. Can you please share the text you want me to paraphrase into a like version?
Sources; 
H., Jaeger (2002). A study on the "echo state" technique for examining and improving neural networks—with a correction note added later on in Bonnevilles research facility, in Germany under the GMD Technical Report series number 148 where there are 34 pages. 
In a paper by Han et al., titled "Learning both weights and connections for neural networks " presented at the Advances, in Neural Information Processing Systems conference in 2015. 