This document presents a method for creating character language models (CLMs) which includes designing a domain language (DSL). The results from tests show varying performance levels when compared to CLMs in analyzing data from the Linux kernel and Wikipedia articles.While the proposed DSL models are more concise and efficient for queries, than neural CLMs. While there are benefits to this method it can be hard to understand. Seems to cater to a specific group who already know about it making it unclear for the wider ICLR audience. A major issue is the papers lack of proof that the suggested DSL's a valid probabilistic model and how the model is adjusted to match the data especially since there is no gradient based training method, in place. The experiments seem unfinished, without showing examples created by the model or examining the acquired knowledge to figure out what the model has learned from them. Overall the paper lacks information for readers to completely comprehend or reproduce the suggested method. 
The model section primarily discusses the DSL. Lacks clarity in explaining how probabilities are calculated within this framework or how training is carried out precisely How are probabilities actually represented in the model The description of the DSL appears to be based entirely upon definite choices, without a distinct inclusion of probabilistic aspects. 
In studies on this topic might have covered training aspects already; however in this paper it is important to delve into the details of the training process, for better understanding and clarity purposes Section 2 point 6 fails to sufficiently elaborate on how the training is conducted or how the idea of reaching optimality is realized. 
In this scenario where the model works in an idea space compared to neural models or ngrams systems being able to analyze the samples generated by the model holds significant importance.It's interesting to observe from the experiments how well the model can evaluate phrases; however it would be quite intriguing to witness the models capability, in generating organized outputs exceeding what neural methods offer,such as ensuring proper syntax rules like maintaining balanced brackets over long distances. 