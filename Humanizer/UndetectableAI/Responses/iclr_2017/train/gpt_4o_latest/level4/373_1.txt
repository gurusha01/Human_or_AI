The writers introduce types of transfer learning techniques to neural network models for use, in various natural language processing tagging assignments. 
The realm of task learning is extensive; however the suggested methods do not seem notably groundbreaking in terms of machine learning perspective. Elements of a standard NLP framework are distributed among various tasks based upon the specific task requirement, at hand. 
The uniqueness primarily comes from the structure used in NLP tagging assignments. 
The results of the experiment show that the suggested method works well in situations where there is not labeled data available (see Figure 2). Nevertheless applying it on a scale only shows minor enhancements as shown in Table 3. 
The findings presented in Figure 2 raise some doubts; it appears that the authors kept the structure constant while adjusting the quantity of labeled information used in their study. 
Overall the paper is nicely. It seems like the new ideas are limited and the experiment testing is not very impressive. 