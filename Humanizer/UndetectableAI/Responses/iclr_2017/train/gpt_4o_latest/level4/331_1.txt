This paper presents a technique for sharing skills across tasks in a controlled environment trained through reinforcement learning (RL). The main concept involves ensuring that the embeddings acquired for two tasks are similar, by applying an L^² penalty approach to the learning process. The testing is carried out in the MuJoCo domain with one group of trials utilizing joint/link states as input (Sections 5​​​​​. ​​​​​ 5 And 5​​​​​. ​​​​​  6 ). Another group employing pixel based inputs (Section 5. 7 ).The outcomes show that the transfer was effective between arms with varying numbers of links and, between an arm driven by torque and one driven by tendons. 
One drawback of the paper is that it assumes time alignment is easy since the tasks are episodic and related to the domain.This overlooks the importance of time alignment as a type of domain adaptation. Transfer that the paper fails to discuss.This issue could potentially be addressed using methods like subsampling,dynamic time warping or developing a matching function (, for example a network).
In observations are that the method is being measured up against canonical correlation analysis (CCA) which is seen as a suitable point of reference here. However since the focus of the paper is mainly experimental, in nature it might be beneficial to introduce another reference point where "f" and "g" (the functions used for embedding in both domains) are set as projections initially. This would serve to verify if the underperformance of the "no transfer" version is a result of the embeddings being too specialized. Moreover it could be worthwhile to mention that the issue of acquiring feature areas is connected to metric learning (, for example Xing et al.,2002). Furthermore the study fails to establish links to multitask learning in the field of machine learning.In the context of knowledge transfer as outlined in Section 4 point 11,it may also be beneficial to contemplate adjusting the alpha parameter. 
The experimental section seems a bit hurried.The baseline consistently hitting a performance of 0 (showing no transfer happening) doesn't provide insight; testing a larger sample budget would have been beneficial.Additionally,"CCA" and " mapping" results are missing in Figure 7.b. prompting queries.Another concern is how the authors managed the number of training iterations for the embeddings, between the transfer and non transfer scenarios. 
In conclusion the exploration of transfer learning in reinforcement learning is an addition to the field. The experiments outlined in this paper are quite intriguing and deserving of publication. Nevertheless a deeper dive, into the methods and results would enhance the papers quality significantly. 