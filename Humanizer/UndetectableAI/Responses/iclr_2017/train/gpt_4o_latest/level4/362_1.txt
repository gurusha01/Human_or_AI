This study adds to the expanding collection of research on learning optimizers/algorithms, a subject that has been receiving increased interest lately The writers utilize the guided policy search framework at the level for training the optimizers In addition, to this they decide to train based on random objectives and assess how well the learned optimizers can be applied to various simple tasks. 
As mentioned on in this discussion thread such a input is indeed quite significant and appreciated. 
The rationale for opting for reinforcement learning (RL) over gradient based techniques at the meta level lacks clarity and persuasion in the sections of the manuscript. I urge the authors to conduct an experiment, between these two methods and showcase the findings for comparison. This is a matter as the expandability of the suggested approach could significantly hinge on this differentiation. In reality. Showing the ability to expand to areas. And successfully applying that ability to those areas. Continue to be a key challenge, in this field of study. 
Ultimately even though the concept shows potential the practical assessment falls short in its thoroughness. 
We published our work on arxiv around the time we were about to publish our own research on mastering the art of learning through gradient descent by gradient descent ourselves! Our inspiration came from art and our goal was to swap out the lBFGS optimizer with a neural Turing machine (both having similar equation formats as explained in the appendix of our arxiv publication). Eventually we decided to go with an LSTM optimizer that was trained using SGD. 
This study uses guided policy search to figure out how to update parameters (policy). Additionally it emphasizes the significance of applying learned optimizers to tasks. 
This article makes an addition to the current literature, on learning how to learn and comes at a time when this field has garnered increasing attention in recent months. 