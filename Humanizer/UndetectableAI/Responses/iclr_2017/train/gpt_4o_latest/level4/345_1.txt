This study presents a Bayesian approach, for understanding the neural network parameters and their underlying assumptions. The new technique utilizes a mixture model prior for the weights to create clusters in the weight distribution. These clusters assist in quantifying and compressings network parameters effectively. The authors show that their method achieves compression rates and predictive accuracy as other existing methods. 
Compared to the study by Han and colleagues in 2015 that utilized a three step methodology of eliminating small weight values first and then grouping the leftover weights before refining the cluster centers; the new research takes a more systematic approach, by introducing a unified and iterative optimization method instead of the complex multi stage structure used previously. 
The initial test described in Section 6 highlights how using the Bayes method without including hyper priors results in a clear grouping effect and several weights being set to zero values. An instance of reaching a compression rate of 64% is demonstrated with the LeNet300 100 model. Nonetheless the reference to Figure C in Section 6 seems to be an error. Probably should refer to Figure 3 instead. 
In Section 6 of the document they talk about hyper priors. How they adjust their settings and other factors (like learning speeds) fine tuning them using Spearmint [Reference; Snoek et al., 2012]. The graph in Figure 2 shows the effectiveness of hyper parameter setups; each trained network is linked to a specific accuracy compression rate on the chart. Although the writers suggest that the top outcomes follow a line pattern, on the graph; this claim feels a bit too hopeful considering the limited data available. We could improve this section by discussing whether we would theoretically anticipate such a linear relationship since the current results lack thorough interpretation. 
In Section 6 of the report discuss the outcomes yielded by CNN models and contrast them with the findings of Han et al., 2015 and Guo et al., 2016 studies.The suggested approach attains similar compression rates and accuracy levels.However the writers admit that their method is presently too sluggish for application in bigger models such as VGG 19.Although a few findings related to VGG 19 are briefly mentioned the comparison, with other techniques is missing. It'd be useful to outline the reasons, for the training pace when using weight clustering compared to traditional training and to detail how the algorithm adapts based on the models size and the data it works with. 
The main focus of the document lies in its applications by incorporating familiar concepts from empirical Bayesian learning to implement weight clustering effects during the training of CNNs. It is worth mentioning that despite its simplicity this method yields results on par, with the network compression techniques that are often more arbitrary. To enhance the papers quality, a detailed explanation of the training algorithm and how well it scales to larger networks and datasets would be beneficial. Furthermore it would be beneficial to delve into the process of searching for hyperparameters (likely without utilizing test data) and to explore how different approaches manage the optimization of hyperparameters in balancing accuracy and compression aspects effectively. Ideally evaluating methods at junctures, on this tradeoff curve would provide substantial insights. 