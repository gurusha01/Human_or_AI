This paper introduces a research study focusedon hierarchical control that shows resemblances to the research, by Heess et al.Its experimental findings are strong. Effectively meet benchmarks previous studies could not reach.However some may find the analysis of these experiments lacking in depth. 
As mentioned by reviewers before me the phrasing "intrinsic" motivation seems a bit off especially considering its common association with reinforcement learning. Preparing robots in advance to move swiftly (or encouraging them to grasp objects with an arm) appears very task focused, fitting well with the tasks they will eventually carry out. Although not the same the preparatory tasks share similarities, with those described in the study by Heess et al. 
The Mutual Information regularization method is quite advanced and typically effective; however it does not show enhancements in the intricate mazes 1 to 3 as expected by the authors themselves. Could there be a need, for further explanation or analysis to shed light on this result? 
The explanation for the relationship between Sagent and Srest needs clarity, in the document provided by Duan et al.. It is important for both Sagent and Srest definitions be clearly outlined for replication of the study results â€“ could this aspect have been unintentional or did I overlook it in my reading? 
Analyzing how the agent switches could provide insights into the situation at hand while delving deeper into understanding various policies and their effects, on performance would have been appreciated. 