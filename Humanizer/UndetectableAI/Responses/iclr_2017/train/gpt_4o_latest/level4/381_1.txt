The authors introduce a method to trim down weights in order to lessen computing loads in GFLOPs significantly. Their approach, to pruning is supported by the Taylor expansion of the network function based on feature activations. Their suggested plan removes feature maps that show activation and gradient values simultaneously (Equation 7).
Although its ideal for the gradient of the output concerning activation functions to be zero at the point possible in theory; stochastic gradient assessments make sure this seldom happens in real world scenarios. When there is a slight variation in gradient among mini batches; it implies that a certain network parameter is not likely to undergo significant changes despite the input data. Signifying that these parameters are nearing convergence. Parameters or weights that are close to converging and produce activations are intuitively good choices, for pruning. This is what Eq 7 is about and it probably clarifies why focusing only small activations when pruning is not as successful as shown in the studys findings. The activation based method and the Taylor expansion based method differ in the types of weights they eliminate. 
The Taylor expansion method prunes weights that have activations but very low gradients while the activation based method does not perform this pruning technique. 
Weights, with activations but high gradients are removed using the activation based approach whereas the Taylor expansion method does not prune them. 
It would be intriguing to explore which of these two scenarios (Scenario 1 or Scenario 2 ) has an impact in determining the disparities in weight removal by the two approaches. Generally speaking weights meeting condition (Scenario 2 ) are crucial since they have been converged and play a role, in influencing the networks activation. One potential enhancement could involve adjusting the criterion for example Eq. ( 7 ). Λ × feature activation (where λ is determined through cross validation ) which might lead to improved outcomes albeit requiring parameter adjustments. 
Another interesting point to consider is the comparison with the damage framework which assumes zero first order gradients and prunes based on second order information as mentioned in the appendix by the authors. In this framework only the diagonal elements of the Hessian are calculated. The authors suggest that a comparison with the optimal damage approach was left out because of its inefficiency, in terms of memory and computational resources. Rough estimates indicate that implementing this change would lead to a 50 percent rise in memory usage and computational demands during pruning while not affecting efficiency, in testing scenarios at all. Looking at it from a deployment angle the absence of this comparison appears unwarranted. 
The writers are trying to decrease complexity in operations (GFLOPs) but recent studies have looked into using lower precision calculations to achieve the same goal instead of pruning techniques alone.The comparison between GFLOPs in precision calculations and pruning methods would provide valuable insights.It is suggested that these two methods can be used together to enhance performance; however it is still uncertain how effective pruning can be, in the context of precision operations.An analysis of this trade off would be interesting and informative. Not absolutely necessary. 
In terms of tuning models like AlexNet and VGG on various datasets such as Flowers and Birds separately by the authors in the study report; it would offer better insights if the results for both networks, on both datasets were included to give a more comprehensive view of how well the method can be applied across different scenarios. 
The researchers noted a decrease in performance following pruning but found that the pruned networks were effectively trained for a total of N + M iterations when accounting for fine tuning post pruning on the original network trained for N iterations. To ensure a comparison with the original networks accuracy after N + M iterations of training is complete as well. In Figure 4. Is the performance at 100% parameters of accuracy after N + M iterations or only, after N iterations? 
The paper as a whole is strong both in aspects and empirical evidence. It introduces an approach, to pruning that integrates; 
Utilizing a method rooted in Taylor expansion, for streamlining and optimization purposes 
Using feature normalization to minimize the need, for adjusting parameters and enhancing simplicity. 

However I would appreciate it if you could include the comparisons I mentioned in my comments. If these comparisons are added I would change my rating to acceptance.