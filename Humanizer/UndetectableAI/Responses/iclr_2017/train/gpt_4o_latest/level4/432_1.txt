This paper is written well and effectively brings together some value based and policy based methods (regularized policy gradient). It establishes connections between the value function and policy that haven't been explored before making valuable theoretical contributions that are both unique and thought provoking in a way that could have a considerable influence, in the field of reinforcement learning beyond just the algorithm described in the paper. Additionally the writers apply this concept to create a structure that merges Q learning with policy gradient approaches resulting in a performance that equals or exceeds the best algorithms, on the Atari benchmark collection.The practical segment is well explained enunciating the enhancements that were implemented. 
One small thing to note is the distribution that stays constant for a strategy being utilized. There are distinctions between using a discounted distribution versus a non discounted one that aren't crucial in simple scenarios but might need to be thought about carefully in upcoming research involving approximating functions. However this isn't a problem, for the current draft of the paper. 
In summary this research paper is well suited for acceptance. Is poised to make a significant impact onthe community of reinforcement learning.It sets a path for theoretical progress and innovation, in algorithm creation. 