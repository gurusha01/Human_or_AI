The writers introduce TensorFlow Folds implementation that allows running calculations without the need to adjust the computation graph by creating a universal scheduler within a TensorFlow computation graph that processes a graph description, for execution. 
They effectively showcase the benefits of using this method in situations where the calculation differs for each data point. A scenario commonly seen in Tree Recursive Neural Networks (Tree RNN). 
During their tests and trials of the approach presented in the study report, by the authors involved comparing it with two standard practices as a means of benchmarking its effectiveness. One involving a fixed batch (utilizing the same graph structure repeatedly) and the other employing a batch size of 1. 
I gave it a score of 7 of a higher score because they didn't compare their approach to the main alternative option effectively enough. To clarify further. Than using their graph as the scheduling mechanism, for each dynamic batch generated separately in TensorFlow graph could be an alternative. In terms. One could create each non uniform batch explicitly as a TensorFlow graph and run it using standard TensorFlow features. 