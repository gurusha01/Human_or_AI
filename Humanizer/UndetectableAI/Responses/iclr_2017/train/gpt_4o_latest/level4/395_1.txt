The writers present a software tool for probabilistic programming that takes advantage of the latest tools developed by the deep learning community.The new software seems promising and could change how work is done in the field of probabilistic modeling by allowing for quick experimentation and idea refinement.The application of composability principles is especially interesting. Expanding inference capabilities to methods like HMC goes beyond just simple VI inference (which can already be done with current deep learning frameworks) adding more appeal, to the software package. 
The important factor to consider when evaluating a probabilistic programming language (PPL) is how practical it is for real world applications; however this aspect wasn't clearly showcased in the submission provided for review. While the paper contains code examples, as illustrations​​​​​ many of them lack empirical evaluation​​​​​. One key example is the Dirichlet process mixture model shown in Figure 12. Do the suggested black box inference tools perform well in this scenario​​​​​ ? Additionally​​​​​ will the Generative Adversarial Network (GAN ) outlined in Figure 7 reach convergence when trained with real world data​​​​​ ?In order to convince the community of the usefulness of the package in practice it is crucial to show these aspects through experience. Currently the only model that has been tested is a VAE, with ways of drawing conclusions, which can be easily put into action using just TensorFlow. 
Lets start with the presentation.
The paper could use some enhancements in how it's presented. For instance; the authors can offer more hints to help readers navigate through the explanations better. They introduce terms like 'beta' and 'Z' without clarification, by page 5. It would be helpful if the authors could indicate that an example will be provided shortly after mentioning these terms. 
I suggest adding an explanation in the introduction, about how the layersre put into practice and how the KL divergence is managed in Variational Inference (VI) for instance. 
It would be beneficial to talk about the values that are maximized and which ones fluctuate during inference ( prior to section 5). This aspect lacked clarity, for a portion of the document. 
Lets conduct some tests; 
Why isn't the duration of the program provided in Table 1? 
What challenges are involved in achieving convergence when it comes to entropies discussed here? With increasing automation in inference processes come difficulties, in pinpointin issues accurately. Does the software package include tools to tackle this problem effectively? 
"Was the performance of HMC satisfactory in the experiment discussed on page 8s conclusion, with the runtime being noted?"
Is it difficult to make methods like HMC work properly when users don't have control, over the structure of the computational graph and sampler being used? 
It would be really helpful to have a chart that shows how different inference tools perform in terms of things like speed and accuracy, for models. 
What kind of benchmarks are they thinking of setting for the Model Zoos plans for evaluating and comparing probabilistic models without standard benchmarks in place yet in that field of modeling and analysis tools like Caffes Model Zoo that derive their value from having well established benchmark datasets such as ImageNet; however it remains uncertain which data sets will be employed to assess models, like the Dirichlet Process Mixture Model (DPMM).
I have some feedback, to share.
I recommend contrasting the findings in Table 2 with those of Li & Turner using α value of ̶zero point five (representing Hellinger distance) since their research indicated that this particular value yields results The rationale behind opting for α equal, to negative one remain ambiguous in this context. 
How do you deal with distributions like, in Figure 5?
In Figure 7 the variable `X_real' is not specified. 
Please emphasize the letter " M”, in Figure 8 to make it clearer. 
Please change the period to a comma after "rized) In," on page 8. 
Conclusively speaking of the software advancements discussed in this study brings a sense of enthusiasm as I appreciate the authors efforts in aiming for user friendly "inference for all." Nonetheless, at its stage of development I am inclined to give a rating of 5 to the submission. 