Human like rewritten text; A significant advancement in learning occurred with the introduction of the semi supervised learning framework that utilized unsupervised learning principles to determine properties, like distance metrics or smoothness regularizers and then applied them along with a small set of labeled examples based on the assumption of data manifold smoothness. 
This paper aims to expand upon this comparison in the context of reinforcement learning; though the comparison may be considered fragile in nature. In RL settings rewards do not equate directly to labels, in learning and the connotations of positive or negative rewards differ from those of positive and negative labels. However the paper puts forth an effort to delve into the realm of semi supervised RL. An area that is certainly significant yet remains underexplored. The authors introduce the term "labeled MDP" to refer to a MDP framework where the reward function is well defined. On the hand; they refer to an "unlabeled MDP" when the reward function is unclear even though its technically a controlled Markov process (CMP) not an MDP. This could cause some confusion. 
In RL transfer learning settings the goal is for the agent to transfer expertise from a labeled source MDP to a labeled target MDP. Where both reward functions are understood but the policy is only learned for the source MDP. In the suggested supervised RL framework the target is an unlabeled CMP while the source consists of both a labeled MDP and an unlabeled CMP. The main approach includes using inverse reinforcement learning to deduce the labels " which represent the reward function and then establishing a transfer system afterward for implementation purposes within linearly solvable Markov decision processes (MDPs). For considerations specifically limited to linear MDP solutions only due to practical constraints mentioned above in this context by the researchers presentation of outcomes, from experiments conducted across three intricate domains simulated through the Mujoco physics engine. 
Although the research is interesting to read about the reviewer feels that it doesn't provide an widely applicable structure for semi supervised RL that would capture the attention of many in the RL field. Creating such a framework poses a challenge, for future studies. Nevertheless the current work is. The issue it tackles is certainly worth exploring. 