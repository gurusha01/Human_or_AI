The document mentions that once the controller RNN has completed its training process; the performing RNN cell with the lowest validation perplexity is selected for further investigation through a grid search on parameters such as learning rate variation and weight initialization methods as well as experimentation, with dropout rates and decay epoch settings. Subsequently it proceeds to refine this performing cell by testing it under three different setups of configurations and sizes to enhance its overall capacity. 
Could you please share the hyperparameters and the type of dropout used (like recurrent dropout or embedding dropout)? Not having this information would probably mean having to try things extensively to reproduce the results accurately as shown in "Recurrent Neural Network Regularization" by Zaremba et al., 2014 where sharing a standard set of hyperparameters helped advance the field significantly. 
This depth of information would probably be helpful, for the tests too. Like the language model that works at the character level. 
The paper discusses a research topic within our field. The automation of architecture search process​es​. While the current method is demanding in terms of computing power​ it is anticipated that this trade off will become more favorable, with advancements in the coming years. 
The research covers typical vision and text assignments by testing the approach across multiple recognized datasets to show that exploring unconventional search spaces beyond RNNs and CNNs can lead to notable enhancements in results. It's always good to witness the application of this method to datasets; however the experiments showcased here already indicate that the technique is not only competitive with architectures designed by humans but could even outperform them. Moreover the results propose a strategy for customizing architectures to specific datasets without the need for manual adjustments, at every stage. 
This paper is well crafted and covers a subject matter with impressive findings; I highly suggest accepting it. 