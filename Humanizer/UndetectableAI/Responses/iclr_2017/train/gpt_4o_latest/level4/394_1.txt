The writers present a technique to enhance the performance of recurrent neural networks by regularization efforts in a manner akin to dropout methodology but with a twist. Rather than nullifying units outrightly; they opt to replace them with their corresponding values from the previous time step on an individual basis, with a probability assigned to it. 
The paper is nicely written with a presentation of the method despite some concerns raised in the pre review questions phase.The section on related work is thorough. Probably offers the most extensive examination of literature, on RNN regularization that exists at present. 
In the part of the study they test the new approach against the best existing method using various NLP tests and a made up task. All tests are done with sequences that have values. Furthermore one test shows that the continuous Jacobian is notably greater for connections in contrast, to the standard dropout method. 
Although the paper seems promising however I do have a few concerns.
During the review phase it was observed that the authors did not carry out a comprehensive search for hyperparameters as is customary in this field of study.The absence of a model selection process is worrisome considering the number of authors involved which should have allowed for such an analysis to be conducted.Table 2 indicates that validation error does not reliably predict test error for the dataset being considered.This situation raises worries, about overfitting in the model selection process. Moreover​，the enhancement in performance, from Zoneout seems minimal in other tasks. 
The exploration of Zoneout from a perspective is lacking depth in certain areas. For example a detailed examination of the gradients from unit K at time step T to unit K’ at time step T R could provide insights especially considering that these gradients may not be zero in scenarios involving dropout. Furthermore the potential variational interpretation of Zoneout remains uninvestigated, similar, to the work done by Yarin Gal. While one could argue that implementing Zoneout in a ResNet structure with dropout for components may be feasible; the paper lacks, in depth explanation regarding the effectiveness of Zoneout technique therein. The available resources provide entry points for delving into this analysis; however none of them are explored further. 
The data sets utilized in the tests consist of symbolic data sources. It would have been advantageous to test the approach, across a variety of tasks that include continuous data derived from dynamic systems. It remains uncertain whether the method as suggested could be applied effectively in scenarios. 
There has been a rise in methods to enhance RNN training lately. What sets Zoneout apart from the rest? Despite its user friendly concept the paper lacks strong evidence, in several aspects. The experimental findings are not persuasive (check out points 1 and 3) and the theoretical insights are somewhat restricted (refer to point 2).
To sum it up nicely. The paper presents an improvement termed as an "epsilon improvement " written well but lacks depth in both experimental evaluation and theory comprehension. 