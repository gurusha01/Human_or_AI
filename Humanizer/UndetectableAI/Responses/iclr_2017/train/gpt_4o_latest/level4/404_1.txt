This research paper introduces the Quasi Recurrent Neural Network (QRNN) a model that cuts down on the workload when handling temporal changes in sequences of information effectively and efficiently.It essentially enhances the LSTM design by streamlining it through preserving the diagonal components in the transition matrices and expands connections from lower to upper layers by using general convolutions over time (in contrast to the standard LSTM which can be seen as a convolution, with a one time step temporal window). 
The authors point out that the QRNN has similarities with recent RNN variations such as ByteNet and strongly typed RNN (TRNN). Due to these connections, to existing models the uniqueness of the QRNN is somewhat diminished; however I feel that the research brings forth novelty to justify its publication. 
The researchers present empirical evidence to support the assertions put forth in the document indicating that this particular adaptation of the LSTM warrants additional investigation, from the academic community. 
While I see the input, as a step forward overall I suggest approving it for the next stage of consideration. 