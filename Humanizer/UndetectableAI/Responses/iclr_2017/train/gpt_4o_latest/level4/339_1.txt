The writers suggest a way to include a cache in neural language models that allows for a mechanism to copy recently used words efficiently without the need for extensive training via long term backpropagation like many other approaches in neural network copying mechanisms.This makes it both effective and adaptable for cache sizes.The writers show enhancements, in language modeling performance by adding this cache to RNN baselines. 
The main point of this paper is that by using the states \( hi \) and \( h_t \) as keys and query vectors for words \( xi \) a lookup mechanism can be created that works well even without needing backpropagation based adjustments.This observation may seem simple. Already known to some researchers as common knowledge but it has important implications, for scalability.The results of the experiments presented are convincing. 
The concept of using acquired representations to improve large scale attention in scenarios where backpropagation is usually computationally challenging is fascinating and shows promise, for enhancing various memory networks. 
I have some feedback on this piece regarding its simplicity and gradual progress compared to works in the field of literature before it. It seems like an adjustment to existing NLP models with solid empirical evidence backing it up in terms of simplicity and usefulness; thus making it more suitable for a conference focused on NLP. Nevertheless I feel that methods that simplify and improve upon advancements to create efficient tools, with broad applications should be acknowledged. This approach is likely to benefit a portion of the ICLR community and therefore I suggest its publication. 