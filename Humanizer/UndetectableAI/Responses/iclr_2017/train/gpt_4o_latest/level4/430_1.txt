This document presents a technique for understanding sequence breakdowns like words in speech recognition tasks which addresses a challenge with promising applications in various fields including machine translation as well.I find the method suggested to be original and well supported. I suggest also examining it against byte pair encoding (known as BPE). Comparing it against BPE provides an crucial point of reference highlighting the differences between dynamic and fixed decompositions.The assessment should cover BPEs performance, across vocabulary sizes. 
Could you please provide some feedback on the comments?
The question is whether the identified breakdown align with units that make sense phonetically or if the model is simply recognizing groups of characters based on the example, in the appendix. 
Have you thought about possible uses besides speech recognition tasks as well? Showing its efficiency in areas could really boost its impact even further. But this might not be, within the current focus. 