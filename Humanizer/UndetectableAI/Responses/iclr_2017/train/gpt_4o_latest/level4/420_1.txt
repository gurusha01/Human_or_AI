This paper focuses on how attention mechanismsre employed in neural language modeling and introduces two main contributions; 
The writers suggest using key and value vectors for the attention mechanism instead of depending on a single vector to handle all tasks. A fresh approach that could be relevant in various fields, beyond the standard attention mechanism.   
The writers show that even a brief attention span is enough, for language models (which's n't really shocking) and suggest using an ngram RNN to take advantage of this discovery. 
The article presents approaches to neural language modeling and shares interesting findings from the authors thorough experiments, on different tasks including language modeling and the CBT task. 
The answers provided by the authors to my inquiries meet my expectations. 
A small recommendation would be to make sure the related work section references Ba et al., Reed & de Freitas and Gulcehere et al. 