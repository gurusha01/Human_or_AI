This paper introduces a network model that adjusts its size as it trains without using predetermined parameters. The idea is to remove certain units and use a technique to get rid of unnecessary weights automatically. Think of it as exploring possibilities in a specific way, with the help of this technique used to remove unneeded elements. The issues discussed are important. The results shown in the paper are quite interesting. Here are my main thoughts; 
What is the extra computational load required by the algorithm being suggested here? Breaking down each input weight into perpendicular elements and converting them to radial angular coordinates may lead to a significant increase, in computational demand. The writers should examine how more operations are needed compared to a standard neural network model. Moreover sharing the outcomes of time taken during experiments would be helpful. 
I've noticed that nonparametric networks often create networks on convex datasets which may result in lower performance when compared to parametric networks Could the authors share any thoughts, on this finding? 