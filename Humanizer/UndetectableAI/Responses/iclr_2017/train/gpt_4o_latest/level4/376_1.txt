This paper makes a contribution by thoroughly and clearly examining the performance and trainability features of different neural network structures with a specific emphasis, on the fundamental RNN patterns that have become popular in recent times. 
Advantages; 
The article addresses an issue that has been of interest to many in the community and myself but we lacked the necessary computational power to thoroughly investigate it before now. Leveraging Googles resources in this way is an addition, to the field.   
The research seems to have been carried out and this adds weight to the findings being shared.   
The main finding is that LSTMs are dependable, in training settings but GRUs are frequently seen as the more practical option. A conclusion that is clear and valuable The paper effectively delves into the intricacies of this outcome in a thorough manner.   
The focus placed upon differentiating between capacity and trainability proves to be quite valuable as it helps dispel misunderstandings regarding the effectiveness of gated architectures.The studys observations that gated architectures are notably more trainable but slightly less capacious than vanilla RNNs and that, in demanding tasks trainability surpasses capacity are both thought provoking and clearly explained.   
The insight regarding the identical capacity when the number of parameters is the same is quite enlightening.   
The paper highlights the significance of fine tuning hyperparameters which is occasionally disregarded amid the surge of novel architecture suggestions.   
The inclusion of a measure to assess the proportion of parameters (such as those causing divergence) is a useful and considerate enhancement that tackles a prevalent yet frequently overlooked concern, in the domain.   
The document is extremely clear in its writing style. Is very easy to understand and follow along, with. 
Downsides; 
The parts about UGRNN and + RNN seem a bit introductory to me.The authors haven't quite proven that + RNN should be universally recommended like GRUs are.For instance a robust statistical analysis showcasing the performance disparities between + RNN and GRUs, in Figure 4 (the 8 layer panel) would bolster their argument.The rigorous criteria outlined in the paper for deeming an architecture beneficial downplay the significance of UGRNN and + RNN contributions. Nevertheless having them mentioned in the paper shouldn't pose any issues since the main focus of the study doesn't seem to be, on originality, which's completely fine. Nonetheless I'm unsure how this will be perceived by the area chairs at ICLR.  
The article doesn't delve deeply into the specifics of the hyperparameter tuning method it discusses even though the authors briefly touch on it.   
  We configured the tuners settings to utilize Batched GP Bandits along with an anticipated improvement acquisition function. We also incorporated a Matern 5, over 1.5 Kernel with feature scaling and automatic relevance determination achieved by optimizing over kernel HP parameters.  
  To replicate this method effectively would probably necessitate specific information that isn't included in the current text references.   
Interpreting figures can be tricky initially because of their small sizes in certain cases like Figure 4 and the detailed density alongside subpar choices, for visual clarity.   
The mention of neuroscience in terms of " 5 bits per synapse" seems slightly unrelated to the main point being discussed here about the results and their connection, to experimental neuroscience is not clearly outlined or adequately elaborated upon in my opinion In the discussion section where this reference is made it may not be crucial and could be expressed with more cautionary language. Perhaps of asserting a direct correlation between computational structures and neuroscience findings the writers could propose a thought provoking question such as; "Could it be more, than mere chance that our 5 bit outcome closely aligns with the 4.​gypt 1970 Ninth month of inundation​? 