This study delves into the synchronization of word representations, in languages when the embeddings are trained separately in single language environments.The method tackles an applicable issue since there are potential situations where this approach could be valuable.While the paper is overall well developed the assessment seems constrained.Introducing a robust real world application would have enhanced the research. 
The concept of Softmax being inverted is truly intriguing. 
There are a couple of concerns that need attention in an updated draft of this paper.
The paper fails to mention the study by Haghigh et al. titled "Learning Bilingual Lexicons from Monolingual Corporations " which seems to be a prior research piece concerning the application of CCA for bilingual alignment purposes in the current context. This study and its significance in relation, to the approach need to be addressed in further detail. 
1 Also worth mentioning is the work by Hermann and Blunsom (2013) titled " distributed representations without word alignment," which appears to be a suitable reference, for acquiring multilingual word embeddings using aligned multilingual datasets. 
The studies could have been more interesting if they had involved a range of language pairs instead of just concentrating on European/Romance languages. 
The emphasis on meeting orthogonality criteria appears to be related to the notion of employing a Mahalanobisdistance or covariance matrix for acquiring these mappings, which could benefit from further discussion, in this regard. 
When thinking about how to describe aligning words, across languages accurately than using the term "translation (performance/etc.) it seems like this term may not fully capture the complexity of the process and could benefit from a more precise description. 
The abstract incorrectly cites Mikolov so it needs correction. 