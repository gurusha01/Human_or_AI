The article presents a learning method designed for managing small scale operations in combat situations in real time strategy (RTS\) games\. It focuses on an aspect within the larger RTS field\. The specific approaches and limitations used (such as MDP and action encoding based on distance\) are clearly explained and suitable for the task, at hand\. 
The key innovation of this research revolves around the zero order optimization technique and its implementation in exploration tasks. This marks an application of zero order optimization combined with deep learning for reinforcement learning (RL) supported by reasoning similar to that backing deterministic policy gradients (DPG). The empirical findings show enhancements compared to conventional methods such, as vanilla Q learning and REINFORCE lending credibility to the approach. RTS is definitely an intricate field that deserves deep research attention; however it would have been advantageous to incorporate findings from diverse areas as well. This becomes especially important because the algorithm being proposed seems to hold promise beyond RTS games. Testing it out in domains could offer valuable insights, into how widely applicable it is and the kinds of issues that could benefit from this simplified approach. The authors might want to think about expanding on this idea with explanations or reasons to support their viewpoint. 
Some design decisions may appear random. Are supported by practical outcomes alone. For example the decision to consider the sign of \( w / \Psi_{theta}(sk, ak ) \). Likewise when it is mentioned that "we did not include the argmax operation for selecting actions " it leaves room for questioning intentions behind it. Could this and dividing by \( t \) perhaps aim to maintain values within or close, to the range of [  ̈C ̈01 ]? It could be beneficial to consider shortening or standardizing \( w / \Psi \) as focusing on the direction may lead to losing valuable information. Furthermore comments like "We haven't tested different network structures but discovered that maxpooling and tanh nonlinearity are crucial " and the statement asserting Adagrads superiority over RMSprop, without additional explanation or evidence seem incomplete. The reader may wonder if these observations apply to the RTS setup in this paper or if they have a more general scope. 
The way the paper is presented could use some work since certain ideas are introduced without background information which can make it harder to understand whats being explained unnecessarily confusing at times The part where \( f(\tilde{s} c)\ is defined at the beginning of page 5 doesn't include an explanation for the \( w \ vector leaving the reader unsure about where it comes from or why its there While this gets cleared up later adding a short sentence here to explain its role (maybe with a nod, to the relevant section would make it easier to follow On page 7 it says "since we forgot that a lone \( u \)" is chosen for one whole episode"—this point was brought up earlier in the text and is clear, from the pseudo code; hence the wording can be confusing. 
There's a problem. "Perturbated " should be changed to "perturbed."
Sorry I can't assist with that request.
No response was given in return; hence the evaluation stands as is. 