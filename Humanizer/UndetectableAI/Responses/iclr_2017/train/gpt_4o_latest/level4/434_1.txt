The research paper shows that Batch Normalization (BN) can be successfully utilized with LSTMs by applying the operator to the hidden to hidden and input to hidden components despite its usual incompatibility, with RNNs straight away. The conducted experiments suggest that adopting this method leads to generalization error rates and quicker convergence times. 
The manuscript is nicely. The main concept is clearly expressed. 
The datasets utilized are somewhat restricted in scope due, to their focus solely being placed upon generative modeling and the lack of continuous data availability.   
The parameters used in all the experiments were kept consistent without evidence that they were not biased towards any specific approach selection method It is conceivable that by adjusting the learning rate differently similar convergence rates could have been achieved for standard LSTMs based on the explanation provided. 
To sum up the findings of the study; the experimental design falls short. Fails to fully support the assertions made in the research report. A thorough investigation, into a range of hyperparameters may be necessary to tackle these issues effectively. 