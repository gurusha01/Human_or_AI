This paper presents a framework based on information theory for self guided learning that focuses on maximizing the connection between input and output data using the infomax principle as a foundation.The authors suggest a two step process for learning in this context where they first break down the main task into smaller parts to make it more manageable and solvable, through direct calculations. These initial solutions act as starting points, for the solution and are then fine tuned through the gradient descent method. 
While the main storyline and explanations put forth in the document seem solid in theory; there's room for improvement in terms of clarity and how the content is presented for understanding by readers. One suggestion is to start with a summary of the outcomes and a clear illustration of how the derivations were arrived at before diving into the detailed step, by step derivations. The intricacies of these derivations that might cloud the findings could be saved for later sections or included as an addendum. 
Here are a few points that the authors might want to take into account; 
In the paragraph on page 4 of the document mentioned earlier states that maximizing I(X; R) as indicated in equation 2;20 will lead to the maximization of I(Y; R) and I(X,Yᵤ). While there is evidence to support the assertion through equation 2;20s equality conceptually speaking – the relationship between them is supported by a boundary specified in equation 2;21. Considering there may be a discrepancy between the values of I(X; R) and I(X,Yᵤ) it raises doubts as, to whether maximizing the truly results in maximizing the latter. 
In the paragraph before section 1 of part two of the document discusses how using dropout helps to avoid overfitting by trying to decrease the rank of the weight matrix without providing any clarification, on this comparisons validity. 
Towards the conclusion of page 9 in the text by the authors includes a discussion on finding the solution for C in two particular scenarios mentioned earlier; nevertheless it seems that only local optimal results can be ensured because of the non convex nature of constraint 2;80 (which is a quadratic equation). In case attaining the best outcome proves to be unattainable due to this constraints nature,a revision, in wording should acknowledge and adjust for this constraint. 