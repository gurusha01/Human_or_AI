This paper delves into using flows (NFs) applying them to maximum entropy constrained optimization to create intricate distributions with manageable likelihood values in a way that is simple and easy to understand for readers. 
The originality of the paper is somewhat restricted in scope. Its main contributions involve expanding research on Normalizing Flows to MaxEnt estimation and tackling optimization difficulties related to stochastic approximations of E[||T||] along, with adjusting Lagrange multipliers during annealing. While applying Flows to MaxEnt is not groundbreaking on its own since similar frameworks have been introduced in previous studies. For instance a comparable loss similar to that shown in equation (6) could be obtained through minimizing the Kullback Leibler divergence between the distribution \(p_{\phi}\) and the unnormalized likelihood \(f\) where \(f\) is directly related to exp(\sum_k( \lambda_k T. C_k || T_k ||^{ 2 })). This method reflects a practice, in earlier research that employed NFs for variational inference. 
To strengthen the research findings, in the papers context tests with more intricate data sets would be helpful. While the two experiments carried out show encouraging outcomes; they focus only toy problems that may not accurately reflect real world situations. 
I recommend discussing step 8 in Algorithm 1 to enhance clarity and intuition levels even further considering how intuitive the approach is overall. 