This document introduces the Quasi Recurrent Neural Network (QRNN) which's a model designed to simplify the computational demands linked to temporal changes in sequence information efficiently. In terms the QRNN alters the conventional LSTM structure by focusing solely on the diagonal components of transition matrices and expanding connections, between layers to include overall temporal convolutions redefining the typical LSTMs receptive field for a single time step. 
The writers recognize the QRNNs connection to recent RNN variations, like ByteNet and strongly typed RNN ( T RNN) which somewhat moderates the uniqueness of the model. However based on my evaluation the QRNN still has original aspects to justify its publication. 
The practical findings shared by the authors provide support, for the assertions made in the paper and indicate that this particular LSTM adjustment should be carefully examined by researchers. 
While I view the contribution as small in scale I recommend approving the paper for publication since it offers a meaningful new perspective, to the field. 