This study suggests an idea that involves using a special introspection neural network to predict future weight values by analyzing their past trends.The introspection network is taught using parameter paths collected from training meta learning models with a standard optimizer, like stochastic gradient descent (SGM).
Advantages; 
The layout of the document is quite organized and straightforward to understand.
The new meta learning strategy differs from methods of learning, to learning and provides a new viewpoint.
Areas that could use improvement; 
Extensive testing is required to confirm the effectiveness of the method across various types of neural network structures. These include connected and recurrent networks, with unique parameter configurations that differ from convolutional neural networks (CNN).
The descriptions of the setups, for the MNIST and CIFAR datasets are not sufficiently detailed in terms of architectural specifics.
The paper does not provide details, on the mini batch size used in the experiments.
The paper would be improved by including comparisons with baseline optimizers such as Adam and providing a clear rationale for selecting hyperparameters like learning rate and momentum, for the baseline SGD method. 
The current version lacks experimental details which makes it hard to draw definite conclusions, about the suggested approach. 