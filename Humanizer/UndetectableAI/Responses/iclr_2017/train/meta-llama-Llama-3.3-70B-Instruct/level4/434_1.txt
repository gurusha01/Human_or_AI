This research paper shows that Batch Normalization (BN) originally not designed for Recurrent Neural Networks (RNNs) can be successfully used with Long Short Term Memory (LSTM) networks by implementing the technique for the hidden to hidden and input, to hidden connections. The authors present results indicating that this method leads to better generalization accuracy and faster convergence rates. 
The article is nicely. Effectively communicates its main concept.  
However; there are a constraints that should be considered. 
The testing is limited to a selection of datasets and statistical assumptions; specifically excluding continuous data and focusing solely on autoregressive generative modeling. 
The hyperparameter configurations were mostly consistent throughout the experiments which sparked worries that they might have been cherry picked to support the suggested approach without exploring options like adjusting the learning rate to potentially achieve similar convergence rates, for the standard LSTM model. 
In summary the research methods have some issues. Don't offer enough proof to back up the assertions presented. A deeper investigation, into the range of hyperparameters could address these issues. Enhance the conclusions.