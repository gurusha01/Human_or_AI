The new approach recommends reducing the size of deep neural network weight matrices by adding a penalty, for density diversity and utilizing an optimization method that involves sorting weights and implementing a weight sharing approach.  
The density diversity penalty includes a cost factor that consists of the ltwo norm of the weights to show density and the lone norm of weight variances within a layer. 
Usually in weight matrices the value that occurs often is adjusted to zero to encourage sparsity.  
When the diversity penalty kicks in and pushes the weights to reach values they get linked and then adjusted based o n the average gradient. 
The training process includes two phases; 1. Training with the density diversity penalty and untied weights. 2. Training, without the penalty. With tied weights. 
The tests carried out on the MNIST and TIMIT datasets show that the approach is able to achieve compression rates while maintaining performance levels effectively. 
The article is nicely. Showcases creative concepts while seeming to lead in compression research trends The method of weight typifying could have noteworthy impacts extending past compression by possibly aiding in uncover ing patterns, within data. 
The outcome tables are a bit confusing though. 
There are some concerns to address; 
An English mistake is observed in the statement "when networks include layers".
Tables 1 through 3 are a bit perplexi​ng as the suggested approach (DP​) ​appears to show results than the baseline method (DC​) in various cases​. ​​In particular DP displays less sparsity and greater diversity than DC overall in Table 1 across FC in Table 2 and overall in Table 3. This inconsistency hints at a mix up, in the sparsity measurements potentially indicating non modal values instead of the true sparsity level. 