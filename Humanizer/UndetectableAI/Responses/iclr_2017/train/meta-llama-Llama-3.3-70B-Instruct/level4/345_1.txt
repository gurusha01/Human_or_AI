In this study introduced a Bayesian method for mastering the parameters of neural networks along with their related assumptions by applying a mixed model prior to the weights to create a grouping effect in the weight posterior distributions represented by delta peaks This grouping effect can be utilized for condensing and compress ing network parameters leading to compression rates and predictive precision similar, to current techniques. 
In comparison to studies like the one, by Han et al. the new method follows a more structured and streamlined optimization process instead of the three stage approach involving pruning and clustering cluster centers as discussed in 2015 paper by Han et al.. A preliminary test detailed in section 6 details how using a Bayes technique without hyper priors already leads to significant clustering outcomes by setting numerous weights to zero and achieving a compression rate of 64% on the LeNet300 model. It seems like there is a mistake, in the text mentioning figure C instead of figure 1. 
Section 6 point 2 describes an experiment that includes hyper priors where the parameters of these distributions and other hyperparameters like learning ratesre fine tuned using Spearmint as mentioned by Snoek et al in 2012. The outcomes displayed in figure 2 illustrate how different points perform within the parameter realm. Although the wording hints at results align along a straight line due to the restricted data available; a conversation, about the anticipated correlation would add value to the analysis. Now we need to analyze the findings of this experiment more thoroughly to fully understand them. 
The paper also discusses the findings of CNN models in section 6 with a comparison to studies by Han and Guo in achieving similar compression rates and accuracy levels as them in their research from 2015 and 2016 respectively. Despite recognizing the slowness of their algorithm when dealing with models such as VGG 19; they did manage to provide some initial outcomes for this specific model without making any direct comparisons, to other relevant studies. It would be beneficial to clarify the reasons for the decrease in training speed when compared to training without weight clustering and to explore how the suggested algorithm adapts with regard, to important data and model sizes. 
The main significance of this paper is seen in the outcomes of its experiments by using traditional empirical Bayesian learning principles to incorporate weight clustering impacts in training CNNs. Although the method is quite simple in nature; its ability to produce results on par with cutting edge network compression methods that're more informal is noteworthy. To enhance the papers quality further; a concise explanation of the training algorithm and how scalable it is, for networks and datasets would be advantageous. Furthermoreâ€‹ it would be beneficial to delve into the process of searching for hyperparameters and how different methods handle this search to balance accuracy and compression effectively by evaluating methods at various points, along this spectrum. 