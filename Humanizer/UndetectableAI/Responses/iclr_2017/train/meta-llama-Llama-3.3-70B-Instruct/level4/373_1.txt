The writers introduce a method, for adapting network models to different NLP tagging tasks using transfer learning techniques.  
In the realm of task learning within the field of machine learning approaches appear to lack notable innovation since they encompass exchanging elements of a standard NLP framework based on the requirements of the particular task, at hand. 
Nevertheless the uniqueness of this piece is, in the architectural approach utilized for tasks related to NLP tagging.  
The test outcomes show that this method performs well when theres not labeled information available (see Figure 2) and Table 3 illustrates some slight enhancements when working on a large scale. 
The results shown in Figure 2 can be interpreted differently since the authors kept an architecture size while changing the labeled data amount. Adjusting the architecture, for each data size might have resulted in better outcomes. 
Ultimately​, though​ the paper is articulated well​ the originality may be somewhat limited​ and the experimental segment doesn't quite meet expectations. 