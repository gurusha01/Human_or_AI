This study introduces an evaluation method for generative models that rely on decoders. It utilizes Annealed Importance Sampling (AIS) which helps in estimating log probabilities. This framework aims to fulfill the requirement for evaluations since current assessments often focus more on qualitative aspects when dealing with models, like Generative Adversarial Networks (GAN)s and Generative Moment Matching Networks (GMMNs). Even though methods such as Kernel Density Estimation (also known as KDE for short) are available for use in this context. The researchers showcase the accuracy of AIS and its ability to make detailed comparisons between different generative models, like GANs (Generative Adversarial Networks) GMMNs (Gaussian Mixture Model Networks) and VAEs (Variational Autoencoders).
The researchers present findings where they compare two decoder designs trained on the continuous MNIST dataset using VAE,GAN and GMMN goals.They also conduct training for an Importance Weighted Autoencoder (IWAE), on binarized MNIST revealing that the IWAE limitation significantly underestimates log probabilities by a minimum of 1 nat based on the AIS assessment. 
This paper has some strengths like its open evaluation framework that adds value to the field of study and provides a fresh perspective, on GAN behavior based on log likelihood analysis. It questions the belief that GAN models simply memorize training data and sheds light on their tendency to overlook crucial patterns in the dataset distribution. 
However​. There are a things that could be clearer in the analysis of the data here​. It's a bit confusing how different example numbers (like 100 or 1000 or 10000) coming from different sources (the training set or validation set or test set​. Or even created by the model) are used inconsistently in the experiments​. For example​. Table 2 only shows results using some examples from the test set​. And in Figure 2​​​​​​​​​​​‌‌‌‌​​‌​​‌a comparison, between AIS and AIS plus encoder is made where AIS seems to be slower but its not explained clearly enough especially when it comes to how many intermediate distributions are used in each case. 
The use of 16 separate chains for AIS appears to be below the typical benchmark of 100 chains found in existing literature studies that address similar topics; expanding the number of chains could potentially improve the accuracy of the confidence intervals provided in Table 2 significantly.The substantial difference of 10 nats in BDMC for GAN50 compared to variations raises the need for a more detailed and intuitive explanation, from the authors. 
There are an issues to address regarding Table 1 – it would be beneficial to include references and provide more detailed explanations for the columns used in the table itself. In Figure 2(a) it's important to clarify whether the values presented indicate the log likelihood of either 100 individual examples or the total training and validation set for MNIST. When looking at Figure 2(c) which seems to be based on binarized MNIST data there are points, for AIS compared to IWAE and AIS + encoder without a clear rationale provided for this difference. Regarding the BDMC gaps discussed in Section 5.31 it's essential that they are explicitly confirmed as those outlined in Table 2. In the section of Figure 3 caption mistakenly states "(C)" referring to GMMN 10 instead of the correct representation, for GMMN 50. 