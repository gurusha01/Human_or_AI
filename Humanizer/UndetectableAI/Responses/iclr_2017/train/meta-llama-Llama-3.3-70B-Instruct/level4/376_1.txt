This study makes an impact by thoroughly analyzing how well different neural network structures perform and can be trained. Focusing on the basic RNN patterns that have become popular in recent times. 
The papers strong points are as follows; 
It raises a query that I and other researchers have been eager to delve into but lacked the necessary computational power to thoroughly investigate it; utilizing Googles resources effectively for the communitys benefit. 
The study seems to have been carried out with care to guarantee the accuracy of the findings. 
The results indicate that LSTMs are dependable while GRUs are commonly favored in training settings according to the research findings mentioned in the paper; these conclusions are clear and offer practical insights, into the nuances explored in the study. 
Distinguishing between capacity and trainability is important to understand the effectiveness of gated architectures they are easier to train but may have slightly lower capacity compared to vanilla RNN models; however in challenging tasks the improved trainability outweigh the reduced capacity. 
The point that capacity is the same, with an equal number of parameters is very helpful. 
The article highlights the significance of tuning hyperparametersâ€”a critical element that is occasionally disregarded in the multitude of papers discussing novel designs. 
The concept of measuring the percentage of parameters, like those that deviate is a meaningful addition because it deals with a real issue often faced when dealing with these networks. 
The article is clearly and succinctly written. 
However, 
The investigation of UGRNN and + RNN models appears to be in its stages. It is uncertain if the + RNN model should be endorsed widely as the GRUs. Extensive data analysis is needed to determine the differences in performance between + RNN and GRUs as depicted in Figure 4 (panel, with 8 layers).
The article doesn't delve much into the specifics of the hyperparameter tuning method itself; this lack of detail could pose a challenge, for replication despite citing research studies. 
It can be a bit challenging to read images like Figure 4, at first because the panels are small and theres a lot of details packed in with not so great visual design choices. 
The mention of neuroscience (" 5 bits per synapse") seems a bit unnecessary since the link, between the computational findings and real world neuroscience is not clearly established and could be adjusted to recognize the speculative nature of this comparison. 