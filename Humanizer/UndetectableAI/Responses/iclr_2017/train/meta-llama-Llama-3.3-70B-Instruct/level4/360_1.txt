Supervised learning saw an advancement when semi supervised learning was introduced to make use of unsupervised learnings less robust method in determining certain characteristics like distance measures or consistency regulators.This derived information can then be incorporated alongside a number of labeled instances with the usual belief in smoothness, across manifold assumptions. 
This paper aims to expand on this idea in the context of reinforcement learning but with an analogy that may not align perfectly. The comparison between labels and reward functions doesn't translate directly since positive or negative rewards don't hold the weight as positive and negative labels. However this study offers a perspective, on the concept of semi supervised reinforcement learning which certainly deserves more attention. The writers use the phrase "labeled MDP" to refer to the MDP setup with an undisclosed reward function and dub the term "unlabeled MDP" for a situation where the reward remains unknown; this actually reflects a controlled Markov process rather, than a Markov decision process. 
In typical reinforcement learning scenarios for transfer learning purposes an agent aims to shift acquired knowledge from a specified MDP in the source setting to another specified MDP in the target scenario where both reward functions are recognized; however the policy learned is solely accessible in the source MDP.In contrast, to this process the semi supervised reinforcement learning context comprises a controlled Markov process as the target and a configuration encompassing both labeled and unlabeled MDPs as the source. The suggested method entails utilizing inverse reinforcement learning to deduce the labels " followed by efforts to support transfer learning endeavors successfully underway, in the research field. Furthermore the researchers have set a constraint limiting their focus to solvable Markov Decision Processes (MDPs) for practical reasons. They showcase outcomes across three moderately intricate domains simulated within the Mujoco physics framework. 
While the research is interesting to me as a reviewer; I feel that it doesn't quite capture an simple enough understanding of semi supervised reinforcement learning to truly captivate the reinforcement learning community at large just yet.This poses a challenge that future studies need to tackle; however, for now the current work is sufficiently. The issue being explored is definitely deserving of further examination. 