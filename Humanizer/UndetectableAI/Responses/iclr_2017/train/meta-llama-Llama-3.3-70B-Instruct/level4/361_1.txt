This study addresses the issue of forecasting learning trends by standing out from studies through (a.) developing a neural network capable of adapting to various hyperparameter setups and (b.) incorporating a Bayesian neural network along, with Stochastic Gradient Hamiltonian Monte Carlo (SGHMC). 
The authors effectively showcase how well their suggested method works in predicting learning curves for different network architectures such as connected networks (FC) convolutional neural networks (CNN) logistic regression (LR) and variational autoencoders (VAEs) even when only partially observed curves are available or no curves are observed at all. This indicates a potential for use, in Bayesian optimization tasks and conducting further experiments to compare the benefits of this approach would be very beneficial. 
Exploring the idea of adjusting the learning rate over time could improve the approach more effective. One possible tactic is to apply the algorithm to a chosen portion of the data and then draw insights, from that subset. 
In addition to the squared error (MSE ) and log likelihood (LL) it could be useful to consider different evaluation criteria, for analysis purposes. When dealing with real world situations determining the successful run is crucial. Therefore assessing how accurately each approach identifies the top run could offer perspectives. 
Here are a few small recommendations; 
To enhance readability in copies of figures and axes labels and legends would be beneficial, by enlargening the font size which is currently too small to read comfortably. 
Fig 6 seems to show some discrepancies, in the quantity of lines displayed there.It would be beneficial to provide clarification regarding whether the lines overlap in instances as this could impact how the results are understood. 