This study delves into how adjusting model parameters while training and employing a smoothed objective function for optimization are related concepts explored in the paper. While the description of the gradient g in Equations 4 to 7 may be ambiguous Equation 8 is clear and Section 2 point 3 effectively shows how minimizing the smoothed loss is equivalent to training with Gaussian parameter noise, for certain smoothing functions. 
The authors build upon this idea by introducing smoothing functions to create a more intricate cooling effect that can be utilized with modern neural network designs like deep ReLU networks and LSTM recurrent networks. Nevertheless the resulting cooling effect displayed in Section 4 might seem paradoxical; the Binomial (or Bernoulli ) parameter rises from 0 (indicating identity layers ) to 1 ( indicating deterministic ReLU layers) hinting at an initial introduction of noise, by the network that could potentially reverse the cooling effect. 
The practical methods of annealing implementation seem to be crafted and tailored for specific purposes; for instance Algorithm 1 outlines a detailed 9 step procedure for deciding unit activation at every layer. Considering the application of the smoothing framework to annealing in the authorsâ€™ work it might have been helpful to include a portion, in the paper that delves into examining uncomplicated models utilizing elementary smoothing techniques. It would be interesting to observe cases where the perturbation methods based on the smoothing concept excel over heuristic perturbation methods, in optimization tasks to showcase the efficacy of the suggested approach. 