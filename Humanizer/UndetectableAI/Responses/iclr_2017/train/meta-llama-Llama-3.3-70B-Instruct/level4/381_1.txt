The writers introduce a method, for trimming weight to lessen GFLOP calculations by drawing inspiration from the Taylor series expansion of the network function concerning feature activations. This technique efficiently filters out feature maps that have activation and gradient values simultaneously as demonstrated in equation 7.  
The goal is for the outputs slope and the activation functions to approach zero at the spot; however with stochastic gradient assessments in play this is rarely met in reality. A slight difference in gradients between mini batches indicates that a certain network parameter may not change much and is nearing convergence. Parameters near convergence leading to activations are suitable, for pruning as described in equation 7 essentially. This is why simply discarding weights associated with activations is not as impactful, as the suggested approach showcased in the findings of the research paper.  
The activation criterion and the Taylor expansion differ in how they handle removed weights. 
Some weights are eliminated using the Taylor expansion method when they have activations but very low gradients whereas activation alone does not handle this removal process effectively.  
Weights that have activation levels but high gradients are excluded based the activation criterion but not according to the Taylor expansion method.  
It would be intriguing to investigate which of these distinctions has an impact in the changes in weights produced by the Taylor expansion compared to the activation criterion. On a gut feeling level the weights that meet the criteria hold significance since they are reached and play a significant role in the networks activation. A tweaked criterion combining equation 7 with feature activation multiplied by λ (, with λ decided through cross validation) could potentially yield improved outcomes although requiring parameter adjustments.  
A relevant point to consider is the comparison with the damage framework that involves pruning based on second order data while assuming first order gradients are negligible. The authors touch upon this in the appendix. Argue that it is inefficient in terms of memory and computation usage. However a quick calculation reveals that this might only lead to a 50% rise in memory and computation needs during pruning without impacting efficiency, in testing. Hence from a viewpoint this evaluation appears reasonable and worthy of thought.  
The main objective of the authors is to decrease computing power in GFLOPs by considering precision calculations in their recent works for optimization purposes. It would be beneficial to compare the GFLOP values achieved through precision computations, against those obtained through pruning techniques since these methods complement each other and combining them could enhance overall performance. However it remains uncertain how much pruning can be implemented when operating within the precision threshold; hence an examination of this compromise is worthwhile though not imperative.  
In relation to tuning models like AlexNet and VGG on various datasets such as Flowers and Birds respectively the authors presented their findings.However it would be more informative to have the outcomes of both networks, on both datasets for a thorough comparison.  
The researchers noticed a decrease in performance following pruning efforts; however for a fair evaluation, between the original and pruned networks the original network should be trained for N + M iterations to match the fine tuning of the pruned network. In Figure 4 it is not clearly stated whether the accuracy reported at 100% parameters reflects results after N + M iterations or N iterations alone.  
In general and based on both empirical evidence presented in the paper suggests a new method for trimming using Taylor expansion technique along, with feature standardization and gradual refinement process is effective.. Improving the document by including some of the recommended comparisons could enhance its quality and possibly result in a revised acceptance rating. 