In this article we discuss how a familiar soft mixture of experts model is tailored for a particular transfer learning challenge in reinforcement learning that involves transferring action policies and value functions across tasks. The experimental setup used in the study is similar, to reinforcement learning methods; however the connection is not extensively examined.  
One possible outcome of this study is that decisions regarding the design and algorithms could be based on the desired goal of the task of depending solely on manual intervention by engineers This suggests an interesting path for further exploration, in research. 
The papers notable aspects are; 
Here is a detailed description of how the network structure interacts with standard reinforcement learning configurations to support upcoming research endeavors. 
The experiments work well as demonstrations of ideas. Don't go further in my view. 
The study shows convincing proof that groups of neural networks trained on similar tasks perform better when applied together on new tasks compared to the conventional method of fine tuning, for transfer learning purposes. 
However there are a downsides ; 
Reusing collections of established strategies for assignments has been suggested before in the realm of hierarchical reinforcement learning, as a beneficial approach. Nevertheless the obstacle of constructing these collections still lacks resolution. This study fails to offer compelling perspectives on the matter. 
The chosen transfer tasks effectively showcase the capabilities of the suggested framework. Fail to tackle issues such as negative transfer or reusing components, in complex scenarios as discussed in previous studies. 
The practical aspects of the findings suggest it would be intriguing to display the outcomes depicted in figures 6 and 7 based on real time duration since the moderate data utilization might not pose a considerable hindrance in attaining flawless gameplay, in specific scenarios where data accessibility determines the ultimate performance outcome. Furthermore it could be worthwhile to investigate whether the outcomes presented could be replicated using computation or smaller representations compared to starting from scratch particularly in cases where one of the original tasks involves developing a policy, for the desired task. 
It's of disappointing that the model needs a lot of data just to figure out that there's already a perfect policy, in the expert library! A simpler method could involve checking each expert for a few rounds and then combining their action choices based on a vote. This might deliver similar results using much less data. 