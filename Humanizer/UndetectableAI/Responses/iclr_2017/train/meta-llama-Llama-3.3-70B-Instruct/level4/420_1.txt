This manuscript focuses on how attention mechanismsre used in neural language modeling and highlights two main contributions.
The writers present an attention mechanism that uses separate vectors for key value and predict functions instead of the usual method that depends on a single vector for these tasks.This new approach could have applications, in fields apart from just language modeling." 
The researchers show that language models can perform well with very brief attention spans. A result that isn't entirely surprising and prompts the development of an ngram RNN tailored to capitalize on this trait. 
The study presents structures for neural language modeling and shares some interesting findings with readers. A detailed practical assessment of the suggested ideas is carried out on language modeling. Cbt tasks to offer, in depth insights. 
The authors have provided responses to the initial questions raised during the review process effectively addressing the concerns that were brought up. 
I recommend adding references to Ba et al., Reed & de Freitas and Gulcehra et al in the work section to enhance the discussion on the existing literature, in this field. 