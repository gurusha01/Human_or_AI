This research introduces a method for educating Generative Adversarial Networks (GANs) which involves substituting concurrent Stochastic Gradient Descent (SGD) and unfolding the internal optimization within the min max game as a computational graph structure.The paper is well organized. Provides a clear rationale, for its approach.The issue discussed is important and essential.Although the method is inventive comparable ideas have been examined in areas of study. 
In the quantitative test outlined in section 3 point 3 point 1 of the study involves determining the most effective z value for creating training instances through the use of L BFGS with |the difference between G(z) and x|. According to the writers assertion that locating such a z value enables the generator to create the designated training instance results demonstrate that GAN models with 0 steps encounter difficulty, in producing training instances compared to unrolled GAN models which achieve greater success. However this test is flawed because just locating a z that produces a specific sample doesn't assure that the pattern has a high likelihood of occurrence In reality an identity function could surpass all GAN models in this aspect and this problem remains even in lower dimensional z spaces due to Cantors demonstration of equivalence, among real spaces A more practical assumption is that any picture can be produced by a generator by identifying a particular z but that alone doesn't confirm that the generator isn't excluding certain patternsOf that approach implies that the generator functions similarly to an identity function and can produce any imaginable image possible.This measure might not actually be assessing diversity or mode collapse. Something different entirely.Moreover the challenge, in locating a z value for a particular training instance doesn't necessarily mean it doesn't exist; it just means it's more challenging to pinpoint.This could suggest that unrolled GAN models have a gradual function compared to 0 step GAN models making z optimization simpler. 
In the quantitative study they look at how far apart the generated samples are from the actual data samples. The researchers suggest that when these distances are similar it means there is a variety in the samples.. This method isn't very convincing because it only considers distances, in pixel space which could still show positive outcomes even if the GAN model produces low quality samples. 
The article could benefit from quantitative findings to confirm the effectiveness of the technique in enhancing diversity and quality standards like Inception scores and SSL performance could be incorporated for this purpose. One plausible evaluation measure involves training a GAN using this method on the tri MNIST dataset comprising 1000 modes and showcasing the GANs ability to generate all modes with equal likelihood. Although not flawless in nature this evaluation metric is deemed practical than the ones outlined in the paper as evidenced by a recent submission, to ICLR. 