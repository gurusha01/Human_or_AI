This research makes an impact, in the area of hierarchical control by comparing it to the work done by Heess et al. achieving impressive results in difficult benchmarks that previous studies struggled with successfully but could benefit from a more thorough analysis of the experiments conducted. 
There are a few points that we should talk about; 
The use of the term 'intrinsic' motivation in this context might be confusing because of its usage, in reinforcement learning (RL). The method of pre training that includes robots based on their movement efficiency or ability to grip objects seems to be strongly connected to the particular tasks they will carry out in the future.That reminds me of the training activities discussed by Heess et al. even though they are not exactly the same. 
Utilizing Mutual Information regularization is considered a method that typically produces favorable outcomes; however; it appears to have limited advantages in intricate maze settings, like mazes 1  2  and 3. The writers recognize this discovery. Delving deeper into interpreting or analyzing this result could be advantageous. 
The paper should clearly explain how Sagent and Srest are related for understanding and reproducibility purposes.Duan et al.s paper describes Sagent well. A detailed explanation of Srest is also essential.It's likely that this detail was missed during the review process. 
Examining the way the agent changes its actions and taking a look, at the strategies used in different scenarios. Including when things go wrong and how quickly switching affects performance. Could offer valuable perspectives and improve the studys quality overall. 