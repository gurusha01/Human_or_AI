The writers suggest a method to incorporate a cache into neural language models that allows for the copying of recently used words efficiently. This approach stands out from neural networks that utilize copying mechanisms since it doesn't need extensive training through long term backpropagation. This enhancement boosts efficiency and scalability for larger cache sizes. When this cache is added to RNN baselines as suggested by the authors there are enhancements, in language modeling performance. 
One significant aspect of this study is the recognition that using hidden states hi as word keys xi. H_t as the query vector produces a search method that works well without the need for backpropagation adjustments.This idea may seem simple and possibly familiar to some groups. It carries important implications for scalability and the results, from experiments are quite persuasive. 
Repurposing acquired knowledge to improve memory networks on a larger scale without the costly backpropagation process is interesting and has the potential to enhance various types of memory systems.  
I think this work could be improved by making it more complex and innovative compared to whats out there in the literature world.Its basically just tweaking existing NLP models with some success shown in real world applications and simplicity is key here.Maybe its best suited for a conference focused on NLP topics.Still I believe that simplifying research into something practical and easy to use should be appreciated and this tool seems like it could benefit many people, within the ICLR community making it worth publishing. 