This article offers an addition, with well presented and organized content that expands on the conventional attention mechanism by viewing the attention variable as a probability distribution related to the input and query conditions to incorporate hidden variables into a graphical model context effectively The application of neural network based potentials further improves this method. 
The paper shows how incorporating variable dependencies or structural relationships into attention mechanisms can allow the use of well known graphical models like Conditional Random Fields (CRFs) and semi Markov CRFs to capture the linguistic dependencies present, within attention mechanisms more explicitly. 
The practical tests clearly show that the model works well in scenarios like sequence to sequence and tree structures.The experiments seem to be planned and carried out with a focus on specifics, like marginal normalization.In my view this work is an addition that could help advance research across various fields and challenges. 