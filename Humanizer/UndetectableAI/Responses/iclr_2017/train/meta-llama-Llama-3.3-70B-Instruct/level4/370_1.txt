This study introduces an exploration of a new method to improve the training of complex deep neural networks that are not convex, in natureâ€”a key practical challenge to address.The practical results detailed in the paper and further explained by the authors in their responses during discussions effectively demonstrate how the approach consistently enhances accuracy across structures and datasets. One of the advantages of the suggested algorithm is its straightforward approach which includes a cyclic process of training a fully connected network and its sparser version. A feature that makes it more appealing, for adoption by a wider research community because of how simple it is to put into practice. 
In order to enhance the manuscript more effectively it is advised for the authors to include extra experimental findings and insights discussed earlier on in the conversation. Pay attention to comparing the levels of accuracy obtained while adjusting for variations, in training epochs. 