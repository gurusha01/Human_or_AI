This research paper introduces a nonparametric neural network method that adjusts the model size as it trains by incorporating random zero units and utilizing sparse regularization to prune unnecessary weights effectively and efficiently over a discrete space through a random search approach assisted by sparse regularization to get rid of unnecessary units to enhance performance and efficiency in training, without compromising accuracy or effectiveness. It tackles an issue and presents interesting findings that raise some points for deeper exploration; 
The complexity of the algorithm we're suggesting needs explanation to understand it better! To be specific; breaking down the weights into parallel and orthogonal parts and then converting them into radial angular coordinates might lead to higher computational expenses. It would be helpful to have a comparison of how complex this approach is compared to the usual parametric neural networks. Moreover; testing its runtime performance, in experiments could give us some information. 
When using networks on convex datasets they tend to produce smaller networks that may lead to lower performance than parametric networks do This prompts the need, for a better grasp of why this happens and what causes it. 