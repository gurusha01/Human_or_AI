This document offers an analytical framework, for forecasting the time needed to train and evaluate different neural networks across various software setups and hardware configurations.  
The presentation is very clear and easy to understand because the authors have considered a variety of factors in their calculations for runtime such as the number of workers involved, usage, platform used and the approach to parallelization. It's worth mentioning that their results are consistent with findings, from studies, which lends credibility to their research methods. 
Moreover the writers have shared their code available and offered a practical live showcase that improves the general usability of their tool.  
In their reply it was noted that upcoming revisions will allow users to upload networks and model partitions which will greatly enhance the usefulness of this platform. 
To enhance the research further it would be useful to investigate how this framework can be used with network structures that include skip connections, like ResNet and DenseNet. This would show its flexibility and relevance across an array of models. 