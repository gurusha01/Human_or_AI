This study adds to the expanding field of research on learning optimizers and algorithms which has been receiving a lot of interest recently The researchers use a guided policy search framework, on a level to teach optimizers They choose to train on random goals and assess how well they can be applied to a small set of straightforward tasks. 
The contribution mentioned here is truly valuable. 
However the debate about whether reinforcement learning (RL) or gradients, at the level should be used lacks clarity and certainty in the later discussion that is mentioned later in the text passage provided by you earlier which I have read and reviewed carefully for better understanding of the context and content of this scientific paper or article where it is suggested that authors should carry out a comparative experiment analyzing both approaches and then present their findings based on this study conducted by them which is strongly recommended as it will help shed more light onto this important issue that holds great significance in this field of research as the scalability of this method could potentially hinge upon the outcome of such a study comparing these different methodologies. 
Overall the idea shows potential. Lacks sufficient experimental evidence to back it up completely. 
This study was actually released on arxiv before our paper on mastering the art of learning through gradient descent came out. Our inspiration stemmed from art and the aim to substitute the lBFGS optimizer with a neural Turing machine due, to their comparable equation structures (refer to the appendix in our arxiv version). In the end we crafted an LSTM optimizer through gradient descent (SGD) training. 
This study takes an approach by using guided policy search to figure out how to update parameters (policy) highlighting the significance of transferring optimizers to tackle fresh challenges. 
This paper definitely adds a perspective, to the expanding field of learning how to learn literature that has been growing rapidly in recent months. 