In this study is a technique for reducing the size of networks by trimming weights which was tested on a speech recognition dataset using a recurrent neural network (called RNN). The outcome showed that this method created streamlined solutions and led to reduced computational requirements during testing, without majorly affecting task performance; in fact it sometimes boosted evaluation results. 
The tests make use of a cutting edge RNN system that seems thought out in its approach. The exploration into how impacts large networks is praiseworthy and leads to significant computational advantages. Yet the decision to rely on a private dataset for all tests is regrettable; assessing performance against a standard test set like HUB5, for conversational speech could have been advantageous. Moreover it would have been beneficial to compare the proposed method with pruning techniques such, as those discussed by Han and colleagues to better assess its effectiveness comparatively. 
Although the idea of training in a stage appears sophisticated at first glance; it might not lead to substantial time savings when numerous experiments are needed to identify the best hyperparameter settings for adjusting the threshold scheme effectively.Demonstrating a denser baseline would have been more persuasive if it had included techniques like compressing the model through methods such as training with targets, from a larger network. 
The paper is nicely written; however the descriptions for the tables and figures could be more elaborate in order to provide clarity to the readership. The exploration of enhancements in speed and memory efficiency for sparse recurrent neural networks is intriguing but somewhat disconnected, from the proposed pruning technique. It would be beneficial for the paper to better justify the intricacies of the method by explaining the importance of increasing the threshold over time. 
Sparse neural networks have been widely studied in the realm of intelligence and machine learning along with recurrent neural networks (RNN). The approach put forward in this research bears resemblance to work by Han et al. particularly in its exploration of applying weight pruning techniques to RNNs â€“ known for being trickier to train compared to feedforward networks. While this study may not introduce concepts or profound scientific breakthroughs per se; it does showcase a successful implementation of weight pruning strategies in large scale RNN systems without notable sacrifices, in performance metrics. 
The proposed approach offers benefits by efficiently decreasing the parameters in RNN models while maintaining high performance levels and being adaptable to a cutting edge system for real world tasks. Nevertheless its resemblance to research and the lack of originality along with the absence of comparisons to other pruning techniques and reliance, on proprietary data are noteworthy limitations. 
The research paper offers an outcome by demonstrating that trimming weight can be successfully implemented in extensive RNN setups with minimal effect on performance through a straightforward approach.This discovery is significant to share since enhancing scalability plays a role, in advancing neural network studies.  
Advantages; 
The suggested approach efficiently decreases the number of parameters in RNN models without a decline, in performance. 
The tests make use of a system for a real world purpose. 
Downsides; 
The suggested approach closely mirrors research and presents minimal innovation. 
No other pruning methods can compare to this one. 
Using information hinders the ability to reproduce outcomes. 