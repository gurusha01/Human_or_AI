This paper addresses an issue with generative adversarial networks â€“ their struggle to assess data that was not used during training sessions effectively. Unlike methods like BiGANs and ALI that rely on an inference network, for evaluation purposes the authors suggest adjusting the GANs objective to allow the primary discriminator to act as an energy evaluator. This innovative approach involves calculating entropy gradients of the generated data a task tackled by the authors through two strategies; a method based on nearest neighbors and a variational lower bound technique. The results from the experiments show that the trained discriminator or energy function accurately estimates the likelihood of the data on datasets and offers a trustworthy measure of quality for unseen data, on more intricate datasets. 
The main drawbacks of the paper are primarily related to the constraints of the methods proposed. Specifically the challenges, in scaling up nearest neighbor approximation and ensuring the accuracy of variational approximation as noted by the authors themselves. Additionally and importantly because entropy estimation is closely tied to density estimation it remains uncertain if any real world application of EGAN models will essentially amount to a kind of approximated density estimation. A challenge that GAN models were intended to steer of from the outset. However the detailed math and elegant writing, in the paper add value to the field. 
Furthermore there are a small problems in the writing like the unfinished sentence found at the beginning of page 5 that begins "Finally let's whose discriminative power". It lacks clarity in this context too. Besides that the title might lead to confusion as it implies an enhancement, to an already existing training approach instead of unveiling a fresh training structure, which is the papers true innovation. 