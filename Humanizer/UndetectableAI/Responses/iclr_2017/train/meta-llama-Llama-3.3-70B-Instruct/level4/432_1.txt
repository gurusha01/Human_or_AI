This study introduces an organized and cohesive story that skillfully combines value driven and policy driven methods such as standardized policy gradient techniques while emphasizing fresh links between the value function and policy decisions. The theoretical advancements are original and insightful with promising implications for the reinforcement learning (RL) field, beyond the suggested algorithm. The authors use these theories to create an approach for combining Q learning and policy gradient techniques in a way that achieves results on par with or exceeding the best algorithms, for Atari games.The practical examination is well detailed with explanations of the optimization methods utilized. 
In the manuscripts context there are minor differences between discounted and non discounted distributions within a policys stationary distribution that may have nuances to consider in function approximation situations; however such nuances are not a major concern, at this point. 
Ultimately the paper should be accepted because of its discoveries that are likely to shape various research in RL. By setting the stage for theoretical and algorithmic advancements this study could have a significant influence, in the field for years to come. 