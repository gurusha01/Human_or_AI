This document introduces a learning model using Long Short Term Memory (LSTM) networks to understand the optimization process of a different learning model like a neural network (NN). The paper is well organized and effectively communicates its findings.The central concept of linking the Robbins Monro update rule, with the LSTM update rule to meet the needs of few shot learning – quick absorption of new information and gradual retention of general knowledge – is quite intriguing. 
The use of methods like sharing parameters and normalization techniques were influenced by earlier research (Andrychowicz et al., 2016) along with new design decisions, like a unique approach to batch normalization that are well reasoned out in this studys findings which make it a promising paper overall.. I do have a couple of reservations and queries; 
Is it important for the learning process to include loss functions, gradients, and parameters, as inputs or would simpler combinations suffice instead? Have they conducted ablation studies to explore this question? 
Would it be helpful to consider investigating architectural elements of the network in a similar way like varying the number of neurons or types of units used in it ? Any insights from the authors, on this matter ?
The section discussing research on meta learning could be more detailed and thorough in its coverage of the topics history and alternative approaches that have been suggested to tackle similar issues without relying on LSTMs specifically. For instance Samy Bengios doctoral dissertation from 1989 and the application of programming to explore fresh learning guidelines for neural networks by S.Bengio,Y.Bengio and J.Cloutier, in 1994 are notable examples worth mentioning. Schmidhuber has definitely made some contributions as well; I suggest updating the section, on related work to incorporate them. 
In my opinion I enjoyed reading the paper and think the topics discussed would interest a range of people, in the ICLR community. 