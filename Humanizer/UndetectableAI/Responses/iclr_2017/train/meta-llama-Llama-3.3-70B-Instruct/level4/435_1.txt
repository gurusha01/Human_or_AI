The suggestion to improve descent in image classification seems simple and effective but was seen as more suitable for a workshop presentation at first glance.The algorithm was only tested on one task. CIFAR. Without theoretical background.This raises questions, about how it would work on other tasks. 
In my experience with Deep Neural Networks (known as DNNs) in Natural Language Processing (referred to as NLP) I found that some of the discoveries in the research paper didn't quite match up with what I've seen. Specifically speaking about architectures that combine types of layers like embedding layers and recurrent neural networks (RNN) convolutional neural networks (CNN) and gating mechanisms. I noticed a clear advantage when using ADAM type techniques over the simpler stochastic gradient descent (SGS) method with momentum. This advantage stems from the fact that ADAM type techniques remove the necessity to manually tune the learning rate, for each type of layer involved.. I discovered that ADAM performs best when paired with Polyak averaging since it tends to vary significantly from batch to batch. 
After making revisions to the papers content and conducting experiments on datasets, than CIFAR as well as reclassifying the workshop track as breakthrough work instead of the original recommendation I made earlier on; I have adjusted my evaluation and upgraded my rating accordingly. 