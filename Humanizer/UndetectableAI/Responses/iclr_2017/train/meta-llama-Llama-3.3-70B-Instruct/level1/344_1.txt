In this study a new method is suggested to handle situations in the strategy game StarCraft using reinforcement learning techniques. The researchers present an algorithm named zero order (ZO) reinforcement learning that merges direct exploration in the policy realm and backpropagation. This algorithm is crafted to tackle the difficulties posed by action choices and delayed rewards, in micromanagement scenarios. 
The research asserts its impact on the realm of reinforcement learning by presenting a standard for reinforcement learning techniques and unveiling an innovative algorithm that surpasses established approaches, like Q learning and REINFORCE when dealing with micromanagement situations. 
"I have chosen to approve this paper with some revisions based upon the following reasons;"
The article addresses an particular issue within the realm of reinforcement learning; micromanagement, in real time strategy games. 
The writers suggest a method that tackles the difficulties posed by extensive action choices and postponed rewards, in micromanagement situations. 
The results of the experiment indicate that the new algorithm performs better than established approaches like Q learning and REINFORCE, in situations requiring management. 
The reasons backing up this choice are; 
The paper offers an brief overview of the issue of micromanagement in real time strategy games along, with the difficulties of implementing reinforcement learning in this area. 
The writers offer an, in depth explanation of the suggested algorithm that covers its framework and practical application specifics. 
The comprehensive findings are well laid out. Effectively showcase how the suggested algorithm performs in micro management situations. 
In order to enhance the document further I recommend incorporating the following suggestions; 
The authors should include information, about how they adjusted the hyperparameters and how the algorithm responds to various hyperparameter configurations. 
The writers have the opportunity to offer perspectives on the acquired policies and the tactics unearthed by the algorithm in various micro management situations. 
The authors might consider talking about how the proposed algorithm could be used in fields as well as the difficulties and restrictions of using it in more intricate situations. 
Can the authors please address some questions I have to help me better grasp the content of the paper?
Could the writers offer information regarding how the Ziegler Olsen algorithm is put into practice; this includes detailing the exact structure of the neural network and the steps involved in optimization? 
How do the authors address the balance, between exploring and exploiting in the Ziggurat Out algorithm. What impact does this balance have in terms of the algorithms effectiveness? 
Could the writers offer information on the strategies and tactics uncovered by the algorithm in various micromanagement situations and how these approaches align, with human gameplay? 