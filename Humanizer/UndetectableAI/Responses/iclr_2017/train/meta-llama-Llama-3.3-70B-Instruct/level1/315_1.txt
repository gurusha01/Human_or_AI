Paper Summary; Key Points
The study examines why there is a difference in performance between batch and small batch techniques in deep learning models by investigating the reasoning, behind it using numerical data analysis and proposing solutions to improve the generalization issue commonly seen in large batch methods used for training neural networks. 
Choice
"I have chosen to approve this document with some adjustments.The document addresses an issue within the realm of deep learning and offers valuable perspectives into the functionality of large batch techniques.The authors back their observations and conclusions, with numerical data and the document is nicely composed and straightforward to comprehend."
Arguments, in favor.
The study focuses on an issue concerning the gap in performance between large batch methods and offers a well explained strategy to address this issue effectively and logically. The researchers utilize experiments and visual aids like parametric graphs to demonstrate the distinctions between using large batches versus small batches in their experiments. Moreover they delve into tactics to alleviate the generalization challenge posed by large batch techniques such, as incorporating data augmentation techniques and implementing prudent and robust training methodologies. 
More Input Required
To enhance the paper further and make it more comprehensive; I propose that the authors include information regarding the experimental configuration and the specific hyperparameters employed in their trials. Moreover discuss the significance of their discoveries on shaping deep learning algorithms and determining batch size. Additionally it would be beneficial to include comparisons with other relevant studies, in this area. 
Questions to Ask the Writers.
Could the authors please provide explanation on the following topics?
How do the writers determine the batch size, for their big batch experiments. Why do they opt to use 10 percent of the training data as the batch size? 
Could the writers offer information, about how they put the conservative training method into practice and why they chose the regularization parameter Î»? 
How do the writers intend to expand their research to encompass deep learning structures and objectives and what drawbacks might arise from their methodology? 