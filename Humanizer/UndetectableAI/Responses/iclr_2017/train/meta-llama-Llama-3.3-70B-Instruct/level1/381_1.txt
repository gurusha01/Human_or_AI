The research paper suggests a method for trimming convolutional neural networks (CNNs) by combining a greedy criterion based pruning technique with fine tuning using backpropagation in an alternating manner.. The authors present a criterion based on Taylor expansion to estimate the cost function modification caused by trimming network parameters.. They test the method on datasets such, as Birds200 Flowers102 and ImageNet and show better results compared to other pruning techniques.. 
Here are the reasons why I have chosen to approve this paper; 
The article addresses an significant issue within the realm of deep learning; the quest for effective inference in CNN networks, by means of pruning techniques. 
The strategy is thoroughly thought out and supported by existing research findings and a detailed explanation of the shortcomings of methods and the benefits of the suggested approach. 
The research paper presents a range of experimental findings that not only compare different pruning methods but also assess the proposed approach, on multiple datasets to showcase its effectiveness. 
Reasons backing up the decision consist of; 
The document thoroughly explores the process of the oracle and the significance of normalizing per layer that underscores the difficulties, in trimming down CNN models and stresses the importance of carefully determining the criteria for pruning. 
The results of the experiment clearly show that the Taylor criterion outperforms pruning criteria, like the oracle loss and oracle abs criteria and proves that this approach can reduce computational costs significantly while still maintaining strong generalization performance. 
The article also includes a comparison with various pruning techniques such, as weight regularization and Optimal Brain Damage to showcase the benefits of the proposed method in terms of both computational speed and precision. 
Here are some suggestions to enhance the paper further; 
To delve deeper into the expenses of the suggested method and outline the time and memory needs, for trimming and refining it further. 
Exploring how the method can be used with kinds of neural networks, like recurrent neural networks and fully connected networks. 
Investigating how the suggested method can be enhanced by integrating strategies, like knowledge distillation and quantization to boost the effectiveness and precision of CNN models. 
Queries that the authors might have include; 
Could you give me information, about how the Taylor criterion works and how to calculate the gradient and activation terms involved in it? 
How do you decide on the settings for the process, like the learning rate and how many fine tuning rounds to go through before each pruning cycle? 
Have you tried using different optimization algorithms, like Adam and RMSProp to refine the network? 