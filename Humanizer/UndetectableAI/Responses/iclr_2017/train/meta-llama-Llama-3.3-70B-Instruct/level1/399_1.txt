Synopsis 
The article presents a method for controlling computation in deep neural networks known as the Sparsely Gated Mixture of Experts (MoE) layer.It involves a group of expert networks and a trainable gating network that picks experts to handle each input efficiently.This technique was successful, in enhancing large scale language modeling and machine translation tasks by improving model capacity and computational efficiency. 
Choice
 I have chosen to approve this document for two reasons; 
The article addresses an issue, in the field of deep learning. The challenge of model capacity being restricted by computational limitations. 
The writers suggest an innovative method to tackle this issue and offer thorough experimental proof to back up their arguments. 
Arguments, in favor 
The research paper presents a defined and detailed rationale for the suggested method by examining the difficulties and constraints of current techniques extensively.The writers also offer an explanation of the MoD layer and its elements such as the gating network and expert networks.The trial outcomes are striking as they show enhancements, in model capability and computational effectiveness when applied to extensive language modeling and machine translation assignments. 
More Feedback Received.
To enhance the paper further and make it more informative and engaging for readers like myself I recommend that the authors delve deeper into explaining the training process and the fine tuning of hyperparameters used in their research study. It would also be fascinating to have an extensive discussion on how the expert networks specialization influences and enhances the overall effectiveness of the model, under consideration. Some queries that pique my interest are as follows ;
How do expert networks focus on tasks or fields of expertise? 
How does the quantity of experts and the sparse nature of the gating network impact the effectiveness of the model? 
How does the Ministry of Education layer stack up against methods of conditional computation, like hierarchical or recursive models? 