
The research paper suggests a way to adjust Long Short Term Memory (LSTM) networks by adding batch normalization techniques commonly used in feed forward neural networks for improved performance and optimization in recurrent neural networks hidden states across tasks like sequence classification and language modeling as well, as in question answering scenarios The study also discusses the significance of correctly setting up batch normalization parameters and shares findings from experiments conducted on various standard tasks. 
The Main Reasons
After reviewing the paper and considering all factors into account I have chosen to approve it.The main reasons, behind my decision are as follows; 
The article addresses an well thought out issue related to recurrent neural networks; addressing the decrease in internal covariate shift, across different time steps. 
The authors thoroughly analyze the advantages of batch normalization in neural networks and present empirical results, on various benchmark tasks to showcase the efficacy of their suggested method. 
Here are some points to consider.
The article offers a defined and organized explanation of the suggested method while delving into the advantages of batch normalization in recurrent neural networks in detail. The authors also conduct an assessment of their method, on various standard tasks to showcase its ability to enhance optimization and generalization performance effectively. Furthermore the article dives into an examination of the significance of correctly initializing batch normalization parameters which is a vital aspect of the proposed method. 
Further. Recommendations
To enhance the paper further I recommend that the authors offer an insight into the hyperparameter exploration conducted for every experiment. Furthermore it would be beneficial to incorporate visuals of the outcomes like graphs showing the progress, over time and diagrams representing the activations to provide a clearer comprehension of the advantages of the suggested method.
Authors I have some queries for you.
To make sure I get the paper in my head I have a few questions, for the authors; 
Could you please elaborate further on how the batch normalization parametersre set up and how they impact the effectiveness of the proposed method? 
How do the writers intend to tackle the problem of escalating gradients when it comes to batch normalization, in deep recurrent neural networks? 
Could you please share details regarding the computational expenses associated with the suggested method and how it stacks up against other techniques for mitigating internal covariate shift, in recurrent neural networks? 