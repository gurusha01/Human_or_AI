In brief
The paper presents a model called the Gated Graph Transformer Neural Network (GGTNN) an extension of the Gated Graph Sequence Neural Network (GGSNN). Unlike GGSNN which works with sequences only GGTNN can handle graph structures in between processes too! This enables the model to create and adjust graphs in ways based on text inputs and utilize these graphs to generate various results effectively. The models performance is assessed using the bAbi tasks. A series of language related tasks. And it achieves impressive accuracy across most of them. Furthermore​ the model is used for two tasks of discovering rules; analyzing a one cellular automaton and a Turing machine with four states and two symbols​; it effectively grasps the principles guiding the progression of graph structures, over time​. 
Choice
After consideration of the papers content and analysis of its findings and methodology I have chosen to approve it due to the strong rationale behind the approach and the notable outcomes that highlight the GGT—NN models success, across different tasks. 
Arguments, in favor 
The document addresses an issue or query by presenting a fresh framework capable of managing data that is structured in graphs and text based information simultaneously.The method is thoughtfully justified as it expands upon research related to GGS Neural Networks and graph neural networks.The outcomes are remarkable as the framework attains precision, across the majority of bAbI assignments and effectively grasps the principles that govern the transformation of the graph structure throughout time in the two tasks involving rule discovery. The document also offers an lucid description of the model and its elements which simplifies comprehension and navigation. 
More Input Required
To enhance the paper further I recommend that the authors delve deeper into analyzing the intricacies of the GGT NN model and its ability to scale up for larger graphs and more intricate tasks. It would also be intriguing to see comparisons with alternative models and structures capable of managing graph based data and textual inputs. Lastly it might be beneficial for the authors to incorporate visual aids and instances showcasing the graphs generated by the model, in order to better showcase its strengths and weaknesses. 
Queries, for the Writers.
Could the authors please provide clarification on the following aspects?
How does the GGT NM model manage graphs that have sizes and configurations? 
Can this model handle challenging tasks, like dealing with multiple graphs or graphs containing various types of nodes and edges? 
How well does the models performance stack up against models and architectures that are capable of processing graph based data and textual information? 
Are there any intentions to share the code and data mentioned in the paper to support reproducibility and future studies? 