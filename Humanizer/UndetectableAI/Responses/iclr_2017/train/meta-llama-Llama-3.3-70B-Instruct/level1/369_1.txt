Lets provide a context, to the summary. 
The study suggests Trained Ternary Quantization (TTQ) as a technique to decrease the precision of network weights to ternary values for efficient model compression, without compromising accuracy and in some cases even enhancing it. 
Choice
Based on two factors. The well founded approach addressing model compression for mobile devices and the comprehensive empirical support from experiments on various datasets and comparisons, to prior research. I have chosen to approve this paper for publication. 
Points, in Favor 
The paper presents a defined and organized explanation of the new approach with a thorough overview of how quantization is done and the steps involved in training it.The authors also delve into an in depth examination of how the balance's maintained between sparsity and accuracy in their method and demonstrate that it strikes a solid equilibrium between compressing the model and maintaining performance levels.The results, from their experiments are remarkable as their method surpasses both past weight networks and sometimes even full scale models. 
More Input Needed.
To enhance the paper further I recommend that the authors offer perspectives on the acquired ternary weights and scaling coefficients through visual representations or examinations of the acquired patterns. It would also be intriguing to witness trials on alternate datasets and models to strengthen the overall applicability and efficiency of the suggested approach. Additionally I am curious about an aspects for the authors ; ( 1 ) What is the correlation, between the learned scaling coefficients and the layer specific thresholds and can they guide in determining suitable threshold values ? Is it possible to use this approach with neural network models like recurrent or graph neural networks? Are there any intentions to share the implementation of this approach, as open source code to support research and practical uses? 