In this paper the main points and contributions are outlined.
The study suggests a method for acquiring dual language word representations without real time processing by utilizing a linear mapping between two sets of vectors that should be perpendicular, to each other. The researchers present a technique called "inverted softmax" to recognize translation matches and exhibit its efficiency in enhancing the accuracy of word translations. They also illustrate that perpendicular mappings are better equipped to handle disruptions allowing for the utilization of pseudo word lists derived from matching character sequences in both languages. The research paper goes on to expand the approach, for extracting translated sentences from a collection of sentences and attains a precision rate of 68%. 
Significant Factors
After reviewing the paper I have made the decision to accept it. The primary reasons, behind this decision are; 
The article addresses an well founded issue, in the realm of bilingual word embeddings by offering a straightforward and succinct overview of the subject and its importance. 
The method is backed by both knowledge and practical evidence that show how effective it is in enhancing translation accuracy and resilience, against disturbances. 
Reasons In Favor
The research paper thoroughly examines the issue by discussing the constraints of methods and the advantages of employing orthogonal transformations in a lucid manner. Moreover the authors elaborate extensively regarding their methodology that involves utilizing value decomposition (SVD) for acquiring orthogonal transformations and introducing the innovative "inverted softmax" technique. The experimental findings are thorough and well articulated showcasing the efficiency of the suggested approach across scenarios, such, as utilizing expert training dictionaries and pseudo dictionaries. 
More. Queries
To enhance the paper further suggestion would be for the authors to delve into details about how their approach could be applied in areas like machine translation and language independent text classification with clarity, on computational complexity and scalability to handle larger datasets. 
Some queries that I hope the writers will touch upon are; 
"How do the authors intend to expand their method to accommodate a range of language pairs, with restricted identical character string counts?"
Could the writers offer information about how the new method relates to other techniques for learning bilingual word vectors, like CCA and online learning methods? 
How will the authors assess the effectiveness of the acquired bilingual word embeddings, from just using precision and recall measures? 