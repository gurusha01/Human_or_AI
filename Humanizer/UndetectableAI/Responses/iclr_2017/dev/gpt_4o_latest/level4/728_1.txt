The paper starts by emphasizing the importance of developing techniques that combine state and temporal representation learning in reinforcement learning (RL) along, with providing interpretability that allows human operators to step in when necessaryâ€”an objective that holds practical relevance.Its heartening to witness research efforts being channeled towards this direction.I commend the authors for their work. Urge them to persist in this research avenue. I'm not entirely convinced that the current iteration of the document offers the solution, to this issue; certain concerns relate to how its presented and others might call for a deeper reconsideration of the strategy. 
The authors focus on achieving "interpretability" by implementing techniques for simplification in their work process. For example the skills they develop always start with one skill initiation state. End in a single state. However this decision seems limiting and its not clear why they imposed this constraint other than simplicity. Likewise using clustering as the foundation, for creating higher level states depends on a type of clustering method without explaining why this specific approach was preferred over other options. MoreoverI found the approach to maintaining consistency over time to be overly restrictive I couldn't find the detailed explanations of their design decisions mentioned by the authors in the paper provided It would be beneficial for the authors to offer a rationale, for the significance of such specific decisions or better yet investigate if there is room to loosen these restrictions without compromising clarity 
From a presentation standpoint it would be helpful for the paper to include definitions of AMDP and SAMDP along with detailed explanations of the methods employed to create these models (such as the Bellman equations for the models and the update rules, for the learning algorithms). Although the paper offers descriptions of these concepts it lacks explicit mathematical details. Furthermore it would be beneficial to address the resources required (both time and space wise) when building an SAMDP. 
The tests were carried out effectively. Its worth noting that the authors covered both gridworld experiments for clear understanding and Atari games as well. Although some may disagree with this view; gridworld experiments still hold merit in this field of study. The outcomes look promising. The suggested method comprises many elements that hinge upon particular design decisions; thereby casting doubt upon its overall importance and widespread relevance, at present. Having a comprehensive supplemental document could possibly tackle some of these issues efficiently. 
A quick note. There are some errors in the notation as well as a wrong sign in the equation in the two lines, after Equation 2. 