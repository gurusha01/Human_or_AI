This study delves into the reasons behind the success of deep networks and investigates how adjusting the pooling structure allows deep networks of polynomial size to effectively compute functions with a significantly high separation rank (, within certain partitions).
In their research studies showed the benefits of deep networks compared to shallow ones while utilizing ReLU as the activation function, alongside max and mean pooling techniques. Nonetheless in this report the convolution layers do not incorporate activation functions and pooling is executed based on node values multiplication method. Nevertheless the experimental outcomes take into account both situations. 
Explaining this phenomenon, in theory can be quite tricky. It's not a big problem overall.The new information adds value to what we know. 
The paper discusses convolution circuits and shows how this model accounts for inherent biases along, with ways to customize pooling mechanisms to shape these biases effectively. 
This insightful piece delves into how neural networks can detect relationships, among input factors despite having a size and dealing with extensive correlations. 
The authors have put in an effort to define their notations and concepts clearly; however; providing more details, on their definitions expressions and conclusions could make it easier to understand and enhance clarity. 