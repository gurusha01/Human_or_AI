The paper delves into the issue of using real life interactions to predict results—a subject that resonates with the increasing interest in physics, within the machine learning community. The authors push boundaries by creating scenarios that do not rely on pre existing knowledge of physical laws and properties; instead they employ deep reinforcement learning to address the issue. In my view of this paper overall is that it introduces a concept and makes a commendable effort; howeverl its actual contribution remains somewhat vague. 
The experiments are quite interesting and interactive in nature; they aim to identify the weight of blocks and figure out which ones are stuck together by merely interacting with them through actions like pushing and pulling without any prior information or hints about them. The research showcased results, in these activities and presented specific situations to back up its conclusions. 
Despite the experiments and findings presented in the study the significance of the research remains uncertain to me. My main focus is to understand whether these results bring about any perspectives. Even though the experiments concentrate on physical tasks they do not seem markedly different (and might even be considered simpler ) when compared to established DRL challenges like mastering games such, as Atari. This prompts me to question ; do these tasks truly stand out from common DRL benchmarks ? I would have been more interested if the authors had delved deeper into the topic by providing insights or conducting additional experiments like examining the learned representations or investigating links, between the findings and human behavior or natural laws. The paper currently focuses on detailing the setup and the performance metrics of the agent. For example it explains what was carried out and how well the agent did. It does not discuss the underlying representations or wider implications in depth. The writers could improve their work by delving into the learned representations or exploring how the findings connect with basic physical principles or human thought processes. 
Hmm... I'm of torn on how to rate this one overall! On one hand its got an idea thats pretty innovative but maybe needs a deeper dive in the analysis department? Still though I think it's worth accepting just because the concept is solid!
Here are a few specific questions and comments that are not critical, to my evaluation; 
On page 2 of the text provided by the authors it is mentioned that without any knowledge of the physical properties of objects or the laws of physics an agent must engage with these objects to acquire knowledge that enables them to respond to questions about said properties. Is interacting with objects for learning about their properties or is observation alone adequate, for this purpose? 
Figure 1 on the side is missing the label, on the Y axis. 
Page 03 of the document caught my attention with its connection to bandit problems; however the reliance solely on DRL in the formal approach raises the question of whether there is room, for further exploration of this link. 
On page 5 of the document the phrase "which makes it challenging to differentiate between the two blocks" lacks clarity in its explanation.Why would a small difference in mass complicate the task unless the difference is almost negligible)? Should a machine not be able to recognize slight variations, in velocity perfectly fine without any issues arising from this circumstance Could this constraint possibly be attributed to how the network is set up? 
On page 5 of the research paper it is mentioned by the authors that they found results when using pixels and features in their experiments mentioned in this section for feature observations because training these agents is notably quicker than using pixels only. Is it feasible to establish a connection between performances at a level rather than basing it solely on overall performance metrics? Despite that thought provoking question raised above regarding the correlation between performances at an instance level versus performance metrics; the conclusion drawn does seem somewhat ambitious, in its scope. 
In the paper overall​l​y speaking​l​y​​​​�+, numerous observations (such, as the complexity of tasks) seem to be linked to a set of training examples​​�+. For instance​s​nce​​+d​​�+ how does the agent determine when a task is particularly challenging​​thouough��+gh�+ isn't this heavily influenced by the knowledge gained from training examples (for example​* P(m3 | m1​, m2) where m sub i represents the masses of objects 1​* 2​ and 3)? Put ™smpl™a+™rly++ does considering difficulty matter significantly without conducting broader tests across various sets of examples∓piecss∞es??
Do they compare their findings to any existing methods for a thorough analysis, in the paper? 