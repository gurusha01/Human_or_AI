The research delves into the idea of integrating connections over time in RNN models.The central concept is not revolutionary; however the authors propose methods, for blending this data into the existing hidden state by employing various pooling functions.These models are tested against two used text benchmarks. 
Some things I've noticed; 
The research only involves NLP tasks. Is centered around making predictions exclusively. It would have been more helpful to observe how these models could be used in areas like predicting a conditional distribution p(y|x) instead of just p(x). Moreover conducting tests using data, like audio or video could have offered a more comprehensive understanding of their capabilities. 
Other reviewers have pointed out that the comparisons with models may not be completely fair and that it is difficult due the fast advancement of state of the art in natural language processing (NLP). Contextualizing the experiments, within the landscape is a challenge indeed. 
The authors argue that the method they suggest helps with forecasting long term outcomes but fail to provide an analysis supporting this assertion as I pointed out in a previous inquiry. 
In my opinion and basedon what I've seen in the industry the suggestion that LSTMs are slow to train and challenging to scale doesn't match up with what I've encountered far.Additionally the fact that known companies such, as Google,Baidu and Microsoft use LSTM systems extensively goes against this claim. 
Although I do appreciate the core concept presented in the paper and find it intriguing I must say that based on the discussed concerns it seems premature to consider it suitable for publication, at this stage. 