This study presents a method for protecting against adversarial examples through the training of an additional classifier to identify them effectively. The results from the experiments show that spotting examples is quite achievable. Moreover the suggested detector shows adaptability to similar or less potent adversarial instances. The main concept of the study is simple yet challenging. Though the paper does not offer a plan for using this concept to build defensive systems completely it does pave the way for exciting new avenues, for future research. In my opinion it's a piece and I suggest approving it.
My main issue with this paper is that it seems incomplete, in addressing the challenge of countering adversaries effectively. The paper lacks a method to tackle this complex problem and only briefly touches on the topic without delving into it deeply. Exploring the difficulties of defending against adversaries is an important and interesting question raised by the papers findings. By investigating this issue we could gain valuable insights into adversarial examples and improve our understanding in this area. 
Nonetheless the paper makes a contribution, in terms of originality. 
Just a quick note;   
The paper might be easier to understand with some clarity improvements suggested for its benefit missing important details like a more detailed explanation, about dynamic adversaries and the training methods to deal with them. 