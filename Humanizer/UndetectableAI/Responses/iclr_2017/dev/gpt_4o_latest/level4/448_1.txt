My knowledge of mean field techniques may fall short in evaluating the accuracy of Equation 2 however I am inclined to believe it as it has been presented. 
I had a suggestion regarding clarity in your explanation of the "evolution" of \( x{i;a}\). It might help some readers to understand better if you discuss how it progresses through the network explicitly. However I found it a bit unclear because \( x{* ; a}\) which signifies the input vector while the newly introduced variables, like \( z \) and \( y \) are the ones that really portray its evolution as mentioned. Is my interpretation correct? 
When looking at this analysis closely a network could still be taught even if information doesn't flow through it at first. This happens when the training changes the weights in a manner that allows information to move through the network and doesn't then adjust the weights to stop this flow on. Could we clarify this by explaining what we mean by a "training algorithm"?
Thoughts, on the arguments; 
Previous studies have explored methods to start networks that promote the flow of information (, for example Glorot & Bengio).