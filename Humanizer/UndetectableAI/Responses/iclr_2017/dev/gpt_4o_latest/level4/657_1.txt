The article presents methods for condensing a broad but not deep text classification model that utilizes n gram characteristics such as (optimized for). These methods include compress the embedding weights through (optimized for) removing vocabulary elements through pruning steps and using hashing to decrease vocabulary storage space (although this is a smaller part of the article). The main emphasis is on models with extensive vocabularies; the authors show a decrease in model size with a slight drop, in accuracy. 
The challenge of compressing networks is important and interesting to tackle with enthusiasm. The explanation provided in the methods section is well written. Includes clear descriptions at a high level along with references that are pertinent. However the impact of the paper within the realm of machine learning seems constrained. The results from experiments lack persuasion since they mainly focus on benchmarks that are not commonly used. Moreover it's unclear how relevant the paper is to the RNN based models, for text classification. 
The application of (optimized) product quantization to approximate products is not particularly groundbreaking since comparable strategies have been investigated in previous studies. A significant portion of the reduction in model size comes from trimming down the vocabulary. The pruning technique suggested relies on the idea that embeddings, with L2 norms hold more importance and incorporates an additional coverage heuristic. Looking at this from a machine learning angle suggests that a better starting point to tackle this issue would be to include a group of (modified) values, for every embedding vector and training these values together with the weights simultaneously. Adding some sparsity could be achieved by incorporating an L1 regularizer to the values. On a practical note though. There's a crucial starting point missing here. What if we just lessen the range of words available? For instance. By utilizing subword elements (as mentioned in...)?