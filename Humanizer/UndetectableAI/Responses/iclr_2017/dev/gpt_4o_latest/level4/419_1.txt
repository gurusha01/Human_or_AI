This study introduces a model that merges ideas from topic models and recurrent neural network (RNN) language models. The authors test their suggested approach on two benchmarks. One for document level classification and the other, for language modeling. Showing encouraging outcomes. Furthermore the study examines the topics understood by the model and its text generation capabilities. In general the paper is nicely written,. By sharing their code others should be able to reproduce the method. On the hand I do have two important concerns that I would like the authors to look into; 
Traditional LDA topic models usually assume exchangeability by following a bag of word assumption. The paper should clearly state whether the same assumption applies in the process of TopicRNN as well. Initially it seems likely because \( yt \)s sampling involves the documents topic vector and \( ht \). However in reality \( ht \)s derivation comes from a model that considers \( y_{ t.   } \).The connection between how the generative model's defined and how its put into action isn't clear in the text "Generating Sequential Text." It's noticeable that the topic model relies on \( y_ { 0 } \) \( y_ { 2 } \)... And \( y_ { t. 2 } \) Which seems to conflict with the description of the model itself, in the paper. This inconsistency should be discussed further in the paper to present an consistent view. 
The topic model confines the interactions of the topic vector \( \theta \) to be linear in nature which could be essential for keeping the model manageable but might also seem restrictive in its approach.. Common sense dictates that one would anticipate a nuanced interplay between the topic representation and the language model to allow for nonlinear modifications to word probabilities within a document.. The authors ought to delve into the reasoning, behind this modeling decision and potentially propose ways for studies to loosen this constraint. Alternatively they ought to offer reasons why this assumption might not be as restrictive as it first seems. 
The colors in Figure 2 are hard to tell and could use some enhancement, for easier reading. 