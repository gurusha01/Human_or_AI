
Summary of the paper
This research paper presents a model driven method for deep reinforcement learning called Predictive Reinforcement Learning (PRL). It tackles the complexities of task learning within reinforcement learning settings by suggesting a structure that separates the environment comprehension, from the strategy implementation. Allowing the model to grasp various tasks at once without compromising performance. The paper presents a recurrent neural network structure called Residual Recurrent Neural Networks (RRNN) which aims to separate memory from computation to address the needs of settings with intricate dynamics but constrained memory capacities effectively. Experimental findings showcase that PRL attains performance comparable to that of humans in three ATARI games (Breakout, Pong and Demon Attack) while also displaying performance through transfer learning mechanisms. Furthermore the authors underscore the possibility of generalization and strategy autonomy, in their methodology. 
The decision has been approved.
Important Factors; 
The research paper introduces a model driven strategy for reinforcement learning that differs significantly from the prevalent model free techniques commonly used in the field.The incorporation of RRNNs and their proven advantages, in task learning settings adds substantial value to the papers findings. 
The practical testing confirms that the suggested method not maintains performance, in situations involving multiple tasks but also reaps the benefits of transfer learning effectively and conclusively documented. 
Presenting Arguments
The paper starts by highlighting the importance of the issue at hand. Tackling task reinforcement learning challenges in a comprehensive manner within the field of study by discussing the drawbacks of Q learning approaches and effectively placing their research within the context of existing literature. 
The methodology is well explained. The experiments are set up to thoroughly assess the hypotheses put forward.The inclusion of ATARI games as standards and the contrast with single task models offer compelling support, for the success of the suggested strategy. 

Ways to Enhance Your Work
Clear Understanding of Strategic Autonomy; Although the paper suggests that the model can execute strategies beyond those it was trained on the experiments predominantly utilize strategies. It would enhance the study to showcase experiments or analysis illustrating the models capacity to adapt to new and unfamiliar strategies effectively. 
Addressing long term dependencies is a challenge according to the authors, who suggest discussing solutions like incorporating hybrid approaches, with Q learning to enhance the papers strength. 
Training Consistency Concerns; The fluctuations in performance observed in Demon Attack indicate a lack of stability during training sessions. A thorough examination of this matter and exploration of remedies such, as curriculum based learning or adaptive sampling could offer valuable insights. 
The authors mention that their method doesn't reach the results compared to others but suggest that a thorough comparison, with top methods would offer more insight into the papers impact. 
Queries for the writers.
How does the model cope with dynamics in different settings and can the RRNN design remain efficient under such circumstances? 
Is it possible to adapt the suggested method for situations that demand planning over extended periods of time like, in strategy games or tasks involving robotics? If yes what modifications would need to be made to the framework? 
How much does the model get affected by the selection of hyperparameters, for the Perception and Prediction networks? 
In terms and perspective wise this article adds value to the realm of reinforcement learning by presenting a fresh model based technique and showcasing its efficiency in diverse task scenarios.The paper has its ups and downs. Its strong points overshadow the weaknesses so my suggestion is to give it a go. 