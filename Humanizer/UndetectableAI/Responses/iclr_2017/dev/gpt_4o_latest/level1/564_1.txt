
Here are the key points of our contributions.
The paper presents Higher Order Recurrent Neural Networks (HORNN) a variation of recurrent neural networks (RNN) created to improve the modeling of long term relationships, in sequential information flow. By integrating memory units that monitor past states and loop them back through weighted connections HORNN aims to improve short term memory function and address the issue of diminishing gradients found in conventional RNN models. The authors compare filters in signal processing to explain how three pooling methods. Max based pooling (MAX) FOFE based pooling (FOFE) and gated pooling. Are used to adjust feedback signals from various sources in their studys models. They test these models using two language datasets; Penn Treebank (PTB) and English text8; showing that HORNN models outperform both RNN and LSTM models in terms of performance. Their experiments show that HORNN models are not efficient but also scalable, for sequence modeling tasks. 
Decision has been approved.
The research paper introduces a thought out and innovative method to enhance RNN performance, in capturing long term dependencies effectively through thorough experimentation and achieving top tier outcomes. Key factors contributing to its acceptance include; 
Introducing an approach to tackle the challenges faced by RNN models, like the vanishing gradient issue while still being computationally efficient is one of the key strengths of the HORNN architecture. 
There is real world proof to support the effectiveness of HORNNs as shown by the successful outcomes on PTBs and text datasets which outperform established models such, as LSTMs significantly. 
Arguments, in favor
The paper extensively discusses research and highlights the shortcomings of current models like RNN and LSTM while showcasing HORNN as a valuable advancement, in the field. 
The experiments are carefully planned with ablation studies to understand how model order and pooling mechanisms affect the outcome. The results are compared against starting points and the authors offer clear implementation details, for reproducibility. 
The study highlights that HORNN models can be applied to sequential tasks and showcases the models computational efficiency as a valuable addition, to the field. 
Ways to enhance your work
The paper is quite thorough in its content; however the mathematical notations and explanations of pooling methods such, as FOFE based and gated pooling could be made easier to understand for readability. Adding diagrams or pseudocode would assist in clarifying the implementation specifics. 
The authors argue that while HORNN models are said to be computationally efficient in the papers analysis of trade offs; a more elaborate examination of training duration and memory consumption among various models such as RNN and LSTMs, alongside HORNN would enhance the papers credibility. 
Ablation Studies on Pool Mechanisms. The findings indicate that the use of FOFE based and gated pooling yields results compared to max based pooling; however further analysis could be provided in the paper explaining the reasons, behind this superiority especially regarding gradient flow and convergence patterns. 
The research primarily looks at language modeling assignments; however examining HORNN performance on sequential tasks like speech recognition or sequence, to sequence modeling could offer more robust proof of their adaptability and versatility. 
Queries, for the Writers
How much do HORNN models get affected by the selection of hyperparameters like the models order and the forgetting factor, in FOFE based pooling? 
Have you tested how well HORNN models perform on datasets or tasks with longer sequences yet ? Are there any restrictions, in memory usage or computational resources that might affect their efficiency ?
Is it possible to use the suggested pooling methods with types of architectures, like LSTMs or GRUs to improve their effectiveness even more? 
In summary​ the paper offers an addition to sequential modeling and is highly suitable for, approval​. Implementing the recommended enhancements could improve the clarity and effectiveness of the research even more​. 