This document presents a method for transforming images without supervision that allows mapping samples from one domain to another without needing paired training datasets for both domains. The main breakthrough of this research lies in its use of Generative Adversarial Networks (GANs) which have been modified for learning through the division of the generation process into two key components; an encoder that identifies a common feature space, between the two domains and a decoder that produces samples in the target domain. In order to avoid the model reaching solutions easily understood by the authors suggest using two additional loss functions to add complexity and depth to the training process. The first loss measures how different the features are between a sample and its transformed version while the second loss evaluates the disparity in pixels between a target sample and its recreated form. The effectiveness of this approach is proven through a series of experiments that involve transferring SVHN digit images, into MNIST style and converting face images into emoji styles. 
The papers strong points lie in its ability to make contributions to the wider area of unsupervised domain transfer through its innovative learning approach.The detailed ablation studies featured in the manuscript offer perspectives on how the system components operate.The visual quality of the transferred images is quite striking. The numerical outcomes suggest a notable preservation of image identities, between different domains. 
There are aspects that could be delved into more deeply to improve the paper. For example broadening the findings to areas like combining text and images might provide a fuller understanding of how adaptable the method is. Furthermore in addition, to examining the maintenance of identities it would be interesting to explore how well facial features are maintained when images are transferred to the target area giving a more detailed insight into the change process. 