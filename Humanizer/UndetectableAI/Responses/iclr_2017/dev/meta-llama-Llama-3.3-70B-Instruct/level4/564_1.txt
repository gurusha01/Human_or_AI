The researchers in this study explore the idea of adding connections gradually in Recurrent Neural Networks (RNNs) even though the core concept is not particularly new or groundbreaking. However they examine approaches, for incorporating this information into the existing hidden state through diverse pooling functions. These models effectiveness is tested on two used text performance benchmarks. 
There are issues that need to be addressed.
The study focuses on NLP tasks involving prediction tasks and suggests broadening evaluations to include other areas like modeling conditional distributions beyond just x values and integrating sensory input such, as audio or video for a more thorough analysis. 
According to reviewers feedbacks mentioned earlier on this matter; the evaluations against current models might not be completely justifiable since the field of natural language processing (NLP) being in a state of continuous evolution and progressiveness; thereby posing difficulty, in contextualizing the experiments within a larger framework of understanding. 
The paper suggests that the new method improves long term prediction abilities but doesn't provide an analysis to back up this claim as I've mentioned before. 
The writers claim that training LSTM models can be slow and challenging to expand a viewpoint that goes against my own encounters and is also contradicted by the extensive use of LSTM systems in real world applications by big companies such, as Google Baidu and Microsoft. 
Although the central concept of the paper is interesting to me personally I think it needs work before it can be ready, for publication. 