This document delves into why deep networks work in real world scenarios and how changing the pooling structure affects the ranking of large deep networks in polynomial size, under different partition schemes. 
The previous studies by the authors showed that deep networks outperform ones when using ReLu activation functions and max or mean pooling techniques. However In this study they investigate a scenario where no activation function is used after convolution and introduce a pooling method that involves multiplying node values. They also discuss the outcomes, for both scenarios. 
Considering the nature of this issue and the limited scope of generalizability posed as a minor constraint; the current input effectively broadens the already existing pool of knowledge.  
The study explores convolution circuits and demonstrates how this framework can grasp inherent tendencies while also showcasing how pooling techniques can fine tune these inclinations.  
One important discovery from this research is the realization of how deep networks can grasp connections, between input factors even with a polynomial network size but an exponential correlation present. 
The authors have clearly defined their notation and terminology; however additional explanation, on these aspects would make their definitions and conclusions more accessible. 