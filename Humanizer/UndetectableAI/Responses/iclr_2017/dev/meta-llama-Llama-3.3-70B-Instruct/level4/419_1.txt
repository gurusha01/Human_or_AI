This research paper introduces a strategy that combines ideas from generative topic models and recurrent neural network language models to achieve a promising outcome in evaluations for document level classification and language modeling benchmarks were showcased by the authors effectively in their work. Moreover the authors delve into analyzing the topics learned by the model and its ability to generate text in detail which adds depth to their findings. The paper is organized in a manner, with transparent writing style throughout; furthermore the authors have pledged to share their code for others to replicate their methodology. I do have two questions that I would like the authors to answer.
The storytelling process of TopicRNN seems to assume interchangeability to LDA topic models when selecting yt based on the document topic vector and ht alone. However since ht is produced by a model that looks at yt−₁ it is uncertain how this assumption corresponds with the practical application. The conversation in the section, on Generating text implies that the topic model depends on y₁− t−₁ to produce words, which contradicts the defined generative model. The writers need to make sure they address and thoroughly talk about this assumption, in the article to guarantee a presentation. 
The topic model constrains how the topic vector theta interacts through transformations; this restriction might be crucial for practical reasons but appears somewhat restrictive, in nature. One might naturally anticipate that the representation of topics would engage with the language model in an intricate and nonlinear way to create subtle modifications to the probabilities of words within a document. The authors need to explain why they chose this design and maybe talk about research directions to adjust or change this assumption or clarify why it might not be as limiting as it seems at first glance. 
The color palette, in Figure 2 is quite difficult to differentiate which could make it harder to understand the findings. 