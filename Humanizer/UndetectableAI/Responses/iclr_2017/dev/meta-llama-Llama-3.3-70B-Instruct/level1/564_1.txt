This study introduces a neural network design called Higher Order Recurrent Neural Networks (HORNNs) aiming to improve the representation of long term connections in sequences of information. The researchers enhance the Recurrent Neural Network (RNN) layout by incorporating additional memory units to remember previous states that are then relayed to the hidden layers as feedback, via diverse weighted routes. Moreover it delves into pooling mechanisms to adjust the inputs from various feedback channels. 
After consideration of the paper, at hand I have chosen to approve it based on two main factors; Firstly the approach is well founded and contextualized within existing literature. Secondly the paper substantiates its assertions through experimentation and results. 
The method is well thought out as it tackles the issue of the vanishing gradient problem in RNN models that restrict their capability to understand long term relationships effectively. The authors offer an explanation of the drawbacks of traditional RNN and LSTM models and introduce an innovative approach that expands upon the conventional RNN framework. Moreover the paper is well situated within existing research as it cites studies in the field and conducts a comprehensive evaluation, against current models. 
The research paper backs up its arguments with experiments and findings based on two well known language modeling datasets; Penn Treebank (PTB) and English text8.The findings indicate that the suggested HORNN models outshine conventional RNNsand LSTMs and attain top notch performance on both datasets.The authors also conduct an, in depth examination of the impacts of pooling functions and model setups showing the efficiency of their methodology. 
To enhance the paper more I recommend that the authors delve deeper into the computational intricacies of HORNN models and how well they handle larger datasets. Moreover it would be valuable to include comparisons with cutting edge models, like attention based and transformer based models. 
"I have some questions for the writers regarding their plans for expanding HORNNs application to different sequential tasks like speech recognition and sequence to sequence modeling. Additionally could they share insights, into the implementation of HORNN in terms of specific designs and hyperparameters utilized during the experiments? Lastly how do they intend to tackle the challenge of overfitting in HORNN models when dealing with sizes and intricate pooling functions?"