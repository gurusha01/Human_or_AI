In summary 
The research paper introduces a Hierarchical Memory Network (HMn) which utilizes Maximum Inner Product Search (MIPS). This approach enables retrieval of information from a vast external memory storage system. The HMn aims to address the drawbacks associated with attention mechanisms that are resource intensive and can lead to scattered gradients and, with hard attention mechanisms that pose training difficulties. The authors showcase how well HMNs perform on the SimpleQuestions dataset by proving that precise K MIPS attention surpasses attention and that using approximate K MIPS methods can enhance speed while sacrificing a bit of accuracy. 
Choice
"I've made the decision to approve this paper for two reasons. First off is how it addresses a crucial issue within the realm of neural networks with memory and secondly is the strong motivation, behind the approach backed by actual results."
Arguments, in favor
The article accurately points out the drawbacks of attention mechanisms and puts forward a fresh approach to tackle these issues effectively. The writers offer an examination of the HM architecture and its parts such as the memory layout and the reader module. The practical outcomes showcase the efficiency of HM on a question answering project on a significant scale; moreover they present an, in depth contrast of various approximate K MIPS methods. 
Further Input Required; 
To enhance the paper further suggestions could be made to the authors by adding information about the computational intricacies of the HMNN design and its various elements I also recommend delving deeper into examining the balance between precision and acceleration that can be attained through using approximate K MIPS methods Furthermore it might be worth exploring potential uses of HMNN in different areas, like generating image descriptions or conversational interfaces
Queries, for the Writers 
Could you please provide insight into the paper by addressing the following questions?
Could you give me information, about the memory layout and how its structured in a hierarchical manner? 
How do you decide on the hyperparameters, for the K MIPS methods and how do these hyperparameters impact the performance of HMNs? 
Have you ever thought about incorporating types of attention mechanisms like hard attention or reinforcement learning based attention, alongside HMNs? 