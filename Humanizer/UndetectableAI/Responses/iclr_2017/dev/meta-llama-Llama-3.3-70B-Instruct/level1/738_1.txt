
The paper presents a method to encourage sparsity in the gradients of Long Short Term Memory (LSTM) based Recurrent Neural Networks (RNNs). This is achieved by utilizing the distribution of gate activations, in the training process. The authors introduce a impactful approach called "sparsified" Stochastic Gradient Descent (SGD). This technique involves rounding off gradient values to zero which leads to enhanced sparsity without compromising performance. The results from the experiment show that the new method is able to reach a sparsity level of than 80 percent in linear gate gradients which results in reducing more, than half of the unnecessary multiply and add (MAC) operations during the entire LSTM training phase. 
Choice
After consideration and evaluation of the paper at hand I have decided to approve it for the following reasons; The approach is well grounded in motivation and the results demonstrate scientific rigor and accuracy. The paper offers an analysis of the issue at hand, with a straightforward yet efficient proposed solution. 
"Arguments, in favor" 
The paper addresses an issue within the realm of deep learning. Enhancing the energy efficiency and decreasing the memory usage of LSTM based RNN models in a specific manner that fits well with prior research on sparsity focused optimization methods for Convolutional Neural Networks (CNN). The authors conduct an examination of the uneven distribution of gate activations and its impact, on gradients which presents a crucial understanding supporting the solution they propose. The results of the experiment are quite persuasive as they show how well the suggested method works in scenarios and, with different sets of data. 
More Input Needed 
To enhance the paper further I recommend that the authors delve deeper into analyzing the trade offs between sparsity and performance and investigate how the proposed technique can be applied to other RNNs and deep learning models.It would also be intriguing to have a conversation about the potential hardware impacts of the proposed technique,such, as designing specialized hardware accelerators to leverage the induced sparsity. 
Questions to Ask the Writers 
To ensure I comprehend the paper fully and accurately grasp its content I kindly request the authors to address the inquiries; 
Could you explain further about how selecting a specific threshold value influences the balance, between having data points and achieving better results in practice? 
How would you suggest incorporating the suggested method into deep learning frameworks and hardware accelerators? 
Have you thought about investigating whether the suggested method could be used with kinds of RNN models, like Gated Recurrent Units (GRUs) or Bidirectional RNN models? 