
Overview of Contributions
This study presents a method of batch active learning for deep neural networks like Convolutional Neural Networks (CNNs) utilizing a variational inference technique as the foundation.The writers suggest a learning criteria that is both scalable and computationally efficient, by blending Maximum Likelihood Estimation(MLE) and Bayesian inference principles.Approximations of network weights posterior and prior distributions using Fisher information and the use of Kronecker factored approximations help to avoid the computational load that comes with backpropagation during active learning. The study showcases how well the suggested approach performs on the MNIST and USPS datasets by reaching optimal results with just 30% of the labeled training data utilized. It is notable for being the pioneering effort in adapting batch learning to deep networks through variational free energy techniques and holds potential, for enhancing both active and curriculum learning methods. 
Outcome choice made is to proceed.
The research paper shows motivation and a rigorous scientific approach while providing valuable insights, at the crossroads of active learning and deep learning methodology that contributes significantly to the field. 
Novelty and Significance; The method suggested is groundbreaking as it pioneers the expansion of batch learning to deep networks through variational inference—a significant contribution that fills a crucial void, in existing literature. 
The tests conducted on the MNIST and USPS datasets clearly prove that the method is efficient and reliable when compared to approaches, in terms of effectiveness and adaptability. 
Reasons to back up your claims.
The paper effectively outlines the difficulties of integrating learning with deep learning by highlighting the computational inefficiencies of conventional methods in a well researched manner based on prior studies, on variational inference and Fisher information while also placing their innovation within the context of existing literature. 
Scientific Precision; The development of the learning principle is meticulously done and based on statistical theory principles with suitable simplifications to facilitate computation accuracy. Utilizing Kronecker factored approximations for Fisher matrices represents an adjustment for complex structures, in deep learning models. 

Ways to Enhance 
The theoretical derivations are thorough. May be challenging for readers who are not well versed in variational inference or Fisher information due to their complex nature and lack of clarity in presentation style; incorporating a more intuitive explanation or visual aids, for the main concepts could enhance understanding for a wider audience. 
The paper points out that hyperparameters were not fine tuned for varying training set sizes suggesting that delving into the impact of hyperparameter choices, on the methods sensitivity could enhance the assessment. 
The paper should also consider comparing the proposed method to submodular or Bayesian active learning approaches in addition, to random sampling and curriculum learning for a more thorough assessment. 
Lets talk more about how this method could be applied to types of architectures besides just CNN models, like transformers or recurrent networks. 
Questions to Ask the Writers
How much does the method depend on choosing the hyperparameter γ in the free energy formulation and can this parameter be adjusted automatically during training? 
Have you thought about using this method for areas like natural language processing or analyzing time series data instead of just image classification tasks, in the experiments? 
How well does the technique work when the dataset includes noisy or mislabeled data points, like those often found in real world situations? 
In summary this paper provides insights into active learning, for deep networks. Although there are areas that can be enhanced the originality, thoroughness and practical findings support its approval.