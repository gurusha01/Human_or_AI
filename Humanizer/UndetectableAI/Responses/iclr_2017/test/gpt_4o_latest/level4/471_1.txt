This research paper applies conversational models within the context of batch reinforcement learning techniques.The main concept involves using human evaluation data to assess dialogue model responses; however acquiring evaluations can be expensive.Thus it makes sense to utilize, off policy learning methods; a foundational policy is trained using data and then used to collect human ratings which are later used to enhance the model offline. 
While the impact might be modest by integrating, off policy actor critic techniques into dialogue generation procedures in this study is well supported and the document is presented in a straightforward manner that is easy to grasp. 
My main worry is about the dataset used to assess restaurant suggestions. It's quite small with 6K conversations compared to the larger datasets like Twitter or the Ubuntu Dialogue Corpus often used in dialogue creation tasks in the field. It's kinda surprising that RNN based chatbots can still come up with responses, without any extra tweaks given such a small amount of data. Wen and colleagues achieved results using a small dataset from a restaurant study in 2016; however their approach involves linking dialogue states to surface forms directly instead of using contextual embeddings, like this paper does.This raises the question of whether the techniques discussed here would lead to enhancements when implemented on significantly larger unsupervised datasets. 
Sources; 
The authors of the study titled "An End to end Trainable Dialogue System, for Task oriented Conversations" are Wen et al.. Their work was published in the arxiv preprint arxiv 1604.XXXX in the year 2016. 