This article presents a model for NLP tasks that require comparing and aggregating text sequences semantics in applications, like question answering and textual entailment.   
The main framework of the model includes using a neural network, for aggregation followed by a comparison element wise operation carried out on the attentive outputs of the LSTMs.   
One significant aspect of this study is the comparison of techniques, for align and compare text sequences; it was found that using element wise subtraction and multiplication consistently resulted in better performance across four separate datasets.   
On the side the primary drawback is that the project shows only minor improvements and lacks any groundbreaking ideas or advancements. Incorporating an in depth examination of the behaviors of subtraction, multiplication, and other comparison operations, across various sentence types would have brought more complexity and intrigue to the study..