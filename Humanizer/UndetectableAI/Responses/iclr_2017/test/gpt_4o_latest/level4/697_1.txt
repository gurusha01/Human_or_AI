The research paper delves into how Grassmannian SGD's utilized to enhance word embedding learning through optimizing the skipgram negative sampling (SGNS) objective function. Yet it's not entirely evident what makes the suggested optimization method superior, to the conventional vanilla SGD approach. Both methods lack theoretical backing and the practical outcomes only show slight enhancements. The fundamental idea. The algorithm for dividing the projector. Has found use in different machine learning scenarios as shown by previous studies like Vandereyckens work on completing matrices and Sepulchres research, on factorizing matrices. 
The analysis of the expenses for both approaches lacks sufficient detail in its explanation. For instance; How much computational effort does it take for the SVD calculation in equation (7)? Conducting low rank updates on the SVD is possible; where a rank one update needs O(nd) operations. So what would be the cost, for each iteration of the suggested method? 