There is a lot of interest in making network models smaller these days! One popular approach involves using precision for the weights of the model – sometimes as low as 1 or 2 bits! However... This has often led to a decrease, in accuracy levels when tested out in practice. This study introduces a method called iterative quantization where the network weights are gradually converted into quantized values over time – starting with the largest weights and moving down from there! The aim is to maintain accuracy while reducing size. The results of the experiments show that this method works well as it creates models with 4 bit or 3 bit weights without compromising on accuracy significantly. Even though there is a decrease in accuracy, at 2 bits level; the overall performance still outshines other methods of quantization. 
In conclusion the paper is nicely written with an approach the experiments are thorough and the results are quite persuasive. I suggest accepting it. Though it could use some editing, for better style and grammar. Also the explanation of the strategy inspired by pruning could be clearer. For example the 50% splitting ratio is only mentioned in a caption of a figure. Not directly explained in the main text. 