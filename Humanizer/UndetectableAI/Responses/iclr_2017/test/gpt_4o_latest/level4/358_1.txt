This paper presents a generative model with two tiers in place. One tier handles data points within datasets while the other deals with unordered groups of datasets. The key idea revolves around using a "variational bound where a latent variable, at the higher level defines datasets and another at the lower level describes individual instances. 
Hierarchical modeling is an influential field of study that I think has not received as much attention, in the Deep Learning community as it deserves. 
Advantages; 
  The outcomes of few shot learning look positive, to me even though I don't have expertise in this field.   
  The idea of using a "variational bound in a hierarchical generative model is well explained and seems to have wide ranging potential applications.   
Queries; 
  Are batches (meaning smaller sets of examples used during training the statistical network)?  
  When using minibatches in this method does it give an estimate of the complete gradient as if we used all examples? For example think about a situation where the neural network tries to detect if any data point, in the dataset has an attribute and uses that as the datasets representation. This matches the model shown on the right side of Figure 1. When the statistical network is trained in batches of data samples called minibatches of using the whole dataset at once it might not capture certain patterns accurately because some dataset examples may not be included in a specific minibatch. This could limit the models ability to express relationships effectively.   
Here are a few ideas for you to consider.
  Delving into applications of forecasting like predicting electricity usage or sales could offer a fascinating and useful scenario, for utilizing this model. 