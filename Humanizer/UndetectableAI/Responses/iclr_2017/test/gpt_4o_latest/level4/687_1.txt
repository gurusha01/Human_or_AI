The authors have put in a deal of work to investigate the "basic essence of learning representations, in neural networks " which is a topic that garners a lot of attention and importance within our community.They seek to do this by using a series of straightforward pruning algorithms to study how performance deteriorates with unit pruning.Although the concept is interesting and shows promise I feel that the paper does not fully tap into this potential. 
The initial part about pruning introduction seems a bit lengthy without presenting any new or surprising ideas of note. For example. The inclusion of Figure 1 seems unnecessary. So does a large portion of the preliminary information, in Section 3 point 3 point zero. The pruning algorithms themselves are sensible though rather simple; this wouldn't be an issue if they effectively tackled the research question. Yet when it comes to contributions made by the paper. I don't see it offering an concise perspective that feels particularly fresh regarding pruning. 
Section 4 barely touches upon the topic at hand. Lacks depth in its analysis. A crucial aspect for my overall assessment of the matter at hand.The visuals mainly depict the expected decrease in efficiency when neurons are pruned or values are eliminated without delving into profound perspectives.The research is confined to a scope, with emphasis placed only upon a simplistic issue and MNIST dataset.This fails to persuade me that the conclusions drawn can be applied to inquiries concerning neural networks. 
In the end of it all the paper lacks any algorithmic, architectural or mathematical revelations, which I believe are crucial, for all studies except those focused primarily on empirical data. 