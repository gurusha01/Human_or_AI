This study delves into exploring how actor critic algorithms with experience replay can learn off policy to boost the sample efficiency of reinforcement learning techniques â€“ an complex challenge in the field. The authors address this issue by suggesting a strategy for trimming importance weights and introducing a modified trust region optimization approach along with incorporating the retrace method. By combining these methods in their research work on Atari and MuJoCo benchmarks they showcase notable advancements in enhancing sample efficiency. My key focus lies in comprehending the impacts of each technique on the overall enhancement, in performance. Performing tests to separately identify and assess the benefits of each of these elements would offer perspectives. 