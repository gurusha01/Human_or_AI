This study discusses the effort to reduce the size of neural network models through using low bit weight representations specifically focusing on 1 or 2 bit weights in previous methods that faced challenges due, to accuracy drops.The innovative iterative quantization approach suggested in this paper presents a solution.By quantizing the network weights starting from the largest ones and holding them fixed while allowing unquantized weights to adjust and minimize errors the authors showcase remarkable efficiency. The test outcomes show that achieving models with 4 bit or 3 bit weights results in reduction in accuracy; even at 2 bits level the decline, in accuracy is slight compared to alternative quantization techniques. 
The document is nicely. The approach seems original, from what I know best of my understanding so far.The trials are thorough. The outcomes are very persuasive which makes me inclined to suggest approval.However a second check of the writing style and grammar could be useful to improve clarity. The description of the pruning driven partition approach might need enhancement by offering a concise rationale, for selecting the 50 percent splitting ratio. Presently only noted in a figure caption and not thoroughly addressed in the main body of the text. 