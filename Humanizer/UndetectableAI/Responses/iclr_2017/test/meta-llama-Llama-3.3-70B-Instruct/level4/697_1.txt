This study delves into using Grassmannian SGD to enhance word embeddings by optimizing the gram negative sampling (SGNS) objective function; however it's not clear how this optimization method differs significantly from the conventional SGD approach in terms of benefits since both lack theoretical guarantees and show only minor enhancements in empirical comparisons. Enough the idea of projector splitting algorithms has been used before in different machine learning scenarios as evidenced by Vandereyckens work, on matrix completion and Sepulchres study on matrix factorization. 
There hasn't been an examination of the computational expenses linked with both methods yet; for instance there's no mention of the cost involved in executing SVD in equation (7). It's important to highlight that a cost effective low rank update can be carried out on the SVD with a rank one update demanding O(nd). Therefore it would be helpful if the computational cost, per iteration of the suggested method is explained clearly for a grasp of its real world implications. 