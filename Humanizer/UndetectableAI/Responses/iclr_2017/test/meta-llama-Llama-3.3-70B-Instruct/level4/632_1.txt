The main points of this paper can be summarized as follows; 
A new TransGaussian model similar, to Translational Embeddings (Trans E) has been proposed by Gu et al. in 2015 for parameterizing subject and object embeddings using a distribution method to better handle path queries efficiently. 
Developing an LSTM + attention model that utilizes entity and relation representations acquired through TransGaussian to grasp a distribution of relationships, for answering questions based on natural language queries. 
Exploratory research conducted on the World Cup 2014 dataset emphasized path queries and conjunctive queries. 
Overall the Gaussian parameterization shows a lot of potential for applications, like knowledge base completion and question answering.This being said there are some parts of the document that could use solid evidence to support them and the writing could be enhanced.There are thoughts mentioned below; 
I have some feedback, to share.
One major issue is the reliability of the assessment findings presented in the paper.The fact that established benchmarks like FB15K and WebQuestions are available for evaluating knowledge base completion and KB oriented question answering makes relying solely on the small World Cup 2014 dataset appear unconvincingly justified.Additionally the employment of template generated questions that differ greatly from authentic language queries brings up doubts regarding the need for implementing an LSTM in this scenario.Demonstrating the models performance, against recognized benchmarks would greatly enhance the credibility of the paper. 
When dealing with linked queries inquiring about identified elements being associated with relationships simultaneously for conjunctions to be possible creates an assumption that may not consistently apply in practice; hence verifying this method using actual question and answer data is crucial. 
The label "Gaussian attention" for the model could be confusing since it appears to have ties to the literature on knowledge base embedding than, to the conventional attention mechanism. 
I'd be happy to help with that. Let me know if you need assistance, with anything.
Figure 2 can be a bit unclear. The top row of orange blocks shows KB relations and the bottom row represents words, from the natural language question. 
Acknowledging the significance of a component known as "entity linker," which connects text mentions to entities in a knowledge base is crucial, alongside "entity recognition."