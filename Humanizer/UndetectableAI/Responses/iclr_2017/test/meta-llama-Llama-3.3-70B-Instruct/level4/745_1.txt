This research paper introduces a parallelized stochastic gradient descent (SGS) approach tailored for situations where gradients can be calculated through operations like least squares linear regression and polynomial regression tasks.The aim of the authors is to mimic the outcomes of SGS by utilizing a suggested combiner that is fine tuned through the utilization of a randomized projection matrix for reducing dimensions.Experimental findings suggest that the proposed technique outperforms existing methods such, as Hogwild!. Allreduce in terms of speed enhancements. 
It seems like there's a misunderstanding about SGD (Stochastic Gradient Descent). Creating the combiner matrix M can be both expensive and extensive in size. Comparing it to the SGD method that needs O(f) level space and time to adjust the weight vector w (where f represents the feature count) the parallel SGD technique presents a space and time complexity of O(f^2) mainly because of M being an f, by f matrix. To achieve speedups would require a significant number of processors (O(f)) making it impractical, for datasets containing thousands or millions of features. 
Updating the product of a matrix Mi and vector v in f dimensions does not always demand space and complexity of O(f^2). Given that Mi's a low rank matrix, in the format (Identity matrix. Outer product of ai) the complexity can be lowered to O(f) calculated as O(v. Ai multiplied by (ai transpose multiplied by v)). If M_i represents the result of multiplying n rank 1 matrices in this scenario discussed in the paper. Where n is much smaller than f. There are doubts, about the accuracy of the authorsâ€™ assumptions and methods used in their experiments that appear to stem from a misinterpretation of the space and complexity involved in SGD. 
The reason behind the increase, in speed using this method is not clear since its uncertain which calculations are being run simultaneously and why the sequential algorithm would result in faster performance if M_i*v is calculated effectively. To improve the clarity and theoretical foundation of this paper the authors should consider; 
Can you share the difficulty, for each step of the algorithm you're suggesting?
Analyzing the rate at which convergence occurs is crucial in grasping the impact of reduction, on complexity since solely examining convergence is not enough. 