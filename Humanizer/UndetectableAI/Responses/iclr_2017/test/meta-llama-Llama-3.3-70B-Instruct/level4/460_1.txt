This research paper delves into the exploration of off policy learning in actor critic methods with the utilization of experience replay â€“ an intricate issue impacting the effectiveness of reinforcement learning algorithms when it comes to sample efficiency enhancement. The authors tackle this issue by introducing a method for truncating importance weights and implementing a modified trust region optimization strategy while also incorporating the retrace method into the mix. The collaborative application of these strategies produces outcomes on Atari and MuJoCo benchmarks by showcasing improved sample efficiency. A crucial question arises regarding how each technique contributes to the overall enhancement, in performance. Carrying out tests to measure the individual advantages of these approaches would offer valuable knowledge and shed light on their distinct contributions, to the improvements noted. 