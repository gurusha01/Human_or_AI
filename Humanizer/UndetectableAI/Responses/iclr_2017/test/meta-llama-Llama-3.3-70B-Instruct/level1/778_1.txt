Summary of the Research Article
The article introduces a method to lessen the computational burden at testing time for deep neural networks by breaking down weights and activations into integer and non integral parts.It involves approximating the weight matrix with real valued coefficient matrices and converting the activation vector to a sum of weighted binary vectors.This simplifies forward propagation through logical operations making it ideal for use, on low power CPUs or specialized hardware. The authors showcase how well their method works on three networks by achieving notable speedup and reducing memory usage with only a slight uptick, in errors. 
Choice
"I have chosen to approve this paper for a key reasons;"
The method is well supported and situated within the existing research landscape by expanding on studies involving matrix/tensor factorization and integer breakdowns. 
The study includes in depth tests on different networks to show how well the suggested approach reduces the need, for computing power and memory usage. 

The document offers an concise description of the suggested methods details with mathematical equations and algorithmic specifics clearly outlined.The studys findings are thorough. Effectively demonstrated across different performance metrics including speed enhancement rate,memory usage reduction rate and error margin expansion.The authors also engage in an analysis of previous research, in the field to underscore both the strengths and weaknesses of their proposed strategy. 
Further Input Needed 
To enhance the quality of the paper more. I recommend that the authors take into account the following suggestions; 
Delving deeper into the selection of hyperparameters, like the quantity of basis vectors (kw and kxs). How they influence the methods effectiveness. 
Exploring how well the suggested method could work with kinds of neural networks, like recurrent neural networks ( RNN s ) or long short term memory ( LSTM ) networks. 
In considering the capability of the suggested approach for uses, like edge computing or instant data processing. 
Queries, for the Writers
To make sure I get the paper right in my head. Could you help me out by answering these questions? 
Could you give me information, about how complex the new method is computationally speaking and specifically in terms of the number of logical operations needed for feed forward propagation? 
How do you intend to tackle the problem of error spreading when condensing convolutional layers as highlighted in the paper? 
Is there any intention to make the proposed methods implementation available as open source code, for the community to use in their research and experiments? 