This study introduces a method for training word embeddings with the Skip Gram Negative Sampling (SGNS) model that is widely used for developing word representations.The researchers suggest a way to solve the SGNS optimization problem in two steps; first finding a low rank matrix that maximizes the SGNS goal and then deriving word embeddings, from this matrix. 
The paper suggests that it adds value to the realm of natural language processing by presenting an algorithm that utilizes a Riemannian optimization framework for directly optimizing the SGNS objective on low rank matrices.In their study the authors showcase the effectiveness of their method by contrasting it with leading competitors like the SGNS optimization technique and the SVD, over SPPMI approach. 
I have chosen to approve this document for two reasons;  
The paper addresses an specific issue, within natural language processing. Focusing on enhancing the SGNS model for word embeddings.  
The authors approach is well supported by research and the experimental findings show that their method is effective. 
The paper presents a defined and organized explanation of the suggested method that includes a thorough account of the Riemannian optimization framework and the projector splitting algorithm employed to enhance the SGNS objective function calculation process. Furthermore the authors conduct an in depth examination of the outcomes by comparing them with top competitors in the field and testing the effectiveness of their approach, on various standard datasets. 
The article backs up its arguments by using both analysis and real world testing results to support them effectively. The writers explain the SGNS optimization issue thoroughly and reframe it as a two step process; they validate the efficiency of their method with tests, on standard datasets. 
To enhance the paper more effectively and thoroughly explain the implementation of the Riemannian optimization framework and projector splitting algorithm with specifics, on hyperparameter tuning and optimization methods used for enhancing the proposed approachs performance would be beneficial suggestions for the authors to consider.The authors could also delve deeper into analyzing the complexity of their approach and its ability to scale with large datasets. 
What I'd appreciate is if the authors could address a queries to help me better grasp the paper. 
Could the writers offer information regarding the selection of hyperparameters, like the size of the word embeddings and the number of optimization procedure iterations used? 
How does the suggested method deal with words that're not, in the vocabulary or were not encountered during training? 
Could the writers offer insights, on how easy it is to understand the word meanings embedded in the suggested method by showcasing visual aids or sharing their qualitative assessments of the acquired representations? 