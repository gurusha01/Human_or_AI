In brief...
The article introduces a method for developing state representations in multi task reinforcement learning known as Multi task Learning with Robotic Priors (MT LRP). MT LRP discovers simplified state representations from unprocessed observations without supervision on the task being performed or the quantity of tasks involved. The approach relies on a network structure, with gates and is educated using an expanded version of the robotic priors learning objective. The researchers show in their experiments that MT LRP is effective in creating state representations, for reinforcement learning and they delve into the reasons and conditions under which it succeeds in doing so. 
Choice
 I have chosen to approve this paper for two reasons; firstly because the approach is well founded and contextualized within existing literature and secondly because the paper provides comprehensive experiments and analysis to support its assertions. 
Arguments, in favor 
The research paper offers an brief overview of the challenge of learning state representation for multiple tasks in reinforcement learning and emphasizes the importance of a fresh perspective, on the issue. The writers conduct an examination of previous studies that points out the drawbacks of current techniques and underscores the benefits of their method. The testing phase is detailed and carefully planned to showcase how well MT LRP performs across situations. Studying the outcomes gives us knowledge on the effectiveness of MT LRP and sheds light on possible directions, for future research endeavors. 
More Input Required
To enhance the paper further I recommend that the authors delve into specifics regarding the implementation of the gated neural network structure including details on the activation functions employed and how the weights are initialized.Additionally it would be intriguing to observe experiments testing MT LRPs resilience to various forms of distractions and noise in observations.Lastly the authors could explore real world applications of MT LRP in fields, like robotics or autonomous driving. 
Queries, for the Writers 
I'd like to ask the authors a questions to better understand their paper; (1)" Could you elaborate on how the task detector Ï‡'s trained and how it determines task switching? (2)" How would you address situations where the number of taskss uncertain or changes dynamically? (3)" Could you shed light on how the task separation impacts the learning performance of MT LRP?"