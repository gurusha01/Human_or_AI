Summary 
The article presents a method for selecting batches of data to train deep neural networks like Convolutional Neural Networks (CNNs). This technique uses techniques to estimate the uncertainty, in the model weights and make informed choices about which data points to include in each batch for training purposes. The approach was tested on the MNIST and USPS datasets. Showed enhanced test accuracy when compared to random selection methods while also proving to be scalable as the query size increased. 
Choice
My decision is to approve this paper for two reasons. Firstly because it addresses a crucial issue in deep learning by expanding active learning to deep networks and secondly because the proposed method is well supported by incorporating established principles from Bayesian inference and variational methods. 
Reasons to back up your points.
The document offers an organized overview of the suggested method with an in depth exploration of prior research and a comprehensive explanation of the underlying principles.The practical assessment showcases the efficiency of the technique by showcasing enhancements in test accuracy when compared to standard methods.The implementation of a selection process and approximations to the Fisher information matrix allows for scalability making it feasible, for extensive datasets. 
Extra Input 
To enhance the paper more I recommend that the writers take into account the following aspects; 
Could you please offer a thorough examination of the approximations employed in the analysis? Specifically delve into the Kronecker factored approximation of the Fisher information. Consider any constraints that may arise. 
Explore how well the method holds up against types of disturbances and unusual data points. 
Lets explore expanding into types of deep learning models, like recurrent neural networks or transformers. 
Could you elaborate further on how the method handles tasks efficiently and include a thorough examination of the time complexity involved?
Queries, for the Writers
Could you please answer these questions to help me better understand the paper?
Could you explain further why the free energy is a good measure for guiding active learning, in deep networks? 
How do you aim to handle the uncertainty of the long term distribution employed to estimate the posterior probabilities in active learning scenarios, with limited sets of observed data? 
Can you talk about links between your method and other maximum likelihood estimation (MLE) based active learning standards like the one introduced in the research, by Zhang and Oles in 2000?