
The study introduces a neural network design known as Doubly Recurrent Neural Networks (DRNN) aimed at creating tree like objects from encoded representations.The structure includes two recurrent components. One for ancestry details and another for sibling relationships. That work together to forecast the designation and structure of each node in the tree.The researchers assess the efficiency of DRNN in assignments such as reconstructive synthetic trees,sentence, to program conversions and language translation tasks. 
Choice
After consideration of the paper at hand and its contents 	I have chosen to approve it for two main reasons; 	Firstly 	The paper introduces an well supported framework for decoding tree structures. 	Secondly 	The results from the experiments validate the efficiency of DRNN models, across assignments. 
Points, in Favor 
The paper presents a concise and organized overview of the challenges associated with tree based decoding methods and the shortcomings of strategies in this area of study.The suggested DRNN design is thoughtful and supported by existing research findings; the authors offer a description of the model and its elements.The results, from the experiments are remarkable as they showcase how DRNN models can reconstruct tree patterns within sequences and convert sentences into basic functional programs. The findings also indicate that DRNN structures possess characteristics like adaptability to modifications in structure and gradual refinement in producing translations, for machine tasks. 
Extra Input; Thoughts 
To enhance the paper more effectively I recommend that the writers include additional information about how they trained and adjusted the hyperparameters for the DRNN models in more detail. It would also be beneficial to have visual representations of the trees produced and a thorough examination of the mistakes made by the model. Lastly the authors should think about testing the architecture on more challenging tasks, like parsing or generating code to showcase its efficiency further. 
Inquiries, for the Writers.
Could the authors please provide information on the following points?
How do the creators intend to expand the DRNN design for more intricate assignments, like parsing or generating code? 
Could the writers offer information regarding the computational effectiveness of the DRNN structure in comparison, to current methods? 
How are the writers intending to tackle the exposure bias problem, in the DRNN design when the model is trained on labels but produces labels while being tested? 