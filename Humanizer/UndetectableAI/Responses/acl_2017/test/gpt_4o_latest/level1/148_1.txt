Assessment of the Entry
  
This study explores how good reading comprehension datasets are by introducing two types of evaluation measures. Prerequisite skills and readability levels.The researchers examine six reading comprehension datasets (such as MCTest and SQuAD) using these measures and uncovering connections, between text ease of understanding and question complexity.The research shows that easy readability doesn't always mean questions and suggests the possibility of creating reading comprehension datasets that are simple to read but demanding in terms of answering questions. The authors further develop the categorization of skills for RC by integrating findings from psychology and textual inference studies while also analyzing six datasets, with these metrics and sharing the outcomes with the public domain. The primary achievements of this study include;   
Two categories of evaluation metrics are utilized to assess reading comprehension datasets. They indicate a limited link, between text readability and the complexity of questions posed.   
Enhancing the skills, for reading comprehension by incorporating both psychological and linguistic perspectives.   
Annotating six datasets with the suggested metrics to create a reference, for studies.   
Areas of expertise  
A novel framework for analyzing datasets has been introduced with two new evaluation metrics. Prerequisite skills and readability. Which's a notable addition to the field of research in dataset analysis for reading comprehension (RC). This approach offers a nuanced perspective on the quality of RC datasets compared to conventional metrics such, as accuracy or question formats.   
Intriguing Discoveries. The discovery in the paper that the level of readability and the complexity of questions are slightly connected is groundbreaking and significant.These findings question established beliefs in designing reading comprehension datasets and pave the way, for developing datasets that're easy to understand and yet challenging.   
  
The theoretical foundation for honed abilities is firmly rooted, in psychological and linguistic theories which bolster the validity of the suggested measures.   
The research provides suggestions for creating RC datasets and developing systems by utilizing the metrics to create gradual curricula, for RC systems.   
Areas of improvement  
The studys dataset selection is somewhat restricted as it only considers six datasets and overlooks known ones such as CNN / Daily Mail and CBTest This could affect the applicability of the results as these popular datasets were not included for reasons like errors or misalignment, with the task objectives It would have been beneficial if more effort was put into addressing these issues to ensure a broader dataset representation.   
Subjectivity in Annotation; The process of annotation involves annotators and can lead to subjective decisions despite being meticulous in nature. The paper lacks an explanation of how conflicting opinions among annotators were addressed or how uniformity, in annotation was maintained.   
The paper lacks an evaluation of any reading comprehension systems using the metrics meant to steer the development of systems.This absence undermines the usefulness of the framework suggested in the paper.   
The paper places an emphasis, on readability metrics without delving into how these metrics correlate with the comprehension abilities of machines. Such as how readability metrics meant for humans may not necessarily reflect machine performance directly.   
The paper briefly touches upon challenges related to "senseless" queries and questions, without answers. It does not offer any specific solutions or approaches to tackle these issues.   
Queries, for Writers  
How did the annotators handle disagreements while working on annotations. Can you give further insights into how much they agreed with each other?   
Have you thought about testing the RC systems with the datasets to confirm if the suggested metrics are useful?   
Is it possible to adjust the readability measures to accurately represent the level of understanding, by machines?   
General Evaluation  
This paper provides an addition to assessing RC datasets with its unique measurements and in depth examination.However the absence of system evaluation and the restricted dataset range slightly lessen its effectiveness.Correct ing these shortcomings, in studies could greatly improve the usefulness and applicability of the suggested framework. 