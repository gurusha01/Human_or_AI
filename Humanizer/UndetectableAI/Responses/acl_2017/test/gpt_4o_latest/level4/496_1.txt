The authors thoroughly delve into language contexts to study the interplay between linguistic similarity and morphological intricacy (for example translating between closely related languages with complex morphology versus distant ones) and how these factors impact the systems grasp of morphology.They offer an evaluation of the architectural elements that aid, in morphology learning by emphasizing the role of attention mechanisms in generating less intricate target side representations.Their discoveries hold relevance and practical importance for the wider NMT community. 
The paper has some weaknesses in terms of explaining how they implemented their character based encoder in detail properly and discussing the methods for learning character based representations that could have been used could raise doubts about how broadly their results can be applied to other scenarios.The analysis would have been more effective if they had chosen languages with morphological systems, like Turkish or Finnish and delved deeper into predicting and analyzing morphology. 
This research delves into the understanding of how NMT models grasp morphology through training NMT systems and utilizing encoder or decoder representations as input for tasks like POS or morphology tagging classification tasks building upon the approach used in "Does String Based Neural MT Learn Source Syntax?". Focusing on morphology instead.This study offers insights, into the ability of NMT systems to learn morphology effectively. 