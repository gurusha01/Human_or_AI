Here is the paraphrase; "Rephrasing the evaluation"
Strengths; 
This study presents an enhancement to attention based neural machine translation (NMT) systems by integrating source sentence segmentation as an extra source of data input.The design is modified so that the segmentation data is applied in ways across two recurrent layers; one layer creates a segment at a time while the other concentrates on generating the words within each segment.This strategy is fresh and captivating; I think it will attract readers interested, in exploring approaches and their effectiveness. 
The manuscript is well written and provides clear explanations along with detailed comparisons, to other methods used in the field of study. The evaluation process is well done as it directly compares the method with previous works using comprehensive tables and analyses. Although there are areas that could be enhanced (as indicated in my feedback the results are persuasive and contribute positively to the field. 
Limitations; 
The experimental section could benefit from details to strengthen the case for chunk based models as shown in Table 3 with promising outcomes for Model 2 and Model 3 in comparison to previous studies; however a critical reader might wonder whether these improvements are mainly attributed to transitioning from LSTMs to GRUs instead of the chunk based method itself. To address this concern effectively and validate the significance of the chunk based approach in contributing to the enhancements observed in the results presented for Model 2 and Model 3 compared to research outcomes; it would be advantageous to include findings from a GRU based tree, to sequence model analysis. 
One important drawback is the lack of combining outcomes in the research findings highlighted by the writers as they claim their model is currently the most effective individual NMT model available – a statement that seems plausible indeed! Nonetheless! The finest WAT system for translating English to Japanese achieves a BLEU score of 38;20 (if my understanding of the table is correct) utilizing a trio of models, for processing. Should the authors be able to show that their chunk based models three way ensemble can surpass this benchmark level; it could drastically elevate the significance and impact of their paper. 
Finally adding decoding times to Table 3 would enhance its clarity. While the authors mention in passing that the character based model is less time consuming (possibly referring to Eriguchi et al., 2016) they do not offer a citation or exact numbers. Moreover there are no details provided about the decoding speed of the chunk based model. Is it quicker or slower than word based decoding ?. Is it similar ? Having a section in Table 3 dedicated to decoding times would provide context, for understanding the findings.
Lets talk about stuff. 
In my opinion I think this paper is quite intriguing and deserves to be published. I do have some ideas to make it clearer and better presented which I'll elaborate on below; 
The writers need to mention at the beginning of the document that the sections are provided externally and that the model does not independently learn to section them out itself. 
The repeated mentions of comparing to the character based standard in the text (such as the + ̧ 68 BLEu improvement highlighted times) may not be needed as much as discussing the performance improvements, over the most robust baseline, which readers would likely find more engaging. 
It could be beneficial to offer explanation regarding the neural decoders approach, to managing unknown tokens (UNK). Another option would be for the authors to cite the dictionary based replacement strategy they are employing. 
The sentence found on line 212 is not completely correct as it states "We train a GRUR that encodes a source sentence into a vector." To be more accurate and descriptive; a bidirectional encoder is employed to encode the source sentence into a group of vectors as illustrated in Figure 2. 
The illustration that was mentioned in lines 69 to 87 seems a bit ambiguous to me too; Does the term "you" rely on "bite ". Is its reliance based on the context of the source side instead? If the dependency isn't on "bite " then it doesn't appear that the argument, about this being a long dependency matter holds true. 