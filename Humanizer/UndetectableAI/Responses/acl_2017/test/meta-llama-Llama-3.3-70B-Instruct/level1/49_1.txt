This study introduces a method for enhancing neural machine translation (MT). The key advancements, in this research include; 
The authors suggest incorporating chunks (phrases) of single words alone in Neural Machine Translation (NMT) to better represent long range connections and address the complexities of languages with flexible word order such, as Japanese. 
The authors created a decoder setup that includes a word level decoder and a chunk level decoder to represent the chunk structure, in the target language. 
The writers show that their decoder system using chunks can greatly enhance the translation results, for the WAT ’16 task of translating from English to Japanese and surpasses all models mentioned in the WAT ’16 report. 
The notable aspects of this paper include; 
A perspective on neural machine translation (NMT); The creators suggest a novel method, for NMT that integrates chunk based decoders to grasp distant connections and manage languages with flexible word orders proficiently. 
The researchers showcase enhancements in translation accuracy when working on the English to Japanese translation task, in the WAT '16 evaluation. 
The authors offer a rationale, for their hierarchical decoder design and show how it successfully captures the chunk organization in the language being targeted. 
The paper has its drawbacks; 
The authors only tested their method on one translation task, from English to Japanese without offering a thorough assessment, across different language pairs or translation tasks. 
The authors suggest that their hierarchical decoder design might pose complexity and challenges, in training compared to conventional NMT models. 
The authors missed comparing their chunk based NMT approach, with others which could have helped in understanding its strengths and weaknesses. 
Queries for the writers; 
How do the authors intend to apply their method to languages and tasks, in the future? 
Could the writers offer information regarding how they trained the model and the specific hyperparameters they employed in their experiments? 
How are the authors planning to tackle the intricacies of their hierarchical decoder design and improve its training efficiency? 