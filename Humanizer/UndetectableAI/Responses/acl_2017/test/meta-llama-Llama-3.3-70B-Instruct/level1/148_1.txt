This study provides an examination of reading comprehension (RC) datasets by assessing two types of evaluation measures. Essential skills and text readability. The researchers enhanced the skills initially introduced by Sugawara et al. (2017) And Sugawara and Aizawa (2016) outlining thirteen skills such, as object tracking, mathematical reasoning and coreference resolution. Additionally they utilized readability measures that rely on features suggested by Vajjala and Meurers (2012). The writers have added these measurements to six established reading comprehension datasets. Shared the outcomes to showcase the unique features of each dataset. 
The key highlights of this paper include; 
The use of two types of evaluation measures to assess RC datasets offers a thorough insight into the quality and attributes of the datasets. 
Refining the skills, for reading comprehension involves categorizing them into thirteen different skills that encompass various elements of understanding written text. 
We have annotated six RC datasets using the specified metrics and have shared the results, for future research purposes. 
The advantages of this document are; 
The in depth examination of RC datasets, with the use of two types of assessment measures offers an insight into the unique features of the datasets. 
The improvement of abilities, for reading comprehension that can be utilized to enhance the efficiency of reading comprehension systems. 
The release of datasets to the public can aid in advancing research, in Reading Comprehension (RC) in the future. 
Some areas that could be improved in this paper are; 
The datasets annotated are limited in number. May not fully represent all reading comprehension datasets, in general. 
The authors had to depend on annotators to choose sentences and annotate prerequisite skills which could introduce bias in the annotation process. 
Evaluating the competence of choosing sentences in reading comprehension lacks a defined methodologyâ€”a crucial element, in the process. 
Questions, for writers; 
How do the authors intend to tackle any biases during the annotation process and what steps are being considered to guarantee the credibility of the annotated datasets? 
Could the writers offer information about how they calculate the distance between sentences and its relevance, to determining the effectiveness of choosing essential sentences? 
How do the writers intend to utilize the examination, in this research to develop a system that can be used across datasets and what difficulties and constraints could arise with this method? 