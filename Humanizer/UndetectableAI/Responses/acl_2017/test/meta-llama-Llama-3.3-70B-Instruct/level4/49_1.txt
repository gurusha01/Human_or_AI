Here is the review.
Strong points; 
The paper introduces an expansion of attention driven neural machine translation methods by integrating source sentence chunking as an extra source of information. This chunking data is applied in ways by two recurrent layers; one layer produces a chunk sequentially while the other concentrates on generating words inside the chunk. The document is nicely. Provides clear comparisons, with alternative methods. The assessment is effectively carried out with references, to other research articles and comparison tables to engage readers in understanding this method and its effectiveness. 
Shortcomings; 
To provide support for chunk based models and validate their effectiveness further experiments could be carried out. For example in Table 3 there are encouraging findings for Model 2 and Model 3 compared to studies but it remains uncertain whether the enhancements are a result of transitioning from LSTMs to GRUs. Incorporating the GRU tree to sequence outcome would confirm the efficacy of the chunk based method. Furthermore the absence of results is noteworthy as the authors highlight that this is, among the top single NMT models presented in literature. Sharing the outcomes of incorporating a 3 way ensemble based on chunks could greatly boost the papers influence. Moreover adding the decoding durations in Table 3 would enhance the papers worth by enabling a comparison of the chunk based models effectiveness, with models. 
Casual Conversation; 
I find the paper quite intriguing and worthy of being published! A few small suggestions for enhancing it; it might help to explicitly mention the external origin of the chunks since this detail only becomes clear when discussing Cabocha in depth around page 6. Additionally of frequently comparing to the character based baseline the authors could consider emphasizing improvements over the top baseline choice. More information, about how words (UNKs) are handled by the neural decoder or references to the dictionary based replacement approach could add value as well!Additionally there is an error in a statement on line 212 related to how the encoding's done. This can be rectified by mentioning the use of an encoder. Lastly the example that serves as motivation from lines 69, to 87 can be improved to highlight the issue with dependencies more effectively. 