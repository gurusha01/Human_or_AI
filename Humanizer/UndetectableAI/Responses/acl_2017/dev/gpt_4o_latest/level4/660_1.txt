The paper presents two approaches to creating English poetry creatively; The initial approach merges a neural phonetic encoder that anticipates the following phoneme with a phonetic to letter HMM decoder that identifies the most likely word connected to a series of phonemes. The second method blends a language model based on characters with a weighted FST to uphold rules, on the language models results. In the method suggested by the authors as well; they introduce a heuristic system that limits the poetry generated by themes (like love) and poetic elements (such, as alliteration). The poems created are analyzed using two types of measures – extrinsic ones; Internally by comparing the flow of the created verses with a benchmark Externally through a research study with 70 human assessors who (a.) determined if the poem was written by a person or a machine and (b.) rated the poems based on how easy they are to read and understand. The findings show that the latter model performs better, than the former and that human assessors often find it challenging to differentiate between poetry written by humans and that generated by machines. 
This paper is excellently. Captivating, showcasing fresh concepts in the form of two unique poetry generation models. One based on phonetics and the other, on characters yielding impressive outcomes. 
The section on evaluation could use clarity regarding who the evaluators were and the evaluation process details for better understanding.. For example; Did all evaluators review all poems equally?. If not so,. How many evaluations were gathered for each poem in every task?. Additionally,. The research mentions nine native English speakers as part of the evaluation team.. Considering the challenge of interpreting poetry. How proficient were these participants, in English?.
In the case of the model (character based) it's not clear if theres a way to stop the creation of made up words or not. If there isn't such a way, in place how often do those made up words show up in the poems generated by the model ?
Why did they opt for using an HMM of a CRG in the initial model for converting phonetic, to written representation? 
Given that the initial model wasn't quite up to par, as a versatile poetry generator model it would've been better to provide less detail on it and put more emphasis on assessing the second model instead​—especially diving deeper into the methods used to limit themes or poetic elements in a thoughtful way​—exploring how these limitations impact the evaluation outcomes​—and considering if they could be merged to restrict both themes and poetic elements at once? 
In the end pairing a model with a WFST brings to mind earlier research that merged a character based neural model to produce text from dialog acts with a WFST to avoid generating nonsensical words. The authors might want to connect their research with this study and include a citation, for it; 
Exploring the Creation of Human Like Language Using Character Based RNN Models, with Finite State Background Information.  
Authors Goyal and Raghav collaborated with Dymetman and Gaussier from LIG at Uni.   
COLING 2016 conference was held in the year 2016.