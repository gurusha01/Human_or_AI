This research introduces setups and learning goals, for neural sequence models when dealing with multiple tasks simultaneously—an important aspect since certain tasks overlap and mastering several tasks can boost overall effectiveness as emphasized by the authors. 
The methods section is quite clear and makes sense overall; however; it could be better organized for clarity and flow upon deeper analysis to identify two main concerns discussed by the authors. The existence of common attributes in exclusive feature spaces and unique features, in shared spaces. They propose two novel approaches to address these issues; one involves using an adversarial loss to keep task specific features out of shared representations while the other utilizes orthogonality constraints to prevent shared features from infiltrating task specific representations. The adversarial system can be confusing due to the layer incorporating parameters U and b in D(s^kT θ D) where the output is viewed as a probability distribution when compared to the real distributiona This may lead to worries that the system could end up masking task specific details, from LSTM outputs instead of ensuring their elimination. 
The assessment part closely matches the experimental setup and lays a strong groundwork for proving the effectiveness of the suggested techniques. It would be helpful to incorporate the recognized outcomes related to these tasks for better understanding. A notable issue lies in the explanation behind the chosen approach; expanding into partially shared characteristics appears more like a response, to challenges encountered in fully shared models. The introduction could be strengthened by presenting a coherent argument flow that begins with discussing the drawbacks of fully shared models and then explores the possibilities offered by shared private models along, with the practical challenges associated with them. 
The document might be improved by offering an explanation of the challenges associated with shared private models using straightforward examples and emphasizing the common patterns that determine which terms are shared or kept private. Insights, into the shortcomings of shared models and how the suggested approaches tackle these challenges are presented in Table 4. Furthermore " the study connects to unquoted multi task learning (MTL) researches like profound hierarchical MTL that talk about the characteristics enabling MTL and suggests that MTL is effective only when tasks share a significant similarity." The suggested approach might grasp the concept of " similarity " enabling it to dynamically determine when to exchange insights, across tasks. 