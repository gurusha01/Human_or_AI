This study suggests a method, for understanding phrasal structures and incorporating them into RNN driven language models and neural machine translation systems. 
The positive aspects of this project are; 
The inclusion of details, in the assignment is an interesting idea. 
There are a number of drawbacks that have been highlighted; 
The content can be hard to understand; getting a native English speaker to review the document could help make it clearer. 
The assessment method used in this approach has drawbacks. 
In the conversation; 
The model does not use the fixed length word embedding vector described in Equations 1 and 2; instead it depends on an RNN for representation generation without an explanation, for including this description. 
Using GRUs for the Pyramid and LSTMs for the aspect leads to some uncertainty. Could this blend of architectures be the reason, behind the enhancements we're seeing? 
Moreover it is said that the simplified form of the GRUs shows performance compared to its original version; however the underlying reasons for this and its effectiveness, with extensive datasets are not detailed. 
Table 4 includes RNNsearch (groundhog) well as RNNsearch (baseline) without clarifying the difference, between the two models. 
Using ending phrases without incorporating starting phrases lacks sufficient justification. 
It's not clear if the pyramid encoder was used on its own and how well it worked in doing this would create a fairer comparison by introducing more complexity to the model. 
Also not explained is why RNNsearch is executed times while PBNMT is only run once. 
In Section 5 point 3 of the document lacks explanation, about its intent and objective. 