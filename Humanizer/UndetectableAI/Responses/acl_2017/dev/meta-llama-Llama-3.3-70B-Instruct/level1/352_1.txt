This study introduces a task learning approach, with adversarial elements for dealing with text classification assignments highlighting the key advancements made in this research work; 
A new method called multi task learning framework is presented by the authors in their research paper to teach a training strategy that extracts features common, to different tasks while safeguarding against task specific features mixing into the shared space. 
The writers suggest adding orthogonality constraints to penalize latent representations and promote the shared and individual extractors to capture distinct features of the inputs. 
The authors have expanded the adversarial training approach to include multi class scenarios where multiple tasks can be trained together simultaneously. 
The positive aspects of this document are; 
The authors showcase how adversarial training can be a tool, in acquiring task invariant features and enhancing the capabilities of multi task learning models. 
The authors demonstrated that their suggested model surpasses baseline models in 16 different text classification tasks leading to enhanced performance, in those areas. 
The authors offer an examination and visual representation of the acquired characteristics to gain an understanding of how well their suggested model performs. 
The papers drawbacks include; 
The complexity of this model is quite high as it incorporates elements such as adversarial training and orthogonality constraints, alongside multi task learning techniques that could pose challenges during the training and tuning process. 
The analysis provided by the authors is somewhat restricted in its understanding of the outcomes as it lacks clarity, on how the acquired features correspond to the tasks and datasets employed in the study. 
The authors have not included a comparison of their proposed model with multi task learning models besides a few baseline models mentioned in the paper.It remains unclear how their model fares when compared to top performing multi task learning models available, in the field. 
Queries, for writers; 
How do the writers intend to expand their suggested model to cover NLP tasks, like labeling sequences or translating machines? 
Could the writers offer an in depth examination of the acquired features and their relevance, to the particular assignments and data sets employed in the studies? 
How do the writers intend to tackle the intricacy of the suggested model and enhance its effectiveness, for training and adjustment? 