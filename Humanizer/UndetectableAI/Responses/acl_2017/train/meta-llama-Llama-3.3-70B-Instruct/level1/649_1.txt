This study presents a method for assessing the effectiveness of conversational responses automatically—a key challenge in advancing conversational AI technology.The researchers introduce a model named ADEM that is designed to estimate human ratings, for given responses by leveraging a novel dataset of human generated scores.This research offers insights by introducing; 
The ADEM model introduces a recurrent neural network ( RNN ) based approach that goes beyond simple word overlap statistics to measure semantic similarity taking into account both the context and reference response when evaluating the models output score. 
The writers show that ADEM has the ability to assess models that were not seen during training sessions effectively establishing a solid foundation for evaluating automatic dialogue responses with accuracy and success, in new contexts. 
The authors demonstrate that ADEM scores have a correlation with human evaluations on both the individual utterance and system levels which surpasses traditional word overlap metrics, like BLEUI. 
The paper excels, in the following areas; 
A fresh perspective is introduced by the authors as they suggest a method, for assessing conversational responses that overcomes the shortcomings of traditional word matching metrics. 
The authors showcase the efficiency of ADEM by presenting experimental findings that incorporate correlations at both the level of individual speech and overall system performance, with human evaluations. 
The authors demonstrate that ADEM has the ability to assess models effectively—an essential trait, for a dialogue evaluation model. 
The flaws, in this paper are; 
The writers concentrate on systems that are not task oriented and may not be directly relevant to systems with specific tasks, in mind. 
Relying on annotations to train and assess ADEM can be quite laborious and costly to acquire. 
A possible issue to consider is that the ADEM model might tend to favor generalized responses as mentioned by the authors of the study. 
Questions, for writers; 
How do the writers intend to tackle the partiality of ADEM, towards general responses? 
Can the authors offer information about the dataset utilized in training and assessing ADEM (Automatic Dialogue Evaluation Metric) such, as the range of response scores and the reliability of the human annotations provided? 
How do the authors foresee the applications of ADEM and its potential impact, in shaping conversational AI systems development? 