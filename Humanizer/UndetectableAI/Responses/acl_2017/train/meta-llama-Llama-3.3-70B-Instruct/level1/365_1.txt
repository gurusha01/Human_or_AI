This research introduces a method, for standardizing historical texts by applying encoder decoder structures alongside multi task learning techniques highlighting the key advancements made in this study; 
Introducing encoder decoder structures, for the normalization of texts is a fresh perspective put forth by the writers. 
The researchers utilized multi task learning to enhance the encoder decoder models effectiveness by incorporating a task of mapping from written letters, to spoken sounds. 
The researchers thoroughly examine how multi task learning impacts the attention mechanisms, within the encoder decoder model. 
The key advantages presented in this paper are; 
The new method delivers outcomes on the Ansel dataset and surpasses various standard models. 
A new way of using task learning has been explored in historical text normalization by incorporating a secondary task of mapping from letters to sounds, which is a unique application of this technique. 
A thorough examination of attention mechanisms is conducted by the authors in their study on the impact of task learning on the encoder decoder models attention mechanismsâ€”an insightful addition, to our comprehension of these mechanisms. 
The shortcomings of this document are; 
The authors only tested their method on one dataset. The dataset. Which may not accurately represent all historical text normalization tasks due to its limited scope of data sources and variations in language use, over time. 
The authors did not compare their method to other neural network architectures and neglected to assess alternative neural network models that could be appropriate, for historical text normalization purposes. 
There is a necessity for examination of the impacts of adjusting hyperparameters as the writers only adjust hyperparameters in one document; this might not be adequate to guarantee that the outcomes are applicable, to different datasets. 
Queries, for writers; 
How do the writers intend to apply their method to historical text standardization databases? 
Could the writers elaborate further about how they tuned the hyperparameters and guaranteed that the outcomes could be applied to different datasets as well? 
How do the writers believe their method could be utilized in language related tasks like translating text or categorizing text content? 