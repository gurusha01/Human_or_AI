This paper introduces an A star CCG parsing model that breaks down the likelihood of a tree into components based on CCG categories and its dependency structure using bidirectional LSTMs as the framework, for these definitions. Key highlights of this study include; 
The authors suggest an approach that integrates CCG categories and dependencies to enable effective A star search and deliver top notch outcomes, in English and Japanese CCG parsing. 
The authors have introduced a model that focuses on predicting heads in bi LSTMs to address attachment ambiguities more effectively compared to older approaches based on fixed rules. 
The researchers demonstrate that their combined approach is compatible, with A* search algorithm – known for its efficiency and optimality – yielding outcomes compared to earlier techniques. 
The paper excels, in the following areas; 
The authors have achieved the F score results, in English CCG parsing compared to previous leading models. 
The writers’ parsing tool works efficiently by handling than five times the number of sentences compared to a leading parser, in A star search. 
There has been an enhancement in the performance of Japanese parsing with the method proposed by the authors surpassing a basic shift reduce parser, in CCG parsing tasks. 
The paper has some shortcomings, which include; 
The model created by the authors is quite intricate as it consists of elements such as bi LSTMs (bidirectional Long Short Term Memory networks) MLPs (Multilayer Perceptrons) and a biafline transformation technique.It could pose challenges when it comes to training and optimizing due, to its complexity. 
Relying on existing word embeddings can be limiting since they may not always be accessible, for all languages and domains as needed by the authors. 
The assessment of Japanese parsing in the study is. Further experiments are required to thoroughly evaluate how well their approach performs in this language. 
Questions, for writers; 
How do the writers intend to tackle the intricacy of their model and what methods can be employed to streamline or enhance it? 
Could the writers offer information regarding how they conducted the training phase of their work  such, as the specific hyperparameter configurations and the optimization algorithm that was utilized? 
How do the writers aim to expand their approach, to languages or fields and what difficulties do they foresee in the process? 