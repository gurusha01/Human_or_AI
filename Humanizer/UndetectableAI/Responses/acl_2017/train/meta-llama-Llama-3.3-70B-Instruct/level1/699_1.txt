Key Points, from the Research Paper
The research paper suggests a method to predict keyphrases in text by using an encoder decoder framework, with a recurrent neural network (RNN) and a copy mechanism. The goal of the model is to understand the meaning of the text and produce keyphrases that may not be explicitly mentioned in the text itself. The authors test their model across six datasets which includes a dataset named KP20K and show how well it predicts both existing and new keyphrases. 
Key Contributions
The authors suggest a generative model using RNN for predicting keyphrases, which includes a copy function to address less common phrases effectively. 
The model can anticipate missing phrases successfully—a difficult task that previous approaches haven't tackled yet. 
The authors extensively assess their model across six datasets. Showcase its efficacy in predicting keyphrases that are both present and absent, in a new dataset named KP20K. 
Areas of expertise
Keyphrase Prediction Efficiency; The model suggested shows results, in forecasting current keyphrases and can also anticipate missing keyphrases—a notable advancement compared to earlier approaches. 
The model can adapt to areas and types of content, like news articles without needing extra training. 
The authors thoroughly assess their model, across datasets to gain a broad perspective of its strengths and limitations. 
Areas of improvement
The suggested model lacks transparency as it operates like a box with results that are challenging to interpret and comprehend the rationale behind the predictions of specific key terms. 
The model needs a volume of training data to grasp efficient representations—a resource that might not be accessible, for every domain and text category. 
Computing Expense Issue; Training and assessing the model substantial computing power that could pose constraints, on its application at a larger scale. 
Questions, for Writers
How do the writers intend to tackle the challenge of explaining the proposed models limitations ? 
Could the writers offer information regarding how they trained the model and the computing resources needed for training it? 
How are the writers intending to expand the model to cover areas and genres of text, like novels or internet feedbacks? 