This study suggests incorporating ngrams into four techniques for word representation—SGNS and GloVe embedding methods along with PPMl and its SVD decomposition factors—bringing insights and advancements, in this field. 
Introducing ngrams into word representation techniques enhances word representations by expanding the information source from word co occurrences to ngram co occurrences statistics. 
A new method, for creating an ngram cooccurrence matrix is suggested by the authors to decrease disk input/output and ease the hardware load associated with ngrams. 
The authors perform experiments and assessments by testing word analogies and similarities, in various tasks and qualitatively assessing the ngram representations they've trained. 
The paper excels, in the following areas; 
Enhanced word representations have shown advancements in tasks such as word analogy and similarity, with the incorporation of ngrams. Showcasing the efficacy of the suggested method. 
Innovative method for creating a frequency matrix allows ngram based models to operate on affordable hardware solutions that are now within reach, for both researchers and industry professionals. 
High caliber ngram embeddings capture nuances and grammatical structures effectively to support a range of natural language processing tasks. 
The paper has some areas that could be improved such, as; 
The authors briefly looked into hyperparameters and stuck with the standard settings of the baseline models without considering if they were the best fit, for the ngram based models. 
The authors did not assess their method, against ngram based approaches to determine the uniqueness and efficiency of their proposed method. 
The authors only assess the ngram embeddings qualitatively. Do not offer a comprehensive evaluation of how well they perform in different natural language processing tasks. 
Queries, for writers; 
How do the writers intend to investigate the connections, between n word groupings and different hyperparameters in order to enhance the effectiveness of models based on n word groupings? 
Can the authors offer a comparison, with ngram based techniques to assess the originality and efficiency of the suggested method? 
How do the writers intend to assess how well the n word embeddings perform in different natural language processing tasks? 