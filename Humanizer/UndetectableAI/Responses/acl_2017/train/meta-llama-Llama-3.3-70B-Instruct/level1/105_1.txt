This article introduces a neural model for creating morphological inflections that uses a hard attention mechanism to accurately represent the alignment, between input and output character sequences in a nearly linear fashion.The key highlights of this research include; 
A hard attention model is suggested by the authors for generating inflections in a way that directly represents the consistent alignment, between input and output character sequences often seen in tasks related to morphological inflection generation. 
The researchers tested their model on three known datasets for morphological inflection generation (CELEX,Wiktionary and SIGMOHRPHON2016). They demonstrated that their model outperformed neural and non neural methods, in different scenarios. 
The writers examine the representations acquired by both hard and soft attention models to uncover the features these models extract for the inflection generation task. 
The paper has strong points; 
Efficient management of training data was showcased by the authors to prove that their model outperforms previous neural models when dealing with small training setsâ€”a significant advancement, in the realm of morphological inflection generation. 
The authors have also demonstrated that their model excels when trained on datasets. Proving its reliability and competitiveness, in generating morphological inflections effectively. 
The authors offer an examination of the acquired representations that reveals the characteristics that the models capture and their distinctions, between hard and soft attention models. 
The papers shortcomings include; 
A greater variety of comparisons to models would have been beneficial, for a more thorough assessment of the strengths and weaknesses of the authors approach. 
The writers did not include an examination of their model in their work; this analysis would have been beneficial, for comprehending the fundamental mechanisms and constraints of their methodology. 
The authors have only tested their model on tasks related to inflection generation; it would be intriguing to observe its performance in tasks, like machine translation or abstractive summarization. 
Queries, for writers; 
How are the writers intending to expand their model to deal with monotonic alignments that frequently occur in other sequence, to sequence assignments? 
Could the writers please offer information, about how they conducted the training and the specific hyperparameters they utilized to train their model? 
How do the writers believe their model could be used in tasks, like machine translation or creating concise summaries and what changes would be needed to ensure good results in these areas? 