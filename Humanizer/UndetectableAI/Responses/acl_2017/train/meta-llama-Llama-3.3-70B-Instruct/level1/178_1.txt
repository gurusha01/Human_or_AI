This study introduces an approach for simultaneously acquiring concept embeddings and word embeddings from an untagged text dataset by incorporating relevant phrases that represent ontology concepts as a form of indirect guidance or distant supervision method, into the learning process. 
The authors suggest a technique to concepts, phrases and words together in a common vector space enabling the representation of semantic connections, among them. 
Using ontology concepts through supervision is a method employed by the authors where representative phrases are utilized to train embeddings without requiring manual annotation. 
The authors show that their embeddings perform well on tasks involving similarity and relatedness when compared to methods that need manual annotation. 
One notable aspect of this paper is its strengths.
A fresh perspective, on concept embedding is introduced by the authors through a method that utilizes distant supervision and the combined embedding of concepts, phrases and words. 
The authors test their approach, on a dataset to showcase its efficiency across various ideas and expressions. 
The authors show that their approach performs well in comparison to methods, on tasks related to similarity and relatedness. 
The shortcomings of this document are; 
The authors did not clearly outline the connections between concepts in their work; this could hinder their methods effectiveness, in capturing semantic relationships. 
The writers depend on the ontologys quality to offer representative expressions for ideas; however lack of completeness or accuracy, in the ontology may pose a restriction. 
The authors need to fine tune the hyperparameters to achieve the performance levels; this process can be time consuming and resource intensive. 
Queries, for writers; 
How are the writers intending to tackle the challenge of dealing with connections, between ideas that are not clearly outlined in their existing method? 
How do the writers intend to assess the effectiveness of the ontology. How it influences their approachs performance? 
Could the writers share information, about how they fine tuned the hyperparameters and the computing resources needed to achieve the best results? 