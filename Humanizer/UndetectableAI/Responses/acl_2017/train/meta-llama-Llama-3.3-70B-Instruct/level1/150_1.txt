Overview of the Document
The research paper introduces a design, for neural machine translation (NMT) incorporating two recurrent networks and a hierarchical decoder to grasp morphology and perform character level translation effectively to tackle the challenge of handling extensive vocabulary in word based NMT models while delivering results on par with the best existing systems.The model comprises six neural networks and undergoes end to end training leveraging an adaptation of the gate recurrent unit (GRUs) referred to as HGRUs. 
Key Contributions
The paper presents an approach to Neural Machine Translation (NMT), with a unique architecture that employs two recurrent networks and a hierarchical decoder to understand morphology and translates content at the character level. 
Efficient Training Point 1. The system can effectively train a neural machine translation model at the character level, with superior results compared to word level models and similar results to top notch character level models. 
The research paper shows that the system can grasp morphology effectively to accurately interpret words and nonce words. 
Advantages
Solving the OOV Problem; The paper suggests a way to tackle the out of vocabulary (OOB ) problem faced by word level models by adopting a character level strategy. 
When training the model efficiently for a character level neural machine translation model is successful it leads to better performance compared to word level models. 
The research shows that the system can grasp morphology effectively to accurately translate words, with spelling errors and coined words. 
The model shows performance to the best systems available today which proves its effectiveness. 
The paper mentions that the approach could be used for tasks beyond speech recognition and text summarization due, to its flexibility. 
Areas, for improvement
The model includes six neural networks which might increase its complexity and make training more challenging compared to other models. 
The paper notes that the model needs a lot of computing power to run smoothly and this could pose a challenge, for researchers. 
The study assesses the model in three language pairings; however this might not be adequate to showcase its efficacy across multiple languages effectively. 
The paper discusses how the model measures up against character level models but could benefit from comparing it to word level models to showcase its strengths. 
The article lacks a theoretical examination of the models implications and potential uses. 
Queries, for Writers 
How does the model manage languages with morphology, like Arabic or Chinese? 
Can the system serve purposes, in natural language processing like creating language models or categorizing text? 
How does this model stack up against character level models when it comes to the resources required for computation and training duration? 
Is it possible to utilize the model for languages, with resources and limited training data available? 
How does the system manage words that were not part of the training data and are thus unfamiliar, to it? 