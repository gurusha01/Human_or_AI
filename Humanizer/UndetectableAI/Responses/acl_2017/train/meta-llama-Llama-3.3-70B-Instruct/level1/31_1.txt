This study introduces a learning model for determining the certainty of events in natural language processing tasksâ€”a significant aspect to consider in NLP applications.The process involves extracting details from original texts and then assessing event certainty, through a sophisticated neural network. 
The primary accomplishments of this study include; 
A method, with two stages to determine the accuracy of events described in texts. 
Using an attention based CNN to identify source introducing predicates ( SIP s). 
A new idea suggests using a neural network model that incorporates both Bi LSTM and CNN to determine the accuracy of events. 
The paper excels, in the following aspects; 
The new framework excels in delivering top notch outcomes on the FactBank dataset. Surpasses a number of basic models, in performance. 
The model effectively captures information from syntactic paths and words by utilizing attention based neural networks. 
The system can better recognize speculative values with the assistance of relevant indicators. 
The papers shortcomings include; 
The effectiveness of the model is greatly dependent on the quality of the identified characteristics, like SIPs (Service Improvement Plans) sources of information and signals. 
The model doesn't perform well on embedded sources because of the complex sentence structures. 
The model needs an amount of labeled data for training purposes; this could pose a challenge for languages or fields, with limited resources. 
Questions, for writers; 
How do the writers intend to enhance the models efficiency when it comes to embedded sources? 
Could the writers offer information about the attention mechanism utilized in the model and its role, in capturing significant details from syntactic paths and words? 
How do the writers intend to modify the model for languages or fields, with annotated data availability? 