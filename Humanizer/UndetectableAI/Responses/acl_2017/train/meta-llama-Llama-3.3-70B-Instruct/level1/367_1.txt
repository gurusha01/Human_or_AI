The research paper explores in depth the assessment of automated measurements for Natural Language Generation (NLG) systems specifically emphasizing on end to end data strategies.The study delves into word based and grammar based metrics including the most advanced ones and illustrates that they only marginally align, with human evaluations of system outputs. 
The key highlights of this study include; 
An in depth assessment of automated measures for natural language generation systems to assess word usage and grammar accuracy in relation, to evaluations. 
A novel metric called RAINBOX has been introduced to merge automated metrics advantages and attain a stronger correlation, with human evaluations. 
An, in depth examination of errors that showcases the challenges automatic measures face in distinguishing between system results of high quality. 
The paper has notable strengths; 
An in depth assessment of automated measurements, for natural language generation systems is conducted to gain a grasp of their capabilities and constraints. 
A new measure called RAINDBOW has been. Seems to be enhancing its correlation, with human assessments. 
Examining the error breakdown offers valuable insights into the constraints of automated metrics and emphasizes the necessity for additional exploration, in this field. 
The shortcomings of this document are; 
The evaluations narrow focus on end, to end data driven NLQ systems may not apply broadly to NLQ system types. 
Relying on opinions from the crowd can be tricky since they may not always be accurate or fair due, to biases and inaccuracies. 
The absence of a reason, for why automatic metrics struggle to differentiate between medium and high quality system outputs is puzzling. 
Queries, for writers; 
How do the writers intend to overcome the constraints of the assessment range and ensure that the findings can be applied to kinds of natural language generation systems? 
How can we enhance the quality and trustworthiness of judgments gathered from crowdsourcing efforts? 
Could the writers offer explanations, on why automatic metrics struggle to differentiate between system outputs of moderate and high quality? 