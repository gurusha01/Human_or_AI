This research introduces an approach called Knowledge Guided Structural Attention Networks (referred to as K SAN) which utilizes existing knowledge to integrate complex structures and adapt attention effectively to various subcomponents crucial, for specific purposes. 
The model learns automatically with the assistance of knowledge throughout the process in an end, to end manner by focusing on key substructures using an attention mechanism. 
The model is versatile in interpreting parsing outcomes like dependency connections and knowledge graph related relations as well as the parsing results, from manually designed grammatical structures serving as a source of guidance. 
Efficiency and ability to train models concurrently are aspects here. The training time doesn't necessarily grow proportionally with the length of the input sentence. 
The paper excels, in the following aspects; 
The model effectively utilizes knowledge to direct the attention mechanism and enhance the performance of the natural language understanding (NLU).
The new model performs well on the ATIS benchmark dataset compared to other standard models. 
The model demonstrates improved adaptability and resilience, to data availabilityâ€”particularly noticeable when the training dataset is small. 
Flexibility in representing knowledge is a feature of the model as it can work with various forms of knowledge representations, like dependency trees and Abstract Meaning Representation (AMRs).
The analysis of attention reveals that the model appropriately focuses on subcomponents with guidance, from external information despite limited training data availability. 
The paper has its flaws ; 
The models complexity could pose challenges during training and optimization due, to its architecture. 
The model heavily depends on information that may not always be accessible or precise. 
The model is assessed based on the ATIS benchmark dataset; however it may not accurately reflect performance, on other NLU tasks or datasets. 
The paper does not provide a comparison, with attention based models that could potentially have similar performance levels as the proposed model. 
Hyperparameter tuning is necessary, for the model. Can be quite time consuming and demanding in terms of computational resources. 
Questions, for writers; 
How does the system deal with situations when the existing information is not fully accurate or is lacking in details? 
Can this model be used for NLU tasks, like identifying intentions or analyzing sentiments? 
How does this model stack up against attention based models, like the ones found in machine translation or question answering tasks? 
What is the expense of training the model computationally. How does it change as the size of datasets increases? 
Is it possible to utilize the model in a scenario of task learning by training it on various Natural Language Understanding tasks all at once? 