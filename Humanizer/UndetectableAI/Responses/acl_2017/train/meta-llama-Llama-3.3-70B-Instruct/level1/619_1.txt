This paper introduces a collection of marked edits in persuasive essays that allows for, in depth studies in comparing writing and analyzing revisions effectively. 
The writers have put together a collection of essays that contain three versions annotated with notes on revisions for purposes like analyzing revisions and comparing paraphrases, for computational stylistics applications. 
The paper suggests a two part system for classifying edits to understand why changes were made and spot trends, in how people write. 
The authors showcase the corpuss capabilities by conducting two studies focusing on student revision patterns and predicting revision intentions. 
The paper excels, in the following aspects; 
The uniqueness of the collection of texts is noteworthy in its emphasis on the iterative revisions made within complete essays showcasing a valuable addition, to the realm of Natural Language Processing (NLP). 
The planned annotation framework, with its two tiered approach, to organizing edits shows clear reasoning and proves useful for examining different facets of writing habits. 
The paper showcases how the corpus can be used in real life scenarios by highlighting two studies that illustrate its benefits in enhancing writing guidance and creating automated revision analysis tools. 
The flaws, in this document are; 
The corpuss size is small, with 60 participants and may not accurately represent the broader population of writers. 
The paper mainly examines revisions at the sentence level. Doesn't delve into other facets of revision analysis, like sub sentence scopes or evaluative criteria. 
Further validation is required as the outcomes of the two studies outlined in the document show promise; however there is a necessity for confirmation to establish the efficacy of the dataset and the proposed framework, for analysis revisions. 
To all the writers there; 
How do you intend to broaden the range of texts to encompass a number of contributors and various writing examples? 
Could you please elaborate further on how the annotation process works and how well annotators agree with each other on the annotations? 
In what ways do you see the corpus being applied in real world scenarios like writing support or automated editing evaluations? 