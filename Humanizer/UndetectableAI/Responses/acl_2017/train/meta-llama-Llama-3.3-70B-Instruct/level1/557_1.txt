In this document suggests a neural network design for complete relation extraction that delivers top notch outcomes in two standard datasets.The key advancements, in this research are; 
The authors suggest a model that is globally optimized for complete relation extraction to enhance the overall score of the gold standard label sequence in a single sentence as a whole unit—an approach proven effective, in various NLP tasks but not yet tested in relation extraction previously. 
The writers present features, for LSTM models that include segmental and syntactic elements to depict the input sentence and enhance the models effectiveness. 
The authors suggest an approach, to incorporating syntactic details into the model without relying on parser results; this technique has proven to enhance the models effectiveness. 
The paper excels, in the following areas; 
Cutting edge outcomes are observed with the suggested model excelling in top tier results, across two datasets—which showcases the approachs efficacy. 
The paper introduces a variety of ideas such as an optimized neural model, at a global level and innovative LSTM features combined with syntactic information that are expected to capture the attention of the NLP community. 
The paper is nicely. Easy to understand the proposed approach as it is clear and well explained. 
The shortcomings of this document are; 
The models complexity poses a challenge as it demands computational resources for both training and testing phases; this could potentially restrict its effectiveness, on larger datasets. 
The paper could benefit from a thorough analysis and discussion of the strengths and weaknesses of the proposed approach in addition, to what is already provided in the analysis of the results. 
The paper would be more informative with an evaluation of its performance in relation to other advanced models, for extracting relationships in order to offer a comprehensive assessment of the advantages and limitations of the proposed method. 
Queries, for writers; 
How is the suggested model able to deal with words and entities that're not in the training data set? 
Could the authors please give us information, about the computational resources needed to develop and evaluate the suggested model? 
How well does the suggested model work on datasets that vary in characteristics, like datasets containing entities or relationships? 