This study introduces an improvement to the beam search process from left to right that allows for the inclusion of specific word sequences in machine translation results as lexical requirements.The suggested technique shows effectiveness, in situations involving translation and adapting to different domains. 
Although the proposed extension seems simple to grasp at first glance,l the way this idea is formally presented in the paper adds significant value to the discussion on neural machine translation (NMT). It's particularly interesting to note how NMT can adeptly deal with a group of constraints even in the absence of alignment information connected to them. The potential uses of this method go beyond the areas already examined; for example it could enhance NMTs ability to handle non compositional structures—a domain where NMT might not be as advanced, as traditional statistical machine translation (SMT).
The main drawback of the paper is that the experiments were limited in scope. Although the interactive machine translation simulation confirms that the method works well it's difficult to evaluate its performance especially when it comes to how often constraints are successfully integrated (since the significant increase in BLEU scores only gives indirect evidence). Additionally it would have been beneficial for the adaptation experiments to compare results, with a fine tuning" baseline, which could have been easily done using the 100K Autodesk corpus. 
Despite this downside the paper offers a contribution deserving of publication. 
Additional remarks; 
In the world of phrase based machine translation (PBMT) the term "coverage vector" can sometimes be confusing. It seems that a "coverage ​​set" might be a fit as a type of structure, for this purpose. 
Table 2 could be improved by adding the number of restrictions for each source sentence in the test data sets to offer a more detailed understanding of the enhancements, in BLEUL scores. 