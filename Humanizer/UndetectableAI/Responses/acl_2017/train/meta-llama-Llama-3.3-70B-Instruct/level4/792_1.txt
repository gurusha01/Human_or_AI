Lets take a look, at; 
Advantages; 
The opening parts of the document are nicely laid out with an organized flow that keeps the reader interested.The reasoning presented is solid, for the part and adds to the overall cohesion of the paper. 
The study addresses an issue by incorporating the sequencing of words, into word and meaning embeddings and suggesting an interesting solution that deserves consideration. 
Areas, for improvement; 
Unfortunately the outcomes show some inconsistencies. Do not effectively prove that the suggested models are better than current options. This is especially evident when factoring in the increased complexity. While negative results can offer insights the analysis provided is inadequate for extracting conclusions. Additionally the exclusion of findings, from the word analogy task. From just stating that the models are not competitive enough. Is significant and requires more scrutiny. 
The experimental design has some unmotivated aspects specifically in terms of how corporas and datasets were chosen and used. 
The quality of the paper declines towards the end which might leave the reader feeling a bit let down not by the outcomes but also, by how its presented and argued since they don't uphold the same level set at the beginning. 
Lets talk about topics.
The authors only partially reach their goal of developing shared word and sense representations in a dimension with the LSTMEmbed_SW model lagging behind other options consistently.There is room for improvement in explaining the reasons, behind learning these shared representations and it merits more exploration and explanation. 
The reason or logic, behind how the pre trained embeddings predict is not clearly explained in detail in the LSTMEmbed_SW model experiments.inferring whether these pre trained embeddings represent words or senses alone or a mix of both is also uncertain.also the specific configuration used in the experiments is not specified. 
Understanding the significance of acquiring sense embeddings is recognized; however their assessment lacks clarity in presentation, in many word similarity datasets where words are often considered without context which may not accurately evaluate the sense embeddings. 
The amount of data used for training is not mentioned in detail. This could affect how the results are compared; particularly when mixing various proportions of datasets such as BabelWiki and SEW together.The small size of SemCor is also worth noting as its usually seen as inadequate for training embeddings using models like, word2vec.If the suggested models work well with datasets specifically this aspect should be emphasized and assessed accordingly. 
Using test sets that're not independent, like WS353 and WSSim can make it tricky to compare results accurately and might lead to misinterpretation of data by favoring certain models and increasing their apparent success rates. 
The assertion that the suggested models can be trained quickly because of the incorporation of pre trained embeddings, in the final layer lacks concrete evidence to back it up backing up this statement with proof would enhance the credibility of the paper. 
Table 4 would provide a comparison if all models used the same dimensionalities for consistency. 
The section on measuring similarity lacks an explanation of how the multiple choice task, for identifying synonyms is addressed. 
Reference, to Table 2 is missing in the text. This could make it challenging to follow the discussions related to it. 
The word analogy tasks training process is not detailed in the context of the dataset it pertains to. 