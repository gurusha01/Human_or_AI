I have completed the task.

The task, at hand is quite significant. 
The suggested model is straightforward. Manages the top performance, on SQuAD using just one model. 
The assessment and contrast techniques are skillfully carried out.
"Areas, for improvement;"
The evaluation of mistakes and outcomes is insufficient (refer to the remarks provided below).
Lets talk about topics.
This article introduces a method for searching Wikipedia directly to respond to general questions by utilizing two main elements. A tool, for searching and retrieving pertinent Wikipedia entries and another for addressing queries using the identified articles. 
The system for finding documents uses a method of information retrieval (IR) which depends on how often terms appear and counting n word sequences called n–grams. On the hand the system for answering questions uses a way to describe paragraphs that includes word embeddings features to show if a word in a question is found in the paragraph features at the individual word level like part of speech ( POS ) and recognizing named entities ( NER ) and a soft feature to measure how similar words in the question and paragraph are, in an embedding space. The feature representation combination is used as input for a directional Long Short Term Memory (LSTM) based Recurrent Neural Network (RNN). When it comes to questions specifically​ ​a​n RNN working with word embeddings is employed​; these embeddings are then leveraged to train an overarching classifier for the beginning and end sections of sentences, in a paragraph to address inquiries​. 
The system has undergone training using open domain question answering datasets, like SQuAD and WebQuestions by adjusting the training data content with articles retrieved by the IR engine instead of solely relying on the correct document or passage itself. 
The paper piques my interest. Is quite easy to understand. However, 
The information retrieval system attains an Accuracy of over 75% at the 5 positions while the document reader shows strong individual performance by surpasssing the leading single models on SQuAD metrics.The authors note a decline, in effectiveness as depicted in Table 6.They highlight that opting for the most suitable paragraph instead of the initially retrieved outcomes raises accuracy levels from 0..26 up toc0..49.However this figure still falls short of the 0..78.079 range accomplished in the SQuAD assignment. This difference implies that the neural network may not be grasping the solutions well when utilizing the adjusted training data with retrieved articles as opposed to the document comprehension task.It is necessary to conduct an examination of this matter by presenting the training accuracy, in both scenarios and suggesting possible enhancement approaches.Although the authors touch upon this aspect in their conclusion it would be beneficial to delve into it within the paper to provide valuable perspectives. 
The authors chose to approach this as a machine comprehension task and refrained from using external sources like Freebase, for entity typing support – a decision that makes sense on one hand but could have sparked more curiosity if they had delved into utilizing such resources instead. Regarding the query raised; Is it possible that employing entity typing could have enhanced the outcomes as implied by the authors in attributing errors to closely relevant topical sentences? 
The authors should also think about mentioning QuASE (Sun et al., 2015 at WWW2015) along with similar systems in their literature review section too. QuASE is an open domain question answering system that provides answers by fetching passages, from the internet of solely relying on Wikipedia. 