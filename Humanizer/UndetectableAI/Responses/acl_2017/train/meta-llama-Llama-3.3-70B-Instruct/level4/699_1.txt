This study introduces a method, for generating phrases using an encoder decoder framework and shows through experiments that it outperforms other methods when trained with supervised data. 
Strengths include the following points; 
The way the paper is structured and explained really stands out. It makes it easy for readers to grasp the idea behind the proposed method without too much difficulty.The authors have included information so that others can replicate the experiments successfully.While using the encoder decoder framework with a copy mechanism may seem simple the results of the experiments are convincing and provide strong evidence, for the papers argument about generating missing keyphrases. 
"Areas, for improvement;"
The suggested method mentioned previously lacks originality. Fails to show significant innovation.In addition to the discussion in Section 5 point 4,the model that was trained demonstrates limited adaptability, to unfamiliar domains and underperforms compared to models that do not require supervision.The paper contributes to the upkeep of high quality training datasets but does not emphasize this aspect explicitly. 
Lets talk about a variety of topics.
The paper was really interesting. I'd be happy to see it get accepted! To make it even better though it might be helpful to look into how the size and diversity of the training data affect how well the method works. Adding the values of pg and pc into the CopyRNN model could give us a better understanding too. Especially with some examples in Figure 1, for clarity. In my experience working with CopyNet before I've noticed some behaviors from the copying mechanism which definitely needs more investigation to figure out whats causing them. 