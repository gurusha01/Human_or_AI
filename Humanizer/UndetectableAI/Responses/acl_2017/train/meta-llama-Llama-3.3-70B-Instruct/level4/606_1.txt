This study introduces a method for understanding language meaning by using a neural sequence, to sequence model called the "programmer" to interpret natural language queries and create programs based on them.. The programmer includes a memory component called 'key variable' which keeps track of entities mentioned in the queries and temporary values used while running the program to help in building the program further. Furthermore the system includes actions like finding the maximum value or moving to adjacent relationships in a knowledge base (KB). These actions are carried out by a interpreter/computer," which also keeps track of interim results. This element serves as a syntax and type validator that confirms the decoder produces elements by stipulating criteria like needing the second parameter, for the "hop" operation to be a KB predicate. The system is trained with guidance by directly focusing on the evaluation metric (known as F score). Because of the operations and reward functions that cannot be differentiated smoothly in this process; policy gradients (REINFORCE method) are utilized for optimization purposes. To address the issue of gradients obtained from using REINFORCE methodology; the study follows an iterative maximum likelihood approach to identify effective sequences of actions. The outcomes and discussions are clearly outlined in the paper; showcasing the models performance compared to other weakly supervised models using the WebQuestions dataset. 
The paper is nicely written and clear to understand; it introduces an intriguing avenue that could inspire further exploration, in the future. 
Important inquiries, for the writers; 
Has there been any consideration given to trying out a training method that utilizes bootstrapping the parameters (Î¸)? This could potentially remove the necessity for pseudo gold programs, in the beam by eliminating Line 510 from the ML process altogether. 
The paper should provide a discussion of the baseline model, in REINFORCE and the value function prediction network if it is utilized. 
Were there any tasks that involved hop actions or were they restricted to just single hops? If the usage of hops occurred in some cases it would be helpful to present an instance (while keeping in mind the word limit).
Could you provide an example that demonstrates how the filter operation is used?
I'm not sure why they used the ENT symbol instead of actual entities, in the question. 
Could you provide context or details, about the minor comments you mentioned earlier?

The decoder generates of reads, at line 318.