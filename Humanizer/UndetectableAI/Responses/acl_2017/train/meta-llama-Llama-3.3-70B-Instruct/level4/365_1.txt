After hearing from the authors perspective I have reconsidered my opinion, on the arrangement of hidden units. While it doesn't quite match my sense of logic and past encounters I am willing to entertain the idea that I might be mistaken in this specific scenario. It's important for the written work to delve into this arrangement further and perhaps incorporate a validation step to confirm its consistency across starting points. 
Considering the 10% enhancement in performance achieved by the new model while being quite different from the old one I recommend investigating the possibility of creating an ensemble model that combines both the new and old models. Should the weaknesses of these models be unique utilizing an approach might lead to a notable improvement, in performance. 
In this paper discussed an approach to standardizing historical texts that the model excels compared to current standards.The link, between attention mechanisms and multi task learning (MTL) stands out as intriguing; suggesting attention in the task can be acquired through MTL with a pronunciation task. 
The paper showcases strong points.
The compelling evidence that links attention to the temporal lobe is quite fascinating. 
The techniques used are suitable. The models show strong performance compared to the latest advancements, in the field. 
There are shortcomings as well.
The paper lacks a detail that is essential, for fully grasping the methodology used in the study. 
The models showcased are not very unique or groundbreaking; this somewhat reduces the significance of the paper. 
The main point of the paper is that attention mechanisms in text correction can be acquired through Multi Task Learning (MTL) with the additional task being pronunciation relatedâ€”a notable link between attention and MTL, in this context. 
There are two areas that could be enhanced in the paper; Firstly it lacks a clear explanation of why the pronunciation task would require an attention mechanism like that used for normalization. Though its noted that spelling variations often come from pronunciation differences the reasoning behind how MTL in both tasks would create an attention mechanism (which is actually impeded by an explicit attention mechanism) is not well defined. Offering least a speculative response, to this query would improve the paper. 
Furthermore there is some concern about clarity in the writing despite its clear nature as there are some missing details to note specifically regarding the attention mechanism description which was not thoroughly explained in the paper but rather referenced from prior research The section discussing this in section 3 point is somewhat confusing, to me. 
Further questions come to mind about how we can compare the output vectors of two models (as shown in Figure 4). Even if the output dimensions match up between them both models might have different ways of organizing their hidden states or could be permuted differently from one another which I found quite puzzling, in Figure 4. 
It could be helpful to compare the Kappa statistic between attention and MTL, against the base models Kappa statistic for a thorough analysis. 
Additionally. In conclusion of Section 5 is the ambiguity surrounding whether the row < 0.21 serves as a maximum limit, across all datasets. 
In the end of the analysis in Section 5 suggests that the methods involving attention and multi task learning have an impact on the models performance based on comparisons like Figure 5; nonetheless the real world enhancements in accuracy are only marginally better (approximately 2%) which appears somewhat perplexingly incongruous with expectations, from the data presented in the paper itself. Moreover I found the paper to be well founded and commendable; I particularly value the authors response that prompted me to elevate my rating to a 4 upon reflection. 