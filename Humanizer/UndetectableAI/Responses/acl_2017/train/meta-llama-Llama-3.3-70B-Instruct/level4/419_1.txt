The research paper introduces a yet powerful method for completing morphological paradigms in languages with limited resources using a character based seq2seq model approach.. This model is trained using a mix of samples from two languages. One with resources and another closely linked but with ample resources.. Each training sample includes paradigm characteristics and a language identification tag to aid in cross language transfer learning for languages, with characters and paradigms.. While the idea of a language solution isn't groundbreaking in itself. Similar frameworks have been studied in syntax and machine translation. This paper introduces a fresh approach by focusing on morphology instead of the usual areas like syntax and language modeling.The results from the experiments show enhancements compared to single language models and offer an, in depth examination of how language similarities influence outcomes.The paper's thought provokingly written and appears to bring valuable insights to the conference agenda. 
I appreciate your comments.
One key question is why the suggested broad multilingual approach was limited to pairs of languages of expanding to groups of related languages? For example. Including all Romance languages in the training data could potentially boost the completeness of paradigms; and combining all Slavic languages that utilize the Cyrillic script might enhance Ukrainian paradigm completion as well It would be interesting to investigate expanding the models, from bilingual to multilingual configurations. 
Using Arabic as a reference point appears unjust and insignificant due to the variations in its writing system and structure compared to the languages being studied. A persuasive approach would involve using a language that shares some similarities in alphabet but differs in typology; for instance a Slavic language using the Latin script to serve as a foundation for Romance languages. When excluding Arabic and opting for a more distant language within the same family, as a benchmark the outcomes of the experiments stay strong and reliable. 
The brief section discussing the role of Arabic as a regularizer doesn't really add much to the papers value. Consider excluding Arabic from all experiments and replacing it with a regularizer as suggested in footnote 5; this approach seems to be more beneficial, than using Arabic as a transfer language. 
The section discussing research does not cover the concept of "language universal" RNN models that learn common parameters for inputs across different languages and use language tags to distinguish between them. Some notable studies in this area are a parser (by Ammar et al., 2016) language models (by Tsvetkov et al., 2016) and machine translation (, by Johnson et al., 2016).
Could you share some feedback on this matter?
In line 144 the assertion that POS tags can be effortlessly applied across languages might not be entirely precise as transferring POS annotations presents its own share of difficulties. 
References; 
"Waleed Ammar along with George Mulcaire and Miguel Ballesteros collaborated with Chris Dyer and Noah A Smith on the paper titled 'Many languages, under a parser' published in TACL 2016." 
The study "Polyglot neural language models‚Äù, by a group of researchers including Yulia Tsvetkov explores the topic of cross lingual phonetic representation learning as presented at the NAACL 2016 conference. 
Melvin Johnson and his team, at Google introduced a multilingual neural machine translation system in their research paper titled "Enabling Zero Shot Translation " published on arxiv in 2016. 
Response, to the authors reply; 
Thanks, for getting to me; I can't wait to see the finished product!