
Advantages; 
The writers introduce a selective encoding model that expands the sequence to sequence framework for summarizing sentences in a creative way.The paper is nicely written with explained methods.Their method is tested against standards and compared to other advanced tools, with significance scores included. 
Areas of improvement; 
Some additional explanation is needed to clarify implementation specifics and draw comparisons, with other systems. 
Lets chat about topics.
**Main Assessment;** 
  The summaries created using the approach seem to focus more on extracting information rather than generating new content creatively similar to what is illustrated in Figure 4 as an example scenario. It would be beneficial to select an accurate example and include data on the word count differences between the output sentences and input sentences, across all test datasets. 
  The authors should think about whether its necessary to have both vectors hi and s since there seems to be some uncertainty, about their mathematical properties. 
  The specific neural network library, for implementation was not. There is a lack of details regarding how it was implemented. 
  The authors did not specify the training data used for each system being compared. It remains unclear if they trained any of the systems themselves. 
Just a quick check; 
  "In order to enhance clarity consider relocating the explanation of extractive summarization to the introduction section."
  Could you please provide a source for the section that talks about how effective sequence to sequence modelsre in tasks such, as neural machine translation?
  The importance of the study could be highlighted by giving focus to the work done and considering relocating the literature review section before detailing the methodology. 
  Figure 1 and Table 1 seem to cover the information unnecessarily; we could do without one of them to streamline the presentation. 
  Passages discussing the sequence to sequence machine translation model and earlier studies using this approach, for summarization tasks require referencing. 
  The paper does not provide a definition, for the term "MLD."
  The formulas, in section 3.  Sigmoid function and element wise multiplication are not explicitly outlined. 
  Several components of the equations remain undefined such, as b, W, U and V.
  The state of readout denoted as "rt " isn't shown in Figure 2. 
  The significance of the term "(ref)‚Äù, in Table 2 is not clear. 
  Model parameters, like word embedding size and GRu hidden states need explanations. 
  A citation is required for the beam search technique. 
  There are some spelling errors like "supper script," of "superscript " and a possible mistake in the actual sentence, in Figure 4. 