The article highlights strengths worth noting; its clear and organized writing style stands out prominently among them.The utilization of methods such as global optimization for end to end neural relation extraction and the direct integration of parser representation are particularly noteworthy aspects to consider.Furthermore the system put forward showcases top notch performance, on both the ACE05 and CONLL04 datasets. The authors offer insightful analyses throughout. 
However it's important to note some drawbacks well.The method seems to build upon existing techniques in a step by step manner which could constrain its effectiveness as a whole.As an added point the enhancements in performance hiking up by 1. 2 Percent, in the development dataset is rather modest. The absence of statistical tests brings up doubts regarding the reliability of these advancements. 
Regarding the conversation, at hand there are a few key points to address. To begin with it would be helpful to comprehend the impact of utilizing a developed parser and GloVe word embeddings had  had an effect impact the performance of relation extraction. Additionally it is important to clarify how the authors dealt with predictions when making predictions during that stage.
Some additional comments are needed well; The introductions portrayal of local optimization as entirely "local" is somewhat misleading since it also takes into account the connections between sequential decisions in a structural manner. Moreover It would be clearer to illustrate the points in Figures 6 and 7 using straight lines instead of curves. The depiction of entities, within the "context is also vague and needs more clarification. Some references are missing details like the publication year and page numbers for articles like Kingma et al.s work presented at ICLR, in 2014 and Li et al.s paper from 2014. 