This document talks about the topic of completing knowledge bases. Presents the ITransf model that suggests a new way of sharing parameters among different relationships. Of using a separate matrix for each relationship like the STransE model does ITransf builds a tensor D that consists of various matrices representing different relationships. It uses a vector called alpha for choosing specific relational matrices needed for forming a specific semantic relationship. The authors also explain a technique for creating sparsity, in alpha. The results, from testing on two datasets show that ITransf outperforms earlier suggestions significantly. 
The paper is nicely. The results, from the experiments are quite convincing; nonetheless the authors should consider addressing some concerns.  
Just putting together matrices in a tensor and picking one won't ensure they share information effectively between them because a better way would be to break down the tensor and map the various parts into a shared lower dimensional core tensor to promote information sharing among them all instead of keeping them separate for no apparent reason even though the goal is to share information, across all relational matrices. 
The dual goals of spreading information across relational frameworks and creating a sparse focus on vectors appear conflicting in nature. If the focus vector is genuinely sparse and contains zeros the data won't be forwarded to the relevant sections during the optimization process. 
The writers spend a lot of time talking about methods for calculating attention vectors in their research paper. While they mention that using \ell_{}{l}l​_{​​}l​​​_{}{}l₁ regularization didn't work well in their tests They don't provide any experimental evidence to back up this statement or explain why it wasn't effective. To me, as a reviewer Using \ell_{}{l}l​_{​​}l​​​_{}{}l₁ regularization seems like a starting point because its easy to optimize. Going with \ell_{}{l}l​_{​​}l​​​_{}{}l₀ regularization would lead to hard optimization challenges that could have been avoided if they had used \ell_{}{l}l​_{​​}l​​​_{}{}¹ regularization instead. 
The alpha vector. Assigns importance to the segments of dataset D which may not necessarily be accurately described as "attention " as this term is usually reserved for specific models in natural language processing, like those found in machine translation contexts. 
It's not clear why they start the optimization process with trained embeddings from TransR instead of randomly initializing the embeddings, like in TransR and then adjusting them later on. Starting with TransR as the initial point might not lead to a comparison. 
Exploring the idea of understanding connections between relationships has been a focus in similar NLP challenges like assessing relational similarities and adapting relations over time. It could prove valuable to place the research in the context of these earlier suggestions that analyze the correlation and resemblance, between different relations. 
In terms and to sum up the situation presented in the document is a fascinating strategy, for enhancing knowledge base completion; however by tackling these issues comprehensively will fortify the manuscript and offer a deeper insight into the suggested model. 