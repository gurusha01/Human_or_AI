
Advantages; 
The study adds to creating datasets to train sentence planners for generating text from information, which is an important research area, at the moment.  
Areas, for improvement; 
The uniqueness and significant progress of the paper compared to Perez Beltrachini et al.s (2016)s method for content selection are not clearly outlined without a comparison between them being provided.Critical analysis beyond the surface level is needed as the primary focus appears to be evaluating the performance of a Neural NLG (NNLG ) baseline in relation, to Wen et al.' s (2016). The higher BLEUscores mentioned in Wen et al.'spaper raise questions about whether the NNLGbasis is adequate, for making a worthwhile comparison. 
Lets talk about some topics. 
The writers need to explain the importance of this paper as an improvement over previous research work like that of Perez Beltrachini et al. And provide reasons for choosing the NNLGBaseline model over others.The ACL usually expects findings from a system that employs the dataset for a primary session paper on methodology, for corpus development compared to LREC.  
For improving the papers quality it would be helpful to analyze the sentence structures, in both datasets to make a straightforward comparison of their complexity. It's important to describe how you count the path shapes especially concerning sentence structures.  
Moreover the writers need to recognize the restrictions of their approach in including varied discourse relationships like Contrast and Background that are crucial in Natural Language Generation (NLG). It would be helpful to compare with datasets such, as those detailed by Walker and Isard for further insights. 
References; 
In a study published in the Journal of Artificial Intelligence Research (JAIR) in 2007 by Marilyn Walker et al. the focus was on adjusting sentence planning, for dialogue to suit individuals and domains. 
Amy Isards work titled "The Methodius Corpus of Rhetorical Discourse Structures and Generated Text " was presented at the Tenth Conference, on Language Resources and Evaluation ( LREC 2016) held in Portoro≈æ Slovenia in May 2016. 
Addendum provided after the authors response; 
The authors explanation offers insights that contribute to a higher overall rating increase in comparison to Perez Beltrachini et al.' The improvement made over Perez Beltrachini et al.' could be emphasized explicitly though.The primary disparity between this dataset and Wen et al.' s is centered on the selection of content which holds importance.The process of creating the data to text dataset appears to include crowd sourcing procedures with an emphasis, on highlighting any original features.  
The fact that 8.. The exclusion of 8.. 0M of texts from crowd sourcing during verification is quite significant. Examining the rejected texts could offer valuable perspectives on the datasets quality. The primary benefit involves facilitating comparisons between the two datasets, at both data and text levels.  
When it comes to comparing the NNLB baseline results between the two datasets using sophisticated approaches like those proposed by Wen et al might lessen the performance gap observed initially but its important to recognize this assumption and still consider the comparison, as a valuable aspect of evaluating the dataset.  
The datasets significance outweighs the system results when determining whether to publish in ACL journal articles; furthermore clarifying the concept of domain dependence and defining " coverage'' is crucial, in the papers ultimate edition post approval. 