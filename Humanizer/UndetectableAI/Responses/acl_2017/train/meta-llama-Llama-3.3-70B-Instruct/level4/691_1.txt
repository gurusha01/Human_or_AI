Summary; 
This paper introduces a method, for developing sense embeddings by utilizing WordNet as a lexical and semantic reference point without directly evaluating the sense vectors for significance itself but rather integrating them to create word embeddings that are subsequently tested in a task called PP attachment prediction. 
Advantages; 
The findings, on prepositional phrase attachment seem reliable. 
Areas, for improvement; 
The significance of the sense embeddings has not been thoroughly investigated yet.  
The probabilistic model has some parts that're hard to understand It's not clear whether the \lambdawi are set parameters or taught during training Also the meaning of "rank" is not clear it is uncertain whether this comes from the sense rankings in WordNet 
Previous research has discussed the idea of expressing word embeddings as a blend of sense embeddings in studies like Johansson and Nieto Pi√±as paper titled "Embedding a semantic network in a word space" (NAACL 2015) where they broke down word embeddings into sense embeddings rooted in ontology principles. This approach has also been utilized in unsupervised sense vector training efforts such as the work done by Arora et al in their study "Linear Algebraic Structure of Word Senses with Applications, to Polysemy."
I have some feedback, to share.
We can skip explaining the meanings of types and tokens since they are commonly used terms, in this field. 
The need for the \lambdawi, in equation 4 is uncertain since the probability is not standardized. 
Lets talk about this in general. 