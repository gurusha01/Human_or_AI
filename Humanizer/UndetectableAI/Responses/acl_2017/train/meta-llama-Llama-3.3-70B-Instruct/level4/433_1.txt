The research paper introduces a method using learning to analyze Singaporean English or Singlish through Universal Dependencies framework. The study applies a parsing model inspired by Dozat and Manning (2016) integrating stacking techniques from Chen et al.s work in 2016. By training a model and utilizing certain hidden features for their Singlish parser inputted data helps combine a vast English training dataset, with a smaller annotated Singlish treebank effectively. This method produces outcomes (LAS 76;57) outperforming the use of only an English parser (LAS 65;60) or relying on training a parser exclusively on the restricted Singlish data (LAS 64;01). The authors also perform an investigation to identify which typical structures gain from their methodology. 
Furthermore the article talks about. Assesses a stacked POS model inspired by Chen et al.s work in 2016.It also delves into examining typical structures within the Universal Dependencies framework and offers a treebank with annotations for 1,200 sentences.A portion of 100 sentences underwent annotation, by two people leading to a rater agreement of 85. 3. 75. 7 LAS. 
"The paper excels, in the following aspects;"
The authors have shown outcomes and their experimental arrangement seems to be well thought out. 
They conduct analyses to investigate how different factors affect their model. 
Having a Singlish treebank annotated following the Universal Dependencies, v ersion  of guidelines is a significant addition. 
The suggested principles, for examining structures of Singlish in Universal Dependencies are logical and backed by good reasons. 
The approach considers language aspects. Effectively leverages the commonalities, between formal English and Singlish dialect. 
The paper discusses a known language and introduces an approach that could be adapted for use with similar language pairings, in the future. 
"The treebanks method, for selecting sentences is thoroughly justified." 
The article is nicely. Straightforward to understand. 
However the flaws consist of; 
The quality of the annotations seems not quite up to par as there is only a 75..72 % agreement on LAS between annotators which raises doubts about how accurate the LAS estimate from the model is.. However the authors’ response effectively deals with this problem by showing that the low agreement was mainly due to the annotator straying from the annotation guidelines.. Once this problem was fixed the agreement, between annotators improved to a level..
Overall it seems that despite some worries about the quality of annotations in the paper; it has been positively received and its contributions are considered valuable, for the conference. 
Authors are asked the questions; 
Who was responsible, for adding notes to the sentences. How did they go about it? 
What were the main reasons for discrepancies in the agreement, among annotators and how were these conflicts resolved? 
The noticeable prevalence of discourse connections in the treebank raises questions; is this a feature of speech or a reflection of the guidelines, for annotation? 
The categorization of elements and external terms, in Table A2 might benefit from further explanation by using distinct tables and explanations. 
Low level remarks indicate; 
Comparing the method proposed by Martinez and colleagues, in 2017 could be intriguing. Provide valuable insights. 
One way to enhance clarity is by substituting the term "grammar”, with " structures.”
The analysis presented in Figure 2 seems solid; however perhaps we could do without mentioning "it extraposition."
The model developed by Doxat and Manning in 2016 is no longer considered cutting edge technology. A term, like "highly effective model" might be more suitable now. 
Adding explanations, in Figure 2 would improve how easy it is to read and comprehend the analysis. 