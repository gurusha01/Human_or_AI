The article introduces an easy to understand advancement in the field of interactive topic modeling by allowing human annotators to offer various "anchor words" for topics generated by machines​ The paper is well organized and benefits, from the inclusion of both simulated and real world experiments​
The papers focus is limited when it comes to comparing interactive topic model methods.The authors mention existing approaches and explain why they are not included because of speed or compatibility issues, with interaction; however having actual proof to back up these assertions would be helpful.  
Furthermore it would be beneficial to try out a variety of datasets apart, from the 20 newsgroups dataset, which has been heavily used already. 
In terms and with new insights, into the field of interactive topic modeling are provided by this papers research contribution.The authors have extensively assessed method variations using simulated tests and have carried out detailed quantitative evaluations of both simulated and user experiments by employing various measures to evaluate topic quality from diverse viewpoints. 