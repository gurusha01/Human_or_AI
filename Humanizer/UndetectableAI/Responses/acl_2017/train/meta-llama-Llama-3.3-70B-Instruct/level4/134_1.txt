
Advantages; 
The document is nicely. Clearly outlined for easy understanding The methods and findings are fascinating and show a comprehensive research approach. 
Areas that could be improved upon; 
However the assessment and outcomes might be affected by issues that are discussed further below. 
Lets talk about topics. 
This paper presents an argumentation mining system that uses neural networks and addresses the issue using two different methods. Sequence labeling and dependency parsing techniques are employed for this purpose. Additionally the authors investigate a multitask learning scenario for the sequence labeling technique. Offer a detailed rationale behind the models development. In contrast to approaches that depend on Integer Linear Programming (ILR) manual feature creation processes and manual ILR constraint formation steps; the new model removes the necessity, for manual input efforts. The model learns aspects of argumentation mining together to prevent mistakes from spreading through the system like in traditional methods, with separate steps involved in the process.The explanation of the methods is clear overall even though there are a missing details mentioned in the description. 
The research setup is robust and thorough with executed comparisons leading to significant findings Nonetheless a key issue arises from the small dataset size and the extensive capacity of the utilized (Bi)LSTM based recurrent neural networks (BLRC and BLCC). The training dataset comprises around 320 essays while the testing set consists of 80 essays without any mention of the development set size, in both the main text and additional materials This lack of clarity regarding training data adequacy poses a crucial concern. In contrast to tasks where there are usually hundreds of thousands or even millions of tags involved in sequence labeling tasks traditionally require dealing with only a limited number of tags during training. Probably just a couple of thousand at most. Which raises concerns about the adequacy of model parameter training efforts For example the manuscript lacks an examination of overfitting issues and it could be advantageous to incorporate measures such as tracking "loss" values for training and development stages, throughout the training process. The writers present some proof that could suggest overfitting. In Line 622 they mention that taggers are local models and therefore require less training data and are less susceptible, to overfitting. 
Because of these worries about the reliability of the models being used is also in doubt well; in order to tackle this issue head on and ensure accuracy and reliability in our findings and results, across different scenarios with various starting parameters should be measured through their mean and standard deviation analysis. Conducting tests of significance would offer further clarity into the consistency of the models and the trustworthiness of the outcomes yielded from them. Without such examinations being performed it becomes quite a challenge to ascertain whether any enhancements seen are truly a result of the proposed methods superiority or merely a stroke of luck. 
While neural networks used for modeling regularization techniques to some extent; the limited dataset size calls for a closer focus on regularization methods in practice, than what is currently discussed in the manuscript and supplementary material concerning LSTM errors." This aspect should be thoroughly. Addressed within the manuscript. 
Of the hyper parameter optimization technique mentioned in the additional materials currently being used you might want to look into utilizing Bayesian optimization approaches instead. 
I suggest transferring the details regarding trained word embeddings and error analysis from the supplementary material to the main manuscript; allocating an extra page should be adequate, for this addition. 
Adding in the agreement scores among annotators would offer insights into how well the systems are performing and where there is room for enhancement with key details available, in the paper outlining the dataset. 
Consider adding colors to enhance the visual appeal of Figure 1 when printing it in black and white. 
I'm ready to assist you. Just let me know how I can help!
