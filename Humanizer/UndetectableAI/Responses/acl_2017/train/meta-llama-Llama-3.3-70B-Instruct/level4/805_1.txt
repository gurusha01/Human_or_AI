The entry shows strong points such as the innovative CORE assessment metric and the precise similarity measure suggested as well as the thorough assessment across a wide array of datasets, with varying characteristics. 
Nonetheless there are an areas that require attention. To begin with the document includes spelling errors that ought to be rectified, such as; 
The text detector looks at heuristics and perplexity to decide if your text was created by a machine learning model or written by a human. It examines the distribution of parts of speech and typical markers of AI generated text.
Change the mention of "figure 2" to "figure 1" in line 176 to 177.
The phrase "within the ranks" shows a duplication and should be adjusted to "among the top.”
In the article it would be helpful to mention "figure 4' earlier to provide context. 
Line 434 should be amended to read "the dataset contains," as it is grammatically correct. 
Please correct "table 3" to "table 1”, for consistency. 
Furthermore​s​ it is important to substitute all mentions of "Tensorflow​s​, with "TextFlow​s​ for representation. 
Moreoverm there are inaccuracies that need to be addressed.  
The paper should provide an explanation of the precision in computing features like lemma and part of speech (POS) or WordNet synset accuracy and its potential impact, on assessing overall similarity accuracy. 
Even though the neural networks are coded in Python language the experiments code availability, for replication is not specified.  
The text talks about sharing training and evaluation sets. Does not explain how exactly they are shared. Like whether its done upon request or under a license. Which is important, for replicating the experiment accurately. 
During the conversation or discussion, about this topic area it's important to keep in mind these aspects to improve how clear, repeatable and high quality the content is. 