This study introduces a model for evaluating word embeddings by considering their ability to efficiently use data and perform well in supervised tasks. The main reason for this strategy is that word embeddings are commonly used in transfer learning scenarios where their performance is assessed based o the time it takes to train a desired model. To measure their effectiveness the suggested approach includes simple tasks, like word similarity and word analogy tests that are evaluated in a supervised way. In experiments using types of embeddings it was found that rankings differ based on the specific task and the quantity of training data used. 
Areas of expertise
The idea of drawing inspiration from transfer learning and maximizing data usage is quite fascinating since it relates directly to the practice of utilizing embeddings, for supervised learning purposes. 
Areas, for improvement
A thorough assessment process should have an impact on tasks that follow it in sequence.For instance the suggested strategy must produce a ranking for a given set of embeddings across various tasks like text categorization,parsing and machine language translation.It's difficult to have confidence, in the effectiveness of the method when this factor is not considered and compared with approaches. 
The talk about embeddings seems disconnected, from the main points of the paper and doesn't really add to its comprehension. 
The experimental part is confusing, to me— Section 3 ⸻ it talks about how studying whether it’s beneficial to use syntax specific embeddings with extensive supervised data raises questions but doesn't give a clear answer. 
In addition to that in Section 3 point 6 of the paper mentions that the results indicate that solely relying on extensive pretraining might not be ideal for NLP tasks. A daring claim lacking strong backing, from the assessment strategy put forward. 
The evaluations validity is restricted due to the trained embeddings being acquired without control, over the training data sources used. 
The document needs a review for errors in citing figures; for example referencing Figure 1 on page 6 when its actually, on page 3. 
Lets talk about topics and have a casual chat.
The paper starts with a motivation but falls short in properly assessing the efficiency of its approach as mentioned earlier on any internal evaluation method should ideally be supported by a study exploring whether its findings apply to practical tasks in this paper that is not carried out The absence of clearness and the necessity for proofreading in the document also impede understanding To elevate the paper upcoming research could gain from external investigations and a more regulated experimental structure like training all embeddings, on identical corpora. As it stands now though the paper doesn't add value to the conference. 