The researchers in this research project enhance the MS COCO dataset by adding a caption to every current one with just one varying word each time. They show that performing procedures for visual question answering and captioning face notable challenges, in detecting false captions and choosing an appropriate substitute while pinpointing the error term.  
This study adds to the research that sheds light on the subpar performance of vision language models and prompts important inquiries into their actual capabilities in handling intricate tasks effectively or otherwise discussed by the authors across different tasks and models with in depth insights into the reasons, behind model shortcomings as depicted in Figure 3. 
One main thing I'm worried about is how close this work is to what Ding and others did in their research work. The authors present a case by demonstrating that even when using a simpler method where captions vary by just one word the models experience significant failures. This key difference highlights that the complex techniques employed by Ding and others are not required to reveal the flaws, in these models. 
One more thing to consider is how NeuralTalk is utilized to pick the difficult foils; while this approach is new and inventive it might create a self reinforcing bias that could lead to embedding NeuralTalks biases, within FOIL COCOL
The section, on results could use a bit detail to match the depth found in other parts of the paper – adding extra paragraphs might offer a better understanding of the discoveries made. 
In my opinion this paper makes an addition to the field by expanding on earlier findings that point out shortcomings in combining vision and language. The resemblance, to Ding et al.' s study doesn't diminish its importance. Actually shows how vision language models can be deceived quite easily. 
A few small ideas to consider are adding a model that combines bag of word features, with extracted CNN features and trains a softmax classifier, which could enhance the credibility of the papers argument even more. 
A few technical clarifications are required well; for example defining a supercategory and its source—whether from WordNet or COCO is essential to understand fully Moreover making minor tweaks in wording and correcting typos like changing "has been" to "were " "that" to "than," replacing "artefact”, with "undesirable artifacts " and also changing "ariplane" to "airplane " would enhance the quality of the manuscript significantly.`
Adding a model to Table 1 could offer a more defined reference point, for comparison and help clarify any uncertainty regarding the constant prediction baseline. 
Upon examining the authors reply I'm satisfied to note that my worries about NeuralTalk biases and the necessity, for more baseline data have been taken care of. I believe these concerns can be sorted out in the draft so I will stick with my original rating. 