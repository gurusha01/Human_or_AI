This document presents a gated attention system tailored for machine comprehension which extends the Attention Sum Reader (Kadelec et al. 2016) through the incorporation of a detailed gated filter to support multi level reasoning processes effectively.The idea is intriguing. Makes sense in the realm of machine comprehension.Despite its nature and notable enhancements on standard datasets a few significant issues must be resolved before it can be deemed suitable for publication, in ACL. 
The Gated Attention (GA) mechanism looks promising. Doesn't quite prove its superiority over other advanced systems convincingly enough. One reason for this is that the engineering techniques discussed in section 3 also significantly enhance accuracy levels, which complicates determining the effectiveness of the GA method, among the mixed outcomes obtained. 
Additionally the list of sources is not fully complete as most of the works referenced are based on arxiv preprints. This lack of completeness may raise doubts regarding the depth of the analysis in relation, to research particularly for those reading and reviewing the material. It's crucial to replace these references with the versions that have been officially published when possible. 
The authors don't need to include results from work like the GA baseline in tables 1 and 3 since its mentioned as a preprint; they could use a vanilla GA or a variant of the proposed model, for better comparison instead. 
There seems to be a discrepancy between the data provided in tables 2 and 5A that catches the eye away! In table 2 we see GA  which appears to correspond to K being equal to 7 in the AS Reader table from table 5A but then GA align with K being equal to 8 of what was mentioned before which is quite puzzling when it comes to figuring out if GA  is indeed a revamped version of the AS Reader and if it is why there are inconsistencies, in the use of GloVe initialization and token attention between GA  and K equals to 8 in the AS Readers table that are left unaddressed. 
It would be helpful to provide an analysis of the new approach compared to previous studies in the relevant section of related works as it can shed light on the unique aspects of the current research in contrast, with established findings. 
Figure 2 does a job showcasing how gated attention enhances multi hop architecture translation in an impressive way! To enhance the manuscript further and provide a view of its effectiveness mechanisms qualitatively speaking including comparative examples would be beneficial. 