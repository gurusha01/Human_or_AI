Lets evaluate.
Areas of proficiency; 
This research paper introduces a method for measuring the effectiveness of conversational responses in dialogue systems that are not task oriented. The innovative utilizes continuous vector space representations created by RNNs and is divided into two parts. One that assesses the similarity between the context and response provided and another that evaluates how closely the given response aligns with a reference one. To make these comparisons accurately the responses are projected onto the context and reference response spaces using dot product calculations; the projection matrices are trained by minimizing errors, between model predictions and human feedback. 
The study marks an advancement in assessing dialogue systems that are not task oriented by going beyond mere semantic similarity through the acquisition of projection matrices that convert the response vector into representations in context and reference spaces The authors’ method is quite refined; it would be intriguing to investigate the differences, between the M and N projection matrices post training compared to their initial identity initialization Exploring this aspect further would add more depth to the paper instead of just concentrating on the correlations that result. 
Areas of improvement; 
The paper brings up queries about how things were put into practice. For example; It's not clear if the scores from humans for training and evaluation were from one AMT annotation or the average of annotations. Also; There's a lack of clarity, in explaining how the dataset was divided into train/dev/test and the use of n cross validation. The way they show the correlation results for the scores related to the Adverse Drug Event Model (ADEM) in Table 2 seems a bit confusing to me too. I don't get why they use validation and test sets specifically for the ADEM scores while other scores are talked about in terms of the dataset and test set altogether. I found the part about pre training, with VHRED a bit to follow as well. It could definitely benefit from a clearer explanation of how pre training works and why its advantageous. 
Lets talk about some topics.
The claim that "There are instances where these metrics fall short because they frequently overlook the semantic resemblance, between replies" is misleading; the problem doesn’t lie in semantic similarity but in the fact that various semantic hints may form contextually valid responses instead. Thus relying upon semantic similarity proves inadequate when assessing dialogue system responses; the suggested M and N matrices aim to tackle this constraint effectively. 
The reference, to the Turing test is not entirely accurate since the original purpose of the Turing test was to establish what constitutes behavior rather than judge the quality of dialogue responses automatically assessing dialogue responses doesn't equate to conducting an automatic Turing test so the title "Towards an Automatic Turing Test " is somewhat deceptive. 
The idea that a chatbot is considered "good" when its responses are rated as appropriate by evaluators is a valid perspective to address the issue of non task oriented conversational systems. It may be helpful to mention research, like the workshops organized under the banner of the World Open Domain Chatbot Challenge (WOCHAT) to give additional background information in this regard. Lastly in the discussion segment there was an error spotted; "and has has been used" should be rectified to read as "and it has been used”.