This research paper introduces an assistant that uses the REINFORCE algorithm to improve both the belief tracker and dialogue manager by engaging with a user simulator in a two step training process. In the phase of training; the system learns through imitation with initial guidance, from a rule based model using supervised learning. In the phase; it refines its performance further through reinforcement learning to enhance the systems capabilities based on RL objectives. 
The key advantages of this paper are found in its introduction of a framework that incorporates the ability to access the knowledge base (KB ) in a manner during the collaborative optimization process—a notable accomplishment, in the research. 
Nevertheless there are some drawbacks pointed out too.As a starting point the system is not viewed as end to end because the method of producing responses is created manually rather than learned.Moreover the end to end model shows signs of overfitting to the simulator which leads to subpar results in assessment.This difference prompts inquiries about the authors’ focus on end, to end learning compared to the importance of accessing KB... The improvements from utilizing soft KB access are more evident compared to the advantages of end to end learning as per the authors’ presentation in Figure 5 lacks persuasiveness in showcasing the benefits of end to end learning strategy adoption; moreover opting for the REINFORCE algorithm without justification due, to its high variance problems while overlooking possible enhancements through using a baseline or exploring the natural actor critic algorithm alternatives. 
Overall and despite its flaws that were mentioned on; the experimental setup is solid which makes this paper acceptable in quality. However if the writers shift their focus in the paper towards the aspect that actually improves performance. The KB access. Rather than solely concentrating on the end, to end learning concept; it would greatly enhance the manuscript. 