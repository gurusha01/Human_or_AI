This document presents a collection of annotated essay edits along with two practical examples – studying how students edit their work and automatically recognizing revisions through text classification using a support vector machine (svm). The authors aim to share the corpus, for research purposes. 
The paper is impressive due to its presentation and the thorough annotation method used by two annotators that greatly boosts the corpuss significance for researchers interested, in writing process research and related fields. 
Nonetheless there are two critiques that must be tackled. The initial one could be resolved with modifications, in the event of paper acceptance; however the latter demands more extensive efforts.
One crucial point missing in the paper is the absence of information regarding the corpus content. It would be beneficial to include details like the total number of documents (probably 180 based on 60 essays with 3 iterations each) the token count (estimated at around 400 words per essay) and the sentence count. If we assume there are 60 essays with an average length of roughly 400 words for each one the total word tally would amount to approximately 24 000 words. Considering all three drafts would likely lead to a word count of about 72 000 words although there may be some significant duplication, within this figure. Adding a table showcasing these statistics would improve the paper. 
If the estimates mentioned earlier are right as per what you've said before with regard to the corpus size being somewhat limited in scale when compared to others that have been discussed in contexts; it's important to acknowledge the effort involved in creating hand labeled data which is definitely a strong point of this studys methodology. However the wider usefulness of this data for the field of Natural Language Processing (NLP) as a whole raises some doubts. It may be more fitting to introduce such a resource at an event like BEA or a conference that specifically focuses on language resources like LREC rather than at a general NLP conference such, as ACL. The authors’ decision to enhance the collection by adding notes makes one wonder if they are also considering including extra essays, in it. 
Minor feedback and recommendations consist of; 
Both native and non native speaker essays, in this collection could also be used to identify a persons language, known as native language identification ( NLI).
On page 7 of the document,"word unigram" would provide clarity than just using "unigram feature".
On page 7 the use of the phrase "and the SVM classifier was utilized as the classifier" seems unnecessary. Could be left out. 