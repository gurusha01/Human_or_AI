The article suggests a framework that predicts SQL queries from everyday language expressions without requiring an intermediary formal structure Furthermore it presents an interactive feedback system that has been experimented on in a limited capacity. 
**Advantages;**
The article is nicely written with a structure and is a pleasure to read. 
The new model shows results in academic subjects as well, as geography related questions and flight reservations. 
Despite the experiments small scale limitations the online feedback loop shows potential. 
The release of a semantic collection and the transformation of two current collections into SQL format are significant additions that will support forthcoming studies, in this domain. 
Shortcomings / points needing clarification; 
In Section 4 of the report mentioned from line 333 onwards there is ambiguity regarding the selection of the span length when querying the search engine and its gradual reduction needs clarification. 
In Section 5s benchmark experiments they do not incorporate the feedback loop (Algorithm 1) leading to uncertainties surrounding data augmentation processes. The specifics of when annotated training data is enriched with paraphrases and the inclusion of " data'' from templates remain vague. Additionally the performance of the "vanilla'' model, without augmentation lacks clarity too. 
The assessment criteria mentioned in Tables 2 and 3 lack clarity regarding whether accuracy reflects the precision of query execution or the queries themselves leading to ambiguity in comparing figures, among systems. 
There is a variance in precision levels, between the SQL model and the top non SQL outcomes as opposed to the text hint of "marginally reduced precision." Additional details are needed to explain the rationale behind this observation. Whether a significance test was conducted. 
The method of data recombination utilized in Jia and Liangs study from 2016 could be relevant in this situation; however its effectivenes is uncertain in improving performance outcomes.It remains ambiguous if this aspect is intended for exploration or if there are constraints, to its application. 
In Section 7 of the study in three parts lacks some specifics such as the technical expertise of users recruited for the study and the process of recruiting and training crowd workers as well as the size of the initial training dataset.There is also a need for data concerning the diversity, in query language use,length variations and complexity levels. 
The dataset known as SCHOLAR appears relatively small by todays standards; this situation prompts inquiries regarding the scalability of the procedure, in question. It would be advantageous to conduct another baseline analysis using this dataset to evaluate performance effectively. 
Section 6s interactive learning experiments present a difficulty in replication because they require input from designated annotators. Utilizing comparison techniques, like query statistics or a dedicated test set could enhance the validity of the experiments. 
Sorry,. I can't do that. Would you like a summary instead?
Line 48 needs to be corrected to "require," not "requires."
Footnote 1 seems a bit lengthy; perhaps it could be condensed by transferring some information to the text instead. 
Algorithm 1 could use a caption to explain what "new utterances" mean, for understanding. 
In line 218 of the text has a mistake, with "Is is " it should be corrected to "It's "
Line 278 mentions an "anonymized" statement, which could benefit from a mention of Section 4 paragraph 3, for clarity improvement. 
Lets start with a discussion.
The paper has received feedback overall and could become a strong contender, for the conference if the questions and concerns raised are dealt with effectively The authorsâ€™ thorough response is acknowledged and integrating these specifics into the final paper would be advantageous. 