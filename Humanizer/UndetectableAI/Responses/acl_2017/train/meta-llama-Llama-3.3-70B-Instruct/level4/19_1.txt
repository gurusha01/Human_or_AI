This article introduces a method to improve the performance of resolving zero pronouns. It highlights contributions such, as;  
A simple approach to creating a training dataset, for tasks related to resolving zero pronouns automatically. 
Step 1; Two step learning involves transferring information from extensive datasets to targeted domain data. 
The paper is well organized. The experiments are carefully planned out to include a strategy, for handling unfamiliar words using unique tags. 
Numerous inquiries emerge when it comes to identifying the antecedents of zero pronouns. 
How is the antecedent of a pronoun determined in situations where the head word of a noun phrase's not a pronoun, in the proposed method? 
What occurs if the word anticipated is a noun that has not been mentioned in the preceding context? 
Considering the systems strong performance, on datasets is prompting an evaluation of its efficacy in a two step approach;  
Judging the models capacity to anticipate and restore omitted zero pronouns. 
Secondly assessing how well it can recognize previous references. 
Can you provide details about why attention based neural networks were chosen? A short explanation of the reasoning behind this decision could offer insights, for other researchers. 
One small recommendation would be to use labels like s01 and s02, in Figure 02 of d01 and d02. 
This paper is excellent as it presents concepts and a strong experimental framework that adds significant value to the field. 