This document presents a neural network structure designed for understanding natural language and textual inference tasks using a three step approach involving encoding information processing mechanisms like attention driven matching and aggregation techniques. Two different versions of the model are described in the paper. One that utilizes Tree Recursive Neural Networks (TreeRNNs) and another that employs Bidirectional Long Short Term Memory networks (Bi LSTMs). Impressively the sequential model outperforms all reported findings and combining the tree based model with the sequential model results, in even higher performance. 
The document is nicely organized with a thought out model and impressive findings presented within it. Though the advancements may seem modest in nature overall I suggest accepting it based upon the robustness of the research, as a whole. 
There are a few points that we should talk about; 
The suggestion that the new system could set a standard for upcoming NLI studies may hold some validity; however it lacks uniqueness or significance since this claim could apply to almost any model designed for any task at hand.A convincing point of view could highlight the models straightforwardness or refinement; nevertheless the proposed structure does not seem to prioritize this aspect, over others. 
The way the model is designed with both backward attention across sentences and separate networks for inference, in each direction seems a bit repetitive and could almost double the time it takes to run the model. Since NLI tasks are naturally one sided in nature it's not clear if this added complexity is really needed. It might be helpful for the authors to do some tests to explore this aspect further. 
The outcomes for the sequential model (ESIM) as well as the combination of ESIM with the tree based model (HEM) are showcased; however the outcomes, for the standalone tree based model are omitted. Incorporating these results would provide a thorough insight into the contributions of each model. 
Minor. Ideas, for enhancement to consider; 
The statement by Barker and Jacobson might not completely align with the intended argument since it relates to compositionality, in formal grammar; a broader mention of the principle of compositionality could be more fitting. 
Although the concept of utilizing vector variance as a feature is not new and may introduce parameters in certain models that utilize vectors a and b for matrix multiplication alongside their difference (a. B) an alternative model could achieve similar results with just a and b by adjusting matrix parameters accordingly。While there might be valid reasons for this approach from a learning perspective，further discussion, on its implications is necessary. 
It would be helpful to have information, about how the tree shaped elementsre put into practice and if there are any concerns regarding speed or scalability to consider. 
There's a typo in the citation (referencing Klein and D Manning, from 2003).
Figure 3 could be enhanced by utilizing used tree drawing tools such, as tikx qtree to create parse trees that are easier to read and don't have lines crossing over each other. 
After reviewing the authors’ feedback, on the manuscript I continue to endorse its publication with confidence. Though the research might not be groundbreaking it presents aspects and unexpected findings that bring significance to the conference validating its acceptance. 