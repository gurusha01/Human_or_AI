Review of the Document
Summary of the research article;   
The paper discusses the issue of assessing dialogue responses in non task oriented systems noting that conventional metrics such, as BLEU and ROUGE do not align closely with human evaluations. To address this challenge the authors introduce ADEM (Automatic Dialogue Evaluation Model) a machine learning based framework that estimates how like dialogue responses are. ADEM is developed using a dataset of appropriateness scores annotated by humans. Utilizes a hierarchical recurrent neural network (RNN) encoder pre trained with the VHRED model. The model shows enhancements in alignment with human assessments at both the individual expression and system tiers while surpass the standard word match metrics used traditionally. Moreover ADEM performs across unfamiliar conversational models indicating a positive advancement, towards scalable and precise automatic assessment of dialogue systems. 
Key Contributions;   
The paper presents the ADEM Framework, for Automatic Evaluation which introduces an assessment model focusing on semantic similarity by taking into account dialogue context and reference responses – going beyond simple word overlap metrics to better handle response variations and contextual dependencies.   
  
The research paper presents experimental findings indicating that ADEM demonstrates significantly stronger correlations with human assessments, at both the individual statement and system levels when compared to current metrics.   
One of the aspects is strengths.  
A new method of assessment, ADEM creatively utilizes RNN technology and prior training with VHRED to understand context and semantic connections effectively which is both inventive and well supported in its approach.It tackles drawbacks seen in current metrics such, as BLEUs and ROUGE.   
The authors present a range of real world test results, in their research paper that involve comparing with various standard methods and assessing performance across different systems and scenarios to demonstrate ADEMs strong effectiveness as indicated by a high Pearson correlation score of 0 954 at the system level.   
The practicality of ADEM lies in its capability to apply to models swiftly and its efficient evaluation process – qualities that render it valuable, for real life scenarios where constant human assessments are not viable.   
The authors plan to make the ADEM implementation available as source after its publication, which will support reproducibility and stimulate additional research, in this field.   
Areas, for improvement;   
In score predictions for ADEM model tend to be more conservative and closer, to the scores according to the qualitative analysis findings mentioned in the studys results section which could potentially limit its accuracy in distinguishing between highly suitable and unsuitable responses effectively.   
The model might lean towards answers since people tend to consider them quite suitable, in assessments.This inclination could impede the assessment of dialogue systems that're more innovative or varied.   
Limited Domain Exploration Issue Addressed The researchers mainly focus their experiments using the Twitter Corpus but note that this approach has limitations in terms of generalizability due to a lack of evaluation using datasets, like task oriented or domain specific dialogues.   
Queries, for Writers;   
How well does ADEM fare when assessing answers, in task focused dialog systems or datasets to particular domains?   
Could the model improve its accuracy in predicting scores by considering an approach in calculating errors that doesn't heavily penalize deviations, from extreme values?   
Have you thought about using training to tackle the tendency, for generic responses?   
In closing;   
This paper introduces a progress in assessing dialogue systems automatically by providing a scalable and precise substitute for conventional metrics despite some constraints, like cautious predictions and potential biases. The advantages of ADEM surpass its drawbacks with executed work supported by solid empirical evidence and practical implications that enrich the field. I suggest accepting it with revisions to tackle the noted weaknesses effectively. 