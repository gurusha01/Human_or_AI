This study introduces a technique using a gated attention based neural network for reading comprehension and answering questions more effectively by including a self matching attention mechanism that improves contextual understanding when analyzing passages compared to traditional approaches. Furthermore​. the researchers employ pointer networks guided by cues from the question focused vector​​​​​to anticipate the beginning and end of the answer. Results, from experiments conducted on the SQuAD dataset reveal performance compared with various recent techniques​. 
The document is nicely crafted with a structure and thorough explanation provided throughout it.In my opinion the mathematical equations seem to be well founded.I see this as an addition that could hold significant worth, for scholars in the field of question answering research. 
"I'm interested to know whether the writers intend to make the code available for this method in the publication? I believe it would be beneficial for the paper to include explanations about the technologies used for implementation such, as Theano and CUDA as it could assist readers." 
It would be beneficial for the authors to also perform a statistical significance analysis, on the outcomes to enhance the credibility and reliability of their discoveries. 
Finally I get that space constraints can be an issue but adding an assessment, on another dataset would really boost the credibility of the method being proposed. 