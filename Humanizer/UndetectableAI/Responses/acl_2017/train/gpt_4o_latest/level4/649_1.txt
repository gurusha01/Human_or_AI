Strengths include;   
This study presents a measurement tool created to evaluate the effectiveness of conversation replies in chat systems that are not task driven in nature.The new tool utilizes vector representations generated by RNNs and comprises two main aspects; one that assesses the connection, between the context and the provided reply and another that contrasts the reply with a model response.These evaluations involve utilizing dot products after mapping the reply onto the context and reference response areas. The projection matrices are taught by reducing the difference, between the models forecasts and annotations provided by people. 
This research marks a progress in assessing dialogue systems that are not task oriented in nature. In contrast to methods in this field that mainly emphasized semantic resemblance the authors introduce a novel strategy by training projection matrices to align the response vector with context and reference space representations. This method surpasses semantic similarity in a refined way. I am especially curious about how the projection matrices M and N change from their identity state, as training progresses. To improve the papers worth significantly focus more on discussing this aspect in detail than placing too much emphasis on the resulting correlations. 
"Areas, for improvement include;"  
The document lacks clarity, on aspects of implementation details. For instance it does not specify whether the human ratings utilized for training and evaluation were based on AMT annotations or averaged across several annotations. Moreover there is no explanation of how the dataset was divided into train/dev/test sets or whether n fold cross validation was carried out. In addition to the data in Table 2 being a bit confusing with how it shows correlations for the ADEM related scores compared to scores that are displayed differently between the validation and test sets versus the full dataset and test set sections is unclear at first glance. Additionally found that the part about pre training using VHRED is a bit over the place and hard to grasp. A straightforward explanation of why pre training is beneficial along, with less jargon would likely make it easier to understand. 
Lets talk about something.  
"There are instances where these metrics fall short since they frequently overlook the semantic connection between replies (refer to Figure 1)." It's essential to tread with such assertions as the problem doesn't stem from semantic similarity per se; rather it's, about the complexity where contextually appropriate responses can stem from entirely divergent semantic hints. Henceforth mere semantic similarity doesn't suffice in assessing responses of dialogue systems. Assessment should go beyond the meaning—it's exactly what your M and N matrices are working toward accomplishing. 
An accurate system capable of assessing the quality of responses automatically—essentially an automated version of the Turing Test—originally aimed to gauge and define intelligent actions by determining if a machine can replicate human behavior to a degree where an average person cannot differentiate between the machines answers and those of a human being. The theme is somewhat connected to how dialogue systems perform; however it's not entirely correct to compare assessing the quality of dialogue responses automatically with conducting a Turing Test automatically as stated in the paper titled "Towards an Automatic Turing Test." This title might be slightly misleading. 
The idea that defines a chatbot as 'good' is when its responses are rated highly for appropriateness by evaluators is a valid simplifying assumption to consider when discussing non task oriented dialogue systems instead of relying solely on the Turing Test concept for evaluation purposes. If you're interested in learning more about this subject matter and exploring insights into it you might find valuable information, in the research presented at the WOCHAT workshop series including task descriptions and annotation guidelines that have been shared within that context. 
In the discussion section the phrase "and has has been used " should be revised to ". It has been used."