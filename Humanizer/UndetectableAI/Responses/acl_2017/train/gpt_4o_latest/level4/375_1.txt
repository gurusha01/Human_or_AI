This paper addresses the issue of network embedding by introducing a network model that combines the network structure with text associated with nodes and incorporates an attention mechanism to modify the textual representation using the text from neighboring nodes. 
One of the advantages is; 
The suggested model cleverly merges network layout. Written details to create hidden representations and the mutual attention process seems like a sensible strategy. 
The report offers an assessment by using various datasets and benchmark techniques across different evaluation tasks. 
Shortcomings; 
In the realm of "network embedding " like scholarly articles using neural network methods influenced by word embeddings to create hidden representations of network nodes do not consider earlier studies on statistical and probabilistic modeling of networks. It is essential for all "network embedding" research to start by acknowledging and evaluating works such as the latent space model by Peter Hoff et al. in addition to later advancements, in statistical and probabilistic machine learning literature; 
Authors P.D.Hoff,A.E.Rafter. M.s.handcock published a paper on latent space methods for studying social networks, in the Journal of the American Statistical Association in 2002. 
Early, as 2002 a model called latent space was created to embed nodes into a lower dimensional latent space before neural network based network embeddings came into existence. 
The paper aims to analyze the roles played by individuals, in social networks and should also highlight and contrast its methodology with the mixed membership stochastic blockmodel (MMBS).
In a research paper, by Airoldli and colleagues published in the Journal of Machine Learning Research in 2008 discusses mixed membership stochastic blockmodels. 
The MMS Bayesian framework allows nodes to take on roles" in a probabilistic manner when establishing connections. 
Lets chat about topics.
The statistical models discussed earlier do not include text data. Use scalable neural networks with methods like negative sampling but are based more solidly in generative modeling principles rather than heuristic neural network goals However recent advancements, in these models have enhanced scalability and integrated textual data. 
Is there a variation in performance between CENE and CAN in the third illustration provided here in Figure 3 and were the trials repeated with various random divisions, for training and testing purposes? 
In Section 5 point 4 of your document where you mention the hyperparameter grid searches. Were these conducted using the test set (which could present issues) a validation set or the training set? 