The article discusses a strategy for improving NLP tasks by using embeddings generated from language models. Despite the success of context word representations in the past this study presents an interesting advancement by employing context dependent word representations obtained from neural language models hidden states. The researchers showcase enhancements in tagging and chunking tasks by integrating embeddings from extensive language models. Moreover the paper provides analyses that tackle various common queries, in the field. 
I find this paper to be quite solid; however I do have some suggestions, for how it could be enhanced.
I recommend updating Tables 5 and 6 to show results based on the development data than relying heavily on test set experiments. 
We should try out the method on a variety of tasks for a better evaluations sake. Named Entity Recognition (NER) tagging and chunking don't really show complex long distance relationships where the language model could excel. It would be interesting and informative if we could see how it performs on tasks such, as Semantic Role Labeling (SRL) or Combinatory Categorial Grammar (CCB) supertagging. 
The paper argues that using a task RNN is crucial since a CRF applied to language model embeddings shows weak performance in this context. However it remains uncertain if the language model embeddings used in this study underwent fine tuning, via backpropagation. Should this not be the case it appears reasonable to assume that fine tuning could potentially obviate the requirement for a task RNN. 