
"Strengths wise " I would say this paper is quite strong and well put together overall."
Weaknesses include the need to elaborate on comparisons, with methods presented in the study. 
Lets chat about; 
The main point of this paper is to introduce an approach to understanding how words have multiple meanings by using Gaussian mixtures to represent those meanings—essentially viewing a word as a set of Gaussian distributions rather than just one distribution per word like in the previous work by Vilnis and McCallum (2014). This new method tackles the issue of polysemy effectively by considering meanings, for each word. 
This paper is well written and organized with a presentation overall.The experiments were carried out in a manner and the qualitative analysis in Table 1 shows results that align with the suggested method.There are hardly any concerns to point out; the feedback, below aims to improve the papers clarity and thoroughness further. 
Some thoughts; 
It could be useful to explain how the current method is different from that of Tian et al.s (2014) as both approaches break down single word representations, into multiple prototypes using a mixture model. 
In the work section there are some citations that seem to be missing and could be included.
Exploring the Non parametric Assessment of Multiple Word Embeddings, in Vector Space. Neelakantan et al. presented at EMNL P 2014   
"Can Multi Sense Embeddings Enhance Comprehension of Human Language?”. Article by Li and Jurafsky, at EMNL 2015.   
"Exploring Topic based Word Embeddings, by Liu Y., Liu Z., Chua T. And Sun M. presented at AAA 2015."  
Adding the outcomes of these methods to Tables 3 and 4 might also prove to be quite useful. 
Authors I have a question for you. What do you think is causing the decrease in performance of w3gm in comparison to w3, in the SWCS analysis? 
After going through the authors’ response I've had a chance to review it thoroughly. 