Pros;   
This study presents an approach to tracking conversation stages using pretrained embeddings to depict slot values and merge them into distributed representations of user statements and conversation context effectively.The tests carried out on two sets of data show notable enhancements in contrast to the traditional delexicalization technique.Furthermore the authors investigate pretraining techniques for word embeddings, like XAVIER,GloVe and Program SL999. 
Areas needing improvement;   
In the studys datasets used to test the proposed method there seems to be a limitation in representing dialogue domains effectively with embeddings due to the focus on delexicalization scaling issues; however it would be intriguing to observe the performance of the suggested approach in intricate dialogue situations both with and, without a specific slot tagging feature. For example when determining how similar utterances are, to slot value pairs it could be an option to limit the assessment to the range of the slot values. This approach could still be relevant even if the values do not align perfectly. 
The instances mentioned in the beginning may be a bit deceptive. For example shouldn't the conversation state also mention "restaurant_name as The House"? This brings up another query. How does resolving coreferences affect the tasks effectiveness? 
Lets talk about it in general.  
The idea of employing existing word embeddings is really smart and the approach suggested for using them seems quite promising and engaging. 