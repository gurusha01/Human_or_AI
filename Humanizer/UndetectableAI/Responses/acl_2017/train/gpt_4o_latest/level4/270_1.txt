This study presents a developed neural network structure, for textual inference or natural language inference (NLI) consisting of three main stages; encoding information processing step using attention based matching and accumulation of results obtained in the models process.. There are two versions of the model presentedâ€”one that makes use of Tree Recursive Neural Networks (TreeRRNs) and another that utilizes sequential Bidirectional Long Short Term Memory networks (BiLSTMs). The sequential variant achieves top notch performance levels while combining both tree based models in an ensemble produces even more impressive outcomes. 
The document is nicely crafted with a planned model and noteworthy results showcasing a thoughtful approach, to the subject matter at hand. 
Key topics to address; 
The paper often mentions that the suggested system could set a standard, for future NLI studies; however 
The design of the structure shows a level of balance in some areas that might be considered too much at times.One example is how attention is calculated in both directions within sentences and a distinct inference composition network is used for each direction.This probably increases the time taken by the model.Is it really essential to have this degree of balance for a task, like NLI which's inherently not symmetrical.Have you performed any experiments to investigate this aspect?>  
Why was the performance of the tree based model not reported when results were presented for both the full sequential model (ESIM) and the combination of sequential and tree based models ( HLM)?
There are a minor details to consider. 
The quote by Barker and Jacobson may not fully capture the intended message in its context as it pertains to an unresolved issue within formal grammar related to direct compositionality. Perhaps a fitting approach would be to make a general statement, about the extensively acknowledged concept of compositionality instead.   
The feature of the vector difference utilized in the model (which has been observed in studies as well) is somewhat unique because it adds unnecessary parameters by using vectors 'A', 'B' and '(A. B)' as inputs for matrix multiplication which is essentially the same as using only 'A' and 'B' with a distinct matrix parameter in a different model mathematically speaking. Although there could be advantages in terms of learning associated with incorporating this feature into the model design process; it would be beneficial to have a discussion, about its inclusion.   
How do they put together the components of the model that are structured like trees and do they face any issues with speed or scalability in doing so?   
 Typographical error found in the reference (citation Klein and D.Manning (2003)).  
Consider utilizing tree drawing instruments such as (TikZ )Qtree, for generating parse trees that are easier to read and comprehend without overlapping lines. 
I'm sorry. I cannot proceed with the task without the actual input, from you. Could you please provide me with the text you'd like me to paraphrase?
Thank you for getting to me with your reply!. I am still in favor of publishing this piece of work. Though it may not be groundbreaking, per se it does bring in some elements and the outcomes are quite unexpected enough to bring value to the conference. 