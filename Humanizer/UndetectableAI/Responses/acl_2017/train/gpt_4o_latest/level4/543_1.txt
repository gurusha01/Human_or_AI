Following the rebuttal make sure to update.
The authors have done a job in explaining the initial setup and backing up their findings with solid evidence; these explanations should definitely be included in the final version for publication purposes! I'm intrigued by the concept of utilizing elements for these languages and am excited to see how this method could potentially be applied to tackle more complex tasks, in upcoming studies. 
**Advantages;**
  Visualizing Chinese, Japanese and Korean characters as visual elements presents an captivating concept! 
Shortcomings; 
  The findings from the study show slight enhancements compared to the initial measurements and the test conditions pose a challenge in confirming a crucial assertion. That visual attributes boost effectiveness, with less common or unfamiliar terms. 
  There are still some aspects in the basic implementation details that make it hard to understand the results and create difficulties, for reproducibility. 
Lets talk about topics.
The study delves into how computer vision methods (using CNN on text images) can improve language processing for Korean languages that have intricate character structures and properties. The researchers tested their model on a text categorization assignment involving assigning Wikipedia page titles to categories. Their results suggest that while a basic one hot character representation surpasses CNN based representations in performance merging representations, with one hot encodings produces superior results compared to using either method independently. They also offer some proof that visual characteristicsre more effective than one hot encodings, for uncommon words and present qualitative findings indicating that the CNN grasps significant semantic representations of characters. 
The notion of analyzing Japanese languages visually is intriguing and the papers rationale seems sound to me; nevertheless I have reservations, about the validity of the experimental outcomes presented in it.The assessments appear lacking in rigor which makes it challenging to ascertain if the findings are reliable or merely happenstance.I would appreciate seeing a thorough evaluation conducted to bolster the papers argument for publication. If the writers can show a statistical relevance, in their counterargument I would be more likely to back their acceptance; however a more focused assessment would be preferred. 
Sorry I can't do that.
  In Section 3 of the document in question where the "lookup model" paragraph is found; it is not explicitly mentioned which embeddings are being used or whether they undergo tuning through backpropagation similar to the visual embeddings process outlined earlier in the text.You can find this lack of clarity affecting how easily one can compare the visual embeddings.If the baseline embeddings were general purpose while the visual ones were tailored to tasks it does raise questions, about how fair the comparison can be deemed. 
  The decision to assess the categorization of Wikipedia page titles is quite surprising to me. One main reason for utilizing the model is its capacity to adapt to infrequent or unknown characters. Instead of concentrating on a task that specifically examines this ability? For instance assessing the translation of out of vocabulary (OOV) words, in machine translation would effectively showcase the benefits of visual characteristics. Although the authors make a point about visually representing some languages conceptually the current assessment fails to clearly highlight the drawbacks of traditional methods. This weakens the argument, for incorporating elements.
  "Could you please confirm if the enhancements mentioned in Table 5 hold significance?" This clarification would provide support, for the assertions made in the paper. 
  Figure 4 seems a bit tricky to understand. Could be clearer in highlighting the importance of the models findings since its a significant result. From what I gather the x axis depicts word rarity (probably log frequency) with common words on the left side. The positioning of the model intersecting the x axis more towards the left compared to the lookup model indicates superior performance, with rare words. Why is it that both models don't meet at the point of the x axis even though they are tested with the same data and trained using the exact dataset provided to them? It would be beneficial if you could provide an overview of what message this figure aims to convey in your response. 
  Why not showcase the performance at thresholds, for fallback fusion instead of just focusing on one threshold like 0 which seems like an exceptional scenario and may not fully represent how widely the technique can be applied? 
  The basic and conventional test for characters shows potential but seems to be overlooked in the research findings.I wish there was a thorough assessment in this aspect, like categorizing new words. 
  Please include translations for Figure 6 to assist readers who may not be familiar, with language. 