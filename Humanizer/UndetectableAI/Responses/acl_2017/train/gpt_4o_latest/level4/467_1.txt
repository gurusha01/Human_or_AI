  
This study presents a step by step method to create word embeddings using extensive monolingual data and initial mappings between two languages that are minimal or automatically generated without human intervention.This approach stands out compared to techniques that depend on sizeable bilingual dictionaries or parallel/comparable text collections as it delivers outstanding outcomes with minimal need, for manual editing or input curation. 
Shortcomings;   
The paper could be improved by discussing the errors in the method and exploring adjustments to rectify these limitations. 
Lets chat about some topics.  
Do the occurrences of the terms, in the single language datasets impact the outcomes?   
It could be helpful to see how the words in both languages change over time (for example after n iterations), for a few specific examples.   
How is the approach used when dealing with interpretations of a single term (for example terms, with multiple meanings)?  
German and English have a difference when it comes to using compounds â€“ German tends to use them more frequently than English does. How does the method deal with these compounds. What do they represent or correspond to? Would it be beneficial to enhance the outcomes with a step that involves breaking down compounds (perhaps utilizing information from, within the corpus)?  
What is the highest possible limit for this method in theory? Examining mistakes like words that are matched with those, in the language at a great distance would provide valuable insights. Furthermore talking about where these errors come from and exploring whether the suggested approach could be adjusted to reduce them would improve the paper. 