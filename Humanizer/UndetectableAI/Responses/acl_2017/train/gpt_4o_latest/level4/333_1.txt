Here are some positive aspects;   
The writers present an encoding model as an improvement to the sequence to sequence structure for creating concise summaries of sentences in an abstract manner.The document is well organized. Explains the techniques clearly.The suggested method is tested using standards and compared to leading techniques, with statistical significance assessments. 
Areas, for improvement;   
Some details regarding how things were carried out and the specifics of the systems being compared need clarification and additional explanation. 
Lets talk about something, in general.  
  **Important Evaluation;**  
  I'm not sure if the summaries produced by the method are really abstract in nature as intended.The way I see it is that the target words used are based off of those in the training data summaries. But looking at Figure 4 makes it seem like the summaries lean more towards being extractive of truly abstractive.The authors might want to consider choosing an example for Figure 4 that is more representative and also provide some stats showing how many words in the output sentences don't appear in the input sentences, across all test sets.   
  On page 2 of the document in lines 266 to 272 it is observed that even though there is a mathematical difference, between the vectors \( h_i \) and \( s \) they both appear to convey the inputs meaning similarly. Is it essential to include both of them in the analysis or did the authors conduct any experiments by utilizing one of these vectors?   
  The paper should mention the network library used for implementation; this detail is crucial to include.   
  On page 5, in Section 6 of the report or study being referenced here indicates that there was no mention of the training data employed for the different systems assessed in the experiments conducted by the authors involved in this research project or study endeavor – it raises the question of whether these systems were developed by the authors themselves or not.   
  Just a quick check;   
  On page 1 at line 44 of the document it talks about the difference between extractive summarization in Section 2; however it might be better to explain this concept earlier, in the introduction since some readers might not know about it yet.   
  Could you please provide a source for the claim that "This method has been highly effective in applications such as machine translation that necessitate alignment, between every aspect of the input and output"?  
  On Page 2 of Section 1 in the paragraph of the document mentioned above regarding the works significance is well stated; however it would be beneficial for the authors to highlight that no such selective encoding model has been put forth previously (if this statement is indeed correct). Furthermore the section, on related works ought to come before detailing the methods employed in the study.   
  Figure 1 versus Table 1 comparison is made by the authors to showcase summarization with two examples; however using just one example would have been sufficient as it seems inconsistent to label one as a figure and the other, as a table.   
  Section 3.2 requests references for the statements about how the encoder and decoder function in the sequence, to sequence machine translation model to process input sentence data and create an output sentence and for studies that have used this approach in summarization tasks.   
  In the paper they mention "MLPs ". They don't provide an explanation, for it.   
  On page 3 of the document at lines 289–290 it is mentioned that the formulas in Section 3 would benefit from including definitions, for the sigmoid function and element wise multiplication.   
  On the page in the first column of the document it mentions that some elements, in the equations are not clearly defined. These elements include \( b \) \( W \) \( U \) and \( V \).  
  On page 4, at line 326 of the document however the status of "r_t" does not appear in Figure 2 (workflow).  
  Table 2 lacks clarity in defining the term "(ref)". Requires further explanation, for better understanding.   
  In Section 4 of the paper titled "Model Parameters and Training " it would be beneficial for the authors to provide insights into their decision making process regarding the selection of parameters like word embedding size and GRu hidden states as well as variables such as \(, \alpha \) \( \beta_{\text{one}} \) \( \beta_{\text{two}} \) \( \epsilon \) and beam size.   
  Page 5, on line 450 could benefit from omitting the word "the " changing it to "SGd as our optimizing algorithms." This adjustment can help streamline the sentence.  
  On page 5 of the document regarding beam search methodology mentioned earlier in our discussion – could you kindly provide a reference for clarification, on the topic? Thank you!  
  Figure 4 shows an error in the correct sentence "the council of europe once again criticizes French prison conditions." Should "again " be replaced with ", against"?  
  