Here are some positive aspects;   
  The article tackles an pertinent challenge.   
  Its written in a way thats easy to understand and follow.   
  "The dataset produced in this project could prove beneficial, for researchers."  
  The research paper offers a review of how well the model performs.   
"Areas of improvement;"  
  The paper does not incorporate any established approaches, from research for a direct comparison of results.   
  Further exploring the distinctiveness of the assignment and the constraints of studies, in tackling this issue could enhance the recognition of the advancements made in this study.   
Lets talk about topics.  
The paper discusses methods, for categorizing frames in tweets using weakly supervised models.It creates predicate rules by combining Twitter specific behavioral cues.These rules are applied in a soft logic framework to develop classification models.The goal is to classify tweets into 17 frames with multiple labels.The results of the experiments highlight the effectiveness of using predicates based on signals.Here are my extensive thoughts;   
The article could be improved by exploring the differences, between frame classification and stance classification and discussing whether these tasks are conceptually linked but vary in their level of detail.   
The paper would benefit from an exploration of the difficulties that arise when moving from lengthy congressional speeches to concise tweets. It might be valuable to investigate whether previous studies depend heavily on features that span sentences which may not be as relevant in the context of tweets. Furthermore it could be interesting for the authors to explore the possibility of applying a frame classification method tailored for speeches (or a stance classification method suitable for any type of text) to Twitter data, within the limitations of the platform and compare its outcomes with those proposed in the paper.   
The paper seems to use the terms " supervised'' and "unsupervised'' interchangeably (if this is incorrect please explain in the authors response). In my opinion,"weakly supervised'' is the fitting term for the described setup, in this study and should be consistently used throughout the text. Although the original data may not have been annotated by labelers initially the classification procedure depends on labels that are weak or noisy and the keywords are generated from expert suggestions. The technique diverges from unsupervised methods such, as clustering and topic modeling that rely on entirely unlabelled data sources.   
The Cohens Kappa value calculated may not accurately represent the challenge of categorizing tweet frames (lines 252 253). Asserting it as evidence of this challenge appears assertive. Primarily reflecting annotation complexity or disagreement the Kappa value can be affected by aspects like inadequately formulated annotation instructions or biased annotator choices and inadequate training for annotators apart, from the inherent task difficulty. The Kappa value of 73..6 percent is quite robust, for this task. Affirms the trustworthiness of the labeled data.   
Does the formula in (1) which is discussed in lines 375 to 377 of the text overlook details like context (such as negations or conditional statements) impacting how well it predicts frames based on tweets similarity levels?. Have the authors explored methods like skip thought vectors or vector compositionality techniques to factor in context and enhance similarity calculations, for longer text segments?.  
In an experimental design scenario for research purposes it would be best to not include labeled information when analyzing data to determine the most significant bi/tri grams (as stated in line 397 where the complete tweet dataset was referenced). Including statistics, from test partitions (or labeled data in a supervised setup ) could unintentionally sway the selection process. Although this may have had effect on the current findings given the extensive nature of the unlabeled dataset following this more meticulous experimental approach would enhance the validity of the results.  
Could you add details, on precision and recall metrics in Table 4 for an assessment please?  
Here are a few minor comments;   
Please double check the footnotes placement to make sure they align with the formatting guidelines and follow the punctuation rules consistently. 