This study presents a method to assess word embeddings by looking at their effectiveness with data usage and simple supervised assignments in a direct manner. The main reason behind this is the application of word embeddings in transfer learning situations where the evaluation typically centers on the speed of training a target model. The suggested strategy involves using supervised assignments like standard tests, for word similarity and analogy. Tests carried out on types of embeddings show that rankings differ for each task and are influenced by the quantity of training data accessible. 
Areas of expertise
The emphasis, on transfer learning and optimizing data usage is intriguing because it fits well with how embeddingsre commonly used as a simple "semi supervised" approach. 
Areas, for improvement
A reliable assessment method should show stability in tasks that come later in the process flow. In particular if the method assigns a R to a group of embeddings this ranking should ideally remain consistent for final tasks like categorizing text parsing sentences or translating languages. However the study fails to examine this aspect creating challenges in deciding if the suggested approach is truly superior, to methods. 
The mention of embeddings seems off topic and doesn't seem to add much to the overall comprehension of the paper. 
The experimental section lacks clarity as seen in Section 3 point 9 where it discusses the importance of incorporating syntax embeddings even with a large supervised dataset without fully explaining how these conclusions were drawn from the evaluation results. 
In Section 3 point 5 of the document mentions that " unsupervised large scale pretraining may not be suitable for NLP applications." While this statement is bold. Raises questions about its connection, to the evaluation method used. 
The pre trained models employed in the experiments are readily available for use without any modifications, to the training datasets they were trained with.This lack of control undermines the credibility of the assessment outcomes. 
The manuscript needs to be checked for errors in citing figures; for instance Figure 1, on page 3 is mentioned on page 6. 
Lets talk about topics and have a chat.
The paper starts with a premise but falls short in effectively proving the success of the suggested method. It is highlighted that it is crucial to back evaluation methods by demonstrating their applicability to practical tasks; however this aspect is overlooked in the present study. Furthermore the lack of explanation and the necessity for proofreading make it harder to follow. Looking ahead the paper could greatly improve from evaluations and a stricter experimental design (such, as using consistent corpora for all embeddings).I don't think it's an idea to include it in the conference as it is now. 