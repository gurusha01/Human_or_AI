This paper is really well done! Its written well and has some interesting findings and a creative approach compared to previous research methods used in the field of study. The detailed corpus annotations will definitely be helpful for researchers in this area too. Section 5s qualitative discussion was particularly insightful and engaging. Unlike machine learning papers that just list results without much explanation or depth this paper offers valuable insights, for readers. 
Weak points can be found in Section 4A where the phrase "The remaining parts of the models input are adjusted to zero..." may not be completely clear until you refer to Figure 2 for details about it.A short explanation here could make things easier to understand.Furthermore in Figure 2,the input layers, for the LSTMs are marked as "5 times Embeddings (50 dimensions) " for the networks handling dependency labels.This appears to be incorrectâ€”or if accurate it requires explanation to clarify its meaning. 
In Section 4. Discussion Point; I find the comment about LSTMs being great for modeling language sequences and being the preferred model choice a bit strange here. The issue we're dealing with isn't exactly sequential in the sense. In this scenario each example feeds all five words to the network at once for every data point. Theres no connection, between consecutive examples. Even though using an LSTM architecture could be the decision the rationale mentioned doesn't quite match up. Hey there! Could it be that I've gotten something here? I'd love to hear the authors elaborate, on this point. 