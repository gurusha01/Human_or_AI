This article presents a method for zero resource translation when there are parallel corpora for (source,pivot) and (pivot target). The strategy includes educating a teacher model for predicting target language based on pivot language using (pivot,target corpus) and then training a student model to predict target language based on source language, with minimal relative entropy compared to what was taught by teacher model using (source,pivot corpus). Using word level entropy on examples, from the teacher allows the new method to show better results when compared to previous pivoting methods and other strategies that lack resources. 
This piece makes an impact as the new concept is fresh and well explained with convincing real world proof to back its claims up nicely.It differs from methods by keeping its assumptions about the NMT systems at a minimum level which enhances its versatility across various scenarios. 
Here are a couple of suggestions for tests to try out; 1. It could be interesting to test how well this method holds up when dealing with source and pivot languages that are quite different from each other in terms of their target probabilities. 2. Since word diversity has shown to be effective, in experiments it's surprising that there haven't been any tests done involving n best sentence lists or sampling sentences. Even though these tests may require computing power and resources to implement effectively than before due to beam search already in use with the instructor model; it shouldn't be too burdensome cost wise overall. Moreover. Going back to what was mentioned. It might be a good idea to explore moving away from focusing solely on word variety towards considering diversity, on a sentence level as the student model progresses and begins to rely less on rare word cues. 
Additional remarks; 

Please clarify "target word y " not "target sentence y " in line 277.
Can you please explain if when K equals 4 and K equals 8 mean comparing the likelihoods of the likely word and the top eight most likely words in the present situation? Also clarify if the current situation is chosen based on benefits or, through a more thorough search process. 
Section 4 point 2 talks about comparing an uniform distribution which doesn't offer much insight because its highly likely that y given z will be much more similar to y given x than to a uniform distribution probability wise. A useful way to look at this would be to see how valuable y given z remains as y given x gets better and better over time or, with increasingly more data used during training iterations. FurthermoreÂ¸ it would be intriguing to explore how the mode approximation fares in contrast to best lists, in determining sentence level scores. 
"I find it interesting that word beam search is less effective than word search even though word beam search is expected to be more similar, to word sampling in theory." Can you explain why this is the case? 
The claimed benefit of sent beam seems dubious since it might just be fluctuations given the significant variability, in the associated data points. 