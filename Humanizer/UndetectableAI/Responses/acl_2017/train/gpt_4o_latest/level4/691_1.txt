In summary   
This paper presents an approach to developing sense embeddings based in a lexical semantic reference, like WordNet. However the paper does not directly assess the significance of the acquired sense embeddings. Instead these sense embeddings are combined into word embeddings. Then evaluated through a subsequent task which involves predicting prepositional phrase (PP) attachment. 
  
The findings regarding predicting PP attachment seem strong and persuasive. 
"Areas, for improvement;"  
The significance of the sense embeddings is not assessed directly.   
Some parts of the model seem unclear to me. For instance are the lambda i parameters considered hyperparameters or are they acquired through training? Moreover I find the origin of the term "rank" quite confusing. Does it represent the sense rankings found in WordNet?   
In studies on this topic of related research work on representing word meanings as a mix of different senses has been looked into before. For instance the study titled "Placing a meaning network within a language space" (presented at NAACL in 2015), by Johansson and Nieto Pi√±a broke down word meanings into senses connected to a structure using a method. The same concept has also been used in the training of sense vectors in a manner. This approach is evident, in the work titled "Linear Algebraic Structure of Word Senses" authored Arora et al. It includes applications related  words.
I just have a small remarks;   
No definitions, for types and tokens are necessary since these terms are commonly used in the field.   
Why do we need the \(\lambdawi\) in equation 4 when the probability is not normalized yet? 
Lets talk about it in general.