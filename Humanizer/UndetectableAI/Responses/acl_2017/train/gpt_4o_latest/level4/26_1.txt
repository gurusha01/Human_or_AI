A Comprehensive Approach, to Answering Questions Using Cross Attention with ranging Knowledge in Knowledge Bases
This paper suggests a framework for answering fact based questions using a knowledge graph called Freebase. The goal of the model is to create a connection between different elements of potential answers (such as answer type and relevance to the question entity) and specific groups of words, in the question. Each aspect of the answers is represented by a unique component that establishes this connection. The paper makes two contributions; ( ) introducing distinct elements to capture various facets of potential answers rather than depending solely a unified semantic representation ( ) incorporating global context from the Knowledge Base (KB).
I find the breakdown of candidate answer representation into aspects in this work particularly interesting.The method allows neural model developers to have control and steer the neural network towards more pertinent information for decision making.It reminds me of algorithms that relied on feature engineering; however in this case the "feature engineering"(or aspects) is more subtle and less cumbersome.I suggest that the authors further develop this approach, in revisions. 
Although the main concept is understandable for those who're knowledgeable about the topic at hand​ some aspects may be difficult for certain readers to fully comprehend​. There are sections of the document that could be improved with more explanation​, such, as; 
The explanation of the context aspect, in candidate answers (denoted as e_c ) lacks clarity in Section 3. 22 With the final two sentences being especially ambiguous.
The abstract and introduction need explanation regarding the mention of OOV (out of vocabulary). The current description appears to assume an understanding of previous research that may not be accessible, to all readers. 
The experiments mainly examine information retrieval (IR) based systems which seems like a decision to focus on them Here is a reasonable choice However adding Yang et al.s 2014 approach described as SP based brings up concerns about how consistently the comparison is made It's good to see more systems being evaluated but it would be beneficial to explain the reasoning behind what gets compared and what doesn't Furthermore providing performance data, for the top SP based systems would give context. 
The article mentions that embeddings are derived from the training data set.I wonder how starting with initialization affects the ultimate performance.It would be helpful to document any variations noticed.It would be intriguing to investigate whether using pre trained embeddings (such as those, from word2vec) as opposed to random initialization would impact the outcomes. 
While reading the paper and thinking about work possibilities that could be explored further in this area is the idea of incorporating structured queries (from SP based methods) into the cross attention mechanism discussed in the paper. In addition to utilizing candidate answers different aspects as features for improvement in performance or accuracy of the model or system being studied; structured queries responsible for generating candidate answers can also be considered as an added layer of information. This means that an attention mechanism could be used to focus on elements, within the structured query and how they align semantically with the input question being posed. Potentially offering a valuable additional signal for the neural model to consider during its processing. Here's an idea worth considering for investigation. 
When it comes to where the paper stands; 
It makes me unsure about calling the suggested model an "attention" model because from what I know about attention mechanisms is that they usually work within "encoder decoder" frameworks where information in one form (like an image or a sentence in one language) is converted into a general representation and then transformed into another form (for instance a caption or a sentence, in a different language).Attention mechanisms enable the encoder to concentrate on sections of the input while the decoder produces the output, in this paper does not appear to match that description accurately; using the term "attention " might lead to confusion among a wider audience. 
I'm sorry I cannot provide a paraphrased text without the input. Could you please provide the text you'd like me to rewrite in a like manner?
Thanks, for the explanations given in the authors response. 