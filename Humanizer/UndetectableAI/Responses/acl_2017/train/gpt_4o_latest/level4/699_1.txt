This study presents a framework for generating keyphrases using an encoder decoder approach with results showing that the model outperforms methods in cases where there is labeled data, for training purposes. 
"Advantages;"  
The document is nicely. Clear in its explanations of the methods underlying principles.It gives information to make it easy to replicate the experiments.The use of an encoder decoder framework with a copy function may seem simple. The results, from the experiments are persuasive and support the papers assertion about generating missing key phrases. 
Shortcomings;   
The suggested method mentioned previously isn't particularly innovative and falls short in providing generalization to new areas as highlighted in Section 5 point 3 of the discussion section; it performs below par compared to unsupervised models. Even though one of the aspects highlighted in this paper is the importance of having a substantial amount and high quality training data sets available, for use; this specific point is not explicitly given much emphasis. 
Lets talk about this topic in general.  
The paper was a read. I hope it gets accepted! I'm interested in how the training datas size and variety affect the methods performance. It would also be great to have the values of pg and pc (with examples from Figure 1) included in the CopyRNN model. In my past work, with CopyNet, sometimes the copying mechanism acts strangely and its not clear why that happens. 