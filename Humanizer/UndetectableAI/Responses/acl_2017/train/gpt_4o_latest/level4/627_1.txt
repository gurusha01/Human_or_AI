This paper presents an agent that optimizes both the belief tracker and dialogue manager using the REINFORCE algorithm in collaboration with a user simulator for learning purposes.The training process consists of two phases; first an imitation learning phase where the system is kickstarted with supervised learning, from a rule based model and then a reinforcement learning phase where the system is optimized together using the RL objective. 

Weaknesses of the system include the fact that it's n't entirely end to end since the response generation part is manually created rather than learned organically and that the end to end model tends to overfit to the simulator and fares poorly in human assessments. This brings up questions about whether the paper's promoting end to end learning or the soft KB access method. Although utilizing soft KB access consistently enhances performance benefits of end, to end learning are not as compelling. The writers try to show the importance of end to end learning in Figure 5; however it is not clear cut in their explanation. Additionally the paper fails to explain why the REINFORCE algorithm is being used, which is recognized for having high variability. The authors did not tackle this problem by including a baseline or investigating methods, like the natural actor critic algorithm, which is usually more successful. 
In discussion terms. Though there are weaknesses mentioned. The experiments are solid and the paper is mostly acceptable. Nevertheless it would enhance the papers strength to concentrate specifically towards the aspect that evidently boosts performance. The soft knowledge base access. Instead of highlighting end, to end learning which has a lesser impact. 