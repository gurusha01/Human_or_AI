
Advantages; 
The task is clearly outlined. 
A basic model that has accomplished performance, on the SQuAD dataset (using only one model).
A thorough. Comparison. 
Areas, for improvement; 
Upon examination there is a restricted review of mistakes and outcomes (refer to comprehensive remarks provided).
Lets talk about some topics.
This article presents a technique for searching Wikipedia to respond to general questions without limitations. The suggested system consists of two parts ; one, for searching and extracting Wikipedia posts and the other for responding to questions using the extracted posts. 
The part of the system that finds documents is like an information retrieval system that uses term frequency models and counts n word sequences for analysis purposes. The answering part uses a way to represent paragraphs that involves word meanings clues to tell if a word in the paragraph is also in the question being asked, details about words like their parts of speech and named entities and a gentle way to measure how close question words are to paragraph words in terms of meaning. These details are put together. Fed into a bi directional LSTM RNN, for processing. A different Recurrent Neural Network is used for inquiries by analyzing word representations.The system trains classifiers individually to anticipate the beginning and conclusion of sentence segments in passages, for producing responses. 
The training procedure includes open domain QA datasets, like SQuAD and WebQuestions by adjusting the training data with articles obtained from the IR engine instead of solely depending on the accurate document or passage. 
This paper is quite interesting and well presented. However A few things have caught my attention. I have some questions.
The information retrieval system shows a success rate of more than 75% at the top 5 accuracy level and the document reader performs admirably on its own by surpassing the performance of top individual models on SQuAD dataset tests. However observed in Table 6 is a drop in performance that stands out noticeably. The authors acknowledge that by conducting tests using the paragraph instead of retrieved results there is an accuracy improvement from 0. 26 (Initially obtained) up, to 0. 49; Nevertheless this still falls short of the 78%. 79% Accuracy range typically achieved in SQuAD tasks. It seems like the neural network has a time learning efficiently when trained with a modified dataset that includes retrieved articles as opposed to being trained and tested directly for the document understanding task itself. The paper could be improved by looking into this issue such as comparing the training accuracies between the two scenarios and discussing potential strategies to bridge this gap. Although the authors briefly mention this in their conclusion a deeper exploration, within the paper would offer valuable insights. 
The authors decided to approach this as a machine comprehension challenge without utilizing external tools like Freebase that could have provided assistance with entity classification tasks.They stayed true, to their objectives in doing however it would have been intriguing to investigate how incorporating such resources could have affected the outcome.Expanding on the query; considering the possibility that errors arise from extremely pertinent topical statements as indicated by the authors would employing entity typing potentially decrease these errors? 
Furthermore it is important for the writers to mention QuASE (Sun et al., 2015 WWW 2015 ) and other comparable systems, in their literature review. QuASE is an open domain QA system that responds to queries by extracting passages. It utilizes the internet instead of just relying on Wikipedia. 