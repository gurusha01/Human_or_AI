This study examines methods for creating word embeddings used in sentiment analysis tasks and explores how the choice of corpus impacts the effectiveness of these embeddings for the task at hand.The researchers find that larger corporas may not always yield the results; instead they discover that embeddings from corporas rich, in subjective content—referred to as "task specific data"—tend to perform better.Furthermore the study delves into embeddings that blend information from both "task data" and general corporas demonstrating that such blends produce superior results compared to embeddings sourced from a single corpus. The results have been confirmed not for English but also for a less widely used language, like Catalan. 
Things we are good, at;   
The article delves into an aspect of sentiment analysis focusing on how to create embeddings efficiently for training supervised classifiers, in polarity classification tasks.It is well organized and written in a manner.The researchers offer experimental proof to back up their main arguments. 
Weak points;   
The results of the experiment are mostly. Not surprising at all The techniques used are simple and improvised without bringing anything new to the table The main concept. Emphasizing the use of data specific to tasks to enhance the accuracy of embeddings. Has already been investigated in previous studies like Joshi et al (2015)s research in relation, to named entity recognition The advancements presented in this study are minor. Do not introduce any groundbreaking innovations   
Some studies seem inconclusive because they didn't conduct statistical significance tests comparing the classifiers used in Tables 2 3 and 6 where multiple classifier settings produced results. It's crucial to perform statistical significance tests to ascertain the significance of these variations. In Table 3 on the side depicting RT results; It's not clear whether there's a notable distinction between the "Wikipedia Baseline" and any of the combinations or among the combinations themselves, like "subj Wiki " "subj Multiun " and "subj Europarl."  
Though focusing on subsets may lead to improvements in theory it may not always be practical in situations where resources are limited.This is especially evident when considering the selection step outlined using OpinionFinder; it may not be viable for languages other than English due, to the absence of similar tools or detailed datasets.This restriction becomes apparent in the Catalan experiments where such data is not employed.   
Small Details;   
  The dataset discussion is not very clear as it seems to focus polarity classification task but then introduces terms, like "opinion holder" and "opinion targets." If these details are not essential for the experiments being conducted they could be left out.   
  The reason, behind the "splicing" variation seems unclear to me.What makes this method essential. How does it enhance compared to mere "appending"?  
  How do these setups pinpoint data exactly Is OpinionFinder utilized once more without a direct mention of it being so  
  The variable definitions do not match Equation 3 as \( n_k \) for example is not included in the equation.   
  They're not sure if they're taking the opinions of both sides into account in the way as before.   
After the authors reply;   
Thanks for clearing things up for me!. I'm still a bit confused, about how opinion holders and targets are supposed to be integrated.   
In terms I stick to my original evaluation. This piece of work does not show originality and the issues mentioned in the authors' reply do not fully deal with this problem. The paper still adds little to the field. 