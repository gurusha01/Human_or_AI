The study explores how organizing text based Rhetorical Structure Theory (RSF) in order to improve text classification tasks is beneficial. A Recurrent Neural Network (NNN), with a focus mechanism's employed to create text interpretations. The trials carried out across datasets show the success of the suggested method. Here are my thoughts; 
In Table 2 data analysis shows that the "UNLABELED" model performs better than the "model on four of five datasets which is quite surprising! The authors should delve deeper into why thiss the case since adding more relation labels usually leads to better results. Could it be that the accuracy of relation labeling is not up, to par and affecting the performance negatively? 
Is it possible to keep the tree structure in the RST paper and train a hierarchical model, on it directly instead of converting it into a dependency structure beforehand? 
When it comes to the data sets¸ instead of just comparing each previous study with a single dataset¸ the authors could think about running experiments, on commonly used datasets from past research to offer a more thorough comparison. 