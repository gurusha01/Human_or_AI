Pros; 
The article is nicely crafted and organized;, in the beginning parts where the presentation shines and the arguments are generally compelling.   
It addresses an issue by investigating how word order information can be integrated into word embeddings (and senses) and the suggested method is fascinating. 
Areas needing improvement; 
The results are not consistent enough to say that the suggested models are better than current options – especially considering their increased complexity raises questions about their effectiveness overall.While acknowledging results is fine the analysis provided doesn't offer enough information to draw significant conclusions.Furthermore the lack of results for the word analogy task, beyond confirming that the suggested models weren't competitive represents a missed chance to delve deeper into exploration and analysis.   
Some elements of the configuration lack clarity or adequate justification specifically concerning the selection of corporal and data sets (detailed explanations below).  
The papers quality decreases in the parts which might leave the reader feeling a bit let down—not just by the outcomes but also, by how clearly its presented and how strong the arguments are made. 
Lets talk about topics.
The authors intend to "develop concepts for words and meanings in an evolving area " but this approach is solely applied in the LSTMEmbed_SW model that consistently falls short when compared to other options. Additionally Also Besides  the rationale behind creating concepts, for words and meanings lacks clarity and lacks thorough discussion in the paper.   
The reason for using trained embeddings is not clearly stated in the LSTMEmbed_SW model description and its not specified if these embeddings represent individual words or meanings or a mix of both aspects.With potential setups available, for use the exact configuration used in the experiments needs to be clearly outlined.   
The authors acknowledge the significance of understanding sense embeddings; however their explicit evaluation remains unclear. Lacking altogether in some cases, in certain word similarity datasets used during the experiments as they do not take into account the context of words.   
The training datas size is unspecified in the studys findings illustrated in Figure 4 where varying proportions of BabelWiki and SEW are compared; however; this comparison could present challenges if there are differences in the corpus sizes between them.The SemCor corpus is notably small. It is atypical for methods like word2vec to be used for training embeddings, with such a restricted dataset.If the suggested models are designed specifically for corporas this should be clearly mentioned and assessed.   
Some testing datasets like WS353 and WSSim are interconnected and not entirely separate, from each other; this situation makes it challenging to make comparisons accurately. Could potentially lead to an overestimation of successes by showing multiple wins instead of just one.   
The paper suggests that the suggested models can be trained quickly because they utilize pre trained embeddings, in the output layer; however there is no evidence presented to back up this assertion. Providing evidence could enhance the papers credibility.   
Table 4 shows that the embedding sizes vary between models which makes it challenging to compare them accurately.   
The paper is missing a part about finding synonyms when comparing similarities. Should include details, about the method used for the multiple choice assignment.   
The text fails to mention Table 2 as a point of reference.   
The word analogy task doesn't include any details, about the training process despite referencing the dataset associated with it. 