One of the aspects is strengths.
The paper is nicely. Written in an easy to understand way. The methods and results that are shared are interesting. Make you think. 
Areas, for improvement; 
The assessment procedure and its subsequent results give rise to apprehensions (please see my elaborated remarks below).
Lets talk about topics.
This research presents a method for analyzing arguments from beginning to end by utilizing neural networks. The authors tackle the issue using two approaches. Sequence labeling and dependency parsing. Moreover the study delves into incorporating a multitask learning structure within the sequence labeling strategy. The reasoning behind the model is clearly explained. In contrast to techniques that depend on ILPe manual feature creation and manual ILPe constraint design this new model removes the need, for such manual interventions. Moreover it works hand in hand to understand the parts of argumentation mining and helps reduce the problems of error propagation often faced in step, by step processes.\t Some specifics are not fully detailed (as mentioned) but overall the techniques are explained quite well. 
The setup for the experiment is thorough. The comparisons are well done with results that are quite interesting to consider. However my main worry is about the limited dataset size and the excessive capacity of the (Bi)LSTM based neural networks (referred to as BLC and BLCC). The training dataset includes 320 essays, with 80 essays set aside for testing purposes. Unfortunately the exact size of the development set is not mentioned in either the paper or supplementary materials. This situation is worrying because the small amount of training essays presents an obstacle to overcome.. The quantity of tags in the training data is probably a few thousand which is much less than the usual number in typical sequence labeling tasks that can have hundreds of thousands or even millions of tags.. This leads me to question whether the model parameters have been trained sufficiently.. Additionally there is no mention in the paper, about addressing the problem of overfitting.. It could be helpful to examine the training and development "loss" values throughout the training process (such as after each parameter update or epoch). The authors hint at overfitting in their statement found in Line 622 which suggests that taggers are simpler local models requiring less training data and being less susceptible, to overfitting. 
I have doubts about how reliable the modelsre for similar reasons too Like showing the average and spread from various runs using different initial settings could help address this concern Also running statistical tests would give us more insights into the stability of the models and how dependable the results are Without these tests it's hard to tell if the better outcomes are because of how effective the new method is or just, by random chance
While the studys neural networks use regularization techniques to some extent in this research project due to the limited dataset size here requires more focus, on regularization methods overall. The paper doesn't delve deeply into the topic of regularization. Only briefly touches upon it in the supplementary materials concerning LSTM. ER. This concern should be thoroughly. Explained within the manuscript. 
The writers could also think about using Bayesian optimization techniques to tune hyperparameters than sticking to the current method detailed in the supplementary materials. 
Moreover I suggest transferring the details regarding trained word embeddings and the error analysis from the supplementary materials, to the main paper to better integrate this information within an extra page. 
It would also be beneficial to incorporate ratings of agreement among annotators in the analysis of the dataset described in the paper as this information could offer perspectives on system performance and areas, for enhancement. 
Consider adding colors to Figure 1 to make it clearer and more visually appealing when printed in black and white. 
Certainly! Here is the paraphrased text; "Make revisions."
Thank you for addressing my queries and concerns promptly! I have now adjusted my rating to a 4 based on your responses and clarifications provided earlier. It would be greatly appreciated if you could incorporate the F score intervals into the document along with the variability values for various configurations as this will immensely aid in understanding the results. My apprehension lies with the consistency of the modeling outcomes especially when there is a fluctuation in performance noticed within the Kiperwasser setup which necessitates further investigation. The margin of error being relatively wide, at an F score range of [0."56" 0."61"] is something that should be highlighted in your publication to ensure that others can replicate your findings effectively. 