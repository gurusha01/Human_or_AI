This paper introduces an ambitious project where Universal Dependency (UD) grammar structures are automatically converted into semantic logical form representations by the authors new method.. The strategy involves associating a form structure with each UD element and outlining a conversion process that works from the inside out using an intermediate representation, to correctly nest substructures within their enclosing structures. The researchers performed two assessments – one to compare the results with established structures and another to evaluate how effective the lambda expressions were in addressing questions, from two QA datasets. 
The papers space constraints pose a challenge in presenting a description of the work at hand by the authors despite their attempts to touch upon crucial aspects there are notable omissions in details that deserve attention. It would be greatly appreciated to have a version of the paper for a more thorough exploration of the discussed QA results which currently appear somewhat lacking in depth. Understanding the types of unanswered or inaccurately answered questions and the reasons, behind these errors would offer valuable insights shedding light upon the boundaries of the proposed logical form representations. 
This leads me to my worry; the representation of logical form put forward in the paper doesn't quite capture true semantics but rather appears as a refined version of the input dependency structure with some notable efforts towards semantic enhancement such as the incorporation of lambda operators and the explicit inclusion of omitted arguments (through enhancement) along with assigning suitable types or units for expressions like eventive adjectives and nouns (for instance "running horse" or "president, in 2009"). Many key elements of meaning seem to be either absent or inaccurate. Some key components that are absent are quantifiers like "every" or "all " numerical values such as "20" or "just over 1000 " different types of references like "he " "that man," or "what I mentioned earlier " negative statements and modals that can change the meaning significantly and connections between events such as the subevent relationship in phrases, like "the holiday was enjoyable but the journey was troublesome."One could initially tackle these concerns by categorizing them as terms and using basic lambda formulas; however it demands a more intricate approach to address them effectively. For example dealing with values involves establishing distinct collection objects in the scheme along with recognized variables for accurate referencing (such as "a particular one"). Likewise' representing varying viewpoints on an incident (like Person As perspective versus Person Bs viewpoint) calls for symbols, for the event and procedures to correlate and connect them smoothly. The organization of events and situations over time is crucial. Needs consideration as well as attention to detail in handling these issues can be quite challenging as illustrated by models, like Discourse Representation Theory (DRT) which highlight the intricacies of quantifier usage and references. 
Although it might come across unjust to fault the paper for overlooking all conceivable semantic elements and equally crucial to steer clear of committing basic mistakes in analysis. One troubling point is how the paper views event relations in line, with the syntactic functions of verbs or nouns. For instance; The semantic roles played by "he" and "the window" differ in "he broke the window" compared to "the window broke”.The papers disregard for addressing this matter in semantic analysis is disappointing to me. This issue demands recognition and at least an initial resolution even within the confines of this study. To me this stands out as a flaw in the paper and a key factor in my decision on whether to recommend its presentation, at the conference. If the authors had been more upfront about this issue and proposed a strategy to overcome it in their research by mentioning tools such, as FrameNet and its semantic role criteria I might have been more willing to back their paper. 
I'm glad to say that the process of converting notations is explained well and easy to understand on a note from my end! I like how it's cleaner and simpler compared to the method (which was based on Stanford dependencies). Furthermore I admire the authors for presenting neural research to ACL at a time when neural approaches are in high demand, in this field—an evidence of their dedication to exploring different methods!