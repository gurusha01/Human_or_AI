The article discusses the challenge of choosing the written description for a specific scene or image from a set of similar options and introduces several initial models as well as a method, for evaluation and human rated scores. 
Advantages; 
The article is nicely. Structured.   
The ideas presented are well explained. Backed up with real world evidence which helps in making them comprehensible and coherent, to readers.   
The drive for this project is robust. Developing a technique to choose the caption from a set of deceptive options could improve image captioning and comprehension models significantly as an additional step, after generating captions.   
Areas, for improvement; 
The proposed algorithms ability to create decoys is uncertain. Brings, into question the credibility of the papers assertions.   
The algorithm picks captions that share descriptions and wording but don't match the exact image its targeting; however the challenge with this method is that just because a caption doesn't belong to image A doesn't always mean it's not suitable, for describing image A accurately—especially if the way its presented and described is alike. In Figure 2 provided in the reports illustration section the fake targets are lacking similarity to the target (like comparing a giraffe to an elephant) or they can be realistic alternatives for the target (such, as a young boy flying kites instead of just a boy playing with them).  
Therefore the data produced by this method may not adequately prepare a model to advance beyond recognizing keywords as asserted in the document. In the illustration provided (see Figure 1) numerous misleading options can be eliminated through keyword disparities—such as giraffe versus elephant, pan versus bread, frisbee, versus kite. In instances where keyword disparities do not arise the misleading options frequently seem convincing to be considered plausible choices.   
Moreover the fact that humans achieved an accuracy of 82..un out of every hundred examples, in a test has sparked some inquiries.. Does this imply that certain instances are just too difficult for humans to categorize accurately?.. Could it be pointing to the possibility that some misleading choices are so persuasive that they serve as acceptable alternatives (possibly even superior ones) causing humans to favor them over the true descriptions provided?.  
Lets talk about a topic.
This paper is excellently crafted, showing motivation and thorough experimentation.   
I have reservations regarding the consistency between the data generation algorithm and the dataset in relation to the stated motivation which impacts the trustworthiness of the findings, in this paper; hence I am leaning towards rejecting it unless my concerns are satisfactorily resolved in the rebuttal stage. 