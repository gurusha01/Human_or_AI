This paper introduces an enhancement to the traditional left to right beam search method by allowing the inclusion of specific word sequences as lexical constraints in machine translation outputs.This algorithm has proven its effectiveness in scenarios, like interactive translation and adapting to different domains. 
Although the extension is straightforward on its merit the papers significance lies in its formalization of the concept. Additionally it is interesting to note that NMT shows effectiveness, with a collection of random constraints that do not have clear alignment information. The approach outlined in this study seems to offer possibilities beyond what was discussed in the paper including improving NMTs capacity to manage compositional structures—an aspect where traditional SMT might still hold a edge. 
The main drawback of the paper is that the experiments have a limited scope. While the interactive machine translation simulation validates the functionality of the method it is difficult to gauge the level of success. For instance its challenging to determine how frequently the constraints were integrated in a way as the significant improvements, in BLEu scores only offer indirect proof. Similarly the domain adaptation tests would have been stronger if they had included a comparison, with the fine tuning'' baseline—a task that could have been easily carried out using the 100 thousand Autodesk corpus. 
I think the paper makes a contribution and deserves to be published despite this drawback. 
Additional remarks; 
In PBMT the term "coverage vector" could be confusing as it is commonly used in that context; a fitting term could be "coverage collection" to accurately represent the data structure it refers to. 
Table 2 should include details about the constraints per source sentence in the test data to give an understanding of the enhancements, in BLEu scores. 