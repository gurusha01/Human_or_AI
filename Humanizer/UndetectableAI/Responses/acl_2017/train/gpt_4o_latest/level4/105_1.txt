Pros;   
The idea of monotonic attention is groundbreaking and markedly different, from current methods. 
Shortcomings;   
The results of the experiments in generating inflections show a varied outcome." The suggested model proves to be efficient in situations where there is training data (such as CELEX) or when the alignment is mostly monotonic and relies less upon context (like, in Russian, German and Spanish). 
Lets talk about topics.  
The writers presented a neural model for creating morphological inflections that uses "hard attention " incorporating character alignments acquired independently using a Bayesian technique for transliteration purposes. This method stands out from the leading neural model in this area that uses "soft attention " dealing with character alignment and conversion together, within a probabilistic framework. 
The suggestion put forward is innovative. Has a strong foundation in place.The article is articulate. The practical assessment is comprehensive.However a significant drawback is that the suggested approach does not always attain top tier performance across every scenario.It demonstrates suitability for tasks focused on primarily monotonic alignment and less contextually sensitive aspects.To enhance the papers appeal it would be beneficial to expand on the real world benefits of the proposed technique,such, as its user implementation and computational effectiveness. 