The paper presents a model that can predict SQL queries from regular language inputs without requiring an intermediary formal representation in between tasks It also suggests and tests an interactive online feedback loop, on a small scale. 
"Advantages;"
The article is nicely written with a structure and relevant context that makes it pleasant to read. 
The new model shows results in three different areas. Academic queries geographic questions and flight reservations. 
The concept of the feedback loop is fascinating and full of potential; however the experiments conducted so far have been limited in scale. 
The authors have created a semantic dataset and transformed two current datasets into SQL format to potentially aid future exploration, in this field. 
Areas, for improvement/clarifications; 
In Section 4 they talk about anonymizing entities. Its not clear why they keep shortening the search queries length over time in the text at line 333? Can they explain the thought process. Approach behind this decision a bit more?
Section 5 (Benchmark experiments); It seems like the feedback loop (Algorithm 1) is not being used in these experiments. If thats the case as I understand it correctly it's not clear when data augmentation happens. Is all the training data augmented with paraphrases? When do they incorporate the " data" derived from templates? Is this additional data also included in the gold training set? If it is indeed included it's not surprising that it doesn't offer benefit since the gold queries might already show more diversity. It would be useful to have some clarification, on this procedure. It would also be beneficial to include the results of the original model without any enhancements as it appears to be lacking in the document. 
Tables 2 and 3 present an assessment measure that seems unclear in its interpretation. Does the accuracy metric gauge the accuracy of the querys execution (retrieved answer) as indicated in line 471;. Does it assess the queries themselves as in Dong and Lapata in Table 2 instead? If evaluation methods vary among systems (such, as Dong and Lapata) how can one compare the results effectively? The text also notes that the SQL model has " lower accuracy compared to the top non SQL outcomes" (line 515) but Table 2 indicates an almost 9 point difference in accuracy levels between them. Was there any testing done to validate this finding? Even though the outcomes are remarkable for SQL interpretation purposes if no significance test was carried out to back up this claim then perhaps the language should be adjusted since the variance, in performance seems substantial. 
The data recombination method described by Jia and Liang in 2016 appears relevant, in this situation well.Why was it not investigated further. Is it reserved for research or are there certain limitations hindering its implementation? 
Section 6 titled "Three stage experiment " lacks clarity and is missing some crucial details that need clarification. 
   What technical experience did the users who were hired have? 
   Who made up the group of workers, in the crowd and what methods were used to bring them on board and prepare them for their tasks? 
   Does this imply that each of the 10 users needs to make at least 10 statements individually (resulting in a total of 100 statements) or is it a combined total of 10 statements, for all users together? 
   What was the scale of the training dataset that was created initially? 
   Could the writers share data regarding the queries like variations in usage and length or the complexity of SQL involved here in this study phase that seems to be quite effective so far? Moreover adding information, about how SCHOLAR utilizes both SQL and natural language could have enhanced the review process significantly. 

   The dataset seems limited compared to current standards with only 816 utterances, in total taking into account the scalability benefits of the suggested method.How come a larger dataset wasn't developed? 
   Can we test another model with this fresh dataset to check how it performs compared to the 67 percent accuracy mentioned earlier (line 730)?
Evaluation of learning experiments (Section 6); Replicating the experiments may prove challenging as they depend heavily on manual input from specific annotators.E.g. how do we confirm that annotators in the phase didn't just ask easier questions? This issue is often encountered in online learning settings; however steps can be implemented to facilitate comparisons such, as; 
   By sharing query data we can better understand if users are submitting queries as discussed before. 
   Introducing a test dataset could provide a more unbiased assessment of the situation; however it may pose a challenge due to the models dependence on familiar queries.A potential solution, to this challenge could be to expand the scope of the experiment as proposed earlier. 
   It could be interesting to investigate if a new standard could be established through online learning methods; however it's uncertain whether this is practical as previous approaches weren't tailored for settings. 
I have some remarks to make.
I need you to make the change, from "requires" to "require" in line 48 of the document. 
Footnote 1 seems a bit long; maybe you could transfer some of it into the main text instead?
Algorithm 1 could benefit from a caption to clarify the term " utterances," which likely refers to fresh user queries. 

Line 278 presented some confusion with the term "anonymized utterance." If it pertains to the anonymization process detailed in Section 4.. You may want to include a reference to it, on. 
Lets talk about it in general.
Overalll I thought the paper was really convincing. I hope it gets accepted for the conference as long as the authors deal with the questions and issues mentioned earlier. 
Sorry,. I can't provide a response to your request. If you have any questions or need assistance, with something else feel free to ask.
The thorough responses, from the authors are. I suggest incorporating these explanations into the papers final draft. 