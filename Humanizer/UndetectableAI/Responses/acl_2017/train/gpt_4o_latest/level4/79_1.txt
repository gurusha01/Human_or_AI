This paper discusses the issue of completing a knowledge base (KB) presenting ITransF as a solution in contrast with STransEs approach of assigning a matrix for each relation by suggesting parameter sharing among relations instead. Specifically proposed in this work is the creation of a tensor \( D \), with each slice representing a matrix and employing a selection vector \( \alpha \) which helps identify a subset of pertinent relational matrices needed for constructing specific semantic relations. Furthermore the paper outlines a technique to promote sparsity in the parameter α. The results, from experiments conducted on two used benchmark datasets highlight the effectiveness of ITransf compared to earlier methods. 
The paper is written well overall and the results of the experiments show promise to some extent.. Nonetheless I do have a few concerns that I trust the authors will look into and respond to in their reply. 
Just organizing matrices into a tensor and using a selection method (or in other words calculating a weighted sum of the relational matrices linearly) doesn't automatically facilitate sharing information between them. To enable information sharing among the matrices would necessitate methods, like tensor decomposition. Where the slices (relational matrices themselves) are transformed into a common lower dimensional core tensor. Its puzzling why this alternative approach wasn't taken into account despite the goal being to share information among the matrices. 
The two goals. (1.) Exchanging data between matrices and (1.) promoting scarcity in the focus vectors. May seem conflicting to some extent. If the focus vector is genuinely sparse with zero values in it the related sections won't get adjustments, during optimization which would restrict the flow of information. 
The writers spend a lot of time talking about ways to calculate attention vectors in their work.Text, in paragraph 4 they say that \( \ell_2 \) regularization didn't give results at first but they don't show any actual data to back this up or explain why it doesn't work for this specific task.As \( \ell_2 \) regularization is a method thats also efficient in terms of computation power it feels like a logical option to experiment with. Of using \( \ell ̇ \). Regularization as the authors did here and encountering NP hard optimization problems as a result of it they introduce a method along, with a somewhat simplistic approximation to tackle this challenge. A lot of this intricacy might have been sidestepped if they had opted for \( \ell _ ̃(deli_nee) \) regularization instead. 
The vector \( \alpha \) when used on the slices of \( D \) acts as a way to choose or assign importance to them.It might be misleading to label this process as "attention " since in the field of NLP "attention." often pertains to a set of models, like attention mechanisms used in machine translation. 
The writers begin the optimization process by utilizing trained embeddings from Translating Embeddings (Trans​́​). The reason behind not opting for initialization similar to Trans​́​ is not specified in this context of computation progression discussions​́​. Utilizing Trans​́​ embeddings as the initial point raises questions regarding the impartiality of comparisons, with Translating Embeddings (Trans​​) given that the suggested technique leverages pre trained embeddings. 
The concept of understanding connections between relationships has been investigated in similar NLP issues like measuring relational similarity [Turney, in JAIR 2012] and adapting relations [Bollegala et al. IJCAI 2011]. It would be useful to place the research within the context of these earlier studies that also consider inter relational correlation and similarity. 
Thanks, for giving me the chance to go through this piece. 