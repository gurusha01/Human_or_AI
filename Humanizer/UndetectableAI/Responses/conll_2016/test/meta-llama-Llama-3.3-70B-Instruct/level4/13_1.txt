This study suggests using a neural network (CNN) method to link events by creating vector representations from word embeddings and incorporating linguistic traits to determine a coreference score. The system employs a one layer network to calculate similarities among event references and is assessed using the ACE dataset along with an extended edition and demonstrates outcomes similar to earlier systems, with rich features. 
I believe the main achievement of this study is how it introduces a method for connecting entities that combines word embeddings with language features to show that using only word embeddings is not enough, for the best results achieved in this area of research and development work. 
The setup, for the experiment was as follows; 
The authors acknowledge the use of predetermined gold trigger words of predicted ones and offer a rationale for this decision which could potentially influence the fairness of comparisons, with other systems relying on predicted triggers. 
It is worrying that there are variations, in the way train/test data is divided in research papers; its suggested that authors stick to common divisions whenever they can. 
I'm not sure I understand all the details. Could you clarify?
The data supporting the importance of information between sentences is strong; however the message in the paragraph (lines 65, to 70 ) lacks clarity. 
The way position embeddings are created is likened to word embeddings but lacks clarity in its specifics. It's uncertain if they start off randomly or, through lexicalization and why neighboring words relative positions should have identical embeddings remains ambiguous. 
The explanation regarding the use of neighboring elements on both sides to generate representations (lines 307–311) lacks clarity. Leaves uncertainty about its impact solely, on the max pooling process. 
The choice to include word embeddings of a word both before and after the trigger words appears random without any explanation, for this decision being given. 
The mention of the event representation "ve”, in line 330 is puzzling since later sections only mention "v {sent + lex}".
In section 3 of the document lacks documentation concerning how pairwise features are integrated. Specifically when it comes to how binary features and the distance feature are encoded and if they remain constant during training. 
Additional. Recommendations; 
Exploring the usefulness of this method for resolving entity coreference could lead to valuable insights by allowing comparisons with established research and widely used datasets such, as Ontonotes. 
Exploring the function as a nonlinearity is quite interesting; its uniqueness and the possibility of applying it to other tasks are definitely worth delving into. 
Releasing the ACE++ dataset would make it easier to compare with approaches and evaluating feature rich systems on this dataset could be advantageous. 
Significance testing could be used to support the comparisons made between the results. 
The section on research could benefit from including the neural network method developed by Wiseman et al. in 2015 for resolving coreferences, in entities. 
There are some concerns.
In line 143 the use of "that'''s unnecessary. 
There is a discrepancy in using " type" for a baseline in table 6 and "same event”, in the text (line 670).
Sources; 
Wiseman et al. in their study titled "Learning Anaphoricity and Antecedent Ranking Features, for Coreference Resolution " presented their research at ACL 2015. 