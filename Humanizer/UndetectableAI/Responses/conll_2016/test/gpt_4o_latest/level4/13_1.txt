This study suggests a framework for connecting events using neural networks (CNN). The writers create vector representations for event references by running word embeddings through a CNN and applying max pooling afterward​. These representations are combined with word embeddings, from the neighboring context​. Alongside pairwise characteristics​. The system employs a layer neural network to calculate a similarity vector and determine a coreference score.  
The method was tested using the ACE dataset and an enhanced edition with results to previous sophisticated systems performance levels The key innovation in this study is introducing a neural approach for linking events that combines word embeddings with language characteristics While word embeddings alone are found to be inadequate, for achieving high performance levels according to the research the linguistic features utilized are basic and do not depend heavily manual external resources

The writers use key words instead of anticipated ones, in their work justification as acceptable practice choices. However I would have liked to observe outcomes utilizing expected prompts because one of the fundamental systems employs forecasted prompts resulting in a less fair comparison. 
I find it worrisome that various studies use train/test divisions.I suggest that the authors stick to known divisions, for consistencys sake whenever feasible. 
I'm not sure I understand all the details.
The assertion that information spanning sentences is essential is backed by data; however the conclusion in the latter part of the second paragraph (lines 65, to 70 ) lacks clarity. 
The positions embeddings are said to be created reminiscent of word embeddings without an explanation of the specific procedure – are they initialized randomly or based lexically for instance?. The reason behind words sharing an embedding when, in the same relative position is not obvious. 
The method of including elements on both sides in the depiction (lines 307–311 ) seems confusing, to me. Does this only affect the step involving max pooling ?
Why was the choice made to include word embeddings for a word before and, after the trigger as it appears to be somewhat random compared to other possibilities available? 
The purpose of the event mention representation \(ve\) mentioned in line 330 is not well defined in the context provided raising questions about its relevance in comparison to the emphasis on \(vsent + lex\) in the following sections without any reference, to \(ve\) thereafter. 
In Section 3... Can you clarify how pairwise features are used? Do binary features get represented as a vector? How do we deal with the distance feature? Are these features kept constant while training? 
Other. Recommendations 
Is it possible to expand the suggested method to include entity coreference resolution well which would allow for comparisons with previous studies and datasets such, as Ontonotes? 
Using a function as a nonlinearity is quite interesting don't you think?. Could this be considered an idea?. Can you envision where this function could be applied in different tasks?.
When it comes to datasets and their accessibility to the domain. One dataset is openly accessible while the ACE++ dataset remains unavailable for now. Any intentions to make ACE++ accessible in the future to allow for comparisons, with upcoming methods? It would also have been beneficial to assess feature systems using this dataset for a more comprehensive analysis. 
Several findings mentioned in the study are quite similar, in nature. Employing statistical significance tests could enhance the assertions put forth in the research paper. 
In the section discussing research findings it could be beneficial to include the method, for neural coreference resolution introduced by Wiseman and colleagues in 2015. 
Slight Problems

Table 6 includes a baseline referred to as " type " but in the text (line 670) it is mentioned as "same event." This discrepancy needs to be resolved. 
Citations 
"Studying Anaphoricity and Antecedent Ranking Traits for Resolving Coreferences." Authored by Sam Wiseman and colleagues (Alexander M Rush et al.) this research was presented at the ACL conference in 2015. 