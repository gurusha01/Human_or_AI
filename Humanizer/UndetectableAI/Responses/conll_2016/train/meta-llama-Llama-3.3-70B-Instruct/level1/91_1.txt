The study suggests a efficient technique to condense Neural Machine Translation (NMT) models by employing weight pruning based on magnitudes. The researchers explore three pruning approaches. Class blind pruning is shown to be more effective compared to class uniform and class distribution methods. They show that a NMT model with than 200 million parameters can be pruned by 40% maintaining performance levels without significant loss; furthermore with the inclusion of a retraining phase in the process they are able to prune 80% of the parameters without any impact, on performance. 
The key accomplishments of this study include; 
The researchers demonstrate that using magnitude based weight pruning is an approach, for condensing NMT models by attaining a substantial compression ratio while experiencing minimal performance decline. 
They. Evaluate three different pruning methods and conclude that pruning without considering class is the most efficient approach. 
The writers offer perspectives on how redundancy's spread within NMT structures by emphasizing the significance of upper levels as well, as attention and softmax weights. 
The paper excels, in the following areas; 
The authors thoroughly assess their approach by comparing pruning methods and analyzing the distribution of redundancy, in NMT structures. 
The findings show that there is a reduction in file size without much impact on performance quality when using this technique to implement NMT models on devices, with limited resources. 
The authors have effectively. Presented their work in a concise and coherent manner for readers to easily comprehend and engage with. 
The paper has its flaws, including; 
The authors have not extensively compared their approach with compression techniques, like low rank approximations or knowledge distillation methodologies. 
The authors did not study how pruning affects the interpretability of the NMT model in their research paper; this aspect could be crucial, for use cases. 
The authors haven't clearly explained why class blind pruning performs better than the two methods; this could be an intriguing topic, for future research. 
Queries, for writers; 
Could you please give me information regarding the computational resources needed to prune and retrain the NMT model? 
How do you intend to leverage the nature of the pruned models to accelerate both training and runtime processes? 
Could you offer an in depth analysis comparing it to other compression techniques, like low rank approximations or knowledge distillation? 