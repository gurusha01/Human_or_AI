In this research paper we investigate three methods for reducing the size of neural machine translation ( NMT ) models by removing unnecessary weights and found that pruning based on magnitude is more effective than other methods. Furthermore our findings indicate that performance can be restored to its level through retraining, after pruning even when a large number of weights are removed.
The main advantage of this paper is the straightforwardness of the method and the impressive outcomes it has produced.The writing is easy to understand. The paper effectively places the work in relation, to earlier research. 
However one noticeable drawback is the lack of originality in the research since it mostly utilizes a known method, for a different kind of neural network and job ( NMT ) producing outcomes that are not very surprising.  
The implications of the findings in terms remain uncertain at this point in time.The utilization of results would necessitate the use of sparse matrix representations that pose challenges when optimizing for GPU performance.Speed is highlighted as the limitation rather than memory in Neural Machine Translation (NMT) systems; thus casting doubt upon the significance of pruning techniques.It is suggested that any recent research addressing this matter be examined and clarified within the paper to elucidate the importance of pruning, in this scenario. 
One possible approach to overcome this challenge is to utilize the findings from pruning to inform adjustments, in the architecture design. For instance as depicted in Figure 3 there is an opportunity to slim down the hidden layers to two and potentially lessen the size of both source and target embeddings. 
Have you considered looking into how pruning with retraining and dropout are connected? For example " A Grounded Application of Dropout in Recurrent Neural Networks " a paper by Gal on arxiv, in 2016 might offer some valuable theoretical insights. 
I appreciate the feedback provided. Thank you for taking the time to share your comments.
On line 111 you might want to consider using "output embeddings" of "softmax weights" as it could be a more accurate term. 
  
In Section 3 describing n as the networks "dimension" could be confusing The idea that parameter sizes are multiples of n might suggest a limitation that doesn't necessarily apply in practice. 
Make sure to reference Bahdanau et al.s work for the attention mechanism of Luongs since they were the ones who first introduced the concept. 
In Section 4 of the report titled "Differentiating class uniform and class distribution pruning techniques shows variance, in outcomes." It may be beneficial to consider eliminating one of these methods based on their effectiveness. 
Lets consider looking into a mix of pruning approaches for classes and embeddings, in Figure 3. 

Section 4 subsection 4 and the corresponding Figure 6 need clarification, on the pruning technique being implemented. 
Figure 7 indicates whether the loss relates to the data used for training or, for testing purposes. 
It looks like Figure 8 may be missing information on softmax weights and can be a bit tricky to understand at first glance. It might help to include some stats indicating how much of each class is eliminated through class blind pruning at various stages, for better clarity. 
Make sure to reference the work by Le et al., titled "An Easy Method to Start Recurrent Networks of Rectified Linear Units " published on arxiv in 2015 as a source. 