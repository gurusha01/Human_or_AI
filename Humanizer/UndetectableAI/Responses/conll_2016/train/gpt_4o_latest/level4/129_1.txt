The research paper introduces a method for choosing training data for machine translation using a CNN classifier to assess and order sentences from a range of topics. The thorough comparison to studies that use language models based on continuous or n grams is well done. However it remains uncertain if the paper also tests its technique against approaches that select data like comparing the changes, in cross entropy sums.
At first it wasn't clear why a CNN was chosen over an RNN/LSTM in the studys rationale. The paper effectively makes a case for CNNs strength, in pinpointing key parts of text or sentences – a notable advantage indeed! However the paper didn't. Show which representation. BOW or SEQ or a mix of both. Is more crucial and why. 
The description of the CNN structure is quite clear and informative as it delves into the utilization of one semi supervised embeddings with pre trained vectors while emphasizing crucial elements, within it; nonetheless a graphical depiction showcasing the assembly of inputs in the CNN layers could greatly improve comprehension. 
In terms the document is nicely composed; however there are some inconsistencies in the application of citation styles, like \citet and \citep that need to be addressed as noted in line 385. 
The tests and assessments mostly back up the arguments in the paper; however there are reservations about how the count of chosen, in domain sentences (line 443)'s figured out using a distinct validation set. 
What kind of data is used to validate this process ? Also unclear is how the CNN models hyperparameters are selected and how impact these choices have. 
Table 2 would be best used to compare how various methods perform when using the number of sentences selected for evaluation purposes if possible. Based on the data presented in Figure 1 and its implications about the proposed methods superiority over methods in similar scenarios. A comparison, like this could provide added depth to our assessment. 
Extra. Remarks; 
"I think it would be really cool to conduct a study that compares the suggested method with existing standards using an amount of relevant data, beyond just what is used in the initial phase."
In the results or discussion section you could incorporate sample sentences chosen using methods to reinforce the arguments presented in section 5·⁴. 
In section 5 of the discussion when discussing the concept of moving beyond surface appearances a good reference point to consider is Axelrods study from 2015, where specific words are substituted with parts of speech tags to address the issue of limited language model training samples for analysis purposes.This comparison could shed light on any benefits that word embeddings from word vectors may provide in comparison, to this method. 
Using the combined scores from both the source and target classifications is similar in concept to the Lewis Moore LM data selection approach, which involves summing up discrepancies in cross entropies between data points for reference purposes, around line 435. 
Finally,\* it might be intriguing to investigate if the CNN model could be expanded into an parallel environment to determine weights, for merging the classification scores of both the source and target.\*