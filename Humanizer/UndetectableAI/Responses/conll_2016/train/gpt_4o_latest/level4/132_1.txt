It could be quite intriguing to see how word embeddings like word vectors (wordvec) and Latent Dirichlet Allocation (LDA) can work together in this context; however the main problem with this research paper is the presentation of its technical aspects in Section  two. It's essential to revise Section  two so that readers who are familiar, with word embeddings and LDA can easily grasp how these models are combined at a high level. Something the current explanation doesn't achieve effectively. 

The third section, in the introduction is a bit confusing. What exactly is meant by "deriving an approximation"? What are we approximating here specifically?" Also I'm unsure why creating prototypes is mentioned as being time consuming and why evaluating features is seen as the task. 
Why do we use the word vectors for both pivot and target words in this scenario instead of using separate ones as in the case of word embeddings like word 20vec? What is the reasoning, behind making this choice? 
Could you explain the concept of extracting words from a distribution? 
Could you please explain what co adaptation means, in this situation? 
The statement "If we have only considered the structure mentioned far " lacks clarity. What specific type of structure is being discussed here? 
The term "its similarity' needs to be changed to "its similarity."
Note 1 reveals the identity of the individual involved. 
The lack of an evaluation in the paper is concerning as just presenting example clusters is no longer considered satisfactory, in NLP research nowadays.The quantitative evaluation hinted at in Figure 2 appears unclear. Gets lost within a lengthy caption that lacks adequate explanation. 
The assertion made in the conclusion stating that the model effectively addresses word analogies is exaggerated.The document merely showcases a few examples, like king + queen to support its argument without fully validating the overarching assertion. 
The mention of Chang in the citation provides the location as "Advances, in... ". It lacks completeness by not specifying the full conference or journal title which could be challenging for readers to infer accurately. 