In this paper four techniques are presented to create word embeddings that work across languages along, with a revised QVEC measurement to assess their performance. 
The multiCluster technique utilizes a dictionary to categorize words into multilingual clusters and subsequently calculates cluster level embeddings to represent all words, within each cluster. 
Expanding upon the embedding method introduced by Faruqui and Dyer (2014 this technique broadens its application to the multilingual domain by leveraging English embeddings as the foundational space of reference alongside utilizing bilingual dictionaries to map embeddings from various languages onto the common anchor space. 
The multiSkip technique expands on the embedding method described by Luong et al (2015) incorporating source and target context alignment into a multilingual framework and enhancing the objective function by incorporating terms, from all parallel corporas that are accessible. 
Translation invariance is achieved by utilizing a low rank breakdown of the word PM cooccurrence matrix while integrating bilingual alignment frequency elements into the goal setting process This technique might only be suitable, for bilingual word representations. 
The assessment approach utilizes CCA to enhance the connection between word representations. Possibly manually created linguistic information. Primary vectors are obtained for aspects to generate a rating that remains unaffected, by rotation and linear modifications. The suggested assessment technique is also expanded to accommodate environments. 
In general the document is nicely. Effectively conveys the suggested approaches. Nevertheless there are a couple of issues; 
The unique aspect of Translation Invariances novelty lies in its comparison with the method outlined by Gardner et al. concerning multilingual embeddings extension; hence the authors should elaborate on this distinction, for clarity. 
Concerns are being raised about the utilization of sense annotations in various languages due to potential sparsity in feature intersections across languages.How do the authors tackle this issue beyond the reference, in footnote 9? 
In Table 2s coverage analysis for dependency parsing scores of multiCluster and multiCCA are comparable despite differing coverage levels. Prompting the need for clarification, in this regard. 
Table 3 shows outcomes where multilingual embedding techniques often do not surpass others on internal measures according to the text analysis conducted in the study report mentioned above. One notable observation is the underperformance of these methods compared to the translation invariance approach in word translation tasks, within the language domain. A key focus of this research endeavor. It's quite astonishing that the multiCluster approach performs the best, on measures despite not taking into account inter cluster semantic details from both words and language. 
Could I ask the authors a few questions?
What happens to the accuracy drop when keeping word embeddings fixed in the dependency parsing job compared to the improvement in performance when substituting them for embeddings, in the LSTM stack parser? 
Is Table 3 the mean, across all 17 embeddings mentioned in Section 5 of the document? 
What benefits does the multiSkip method offer over learning embeddings and then applying multiCCA projections, in separate spaces? 
Did the authors try using multilingual dictionaries instead of the dictionary extraction method that relies on parallel corporas or Google Translate as it may not fully capture the complexities of real world lexicons? 