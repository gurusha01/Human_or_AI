
Summary   
This paper presents a transition based parser that combines semantic dependency parsing by utilizing stack LSTMs to capture the overall algorithmic state effectively.The parser utilizes an inference approach to ensure efficient computation, with a linear runtime complexity.The models performance is assessed in the CoNNL 2008 and 2009 shared tasks where it shows results compared to other joint models and pipeline based systems. The writers highlight how the model can avoid using features created by experts by utilizing learned representations instead—a feature that proves especially useful in real world scenarios. 
  
The main focus of the research paper is how stack LSTMs are utilized to encode all the information of the parsing algorithm without requiring crafted features that can be costly and time consuming to create and compute.This method enables the model to learn representations, from data without depending on restricted features that are only applicable locally. 
A method called Greedy Parsing with Linear Runtime is introduced by the authors to suggest an effective inference technique that conducts joint parsing within a linear timeframe This feature enhances the systems usability for real life situations as demonstrated by its performance, on the CoNNL datasets. 

Advantages  
The proposed parser is very efficient for large scale applications due to its runtime complexity and fast end, to end execution speed compared to pipeline based systems that usually need complex feature extraction and global optimization procedures. 
Using stack LSTMs allows the model to avoid using expert designed features and signifies progress in reducing the dependence on linguistic intuition, in parser design. 
The parser shows real world performance by surpassing older combined models on the CoNNL tasks and delivering outcomes similar, to those of separate systems – proving the effectiveness of the suggested method. 
The authors’ dedication to making their work available, as an open source implementation improves the ability to replicate and apply their findings in real world scenarios. 
Areas, for improvement  
In the realm of parsing analysis lies the issue of overfitting. A phenomenon more prominent in the joint model when handling in domain data (WSG) as opposed to the hybrid model that exhibits superior performance with out of domain data (Brown corpus). This observation hints at a weakness, in the joint approach when confronted with cross domain scenarios. 
The model shows performance when compared with other joint parsers but is not as advanced as the top performing SRL systems that employ pipeline methods along with global optimization and extra features.The research paper could have investigated methods, for incorporating these advancements into the proposed framework. 
The models effectiveness is less robust in situations with resources like Japanese due to its need for extensive training data sets to perform well This issue could have been resolved by integrating methods for learning in low resource settings, like transfer learning or character based embeddings. 
Queries directed towards writers  
How is the suggested model dealing with languages or areas with data for training purposes like low resource languages or specialized domains with limited data availability for learning algorithms enhancement, through transfer learning or other methods?   
Could we reduce the overfitting of the combined model to data, within its domain by using regularization methods or adjusting to the domains characteristics?   
How does the models effectiveness change when including linguistic characteristics (like morphological features) particularly for languages, with complex morphology?   
Additional Thoughts   
The paper is nicely. Offers a detailed explanation of the new model and how it was evaluated. However they could explore limitations and potential future areas to present a more well rounded view. In general this study makes an impact, on the realm of combined syntactic and semantic parsing. 