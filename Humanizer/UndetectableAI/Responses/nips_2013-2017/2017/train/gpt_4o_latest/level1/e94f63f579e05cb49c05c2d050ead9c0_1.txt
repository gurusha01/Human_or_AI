The article introduces an approach called deep supervised discrete hashing (DSDH) designed for image retrieval to overcome drawbacks of previous deep hashing techniques by making better use of semantic details. The researchers suggest a single stream structure that combines pairwise labels and classification information to produce hash codes directly in the last layer of a convolutional neural network (CNN). In contrast to methods that either loosen binary constraints or utilize dual stream frameworks this technique maintains the discrete essence of hash codes, throughout optimization through alternating minimization tactics. The results from the experiments show that the new approach outperforms the hashing methods, on well known datasets like CIFAR10 and NUS WIDE. 
Advantages
The paper presents a method that involves setting the final CNN layers outputs, as binary codes directly—a unique approach not often explored in deep hashing techniques to prevent the common quantization error found in methods relaxing binary constraints. 
A cohesive approach is employed where classification and pairwise label data are combined in a framework to ensure the binary codes produced are ideal, for maintaining similarity and performing classification tasks effectively. 
Optimization Technique; The alternating minimization method adeptly manages the characteristics of hash codes—an intricate element of hashing based techniques. 
The research paper conducts experiments on two well known datasets and evaluates the results using various metrics such as MAP and precision recall curves.The findings consistently demonstrate that the new approach performs better, than deep hashing methods especially when an ample amount of training data is available. 
The research is in line with developments in deep learning based hashing and specifically tackles issues seen in previous approaches, like two stream frameworks and errors in quantization. 
Areas of improvement
The papers technical aspects are impressive. Could use clearer explanations in the optimization section to improve understanding for readers unfamiliar, with the topic. Especially when discussing auxiliary variables and the alternating minimization process. 
The tests were done using CIFAR 10 and NUS WIDE datasets which're common benchmarks but may not capture the full range of real world image search tasks accurately enough.. Including other datasets, like ImageNet or MS COCO would enhance the assessment process further. 
Scalability is not addressed in the paper concerning the expenses of the suggested method when dealing with extensive datasets; using an alternating minimization approach could result in additional costs compared to more straightforward relaxation methods. 
In comparison analysis results show that although the approach surpasses current techniques in some aspects for NUS WIDE data set the enhancements are only slightly noticeable according to the authors who suggest that this could be due, to the datasets label structure; however conducting additional analysis or studies focusing specifically remove certain components could provide more insights into this constraint. 
Reasons to Consider Approval
The paper presents an technically robust strategy that tackles significant shortcomings in deep hashing techniques. 
The results, from the experiment show progress and clearly outperform previous studies. 
 The contributions made are substantial. Push forward the current standards, in image retrieval. 
Challenges, to embracing the idea
The paper could benefit from some enhancements in terms of clarity and readability; in the optimization and technical segments. 
The assessment could be expanded to cover aspects such, as incorporating additional datasets and analyzing scalability further. 
Suggestion
This paper greatly enhances the field of hashing for image retrieval with its valuable insights and advancements in technical performance despite some areas needing more clarity and comprehensive evaluation analysis. I suggest accepting it with revisions to enhance clarity and delve deeper into topics, like scalability and dataset diversity. 