This research paper introduces an innovative method to optimize geodesically convex problems by adapting Nesterovs accelerated gradient descent technique, for Riemannian spaces instead of Euclidean spaces. The authors introduce two operators to enhance convergence rates in geodesically strongly convex and geodesically convex problems compared to traditional linear methods. The paper also discusses an example related to matrix Karcher mean issues and confirms the theoretical findings, with real world tests. 
Advantages; 
"Address an issue in the field by expanding Nesterovs acceleration to non linear Riemannian spaces. A valuable addition, to manifold optimization research."
The authors thoroughly examine the convergence characteristics of their approach and offer theoretical assurances align with the best rates in Euclidean scenariosâ€”a significant progression compared with previous studies like Zhang and Sras research, in 2017 which did not have expedited convergence rates. 
The methods usefulness in real life situations, like imaging and radar signal processing is shown through its application to matrix Karcher mean issues. 
The experiments clearly demonstrate that the new method is more effective than gradient descent (RGD) both in terms of how quickly it converges and its efficiency, in runtime compared to limited memory Riemannian BFGS (LRBFGS).
Areas, for improvement; 
The paper is very focused mathematically. Might be tough for readers not familiar with Riemannian geometry due to the complex sections, like the derivation of nonlinear operators and the geometric interpretation; simplifying explanations or adding diagrams could make it easier to understand for a wider audience. 
The experiments mainly concentrate on data related to matrix Karcher mean problems but it would enhance the paper if it includes outcomes, from a wider variety of geodesically convex problems or real world datasets. 
The paper lacks an examination of the computational burden imposed by the nonlinear operators in contrast to the usual RGD method, which could pose challenges, for handling extensive scale issues. 
In the paper there is a mention of the possibility of expanding the approach, to stochastic scenarios but it does not delve into or contrast with current stochastic Riemannian optimization methods. 
Reasons, in favor of approval; 
The article fills a void, in the research field by expanding acceleration methods to Riemannian spaces. 
The theoretical advancements are solid. Show definite enhancements compared to current approaches. 
"The results from the experiment confirm that the method we suggested is actually useful, in real life situations."
Reasons to oppose approval; 
The paper could be made clearer to better serve readers who may not be well acquainted with geometry. 
The scope of the evaluation is quite limited which might affect how broadly the results can be applied in general contexts. 
We should delve more into the trade offs and scalability aspects of computing. 
Suggestion; 
Based on my evaluation of the papers content and findings in the realm of optimization on manifolds, with compelling empirical results presented within it I suggest embracing it as a substantial theoretical advancement. Nonetheless I do urge the authors to contemplate enhancing the lucidity of their exposition and expanding the scope of their assessment in forthcoming iterations.