This paper discusses how reinforcement learning agents can be taught using feedback from people in the context of image descriptions instead of experts like professionals or specialists.. The writers introduce a model for creating captions that breaks down the process into smaller parts and incorporates input from humans who are not experts in the field.. While previous methods have focused on rewards or yes/no feedback. this study uses sentences to offer more informative cues, for learning. pointing out mistakes and offering ways to fix them..The new framework includes a feedback network (FBN) to assess the accuracy of phrases in generated captions using input and incorporates this into policy gradient optimization techniques.The study shows that their method surpasses RL models trained with conventional ground truth captions by enhancing BLEU and ROUGE metrics performance.The research also emphasizes the effectiveness of natural language feedback, in decreasing human involvement efforts compared to approaches. 
Areas of expertise; 
The research paper presents a method of integrating natural language feedback into reinforcement learning for generating image captions—a topic that hasn't been thoroughly investigated before The hierarchical phrase based RNN is especially effective, for incorporating feedback in a detailed manner. 
The emphasis on empowering individuals without expertise to direct agents corresponds with practical uses in everyday scenarios, like home robots or personal aides. 
The researchers carried out testing on the MS COCODataset to showcase how their technique enhances the quality of captions significantly.They also incorporated assessments, by humans to confirm the real world usefulness of their method. 
Releasing open source code and data promotes reproducibility and motivates ongoing research in this field. 
The incorporation of corrections into the reinforcement learning framework is effectively facilitated by a well designed feedback network that ensures clarity, in the integration process. 
Areas that need improvement; 
The paper shows how natural language feedback can be useful in image experiments but it would be good to see if it works well for other RL tasks, with bigger action spaces too. 
The paper mostly contrasts its method with RL and MLE baselines and suggests that including comparisons with other approaches involving human input, like TAMER or policy shaping could bolster its arguments. 
The Feedback Network Dependency relies significantly on feedback that is annotated and may not be easily scalable, for tasks that involve intricate or domain specific adjustments. 
Human Effort Evaluation; Though the document acknowledges decreased human engagement levels​, an examination of the balance, between feedback effectiveness and annotation labor could offer more profound understandings. 
Suggestion; 
This study adds insight to the realm of human involved reinforcement learning by showcasing how using natural language feedback can boost agent performance effectively and efficiently The suggested approach is solid in its technicality and rationale while tackling a real world issue However expanding the scope of the research and including comparisons, with other feedback driven RL techniques would amplify its significance I advise acceptance with slight revisions to tackle the identified shortcomings
Reasons supporting acceptance; 
A fresh and effective method, for incorporating human language feedback into reinforcement learning systems. 
The research findings are robust. Backed by thorough assessment. 
The writing is concise. The methodology is nicely organized. 
Reasons to Oppose Approval; 
The examination of how the findings can be applied to different reinforcement learning tasks is somewhat restricted. 
The absence of comparison, with reinforcement learning methods based on human feedback is notable. 