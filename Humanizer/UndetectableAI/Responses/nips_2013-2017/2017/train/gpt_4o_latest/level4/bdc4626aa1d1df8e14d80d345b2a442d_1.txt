This study explores a version of contextual linear bandits with a cautious restriction. This restriction entails that the total reward of the player at any point in time \( t \) should be at \( ( 𝟏−𝝰 ) \) times greater than that of a reference policy, for a given \( 𝝰 \). 
The authors talk about two situations in their study. One where the initial rewardsre clear and another where they are not known upfront.They create a UCB type algorithm based off of an existing linear bandit algorithm for each case and set limits for potential regrets in their analysis.These limits have two parts. The regret from the algorithm and the regret stemming from following the cautious constraint.Furthermore in their experiments, with the algorithm using known rewards it was shown to meet the cautious constraint effectively in real world scenarios. 
The limitations of the approach seem acceptable along with the suggested methods and regret boundaries derived from them appear satisfactory as well. That said. It would be beneficial to enhance the clarity of the graph presented in Figure 1(a) specifically highlighting the conservative phases of CLUCBs. Additionally. Conducting experiments, for CLUCBs operating in real world scenarios could provide valuable insights. 