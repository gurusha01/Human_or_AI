The study presents a method that incorporates input from people into reinforcement learning (RL) focusing on image captioning as an application area. The researchers collect assessments of captions generated by machines. Scoring them based on quality (ranging from ideal, to significant errors) and making corrections where needed when captions fall short of perfection or acceptability. They then train a feedback network to mimic evaluations by evaluating the accuracy of phrases. The generated input is used in the reinforcement learning training phase to determine rewards based not from automated metrics, like weighted BLEu but also from evaluations mimicking human feedback provided by the network system in simulations conducted by the authors using the COCO dataset. 
The paper discusses an issue but could benefit from enhancements, in various aspects.
The uniqueness of the paper isn't clearly expressed enough to grasp its significance at first glance.The idea of integrating input into reinforcement learning (RL) training is interesting; however the way this is carried out doesn't adhere to a real time human involved approach.Instead it follows a dual phase batch process, for labeling.As a result the authors depend on a simulator (referred to as the feedback network) to mimic feedback during RL training a method already commonly used in various RL scenarios. Moreover in the section discussing research it is highlighted that employing reinforcement learning to enhance non differentiable metrics is not a groundbreaking addition, to the field. 
The main findings are shown in Table 6 from tests done using the COCO dataset. However the basic performance falls well short of top notch standards ( for example state of the art models have BLE U.  1 Scores around 30% whereas the Table 6 baseline reaches only 20%). Additionally the enhancement gained from including feedback is under half a percent, in BLE U. 1 Which is probably not statistically meaningful. 
The paper doesn't provide in depth analysis and concrete examples to show how feedback plays a role in the RL framework effectively. For example if both the RL baseline (RLB) and the feedback enhanced model (RLF) attain scores are they making comparable types of mistakes?. Does the feedback mechanism result, in distinct predictions? These valuable insights are absent. Would enhance the papers overall value. 