This paper presents a method that uses a model based approach to perform independence tests on independent and identically distributed (iid) data sets. The main idea is to use a neighbor bootstrap technique to generate samples that mimic the distribution \( f^{CI} \). Subsequently a classifier is. Assessed to determine its ability to differentiate between the actual data distribution and the nearest neighbor bootstrapped distribution. If the classifiers accuracy is similar to chance prediction results occur when it correctly guesses the null hypothesis that the information shows conditional independence is accepted or rejected. 
The writers establish limits on how similar the nearest neighbor bootstrapped distribution's to \( f^{CI}\) with a focus on total variation distance closeness measurements They also offer insights into the risks associated with empirical data, in optimal classification scenarios and when dealing with almost independent samples. 
In terms and its analysis of a major issue are clearly and understandably laid out in the document. 
I'm providing some feedback.
Important; 
The suggested approach seems to involve two parts; creating samples that mimic \( f^{CI} \) and using a classifier for making decisions. I'm curious to know whether we could substitute either of these stages with methods already found in studies. For example can we integrate the permutation based technique described in [7], with the classification centered strategy to address this issue?. Is it possible to match the nearest neighbor bootstrap with a kernel two sample test? Lets see how these different combinations work out. 
In real world scenarios with samples we observe that the nearest neighbor bootstrap distribution can closely estimate \( f^{CI}\).. What happens if the true relationship between \( x \) \( y \) and \( z \) suggests only a weak dependency, between them? How does the suggested approach address this situation? 
Is the process symmetric when considering both \( x \) and \( y \)? Looking at it from a cause and effect angle. If \( z \) could \( x \) and \( y \) be linked in some way due to two cause and effect scenarios?; Either \( x \) leads to \( y \) or \( y \) leads to \( x \). In these situations should a choice be made, on which variable to study closer? The authors have yet to tackle this problem. 
I am quite worried about the method used to choose the parameter \( \tau \ ), in Algorithms 2 and 3. 
There are some issues that need to be addressed.
In Algorithms 2 and 3 appears to suggest that the empirical risk ought to be divided by the size of the sample. 