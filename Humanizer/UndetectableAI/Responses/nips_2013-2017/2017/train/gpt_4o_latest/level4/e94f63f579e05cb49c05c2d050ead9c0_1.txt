The research article presents a deep hashing technique tailored to situations where the dataset contains class labels for reference during training sessions. The models parameters are fine tuned through a loss formula that considers both pairwise similarities and the distinctness of classes within a linear framework. A significant aspect is its manipulation of binary embeddings without resorting to relaxation methods by employing alternating minimization strategies. The outcomes surpass comparison models across dual datasets with an extensive examination of different design considerations provided in the paper. Overall clarity and coherence make the content easily comprehensible, for readers. 
I'm not an expert in this area so my main focus is, on how new and different this is compared to what has been done in studies referred as [9​] [17​] [21]. It seems like the unique aspect here is adding the classification element to the loss function. Moreover it seems like the whole method is complex and might require a lot of computing power to train (how long does it take?).