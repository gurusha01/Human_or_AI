In this study we introduce a phase decoding approach for Neural Machine Translation (NMT) and summarization tasks. The idea is clearly. The results, from experiments show significant enhancements compared to robust and practical reference points. 
The writers estimate the impact of the hypothesis by using Monte Carlo sampling technique; however it's not clear how many samples they use and if this parameter affects the results in any way Does the number of samples have an impact here Does the authors use beam search method instead of simple sampling These technical aspects need more explanation to be better understood
The paper presents an argument and the results from the experiments are significant in nature; however I felt that the introduction was too ambitious, with two paragraphs dedicated to a "Cognitive' justification as if reinventing an established concept. Over the years researchers have delved into step decoding. While it's praiseworthy to suggest a solution for neural models from start to finish, without disregarding previous efforts or trying to rationalize the method with shallow cognitive reasoning (keep in mind that "science'' is part of " cognitive science").
Page 2 has a sentence, on line 73 that doesn't contribute anything and could be left out. 
Although the assertion holds true for end to end models a well established practice of multi pass decoding exists in the realm of speech recognition. To illustrate this point further initially a word lattice is constructed based on "acoustic and language models, which is subsequently enhanced through progressive iterations with more advanced models. Consequently it would be inaccurate to suggest that this methodology has not been investigated for sequence forecasting. Dismissing research merely due, to the introduction of a new term seems unwarranted. 
On the page;   
