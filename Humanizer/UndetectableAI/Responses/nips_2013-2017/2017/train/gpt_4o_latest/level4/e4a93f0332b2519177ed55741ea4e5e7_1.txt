The research paper presents a RNN design known as the Fast Slow RNN and showcases enhanced results across various language modeling datasets. 
Advantages;   
The suggested method combines the advantages of an extensive transition matrix (fast RNNs) along, with a shorter gradient path (slow RNNs).  
The approach is straightforward and flexible. Can be used with any type of RNN cell.   
Areas needing improvement;   
The initial sections of the paper are quite complex to understand as the authors mention previous approaches without offering clear explanations, for each one. For instance;   
   In line 43 of the document discussing the LSTM models in Figure 2(a) there is ambiguity surrounding the characterization of the transition from a stacked LSTM to a LSTM in Figure 2(b). The functions of \( h_{ t. 11 }^{ 15 } \ ) In Figure 2(b) well, as the significance of \( h_{ t. 11 } \ ) In that diagram remain unclarified.   
   On line 96 of the document it's unclear what is meant by "our hierarchical layers zoom, in time " making it challenging to understand the following sentence as well.   
The assertion about scale operation seems a bit misleading as the fast and slow RNNs don't function at separate physical time scales but rather at logical time scales when the stacks are structured in the computational graph sequentially. As a result the main benefit seems to be, in decreasing the length of the path by using the slow RNN.  
To tackle the problem of gradient path length in stacked RNN models, like Residual Units or fully connecting the stacked cells could be used as options.. The paper does not delve into or compare these methods.   
The data, from the experiments does not include deviations which makes it hard to determine how significant the reported improvements are statistically. 