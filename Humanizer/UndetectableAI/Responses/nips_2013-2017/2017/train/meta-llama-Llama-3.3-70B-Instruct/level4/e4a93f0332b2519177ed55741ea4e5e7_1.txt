This article presents a RNN structure called the Fast slow RNN that shows improved results, on various language modeling datasets. 
Areas of excellence; 
The suggested algorithm cleverly combines the advantages of an extensive transition matrix (fast RNN) and a shorter gradient path (slow RNN) leading to enhanced performance. 
The simplicity and flexibility of the algorithm make it suitable for use with types of RNN cells and enable its broad application, in various scenarios. 
Areas of improvement; 
The beginning of the paper is quite difficult to understand because it presents methods in a dense manner without providing clear explanations, for each approach especially the specific ones mentioned. 
The transition from the stacked LSTM shown in Figure 2(a), to the LSTM depicted in Figure 2(b) lacks clarity. Specifically concerning the management of $ h_{ t. 7 } ^ { 8... 6 } $ And $ h _ { t. 7 } $. 
Line 96 sentences about the hierarchical layers and their connection, to time are hard to understand. 
The idea of using scales seems a bit confusing because the slow and fast RNN models work based more around logical time frames when the stacks are arranged in sequence rather, than different physical time frames specifically defined for them. As a result the main benefit appears to be that it helps shorten the path of gradients through the RNN.
Alternative methods like using Residual Units or connecting stacked cells completely could potentially achieve a reduction in gradient path, with more simplicity; however; the paper does not delve into discussing or comparing these approaches. 
The absence of deviations, in the experimental findings makes it challenging to evaluate the statistical importance of the enhancements mentioned. 