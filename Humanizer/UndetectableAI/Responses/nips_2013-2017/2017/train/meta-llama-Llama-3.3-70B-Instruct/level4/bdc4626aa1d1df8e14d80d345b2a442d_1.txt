This study delves into a version of linear bandits that adheres to a cautious limitation requiring the players total reward at any given time t to be no less than (1 alpha) times the total reward of a predetermined baseline policy, for a specified alpha value. The researchers explore two situations. One involving known baseline rewards and another where they remain unknown. In each situation they encounter an algorithm inspired by UCB principles that expands on a pre existing algorithm designed for contextual linear bandits and establishes a limit on the disappointment experienced throughout the process consisting of both the natural disappointment, from the original algorithm and the extra disappointment resulting from following the cautious constraint closely held by them authors also run tests on the algorithm using predetermined baseline rewards to confirm that it effectively adheres to the cautious constraint. 
The conservative constraint introduction and proposed algorithms seem thought out and reasonable in their approach according to the context provided in the paper. However there's a call, for a revision of Figure 1(a)â€”it could be improved to clarify the conservative phases of CLUCBC. Additionally it is suggested that a similar experimental evaluation be carried out for CLUCBC as it functions in a practical scenario to offer a thorough grasp of its performance. 