This article presents the idea of Population Matching Discrepancy which compares two distributions by calculating the Wasserstein distance between minibatches, from each distribution. This calculation can be done using a method that takes O(N³) time or an estimation method that takes O(N²) time.
The papers advantages are; 
The exploration of calculating the Wasserstein distance is interesting; however the quality of the generated images doesn't exceed those created by the Wasserstein GAN. 
Several shortcomings have been pinpointed.
Calculating the distance might need a N to get a precise value for the Wasserstein distance, between two different multimodal distributions and could result in challenges when trying to compare a mix of Gaussian distributions and understanding their variances effectively. 
If N is not large enough during optimization procedures; it could lead to reaching a minimum that deviates from the actual distribution pattern and consequently lead to a learned distribution, with reduced entropy levels as seen in the limited variety of SVHN samples shown in Figure 4 where digit 8 frequently occurs. 
I found some mistakes; 
I noticed a typo on Line 141; it should say "usually " not "usally."
After the response was given by the authors of the study in question I found that they provided experiments which were well received by readers. However it would be beneficial for the authors to give an explanation regarding the limitations of both Population Matching Discrepancy (PMD) and Maximum Mean Discrepancy (MMD). It is worth noting that MMD can work effectively with a batch size small as 2 and can be trained using Stochastic Gradient Descent (SGC) provided that the unbiased estimator of MMD from the original "A Kernel Two Sample Test" paper is used; this allows MMD to reach convergence, towards the correct distribution even when using small minibatches. On the side PMDs do not have a reliable way to estimate gradients without bias which points out a notable gap, in the abilities of these two techniques. 