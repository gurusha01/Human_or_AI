This document outlines a method, for incorporating selective categorization features into an established neural network system consisting of three main stages; 
By choosing a scoring method to measure how certain the network is about its predictions and comparing MC dropout scores for networks trained with dropout to the softmax score for networks, with softmax outputs. The latter showing improved practical results. 

Using a search to find a score limit that guarantees the classifier achieves an error rate lower than the specified level with the confidence desired by utilizing a known bound on the true error rate (Lemma 2 ) and applying a Bonferroni correction, on the confidence level (Protocol 2).
The results of the experiment support the effectiveness of this method by showing a connection, between the algorithms input settings (target error rate) and the actual error rates observed in a test group. 
The paper shines in its ability to be easily applied in real world settings. This is especially evident with the softmax response score function that allows for integration with any pre trained neural network and the clear definition of the algorithms desired confidence level and error rate based on ideas from reference [5]. While the paper expands on existing ideas and techniques in use, within the field the thorough testing of these concepts adds substantial value to it. 
However it would be helpful for the paper to include references to emphasize the significance of using binomial search and setting limits on the classifiers error rate. In particular it would be interesting to explore what happens when choosing the score threshold as the value at which the error rate on a specific tuning set drops, below a certain level and to contrast these findings with those achieved by applying Lemma 3. The addition of a baseline like that would give a reason, for using the advanced methods discussed in the paper possibly boosting its overall rating. 
There's a problem, with using an uninitialized variable "r*" in Algorithm 1. 