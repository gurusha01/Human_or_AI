The authors show that different techniques mentioned in works to clarify individual model predictions fall under the category of "additive feature attribution." They suggest an additive feature attribution approach inspired by Shapley values and generate explanations known as SHAP values as a result of this method introduction. Furthermore they present a kernel called the Shapley kernel that enables the calculation of SHAP values, through linear regression using a technique named kernel SHAP. The writers also mention ways to enhance techniques like Deep Learning Important Feature Extraction by getting closer, to estimating Shapley values more accurately. 
Here is the review summary; 
Upsides; 
The article introduces an robust theoretical structure for handling model interpretations and fills a notable void, in the field since many current techniques were created on an impromptu basis. 
Versions of this paper have been referenced to support enhancements in techniques, like New DeepLIFFT already. 
Kernel SHAP proves to be an effective approach in estimating Shapley values when compared to traditional Shapley sampling method.It shows a reduction in variance with respect, to the number of model assessments as depicted in Figure 2. 
Lets focus on the downsides.
The Max SHAP algorithm has flaws, in its design. 
The Kernel SHAP proof provided in the supplement appears to be incomplete and hastily composed. 
There are errors in the paper such as conflicting runtimes for Max SHAP and mistakes, in equations. 
The debate over whether SHAP values offer a better method of explaining models is up for debate; some argue that the case studies presented in Figure 5 may have been selectively chosen to support SHAPs effectiveness over other methods mentioned in the paper without addressing concerns such, as the runtime required for kernel SHAP or the optimal number of function evaluations needed for optimal performance. 
Elaborate feedback; 
The application of Shapley values to explain models is an idea and the creation of kernel SHAP is a valuable addition because it reduces variability compared to Shapley sampling methods. Nevertheless there are some problems with the paper. The Max SHAP algorithm has a flaw in assuming that adding an input affects the outcome solely based on its deviation from the maximum reference value among previously included inputs, without taking into account the reference values of inputs that have not been included yet. This situation may result in SHAP values as shown by a specific example. 
The explanation for Kernel SHAP in the section seems to be lacking and unclear which makes it hard to grasp the concept fully.The computational validation given seems to be about comparing the Shapley values generated by kernel SHAP with those produced by the traditional Shapley value calculation for a particular model rather than checking the coefficients used in both approaches.The code, for this verification is also incomplete. Some methods referenced in the code are missing. 
The argument that SHAP values represent a reliable method of explaining models is up for discussion because they may not consistently match human intuition expectations in all cases.The illustration with LIME in Figure 5 doesn't provide a comparison and the authors could strengthen their case for preferring kernel SHAP, over LIME by directly comparing them. 
The part about Deep SHAP raises some concerns because the authors only cover two approaches; Linear SHAP and Max SHAP; however the Max SHAP algorithm is flawed in their explanation. Furthermore the reference to Low order SHAP is ambiguous as it might not be compatible, with a range of neural networks and running it on a GPU could considerably impede backpropagation speed. 
Despite these challenges mentioned above the integration of Shapley values, for model interpretation and the creation of kernel SHAP are contributions that have been carefully revised in the paper to tackle the raised concerns and rectify any problems that arose. 