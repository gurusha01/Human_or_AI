This study suggests a method for incorporating input into reinforcement learning for image captioning tasks by gathering human feedback on captions generated by machines and training a feedback network to replicate human assessment of phrase accuracy before fine tuning the captioning model using reinforcement learning supported by both automated metrics like weighted BLEU and simulated human feedback, from the network. Research studies were carried out using the COCO dataset to evaluate how well the model performs. 
While the challenge is interesting, in nature there are opportunities for enhancement to consider such as; 
The originality of the paper isn't completely obvious at glance.The idea of integrating feedback into reinforcement learning training is intriguing; however its executed as a two step batch process rather than a real time human in the loop setup.This requires using a simulator, like the feedback network to predict input during reinforcement learning trainingâ€”a method commonly used in various reinforcement learning scenarios.Additionally as mentioned in the literature review using reinforcement learning to enhance metrics is not groundbreaking. 
The latest findings presented in Table 6 about the COCO dataset indicate that the initial performance level is below the most advanced techniques available (for instance; BLE. 3 Scores usually linger at 30% whereas the starting outcome in Table 6 stands around 20%). Additionally the progress made by integrating feedback over the result is slightly under 0.,% as, per BLE.. Evaluationswhich is generally not deemed significant. 
The paper doesn't delve into an examination and demonstration of how feedback plays a role in the RL framework, with specific instances to illustrate this point clearly. For example; When the RL baseline (RLC) and the model integrated with feedback (RLF) produce scores; it's not evident if they are making identical mistakes or if the feedback results in entirely different predictions. 