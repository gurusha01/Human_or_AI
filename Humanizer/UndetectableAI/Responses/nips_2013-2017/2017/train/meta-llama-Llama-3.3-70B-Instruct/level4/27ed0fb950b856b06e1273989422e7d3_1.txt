This research introduces a model called the deep dynamic Poisson factorization model that enhances the conventional Poisson factorization (PF). It incorporates relationships by employing a simplified recurrent neural network (RNN). In contrast to dynamic PF methods this model utilizes the RNN to understand extended relationships over time effectively. The models inference process involves inference and includes an extra stage, for fine tuning the neural network parameters. The study showcases outcomes using five different real world datasets. 
Although the idea is interesting to me personally and captures my attention; however I think the paper needs some polishing in how its presented to meet the expectations of a conference such, as NIPS conference (Neural Information Processing Systems). I have provided feedback below for your consideration. 
Equation 4s formulation deviates from the RNN setup since \(h_t^{(n)}\) relies only on the upper layer memory vectors without considering past time steps within the same layer as typically seen in RNN configurations. It would be helpful if the authors could explain their reasoning, behind this simplified design choice. 
Section 2 is not fully detailed in its model equations as some aspects are unclearly explained such as the way the Gamma distribution in Equation 4 and the generation of \(\theta_t\), in Equation 3 are defined. 
"I have an opinion on the topic of 'implicit distributions discussed in lines 110 to 118. Equations 9 and 10 clearly outline Gamma distributions, with defined shapes and rates of implicit distributions as claimed. Equation 9 computes the distribution of \(\theta_{tk}\) given \( h^{ ( 0 ) }\) in a manner while Equation 10 does the same for \( h \). Based on these calculations they should not be categorized as distributions."
The statement in lines 25 to 26 about the methods limitations in analyzing data with long term patterns like extreme or catastrophic data is not clear enough for me to grasp fully; could the authors provide more details, on this aspect? 
The section from lines 133 to 155 is unclear when it comes to explaining what the "loss function'', in Equation 15 represents. 
In cases to Poisson factorization models, metrics such as MSE or PMSE may not be adequate. It is recommended to include predictive log likelihood values in the analysis, for comprehensive results. 
The selected value of \( K \) as mentioned in the experiments (located on line 207) seems to be on the side in my opinion.I recommend that the authors explore using values, like \( K = 100 \) as it could lead to more reliable results. 
The content mentioned in lines 224 to 232 lacks a connection, to Figure 3 and would benefit from additional clarification. 
The formulas used to update the inference process (Equations 11â€“14) could be moved to the supplementary materials without causing any confusion, in the papers presentation. 
The caption, for Figure 4 isn't very clear. Could use some improvement to make it easier to understand. 
It is recommended to follow writing principles when it comes to using the passive voice in your writing; this is particularly important, for academic or abstract writing. 
The writing quality is not up to par due, to typos and grammatical errors found throughout the text. 
 In some cases than when referring to "high dimensional " being precise, with the terminology is important. 
 Sorry for any confusion here is the revised version; "On line 22 of the document, in question; please ensure that the term 'factorize' is utilized accurately."
 Sorry, for the oversight! The phrase "property of the " on line 24 appears to be incomplete. 
 Lines 29 and 34 mention "Dirichlet " but the context is unclear. 
 I would suggest considering the use of "nested" in line 34 for alignment, with the context. 
 In Figure 1 of the document provided to us the terms "visual representation" and "transmission" could benefit from explanations. 
 The papers readability is affected by sentences like line 49 and the absence of articles in sentences such, as "Although Dirichlet distribution is often used as a prior distribution."