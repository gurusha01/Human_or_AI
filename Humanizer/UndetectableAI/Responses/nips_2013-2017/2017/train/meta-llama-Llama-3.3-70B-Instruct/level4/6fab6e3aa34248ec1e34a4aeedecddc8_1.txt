In general what do you think? 
This paper introduces an captivating idea that could spark interest in advancing research on early fusion methods across multiple modalities in the future. However the way the information is presented and written needs to be polished further. While the experiments show how well the approach works for tasks they are a bit limited in scope making it challenging to fully justify the proposed method beyond its application, in vision and language. To strengthen this submission I suggest refining the text. Carrying out additional experiments using different model architectures or diverse types of multi modal data. 
Advantages; 
The reasons behind the basis, for the CBN approach are solid and its straightforwardness is valued. 
The study that compares the effects of tuning batch norm parameters (FT BNs) and question conditioned batch norm predictions is quite insightful. It's interesting to see how adjusting to image statistics with FT BNs leads to notable enhancements of around 1 percent in VQA and 2 percent in Crop GuessWhich tasks; these improvements double when the adjustments are based on the specific question asked. Resulting in about 2 percent increase in VQA and 4 percent, in Crop GuessWhich tasks. 
Having access, to shared code to replicate the experimental findings offers a valuable benefit. 
The t SNE graphs provide insights by showcasing the significant influence of language specific adjustments to visual attributes. 
Areas, for improvement; 
It's not very clear why Section 2. 1 Is included because both Batch Normalization and the proposed Conditional Batch Normalization (CBM are techniques.The explanation of the proposed method seems unrelated to the choice of model. It would be more helpful to focus more on providing motivation and insight into the CBN approach rather than describing the ResNet architecture, in detail. 
The reason behind how early vision's improved by language modulation makes sense from a neurological perspective; however the explanation for adjusting normalization parameters seems less persuasive to me especially in Section 3 of the text provided earlier on this topic by introducing that the suggested method decreases overfitting as opposed to fine tuning but doesn't delve into CBN in relation, to other early fusion tactics as mentioned in the introduction. 
Since CBN is an approach that shows enhanced performance in various vision and language tasks using different model designs would be more persuasive, as evidence of its effectiveness.In particular CBN could be easily integrated into the MCB structure; however the issue of memory constraints arising from backpropagation within the CNN might pose some limitations. 
It's quite unexpected that implementing CBN in Stage 4 (the advanced stage) contributes significantly to enhancing performance in both the VQA and GuessWhat tasks. It would be helpful to delve into this topic in this section for a more thorough understanding of the results presented here. The additional figures provided are intriguing well indicating that separations based on questions in image space only manifest, after progressing through later stages.
"Figures 2 and 3 seem to cover information."
Notable details.
It would be intriguing to see how altering the questions impacts the way an images features are represented through a gradient visualization approach. 
Before citing sources it's better to include a space before the brackets, for reading. 
The emphasis, on models is not uniform. 
In Equation 2 there is a gammaj of gammac. 
Line 34 should be updated to "to allow the question to attend " of "to permit the question to attend."
Line 42 does not have a reference cited alongside it. 
The mention of batch normalization, in the line number 53 lacks a reference. 
Line 58 should be adjusted to "which we call," than "to which we refer as."
Line 89 should be revised to "'s achieved through a " instead of "is achieved a."