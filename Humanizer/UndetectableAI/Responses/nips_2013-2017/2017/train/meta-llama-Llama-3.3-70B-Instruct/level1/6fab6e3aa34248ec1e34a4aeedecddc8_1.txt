This research paper suggests a method to blend language and vision in visual question answering tasks by adjusting the entire visual processing pipeline with linguistic input.The writers present Conditional Batch Normalization (CBN) a technique that adjusts the batch normalization parameters of a trained ResNet based on a language embedding.This strategy named MODERN is tested on two datasets. VQA version 1 and GuessWhat?!.
The article is nicely. The writers present a strong reason, for their method by highlighting the drawbacks of existing VQA models that handle visual and language inputs separately. They explain the aspects of CBN and MODERN thoroughly and conduct a detailed examination of the outcomes which includes an ablation study and illustrations of the acquired representations. 
The papers strong points are as follows; 
A new fusion method has been suggested to adjust how visual information is processed by incorporating language inputâ€”a departure, from existing methods. 
Significant enhancements were showcased compared to established benchmarks, on two question answering datasets. 
The detailed examination of the findings with an investigation, into components and illustrations showcasing the acquired knowledge representations. 
The papers limitations are as follows; 
The use of a trained ResNet might restrict how widely the method can be applied to different architectures. 
The absence of a comparison, with other cutting edge VQA models employing varied fusion techniques is noticeable. 
The authors propose that MODERN could be applied to modalities and tasks beyond VQA; however additional testing is needed to validate this assertion. 
Reasons supporting acceptance; 
The research paper introduces an important idea, to the VQA domain. 
The method is clearly explained with motivation and thorough technical explanations. 
The findings show enhancements compared to well established starting points. 
Points, in favor of acceptance; 
The use of a trained ResNet might restrict how broadly the method can be applied. 
The approachs strengths and weaknesses might not be fully grasped due to the absence of a comparison, with other top VQA models. 
The potential impact of the approach may be restricted due to the assessment on tasks other, than VQA. 
In my opinion the paper brings an addition to the VQA field and should be accepted. Yet I recommend that the authors focus on improving the areas mentioned earlier by offering a more thorough comparison, with leading VQA models and testing the method on tasks beyond VQA. 