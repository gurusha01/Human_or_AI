This study introduces a technique called FREE SUCRL (FSUCRL) designed for mastering Markov Decision Processes (MDPs) with options by incorporating temporal abstraction into the reinforcement learning framework.The researchers tackle the shortcomings of approaches like RMAX SMDP and UCRL SMDP that necessitate familiarity, with the distributions of total rewards and durations of individual options. FSUCRL integrates the concept of Semi Markov Decision Process (SMDP) along with the Markov structure of options to calculate the steady state distribution of a connected Markov Chain (MC). This approach enables the determination of a policy, for each episode. 
The document is nicely. The authors present a concise overview of the issue at hand along with relevant research and their suggested approach. The, in depth technical parts are clearly. Explained well with suitable mentions of prior studies. Furthermore the authors offer an examination of the algorithm which includes a regret boundary that aligns with SUCRLs boundary plus an additional term. 
The papers notable aspects are; 
The writers highlight a drawback of earlier studies by enhancing the algorithms usability and relevance to real life issues. 
The article offers an easily understandable description of the algorithm and its various parts. 
The authors have conducted a theoretical analysis and offer a regret bound that can be likened with SUCRLs performance. 
The drawbacks of the document are; 
The effectiveness of the algorithm greatly depends on the selection and arrangement of its options; this aspect might necessitate planning and adjustments. 
The writers believe that the choices are clearly outlined and meet criteria; however this may not always hold true in real world scenarios. 
The article might be improved by conducting real world tests to include comparisons, with leading edge algorithms and diverse fields of complexity. 
Reasons, in favor of approval; 
The research paper introduces an meaningful addition, to the realm of reinforcement learning involving options. 
The algorithm tackles a drawback of earlier research efforts and renders it more feasible and relevant, to real life issues. 
The thorough theoretical analysis presented by the authors includes a regret bound that matches up with SUCRL. 
Reasons opposing the approval; 
The effectiveness of the algorithm could be influenced by the selection of choices and how they are organized; this might necessitate planning and adjustments. 
The writers presume that the choices are clearly outlined and meet criteria that may not always align with real world scenarios. 
The paper would be improved by conducting empirical tests that involve comparing with other cutting edge algorithms and exploring more intricate domains. 
In my opinion and assessment of the paper as a whole suggests that accepting it would be beneficial since it offers an advancement in the realm of reinforcement learning, with options and tackles a key drawback of earlier studies; nevertheless I would suggest to the authors that additional empirical assessments be conducted and that they consider addressing any potential shortcomings of the algorithm. 