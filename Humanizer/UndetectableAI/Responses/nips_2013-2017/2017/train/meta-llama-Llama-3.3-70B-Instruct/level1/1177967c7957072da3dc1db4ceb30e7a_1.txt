This research introduces an adaptive method, for importance sampling in coordinate descent (CD) and stochastic gradient descent (SGD). The authors suggest that while optimal gradient based sampling is theoretically sound it is often impractical to compute.To tackle this challenge the authors utilize estimates of the gradient and transform the task of selecting the best sampling distribution into a convex optimization problem. 
The papers key contributions are as follows; ( ̶ ̶ ̶ ) A versatile and effective adaptive technique, for importance sampling that can be used with CD and SGD approaches. ( ̶ ̶ ̶ ) Evidence supporting the superiority of the suggested sampling distribution compared to uniform or fixed importance sampling. (  )  An effective method to calculate the solution efficiently. 
The article is nicely written with an explanation of the approach, by the authors showcased in a clear and detailed manner. The theoretical analysis is comprehensive. Backed up by experimental results that effectively showcase the proposed schemes efficacy. 
Advantages; 
The paper suggests a new and effective adaptive importance sampling method that can be used with CD and SGD techniques. 
The writers thoroughly examine the aspects of the suggested plan and offer evidence of its effectiveness and efficiency. 
The results of the experiment show that the suggested plan works well in real world scenarios. 
Areas, for improvement; 
The paper suggests that it may not always be possible to calculate safe lower and upper bounds, on the gradient as assumed. 
The cost of implementing the suggested plan is O(n log n) which could pose a challenge, for handling datasets effectively. 
Reasons supporting acceptance; 
The research paper introduces a new and effective adaptive importance sampling method that can be utilized in coordinate descent (CD) and stochastic gradient descent (SGM) algorithms. 
The theoretical examination is comprehensive. The practical findings show how well the suggested plan works. 
The article discusses an issue, in the field of machine learning and suggests that the proposed method could enhance the effectiveness of CD and SGD algorithms. 
Reasons, in favor of acceptance; 
The paper suggests that it may not always be possible to calculate safe lower and upper bounds, on the gradient as assumed. 
The expense of implementing the suggested plan could pose a challenge when dealing with sets of data. 
In my opinion the paper is nicely. The new approach is both original and effective. The theoretical examination is detailed. The practical findings show how well the new approach works. However the paper could be better if it acknowledges the constraints of the proposed method like assuming that computing safe gradient boundaries efficiently is feasible.  
The quality rates at an 8, out of 10.
The text is very clear and easy to understand.
The originality of the content rates an 8, out of 10.
The importance of this is rated 8 out of 10. 
I suggest accepting the proposal with some changes needed such, as discussing the limitations of the plan and delving deeper into the computational expenses and the efficient computation of safe gradient boundaries both lower and upper bounds. 