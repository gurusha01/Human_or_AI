This research paper presents a method for training a sequence classifier without the need for labeled data by using sequential output statistics (language model). The algorithms functional form poses optimization challenges and a stochastic dual gradient technique is developed to address this effectively. Unlike methods that are more susceptible to reaching simple solutions and depend on a generative model for training purposes; the proposed approach in this study overcomes these limitations. The experimental outcomes, from testing on two real world datasets show that the new method achieves error rates when compared to standard methods. 
Here are some comments, for your consideration; 
What are the meanings of the lambdad and lambdap axes shown in Figure 2(a)?
The main concept of the paper and its approach using primal dual gradient seem persuasive, to me; however an improvement could be made by comparing it to other methods mentioned in references [11 and 30]. Additionally the evaluation would benefit from the inclusion of datasets to ensure its reliability and accuracy. 
It would be beneficial if you could clearly outline the formulas as detailed in step 5 of Algorithm 1. 