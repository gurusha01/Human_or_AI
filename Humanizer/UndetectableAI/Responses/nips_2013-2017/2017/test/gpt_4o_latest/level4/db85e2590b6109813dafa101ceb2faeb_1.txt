The authors suggest adding a term to make sure weight matrices have a simpler structure during training the network model.This method helps to compress the network and simplify the process of reducing ranks resulting in lower computational expenses, for inference.The paper is nicely. Explores an intriguing subject overall. I have a worries though. Firstly the writers don't touch upon variational inference which is crucial as it compresses networks by decreasing their description length in bits and allows for weight pruning post training; you can refer to 'Practical Variational Inference for Neural Networks. In a sense' most regularizers naturally encourage a kind of implicit 'compression aware training' since simpler models usually exhibit better generalization abilities and can be useful for pruning, after the fact. For example; Models trained with l1 or l3 regularization often end up with weights close to zero that could be removed without significantly affecting performance This difference is crucial because the authors use an l3 term in addition to their new regularization method during the training process Also missing from the paper is a comparison of how effective previous low rank post processing methods are when used with or, without the new regularization approach There is also no mention of benchmarking against other common regularization methods found in current literature 
I had a bit of trouble understanding the results presentation which made me possibly miss out on findings.I think it would be more helpful to have a graph showing accuracy plotted against compression ratio using parameters or MAC counts of against regularization strength.This type of graph would allow for a comparison, between the method proposed and previous regularizers or compression methods. 