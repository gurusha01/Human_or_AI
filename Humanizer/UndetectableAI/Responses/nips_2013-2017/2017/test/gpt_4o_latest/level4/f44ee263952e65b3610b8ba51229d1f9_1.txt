This article presents a method for storing past contexts in language modeling without the need for specific parameters or limits imposed on the storage capacity.Under this approach the system retrieves the similar previous states at each stage and employs a technique known as kernel density estimation to create a probability distribution across a wide range of words.This caching method differs from strategies like pointer networks or continuous caches by not setting any constraints on its storage capacity.Experiments demonstrate that this approach is effective in handling language modeling tasks that involve changes over time and topics.It also outperforms RNN based language models, in these scenarios. 
The article is nicely. The idea of an unlimited memory storage is quite new and easy to understand at first glance. I think the suggested method could be used for purposes other, than just language modeling. 
I wish there was a comparison with methods like parametric or local caching systems such as pointer generator networks. I'm also interested, in knowing how efficient this new model is when its actually being used for calculations. For example; It seems demanding computationally to query 1024 closest neighbors to calculate p_{cache} which could potentially slow things down significantly. 