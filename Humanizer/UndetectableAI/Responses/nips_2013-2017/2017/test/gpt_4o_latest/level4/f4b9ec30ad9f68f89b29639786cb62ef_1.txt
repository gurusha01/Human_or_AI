This paper presents a method for combining gradient vectors from different workers in distributed stochastic gradient descent to handle Byzantine failures effectively The authors show that traditional aggregation techniques like averaging struggle to withstand even a single Byzantine failure To tackle this issue the paper introduces the idea of Byzantine resilience to measure the ability to withstand such failures and demonstrates that the suggested aggregation function meets this requirement Results from experiments show that the proposed approach performs much better, than the baseline as the number of Byzantine workers grows. Nevertheless the method brings in computational load even without Byzantine workers present emphasizing the importance of maintaining a balance, between resilience and effectiveness. 
I don't know much about distributed stochastic gradient descent technique. I'm mainly concerned with the machine learning tasks themselves in this scenario.Figure 4 shows that Krum has a learning error compared to simple averaging which raises my curiosity about the magnitude of this difference in error and whether it can be limited.What additional performance cost is involved in ensuring resilience, against failures when employing Krum? 