The article presents a method for sharing parameters in RNN models that involves including convolution operations, in an LSTM cell to handle unstructured input sequences using tensors as convolution kernels and feature maps simultaneously.It also enhances model complexity through postponement of the output target for an amount of steps. 
The idea they suggest is quite interesting. Seems unique from my evaluation of it.The writers lay out an argument which although somewhat intricate is backed up by experimental findings.On a dataset sourced from Wikipedia for language modeling the method delivers results similar, to the best available standards while employing roughly half the parameters. 
However I do have some worries, about this method; 
The effectiveness of producing high dimensional feature maps for various issues remains uncertain. This casts doubt on the methods ability to scale up to dimensional tensors since the experiments are limited to three dimensions only.
Using "depth in time" causes a delay that doesn't work well for real time tasks, like processing speech during streams. 
When dealing with tensors that have many dimensions the amount of hyperparameters involved can increase substantially which might make the optimization process more challenging. 
There's a problem.
Please correct "Fig 5." to "Table 5.‚Äù, at line 242. 