In this study a new method is presented for training feed forward neural networks (DDNs). The approach involves elevating the ReLU activation function to a dimensional realm to create a seamless multi concave structure. The researchers suggest using a block coordinate descent (BCFb algorithm that ensures reaching convergence, to fixed points with an R linear rate of order one. The method was tested on the MNIST dataset. Showed better results, than various forms of stochastic gradient descent (SGDs) performing well in terms of test set errors rates and computational efficiency while also producing less complex networks. 
Advantages; 
The research paper introduces an approach to tackling the DNN training issue by incorporating Tikhonov regularization and multi concave optimization marking a notable shift from conventional methods relying on SGD and offering solutions to issues like gradient disappearance and subpar convergence, in deep networks. 

The practical testing done on MNIST proves that the new approach works well by achieving test accuracy and quicker convergence than traditional SGD solvers do.Its capability to train networks without sacrificing performance is quite remarkable and beneficial for places, with limited resources. 
The way they explain turning ReLU into a projection, onto a convex set and incorporating this into the Tikhonov regularization framework is clear. Makes sense mathematically. 
Areas needing improvement; 
The experiments reach is limited to the MNIST results for now; however including testing with a wider variety of datasets such as CIFAR 10 and ImageNet could bolster the argument, for generalizability and scalability immensely. 
The researchers recognize that their method includes solving a series of programs (QP) which can be quite computationally intensive. While they suggest training times, in their paper compared to SGD (Stochastic Gradient Descent) it's still uncertain how well the method handles very large datasets and deeper networks in terms of scalability. 

Practical Usefulness; Rely on linear SVM for classification during testing might restrict how useful the method is in real world learning situations from start to finish. It's worth looking into or providing reasoning, on this aspect. 
Reasons, in favor of acceptance; 
The research paper significantly enhances the practical aspects of optimizing DNN models by tackling critical issues such, as vanishing gradients and ensuring convergence reliability. 
The suggested approach indicates promise in acquiring networksâ€”a valuable asset for use in embedded systems and environments, with limited resources. 
The research is firmly based on principles and offers a clear direction, for future studies. 
Reasons not to approve; 
The practical testing has its restrictions, in terms of coverage and the computational intricacies of the approach could pose challenges when applied to issues. 
Modern learning pipelines lose some of their end, to end essence when they heavily depend on external classifiers during testing phases. 
Suggestion; 
This study makes an grounded contribution to the area of optimizing deep learning models despite some limitations, in its experimental range and practical utility. Its theoretical progressions and real world findings validate its credibility. I suggest accepting it with adjustments to tackle concerns regarding scalability and practical usage.