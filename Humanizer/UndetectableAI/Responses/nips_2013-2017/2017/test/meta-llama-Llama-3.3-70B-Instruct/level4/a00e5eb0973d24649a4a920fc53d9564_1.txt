This study delves into the boundaries of variance reduced stochastic algorithms such as SAG and SDCA that have made a substantial impact on the fields of machine learning and convex optimization by achieving linear convergence for strongly convex problems with Lipschitz gradients. It is worth mentioning that these algorithms converge based on $ n + \ k $, than $ n \cdot \ k $.
One important discovery is that iterative algorithms that progress step by step require an understanding of the function provided by the oracle in order for them achieve linear convergence rates smoothly and effectively which aligns well with the necessity for techniques such as SAG and SAGGSA which need updates in their memory buffers, for gradients based on the functions position. 
The paper also talks about the constraints of utilizing techniques that reduce variance alongside data expansion, in the learning process; however this limitation is only relevant when the data expansion is random and not derived from a collection of expanded examples. 
A noteworthy finding relates to enhancing the efficiency of variance reduced algorithms following Nesterovs approach to advance convergence speed from $\text{\textit{k}} to $\sqrt{\text{\textit{k}}}$. The study reveals that algorithms deemed "oblivious " which feature fixed update rules averaged over each iteration cycle and cannot be sped up. 
One important takeaway from an outcome is that understanding the strong convexity parameter is crucial, for speeding up convergence in a stationary algorithm. 
Exploring algorithms is expanded by incorporating restart methods that help address the lack of adaptability, in accelerated algorithms when dealing with uncertain local conditions. 
The paper is nicely written and shows teaching practices overall; nonetheless there are a few small problems like using the term CLI before formally defining it in Definition 2 and a typo, in Line 224 where "let us we describe" should be corrected to "let us describe."