This document introduces TrajectoryGRUs (TraJGRUs) a fresh variation of LSTM and GRUs architectures for learning spatial patterns in hidden states during sequential data processing tasks like mapping trajectories in videos or tracking movements, in time series data sets. 
The researchers test their model on tasks involving creating videos using two different sets of data. MovingMNIST that shows three digits at once and the HKO. ̄ ˇ nowcasting dataset.The findings show that TraJGRu is more effective, than models using convolutional methods. 
Numerous particular inquiries and comments come up.
Did the writers examine whether Trajectory GRUs were pitted against Convolutional GRUs using a kernel size than the typical 25 pixels, by 25 pixels setup? 
Is the warping process in Trajectory GRUs computationally expensive than in convGRUs ? Sharing insights on model configurations and computational operations along, with the time taken for experimental assessments would be very helpful and informative. 
Why did the writers choose to set a number of training epochs instead of using early stopping in their approach and is it possible that ending training sessions sooner could enhance the performance of certain models? 
"The paper seems to be solid, in terms of quality."
The paper is clear overall; however it would be helpful to mention the specific warp method employed to enhance the comprehension of TraJGRu further.The details regarding the quantity of examples and the division between training/validation/test sets, for the HKO. Dataset are not explicitly outlined. 
Not many research studies have delved into the concept of warping for video modeling in depth apart from the "Spatio temporal video autoencoder with differentiable memory." It would be advantageous to analyze and juxtapose TraiGRu against these methods, for understanding and insights. 
The research on creating models that can effectively learn video representations is an ongoing challenge in the field of artificial intelligence and machine learning. This study introduces a model that not only learns filter weights but also filter support as a novel approach to enhancing video modeling techniques. However the model has only been experimented with on datasets like MovingMNIST and specialized nowcasting datasets. It would be interesting to explore whether this model can produce improved video representations for video tasks, such, as human action classification using diverse videos. 