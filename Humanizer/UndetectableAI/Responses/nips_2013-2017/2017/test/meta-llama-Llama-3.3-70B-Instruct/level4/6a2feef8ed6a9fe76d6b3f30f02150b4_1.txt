This paper introduces an effective block coordinate descent (BCDD algorithm that includes a new Tikhonov regularization method for training dense and sparse deep neural networks (DDNs) using the ReLU activation function.The authors show that their suggested BCDD algorithm reaches convergence to a fixed point with an R linear convergence rate of level one and performs better than different stochastic gradient descent (SGDD methods, in practical tests. However the reasons for choosing Tikhonov regularization and the block coordinate descent method are not sufficiently clarified in the explanation provided The details presented in the paper are hard to grasp as they lack thorough information Additionally the results shared do not measure up to the latest standards which brings into question how effective the suggested approach would be, for real DNN applications Below you can find a more detailed breakdown of my thoughts and insights regarding this matter
I have some remarks; 
The authors need to explain clearly why they chose to use Tikhonov regularization and how it compares to other regularization methods in terms of benefits and advantages. In the part of Section I of their paper or article writing piece on deep neural networks (DNNs) the authors mention various challenges such as non convexity issues and the presence of saddle points and local extremities, before introducing Tikhonov regularization technique. Can it be inferred from their discussion that Tikhonov regularization's able to effectively tackle these mentioned challenges? 
The authors need to state why they chose to break down the problem into three sub problems and why this approach avoids the issue of the vanishing gradient problem. 
The shift from equation (4} to equation (5} is not well explained in the text at line 133 where the detailed formula, for the matrix Q(\tilde {\mathcal A}) is missing How can we verify that the matrix Q(\tilde {\mathcal A}) is semidefinite? 
We need to provide a mathematical description of the Tikhonov regularized inverse problem. 
The assertion that solving the inverse sub problem can address the vanishing gradient problem, in deep learning lacks both theoretical backing and real world proof so its crucial for the authors to present corroborating evidence to support this claim. 
More information is required to clarify why the "optimal" output characteristics function comparably to target propagation methods. 
We need to include the definition of \mathcal P, on line 246. 
In Figure 5 of the study mentioned above BCD S demonstrates an ability to learn networks that're more sparse compared to others but interestingly the weight matrix of the fourth layer appears to still be dense causing a noticeable discrepancy that needs attention. 
I'm not quite sure what the " Q " prefix means, on line 252. 
There seems to be a word, on line 284. It should be "dense".
The way the reference format is set up is unclear. Could use some adjustments. 
The selection of the DNN architecture depicted in Figure 1 appears ambiguous to me.The model employed in the trials features 3 hidden layers and might not meet the criteria for being labeled as "deep." It's worth contemplating incorporating skip connections into the network design well.Besides that the outcomes showcased do not seem to match up with the cutting edge advancements, in this field.This leads to uncertainties regarding how practical the suggested approach would be when applied to real world DNN scenarios. 