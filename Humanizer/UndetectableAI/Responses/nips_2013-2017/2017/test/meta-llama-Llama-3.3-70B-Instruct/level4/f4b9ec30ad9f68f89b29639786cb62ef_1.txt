This article presents a method for combining gradient vectors in distributed stochastic gradient descent to handle Byzantine failures more effectively. The authors show that traditional aggregation techniques like averaging can be compromised by one Byzantine failure. In response, to this problem they introduce the notion of resilience to assess the systems ability to withstand such failures and demonstrate that their suggested aggregation approach meets this standard. Experiments reveal that their method performs better as the number of workers grows compared to not using it. Nonetheless this sacrifices efficacy, even when there are no dishonest workers present emphasizing the necessity of striking a balance, between resilience and effectiveness.
I am not very familiar with distributed stochastic descent techniques and my main interest lies in the machine learning aspects of it all. In particular the data presented in Figure 4 indicates that Krum results in a learning error when compared to averaging method which caught my attention. I am curious to know the extent of this error difference. Is there a limit on it? Additionally I would like to understand the impact on performance when using Krum for enhancing robustness against failures and how significant this impact is, on machine learning performance overall. 