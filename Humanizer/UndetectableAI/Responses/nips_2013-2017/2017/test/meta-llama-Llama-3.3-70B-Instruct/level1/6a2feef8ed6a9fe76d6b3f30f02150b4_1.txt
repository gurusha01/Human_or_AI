The paper introduces a method for training deep neural networks (, with Rectified Linear Unit activation functions). The authors describe how to reformulate the network training process as a step optimization challenge and suggest using a specific algorithm based on block coordinate descent to address it efficiently showing that the approach reaches stable points with consistent convergence rates. 
The article is nicely. Gives a concise summary of the method being suggested.The writers connect their work to studies in the area like stochastic regularization ADMM and BCD.The results from experiments show how well the suggested algorithm works by indicating that it can attain lower error rates, on test sets compared to conventional solvers based on SGD.
The papers advantages comprise; 
A new Tikhonov regularization approach has been suggested for neural networks (DNN) that is applicable for training networks with varying levels of density â€“, from dense to sparse configurations. 
An algorithm based on Binary Coded Decimal (BCD) has been created to ensure reaching points globally with a convergence rate that is linearly fast and reliable. 
The presentation of an thorough analysis of convergence is crucial here. It should cover both the algorithms overall convergence and its R linear convergence rate. 
The papers shortcomings are as follows; 
The algorithms computational complexity could be significant because it requires solving a series of programs. 
The absence of a comparison to advanced techniques like Entropy SGD and ADMM based algorithms is notable, in the analysis. 
The experiment was only carried out on the MNIST dataset with evaluation of the results. 
Reasons, for approval; 
The research paper suggests an captivating method, for educating DNN models that is capable of teaching both compact and scattered networks effectively. 
The algorithm has been shown to converge to fixed points at an R linear rate of convergence which is a favorable characteristic. 
The results, from the study show that the new algorithm works well by outperforming solvers based on SGD in terms of test set error rates. 
Reasons to oppose it; 
The suggested algorithm might face challenges due, to its computational complexity that could hinder its real world usefulness. 
It might be challenging to assess the importance of the proposed approach without comparing with advanced methods available, in the field. 
The small scale testing might not provide evidence to prove how well the suggested algorithm works in different situations. 
In my opinion the paper is nicely. Introduces a fresh and intriguing method, for training DNN models Despite some flaws the papers strengths overshadow them. I suggest accepting it 