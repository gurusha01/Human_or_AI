This study suggests a method for condensing deep neural networks by considering compression, from the beginning of the training phase itself The researchers introduce a technique that promotes the parameter matrix of each layer to possess a lower rank This enables the learning of more concise models The method is proven to attain superior compression rates compared to current leading techniques while maintaining prediction accuracy at a minimal loss. 
The paper is nicely. Effectively outlines the reasons behind the suggested approach and how it was carried out in detail by the authors. They present an overview of previous research in neural network compression field pointing out the drawbacks of current techniques and the advantages of their method. The results from experiments are striking as they were able to achieve compression rates exceeding 90% across complex architectures, like the 8 layer DecomposeMe network and the 50 layer ResNet. 
One of the positive aspects of the paper is its strengths.
A new and efficient method, for compressing neural networks has been suggested.
A comprehensive examination of research and a detailed description of the approach.
The approach showcased experimental outcomes highlighting its effectiveness.
There is a possibility of achieving savings, in both computing expenses and memory consumption.
Some drawbacks of the document are; 
Not all neural networks and compression techniques may be suitable, for this approach.
Careful adjustments may be needed when selecting hyperparameters, like the regularization level and energy threshold. 
The paper would be improved by delving into the balance, between compression rate, accuracy and computational expenses.
Reasons supporting approval; 
The article suggests an efficient method, for reducing the size of complex neural networks.
The results from the experiment are quite remarkable. Show promising possibilities, for cutting down both computational expenses and memory usage.
The method has the possibility to be broadly relevant and create an influence, in the realm of deep learning.
Reasons to oppose it; 
Not every neural network type and compression strategy may be suitable, for this approach.
The article would be more helpful with an examination of the balance, between compression rate, precision and computational expenses. 
Finding the hyperparameters can be tricky and might restrict how broadly the method can be used. 
In my opinion the paper adds a perspective to the realm of deep learning and compressions in neural networks and I suggest accepting it for publication.The methodology is original yet practical. Could bring about a notable change in this domain.Although there are a couple of drawbacks and areas, for improvement I believe these can be rectified with investigation and advancements. 