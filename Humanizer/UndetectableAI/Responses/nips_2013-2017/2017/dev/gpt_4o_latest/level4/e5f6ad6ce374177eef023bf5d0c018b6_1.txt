The research paper introduces a design for creating videos based on specific conditions by utilizing convolutional Long Short Term Memory (LSTM) networks with the main idea that the layered organization of previous architectures may not be ideal for generating videos as it progresses from basic to abstract scene characteristics in each layers representation. Unlike tasks involving categorization or designation of classes where approximate locality's sufficient for generative models to preserve detailed spatial details, about objects until the final output layer is crucial. The new model called PredRNN builds upon LSTMs by adding two memory cells. One that moves along the same layer over time (similar to ConvLSTMs original design) and another that moves vertically across different layers in the network structure. The authors of the study test their model using two datasets. Videos of moving MNIST digits and the KTH action recognition dataset. Showing MSE results, in predicting video sequences compared to existing methods. The research paper is nicely. Properly acknowledges previous related research studies. 
The suggested design is unique and engaging in nature; however the evaluation against existing research could be enhanced to effectively showcase the practical advancements made in this area of study.To illustrate this point further Kalchrenner et al.(2016) for example argue that they achieved a performance level to the minimum threshold in the dynamic MNIST challenge yet their approach is not utilized as a reference point, for comparison. Moreover " earlier studies such as Kalchrenner et al., 2016 and Srivavasta et al., 2016 have primarily used cross entropy loss in their research on MNIST movement." However " the authors of this study place emphasis on likelihood output instead." Incorporating likelihood metrics would enable a comparison, with the aforementioned previous research endeavors."
In addition, to mentioning efficiency as a benefit of their model the authors fail to offer a thorough evaluation or contrast of computational complexity compared to previous approaches, which makes it challenging to quantitatively evaluate this assertion. 
Here's a slight tweak, in the notation that might be helpful; 
Consider replacing " C " with " M " in Equation 3 for clarity and to establish a more intuitive link, with Equation 4 for the readers understanding. 
Upon reviewing the authors reply and considering the comparisons made with previous studies in the papers recent version I think it has been improved.However I still have doubts, about the assessment method. Wouldn't training the model using Mean Squared Error(MSE) while other methods employ varied loss functions inherently advantage the suggested model when assessing based on MSE?