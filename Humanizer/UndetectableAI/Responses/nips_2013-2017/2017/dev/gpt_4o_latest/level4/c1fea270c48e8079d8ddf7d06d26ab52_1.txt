This article discusses the challenge of reducing a submodular function—an important issue in the field of machine learning with established solutions available for precise resolution but often hindered by lengthy computational processes that are not always feasible in real world situations. To address this drawback effectively and make computations more manageable in scenarios with decomposable submodular functions (DSFM) which comprise submodular functions summed over a smaller support area are commonly explored due to their frequent occurrence, in real world applications. 
The study improves the examination of the efficient algorithms for DSFM by a value equivalent to the quantity of functions in the breakdown process. Moreover it presents a structure that differentiates between "level 0" algorithms that function as sub procedures for quadrilateral minimization and "level 1" algorithms that minimize the function by regarding level 0 as an unknown element. This setup allows for substantial evaluations by normalizing the application of level 0 algorithms, across various techniques. The experiments show a tradeoff. Discrete algorithms need calls to the basic subfunctions at level 0 compared to gradient based methods which place less strain, on level 0 but require more computation at level 1. 
The study is detailed. Involves a mix of different optimization methods to reduce the processing time significantly for a critical issue with complex computations involved in it.The practical outcomes also point out a balance between options that hints at the need to select an algorithm based on the specific situation and computational needs, for DSFM. 
The paper has a drawback in that the writing can be dense at times and may be challenging to understand for some readers. It would have been helpful to have a thorough explanation of the parameters κ and ℓ and their specific bounds in relation to these parameters. Additionally including a comparison of the limits, for RCDM, ACDM and IBFS would have enhanced the papers value. 