I am discussing the issue of optimizing finite sums through the SAGA framework in this paper which expands on Leblond et al.s (2017) research by integrating optimization with a focus, on incorporating non smooth separable regularization techniques.The key innovation lies in the approach suggested for managing smooth regularization while maintaining sparse updates as a core principle.I am intrigued by the method proposed for dividing the smooth regularization. The analysis is mostly based on the model set up by Leblond et al.s work from 2017 and the evidence for the version is influenced by the method described in Mania et al.s research, from 2015. 
I have some observations.
The definition of an " read" requires further clarification; What exactly does the symbol \(\hat{x}_ k\) stand for and how is the value of \( k \) determined in this context ? Is this method comparable to the approaches of Lian et al (2016) and Liu and Wright (2015)? What sets them apart, from each other ?
The explanation for the form of the dissimilarities, between \(\hat{x}_\text{{dt}}\) and \(xt\) is not clearly articulated. 
In order to reach the linear convergence rate and speedup mentioned in the paper’s findings it is important that the authors clearly articulate the underlying assumptions they have made. 
I noticed a minor comments and typos, in the text.
The description of Δ, on line 217 is inaccurate. 
I couldn't find the source you mentioned.
The work, on greedy SDCA is important and should be included in the references.
