This study shows that specific challenging prediction tasks involving types of errors can now be addressed more efficiently than in the past thanks to recent advancements, in the field of optimization theory and techniques. When the sample size is considered in relation (nine) the complexity can be decreased from levels down t quadratic levels and once the optimization problem is simplified effectively through proper regularization methods it can be tackled using an extended version of the SVRG (stochastic variance reduction gradient technique that has been adjusted for Bregman diverges. 
The paper mainly centers on finding a resolution for the adversarial issue by simplifying the F score loss approach and considering if the benefits of this method could be extended to different loss types as well. While the expansion of SVRG offers applicability in general terms the focus of the paper appears to be on adversarial optimization using the F score methodology instead.It could have also been framed as an instance, within the suggested Breg SVRF framework. 
In my opinion the paper seems solid and convincing enough to deserve approval. 