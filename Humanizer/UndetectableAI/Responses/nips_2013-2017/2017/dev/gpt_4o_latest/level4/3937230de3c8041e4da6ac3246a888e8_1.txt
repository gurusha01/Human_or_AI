The writers present a concept called variational memory addressing that improves models by adding external memory and hard attention mechanisms.They specifically develop read and write processes that bear resemblance to probabilistic graphical models rather than the more sophisticated mechanisms seen in methods such, as neural Turing machines. 
The way they describe it is that external memory works of like a universal element in topic models and mixture models â€“ you choose a "membership" with strict attention first and then create the local variable z and data x based on the memory associated with that membership slotting, in this way. I personally think this view offers some insights into understanding how memory fits into latent variable models since it relates to the process of writing lining up with inference. 
The authors noted in their work (such as from line 175 to 186) that the algorithm encounters difficulties with scalability as the external memory size increases.There is an explanation for this. The variance of the black box gradients concerning the q(a ) parameters grows as a becomes larger.It appears doubtful that VIMCO would effectively address this problem.However I must say I am impressed by the authors achievement of producing results even with memory sizes as large, as 1024.| M |). 