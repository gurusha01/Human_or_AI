Lately there has been a lot of buzz among the Algorithmic Game Theory crowd about creating auctions that maximize revenue with a few samples of buyersâ€™ valuations.This brings up a question; how many samples do we need to make sure our auction is honest and can get close enough to the best possible revenue? Normally this depends on using learning theory ideas like VC dimension and pseudo dimension, as boundaries. This article presents an idea known as split sample complexity and applies it to establish sample complexity limits, for typical auction scenarios. 
The idea of split sample complexity involves finding the hypothesis within a subset of half the total samples in a given set of m samples. Lets denote \hat {HS } as the collection of hypotheses obtained from all such subsets. The split sample growth rate measures the size of \hat {HS }, among all sets of size S. 
The writers show that when provided with a set of m samples the anticipated revenue attained by the hypothesis derived from these m samples can be described using the optimal revenue for the fundamental distribution plus an additional error term. This error term is proven to be influenced by the split sample complexity to m. Through setting boundaries on split sample complexity for particular auction categories the authors establish limits on sample complexity, for these scenarios. 
This new approach allows the writers to enhance the existing limits for pricing strategies, for packages and individual items set by Morgenstern and Roughgardens. 
Overall this paper is excellently. Presents a new approach to tackle a current area of study. 