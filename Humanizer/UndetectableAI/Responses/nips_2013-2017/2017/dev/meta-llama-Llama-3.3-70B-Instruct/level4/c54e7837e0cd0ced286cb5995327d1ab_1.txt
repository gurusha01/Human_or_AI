This article presents Batch Renormalization as a way to address the challenges linked with batch normalization in scenarios involving non i.i.d minibatches, which is important when training large neural network models with restricted GPU memory capacity in certain applications. The suggested approach is simple and practical to put into practice. Results from experiments showcase its efficiency with i.i.d minibatches and its capability to enhance performance, for small minibatches when compared to conventional batch normalization techniques. 
The authors start by giving a summary of batch normalization and point out its main drawbacks such as the discrepancy between the mean and variance in training versus inference and the issues with small minibatches causing instability. One possible solution proposed initially is employing moving averages, for normalization; however; this could result in model instability. Henceforth; the authors propose a efficient batch renormalization technique that merges minibatch mean and variance with moving averages. Essentially speaking Batch Renormalization shifts from utilizing the initial batch normalization (, with minibatch mean and variance)) to a modified version that heavily depends on ongoing averages. This method enables the model to benefit from the strengths of averages aiding in achieving successful convergence. 
The paper raises a number of questions.
It's not clear why Batch Renormalization doesn't provide a benefit compared to batch normalization with minibatch sizes, like 32; maintaining consistent mean and variance during training and inference appears to offer little advantage in such scenarios. 
Experiments indicate that using batch sizes like 4 leads to poorer results than larger batch sizes such as 32.Batch size It would be intriguing to investigate employing multiple moving averages for both the mean and variance, with varying update rates like 0. 
The paper doesn't delve into the impact of the parameters rmax and dmax on performance. Implies that thorough parameter tweaking may be necessary. 
This piece of work is impressive overall. It will be intriguing to observe whether upcoming studies can offer a more polished solution, to the difficulties presented by batch normalization. 