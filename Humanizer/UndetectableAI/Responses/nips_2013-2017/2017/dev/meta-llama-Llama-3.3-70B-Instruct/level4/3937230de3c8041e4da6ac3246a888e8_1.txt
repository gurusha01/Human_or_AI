The writers present a method known as variational memory addressing that improves generative models by integrating external memory and hard attention features. A key aspect of this technique is the creation of read and write mechanisms that mirror those seen in probabilistic graphical models instead of the more intricate mechanisms used in models such, as neural Turing machines. 
In this setup external memory works like a variable in topic models and mixture models. The strict focus selects a "membership " then produces variables z and data x based on the memory associated with this membership. This view of memory in variable models. Where writing is similar, to inference. Offers valuable understanding. 
The authors admit that the algorithms ability to scale is restricted by the memorys size as highlighted in lines 175 186 of their work. This restriction can be linked mathematically to the rising disparity in the black box gradients concerning the q(a) parameters as as size increases. While VIMCO may not effectively address this problem the authors have still managed to produce outcomes using external memory capacities of up, to 1024 which is quite impressive. 