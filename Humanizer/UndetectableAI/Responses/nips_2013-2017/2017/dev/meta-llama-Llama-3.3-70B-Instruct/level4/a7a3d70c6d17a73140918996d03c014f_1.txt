Lately the Algorithmic Game Theory community has noticed a growing interest in creating auctions that're close to optimal revenue with only a few buyer valuation samples. An important issue here is figuring out the number of samples required to make a fair auction that gets pretty close to the best possible revenue. Usually the limits, for this are set by linking it to ideas from learning theory like VC dimension and pseudo dimension. This research paper presents an idea known as split sample complexity and utilizes it to establish limits on sample requirements, for typical auction situations. 
The idea of split sample complexity is explained like this; If you have a group of m samples and take any subset that's half the size (m /  22) you can find a hypothesis for it. The collection of hypotheses for all subsets of size m / 22 is represented by H hat subscript S. Split sample growth rate indicates the largest size of the set H hat subscript S, among all sets of size S.  
The authors show that the predicted earnings from the guess based on a set number of samples can be connected to the best possible earnings for the primary distribution with an additional error factor involved.The margin of error is tied to how complex it's to divide the sample as per the number of samples taken.By setting limits on this division complexity aspect the authors establish boundaries, on sample size for types of auctions. 
The new method allows the authors to enhance the existing limits, for pricing bundles and individual items as set forth by Morgernstern and Roughagarden.  
The paper is quite well written. Introduces a fresh approach to tackling an ongoing area of research while making a significant contribution, to the field. 