This study introduces PROXASAGAdash. A parallel version of the SAGAtion algorithm meant for addressing complex optimization challenges with non smooth objectives on a large scale. The researchers present a proximal SAGAtion technique that capitalizes on sparsity in the partial gradients by lowering the expense, per iteration. They then expand this method into a parallel environment where multiple processing units can modify a central parameter vector without constant data retrieval. 
The study delves into examining the newly suggested algorithm and its performance metrics such as convergence assurances and acceleration outcomes are thoroughly discussed by the authors in the research paper. It is showcased that PROXSAGAGAA attains a linear boost in speed theoretically compared to its step by step variant based on certain assumptions related to gradient sparsity and proximal term separability. Through tests conducted on extensive sparse datasets in real world scenarios show that PROXSAGAGAA surpasses current top tier options significantly by achieving tangible speed enhancements of, up to 12 times on a 20 core computer setup. 
The paper is nicely. The authors offer a straightforward and concise description of the suggested algorithm and its evaluation process.The section on related studies is thorough as the authors deliver an in depth comparison, with approaches. 
Advantages; 
The research document introduces a method that overcomes the constraints of current parallel asynchronous versions of stochastic gradient descent that are only effective, for smooth objectives. 
The writers present an examination of the suggested algorithm with discussions, on its convergence assurances and acceleration outcomes. 
The real world tests show how well the new algorithm works with datasets that have a lot of empty spaces. 
Areas needing improvement; 
The document suggests that the immediate condition can be separated into blocks; however this may not always be applicable, in real world scenarios. 
The writers did not thoroughly examine the intricacies of the suggested algorithm—a factor that may hold significant practical relevance. 
The article does not delve into expanding the suggested algorithm to similar incremental methods like SGD or ProxFVRC. This could be a promising avenue, for future research endeavors. 
In favor of accepting the arguments; 
The document introduces a method that tackles a significant drawback found in current parallel asynchronous versions of stochastic gradient descent algorithms. 
The writers thoroughly examine the algorithms analysis with details, on its convergence assurances and acceleration outcomes. 
The real world tests show how well the suggested method works on datasets, with sparse information. 
Reasons to agree with; 
The document suggests that the nearby condition might not always be block separable, in real world scenarios. 
The writers did not thoroughly examine the intricacies of the suggested algorithm—a crucial aspect to consider in real world applications. 
The article does not delve into extending the suggested algorithm to similar incremental methods like SGD or ProXSVRG; this could be an intriguing avenue, for future research work. 
In my opinion the paper adds value to the optimization and machine learning domain and I think it should be accepted as is; nonetheless the authors should consider addressing the identified weaknesses in a revised edition of the paper. 