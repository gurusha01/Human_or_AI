This document presents ways to measure the complexity of a recurrent neural network which include the following; 1. Recurrent depth refers to how many layers are connected over time in recursive connections. 2. Feedforward depth measures how many layers are connected from input to output. 3. Recurrent skip coefficient indicates the level of directness in connections (, in essence the opposite of multi layeredness).  
Apart from defining terms, on paper; the document also presents two key contributions.  
The authors show that these measures, which are defined as limits when the number of time steps goes towards infinity are clearly explained.   
They analyze how these measures are related to real world outcomes and demonstrate that all suggested depth measurements can enhance performance.   
The suggested actions seem like sense and could be beneficial, in practice too!. Some might wonder about their importance since their usefulness is mostly shown through comparing real life data. Which isn't completely conclusive after all. In terms; it would be more convincing if there were solid assurances based directly from the definitions rather than just depending solely upon practical tests. This brings up the point of why these specific measures are considered more important compared to other potential ways of measuring complexity.   
It's a bit unfortunate that most of the information is placed in the appendix at the end of the document. However it's a practice, in NeurIPS submissions so we can't blame the authors for following this trend.