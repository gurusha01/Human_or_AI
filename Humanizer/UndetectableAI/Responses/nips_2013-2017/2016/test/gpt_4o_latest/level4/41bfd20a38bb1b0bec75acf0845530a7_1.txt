This research delves into how structured sparsity (group lasso) can be used to shrink neural networks in a more organized way that is well suited for vectorized calculations on GPUs compared to the recent exploration of Han et al.s l1 regularization technique.I view this as an advancement.The results, from the experiments show promise. I am particularly intrigued by the method of depth regularization.This paper is well crafted with a collection of experimental results. Improving the effectiveness of neural networks poses a significant hurdle, in the realm of deep learning research. Furthermore employ sparsification to garner added advantages like regularization and increased interpretability. It's worth noting that the extent of innovation remains somewhat constrained since structured sparsity is a concept that has long been ingrained in the field. 