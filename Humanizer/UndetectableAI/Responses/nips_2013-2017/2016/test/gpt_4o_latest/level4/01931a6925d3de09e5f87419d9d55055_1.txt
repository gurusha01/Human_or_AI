The document introduces a measurement called LAND that is based on a nonparametric framework and combines concepts from metric learning and Riemannian statistics.The proposed measurement is linked to a normal distribution as discussed in Pennec [15]. To determine the average and spread of this distribution—a sort of Mahalanobis distance for manifolds—the authors create a maximum likelihood method.Geodesic distances, within the manifold are also discussed by Hauberg et al. Distances between points are determined using [9] where the metric is developed based on the inverses of diagonal) covariance matrices. Moreover the authors expand their method to incorporate combinations of LANDs. This study offers perspectives at the junction of metric learning and Riemannian statistics. Parametric and generative models that can extend predictions beyond the training data are extremely advantageous, in scenarios. My main worry is about whether this method can handle real world datasets since they are usually large and complex in nature. LAND requires calculating a (covariance matrix and its inverse for every single data point which can be quite difficult, in high dimensional scenarios. Although the authors touch upon the challenge of high dimensionality briefly (lines 265–266) it would be helpful for the paper to delve deeper into discussing the challenges and boundaries of the metric they propose. In this case specifically how extensive is considered "dimensional"? Are we talking about 3 dimensions, 5 10 or even 100 dimensions here ? Additionally for the convenience of readers it would be helpful if the paper provided more context within the wider field of metric learning literature. For example Hauberg and colleagues [ 9 ] highlight related studies like From and team [ 1, 2 ] well as Malisiewicz and Efros [ 6 ] which also focus on training diagonal metric tensors for individual data points similar, to LAND. In the way local covariances have been utilized in methods such as local PCA (for example; the research by Kambhatla, on reducing dimensions through local principal component analysis) though not applied to each specific point consistently.  
The writers use the Gaussian kernel to establish the concept of locality in understanding the metric; yet they do not thoroughly investigate the impact of the parameter sigma on the outcomes presented in their work. In Section 4. 2 They address the selection of sigma for the sleep data; however this aspect is not deliberated upon for the data in Section 4. 1. Including robustness tests for sigma would add value to their paper. Moreover an inquiry arises. Does sigma remain consistent across all data points regardless of their positioning, in density or high density areas? 
Figures 5 and 6 lack visual comparisons with estimators despite the comparison in Figure 4 showing similar outcomes to LAND estimators but proving GMMs poor performance already demonstrated in Figure 4 limits the insights available from these figures alone. The supplementary material includes results with intrinsic estimators (Figures 2 and 4) showcasing their superior performance over GMM. It would be beneficial, to either substitute the GMM comparisons in the text with these results or present them alongside for a comprehensive analysis. In addition to that point of view of comparing the intricacy of LAND, with intrinsic estimators could prove enlightening in deciding on the more suitable approach based on varying data formats. 
The writers also delve into combinations of LAND models that I consider one of the intriguing aspects of the article since actual data, in the real world is seldom produced by just one element. Expanding on this concept would greatly boost the papers effectiveness.  
The manuscript is written well and is enjoyable to read overall. 