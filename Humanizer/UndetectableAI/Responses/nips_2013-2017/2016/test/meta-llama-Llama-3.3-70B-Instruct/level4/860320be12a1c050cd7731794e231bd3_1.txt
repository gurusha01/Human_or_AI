This study presents ways to measure the intricacy of a recurrent neural network. These include evaluating the depth of recurrence to understand how multiple layers connect over time; assessing feedforward depth to gauge the complexity from input to output connections; and considering the recurrent skip coefficient to determine the directness of connections in relation, to layered structure. The paper presents two findings beyond these explanations. Firstly establishing that these metrics are clearly defined when time steps tend towards infinity and secondly linking them to real world performance improvements by showing that greater depth measured by these metrics can enhance performance outcomes This suggests that the proposed metrics are both sensible and beneficial although their value may be debated if they are mostly validated through comparisons rather, than formal assurances. Basically speaking; If these methods could be proven to be effective in a manner their importance would be undeniable; At this point in time it's reasonable to question whether these methods are better than other ways to measure complexity; Additionally; although using appendices extensively for in depth complexity assessments is a common practice, in NIPS submissions and doesn't diminish the authors credibility; it does somewhat reduce the papers overall clarity. 