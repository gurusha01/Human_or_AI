This paper presents a technique for kernel learning that utilizes random features in a unique way. The method involves two steps; creating random features first and then adjusting their weights to align them optimally with the target kernel. The authors showcase an optimization approach for the model and offer theoretical assurances about its reliability and ability to generalize. They evaluate the method through experiments on standard tasks which highlight its potential as a significant addition, to kernel learning research. One key benefit of this method is its capacity to adapt and create a kernel even when the initial data doesn't match closely with the desired outcome discussed in section 4 point 1. Nonetheless the feature selection tests could be enhanced by contrasting with advanced feature selection techniques as just comparing against entirely random feature selection may not fully showcase the methods potential. The noticeable improvement in speed, over joint optimization methods is also a plus. There is room to enhance the manuscripts clarity since the writing can be a bit confusing and might need several readings to fully understand it. Some ideas, for improvement include;  
I suggest explicitly mentioning the assumptions regarding the feature function Ï† at line 64, for clarity and better understanding. 
From line 76 making the notation clearer by adding the dimensions of variables like W could enhance how easily others can read it. 
Equation (six)s meaning could be made clearer by elaborating on why the square root's incorporated into it. 
Line 169 does not provide an explanation of why selecting the Gaussian kernel leads to certain outcomes and how the specific structure of the feature function is decided upon independently from other options. 