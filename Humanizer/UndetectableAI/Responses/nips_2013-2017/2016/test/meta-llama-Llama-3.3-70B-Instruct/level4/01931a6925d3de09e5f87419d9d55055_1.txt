This document presents LAND as a metric that is based on a nonparametric space and combines ideas from metric learning and manifold learning with Riemannian statistics principles integrated into it. Specifically speaking about the metric proposed here. It resembles a Riemannian normal distribution described by Pennec [15], where a maximum likelihood approach is used to estimate its mean and covariance to calculate the Mahalanobis distance for manifolds effectively. The distances between data points are calculated based on geodesics along the manifold – similar, to the work of Hauberg et al. The researchers used a measurement obtained by studying the opposite of nearby individual covariances in [9]. They also looked into combinations of LAND models. How they relate to metric learning and Riemannian statistics in new ways. Creating models that're flexible enough to work beyond the initial training data is incredibly useful in different situations; however the key issue lies in whether LAND is practical, with real world data which tends to be extensive and high dimensional. Calculating a covariance matrix and its inverse for every point in the LAND method can be difficult when dealing with high dimensions as noted by the authors (lines 265–266). An in depth examination of the challenges and constraints of the suggested metric is crucial for understanding and evaluation. Defining what constitutes dimensionality is important since it can vary greatly—from 3 to 5 to even 100 dimensions—highlighting the need for clarity, on this aspect. Moreover the paper could benefit from an in depth exploration of its place within the extensive body of literature on metric learning as Hauberg and colleagues have already covered relevant studies like those by Frome and others and Malisiewicz and Efros that focus on diagonal metric tensors for each point similar to LAND.The incorporation of local covariances also bears resemblance to techniques such as local PCA (Dimension reduction, by local principal component analysis). In Section 4 of the paper on sleep data analysis specifically mentions the discussion about selecting the Gaussian kernel parameter sigma for defining locality. Does not address its impact on the results of synthetic data in Section 4 as well as the need for robustness tests to determine if sigma is consistent, across all data points regardless of density level. The findings from comparing LAND to estimators in Figure 4 are quite similar in nature; however; the visual representations in Figures 5 and 6 only depict contrasts with GMM—a model already proven to have subpar performance, in Figure 4 The addition of visual outcomes utilizing intrinsic estimators from the supplementary material (Figures 2 and 4) would offer a more informative comparison. The comparison of the complexity between LAND and intrinsic estimators would be intriguing as it might help in deciding which method to use for different types of data structures specifically.The discussion, on combinations of LAND is a highlight of the paper since real world data is frequently produced by various components making this area worth investigating further.In general the paper is well crafted and enjoyable to read. 