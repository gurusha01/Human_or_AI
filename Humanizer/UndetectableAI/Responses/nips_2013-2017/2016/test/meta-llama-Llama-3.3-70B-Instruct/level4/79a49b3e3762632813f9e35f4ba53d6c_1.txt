The research paper discusses a technique for calculating the likelihood of classes using labeled examples and imperfect samples instead of clean positive samples as done previously in a similar method framework.The results of experiments show promising outcomes when compared to techniques available, in the field.  
Upon reviewing the update regarding the comparison of observed and true P(y|x) I find the highlighted outcome to be interesting but not entirely surprising as similar concepts have been previously discussed in [1]. While documenting this is important, in itself I personally feel that it lacks a level of originality to truly stand out.  
When it comes to research on uneven noise in datasets my main focus is on the connection between the findings in (Scott et al. 2013) specifically Lemma 1 and Proposition 3 (part 2) which show how a "mix proportion estimator" (MPE) can be linked to the mixing weights, for various types of uneven positive and negative label noise scenarios. This suggests that Sanderson & Scotts estimation method could potentially be applied in situations involving asymmetric noise. Sanderson & Scott primarily studied how their method can be used for positive + unlabelled learning, in noise free positive instances; however; it is possible that their approach could also be applied in cases where there are noisy positive instances present as well I think the same may hold true for the estimator developed by Ramasamy et al. 
Studying from data that is labeled positively or remains unlabeled has been explored extensively in research work mentioned in the paper. Argues convincingly for focusing attention specifically where the positive data might be influenced by noise factors. The methods currently used in studies to estimate the class prior in partially labeled (PU) scenarios do not directly translate to this context; this highlights the need for developing new approaches tailored to this situation. The paper presents reasons, for its research direction and provides thorough technical insights into the challenges related to identifiability.  
The main concept is to expand on the framework introduced by Jain and colleagues in 2016 and approach the estimation task as identifying the mixing weight for a specific mixture model. This estimation process involves analyzing the results of a sound evaluator, on the input data to sidestep issues related to high dimensional datasets. Nonetheless I do have two reservations regarding the originality of this research.  
The analysis in section 3 seems to mirror the approach taken in (reference) with adaptations to accommodate the presence of noise in scenarios.It doesn't seem like there are any discoveries or methods introduced in the proofs unless I overlooked something.The concept of employing transformations, in section 4 seems quite reminiscent of the methodology employed in (reference). 
Furthermore it might be valuable to delve into the differences between the specific situation discussed here and the broader context of asymmetric label noise. It would be helpful to explain why not all analyses are carried out in the scenario and instead presume that one of the observed distributions represents the underlying marginal distribution. There exist approaches for addressing this scenario; for instance the practical estimators introduced by Sanderson and Scott, Liu and Tao well as Ramaswamy et al. could potentially be utilized in the context, under consideration here.  
In my opinion the paper adds value by expanding on (referencing Jain et al., 2016s work in relation, to positive unlabeled cases) but I find the element of originality lacking a bit here.  
Section 3 must clearly state the results that're absent for the methodology to be complete. 
Using a subscript and superscript like a^(lambda)_mu might be confusing â€“ maybe try using a(lambda,mu instead? 
The presentation of \Pi^{res could possibly be postponed until, after Lemma 1. 
Lets switch statements 4 and 5 with Lemma 1 in mind. 
In the proof of Lemma 1 it might be better, for readability if you don't put all the equations in a line. 
Oops there seems to be a typo, in "Theorem 1 (identifiability)."
Section 4 should provide details, on the recommended algorithm. 
It could be beneficial to provide a detailed explanation of what the probabilistic model shown in Figure 1 represents. 
It's possible that Equation (12) is connected to the findings regarding the likelihood of labels, in Ward et al.s (2009).