The writers introduce a method for teaching update functions in gradient based optimizers by simultaneously coaching both an optimizer and optimize that allows for the adjustment of LSTM based DNN parameters that gather data from various gradients akin to momentum effect on the network model convergence speed improvement over time with training iterations made possible through this approach The trained optimizer could be repurposed for similar assignments and shows some adaptability to new network structures even though the theoretical groundwork of the resulting algorithms remains unexplored practical tests, on a small scale display their competitiveness when compared with existing cutting edge techniques. This document stands out for its approach and relevance to the current context. Highlighting a meaningful progression towards the idea of "learning algorithms" within deep learning technology. The outcomes of the experiments seem promising; however conducting tests on a scale would provide clearer insights into the methods effectiveness and its ability to train intricate models, like upcoming versions of AlexNet. Moreover​, an analysis conducted after the fact could offer valuable insights into the optimization methods utilized by the trained algorithms​ potentially uncover​ ing the underlying cues they leverage and guiding new theoretical paths, for the optimization field​. Though the outcomes reveal that the trained algorithms outperform leading alternatives​ the degree of enhancement is moderate​​ indicating that existing optimization algorithms are quite efficient​​ showcasing the progress made in the industry​​. The methods true potential is revealed when it successfully tackles challenges that were once deemed unsolvable. 