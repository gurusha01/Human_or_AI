This study explores how structured sparsity through group lasso can be used to reduce the size of neural networks in a way that improves consistency and makes vector computations on GPUs more streamlined compared to the traditional method of using simple l1 regularization as explored by Han et al recently examined by Han et al This development is seen as important The results, from the experiments are promising particularly in terms of depth regularization Overall the paper provides a collection of experimental results Improving the effectiveness of neural networks is a key obstacle in the realm of deep learning. Sparsification plays a vital role in enhancing performance by offering better regularization and interpretability benefits to the systems functionalities. While structured sparsity is a known concept within the field of study the unique aspect of this research lies in its integration, with DNN frameworks despite not necessarily presenting groundbreaking innovations as a whole. 