The new "neural transducer" model is created for tasks that involve converting sequences from one form to another in an real time manner. Generating output as it receives input instead of waiting for the entire input sequence to be processed first. This method is made possible through elements such as a recurrent attention mechanism and an end of block symbol in the output set to indicate moving on to the next part of the input sequence; alongside approximate algorithms that rely on dynamic programming and beam search, for both training and making predictions. The models effectiveness is showcased through findings, on the TIMIT speech task delving into different design parameters.  
This article is nicely organized as it addresses the issue of developing and training a sequence to sequence model for real time use online. It presents a framework for addressing this issue and gives a thorough explanation of the design considerations for each element such as predicting the next step, in the sequence and implementing attention and block end modeling techniques. It also effectively explains the difficulties encountered during model training and inference tasks while proposing practical approximation algorithms to overcome these challenges. The speech recognition tests are quite practical. Show how the transducer model can be useful by looking into various design considerations like maintaining recurrent states efficiently and deciding factors such as block size and attention mechanism design along, with model depth. 
However there are some areas that could use some enhancement; On page 3 at line 93 there seems to be a mistake in the sentence explaining how the output sequence probability is calculated. In Section 3 of the document there's no discussion about how the methods for end of block modeling perform in comparison to each other. It would be helpful to know how using the <e> symbol stacks up against the two methods, in terms of performance.  
Section 3 of the document suggests aligns being computed frequently than updating models is reminiscent of the utilization of lattices in training acoustic models through sequence discrimination techniques as previously demonstrated by experts like D.Pove and P.C.Woodland and B.Kingsbury.Many future studies on the transducer model could explore incorporating lattices to convey alignment details and possibly utilize strategies created for revisiting lattices with RNN language models to improve how the model considers all input and output up, until now. 
Minor revisions are recommended. In the seventh page at lines 202 203 it should be revised from "Log Mel filterbanks" to "Log Mel spectra." Additionally some TIMIT experiments differ from methods like phone merging and targeted data exclusion from the training dataset.By adhering to established frameworks as presented in studies by T.N Sainath et al and K.F Lee and H.W Hon and transparently noting any deviations makes it easier for comparisons, with other research works.  
Moreover starting the transducer with HMM GMM alignments and subsequently progressing with alignments deduced by the transducer might result in enhanced TIMIT performance. Is recommended for future investigation. In general the paper makes a contribution, to the field suggesting slight modifications and further investigations to amplify its significance.