This study suggests a method for dropout in deep neural networks to avoid overfitting issues commonly encountered in machine learning models. The researchers propose using sampling for dropout by selecting features or neurons based on probabilities derived from a multinomial distribution. They present an analysis of the risk limit for stochastic optimization with multinomial dropout and show that utilizing a distribution specific dropout technique results, in quicker convergence and lower generalization error rates. 
The article is nicely written with an concise explanation of their proposed method by the authors. The theoretical analysis appears solid. The results from experiments, on various benchmark datasets prove the efficiency of the dropouts they suggested. Additionally to batch normalization comparison provided by the authors which indicates that evolutionary dropout delivers performance. 
The papers noteworthy aspects consist of; 
A new idea has been suggested for dropout that adjusts to the changing distribution of outputs, from layers.
An in depth examination of the risk limit, in optimization involving multinomial dropout techniques.
The results, from standard datasets show how well the suggested dropouts work.
In a comparison, with batch normalization it was found that evolutionary dropout achieves performance levels.
The paper has some shortcomings such, as; 
The article presumes that the sampling probabilities are understood; however this may not align with real world scenarios.
The document lacks an explanation, on selecting the hyperparameters for the suggested dropouts.
The paper would be more valuable with experimental findings, on larger datasets and advanced models.
The paper is nicely written with an effective approach proposed within it! The theoretical analysis seems solid. The experimental results really show how useful the dropouts suggested are in practice! This paper could definitely bring a contribution, to the deep learning field. 
In favor of accepting the arguments; 
The study introduces a method for dropout that adjusts to the changing distribution of outputs, from different layers.
The theoretical analysis appears solid. The experiment outcomes showcase the efficacy of the suggested dropouts.
The research paper compares dropout with batch normalization. Finds that dropout evolution achieves similar performance.
Reasons supporting acceptance; 
The document presumes that the sampling probabilities are familiar; however this might not always be true, in real world scenarios.
The paper lacks an explanation, on selecting the hyperparameters for the suggested dropouts.
The paper would be improved by including experimental outcomes, from larger datasets and more sophisticated models.
My rating for this is an 8, out of 10.
Recommendation; I suggest accepting the paper with some revisions as it is well written and presents a unique and efficient approach. The theoretical analysis appears solid. The experimental findings effectively showcase the benefits of the suggested dropouts. However it would be advantageous to include experimental results using larger datasets and more intricate models. Furthermore a concise guide, on selecting hyperparameters for the proposed dropouts would enhance clarity in the paper. 