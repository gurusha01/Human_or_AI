This study delves into a method to improve the efficiency of deep networks called dropout by presenting a theoretical basis and experimental confirmation of its effectiveness. The authors begin by examining a linear network and establishing risk boundaries and modification guidelines that make use of multinomial sampling to choose dropout neurons during each update stage while considering the statistical characteristics of the datas features. The update rules are applied to networks by taking into account the second order statistics of a layer for batches of data. This also uncovers a link to internal covariate shift as discussed in existing literature. This new method shows results in terms of convergence and accuracy for both simple and complex learning tasks across various datasets. Enough, the way this method is put into practice seems simple but the outcomes indicate substantial improvements in performance compared to the traditional dropout technique setting a new standard, for dropout methods. This project is anticipated to make an impression and be embraced widely in the coming days ahead. The document is well grounded technically with assertions backed by both examination and real world trials offering a thorough piece of work that advances from an initial perceptive observation, to ultimate experiments demonstrating the enhancement in performance of the suggested approach. The prose is lucid. The structure is orderly rendering the material informative and replicable. The method is creative as it addresses the issue of dropout from an angle supported by theoretical constraints that pave the way for additional research and alternatives like delving into more intricate probability distributions, for dropout purposes.The link to covariate shift is notably enlightening. Here are a few ways to make it better; provide details on the techniques cost and clarify if the convergence speed refers to iteration count or performance in time; also fix notation and formatting problems like defining \mathcal{H) in line 104 and rephrasing line 242 (iv) as well as correcting errors, in Theorem 1 and Lemma 1 proofs and the supplementary material. Furthermore explaining Proposition 3 in depth especially how it relates to the KKT conditions could improve the clarity and significance of one of the key findings, in the paper. 