This research paper introduces a version of the CMA–ES algorithm that is a cutting edge stochastic optimization method without utilizing derivatives. By incorporating learning mechanisms into the algorithmic process to adjust the parameters of a multivariate normal sampling distribution effectively alters an underlying metric. The innovative approach suggested in this study involves updating the covariance matrix by using triangular Cholesky factors which leads to a decrease in computational complexity and memory demands in contrast, to current methods. An extensive numerical assessment shows that this innovative method upholds the effectiveness of the CMA–ES by retaining performance levels based on function evaluations while significantly reducing the algorithms processing time on the clock wall—especially evident in scenarios with numerous dimensions involved. This contribution holds importance within the realm of optimization and is expected to establish itself as the preferred option for medium to large scale operations due to its various benefits such as decreased memory usage and complexity along, with the capability to acquire covariance matrix eigenvalues at no extra expense. The paper is nicely. The methodology is thoroughly evaluated.The integration of machine learning and information geometry along with their applications in reinforcement and supervised learning adds value to the NIPS conference.However a small recommendation would be to clarify how the comparability of implementations is guaranteed in relation, to CPU usage and provide specifics on the programming language utilized for implementation. Furthermore the note accompanying Figure 2 implies that distinctions become more noticeable, as the dimensionality increases; however this observation does not seem to apply to the Cigar and Discus functions in 256 dimensions, which calls for a detailed explanation. 