This research delves into exploring how the Expectation Maximization algorithm and other iterative methods perform in achieving Maximum Likelihood Estimation when dealing with learning combinations of spherical Gaussians structures.The main discoveries shared encompass;  
Even if there are plenty of samples and distinct clusters in the mix like three clusters apart, from each other to show how the EM algorithm might get trapped in local optimums. 
When dealing with a combination of k Gaussian distributions in mixtures and employing initialization for the EM algorithms convergence process— the chances of achieving successful convergence decrease significantly as k grows larger due, to exp( Ώ(k)).
Upon studying Gradient EM analysis results it appears that the method tends to steer of getting stuck, in sharp saddle points and indicates that the main problem may stem from unfavorable local maxima positions. 
The paper effectively discusses these findings to show that Expectation Maximization (EM) doesn’t reach the maximum even with an infinite number of samples available for analysis. Nevertheless the reviewer sees these results as somewhat expected as the methods used follow a path. An important instance that highlights this is a scenario with a combination of three elements, where two're close together and one is far apart. This setup leads to a situation where there is a peak to the first two elements and two peaks close, to the distant cluster. This pattern can be predicted in advance when the distance between centers is greater than the root of d and leads to a situation similar, to k means clustering with familiar issues related to local maxima. 
Expanding this scenario to k elements involves some work. Is somewhat predictable as well. Likewise the challenges with starting points resemble the difficulties encountered in k means clustering and come with similar chances of failure. To sum up the findings presented in the paper suggest a narrative, about not reaching the best overall solutions and mostly match what was expected. 
Some interesting points to mention are that the Kumar Kannan study offers reassurance on the convergence of Lloyds heuristic in k means clustering and its connections to mixtures of Gaussians in cases where there is distinction present. It would be advantageous to have a discussion comparing how the EM algorithm operates in relation, to k means. Lloyds heuristic when faced with significant separation scenarios to better understand these findings. 