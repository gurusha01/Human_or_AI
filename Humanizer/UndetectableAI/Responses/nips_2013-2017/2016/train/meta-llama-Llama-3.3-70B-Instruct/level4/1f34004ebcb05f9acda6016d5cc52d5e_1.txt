This paper explores how a non linear loss minimization challenge in a graph focused ranking model approaches convergence when dealing with a random walk influenced by the weights of nodes and edges derived from features linked to nodes and edges through random walks.The current optimization methods that rely on objective values are not suitable, for this scenario; hence a new two tier strategy is suggested. In the beginning stages of the process a method is used to estimate the distribution of the Markov random walk in a way that progresses gradually and consistently over time; this method has been verified through comparison with various other techniques and has shown that it is possible to get a close estimate of the loss function value with a specified level of accuracy.The authors have also created a method based on gradients, for solving constrained linear optimization problems which involves using an imperfect source of information and have proven that it leads to reaching a stable point in the end. The main significance is, in applying this method to restricted optimization issues where the values of functions can be determined accurately and demonstrating its convergence before using it in the part of the suggested algorithm.The paper is well organized. The evidence seems accurate; however a detailed review of all evidence was not performed. There is a concern that the main optimization ideas in this study appear to be influenced by research by Yurii Nesterov and Vladimir Spokoiny on Random Gradient Free Minimization of Convex Functions in the Foundations of Computational Mathematics journal from 2015. Additionally the supervised PageRank algorithm has been suggested before. It might be an idea to add a conclusion if theres enough room, for it. To enhance the understanding of the optimization problem further and place the analyzed algorithm within a context will help clarify its significance compared to existing cutting edge solutions. 