This study expands on Nesterovs acceleration explained by Krichene and others at the NIPS conference in 2015 by offering a strategy to dynamically modify the weights of the algorithms iterative steps for faster convergence speed improvement is assured over conventional fixed schedule averaging methods as opposed to earlier adaptive averaging heuristics The practical outcomes show enhancements, in processing speed across various functions. The article is nicely. Presents a logical mathematical explanation that is simple to understand for those who are familiar with Krichene et al.s research work. The practical results are encouraging well since they demonstrate significant improvements in speed. Although the technical advancements build closely upon Krichene et al.s work this paper has the potential to make an impact due to the widespread use of Nesterovs acceleration technique. It is common, for methods to be assessed against Nesterovs acceleration using a predetermined weight sequence even during practical assessments. Nevertheless the paper points out that by making Nesterovs method adaptable with a tweak can result in significant enhancements. To add technical depth to the paper it would be useful to analyze how the discretized process maintains the rate of convergence. This seems like a simple task to accomplish. Moreover investigating links between adaptive averaging in Nesterovs method and the use of the Conjugate Gradient method, for optimizing strongly convex quadratics could offer valuable perspectives. 