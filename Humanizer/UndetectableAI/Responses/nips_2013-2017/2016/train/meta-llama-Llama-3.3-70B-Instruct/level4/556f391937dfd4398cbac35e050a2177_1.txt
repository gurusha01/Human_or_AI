This document addresses the challenge of developing deep feature embeddings in learning processes where a significant barrier lies in the distribution of features within the space that makes it challenging to perform effective hard negative mining tasks successfully.The writers introduce an element that integrates the average position of a feature pair with its variance by drawing insights from [35] to determine the similarity between the pair of features.By incorporating details into this element it adequately addresses the issue of non uniform density, in the feature space. The unit stands out as being easily distinguishable. Can smoothly fit into current convolutional neural networks (CNNs). A specific loss function with a double header hinge structure is created by considering the contrasts and similarities, between pairs both after the units operation takes place. Within each mini batch of gradient descent (SGD) challenging quadruplets are selected by pinpointing pairs of positive instances with minimal similarity alongside corresponding negative pairs displaying high similarity levels. The effectiveness of this method is shown in tests related to finding images using the CUB_200_2011 and CARS196 datasets well as in tasks involving transfer and zero shot learning with the ImageNet_10k and ImageNet 2010 datasets leading to enhancements in all areas with a focus on reducing training time The research tackles a recognized and important issue, in computer vision with a fresh perspective The presentation is easy to understand with suitable sources cited The findings are interesting as they suggest that combining the suggested unit with hard mining has its benefits. However it would be helpful to have clarity regarding the difference between the "PDDM score" and the "Quadruplet + PDDM" rows in Table 1. Specifically we need to understand if they indicate retrieval results using the PDDM modules output versus the learned embedding before the PDDM module using distance. Moreover there seems to be some uncertainty, about the significance of embedding loss $ E_e $ in Equation (4) especially considering that gradients can be backpropagated through the PDDM modules. Furthermore it's not clear if the characteristics, for negative mining are recalculated with each minibatch or stored and updated periodically to improve efficiency. 