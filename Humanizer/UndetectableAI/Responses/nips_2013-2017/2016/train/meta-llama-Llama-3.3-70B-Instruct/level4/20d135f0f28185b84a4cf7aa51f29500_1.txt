The paper introduces a method for training ensemble models using stochastic gradient descent techniques that build on the previous research of Guzman River and colleagues [reference 8]. It focuses on leveraging the output loss from a single classifier within the ensemble to promote diversity among classifiersâ€”a significant advancement in integrating this approach, with deep neural networks technology.The manuscript is well organized. Holds the readers interest throughout. My main worry is about how innovative the algorithm is. I'm not sure if it's a big improvement over the previous version [reference]. The authors make a case, for their changes making the algorithm more useful which makes me lean towards suggesting acceptance of their work. One idea to explore further could be to see how well the MCL algorithm works using batch sizes compared to using the entire dataset all at once. It would also be helpful to dig into the impact of "diversity" as this could give us more valuable insights and make the research even stronger. 