The manuscript delves into a modern method to improve the efficiency of deep neural networks. Dropout technique is the focus here. It starts by examining a linear network and derives theoretical understanding using risk bounds and update principles that involve using multinomial sampling to pick dropout neurons during each update stage while considering the statistical characteristics of the data features at a second order level. On, in the manuscript the authors expand on these update principles to apply them to deep networks by incorporating the second order statistics of individual layers when dealing with mini batches of data. This method also creates a link to the covariate shift phenomenon that has been extensively researched in existing literature.The new technique shows enhancements in both the speed of reaching a solution and the precision for shallow and complex networks, on various sets of data.  
The paper stands out because it seems easy to put into practice; however the outcomes show enhancements in performance compared to regular dropout methods are evident. This approach establishes a standard for dropout strategies and I expect it to be widely embraced soon. The research is sound from a perspective and the assertions are supported by both theoretical evaluations and practical tests. It is an investigation that starts with an astute observation and methodically advances towards confirming the performance enhancements, through experiments. The manuscript is nicely crafted with a structure that is informative and can be easily replicated.  
The new method is quite unique because no previous studies have looked at the dropout issue from this angle before.Its use of limits not only enhances its significance but also paves the way for additional research possibilities like exploring more intricate probability distributions, for dropout.An insightful aspect is how it relates to covariate shift.  
Feedback; The writers should explain the computational expenses involved in their approach more clearly.Is the speed of convergence mentioned measured based on the number of iterations (as it seems)? If yes how does the method fare in terms of time elapsed rather than just iterations completed What exactly does \(\mathcal{H}\) signify on line 104 Could line 242 (iv) be rephrased to enhance clarity In the proof of Theorem 1 around lines 15 and 16 I noticed that the second term doesn't have the widehat symbol for \(\mathcal{L}\) and the same problem is present, in line 16 well.The phrase "the bound of in terms of" in line 16 needs to be revised for accuracy.In the proof of Lemma 1,the equality should actually be an inequality. In the information provided after line 35 of the document and in the second mathematical line mentioned there's a missing set of parentheses around the term related to the expectation, over \(\mathcal{M}\). Regarding Proposition 3 outlined in the material; while it was established based on the KKT conditions as mentioned in the paper itself; additional elucidation is necessary since it represents a significant outcome of the research paper. 