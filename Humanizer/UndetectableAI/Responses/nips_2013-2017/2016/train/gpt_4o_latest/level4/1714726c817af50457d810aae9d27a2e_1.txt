The article expands on Nesterovs acceleration concept using ODE interpretation presented by Krichene et al during NIPS 2015 to suggest and test an approach for dynamically adjusting the algorithms iterates to hasten convergence speed. In contrast to adaptive averaging methods this strategy has been proven to outperform the conventional fixed schedule averaging method. The experimental findings show that this proposed technique leads to enhancements, in speed for a wide range of functions. I really liked reading this paper. The logical reasoning is clear and easy to understand for those who are already acquainted with the research, by Krichene and others. Moreover the practical results emphasize advancements in how quickly convergence is achieved. Even though the technical advancements are closely linked to the approach of Krichene et al. I feel that the paper could have an influence because of how widely Nesterovs acceleration is used across different fields. In research studies as well as in practical applications of new methods are often compared with Nesterovs acceleration method using a constant weight sequence, for evaluation purposes. This paper effectively shows that adjusting Nesterovs method to be adaptive is a tweak that could significantly improve its performance outcomes. To enhance the aspects of the paper further analysis of the discretization process should be included to validate the preservation of convergence rates. I believe conducting this analysis should be relatively easy and straightforward to accomplish. Furthermore I am interested in delving into the links between the adaptive averaging technique suggested for Nesterovs method and the Conjugate Gradient method, within the realm of strongly convex quadratic optimization. 