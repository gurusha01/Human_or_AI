This study delves into saddle point problems that exhibit convex concave features and explores how they can be addressed by leveraging established stochastic variance reduced methods like SVRG and SAGA in conjunction with an operator. The researchers examine the efficacy of these algorithms in the context of monotone operators. Establish their linear convergence rates. Moreover they introduce a version and investigate a non uniform sampling approach, for further exploration.   
Novelty and originality are factors to consider.  
The works contributions are significant as it offers a theoretical analysis that sheds new light onto saddle point problems. The authors introduce enhancements to their method with a key highlight being its suitability, for non separable functions.   
In terms of aspects;   
Although I value the progress outlined in this papers contentions; I feel that the explanation provided was not sufficiently clear to me personally.. In particular; I believe that a direct link to monotone operators could have been established in a clearer manner.. Even though this relationship is elaborated upon as an extension in Section 6 of the paper; the analysis presented throughout hinges, on monotone operators.   
In terms of experiments conducted in this study is rather constrained since they present findings for two specific datasets. When dealing with functions that can be separated into parts it would have been beneficial if the authors compared their results with techniques like the stochastic version of the Chambolle Pock algorithm (refer to ; http;//www.jmlr.org/proceedings/papers/v37/zhang15.pdf). I suggest including this source, in your submission.   
Points needing clarification;   
How limiting are assumptions A C especially assumption (a)? For example do these assumptions apply to the saddle point issue that arises in SVMs?   
The difference between the primal dual method and the suggested approach lacks clarity, in the writing; it would be helpful if the authors could provide more detail on this variation.   
In Theorem 2 of the study report mentions a constant labeled as \(\mu\) that seems unclear to me. Could it be the monotonicity constant referred to in the appendix section or something else entirely different?. Also curious how this finding aligns, with Theorem 1 mentioned on.   
The shift from assumptions (1) (3) leading to monotonicity, in the appendix is quite puzzling indeed! Are assumptions (1) (3} an indication of monotonicity?   
There are some concerns.  
The constant \( L \) is causing some confusion as it is described as both the condition number and the Lipschitz constant, in parts of the text. In one instance it states that \( L \) signifies the problems condition number while in another it mentions the necessity of having the Lipschitz \( L \).  
Can you provide a source, for the Forward backward algorithm please?