In contrast to methods used in the past this new approach presents notable benefits, such as a considerable decrease in the computational workload for updates and a lower memory demand for storing the algorithms state variables. The suggested technique has been subject to numerical assessment showcasing its ability to uphold the efficacy of the typical CMAâ€“ES in terms of required function evaluations to meet a goal while notably cutting down on the time taken for computation especially in scenarios, with high dimensions. This paper is an addition to the optimization field as it introduces a fresh version of CMA. ES with the potential to be widely adopted for medium to large scale problems due to its advantages like decreased memory needs and simplified computational processes along, with automatically computing the eigenvalues of the covariance matrix. The paper is nicely. The suggested method is thoroughly assessed. It's really nice to see this work being showcased at NIPS because CMA. ES has ties to machine learning and its applications in reinforcement learning and supervised learning are very important, in the field of information geometry.  
Could you please clarify how you ensure that the various implementations utilized in the experiments are comparable in terms of CPU performance? For example. Are they coded in the programming language? Additionally when referencing Figure 2 and stating "the higher the dimensionality is,the more pronounced the differences " it doesn't seem to apply to the Cigar and Discus functions in dimension 256.Would you mind providing an explanation, for this observation?