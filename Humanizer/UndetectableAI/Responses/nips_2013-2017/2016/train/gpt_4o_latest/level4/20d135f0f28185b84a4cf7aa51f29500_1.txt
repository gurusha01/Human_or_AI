This article presents an approach in stochastic gradient descent algorithm for training ensemble models by expanding on the research done by Guzman Rivera et al.[8]. In this approach the loss of the model is determined by the loss of the most effective single classifier within the ensemble resulting in a varied array of classifiers.The key innovation, in this study lies in implementing a gradient descent algorithm for training that broadens the scope of this method to deep neural networks. The article is nicely. Interesting to read.I'm thinking mostly about whether the algorithms uniqueness goes beyond a small step forward, from whats in reference [8]. However the authors have effectively shown that their adjustments notably improve the algorithms usefulness.This convinces me to favor its approval.I also think it would be useful to test how well the MCL algorithm performs when using batches of analyzing the whole dataset. I also want an examination of how the algorithm influences the variety of classifiers.