The research paper delves into how the EM algorithm behaves when applied to learning mixtures of Gaussians through iterative methods for Maximum Likelihood Estimation. Key findings from the study reveal that the algorithm may not always reach the optimal solution; 1. Even in scenarios where there are three well separated clusters within the mixture and an abundance of samples available for analysis  algorithms like EM can still get stuck in local optimal solutions. 2. When dealing with mixtures of k Gaussians and employing initialization for EM  success is only guaranteed with a probability that is at most exp( Î©(k)). 3. Further insights, from the investigation highlight...Gradient EM tends to steer of rigid saddle points in a general sense which suggests that stumbling upon unfavorable local maximums is usually the main concern, at hand here.The research paper lays out an array of findings that demonstrate how EM struggles to reach the best overall outcomes (even when working with an infinite number of samples). That being said though I must admit that I wasn't exactly taken aback by the results and the methods used seemed expected and within the norm.  
The key example provided shows a mix of 3 elements where two are closely related and the third is noticeably distant from them. The writers show that when aiming for the log likelihood objective there's a point where one center aligns with the two elements while the other two centers match up with the far off cluster. This occurrence is quite anticipated when the gap between clusters is significantly larger, than the square root of d. In this scenario of segregation between clusters where there is no overlap present; the process of learning combinations of Gaussians resembles the concept of k means clustering closely. Similar instances have demonstrated results when implementing Lloyds algorithm in k means clustering. Extending this illustration to k components involves a method that is complex, from a technical standpoint but somewhat expected nonetheless. Similarly to how k means clustering encounters issues with random initialization failure probabilities being comparable due to reasons. This similarity can motivate strategies such as opting for k log(k) centers or employing distance squared sampling, in k means.
Overall the findings consistently indicate that the EM method does not reach optima and I believe the conclusions are in line, with what was anticipated.  
I appreciate your feedback. Your thoughts are valuable, to me. Lets keep the conversation going.  
Kumar and Kannan offer assurances of convergence for Lloyds approach in k means clustering, which also applies to mixtures of Gaussians under conditions of separation adequacy. Although EM and Lloyds approach show differences in situations with Gaussians (as demonstrated by Balakrishnan Wainwright Yu) they exhibit similar behavior in scenarios, with substantial separation as illustrated in the examples provided here. In my opinion; Making a comparison, between k means (and Lloyds heuristic) outlining both their similarities and differences would greatly improve the paper. 