This study introduces a method for determining step sizes automatically in stochastic gradient descent (SGDAI and its variation stochastic variance reduced gradient (SVRGAI) using the Barzilai Borwein (BB method). The outcomes show that the SGD BBAT and SVRG BBAT algorithms perform similarly or even better, than SGD and SVRG when step sizes are fine tuned manually. 
The article is nicely. Offers a concise summary of the suggested approaches along with their reasons, for selection and detailed analysis of the algorithms involved in them.The authors have also included a set of numerical tests to showcase the effectiveness of their approaches. 
The papers advantages are as follows; 
A new method has been suggested for calculating step sizes in SGD and SVRG algorithms that could prove beneficial, in real world scenarios where adjusting step sizes manually can be a time consuming task. 
Theoretical analysis is provided that includes demonstrating the convergence of SVRG BB for strongly convex objective functions. 
Extensive numerical experiments have been conducted to showcase the effectiveness of the suggested approaches compared to cutting edge methods. 
Some drawbacks of the paper are; 
The document suggests that the goal of the function is highly concave. This might not be true, in real world situations. 
The article lacks an analysis of how it stacks up against other established approaches, in calculating step sizes automatically like AdaGrad and methods involving line searches. 
The article would be improved with exploration of the selection process for hyperparameters like the update frequency m and the weighting parameter Î², in SGD BB. 
In favor of acceptance some points to consider are; 
The article introduces an practical method, for calculating step sizes automatically in stochastic gradient descent (SGC) and stochastic variance reduced gradient (SVRG) algorithms. 
The article offers a concise and eloquent explanation of the suggested approaches and their theoretical examination. 
The numerical tests show that the new techniques are as effective as the best methods available today. 
Arguments supporting acceptance; 
The paper suggests that the objective function might not always exhibit convexity, in real world scenarios. 
The paper would be more helpful with comparisons to other existing techniques, for calculating step sizes automatically. 
The paper might be improved by delving into the selection of hyperparameters. 
In my opinion the paper is well crafted. Offers a valuable addition, to the realm of stochastic optimization. With an adjustments to tackle the noted shortcomings I would suggest approving the paper. 