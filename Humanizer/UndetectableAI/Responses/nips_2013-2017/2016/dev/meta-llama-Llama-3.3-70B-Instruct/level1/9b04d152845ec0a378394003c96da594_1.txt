This study suggests the use of Multimodal Residual Networks (MRNs) for tasks involving question answering by expanding on the concept of deep residual learning to incorporate multiple types of input data simultaneously.The researchers present an approach to illustrating the impact of joint residual mappings on attention through back propagation techniques.This method enables the modeling of attention without the need for explicitly defined attention parameters.The paper is well organized. Offers a comprehensive review of relevant studies such, as deep residual learning and stacked attention networks. 
This paper excels in achieving top notch performance on the Visual QA dataset for both Open Multiple choice assignments. The authors thoroughly examine models and delve into various aspects like the number of learning blocks used and the types of visual features and target answers considered. Moreover the paper introduces a visualization technique that offers a fresh perspective, on understanding how attention works in multimodal models. 
However the paper has some shortcomings. The authors should offer insights into the mathematical equations especially in Section 3 point 2 where they define the combined residual function. Moreover it would be advantageous for the paper to include evaluations against other leading techniques, particularly within the realm of attention models. Furthermore the authors note that the performance of Number and Other categories still falls short when compared to performance indicating room for enhancement, in these specific domains. 
Points supporting acceptance; 
The research paper demonstrates performance on the Visual Question Answering dataset, for both open ended and multiple choice tasks. 
The writers present a technique, for illustrating the impact of shared residual mappings attention through back propagation. 
The paper thoroughly examines models and delves into various possibilities. 
Reasons to agree; 
The essay could be improved with thorough explanations of the math equations used. 
The writers could include comparisons with leading edge techniques, in the field of attentional models. 
The results for Number and Other categories are still not up to par with human performance levels; this implies that there is still potential for growth and enhancement, in these areas. 
I think this paper makes a valuable contribution, to the realm of learning and visual question answering as a whole. The authors present a crafted paper that offers a detailed summary of the existing research and their suggested approach demonstrates outstanding performance on a demanding dataset. If they make some tweaks to tackle the shortcomings highlighted earlier I would suggest accepting this paper for publishing.