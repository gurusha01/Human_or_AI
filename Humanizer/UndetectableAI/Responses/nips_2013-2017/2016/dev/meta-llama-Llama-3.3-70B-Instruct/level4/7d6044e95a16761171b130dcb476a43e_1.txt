The authors introduce a type of probabilistic models that combine the strengths of graphical models and neural networks by incorporating structured prior distributions from graphical models and nonlinear observation models from neural networks together.This fusion is intended to combine the advantages of models such as interpretability and fast inference, with the strong representation learning abilities of neural networks.One notable aspect of this research is the creation of a stochastic variational inference method tailored for teaching these hybrid models. This method uses a network powered recognition model to handle the non conjugate observation model and make it easier to update the mean field for estimating local hidden variables efficiently. Even though the experimental part is restricted to showcasing results on a synthetic dataset and a small video dataset with low resolution it introduces an interesting idea. Using a neural recognition model to create conjugacy in a substitute goal, for estimating local hidden variables. This method is quite effective but not as straightforward as techniques such as autoencoders (VAEs) where the model that recognizes directly produces the parameters of the variational posterior distribution.Including a comparison of these methods to discuss their advantages and disadvantages would add value to the discussion.The paper is well crafted but dense; it lacks detail in some parts which makes the additional material crucial, for understanding aspects.Despite the briefness of the section and the absence of quantitative results it is clear that the papers substantial conceptual contribution outweighs these limitations. The related work section doesn't mention some sequence modeling studies in the VAE framework and seems to have overlooked the research by Titsias and Lazaro Gredilla on Doubly Stochastic Variational Bayes, for non Conjugate Inference. 