This text delves into adjusting the step size in Stochastic Gradient Descent (SGDs) suggesting the use of the Barzilai Borwein (BB) method for calculating step sizes in SGDs and stochastic variance reduced gradient (SVRG). This proposal offers an approach compared to using fixed or predetermined decreasing strategies and includes a smoothing method for SGDsâ€”an important topic for algorithms, like SGD. In the beginning stages of using the BB method in SVRG (Stochastic Variance Reduced Gradient) simulations show that the process involves learning the step size after an adjustment period. However there seems to be an issue with ending up with step sizes that are too small early on as depicted in Figure 1. This outcome appears less, than optimal. Needs further explanation. When implementing SGD (Stochastic Gradient Descent) a technique for smoothing is introduced; however this has sparked worries as it appears to reintroduce a fixed and unchanging decline rate of 1/(k + 1) which could compromise the flexibility of the suggested approach by resembling the limitations of schemes. Furthermore and worth noting is that, in Lemma 1 the expectation should be contingent to ensure precision. 