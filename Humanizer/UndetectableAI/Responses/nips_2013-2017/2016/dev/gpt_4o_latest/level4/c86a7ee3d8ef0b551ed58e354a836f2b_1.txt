This study delves into the challenge of adjusting step sizes in Stochastic Gradient Descent (SGDD and its variations). It presents the implementation of the Barzilai Borwein (BB method for determining step sizes in SGD and Stochastic Variance Reduced Gradient (SVRG) eliminating the necessity for pre defined static or diminishing step size schedules For SGD a smoothing method is also integrated into the process The research addresses a significant concern for algorithms similar, to SGD The BB method is first utilized within the SVRG structureThe simulations are quite interesting as they show that the best step size is learned over time following an adjustment period. I wonder about the overshooting towards very small step sizes at the start of Figure 1 – it seems less than ideal. In their use of SGD implementation technique involves introducing a smoothing method; however I worry that the smoothing formula brings back a fixed evolving decrease, in adaptiveness. The reduction occurs in a pattern that decreases by ̈\frac{ ̈ e }{e} to follow along with the constraints of step sizes better known from before Additional notes should include rephrasing the expectation as a conditional expectation, in Lemma 01