This research paper presents a technique to estimate depth from an image using a monocular approach. The method includes applying filters to the image to create an extensive feature representation. These statistics are utilized to create a model that is predicted by a neural network at various points in the image. By incorporating uncertainty into the networks output, within this framework the individual estimations are merged to generate a consistent depth map using Half Quadratic Optimization. The findings regarding the NYU2 dataset are. Show promise overall in this papers evaluation is positive with substantial merits despite not introducing revolutionary advancements I find the probabilistic nature of the networks output to be noteworthy even though its not a completely explicit probabilistic model it effectively includes uncertainty, in the forecasts moreover the consensus phase which amalgamates partial observations serves as a refined approach to address uncertainties however I have a few reservations and inquiries 1The concept of relying on predetermined calculated features is intriguing but could we possibly teach the model to learn those features using an iterative approach within the same system instead? Why was K means used to learn the parameters in Section 3. 2 ? Wouldn't Expectation Maximization (EM) be an intuitive option that could potentially deliver superior outcomes? What are the usual durations, for running tests with this method at testing time? It appears that this approach may have computational speeds. Based the information in Table 2 it seems like the order filters are crucial features since there is not much variation, in performance compared to the full model. Do you have any thoughts or comments regarding this finding ?