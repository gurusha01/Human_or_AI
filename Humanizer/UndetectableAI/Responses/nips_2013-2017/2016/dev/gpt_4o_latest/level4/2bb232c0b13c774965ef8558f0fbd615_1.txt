The researchers introduce a system for analyzing handwritten text in paragraph images with several horizontal lines incorporated into the parser established by Graves et al., as well as replacing the initial softmax decoder with Bidirectional LSTMs for text interpretation purposes.The systems effectiveness is tested on the Rimes and IAM datasets. Demonstrates results similar to leading methods on Rimes but slightly lower performance, on IAM. The project showcases an use of directed focus on images to capture series of handwritten text lines and expands on previous methods like the ones introduced by Xu and colleagues [38] along, with other researchers who utilized attention based on image regions.  
The paper could be better by making it stand on its own without needing references like Graves et al.s work to grasp the proposed model fully in section 3. Including an, in depth explanation of the MD LSTM model as outlined in [18] within section 3 would greatly help readers understand it better.  
The authors also need to provide explanation, for certain unexpected findings and assertions.  
Why is it that the suggested technique performs better than methods that rely on line labeling (referenced in Table 2 and explained in section 5)? Could this be because of how the decoder RNN's reset when using correct line breaks but not in the character sequence of the attention mechanism? More information, about how the authors applied Graves et al.s approach could shed light on this issue.   
The authors mention in line 142 that their technique is significantly faster, than the character decoder method proposed by Bluche et al.. They do not provide any direct runtime comparisons to support this assertion. Adding these would bolster their argument.   
In terms of outcomes the system shows accuracy in CER% in a particular setup using the Rimes dataset. However its performance is notably poorer with the IAM dataset because of insufficient training data. One simple test to confirm this theory would involve pretraining the model on both the test and training data, from another dataset before refining it with the IAM training set. The authors are recommended to incorporate this experiment.   
The authors need to clarify in line 213 why the attention mechanism doesn't account for punctuation marks.   
In terms of the system show promise; resolving these issues could improve the clarity, thoroughness and significance of the project. 