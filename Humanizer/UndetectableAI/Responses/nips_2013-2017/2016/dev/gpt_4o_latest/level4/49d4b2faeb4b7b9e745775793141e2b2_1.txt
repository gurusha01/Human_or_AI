The writers suggest that the effectiveness of DRAW models comes from how they generate data step by step and establish clear connections between each hidden aspect and the observation itself. Expanding on these concepts led to the development of latent variable models known as Matryoshka Networks (Matnets). This new design includes a up network (BU) a top down network (TD) and merging components that help share information between the two networks. Matnets share resemblances, with Probabilistic Ladder Networks (PLNs).The study introduces MatNet models that include a pathway from the observed data to hidden layers in the inference model and a top down decomposition of the variational posterior distribution. What sets MatNet apart from PLNs is its emphasis in establishing links between latent variables and observations. Researchers assess the performance of MatNet models in tasks related to density estimation using datasets, like MNIST, Omniglot and CIFAR with results that either match or surpass existing benchmarks.  
This paper is quite intriguing and well put together overall! Its main innovation lies in a VAE design that includes deterministic links in both the inference and generative models to facilitate step by step generation of observations. Similar to DRAW models that take an incremental approach as well but with latent variables not organized hierarchically at different levels like in this architecture mentioned here. Probabilistic Ladder Networks exhibit characteristics to the ones proposed here except for the absence of deterministic connections, in the generative model. The key innovation of this study is the introduction of fixed relationships in a layered creative model system. However the practical portion lacks analysis that specifically proves the significance of these fixed relationships in either deducing or creating models. Instead the experiments aim to demonstrate the effectiveness of the structure, across datasets. Although this is important it does not directly support the argument presented in the paper. 
The paper is pretty well written; however I think they could enhance the presentation of the proposed architecture to make it more accessible for readers. It would be helpful to start with a probabilistic overview of the model before diving into the detailed procedural computations in Section 2. Moreover it's important to include citations in all sections to acknowledge prior work and avoid giving the impression of claiming novelty. For example when discussing GRU modules it's essential to mention their previous use, in generative models like [24].Additionally Despite the explanation of how PLNs and DRAW are connected in the introduction the section, on related work fails to mention PLNs and should be rectified. It is also unclear whether the inference regularization technique outlined in Section 2. Was utilized in any of the experiments.  
There are errors, in Equations 11 and 12 as they should both have a term before p(x | z).