Exploring the Use of Tensor Networks, in Supervised Learning
This study delves into the use of tensor networks in learning activities focusing mainly on matrix product states (MPS). The writers introduce a method where tensor networks serve as a way to express non linear kernel learning models effectively showcased through their successful performance with the MNIST dataset yielding a test error rate below 1%. The document emphasizes the benefits of utilizing tensor networks including steering of issues related to dimensionality overload and the flexibility in optimizing internal tensor sizes while also offering opportunities, for parallel computing. Furthermore the writers offer an explanation of how tensor networks shape the learned models structure providing observations on selecting features and regularizing the model. 
The study expands on investigations in physics and machine learning by utilizing tensor networks to estimate complex tensors in quantum mechanics and condense layers of neural networks effectively without using the kernel trick technique as traditionally done before this work was conducted by the authors involved in this research project. Their innovation involves optimizing the weight vector in a Matrix Product State (MPS) structure for supervised learning tasks rather than employing the conventional kernel trick method. This novel strategy proves to be computationally efficient as it scales proportionally with the size of the training dataset and makes use of techniques such, as density matrix renormalization group (DMRG) in physics algorithms. The paper also positions itself in the realm of tensor techniques in machine learning by mentioning previous studies, on tensor decompositions and kernel methods. 
Advantages; 
The paper is well crafted in terms of aspects; it provides thorough explanations of the optimization algorithm and the structure of the MPS, with clarity and precision.The optimization algorithm is well founded and implemented with efficiency. 
The document is nicely. Offers enough information for others to replicate the work successfully with the added benefit of visual aids like tensor diagrams and a shared code repository, for easier access. 
The use of tensor networks to represent kernel learning models directly is innovative. So is the understanding of the structure of the model learned through tensor decomposition. 
The outcomes observed in the MNIST dataset are remarkable as they demonstrate performance while requiring minimal computational resources in comparison, to other methods used in similar tasks​​​​​. 
Areas needing improvement; 
The evaluation only focuses on the MNIST dataset as a benchmark which might not showcase the approachs effectiveness enough; including diverse datasets, for testing would enhance the paper significantly. 
Address challenges in optimization faced by the authors when combining gradient descent with the sweeping algorithm, which if resolved could enhance the scalability of the method, for larger datasets. 
The selection of the feature map seems arbitrary and its effect, on performance hasn't been fully investigated yet; a methodical examination of feature map design could prove advantageous. 
The paper discusses approaches but does not include real world comparisons, with the latest kernel methods or neural network designs. 
Reasons, in favor of approval; 
The article presents an practical method, for supervised learning that is based on established tensor network techniques. 
It shows performance on MNIST dataset and offers a promising direction, for future enhancements and expansions. 
The models ability to be understood and its flexible design add value to the field. 
Reasons, for not accepting; 
The scope of the assessment is restricted and the effectiveness of the approach, on intricate datasets is still uncertain. 
The optimization method works well. Could use some improvements to handle bigger datasets and include common practices like using mini batches efficiently. 
Suggestion; 
This paper should be approved as it offers an original addition to the realm of machine learning despite the areas that could be enhanced The research establishes a solid groundwork for further exploration, into tensor network models and how they can be applied. 