{
    "version": "2025-03-13-base",
    "scanId": "f8054036-896b-4a7a-97ed-d6d51fc36458",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "This paper introduces an LSTM focused method to pinpoint relationships in the usage of connectors by utilizing the concept that neural models can better grasp the varied and intricate essence of causal expressions compared to syntactic approaches.The tests carried out on the AltLex collection show reliable results endorsing this notion and offer initial clues, for model enhancement.The document features TensorFlow powered models employed in the experiments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999975562095642,
                    "sentence": "Numerous important aspects and inquiries come to mind.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973177909851,
                    "sentence": "The start of the paper seems like a review of existing literature rather than a full summary of its content which duplicates information in the later related work section It is okay to have a unique introduction but this one does not clearly express a position, on causation; it only mentions that causation cannot be simplified to syntax The positive impact is not explicitly mentioned until the end of the document",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "The paper is not very clear about the theory of causation it is based upon as the authors appear to follow a perspective akin, to David Lewis although this is not directly mentioned and the issues linked to this theory are left unexplored.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999970197677612,
                    "sentence": "The remarks made in line 238 and the temporal restriction mentioned in page 3 are confusing; it's unclear which theory is being referenced here.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973177909851,
                    "sentence": "The example mentioned in line 256 is perplexing because the authors seem to consider the statement, as incorrect; however a causal connection should be present if the statement is accurate.It fails to delve into the implications of event segmentation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "How they affect causal theories resulting in an ambiguous explanation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973773956299,
                    "sentence": "The description of Figure 1 seems confusing because it shows the \"Pair_LSTM\" variation with a diagram and unspecified representation layers between input and \" LSTM \" boxes It is essential to depict the connection, between the \" LSTM \" boxes and the layers above clearly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999975562095642,
                    "sentence": "The sentence at line 480 seems unclear because models don't actually need padding by default; if its necessary for TensorFlow or effective training purposes it should be explicitly mentioned in the context of causal meaning encoding, for understanding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977946281433,
                    "sentence": "The researchers discovered that utilizing two LSTMs (\"Stated_LSTM\") proves more efficient than using just a single LSTM; however this situation brings to mind previous debates in the natural language entailment field.Further research is required to ascertain the method for establishing causal relationships.The unexpected finding in line 587 is intriguing as it implies that the presumed connection, between input event meanings is not valid and independent encoding is favored.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "The performing hyperparameters were challenging to understand when comparing lines 578 and 636 because its unclear if these results are due, to the unpredictability of model data interactions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969601631165,
                    "sentence": "In Section 4 point 3 of the report states that the system can accurately determine the interpretation of the term 'which then' unlike Hidey and McKeowns system which lacks this ability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "However only providing one example is not evidence to back up this assertion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999976754188538,
                    "sentence": "To validate this argument effectively a variety of examples should be generated to evaluate how well the system performs and ascertain if the outcome is simply, by luck or not.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999982996057589,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.700394241186484e-06,
                        "ai_paraphrased": 0.9999982996057589
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.7002942411865141e-06,
                            "ai_paraphrased": 0.9999982996057589
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This paper introduces an LSTM focused method to pinpoint relationships in the usage of connectors by utilizing the concept that neural models can better grasp the varied and intricate essence of causal expressions compared to syntactic approaches.The tests carried out on the AltLex collection show reliable results endorsing this notion and offer initial clues, for model enhancement.The document features TensorFlow powered models employed in the experiments. \nNumerous important aspects and inquiries come to mind.\nThe start of the paper seems like a review of existing literature rather than a full summary of its content which duplicates information in the later related work section It is okay to have a unique introduction but this one does not clearly express a position, on causation; it only mentions that causation cannot be simplified to syntax The positive impact is not explicitly mentioned until the end of the document\nThe paper is not very clear about the theory of causation it is based upon as the authors appear to follow a perspective akin, to David Lewis although this is not directly mentioned and the issues linked to this theory are left unexplored. The remarks made in line 238 and the temporal restriction mentioned in page 3 are confusing; it's unclear which theory is being referenced here. \nThe example mentioned in line 256 is perplexing because the authors seem to consider the statement, as incorrect; however a causal connection should be present if the statement is accurate.It fails to delve into the implications of event segmentation. How they affect causal theories resulting in an ambiguous explanation. \nThe description of Figure 1 seems confusing because it shows the \"Pair_LSTM\" variation with a diagram and unspecified representation layers between input and \"​ LSTM​ \" boxes​ It is essential to depict the connection, between the \"​ LSTM ​\" boxes and the layers above clearly. \nThe sentence at line 480 seems unclear because models don't actually need padding by default; if its necessary for TensorFlow or effective training purposes it should be explicitly mentioned in the context of causal meaning encoding, for understanding. \nThe researchers discovered that utilizing two LSTMs (\"Stated_LSTM\") proves more efficient than using just a single LSTM; however this situation brings to mind previous debates in the natural language entailment field.Further research is required to ascertain the method for establishing causal relationships.The unexpected finding in line 587 is intriguing as it implies that the presumed connection, between input event meanings is not valid and independent encoding is favored. \nThe performing hyperparameters were challenging to understand when comparing lines 578 and 636 because its unclear if these results are due, to the unpredictability of model data interactions. \nIn Section 4 point 3 of the report states that the system can accurately determine the interpretation of the term 'which then' unlike Hidey and McKeowns system which lacks this ability. However only providing one example is not evidence to back up this assertion. To validate this argument effectively a variety of examples should be generated to evaluate how well the system performs and ascertain if the outcome is simply, by luck or not. "
        }
    ],
    "editorDocumentId": null
}