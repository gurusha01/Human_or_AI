{
    "version": "2025-03-13-base",
    "scanId": "ee650524-c2c2-4d38-8648-1a58d29d07ae",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999906420707703,
                    "sentence": "This study introduces a method for answering reading comprehension questions by using gated self matching networks that have shown impressive performance, in the SQuAD dataset rankings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999895095825195,
                    "sentence": "The authors suggest a modified version of networks with gated attention to assess the significance of passage sections in relation to the question, at hand.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999886155128479,
                    "sentence": "The authors have presented a self matching attention mechanism aimed at enhancing how the passage is represented by comparing it to itself and capturing information from the passage effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999858140945435,
                    "sentence": "The new model performs well on the SQuAD dataset compared with other competitive systems available, in the market.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999819397926331,
                    "sentence": "The paper has strong points;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999866485595703,
                    "sentence": "The authors illustrate how attention mechanisms are efficient in enhancing reading comprehension by highlighting the significance of parts of a passage, to answering questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999876618385315,
                    "sentence": "An inventive self matching attention mechanism has been introduced, enabling the model to enhance the passage representation and gather evidence from the entire passage effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999865889549255,
                    "sentence": "The paper showcases real world outcomes by achieving top notch performance on the SQuAD dataset and surpassing various rival systems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999851584434509,
                    "sentence": "The paper has shortcomings;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999837875366211,
                    "sentence": "The complexity of this model is quite high as it includes components and mechanisms that might pose challenges in terms of interpretation and analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999884963035583,
                    "sentence": "The authors show that the self matching attention mechanism works well but offer explanation on how it operates and why it is successful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999839663505554,
                    "sentence": "The authors missed out on comparing their self matching attention mechanism to attention mechanisms which could have given a better insight into its effectiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999884963035583,
                    "sentence": "Here are some queries, for the writers;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999870657920837,
                    "sentence": "Could you give me insights, into how the self matching attention mechanism works and show some visual aids to demonstrate the attention weights and how it enhances the passages representation?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999886751174927,
                    "sentence": "How does the gated attention based recurrent network stack up against attention methods, like hierarchical attention or multi perspective attention?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999890923500061,
                    "sentence": "Could you share information about how the training is done.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999902844429016,
                    "sentence": "Such, as the specific hyperparameter configurations and the optimization method being utilized?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999966498623517,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 3.350137648321444e-06,
                        "ai_paraphrased": 0.9999966498623517
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 3.350037648321474e-06,
                            "ai_paraphrased": 0.9999966498623517
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study introduces a method for answering reading comprehension questions by using gated self matching networks that have shown impressive performance, in the SQuAD dataset rankings. \nThe authors suggest a modified version of networks with gated attention to assess the significance of passage sections in relation to the question, at hand. \nThe authors have presented a self matching attention mechanism aimed at enhancing how the passage is represented by comparing it to itself and capturing information from the passage effectively. \nThe new model performs well on the SQuAD dataset compared with other competitive systems available, in the market. \nThe paper has strong points; \nThe authors illustrate how attention mechanisms are efficient in enhancing reading comprehension by highlighting the significance of parts of a passage, to answering questions. \nAn inventive self matching attention mechanism has been introduced, enabling the model to enhance the passage representation and gather evidence from the entire passage effectively. \nThe paper showcases real world outcomes by achieving top notch performance on the SQuAD dataset and surpassing various rival systems. \nThe paper has shortcomings; \nThe complexity of this model is quite high as it includes components and mechanisms that might pose challenges in terms of interpretation and analysis. \nThe authors show that the self matching attention mechanism works well but offer explanation on how it operates and why it is successful. \nThe authors missed out on comparing their self matching attention mechanism to attention mechanisms which could have given a better insight into its effectiveness. \nHere are some queries, for the writers; \nCould you give me insights, into how the self matching attention mechanism works and show some visual aids to demonstrate the attention weights and how it enhances the passages representation? \nHow does the gated attention based recurrent network stack up against attention methods, like hierarchical attention or multi perspective attention? \nCould you share information about how the training is done. Such, as the specific hyperparameter configurations and the optimization method being utilized? "
        }
    ],
    "editorDocumentId": null
}