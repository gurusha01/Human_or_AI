{
    "version": "2025-03-13-base",
    "scanId": "5412fa25-e825-4255-a584-9d86c0fb1393",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9996833205223083,
                    "sentence": "The study suggests a method for analyzing emotions across different modes by considering the connections between statements, in a video setting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996006488800049,
                    "sentence": "The writers present a network based on Long Short Term Memory ( LSTM ) that grasps the interdependencies between statements, allowing the system to identify characteristics that enhance emotion classification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996730089187622,
                    "sentence": "The key achievements of this study include;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996393918991089,
                    "sentence": "The authors suggest using a network based on LSTM that considers the connections, between utterances to enhance sentiment classification by capturing information effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993135333061218,
                    "sentence": "The writers present a fusion framework that merges context independent unimodal characteristics, with context specific traits obtained by the LSTM network to enhance performance significantly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993076920509338,
                    "sentence": "The new technique performs better than the best methods, on standard datasets which shows how effective the contextual LSTM network and hierarchical fusion framework are.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994769096374512,
                    "sentence": "The paper excels, in the following aspects;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999582052230835,
                    "sentence": "The authors suggest a method for analyzing emotions across different modes that relies on understanding the connections, between spoken expressionsᅳan advancement compared to current techniques.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994614720344543,
                    "sentence": "\"Enhanced results are seen with the technique as evidenced by its top notch performance, on standard datasets which proves the approachs efficiency.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995460510253906,
                    "sentence": "The authors show that their method can handle differences by conducting experiments that are not specific, to a particular personᅳa key benefit compared to other methods available.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999578058719635,
                    "sentence": "The researchers offer an examination of the findings in their study and emphasize the significance of different aspects while also acknowledging the constraints of their methodology.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994109272956848,
                    "sentence": "The shortcomings of this paper include;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995166063308716,
                    "sentence": "The researchers have only tested their approach on a number of datasets - just three, in total - which might not capture the full range of potential situations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993576407432556,
                    "sentence": "The authors failed to compare their fusion framework with other fusion methods, like early fusion or late fusion; they only compared it with a non hierarchical framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996181726455688,
                    "sentence": "The authors touch upon the significance of modalities without delving into a thorough analysis of how each modality impacts the overall performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996902346611023,
                    "sentence": "Authors I have some questions, for you;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993375539779663,
                    "sentence": "How do the writers intend to overcome the challenges posed by the methods shortcomings regarding handling audio inputs and subtle emotional tones?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996160864830017,
                    "sentence": "Could the writers offer information, about the qualitative analysis by providing examples of the specific statements that were wrongly categorized and explaining why they were misclassified?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999573826789856,
                    "sentence": "How are the authors intending to expand their method to handle types of sentiment analysis tasks that involve both text and images or audio and visuals together?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999834063790355,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.6593620964542185e-05,
                        "ai_paraphrased": 0.9999834063790355
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.6593520964542215e-05,
                            "ai_paraphrased": 0.9999834063790355
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The study suggests a method for analyzing emotions across different modes by considering the connections between statements, in a video setting​​​. The writers present a network based on Long Short Term Memory ( LSTM ) that grasps the interdependencies between statements​​, allowing the system to identify characteristics that enhance emotion classification​​.\nThe key achievements of this study include; \nThe authors suggest using a network based on LSTM that considers the connections, between utterances to enhance sentiment classification by capturing information effectively. \nThe writers present a fusion framework that merges context independent unimodal characteristics, with context specific traits obtained by the LSTM network to enhance performance significantly. \nThe new technique performs better than the best methods, on standard datasets which shows how effective the contextual LSTM network and hierarchical fusion framework are. \nThe paper excels, in the following aspects; \nThe authors suggest a method for analyzing emotions across different modes that relies on understanding the connections, between spoken expressions—an advancement compared to current techniques. \n\"Enhanced results are seen with the technique as evidenced by its top notch performance, on standard datasets which proves the approachs efficiency.\"\nThe authors show that their method can handle differences by conducting experiments that are not specific, to a particular person—a key benefit compared to other methods available. \nThe researchers offer an examination of the findings in their study and emphasize the significance of different aspects while also acknowledging the constraints of their methodology. \nThe shortcomings of this paper include; \nThe researchers have only tested their approach on a number of datasets – just three, in total – which might not capture the full range of potential situations. \nThe authors failed to compare their fusion framework with other fusion methods, like early fusion or late fusion; they only compared it with a non hierarchical framework. \nThe authors touch upon the significance of modalities without delving into a thorough analysis of how each modality impacts the overall performance. \nAuthors I have some questions, for you; \nHow do the writers intend to overcome the challenges posed by the methods shortcomings regarding handling audio inputs and subtle emotional tones? \nCould the writers offer information, about the qualitative analysis by providing examples of the specific statements that were wrongly categorized and explaining why they were misclassified? \nHow are the authors intending to expand their method to handle types of sentiment analysis tasks that involve both text and images or audio and visuals together? "
        }
    ],
    "editorDocumentId": null
}