{
    "version": "2025-03-13-base",
    "scanId": "d5cd5402-1b10-4fb5-a24e-5882df2d4d57",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999942779541016,
                    "sentence": "This study introduces a method to improve NLP tasks by utilizing embeddings generated from language models instead of traditional context free word representations as a basis for higher accuracy and performance levels in tagging and chunking tasks through the integration of embeddings, from extensive language models extracted from neural language model hidden states.The research also includes an examination that tackles various important queries.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999852180480957,
                    "sentence": "In general the paper is well done.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999898076057434,
                    "sentence": "There are some ways to make it even better;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999926686286926,
                    "sentence": "Using the test set for experiments raises some concerns.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999992847442627,
                    "sentence": "It might be better to revise Tables 5 and 6 to make use of development data instead which could prove beneficial in the long run.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999916553497314,
                    "sentence": "It would be beneficial to explore a variety of tasks since concentrating solely on NER tagging and chunk analysis might not fully showcase the language models capabilities in managing intricate and far reaching relationships across text passages.Such exploration could involve delving into tasks such, as SRL or CCG supertagging for a thorough understanding of the models potential.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999901056289673,
                    "sentence": "The paper suggests that a task specific RNN is needed because adding a CRF on top of language model embeddings didn't work enough yet it's not clear if the language model was adjusted in this test or not; if it wasn't fine tuned before trying out this idea could possibly avoid the need, for a task specific RNN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9999999999999999,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9999999999999999,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9999999999999999,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9999999999999999,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999982292560583,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.770743941755787e-06,
                        "ai_paraphrased": 0.9999982292560583
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.7706439417558171e-06,
                            "ai_paraphrased": 0.9999982292560583
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study introduces a method to improve NLP tasks by utilizing embeddings generated from language models instead of traditional context free word representations as a basis for higher accuracy and performance levels in tagging and chunking tasks through the integration of embeddings, from extensive language models extracted from neural language model hidden states.The research also includes an examination that tackles various important queries. \nIn general the paper is well done. There are some ways to make it even better; \nUsing the test set for experiments raises some concerns. It might be better to revise Tables 5 and 6 to make use of development data instead which could prove beneficial in the long run. \nIt would be beneficial to explore a variety of tasks since concentrating solely on NER tagging and chunk analysis might not fully showcase the language models capabilities in managing intricate and far reaching relationships across text passages.Such exploration could involve delving into tasks such, as SRL or CCG supertagging for a thorough understanding of the models potential. \nThe paper suggests that a task specific RNN is needed because adding a CRF on top of language model embeddings didn't work enough yet it's not clear if the language model was adjusted in this test or not; if it wasn't fine tuned before trying out this idea could possibly avoid the need, for a task specific RNN. "
        }
    ],
    "editorDocumentId": null
}