{
    "version": "2025-03-13-base",
    "scanId": "f59589b0-dc8a-43e6-aa93-c83f2ecb2e71",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.6492211222648621,
                    "sentence": "The manuscript discusses ways to convert number sequences of any length using the major system which links each digit to consonant sounds or combinations of them The conversion results in a string of words where the original digits match characters in the output with added vowels for word formation, without restrictions exploring methods to improve the memorability of the output by using syntax rules and practical strategies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7420032024383545,
                    "sentence": "The use of natural language processing concepts in this paper caught my attention as it delves into a subject within the field of ACL that I found interesting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8022018074989319,
                    "sentence": "Nevertheless the overall strategy and concepts discussed appear a bit outdated with a focus on ngram models common POS tag sequences and other methods that may have been relevant, around 15 to 20 years ago.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7912312150001526,
                    "sentence": "The concepts presented here may not offer newness to justify being published in ACL in 2017 since they do not make a substantial impact on the NLP domain in terms of modeling or search and only provide a limited contribution to a specific case of constrained generation, in the application field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7614898085594177,
                    "sentence": "With the one to one relationship between the input and output sequences in mind it seems like a character driven encoder decoder model, such as the one suggested by Sutskever and team in 2014 could be a suitable choice for this situation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7423660159111023,
                    "sentence": "This method might produce results without the need for additional complexities, like trigram models, POS tagging and scoring rules and refinement using a bigram model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7880750298500061,
                    "sentence": "Furthermore this technique would enable broad scale training across genres without depending on pre tagged datasets or parsers making it a more current and applicable method, for a paper from 2017.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.07332528267997859
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                }
            ],
            "completely_generated_prob": 0.4691260456659605,
            "class_probabilities": {
                "human": 0.5303385425546349,
                "ai": 0.4691260456659605,
                "mixed": 0.0005354117794047223
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5303385425546349,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.4691260456659605,
                    "human": 0.5303385425546349,
                    "mixed": 0.0005354117794047223
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The manuscript discusses ways to convert number sequences of any length using the major system which links each digit to consonant sounds or combinations of them The conversion results in a string of words where the original digits match characters in the output with added vowels for word formation, without restrictions exploring methods to improve the memorability of the output by using syntax rules and practical strategies. \nThe use of natural language processing concepts in this paper caught my attention as it delves into a subject within the field of ACL that I found interesting. Nevertheless the overall strategy and concepts discussed appear a bit outdated with a focus on ngram models common POS tag sequences and other methods that may have been relevant, around 15 to 20 years ago. The concepts presented here may not offer newness to justify being published in ACL in 2017 since they do not make a substantial impact on the NLP domain in terms of modeling or search and only provide a limited contribution to a specific case of constrained generation, in the application field. \nWith the one to one relationship between the input and output sequences in mind it seems like a character driven encoder decoder model, such as the one suggested by Sutskever and team in 2014 could be a suitable choice for this situation. This method might produce results without the need for additional complexities, like trigram models, POS tagging and scoring rules and refinement using a bigram model. Furthermore this technique would enable broad scale training across genres without depending on pre tagged datasets or parsers making it a more current and applicable method, for a paper from 2017. "
        }
    ],
    "editorDocumentId": null
}