{
    "version": "2025-03-13-base",
    "scanId": "c9fd62c5-f05e-46e6-8239-f37cb174bdce",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999918937683105,
                    "sentence": "This paper introduces a way to create word embeddings by developing representations for phrases and ideas that represent words in a similar manner.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999993085861206,
                    "sentence": "The new technique includes assigning a label to clusters of phrases and words that signify the same idea; substituting instances of these phrases and words with the label in the training dataset to form a \"tagged\" dataset; and merging the tagged dataset with the original one, for training purposes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999922513961792,
                    "sentence": "Researchers have created sets of concepts and words based o n an ontology, in the field of biomedicine by using collections of data and ontologies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999992847442627,
                    "sentence": "Additionally they have devised a dataset to evaluate how closely words are linked and how similar real world entities are.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999929070472717,
                    "sentence": "The article is nicely.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999932050704956,
                    "sentence": "The method used is simple but not revolutionary in any way.The impact of the work is somewhat constrained because it concentrates on assessment, within the field of biomedicine.A detailed examination of the test resource created could be valuable since it may be the most notable aspect of the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999959468841553,
                    "sentence": "A small technical problem was found in Equation 8 where the authors tried to explain the MAP calculation methodᅳan effort indeed!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955892562866,
                    "sentence": "However of ranking the entire set of words in the equation they could consider setting a natural limit.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999957084655762,
                    "sentence": "Moreover Equation 8 does not tackle probabilities directly even if we assume a vocabulary sizeᅳthis point can be shown quite easily.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955892562866,
                    "sentence": "Therefore revising the explanation to remove mentions of probability seems necessary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994158744812,
                    "sentence": "Furthermore I recommend an adjustment, for line 556; replace \"most concepts has\" with \"most concepts have.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999983204550271,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.6795449729972892e-06,
                        "ai_paraphrased": 0.9999983204550271
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.6794449729973193e-06,
                            "ai_paraphrased": 0.9999983204550271
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This paper introduces a way to create word embeddings by developing representations for phrases and ideas that represent words in a similar manner. The new technique includes assigning a label to clusters of phrases and words that signify the same idea; substituting instances of these phrases and words with the label in the training dataset to form a \"tagged\" dataset; and merging the tagged dataset with the original one, for training purposes. Researchers have created sets of concepts and words based o n an ontology, in the field of biomedicine by using collections of data and ontologies. Additionally they have devised a dataset to evaluate how closely words are linked and how similar real world entities are. \nThe article is nicely. The method used is simple but not revolutionary in any way.The impact of the work is somewhat constrained because it concentrates on assessment, within the field of biomedicine.A detailed examination of the test resource created could be valuable since it may be the most notable aspect of the paper. \nA small technical problem was found in Equation 8 where the authors tried to explain the MAP calculation method—an effort indeed! However of ranking the entire set of words in the equation they could consider setting a natural limit. Moreover Equation 8 does not tackle probabilities directly even if we assume a vocabulary size—this point can be shown quite easily. Therefore revising the explanation to remove mentions of probability seems necessary. \nFurthermore I recommend an adjustment, for line 556; replace \"most concepts has\" with \"most concepts have.\""
        }
    ],
    "editorDocumentId": null
}