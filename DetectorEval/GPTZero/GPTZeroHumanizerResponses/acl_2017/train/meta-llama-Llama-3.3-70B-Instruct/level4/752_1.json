{
    "version": "2025-03-13-base",
    "scanId": "fbac7843-a9dd-4c8b-9b54-21bfc26d75a7",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999926090240479,
                    "sentence": "The article showcases an use of seq2seq models in handling AMT parsing and construction duties by utilizing a pre prepared and organized form of the AMT graph and corresponding sentence during training sessions together as a pair.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999931454658508,
                    "sentence": "This strategy shows promising outcomes in AMT construction tasks where the integration of extra monolingual data through back translations demonstrates its effectiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999926686286926,
                    "sentence": "Nevertheless the performance, in parsing does not quite match up to studies that made use of additional semantic details as reported.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999945163726807,
                    "sentence": "The paper shines in showing how well seq2seq models work for AMRs by highlighting their ability to handle semantic structures effectively.The models use of paired training and back translation helps it grasp the connections between the AMRs and sentences result in better performance, for AMRs conversion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948143959045,
                    "sentence": "One major flaw in the paper arises from comparing its findings with studies where various aspects are changing simultaneously This makes it hard to make direct comparisons For instance the results in Table 2 are matched against PBMT which was trained using a different dataset (LDC2014TO12) containing 13 051 sentences whereas the new model was trained with LDC2015E86 consisting of 19 572 sentences To make a fair comparison it's crucial to reassess the methods using the same dataset, for training",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999952912330627,
                    "sentence": "The paper brings up points for consideration and debate; for example; there may be some similarity between the sentences in the Gigaword sample and the test sentences of LDC2015E86 that could result in contamination of the test set data.The changes made to the encoder that are mentioned in lines 244 249 require clarification on how they affect the effectiveness of the model.Additionally the specifics of implementation, like the sequence length used and the seq2seq framework employed need to be clearly explained for replication purposes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995231628418,
                    "sentence": "The paper needs some clarification on the tables and figures used - in Table 1 where the column labels need more explanation and there seems to be a discrepancy between the table and the text its paired with The CAMR results in Table 1 don't seem to align with what was reported in the original paper There is also some confusion around how wikification information, from LDC2015E86 was dealt with in the study",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999952912330627,
                    "sentence": "The paper could use a section to wrap up the main discoveries and significance of the study findings effectively outlined in a clear manner.The methods for decoding such as employing beam search should be clearly detailed out.Additionally the experiments vocabulary size and handling tokens in the dev/test sets need to be discussed along with the approach, for replacing unknown words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995768070221,
                    "sentence": "Strengthening the case study could involve assessing how well the model handles aspects like quantification and tense that are typically tough for AMRs to interpret accurately on AMRs might be valuable context, for understanding why AMRs are preferred over other semantic frameworks and how utilizing human generated AMRs can offer benefits in the analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955892562866,
                    "sentence": "When considering research directions it would be interesting to explore the reasons behind the varying effectiveness of the suggested method compared to earlier seq2seq techniques like those discussed by Peng et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948740005493,
                    "sentence": "Identifying how factors such, as architecture design, preprocessing strategies, data organization and overall data quality influence the models performance could offer perspectives for upcoming studies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999946355819702,
                    "sentence": "The paper should undergo proofreading to fix mistakes like punctuation errors and formatting inconsistencies, for better clarity and readability purposes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9999999999999999,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9999999999999999,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9999999999999999,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9999999999999999,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999977041169305,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.295883069539e-06,
                        "ai_paraphrased": 0.9999977041169305
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.2957830695390297e-06,
                            "ai_paraphrased": 0.9999977041169305
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The article showcases an use of seq2seq models in handling AMT parsing and construction duties by utilizing a pre prepared and organized form of the AMT graph and corresponding sentence during training sessions together as a pair. This strategy shows promising outcomes in AMT construction tasks where the integration of extra monolingual data through back translations demonstrates its effectiveness. Nevertheless the performance, in parsing does not quite match up to studies that made use of additional semantic details as reported. \nThe paper shines in showing how well seq2seq models work for AMRs by highlighting their ability to handle semantic structures effectively.The models use of paired training and back translation helps it grasp the connections between the AMRs and sentences result in better performance, for AMRs conversion. \nOne major flaw in the paper arises from comparing its findings with studies where various aspects are changing simultaneously This makes it hard to make direct comparisons For instance the results in Table 2 are matched against PBMT which was trained using a different dataset (LDC2014TO12) containing 13 051 sentences whereas the new model was trained with LDC2015E86 consisting of 19 572 sentences To make a fair comparison it's crucial to reassess the methods using the same dataset, for training \nThe paper brings up points for consideration and debate; for example; there may be some similarity between the sentences in the Gigaword sample and the test sentences of LDC2015E86 that could result in contamination of the test set data.The changes made to the encoder that are mentioned in lines 244 249 require clarification on how they affect the effectiveness of the model.Additionally the specifics of implementation, like the sequence length used and the seq2seq framework employed need to be clearly explained for replication purposes. \nThe paper needs some clarification on the tables and figures used – in Table 1 where the column labels need more explanation and there seems to be a discrepancy between the table and the text its paired with The CAMR results in Table 1 don't seem to align with what was reported in the original paper There is also some confusion around how wikification information, from LDC2015E86 was dealt with in the study\nThe paper could use a section to wrap up the main discoveries and significance of the study findings effectively outlined in a clear manner.The methods for decoding such as employing beam search should be clearly detailed out.Additionally the experiments vocabulary size and handling tokens in the dev/test sets need to be discussed along with the approach, for replacing unknown words. \nStrengthening the case study could involve assessing how well the model handles aspects like quantification and tense that are typically tough for AMRs to interpret accurately on AMRs might be valuable context, for understanding why AMRs are preferred over other semantic frameworks and how utilizing human generated AMRs can offer benefits in the analysis. \nWhen considering research directions it would be interesting to explore the reasons behind the varying effectiveness of the suggested method compared to earlier seq2seq techniques like those discussed by Peng et al. Identifying how factors such, as architecture design, preprocessing strategies, data organization and overall data quality influence the models performance could offer perspectives for upcoming studies. \nThe paper should undergo proofreading to fix mistakes like punctuation errors and formatting inconsistencies, for better clarity and readability purposes. "
        }
    ],
    "editorDocumentId": null
}