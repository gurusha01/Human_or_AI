{
    "version": "2025-03-13-base",
    "scanId": "4afe4ce2-a536-46c2-87b8-74d0444a562e",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999945759773254,
                    "sentence": "This article introduces a method for automated speech recognition (ASr).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999945759773254,
                    "sentence": "It suggests a combined CTC and attention end to end framework that utilizes the benefits of both methods, in training and decoding stages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999943971633911,
                    "sentence": "Strengths include;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999949932098389,
                    "sentence": "The study thoroughly examines the CTC and attention framework and shows its success in training and decoding tasks effectively.Multiple experiments suggest that this approach enhances performance, in both CSJ and Mandarin Chinese telephone speech recognition assignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999932646751404,
                    "sentence": "Areas, for improvement;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999992847442627,
                    "sentence": "There is an issue regarding the resemblance between this paper and the study referenced in Ref [Kim et al., 2016] which is set to be officially published at the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) scheduled for March 2017.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999930262565613,
                    "sentence": "Kim et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999923706054688,
                    "sentence": "in their 2016 work proposed a combined CTC (Connectionist Temporal Classification) attention method utilizing multi task learning (MTL) focusing on English Automatic Speech Recognition (ASr).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999929070472717,
                    "sentence": "In contrast to that approach described by Kim et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999915957450867,
                    "sentence": "this paper expands on the concept, for Chinese ASRs through the integration of joint decoding techniques.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999937415122986,
                    "sentence": "The authors did not effectively clarify the difference between the two works, in their paper; hence it is difficult to pinpoint the contributions made by this study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999820590019226,
                    "sentence": "(a); Heading;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999914169311523,
                    "sentence": "The paper titled \"Joint CTC Attention End to end Speech Recognition\" differs from the one referenced as \" CTC Attention Based End, to end Speech Recognition Using Multi task Learning\" in Ref [Kim et al., 2016].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940395355225,
                    "sentence": "The title seems general; a more focused title that emphasizes the main findings of this paper compared to previous works would be better suited especially with [Kim et al., 2016]s upcoming official publication, before this study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999909400939941,
                    "sentence": "(b); Lets kick things off with an introduction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999984622001648,
                    "sentence": "The writers suggest using the limited CTC alignment within a CTC and attention system by incorporating a CTC objective into an attention based encoder network for regularization purposes resembling the study by Kim et al., 2016.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999873638153076,
                    "sentence": "However the idea is not original since it stems from Kim et al., 2016 originally.The debate on the importance of merging CTC with attention based end, to end ASR is also nothing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999867081642151,
                    "sentence": "Moreover \" suggesting improvements upon the work by [Kim et al., 2016]\" has a clunky tone since it insinuates rehashing existing ideas rather than expanding on them with new insights and enhancements It's crucial to explicitly outline the unique contributions of this study and where it stands in relation, to current research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999830722808838,
                    "sentence": "(copyright symbol ) Findings, from the experiments;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999878406524658,
                    "sentence": "Kim and colleagues (2016) tested their approach on tasks while this study concentrates on tasks in Japanese and Mandarin Chinese instead.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999849200248718,
                    "sentence": "It would be interesting to delve into the unique hurdles linked with these languages that differ from English.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999982476234436,
                    "sentence": "Like handling various potential outcomes (like Kanji, Hiragana and Katakana) when dealing with Japanese speech input without depending on linguistic aids.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999987781047821,
                    "sentence": "This aspect could potentially make a contribution, within this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999865889549255,
                    "sentence": "Lets talk about something, in general.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999746680259705,
                    "sentence": "It's better that you reference Ref [Kim et al., 2016] from the IEEE ICASSP conference instead of the pre published arXiv version by Kim et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999739527702332,
                    "sentence": "which is titled \"Joint CTC.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999982476234436,
                    "sentence": "Attention Based End to.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999762177467346,
                    "sentence": "End Speech Recognition Using Multi task Learning\" and was presented at the IEEE International Conference, on Acoustics Speech and Signal Processing (ICASSP) in March 2017 (pages are yet t be finalized).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.999995723448578,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 4.276551422031949e-06,
                        "ai_paraphrased": 0.999995723448578
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 4.2764514220319795e-06,
                            "ai_paraphrased": 0.999995723448578
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This article introduces a method for automated speech recognition (ASr). It suggests a combined CTC and attention end to end framework that utilizes the benefits of both methods, in training and decoding stages. \nStrengths include; \nThe study thoroughly examines the CTC and attention framework and shows its success in training and decoding tasks effectively.Multiple experiments suggest that this approach enhances performance, in both CSJ and Mandarin Chinese telephone speech recognition assignments. \nAreas, for improvement; \nThere is an issue regarding the resemblance between this paper and the study referenced in Ref [Kim et al., 2016] which is set to be officially published at the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) scheduled for March 2017. Kim et al. in their 2016 work proposed a combined CTC (Connectionist Temporal Classification) attention method utilizing multi task learning (MTL) focusing on English Automatic Speech Recognition (ASr). In contrast to that approach described by Kim et al. this paper expands on the concept, for Chinese ASRs through the integration of joint decoding techniques. The authors did not effectively clarify the difference between the two works, in their paper; hence it is difficult to pinpoint the contributions made by this study. \n(a); Heading; \nThe paper titled \"Joint CTC Attention End to end Speech Recognition\" differs from the one referenced as \" CTC Attention Based End, to end Speech Recognition Using Multi task Learning\" in Ref [Kim et al., 2016]. The title seems general; a more focused title that emphasizes the main findings of this paper compared to previous works would be better suited especially with [Kim et al., 2016]s upcoming official publication, before this study. \n(b); Lets kick things off with an introduction.\nThe writers suggest using the limited CTC alignment within a CTC and attention system by incorporating a CTC objective into an attention based encoder network for regularization purposes resembling the study by Kim et al., 2016. However the idea is not original since it stems from Kim et al., 2016 originally.The debate on the importance of merging CTC with attention based end, to end ASR is also nothing. Moreover \" suggesting improvements upon the work by [Kim et al., 2016]\" has a clunky tone since it insinuates rehashing existing ideas rather than expanding on them with new insights and enhancements It's crucial to explicitly outline the unique contributions of this study and where it stands in relation, to current research. \n(copyright symbol ) Findings, from the experiments; \nKim and colleagues (2016) tested their approach on tasks while this study concentrates on tasks in Japanese and Mandarin Chinese instead. It would be interesting to delve into the unique hurdles linked with these languages that differ from English. Like handling various potential outcomes (like Kanji, Hiragana and Katakana) when dealing with Japanese speech input without depending on linguistic aids. This aspect could potentially make a contribution, within this paper. \nLets talk about something, in general.\nIt's better that you reference Ref [Kim et al., 2016] from the IEEE ICASSP conference instead of the pre published arXiv version by Kim et al. which is titled \"Joint CTC. Attention Based End to. End Speech Recognition Using Multi task Learning\" and was presented at the IEEE International Conference, on Acoustics Speech and Signal Processing (ICASSP) in March 2017 (pages are yet t be finalized)."
        }
    ],
    "editorDocumentId": null
}