{
    "version": "2025-03-13-base",
    "scanId": "08ce9dc6-0ca0-488a-9e4d-b773c7cc3590",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9962373375892639,
                    "sentence": "The document exhibits a variety of points.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9967162609100342,
                    "sentence": "The implementation of a method in recurrent neural network language models has shown better results compared to the traditional SGD method with dropout, across three different tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9966774582862854,
                    "sentence": "The unique aspect of implementing learning in Recurrent Neural Networks has not been thoroughly investigated before.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9964742064476013,
                    "sentence": "The creation of a computational Bayesian method for RNN models could be quite intriguing to the NLP community, for a range of uses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9958605170249939,
                    "sentence": "However there are an areas of concern that must be dealt with.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9928717017173767,
                    "sentence": "One of the issues we need to address is how we evaluate things.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9959222674369812,
                    "sentence": "Specifically;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9965830445289612,
                    "sentence": "Section 5 of the paper discusses how types of architectures like LSTM (Long Short Term Memory) GRU (Gated Recurrent Unit) and vanilla RNN (Recurrent Neural Network) perform in tasks involving character language models and compares different learning algorithms using the Penn Treebank dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9968600273132324,
                    "sentence": "Additionally in this section of research work highlights comparisons between RMSprop and pSGLG in character based language modeling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9978967905044556,
                    "sentence": "Contrasts SGD with or without dropout against SGLG with or without dropout, in word based language modeling tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9954541921615601,
                    "sentence": "The irregularity needs to be resolved by comparing both aspects (structures and learning methods) in character and word language model assignments to evaluate how well the suggested Bayesian learning strategies can adapt to tasks and datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9932154417037964,
                    "sentence": "The paper argues on line 529 that the improvement in performance mainly comes from incorporating noise and model averaging; however this assertion lacks concrete evidence to support it.It is suggested to carry out an A/B experiment comparing results with and, without the inclusion of noise and/or model averaging to validate this argument.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9870964884757996,
                    "sentence": "At line 724 in the report by Gal et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994020462036133,
                    "sentence": "it is noted that Gals dropout technique is solely utilized for the sentence classification task without any mention of its effectiveness, in tasks related to language models and captions tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995483160018921,
                    "sentence": "Given that Gals dropout method's n't restricted to sentence classification alone it would be beneficial to test its performance across all three tasks so that readers can comprehensively evaluate the effectiveness of the suggested algorithms compared to current dropout methods being used.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997116327285767,
                    "sentence": "At line 544 of the document in question is an ambiguity regarding the ordering of the samples (\\theta_1,...,\\theta_K).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996041059494019,
                    "sentence": "Its not clearly stated if samples, with posterior probabilities are more likely to have higher indices assigned to them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995889067649841,
                    "sentence": "Furthermore \" an alternative method could involve disclosing the outcome of selecting K out of S samples as well.'",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992495775222778,
                    "sentence": "Recurrent neural network (RNN) language models are quite costly in terms of computational resources for both training and evaluation tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994414448738098,
                    "sentence": "It would be helpful to compare the training and evaluation durations of the suggested Bayesian learning methods with stochastic descent (SGE) and dropout techniques.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991823434829712,
                    "sentence": "This comparison would allow readers to assess the enhancements against the rise, in training and execution durations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992536306381226,
                    "sentence": "Some additional explanations are required.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993609189987183,
                    "sentence": "In line 346 of the document it's important to clarify what \\theta_s means and whether it specifically denotes the MAP estimation of parameters derived from sample s.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992712140083313,
                    "sentence": "In lines 453 to 454 of the text should provide clarity, on how \\theta relates to dropout and dropconnect techniques.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993540644645691,
                    "sentence": "Finally we need to fix a spelling mistakes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990778565406799,
                    "sentence": "Correction needed for \"output \" as mentioned in line 211.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992877244949341,
                    "sentence": "Please correct \"RMSProp\" on line 738.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997932945046398,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046398,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046398,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046398,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999677936278502,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 3.220637214976131e-05,
                        "ai_paraphrased": 0.9999677936278502
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 3.2206272149761334e-05,
                            "ai_paraphrased": 0.9999677936278502
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The document exhibits a variety of points.\nThe implementation of a method in recurrent neural network language models has shown better results compared to the traditional SGD method with dropout, across three different tasks. \nThe unique aspect of implementing learning in Recurrent Neural Networks has not been thoroughly investigated before. \nThe creation of a computational Bayesian method for RNN models could be quite intriguing to the NLP community, for a range of uses. \nHowever  there are an areas of concern that must be dealt with.\nOne of the issues we need to address is how we evaluate things. Specifically; \nSection 5 of the paper discusses how types of architectures like LSTM (Long Short Term Memory) GRU (Gated Recurrent Unit) and vanilla RNN (Recurrent Neural Network) perform in tasks involving character language models and compares different learning algorithms using the Penn Treebank dataset. Additionally in this section of research work highlights comparisons between RMSprop and pSGLG in character based language modeling. Contrasts SGD with or without dropout against SGLG with or without dropout, in word based language modeling tasks. The irregularity needs to be resolved by comparing both aspects (structures and learning methods) in character and word language model assignments to evaluate how well the suggested Bayesian learning strategies can adapt to tasks and datasets. \nThe paper argues on line 529 that the improvement in performance mainly comes from incorporating noise and model averaging; however this assertion lacks concrete evidence to support it.It is suggested to carry out an A/B experiment comparing results with and, without the inclusion of noise and/or model averaging to validate this argument. \nAt line 724 in the report by Gal et al. it is noted that Gals dropout technique is solely utilized for the sentence classification task without any mention of its effectiveness, in tasks related to language models and captions tasks. Given that Gals dropout method's n't restricted to sentence classification alone it would be beneficial to test its performance across all three tasks so that readers can comprehensively evaluate the effectiveness of the suggested algorithms compared to current dropout methods being used. \nAt line 544 of the document in question is an ambiguity regarding the ordering of the samples (\\theta_1,...,\\theta_K). Its not clearly stated if samples, with posterior probabilities are more likely to have higher indices assigned to them. Furthermore \" an alternative method could involve disclosing the outcome of selecting K out of S samples as well.'\nRecurrent neural network (RNN) language models are quite costly in terms of computational resources for both training and evaluation tasks. It would be helpful to compare the training and evaluation durations of the suggested Bayesian learning methods with stochastic descent (SGE) and dropout techniques. This comparison would allow readers to assess the enhancements against the rise, in training and execution durations. \nSome additional explanations are required.\nIn line 346 of the document it's important to clarify what \\theta_s means and whether it specifically denotes the MAP estimation of parameters derived from sample s. \nIn lines 453 to 454 of the text should provide clarity, on how \\theta relates to dropout and dropconnect techniques. \nFinally we need to fix a spelling mistakes. \nCorrection needed for \"output \" as mentioned in line 211. \nPlease correct \"RMSProp\" on line 738. "
        }
    ],
    "editorDocumentId": null
}