{
    "version": "2025-03-13-base",
    "scanId": "db63a2e6-562c-49c6-9a36-d9bc255c8f6b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999949336051941,
                    "sentence": "Lets take a look.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999954104423523,
                    "sentence": "Summary of the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "The research paper discusses the issue of analyzing emotions in forms by introducing a new framework based on LSTM that looks at how different spoken words in videos are connected contextually.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960064888,
                    "sentence": "Unlike studies that view spoken words as separate elements this new approach focuses on understanding the timing and relationships between them resulting in better accuracy in identifying emotions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "The model combines text based information, with audio and visual components.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958276748657,
                    "sentence": "Uses both structured and unstructured methods to blend data from different sources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961853027344,
                    "sentence": "The test outcomes show a 5 to 10 percent enhancement compared to the existing techniques on well known datasets like MOSITRUSTEER CAPTCHA Service (MOUD) and International Engineering Consortium Multimodal Corpus, for Human Emotion Recognition (IECMMHE).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999964237213135,
                    "sentence": "These results were achieved in situations where the speakers identity's not a factor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999932646751404,
                    "sentence": "Key Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "The key innovation lies in the implementation of a LSTM structure that captures the connections between utterances in a video comprehensivelyᅳan advancement that fills a notable void in existing research practices where individual utterances are typically analyzed in isolation and showcases enhanced effectiveness, in sentiment analysis assignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973177909851,
                    "sentence": "The hierarchical fusion framework stands out for its approach of merging unimodal features prior to multimodal integrationᅳa significant advancement that surpasses non hierarchical fusion techniques and underscores the significance of contextually sensitive unmodal characteristics, in multimodal sentiment analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "The paper highlights the strength of the suggested approach by testing it in conditions where speakers are not identifiedᅳa practical situation that has not received much attention in previous studies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999993622303009,
                    "sentence": "Advantages",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960660934448,
                    "sentence": "The new technique shows enhancements (around 5 to 10%) compared to the best existing methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999957084655762,
                    "sentence": "Especially, in scenarios where the speaker is not pre defined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999954700469971,
                    "sentence": "Highlighting its real world usefulness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999963045120239,
                    "sentence": "The studies conducted are extensive and detailed as they encompass datasets like MOSiE and MOUD across text based and audiovisual modalities that further showcase the models adaptability through cross dataset assessments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997615814209,
                    "sentence": "Utilizing LSTMs to understand the connections between statements is a progress in analyzing sentiments across different modes of communication and tackles an important drawback of previous techniques, in this field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "The paper offers qualitative instances that demonstrate the advantages and drawbacks of various modalities to enrich the assessment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999935030937195,
                    "sentence": "Areas of improvement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979138374329,
                    "sentence": "The methodology of the LSTM framework lacks originality as it heavily relies on established techniques such as LSTMs and hierarchical fusion, for its effectiveness The uniqueness mainly stems from how these techniques are applied and integrated rather than from the core methodology itself.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971985816956,
                    "sentence": "The paper missed an opportunity by not having an attention mechanism to evaluate the significance of statements or aspects in the context based focus it emphasizes This could help in situations where unimportant or loosely related statements have an impact, on predictions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999996542930603,
                    "sentence": "Cross Language Challenges; When examining datasets across languages (from MOSI to MOUD) it was found that the performance of text and audio elements was lacking due to language variations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971389770508,
                    "sentence": "Although this issue is recognized in the papers discussion no specific strategies are suggested to address language situations, which restricts its usefulness, in diverse language environments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "The paper lacks an exploration of the computational efficiency issues related to the hierarchical LSTM framework and its potential impact, on real time applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961853027344,
                    "sentence": "Queries, for Writers",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977350234985,
                    "sentence": "How might the suggested framework deal with situations where the context's unclear or conflicting, between statements made by different speakers?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "Would incorporating an attention mechanism enhance its effectiveness in instances?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "Can we expand the structure to include cross language embeddings or translation methods to enhance results in multilingual datasets such, as MOUD?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969601631165,
                    "sentence": "What is the computational expense of using LSTM compared to basic models without context, in real time sentiment analysis tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "Additional Notes",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969601631165,
                    "sentence": "The article is nicely.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971985816956,
                    "sentence": "Offers a thorough assessment of the suggested frameworks effectiveness; yet integrating an attention mechanism and tackling cross language hurdles could boost its influence even more.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999890743143247,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.0925685675274227e-05,
                        "ai_paraphrased": 0.9999890743143247
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.0925585675274257e-05,
                            "ai_paraphrased": 0.9999890743143247
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "Lets take a look.\nSummary of the paper.\nThe research paper discusses the issue of analyzing emotions in forms by introducing a new framework based on LSTM that looks at how different spoken words in videos are connected contextually. Unlike studies that view spoken words as separate elements this new approach focuses on understanding the timing and relationships between them resulting in better accuracy in identifying emotions. The model combines text based information, with audio and visual components. Uses both structured and unstructured methods to blend data from different sources. The test outcomes show a 5 to 10 percent enhancement compared to the existing techniques on well known datasets like MOSITRUSTEER CAPTCHA Service (MOUD) and International Engineering Consortium Multimodal Corpus, for Human Emotion Recognition (IECMMHE). These results were achieved in situations where the speakers identity's not a factor. \nKey Contributions\nThe key innovation lies in the implementation of a LSTM structure that captures the connections between utterances in a video comprehensively—an advancement that fills a notable void in existing research practices where individual utterances are typically analyzed in isolation and showcases enhanced effectiveness, in sentiment analysis assignments. \nThe hierarchical fusion framework stands out for its approach of merging unimodal features prior to multimodal integration—a significant advancement that surpasses non hierarchical fusion techniques and underscores the significance of contextually sensitive unmodal characteristics, in multimodal sentiment analysis. \nThe paper highlights the strength of the suggested approach by testing it in conditions where speakers are not identified—a practical situation that has not received much attention in previous studies. \nAdvantages\nThe new technique shows enhancements (around 5 to 10%) compared to the best existing methods. Especially, in scenarios where the speaker is not pre defined. Highlighting its real world usefulness. \nThe studies conducted are extensive and detailed as they encompass datasets like MOSiE and MOUD across text based and audiovisual modalities that further showcase the models adaptability through cross dataset assessments. \nUtilizing LSTMs to understand the connections between statements is a progress in analyzing sentiments across different modes of communication and tackles an important drawback of previous techniques, in this field. \nThe paper offers qualitative instances that demonstrate the advantages and drawbacks of various modalities to enrich the assessment. \nAreas of improvement \nThe methodology of the LSTM framework lacks originality as it heavily relies on established techniques such as LSTMs and hierarchical fusion, for its effectiveness The uniqueness mainly stems from how these techniques are applied and integrated rather than from the core methodology itself. \nThe paper missed an opportunity by not having an attention mechanism to evaluate the significance of statements or aspects in the context based focus it emphasizes This could help in situations where unimportant or loosely related statements have an impact, on predictions. \nCross Language Challenges; When examining datasets across languages (from MOSI to MOUD) it was found that the performance of text and audio elements was lacking due to language variations. Although this issue is recognized in the papers discussion no specific strategies are suggested to address language situations, which restricts its usefulness, in diverse language environments. \nThe paper lacks an exploration of the computational efficiency issues related to the hierarchical LSTM framework and its potential impact, on real time applications. \nQueries, for Writers\nHow might the suggested framework deal with situations where the context's unclear or conflicting, between statements made by different speakers? Would incorporating an attention mechanism enhance its effectiveness in instances? \nCan we expand the structure to include cross language embeddings or translation methods to enhance results in multilingual datasets such, as MOUD? \nWhat is the computational expense of using LSTM compared to basic models without context, in real time sentiment analysis tasks? \nAdditional Notes \nThe article is nicely. Offers a thorough assessment of the suggested frameworks effectiveness; yet integrating an attention mechanism and tackling cross language hurdles could boost its influence even more. "
        }
    ],
    "editorDocumentId": null
}