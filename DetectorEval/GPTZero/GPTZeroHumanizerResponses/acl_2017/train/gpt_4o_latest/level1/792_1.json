{
    "version": "2025-03-13-base",
    "scanId": "681716e0-b841-4e6e-a607-dd64473fad5a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999956488609314,
                    "sentence": "Reflection, on the document",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999959468841553,
                    "sentence": "Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969005584717,
                    "sentence": "This research paper discusses LSTMEmbed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969601631165,
                    "sentence": "A model using a bidirectional Long Short Term Memory (LSTM) design to learn word and sense embeddings together effectively according to the authors claims in comparison to traditional embedding techniques like word2vec and GloVe, on common benchmarks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999968409538269,
                    "sentence": "Furthermore this study introduces LSTMEmbedSW as an extension that can learn word and sense embeddings within the vector space.The authors use existing embeddings to improve the representation quality and accelerate the training process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999959468841553,
                    "sentence": "The model is assessed based on tasks like comparing words for similarity and finding synonyms and analogies, in text data that includes both sense labeled collections.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999944567680359,
                    "sentence": "The primary findings of the paper, from my perspective are;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958276748657,
                    "sentence": "The Bidirectional LSTM Embedding Model known as LSTMEmbed merges bidirectional LSTMs with existing embeddings to acquire top notch word and sense representations efficiently This stands as the main innovation showcasing strong performance when compared to current approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999963641166687,
                    "sentence": "A new method called LSTMEmbedSW allows for the learning of word and sense embeddings, within a common vector space but its effectiveness varies somewhat.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948143959045,
                    "sentence": "Using pretrained embeddings as a training goal to enhance meaning and speed up training is an efficient advancement, in technology.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999993622303009,
                    "sentence": "Advantages",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961853027344,
                    "sentence": "Consistent strong results have shown that LSTMEmbed performs better than word embedding models like word2vec and GloVe, in tasks related to word similarity and identifying synonyms.It proves the effectiveness of this method proposed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999951124191284,
                    "sentence": "Utilizing pretrained embeddings to incorporate understanding is a smart decision that enhances the accuracy of representations and cuts down on training time significantly as demonstrated by the tests with more advanced embeddings, like SensEmbed, which confirm the effectiveness of this method further.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "The authors thoroughly test their model across tasks and datasets such, as word similarity assessments and synonym identification to understand its capabilities and limitations better.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999949336051941,
                    "sentence": "Sense Annotated Corpus Application; Incorporating sense tagged corpora, like BabelWiki and SemCor is an approach that showcases the benefits of integrating organized semantic databases with neural networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948143959045,
                    "sentence": "Areas, for improvement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999954104423523,
                    "sentence": "The architecture shows proficiency in using bidirectional LSTMs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999945759773254,
                    "sentence": "Lacks originality in its core design as it mainly relies on established methods, like context embedding and RNN based language models without bringing forth any groundbreaking concepts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994695186615,
                    "sentence": "The LSTMEmbedSW model, which combines word and sense embeddings shows performance in tasks when compared to the LSTMEmbed model sparking doubts, about the effectiveness of the shared vector space and whether the increased complexity is warranted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940395355225,
                    "sentence": "The model struggles with word analogy tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999993085861206,
                    "sentence": "A yardstick, for assessing embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994158744812,
                    "sentence": "Showing weak performance in this area according to the authors explanation that is not thoroughly examined or verified.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999938607215881,
                    "sentence": "Scalability Issues; Depending on sense labeled collections and pre trained embeddings could hinder the scalability of the method to languages or fields, with resources in that regard.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999929070472717,
                    "sentence": "Asking Authors Questions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948143959045,
                    "sentence": "Could you elaborate on why LSTMEmbedSW underperforms compared to LSTMEmbed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999936819076538,
                    "sentence": "Are there particular scenarios where the shared vector space provides benefits?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999937415122986,
                    "sentence": "The reason for the success in word analogy tasks is thought to be due to the complexity of the models design.Is it possible that there are reasons for this outcome such, as the selection of training goals or parameter settings being used?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999946355819702,
                    "sentence": "How well does the model handle words or meanings when working with unprocessed data sets that lack sense annotations?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999932646751404,
                    "sentence": "Additional.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999993085861206,
                    "sentence": "Reflections.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940395355225,
                    "sentence": "In general this study demonstrates a implemented use of bidirectional LSTMs in embedding learning backed by solid empirical evidence and meaningful practical insights.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994158744812,
                    "sentence": "Yet the absence of architecture and the subpar performance of the joint embedding model restrict its influence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940991401672,
                    "sentence": "Tackling these concerns, in research could greatly amplify the significance of this study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999983667324787,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.6332675212243351e-06,
                        "ai_paraphrased": 0.9999983667324787
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.6331675212243652e-06,
                            "ai_paraphrased": 0.9999983667324787
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "Reflection, on the document\nContributions\nThis research paper discusses LSTMEmbed. A model using a bidirectional Long Short Term Memory (LSTM) design to learn word and sense embeddings together effectively according to the authors claims in comparison to traditional embedding techniques like word2vec and GloVe, on common benchmarks. Furthermore this study introduces LSTMEmbedSW as an extension that can learn word and sense embeddings within the vector space.The authors use existing embeddings to improve the representation quality and accelerate the training process. The model is assessed based on tasks like comparing words for similarity and finding synonyms and analogies, in text data that includes both sense labeled collections. \nThe primary findings of the paper, from my perspective are; \nThe Bidirectional LSTM Embedding Model known as LSTMEmbed merges bidirectional LSTMs with existing embeddings to acquire top notch word and sense representations efficiently This stands as the main innovation showcasing strong performance when compared to current approaches. \nA new method called LSTMEmbedSW allows for the learning of word and sense embeddings, within a common vector space but its effectiveness varies somewhat. \nUsing pretrained embeddings as a training goal to enhance meaning and speed up training is an efficient advancement, in technology. \nAdvantages\nConsistent strong results have shown that LSTMEmbed performs better than word embedding models like word2vec and GloVe, in tasks related to word similarity and identifying synonyms.It proves the effectiveness of this method proposed. \nUtilizing pretrained embeddings to incorporate understanding is a smart decision that enhances the accuracy of representations and cuts down on training time significantly as demonstrated by the tests with more advanced embeddings, like SensEmbed, which confirm the effectiveness of this method further. \nThe authors thoroughly test their model across tasks and datasets such, as word similarity assessments and synonym identification to understand its capabilities and limitations better. \nSense Annotated Corpus Application; Incorporating sense tagged corpora, like BabelWiki and SemCor is an approach that showcases the benefits of integrating organized semantic databases with neural networks. \nAreas, for improvement\nThe architecture shows proficiency in using bidirectional LSTMs. Lacks originality in its core design as it mainly relies on established methods, like context embedding and RNN based language models without bringing forth any groundbreaking concepts. \nThe LSTMEmbedSW model, which combines word and sense embeddings shows performance in tasks when compared to the LSTMEmbed model sparking doubts, about the effectiveness of the shared vector space and whether the increased complexity is warranted. \nThe model struggles with word analogy tasks. A yardstick, for assessing embeddings. Showing weak performance in this area according to the authors explanation that is not thoroughly examined or verified. \nScalability Issues; Depending on sense labeled collections and pre trained embeddings could hinder the scalability of the method to languages or fields, with resources in that regard. \nAsking Authors Questions\nCould you elaborate on why LSTMEmbedSW underperforms compared to LSTMEmbed. Are there particular scenarios where the shared vector space provides benefits? \nThe reason for the success in word analogy tasks is thought to be due to the complexity of the models design.Is it possible that there are reasons for this outcome such, as the selection of training goals or parameter settings being used? \nHow well does the model handle words or meanings when working with unprocessed data sets that lack sense annotations? \nAdditional. Reflections.\nIn general this study demonstrates a implemented use of bidirectional LSTMs in embedding learning backed by solid empirical evidence and meaningful practical insights. Yet the absence of architecture and the subpar performance of the joint embedding model restrict its influence. Tackling these concerns, in research could greatly amplify the significance of this study. "
        }
    ],
    "editorDocumentId": null
}