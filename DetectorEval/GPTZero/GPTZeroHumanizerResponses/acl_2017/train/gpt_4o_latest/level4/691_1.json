{
    "version": "2025-03-13-base",
    "scanId": "efc9e374-fc9a-4a39-b093-55d0e32c4f00",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9955567717552185,
                    "sentence": "In summary",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9925084114074707,
                    "sentence": "This paper presents an approach to developing sense embeddings based in a lexical semantic reference, like WordNet.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9925934672355652,
                    "sentence": "However the paper does not directly assess the significance of the acquired sense embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9907213449478149,
                    "sentence": "Instead these sense embeddings are combined into word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9914566874504089,
                    "sentence": "Then evaluated through a subsequent task which involves predicting prepositional phrase (PP) attachment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9933829307556152,
                    "sentence": "The findings regarding predicting PP attachment seem strong and persuasive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9916756749153137,
                    "sentence": "\"Areas, for improvement;\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9937356114387512,
                    "sentence": "The significance of the sense embeddings is not assessed directly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9854301810264587,
                    "sentence": "Some parts of the model seem unclear to me.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.984657883644104,
                    "sentence": "For instance are the lambda i parameters considered hyperparameters or are they acquired through training?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9883140921592712,
                    "sentence": "Moreover I find the origin of the term \"rank\" quite confusing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.993020236492157,
                    "sentence": "Does it represent the sense rankings found in WordNet?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9904677271842957,
                    "sentence": "In studies on this topic of related research work on representing word meanings as a mix of different senses has been looked into before.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9890244603157043,
                    "sentence": "For instance the study titled \"Placing a meaning network within a language space\" (presented at NAACL in 2015), by Johansson and Nieto Pi√±a broke down word meanings into senses connected to a structure using a method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9892100691795349,
                    "sentence": "The same concept has also been used in the training of sense vectors in a manner.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9919593930244446,
                    "sentence": "This approach is evident, in the work titled \"Linear Algebraic Structure of Word Senses\" authored Arora et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9900614619255066,
                    "sentence": "It includes applications related words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9840487241744995,
                    "sentence": "I just have a small remarks;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9882277846336365,
                    "sentence": "No definitions, for types and tokens are necessary since these terms are commonly used in the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9662894606590271,
                    "sentence": "Why do we need the \\(\\lambdawi\\) in equation 4 when the probability is not normalized yet?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9774089455604553,
                    "sentence": "Lets talk about it in general.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.7932264695998812,
            "class_probabilities": {
                "human": 0.20660725234936764,
                "ai": 0.7932264695998812,
                "mixed": 0.00016627805075123634
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.7932264695998812,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.7932264695998812,
                    "human": 0.20660725234936764,
                    "mixed": 0.00016627805075123634
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999600154617518,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 3.998453824823875e-05,
                        "ai_paraphrased": 0.9999600154617518
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 3.9984438248238774e-05,
                            "ai_paraphrased": 0.9999600154617518
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "In summary   \nThis paper presents an approach to developing sense embeddings based in a lexical semantic reference, like WordNet. However the paper does not directly assess the significance of the acquired sense embeddings. Instead these sense embeddings are combined into word embeddings. Then evaluated through a subsequent task which involves predicting prepositional phrase (PP) attachment. \n  \nThe findings regarding predicting PP attachment seem strong and persuasive. \n\"Areas, for improvement;\"  \nThe significance of the sense embeddings is not assessed directly.   \nSome parts of the model seem unclear to me. For instance are the lambda i parameters considered hyperparameters or are they acquired through training? Moreover I find the origin of the term \"rank\" quite confusing. Does it represent the sense rankings found in WordNet?   \nIn studies on this topic of related research work on representing word meanings as a mix of different senses has been looked into before. For instance the study titled \"Placing a meaning network within a language space\" (presented at NAACL in 2015), by Johansson and Nieto Pi√±a broke down word meanings into senses connected to a structure using a method. The same concept has also been used in the training of sense vectors in a manner. This approach is evident, in the work titled \"Linear Algebraic Structure of Word Senses\" authored Arora et al. It includes applications related  words.\nI just have a small remarks;   \nNo definitions, for types and tokens are necessary since these terms are commonly used in the field.   \nWhy do we need the \\(\\lambdawi\\) in equation 4 when the probability is not normalized yet? \nLets talk about it in general."
        }
    ],
    "editorDocumentId": null
}