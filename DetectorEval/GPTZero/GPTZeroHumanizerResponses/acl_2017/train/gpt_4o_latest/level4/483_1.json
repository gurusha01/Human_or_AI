{
    "version": "2025-03-13-base",
    "scanId": "a47ce8f9-5d96-43f9-9649-85ebc133bb4b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999901056289673,
                    "sentence": "This study marks the use of a neural network driven method for analyzing arguments in text form.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999989926815033,
                    "sentence": "The researchers utilize a Pointer Network (PN) model along with task learning and demonstrate better results than previous techniques, in trials conducted on two sets of data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999858140945435,
                    "sentence": "Shortcomings;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999893307685852,
                    "sentence": "The method mainly uses PN in the field of argumentation analysis.Merging PN with multitask learning is new for this task but may not reach the requirements, for a lengthy ACL paper.The lack of qualitative and error analyses is also a drawback.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999876022338867,
                    "sentence": "Lets talk about a variety of topics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999854564666748,
                    "sentence": "Besides the weaknesses mentioned earlier in the text analysis the argument for utilizing PN lacks persuasion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999853372573853,
                    "sentence": "Even though three traits of PN are listed between lines 138 and 143 they fail to offer a reason for choosing PN over bi directional LSTMs with attention mechanisms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999867081642151,
                    "sentence": "It would be beneficial for the authors to provide details, about the particular challenges tackled by PN and assess how well these concerns are rectified in the experiments conducted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999812841415405,
                    "sentence": "Figures 02 and 03 seem a bit tricky to understand at glance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999827146530151,
                    "sentence": "For example what is the significance of the self link connecting to D01 and the links going from D02 to E01 and from D03/D04 to E02?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999843239784241,
                    "sentence": "These seem to represent outputs stemming from the decoder than actual connections.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999810457229614,
                    "sentence": "Also it is worth noting that the decoder LSTM shown here does not receive input, from e_j which goes against Equation ( 03 ).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "Furthermore the abbreviation \" FC \" used in Figure 03 is not clearly defined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999976754188538,
                    "sentence": "Equation (9)s meaning is not clear to me at this moment in time.You determine the likelihood of each component type by figuring out the probability of E_i; this process appears to be somewhat inconsistent, at glance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997615814209,
                    "sentence": "The experimental section lacks clarity, on why only the \"PN\" model was assessed on the microtext dataset when it's not a joint model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "The training method of the BLSTM model using the joint task objective appears unclear well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974370002747,
                    "sentence": "The research paper lacks discussion of previous research on discourse parsing that incorporates attention mechanisms and it would be beneficial for the authors to explain the distinctions, between their work and these existing studies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971985816956,
                    "sentence": "There are some concerns to address.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969601631165,
                    "sentence": "It seems like there's an issue, with the formatting of \"(2015)\" as it looks like its not properly aligned and seems to be floating on its own in the text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977946281433,
                    "sentence": "\"It is able to do \"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "After going through the authors' feedback I discovered that their explanations were persuasive leading me to increase my recommendation rating.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999975562095642,
                    "sentence": "I suggest that the authors delve into discussions about examples produced by PN and conduct thorough analyses, on feature ablations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999974637345529,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.5362654471198017e-06,
                        "ai_paraphrased": 0.9999974637345529
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.5361654471198315e-06,
                            "ai_paraphrased": 0.9999974637345529
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "\nThis study marks the use of a neural network driven method for analyzing arguments in text form. The researchers utilize a Pointer Network (PN) model along with task learning and demonstrate better results than previous techniques, in trials conducted on two sets of data. \nShortcomings; \nThe method mainly uses PN in the field of argumentation analysis.Merging PN with multitask learning is new for this task but may not reach the requirements, for a lengthy ACL paper.The lack of qualitative and error analyses is also a drawback. \nLets talk about a variety of topics.\nBesides the weaknesses mentioned earlier in the text analysis the argument for utilizing PN lacks persuasion. Even though three traits of PN are listed between lines 138 and 143 they fail to offer a reason for choosing PN over bi directional LSTMs with attention mechanisms. It would be beneficial for the authors to provide details, about the particular challenges tackled by PN and assess how well these concerns are rectified in the experiments conducted. \nFigures 02 and 03 seem a bit tricky to understand at glance. For example what is the significance of the self link connecting to D01 and the links going from D02 to E01 and from D03/D04 to E02 ? These seem to represent outputs stemming from the decoder than actual connections. Also it is worth noting that the decoder LSTM shown here does not receive input, from e_j which goes against Equation ( 03 ). Furthermore the abbreviation \" FC \" used in Figure 03 is not clearly defined.\nEquation (9)s meaning is not clear to me at this moment in time.You determine the likelihood of each component type by figuring out the probability of E_i; this process appears to be somewhat inconsistent, at glance. \nThe experimental section lacks clarity, on why only the \"PN\" model was assessed on the microtext dataset when it's not a joint model. \nThe training method of the BLSTM model using the joint task objective appears unclear well. \nThe research paper lacks discussion of previous research on discourse parsing that incorporates attention mechanisms and it would be beneficial for the authors to explain the distinctions, between their work and these existing studies. \nThere are some concerns to address.\n  \nIt seems like there's an issue, with the formatting of \"(2015)\" as it looks like its not properly aligned and seems to be floating on its own in the text.   \n\"It is able to do \"   \nAfter going through the authors’ feedback I discovered that their explanations were persuasive leading me to increase my recommendation rating. I suggest that the authors delve into discussions about examples produced by PN and conduct thorough analyses, on feature ablations. "
        }
    ],
    "editorDocumentId": null
}