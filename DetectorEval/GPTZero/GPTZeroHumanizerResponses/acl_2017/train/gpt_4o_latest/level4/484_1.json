{
    "version": "2025-03-13-base",
    "scanId": "76fa47cd-afd4-4dea-a820-4bc26028de31",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998555183410645,
                    "sentence": "This study presents an ASRs framework that combines CTC and attention methods to capitalize on their respective advantages during training and decoding processes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998838305473328,
                    "sentence": "Advantages;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998825192451477,
                    "sentence": "The research showcases an execution of the combined CTC and attention model for training and decoding purposes with results showing enhanced performance, in Japanese CS J and Mandarin Chinese telephone speech recognition tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998490214347839,
                    "sentence": "\"Areas of improvement;\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998498558998108,
                    "sentence": "The main focus here is how this study compares to the work referenced in Kim et al., 2016 soon to be released at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) scheduled for March 2017.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998476505279541,
                    "sentence": "Kim et al., 2016 presents a combined approach of CTC based attention and multi task learning (MTL) for English ASRs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998500943183899,
                    "sentence": "In contrast to that work this paper expands on the method by integrating decoding and applying it to ASRs, for Japanese and Chinese languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998444318771362,
                    "sentence": "The authors did not clearly explain the difference between the two works, in their paper which makes it difficult to identify the contributions of this study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998080730438232,
                    "sentence": "(a); Heading;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998207092285156,
                    "sentence": "The publication \"Joint CTC Aware End to end Speech Recognition through Multi task Learning\" by Kim and colleagues (2016) contrasts with the title of this paper which's simply \"Joint CTC Aware End to end Speech Recognition.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998463988304138,
                    "sentence": "The title of the paper may seem too general; it would be fitting if it were the inaugural work, on \"Joint CTC awareness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998719692230225,
                    "sentence": "\"If Ref [Kim et al., 2016] were to stay as an arxiv paper before being published earlier on like [Kim et al., 2016] then it would be fine to keep the current title as is; however to distinguish this papers distinctive contributions, from existing works a more precise title would be needed once it gets published.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999866247177124,
                    "sentence": "(b); Opening section;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999299049377441,
                    "sentence": "The authors suggest utilizing the CTC alignment within a combined system of both CTC and attention mechanisms for enhanced performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999030828475952,
                    "sentence": "They advocate for incorporating a CTC objective during training in an attention based encoder network as a form of regularization based on the approach, by Kim et al., 2016.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998776316642761,
                    "sentence": "This approach of integrating constrained CTC alignment into a system was initially proposed by Kim et al., 2016.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998212456703186,
                    "sentence": "Therefore the talk about the benefits of merging attention based and CTC based ASR is not new or groundbreaking in any way.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998006224632263,
                    "sentence": "Moreover using phrases like \"we suggest … as suggested by [Kim et al., 2016]\" can cause issues.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997978806495667,
                    "sentence": "While its acceptable to expand on work with enhancements it's not right to reintroduce an idea that already exists.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998709559440613,
                    "sentence": "Hence it's important for the writers to distinctly outline the contributions of this study and explain its stance in relation, to existing literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999041557312012,
                    "sentence": "(copyright symbol representing \"©\") Findings, from our experiments;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999074339866638,
                    "sentence": "When [Kim and colleagues in 2016] used the combination of CTC and attention models for English speech recognition tasks in their research study; this paper specifically delves into the realm of Japanese and Mandarin tasks instead.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999046921730042,
                    "sentence": "It would greatly benefit the authors to delve deeper into the challenges that Japanese and Mandarin Chinese speech recognition present compared to English speech recognition.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999916672706604,
                    "sentence": "For example; exploring how the system manages handling potential outputs (such as Kanji characters or Hiragana and Katakana scripts) when processing Japanese speech input without relying heavily on linguistic databases could be an interesting avenue for discussion, in this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999037981033325,
                    "sentence": "Tackling these language obstacles could make a valuable impact through this study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999072551727295,
                    "sentence": "Lets chat about topics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998810291290283,
                    "sentence": "It's better to reference the publication, by Kim and others in 2016 from the IEEE ICASSP source instead of the pre published arxiv version.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998323321342468,
                    "sentence": "Kim et al.s study titled \"Joint CTC and Attention Based End to end Speech Recognition with Multi task Learning \" presented at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) in March 2017 is forthcoming, in the proceedings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997932945046397,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046397,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046397,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046397,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999894450296238,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.0554970376188937e-05,
                        "ai_paraphrased": 0.9999894450296238
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.0554870376188968e-05,
                            "ai_paraphrased": 0.9999894450296238
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study presents an ASRs framework that combines CTC and attention methods to capitalize on their respective advantages during training and decoding processes. \nAdvantages;   \nThe research showcases an execution of the combined CTC and attention model for training and decoding purposes with results showing enhanced performance, in Japanese CS J and Mandarin Chinese telephone speech recognition tasks. \n\"Areas of improvement;\"  \nThe main focus here is how this study compares to the work referenced in Kim et al., 2016 soon to be released at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) scheduled for March 2017. Kim et al., 2016 presents a combined approach of CTC based attention and multi task learning (MTL) for English ASRs. In contrast to that work this paper expands on the method by integrating decoding and applying it to ASRs, for Japanese and Chinese languages. The authors did not clearly explain the difference between the two works, in their paper which makes it difficult to identify the contributions of this study. \n(a); Heading;   \nThe publication \"Joint CTC Aware End to end Speech Recognition through Multi task Learning\" by Kim and colleagues (2016) contrasts with the title of this paper which's simply \"Joint CTC Aware End to end Speech Recognition.\" The title of the paper may seem too general; it would be fitting if it were the inaugural work, on \"Joint CTC awareness.\"If Ref [Kim et al., 2016] were to stay as an arxiv paper before being published earlier on like [Kim et al., 2016] then it would be fine to keep the current title as is; however to distinguish this papers distinctive contributions, from existing works a more precise title would be needed once it gets published. \n(b); Opening section;   \nThe authors suggest utilizing the CTC alignment within a combined system of both CTC and attention mechanisms for enhanced performance. They advocate for incorporating a CTC objective during training in an attention based encoder network as a form of regularization based on the approach, by Kim et al., 2016. This approach of integrating constrained CTC alignment into a system was initially proposed by Kim et al., 2016. Therefore the talk about the benefits of merging attention based and CTC based ASR is not new or groundbreaking in any way. Moreover using phrases like \"we suggest … as suggested by [Kim et al., 2016]\" can cause issues. While its acceptable to expand on work with enhancements it's not right to reintroduce an idea that already exists. Hence it's important for the writers to distinctly outline the contributions of this study and explain its stance in relation, to existing literature. \n(copyright symbol representing \"©\") Findings, from our experiments;   \nWhen [Kim and colleagues in 2016] used the combination of CTC and attention models for English speech recognition tasks in their research study; this paper specifically delves into the realm of Japanese and Mandarin tasks instead. It would greatly benefit the authors to delve deeper into the challenges that Japanese and Mandarin Chinese speech recognition present compared to English speech recognition. For example; exploring how the system manages handling potential outputs (such as Kanji characters or Hiragana and Katakana scripts) when processing Japanese speech input without relying heavily on linguistic databases could be an interesting avenue for discussion, in this paper. Tackling these language obstacles could make a valuable impact through this study. \nLets chat about topics.  \nIt's better to reference the publication, by Kim and others in 2016 from the IEEE ICASSP source instead of the pre published arxiv version.   \nKim et al.s study titled \"Joint CTC and Attention Based End to end Speech Recognition with Multi task Learning \" presented at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) in March 2017 is forthcoming, in the proceedings. "
        }
    ],
    "editorDocumentId": null
}