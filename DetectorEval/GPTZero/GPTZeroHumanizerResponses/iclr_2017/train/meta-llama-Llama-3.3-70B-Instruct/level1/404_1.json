{
    "version": "2025-03-13-base",
    "scanId": "8fe4167b-2542-464c-b6a1-efe88d2a19b5",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999930262565613,
                    "sentence": "In brief;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948143959045,
                    "sentence": "The study presents Quasi Recurrent Neural Networks (QRNN) a method, for neural sequence modeling that blends the advantages of convolutional and recurrent neural networks together effectively.. QRNN tackles the drawbacks of RNN structures that struggle with parallelizing computations across time steps by leveraging convolution layers and a simplified recurrent pooling function.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955296516418,
                    "sentence": "The writers show how QRNN technology performs well in language related assignments, like modeling language behavior and determining sentiments accurately and quickly when compared to models based on LSTM algorithms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999895095825195,
                    "sentence": "Choice.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948143959045,
                    "sentence": "After consideration of the papers content and findings presented in a well rounded manner with a strong focus and detailed analysis of QRNN advantages over traditional RNN models; I have decided to approve this submission for publication due to its compelling approach and robust empirical results showcasing the benefits of QRNN implementation in comparison, to conventional RNN structures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999918937683105,
                    "sentence": "Arguments, in favor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999934434890747,
                    "sentence": "The research focuses on an crucial issue within neural sequence modeling - the challenges faced by conventional RNN models when processing lengthy sequences are addressed comprehensively in the paper.The proposed method is compelling as it combines the advantages of both recurrent neural networks to overcome these limitations.The experimental findings are robust.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999945759773254,
                    "sentence": "Highlight the efficacy of QRNN models across various tasks while demonstrating notable enhancements, in computational efficiency.The study also offers a scrutiny of the outcomes and delves into the implications and possible uses of QRNN technology.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999587535858154,
                    "sentence": "More input would be appreciated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999902248382568,
                    "sentence": "To enhance the paper further I recommend that the authors delve into explanations regarding the QRNN architecture and its different versions such as the convolution and pooling elements.It could also be beneficial to incorporate visual aids or diagrams to aid in grasping the QRNN architecture and how it differs from conventional RNN models.Furthermore the authors might want to explore discussing the limitations and obstacles of QRNN technology while also considering avenues, for future research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999884366989136,
                    "sentence": "Questions to Ask the Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999892711639404,
                    "sentence": "Could the authors kindly address a queries to help me grasp the paper better?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999924898147583,
                    "sentence": "Could you give me information, about how the QRNN architecture is put into practice and specifically about the convolution and pooling aspects of it?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999898076057434,
                    "sentence": "How do the writers intend to tackle the constraints and hurdles of QRNN technology when it comes to managing extremely lengthy sequences or sequences with intricate interconnections?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999914765357971,
                    "sentence": "What uses do researchers see for QRNN technology apart, from those examined in the study and how do they think QRNN technology can be practically applied?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997932945046397,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046397,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046397,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046397,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999952451853068,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 4.754814693213723e-06,
                        "ai_paraphrased": 0.9999952451853068
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 4.754714693213753e-06,
                            "ai_paraphrased": 0.9999952451853068
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "In brief;\nThe study presents Quasi Recurrent Neural Networks (QRNN) a method, for neural sequence modeling that blends the advantages of convolutional and recurrent neural networks together effectively.. QRNN tackles the drawbacks of RNN structures that struggle with parallelizing computations across time steps by leveraging convolution layers and a simplified recurrent pooling function. The writers show how QRNN technology performs well in language related assignments, like modeling language behavior and determining sentiments accurately and quickly when compared to models based on LSTM algorithms. \nChoice.\nAfter consideration of the papers content and findings presented in a well rounded manner with a strong focus and detailed analysis of QRNN advantages over traditional RNN models; I have decided to approve this submission for publication due to its compelling approach and robust empirical results showcasing the benefits of QRNN implementation in comparison, to conventional RNN structures. \nArguments, in favor.\nThe research focuses on an crucial issue within neural sequence modeling – the challenges faced by conventional RNN models when processing lengthy sequences are addressed comprehensively in the paper.The proposed method is compelling as it combines the advantages of both recurrent neural networks to overcome these limitations.The experimental findings are robust. Highlight the efficacy of QRNN models across various tasks while demonstrating notable enhancements, in computational efficiency.The study also offers a scrutiny of the outcomes and delves into the implications and possible uses of QRNN technology. \nMore input would be appreciated.\nTo enhance the paper further I recommend that the authors delve into explanations regarding the QRNN architecture and its different versions such as the convolution and pooling elements.It could also be beneficial to incorporate visual aids or diagrams to aid in grasping the QRNN architecture and how it differs from conventional RNN models.Furthermore the authors might want to explore discussing the limitations and obstacles of QRNN technology while also considering avenues, for future research. \nQuestions to Ask the Authors\nCould the authors kindly address a queries to help me grasp the paper better?\nCould you give me information, about how the QRNN architecture is put into practice and specifically about the convolution and pooling aspects of it? \nHow do the writers intend to tackle the constraints and hurdles of QRNN technology when it comes to managing extremely lengthy sequences or sequences with intricate interconnections? \nWhat uses do researchers see for QRNN technology apart, from those examined in the study and how do they think QRNN technology can be practically applied? "
        }
    ],
    "editorDocumentId": null
}