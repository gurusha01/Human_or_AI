{
    "version": "2025-03-13-base",
    "scanId": "8bd204ab-316a-43d4-8dbd-382e937a389a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998905658721924,
                    "sentence": "In brief",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999136924743652,
                    "sentence": "The research paper introduces a language model that incorporates a key value attention mechanism to generate distinct representations for memory differentiation and encoding the distribution of the next word effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999881386756897,
                    "sentence": "Enough it surpasses other memory augmented neural language models on two datasets by primarily focusing on storing the five most recent output representations, in its memory banks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998935461044312,
                    "sentence": "The discovery suggests that a basic model combining output representations performs as well as advanced memory enhanced neural language models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995615482330322,
                    "sentence": "Choice",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999248385429382,
                    "sentence": "My decision to approve this paper is based on the following reasons;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999004602432251,
                    "sentence": "The article addresses an well founded issue in neural language modeling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999248385429382,
                    "sentence": "The challenge of traditional attention mechanisms, in capturing distant connections effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999321103096008,
                    "sentence": "The writers introduce an attention mechanism based on key value pairs and show its success in experiments, on two datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999430179595947,
                    "sentence": "Reasons, for Support",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999756813049316,
                    "sentence": "The article is nicely.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999780654907227,
                    "sentence": "The authors explain their model and its elements in a straightforward manner.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999790191650391,
                    "sentence": "Their experiments are extensive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999735355377197,
                    "sentence": "The results are persuasive in demonstrating that their model surpasses current memory augmented neural language models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999735951423645,
                    "sentence": "Additionally they offer an, in depth examination of the outcomes which reveal a discovery that the model predominantly relies upon a brief memory of recent output representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999555349349976,
                    "sentence": "More Input, for Improvement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999737739562988,
                    "sentence": "To enhance the paper further; I recommend that the authors delve deeper into explaining why the model struggles with longer range dependencies and conduct experiments on different datasets and tasks to showcase how broadly applicable the proposed model is.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999780058860779,
                    "sentence": "It would also be valuable for the authors to offer specifics on how the model was implemented.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999799132347107,
                    "sentence": "Including details, on hyperparameters and training methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999671578407288,
                    "sentence": "Queries, for the Writers",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999836683273315,
                    "sentence": "Could you elaborate on why the model struggles with incorporating connections effectively and suggest any changes that could help improve this issue?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999861717224121,
                    "sentence": "How do you intend to motivate the model to focus on a historical context as discussed in the conclusion?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999864101409912,
                    "sentence": "Could you please share information, about how the model is being applied in real world scenariosᅳlike the specific hyperparameter configurations and the steps involved in training it?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9991781598830806,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9991781598830806,
                "mixed": 0.0008218401169194299
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9991781598830806,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9991781598830806,
                    "human": 0,
                    "mixed": 0.0008218401169194299
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9998674724840451,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.00013252751595488755,
                        "ai_paraphrased": 0.9998674724840451
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.00013252741595488757,
                            "ai_paraphrased": 0.9998674724840451
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "In brief\nThe research paper introduces a language model that incorporates a key value attention mechanism to generate distinct representations for memory differentiation and encoding the distribution of the next word effectively. Enough it surpasses other memory augmented neural language models on two datasets by primarily focusing on storing the five most recent output representations, in its memory banks. The discovery suggests that a basic model combining output representations performs as well as advanced memory enhanced neural language models. \nChoice\nMy decision to approve this paper is based on the following reasons; \nThe article addresses an well founded issue in neural language modeling. The challenge of traditional attention mechanisms, in capturing distant connections effectively. \nThe writers introduce an attention mechanism based on key value pairs and show its success in experiments, on two datasets. \nReasons, for Support \nThe article is nicely. The authors explain their model and its elements in a straightforward manner. Their experiments are extensive. The results are persuasive in demonstrating that their model surpasses current memory augmented neural language models. Additionally they offer an, in depth examination of the outcomes which reveal a discovery that the model predominantly relies upon a brief memory of recent output representations. \nMore Input, for Improvement \nTo enhance the paper further; I recommend that the authors delve deeper into explaining why the model struggles with longer range dependencies and conduct experiments on different datasets and tasks to showcase how broadly applicable the proposed model is. It would also be valuable for the authors to offer specifics on how the model was implemented. Including details, on hyperparameters and training methods. \nQueries, for the Writers\nCould you elaborate on why the model struggles with incorporating connections effectively and suggest any changes that could help improve this issue? \nHow do you intend to motivate the model to focus on a historical context as discussed in the conclusion? \nCould you please share information, about how the model is being applied in real world scenarios—like the specific hyperparameter configurations and the steps involved in training it? "
        }
    ],
    "editorDocumentId": null
}