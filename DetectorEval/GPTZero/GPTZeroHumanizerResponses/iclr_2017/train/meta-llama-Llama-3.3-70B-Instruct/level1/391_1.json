{
    "version": "2025-03-13-base",
    "scanId": "2960bad1-3ac4-4edf-beca-b19819d808e7",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "Overview",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999952912330627,
                    "sentence": "The research article suggests a method to decrease the complexity of Recurrent Neural Networks (RNNs) by trimming weights at the start of network training sessions.This method results in a decrease in model size by 90% albeit with a slight decrease in accuracy.The authors showcase the efficiency of their approach on RNN structures, like basic RNN models and Gated Recurrent Units (GRUs) illustrating its potential to enhance accuracy and reduce inference durations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999866485595703,
                    "sentence": "Choice",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999926686286926,
                    "sentence": "Sure thing!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999926090240479,
                    "sentence": "Here's your rewritten text; \"I have chosen to approve this paper because the approach is well founded and backed by comprehensive experiments conducted by the authors The method they describe is clearly explained in a concise manner and the results show a notable decrease in model size along, with enhanced accuracy.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999920725822449,
                    "sentence": "Reasons, for support",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999920129776001,
                    "sentence": "The paper addresses an important issue in the realm of deep learning; the need to decrease the size of models and speed up inference for RNNs.This method is well grounded in existing research on techniques such as pruning and quantization.The authors extensively test their approach by conducting experiments, on RNN structures and datasets.They effectively show how their technique reduces model size and enhances accuracy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999864101409912,
                    "sentence": "More Input Needed",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999937117099762,
                    "sentence": "To enhance the paper suggest adding more information about how the hyperparameters were optimized and how the threshold function was chosen.Particularly interesting would be a comparison of the suggested method with techniques, like L1 regularization and knowledge distillation.The authors might also want to delve into explaining how easy it is to understand sparse models and discuss uses of this approach in different fields.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948143959045,
                    "sentence": "Dear Authors I have some inquiries, for you.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999906420707703,
                    "sentence": "Could you kindly address my queries to gain a comprehension of the paper?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999884963035583,
                    "sentence": "I would appreciate it if the authors could respond to the questions provided below.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999872446060181,
                    "sentence": "Could you please offer insights, into how you decided on the threshold function and went about adjusting the hyperparameters during the tuning process?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999845027923584,
                    "sentence": "How do you intend to apply this method to areas, like language modeling and computer vision?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999845623970032,
                    "sentence": "Could you share details, on how easy it is to understand the sparse models and how this technique could be used in real life situations?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997932945046396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046396,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046396,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9972548421003259,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.0027451578996741804,
                        "ai_paraphrased": 0.9972548421003259
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.0027451577996741804,
                            "ai_paraphrased": 0.9972548421003259
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "Overview \nThe research article suggests a method to decrease the complexity of Recurrent Neural Networks (RNNs) by trimming weights at the start of network training sessions.This method results in a decrease in model size by 90% albeit with a slight decrease in accuracy.The authors showcase the efficiency of their approach on RNN structures, like basic RNN models and Gated Recurrent Units (GRUs) illustrating its potential to enhance accuracy and reduce inference durations. \nChoice\nSure thing! Here's your rewritten text; \"I have chosen to approve this paper because the approach is well founded and backed by comprehensive experiments conducted by the authors The method they describe is clearly explained in a concise manner and the results show a notable decrease in model size along, with enhanced accuracy.\"\nReasons, for support \nThe paper addresses an important issue in the realm of deep learning; the need to decrease the size of models and speed up inference for RNNs.This method is well grounded in existing research on techniques such as pruning and quantization.The authors extensively test their approach by conducting experiments, on RNN structures and datasets.They effectively show how their technique reduces model size and enhances accuracy. \nMore Input Needed\nTo enhance the paper suggest adding more information about how the hyperparameters were optimized and how the threshold function was chosen.Particularly interesting would be a comparison of the suggested method with techniques, like L1 regularization and knowledge distillation.The authors might also want to delve into explaining how easy it is to understand sparse models and discuss uses of this approach in different fields. \nDear Authors I have some inquiries, for you.\nCould you kindly address my queries to gain a comprehension of the paper? I would appreciate it if the authors could respond to the questions provided below.\nCould you please offer insights, into how you decided on the threshold function and went about adjusting the hyperparameters during the tuning process? \nHow do you intend to apply this method to areas, like language modeling and computer vision? \nCould you share details, on how easy it is to understand the sparse models and how this technique could be used in real life situations? "
        }
    ],
    "editorDocumentId": null
}