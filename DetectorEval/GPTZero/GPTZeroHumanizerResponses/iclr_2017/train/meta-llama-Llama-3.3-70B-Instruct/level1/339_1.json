{
    "version": "2025-03-13-base",
    "scanId": "54c02dca-540f-49d1-87b2-e5fff2cef83c",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999719262123108,
                    "sentence": "In short here's an overview.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999811053276062,
                    "sentence": "The research paper introduces an enhancement to neural network language models known as the Neural Cache Model.It adjusts predictions based on history by keeping past hidden activations in memory and retrieving them using a dot product with the current hidden activation.This method is a form of memory augmented networks and has proven to be effective and adaptable for handling large memory sizes.The authors showcase the success of their approach on language model datasets as well, as the LAMBADA dataset.They achieve performance improvements when compared to recent memory augmented networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999502897262573,
                    "sentence": "Choice",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999791979789734,
                    "sentence": "My decision is to approve this paper because the approach is thoroughly justified and backed by real world data provided by the authors.The Neural Cache Model is clearly explained along with its connection to memory augmented neural networks and cache models.The results from the experiments show how effective the approach is in enhancing language modeling performance, on the LAMBADA dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999980628490448,
                    "sentence": "Reasons, for Support",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999829530715942,
                    "sentence": "The study delves into an issue of adjusting neural network language models to the most recent past eventsᅳknown as a common flaw in conventional neural networks.The method is well grounded in research areas like memory enhanced neural networks and cache models.The writers offer an succinct overview of the Neural Cache Model presented in a manner that is easily comprehensible and reproducible.The testing outcomes are comprehensive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999874830245972,
                    "sentence": "Showcase the efficiency of the strategy, across various datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999972403049469,
                    "sentence": "Extra Input Appreciated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999797344207764,
                    "sentence": "To enhance the paper further I recommend that the authors delve deeper into analyzing the efficiency of the Neural Cache Model in comparison to other memory augmented neural networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999762773513794,
                    "sentence": "Moreover it would be intriguing to explore experiments on how sensitive the model is to hyperparameters like cache size and interpolation parameter.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999821186065674,
                    "sentence": "Lastly it might be worth discussing uses of the Neural Cache Model beyond language modeling such as, in dialogue systems or machine translation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999709129333496,
                    "sentence": "Queries, for the Writers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999751448631287,
                    "sentence": "Could I kindly request the authors to provide explanation on the following aspects;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999816417694092,
                    "sentence": "How does the Neural Cache Model deal with words that're not, in its vocabulary and are there any intentions to expand the model to address this scenario?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999739527702332,
                    "sentence": "Could the writers offer information, about how they conducted the training process in their experimentsᅳlike specifying the number of epochs and the batch size utilized?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999749660491943,
                    "sentence": "How does the Neural Cache Model stack up against memory augmented neural networks in terms of computational efficiency and scalability?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997932945046396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046396,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046396,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999855220020164,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.4477997983601114e-05,
                        "ai_paraphrased": 0.9999855220020164
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.4477897983601144e-05,
                            "ai_paraphrased": 0.9999855220020164
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "In short here's an overview.\nThe research paper introduces an enhancement to neural network language models known as the Neural Cache Model.It adjusts predictions based on history by keeping past hidden activations in memory and retrieving them using a dot product with the current hidden activation.This method is a form of memory augmented networks and has proven to be effective and adaptable for handling large memory sizes.The authors showcase the success of their approach on language model datasets as well, as the LAMBADA dataset.They achieve performance improvements when compared to recent memory augmented networks. \nChoice\nMy decision is to approve this paper because the approach is thoroughly justified and backed by real world data provided by the authors.The Neural Cache Model is clearly explained along with its connection to memory augmented neural networks and cache models.The results from the experiments show how effective the approach is in enhancing language modeling performance, on the LAMBADA dataset. \nReasons, for Support \nThe study delves into an issue of adjusting neural network language models to the most recent past events—known as a common flaw in conventional neural networks.The method is well grounded in research areas like memory enhanced neural networks and cache models.The writers offer an succinct overview of the Neural Cache Model presented in a manner that is easily comprehensible and reproducible.The testing outcomes are comprehensive. Showcase the efficiency of the strategy, across various datasets. \nExtra Input Appreciated.\nTo enhance the paper further I recommend that the authors delve deeper into analyzing the efficiency of the Neural Cache Model in comparison to other memory augmented neural networks. Moreover it would be intriguing to explore experiments on how sensitive the model is to hyperparameters like cache size and interpolation parameter. Lastly it might be worth discussing uses of the Neural Cache Model beyond language modeling such as, in dialogue systems or machine translation. \nQueries, for the Writers.\nCould I kindly request the authors to provide explanation on the following aspects; \nHow does the Neural Cache Model deal with words that're not, in its vocabulary and are there any intentions to expand the model to address this scenario? \nCould the writers offer information, about how they conducted the training process in their experiments—like specifying the number of epochs and the batch size utilized? \nHow does the Neural Cache Model stack up against memory augmented neural networks in terms of computational efficiency and scalability? "
        }
    ],
    "editorDocumentId": null
}