{
    "version": "2025-03-13-base",
    "scanId": "a13d94a7-6005-42f3-af30-6257fa26995f",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9997729063034058,
                    "sentence": "This study introduces a model called the Variational Loss Autoencoder within the framework of a Variational Autoencoder (VAE).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996929168701172,
                    "sentence": "The aim of this model is to filter out information in order to capture valuable global data representations and act as a compression algorithm that sacrifices some details, for efficiency and accuracy purposes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997075200080872,
                    "sentence": "At glance and in simple terms the writers share a detailed explanation of VAEs using the Bits Back concept to clarify when and how the hidden code is ignored.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997729063034058,
                    "sentence": "In line, with what has been written they mention that the part of the model that predicts data patterns usually accounts for most of the information making the hidden variables less important.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997358322143555,
                    "sentence": "To tackle this issue the writers suggest two approaches to make sure that the decoder can effectively use these hidden variables.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997674226760864,
                    "sentence": "The initial method focuses on confining the decoder within a limited local receptive field which requires the model to utilize the hidden code for understanding distant relationships effectively.The second method involves defining the distribution, over the hidden code by employing an autoregressive model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997634291648865,
                    "sentence": "The article also showcases cutting edge outcomes on various sets of information, like binary MNIST (using both dynamic and static binarization) OMNILOT and Caltech 101 Silhouettes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997695088386536,
                    "sentence": "One significant aspect of this research is the explanation of Bits back in VAE model interpretation that provides understanding of the models performance and possible ways to enhance it further.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998533725738525,
                    "sentence": "The capacity to precisely manage the information contained in the acquired representation proves beneficial in a range of scenarios like image search where it could aid in locating items, with shapes irrespective of their textures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999125599861145,
                    "sentence": "The authors suggest two ways to enhance VAEs by improving code compression through information placement and by training the prior using autoregressive flow techniques.\\However\\However\\However\\Nevertheless They don't assess how the use of a PixelCNN decoder of an autoregressive flow prior would affect the latent code in a VAE.\\Moreover\\Furthermore\\Additionally The explanation of WindowAround(i) and its connection to x_{I}\\ix_{I}\\ix_i appears ambiguous.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998660087585449,
                    "sentence": "Requires further clarification, for a better understanding of how the models components interact with each other.\\",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9997932945046397,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046397,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046397,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046397,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999608442425943,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 3.915575740571415e-05,
                        "ai_paraphrased": 0.9999608442425943
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 3.9155657405714176e-05,
                            "ai_paraphrased": 0.9999608442425943
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study introduces a model called the Variational Loss Autoencoder within the framework of a Variational Autoencoder (VAE). The aim of this model is to filter out information in order to capture valuable global data representations and act as a compression algorithm that sacrifices some details, for efficiency and accuracy purposes. \nAt glance and in simple terms the writers share a detailed explanation of VAEs using the Bits Back concept to clarify when and how the hidden code is ignored. In line, with what has been written they mention that the part of the model that predicts data patterns usually accounts for most of the information making the hidden variables less important. To tackle this issue the writers suggest two approaches to make sure that the decoder can effectively use these hidden variables. The initial method focuses on confining the decoder within a limited local receptive field which requires the model to utilize the hidden code for understanding distant relationships effectively.The second method involves defining the distribution, over the hidden code by employing an autoregressive model. \nThe article also showcases cutting edge outcomes on various sets of information, like binary MNIST (using both dynamic and static binarization) OMNILOT and Caltech 101 Silhouettes. \nOne significant aspect of this research is the explanation of Bits back in VAE model interpretation that provides understanding of the models performance and possible ways to enhance it further. The capacity to precisely manage the information contained in the acquired representation proves beneficial in a range of scenarios like image search where it could aid in locating items, with shapes irrespective of their textures. \nThe authors suggest two ways to enhance VAEs by improving code compression through information placement and by training the prior using autoregressive flow techniques.\\However\\However\\However\\Nevertheless They don't assess how the use of a PixelCNN decoder of an autoregressive flow prior would affect the latent code in a VAE.\\Moreover\\Furthermore\\Additionally The explanation of WindowAround(i) and its connection to x_{I}\\ix_{I}\\ix_i appears ambiguous. Requires further clarification, for a better understanding of how the models components interact with each other.\\"
        }
    ],
    "editorDocumentId": null
}