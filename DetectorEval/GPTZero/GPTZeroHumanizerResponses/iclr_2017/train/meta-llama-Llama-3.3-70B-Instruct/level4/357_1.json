{
    "version": "2025-03-13-base",
    "scanId": "b7ec1d7f-9ddf-449c-9765-db8a30738edc",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999968409538269,
                    "sentence": "This paper introduces a model for forecasting upcoming frames in video sequences by dividing motion and content into separate encoding paths and adding multi scale residual connections for enhancement of the systems performance as shown in their analysis, on the KTH Weizmann and UCF101 datasets both qualitatively and quantitatively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999970197677612,
                    "sentence": "The idea of separating movement from content is fascinating.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "Seems to produce positive results for this particular assignment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999966621398926,
                    "sentence": "However the novelty of this idea is somewhat reduced by studies, on multi stream networks and it is still unknown whether this specific separation approach provides substantial advantages or has broad implications beyond just predicting future frames.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967217445374,
                    "sentence": "The findings, from the KTH and Weizmann datasets show progress compared to the baseline results achieved before this study took place.The results are less outstanding when it comes to the UCF101 dataset which is known for its diversity.In addition the examples presented for the UCF101 dataset do not appear convincing based our conversations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967813491821,
                    "sentence": "In short this study is well made with a concept that lacks groundbreaking originality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969601631165,
                    "sentence": "To strengthen the papers influence it would be advantageous to showcase how the motion content decoupling method can be applied to video tasks addressing worries, about its novelty and potential limitations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999984343986883,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.5656013117037335e-06,
                        "ai_paraphrased": 0.9999984343986883
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.5655013117037635e-06,
                            "ai_paraphrased": 0.9999984343986883
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This paper introduces a model for forecasting upcoming frames in video sequences by dividing motion and content into separate encoding paths and adding multi scale residual connections for enhancement of the systems performance as shown in their analysis, on the KTH Weizmann and UCF101 datasets both qualitatively and quantitatively. \nThe idea of separating movement from content is fascinating. Seems to produce positive results for this particular assignment. However the novelty of this idea is somewhat reduced by studies, on multi stream networks and it is still unknown whether this specific separation approach provides substantial advantages or has broad implications beyond just predicting future frames. \nThe findings, from the KTH and Weizmann datasets show progress compared to the baseline results achieved before this study took place.The results are less outstanding when it comes to the UCF101 dataset which is known for its diversity.In addition the examples presented for the UCF101 dataset do not appear convincing based our conversations. \nIn short this study is well made with a concept that lacks groundbreaking originality. To strengthen the papers influence it would be advantageous to showcase how the motion content decoupling method can be applied to video tasks addressing worries, about its novelty and potential limitations. "
        }
    ],
    "editorDocumentId": null
}