{
    "version": "2025-03-13-base",
    "scanId": "41e0636e-8235-4d05-a106-1588d5649e43",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "This paper introduces a method for character language modeling (CLMs) using a domain specific language (DSL).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999970197677612,
                    "sentence": "The results from the experiments show varying performance levels when compared to CLM methods in analyzing Linux kernel data and text from Wikipedia; the DSL models displayed slightly improved efficiency and speed in queries.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977946281433,
                    "sentence": "However \" the overall approach may be difficult to understand.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967217445374,
                    "sentence": "Seems tailored for a particular group within the community with insufficient explanations for a wider audience, at ICLR.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "\"The main issue here is that the paper misses out on aspects like proving the effectiveness of the proposed DSL as a probabilistic model and outlining the training method used to adapt the model to the data without using gradient based techniques apparently included in it all along.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997615814209,
                    "sentence": "Additionally missing are samples generated from the model or an analysis of how the model has been learned to ascertain its abilities, in various scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971985816956,
                    "sentence": "Ultimately falling short in giving an insight into the methodology employed makes it challenging for readers to grasp and reproduce successfully.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971985816956,
                    "sentence": "The model section mainly discusses the DSL without going into detail about how probabilitiesre derived from this model or how training is carried out with it.The DSL description seems to focus o您discrete decisions instead of probabilities a perspective that prompts inquiries into how probabilities are precisely embedded.Although previous works might touch upon training the lack of an explanation regarding the training process and optimization metrics, in Section 2.e needs to be rectified.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997615814209,
                    "sentence": "Comparing this models hypothesis space with neural models or ngrams is crucial when analyzing the generated samples from the models perspective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974370002747,
                    "sentence": "Though the ongoing experiments showcase the models scoring of utterances to a certain extent it would be intriguing to explore whether the model can produce samples with a more organized structure than neural methods, including ones that showcase complex syntax constraints, like extended bracket usage.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.999997128195037,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.8718049629815432e-06,
                        "ai_paraphrased": 0.999997128195037
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.871704962981573e-06,
                            "ai_paraphrased": 0.999997128195037
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This paper introduces a method for character language modeling (CLMs) using a domain specific language (DSL). The results from the experiments show varying performance levels when compared to CLM methods in analyzing Linux kernel data and text from Wikipedia; the DSL models displayed slightly improved efficiency and speed in queries. However \" the overall approach may be difficult to understand. Seems tailored for a particular group within the community with insufficient explanations for a wider audience, at ICLR.\"The main issue here is that the paper misses out on aspects like proving the effectiveness of the proposed DSL as a probabilistic model and outlining the training method used to adapt the model to the data without using gradient based techniques apparently included in it all along. Additionally missing are samples generated from the model or an analysis of how the model has been learned to ascertain its abilities, in various scenarios. Ultimately falling short in giving an insight into the methodology employed makes it challenging for readers to grasp and reproduce successfully. \nThe model section mainly discusses the DSL without going into detail about how probabilitiesre derived from this model or how training is carried out with it.The DSL description seems to focus o您discrete decisions instead of probabilities a perspective that prompts inquiries into how probabilities are precisely embedded.Although previous works might touch upon training the lack of an explanation regarding the training process and optimization metrics, in Section 2.e needs to be rectified. \nComparing this models hypothesis space with neural models or ngrams is crucial when analyzing the generated samples from the models perspective. Though the ongoing experiments showcase the models scoring of utterances to a certain extent it would be intriguing to explore whether the model can produce samples with a more organized structure than neural methods, including ones that showcase complex syntax constraints, like extended bracket usage. "
        }
    ],
    "editorDocumentId": null
}