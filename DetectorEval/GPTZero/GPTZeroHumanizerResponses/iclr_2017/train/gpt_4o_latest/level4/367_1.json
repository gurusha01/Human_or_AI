{
    "version": "2025-03-13-base",
    "scanId": "92afd60f-0e86-4b0f-a2ac-29e65447f473",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999982118606567,
                    "sentence": "The research paper presents an angle, on binary autoencoders by defining the goal as minimizing the reconstruction error over a dataset through a min max approach tied to the observed intermediate representations.The researcher shows that this method leads to a concave problem that can be tackled using alternating minimization methods.This element is not straightforward.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973773956299,
                    "sentence": "Serves as the main highlight of the study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999982714653015,
                    "sentence": "Proof of concept tests were carried out to show how 1 hidden layer autoencoders performed better, than the baseline method in terms of performance enhancements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999982714653015,
                    "sentence": "However the experimental section seems lacking since there is an amount of information in autoencoder research with many different versions such as denoising autoencoders that have proven to be more effective than simple methods without extra complexity involved Despite this drawback the paper offers a thoughtful examination that results in a fresh learning technique for a well known issue which could potentially be a valuable addition, to the field",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9999999999999999,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9999999999999999,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9999999999999999,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9999999999999999,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999981524887107,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.8475112892928283e-06,
                        "ai_paraphrased": 0.9999981524887107
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.8474112892928583e-06,
                            "ai_paraphrased": 0.9999981524887107
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The research paper presents an angle, on binary autoencoders by defining the goal as minimizing the reconstruction error over a dataset through a min max approach tied to the observed intermediate representations.The researcher shows that this method leads to a concave problem that can be tackled using alternating minimization methods.This element is not straightforward. Serves as the main highlight of the study. Proof of concept tests were carried out to show how 1 hidden layer autoencoders performed better, than the baseline method in terms of performance enhancements. \nHowever the experimental section seems lacking since there is an amount of information in autoencoder research with many different versions such as denoising autoencoders that have proven to be more effective than simple methods without extra complexity involved Despite this drawback the paper offers a thoughtful examination that results in a fresh learning technique for a well known issue which could potentially be a valuable addition, to the field"
        }
    ],
    "editorDocumentId": null
}