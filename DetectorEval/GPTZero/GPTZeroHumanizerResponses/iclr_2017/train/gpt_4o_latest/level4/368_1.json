{
    "version": "2025-03-13-base",
    "scanId": "5fbd18d8-28a1-4bd0-9af2-f1ed832a9613",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999850392341614,
                    "sentence": "I just finished reviewing it.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999951124191284,
                    "sentence": "This study presents a method for assessing decoder based models using Annealed Importance Sampling (AIS).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999946355819702,
                    "sentence": "It argues that quantitative evaluations are essential due to the use of qualitative assessments in models, like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999946355819702,
                    "sentence": "The researchers show that while there are techniques like Kernel Density Estimation (KD) the AIS approach offers better precision compared to KD and allows for detailed comparisons among different generative models such, as GAN's,GMMNs and VAEs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999966621398926,
                    "sentence": "The authors share their findings from a study that compares the performance of two decoder structures trained on the MNIST dataset using various objectives like VAE (Variational Autoencoder) GAN (Generative Adversarial Network) and GMMN (Generative Moment Matching Network).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948143959045,
                    "sentence": "They also conducted training for an Importance Weighted Autoencoder (IWAE) on binarized MNIST data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999945759773254,
                    "sentence": "Discovered that the IWAEs lower bound estimates of true log probabilities fall short by at least 1 natᅳan important difference, for this datasetᅳafter evaluating with AIS using the same model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999929666519165,
                    "sentence": "Advantages",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999939203262329,
                    "sentence": "The writers have shared their assessment framework with the publicᅳa contribution, to the research field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958872795105,
                    "sentence": "This study delves into how GAN systems behave by looking at their log probability outlook and questioning the belief that GAN models simply store data from their training sets, in memory.They also note that GAN models struggle to represent key aspects of the data distribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999921321868896,
                    "sentence": "Disadvantages / Queries",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999947547912598,
                    "sentence": "I wonder why the experiments involve using quantities of examples (100 1 000 10 000) sourced from various places (training set validation set testing set or model generated samples).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999333024024963,
                    "sentence": "For example in Table 2 why weren't the outcomes shown for all 10 000 examples, from the testing set?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999237656593323,
                    "sentence": "Why does AIS take longer than AIS combined with an encoder, in Figure 2C.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999486804008484,
                    "sentence": "Do both methods have the same number of intermediate distributions?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999937117099762,
                    "sentence": "Using 16 separate chains for AIS appears to be on the lower side when compared to previous studies that utilized a larger number like 100 chains as seen in works, by Salakhutdinov & Murray (2008) and Desjarins et al., (2011).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999433755874634,
                    "sentence": "Would expanding the quantity of chains possibly result in confidence intervals as shown in Table 2?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999399185180664,
                    "sentence": "The authors should explain why GAN50 shows a BDMC gap of 10 nats that's significantly larger, than the gaps seen in other models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999043345451355,
                    "sentence": "Minor Remarks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999040365219116,
                    "sentence": "Table 1 is.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999454617500305,
                    "sentence": "Not explained clearly with details, about the columns it contains.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999328851699829,
                    "sentence": "Are the values shown in Figure 1(a) the average log probability scores, for 100 training and validation instances (as detailed in Section 5 Subsection 5) or is it something else entirely?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999045729637146,
                    "sentence": "Is the dataset in Figure 2(c) the binarized MNIST dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999158978462219,
                    "sentence": "Can you explain why there are fewer datapoints, for AIS compared to IWAE and AIS + encoder?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999085068702698,
                    "sentence": "Are the gaps identified in Section 5 subsection 5, to the ones listed in Table 2?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999114274978638,
                    "sentence": "There is an error in the caption, for Figure 3 where \"( c ) GMMN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998610615730286,
                    "sentence": "10 \" Is mentioned as labeled incorrectly; however the graph title and sub caption suggest that it should actually be GMMN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997844696044922,
                    "sentence": "50.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9997932945046397,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046397,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046397,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046397,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999945492084873,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 5.4507915127851835e-06,
                        "ai_paraphrased": 0.9999945492084873
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 5.450691512785214e-06,
                            "ai_paraphrased": 0.9999945492084873
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "I just finished reviewing it.  \nThis study presents a method for assessing decoder based models using Annealed Importance Sampling (AIS). It argues that quantitative evaluations are essential due to the use of qualitative assessments in models, like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs). The researchers show that while there are techniques like Kernel Density Estimation (KD) the AIS approach offers better precision compared to KD and allows for detailed comparisons among different generative models such, as GAN's,GMMNs and VAEs.\nThe authors share their findings from a study that compares the performance of two decoder structures trained on the MNIST dataset using various objectives like VAE (Variational Autoencoder) GAN (Generative Adversarial Network) and GMMN (Generative Moment Matching Network). They also conducted training for an Importance Weighted Autoencoder (IWAE) on binarized MNIST data. Discovered that the IWAEs lower bound estimates of true log probabilities fall short by at least 1 nat—an important difference, for this dataset—after evaluating with AIS using the same model. \nAdvantages  \nThe writers have shared their assessment framework with the public—a contribution, to the research field.   \nThis study delves into how GAN systems behave by looking at their log probability outlook and questioning the belief that GAN models simply store data from their training sets, in memory.They also note that GAN models struggle to represent key aspects of the data distribution. \nDisadvantages / Queries  \nI wonder why the experiments involve using quantities of examples (100 1 000 10 000) sourced from various places (training set validation set testing set or model generated samples). For example in Table 2 why weren't the outcomes shown for all 10 000 examples, from the testing set?   \nWhy does AIS take longer than AIS combined with an encoder, in Figure 2C. Do both methods have the same number of intermediate distributions?   \nUsing 16 separate chains for AIS appears to be on the lower side when compared to previous studies that utilized a larger number like 100 chains as seen in works, by Salakhutdinov & Murray (2008) and Desjarins et al., (2011). Would expanding the quantity of chains possibly result in confidence intervals as shown in Table 2?   \nThe authors should explain why GAN50 shows a BDMC gap of 10 nats that's significantly larger, than the gaps seen in other models. \nMinor Remarks.  \nTable 1 is. Not explained clearly with details, about the columns it contains.   \nAre the values shown in Figure 1(a) the average log probability scores, for 100 training and validation instances (as detailed in Section 5 Subsection 5) or is it something else entirely?   \nIs the dataset in Figure 2(c) the binarized MNIST dataset. Can you explain why there are fewer datapoints, for AIS compared to IWAE and AIS + encoder?   \nAre the gaps identified in Section 5 subsection 5, to the ones listed in Table 2?   \nThere is an error in the caption, for Figure 3 where \"( c ) GMMN. 10 \" Is mentioned as labeled incorrectly; however the graph title and sub caption suggest that it should actually be GMMN. 50. "
        }
    ],
    "editorDocumentId": null
}