{
    "version": "2025-03-13-base",
    "scanId": "1cc0d8e6-204d-4fa5-9f76-70ce9e9dcf63",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999997615814209,
                    "sentence": "The research paper shows how adjusting a models parameters while training can improve the optimization of a modified objective function.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969601631165,
                    "sentence": "While equations 4 to 7 posed some difficulty in understanding in clarifying the exact definition of the weak gradient \\( g \\) Equation 8 is easier to grasp.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "Section 2.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977350234985,
                    "sentence": "3 Convincingly illustrates that for a type of modifications minimizing the adjusted loss is essentially the same as training, with Gaussian parameter noise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977946281433,
                    "sentence": "The authors expand on this concept by introducing mollifiers to enhance the annealing effect in a more sophisticated manner that can be incorporated into cutting edge neural network designs like deep ReLU networks and LSTM recurrent networks.However the resulting annealing behavior may seem unexpected.For instance in Section 4,the Binomial (or maybe Bernoulli?)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977350234985,
                    "sentence": "parameter shifts, from 0 (indicating identity layers) to 1 (indicating deterministic ReLU layers) implying that the network goes through an initial phase of adding noise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977350234985,
                    "sentence": "This development might actually work against the desired outcome of temperance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "The strategies for annealing that are showcased seem well thought out in real world applications.For instance the first algorithm that dictates the activation of units, in a layer comprises a series of nine clear steps.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "The authors could have improved their paper by examining models using basic mollifiers instead of generalized ones and providing examples where perturbation schemes from the mollifier framework are shown to be better for optimization, than traditional heuristic methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999910422025685,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 8.95779743139153e-06,
                        "ai_paraphrased": 0.9999910422025685
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 8.95769743139156e-06,
                            "ai_paraphrased": 0.9999910422025685
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The research paper shows how adjusting a models parameters while training can improve the optimization of a modified objective function. While equations 4 to 7 posed some difficulty in understanding in clarifying the exact definition of the weak gradient \\( g \\) Equation 8 is easier to grasp. Section 2. 3 Convincingly illustrates that for a type of modifications minimizing the adjusted loss is essentially the same as training, with Gaussian parameter noise. \nThe authors expand on this concept by introducing mollifiers to enhance the annealing effect in a more sophisticated manner that can be incorporated into cutting edge neural network designs like deep ReLU networks and LSTM recurrent networks.However the resulting annealing behavior may seem unexpected.For instance in Section 4,the Binomial (or maybe Bernoulli?) parameter shifts, from 0 (indicating identity layers) to 1 (indicating deterministic ReLU layers) implying that the network goes through an initial phase of adding noise. This development might actually work against the desired outcome of temperance. \nThe strategies for annealing that are showcased seem well thought out in real world applications.For instance the first algorithm that dictates the activation of units, in a layer comprises a series of nine clear steps. \nThe authors could have improved their paper by examining models using basic mollifiers instead of generalized ones and providing examples where perturbation schemes from the mollifier framework are shown to be better for optimization, than traditional heuristic methods. "
        }
    ],
    "editorDocumentId": null
}