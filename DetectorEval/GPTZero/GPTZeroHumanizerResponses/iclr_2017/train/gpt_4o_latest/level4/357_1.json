{
    "version": "2025-03-13-base",
    "scanId": "de91b4c9-c2c2-4fd4-82d6-4bf62f0ab211",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9996358156204224,
                    "sentence": "In this study a new method is suggested for predicting frames, in videos by encoding motion and content separately and incorporating multi scale residual connections.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998180866241455,
                    "sentence": "The researchers present quantitative findings using the KTH Weizmann and UCF.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996076822280884,
                    "sentence": "101 Datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998970627784729,
                    "sentence": "The idea of separating movement from content is interesting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998769760131836,
                    "sentence": "Seems to work well for the task at hand.However the impact seems small compared to previous research on multi stream networks and its not clear if this particular separation approach is widely useful or significant, beyond predicting future frames.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998487830162048,
                    "sentence": "The findings regarding the KTH and Weizmann datasets are impressive and demonstrate advancements beyond the standard benchmarks; however the outcomes for the UCF-101 dataset with fewer restrictions are not as remarkable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997633099555969,
                    "sentence": "Moreover the visual instances, for UCF-101 are not very convincing as pointed out in the assessment query.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997920393943787,
                    "sentence": "Overall this study is well done with a concept though not particularly groundbreaking.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997824430465698,
                    "sentence": "To strengthen the impact of the research it would be helpful to showcase how the suggested decoupling method can be applied broadly for instance in various other video related activities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999455592459217,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 5.4440754078299356e-05,
                        "ai_paraphrased": 0.9999455592459217
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 5.444065407829938e-05,
                            "ai_paraphrased": 0.9999455592459217
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "In this study a new method is suggested for predicting frames, in videos by encoding motion and content separately and incorporating multi scale residual connections. The researchers present quantitative findings using the KTH Weizmann and UCF. 101 Datasets.\nThe idea of separating movement from content is interesting. Seems to work well for the task at hand.However the impact seems small compared to previous research on multi stream networks and its not clear if this particular separation approach is widely useful or significant, beyond predicting future frames. \nThe findings regarding the KTH and Weizmann datasets are impressive and demonstrate advancements beyond the standard benchmarks; however the outcomes for the UCF‐101 dataset with fewer restrictions are not as remarkable. Moreover the visual instances, for UCF‐101 are not very convincing as pointed out in the assessment query. \nOverall this study is well done with a concept though not particularly groundbreaking. To strengthen the impact of the research it would be helpful to showcase how the suggested decoupling method can be applied broadly for instance in various other video related activities. "
        }
    ],
    "editorDocumentId": null
}