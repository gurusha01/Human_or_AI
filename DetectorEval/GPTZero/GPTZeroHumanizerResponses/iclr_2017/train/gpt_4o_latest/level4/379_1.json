{
    "version": "2025-03-13-base",
    "scanId": "f8bcaa61-9580-40ef-b165-fea19bc3c557",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999552369117737,
                    "sentence": "The writers introduce TensorFlow Folds implementation that allows running calculations without the need to adjust the computation graph by creating a universal scheduler within a TensorFlow computation graph that processes a graph description, for execution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999428987503052,
                    "sentence": "They effectively showcase the benefits of using this method in situations where the calculation differs for each data point.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999285936355591,
                    "sentence": "A scenario commonly seen in Tree Recursive Neural Networks (Tree RNN).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999321103096008,
                    "sentence": "During their tests and trials of the approach presented in the study report, by the authors involved comparing it with two standard practices as a means of benchmarking its effectiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999316334724426,
                    "sentence": "One involving a fixed batch (utilizing the same graph structure repeatedly) and the other employing a batch size of 1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999026656150818,
                    "sentence": "I gave it a score of 7 of a higher score because they didn't compare their approach to the main alternative option effectively enough.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999027848243713,
                    "sentence": "To clarify further.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999900221824646,
                    "sentence": "Than using their graph as the scheduling mechanism, for each dynamic batch generated separately in TensorFlow graph could be an alternative.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999139904975891,
                    "sentence": "In terms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998468160629272,
                    "sentence": "One could create each non uniform batch explicitly as a TensorFlow graph and run it using standard TensorFlow features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                }
            ],
            "completely_generated_prob": 0.9997932945046397,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046397,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046397,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046397,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999822965322488,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.7703467751182086e-05,
                        "ai_paraphrased": 0.9999822965322488
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.7703367751182117e-05,
                            "ai_paraphrased": 0.9999822965322488
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The writers introduce TensorFlow Folds implementation that allows running calculations without the need to adjust the computation graph by creating a universal scheduler within a TensorFlow computation graph that processes a graph description, for execution. \nThey effectively showcase the benefits of using this method in situations where the calculation differs for each data point. A scenario commonly seen in Tree Recursive Neural Networks (Tree RNN). \nDuring their tests and trials of the approach presented in the study report, by the authors involved comparing it with two standard practices as a means of benchmarking its effectiveness. One involving a fixed batch (utilizing the same graph structure repeatedly) and the other employing a batch size of 1. \nI gave it a score of 7 of a higher score because they didn't compare their approach to the main alternative option effectively enough. To clarify further. Than using their graph as the scheduling mechanism, for each dynamic batch generated separately in TensorFlow graph could be an alternative. In terms. One could create each non uniform batch explicitly as a TensorFlow graph and run it using standard TensorFlow features. "
        }
    ],
    "editorDocumentId": null
}