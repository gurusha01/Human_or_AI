{
    "version": "2025-03-13-base",
    "scanId": "d5a2897a-2083-4948-85d7-98cd2ca48284",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "This research paper delves into the exploration of off policy learning in actor critic methods with the utilization of experience replay - an intricate issue impacting the effectiveness of reinforcement learning algorithms when it comes to sample efficiency enhancement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999970197677612,
                    "sentence": "The authors tackle this issue by introducing a method for truncating importance weights and implementing a modified trust region optimization strategy while also incorporating the retrace method into the mix.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999954700469971,
                    "sentence": "The collaborative application of these strategies produces outcomes on Atari and MuJoCo benchmarks by showcasing improved sample efficiency.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999959468841553,
                    "sentence": "A crucial question arises regarding how each technique contributes to the overall enhancement, in performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999966621398926,
                    "sentence": "Carrying out tests to measure the individual advantages of these approaches would offer valuable knowledge and shed light on their distinct contributions, to the improvements noted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999522436762217,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 4.775632377826997e-05,
                        "ai_paraphrased": 0.9999522436762217
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 4.775622377827e-05,
                            "ai_paraphrased": 0.9999522436762217
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This research paper delves into the exploration of off policy learning in actor critic methods with the utilization of experience replay â€“ an intricate issue impacting the effectiveness of reinforcement learning algorithms when it comes to sample efficiency enhancement. The authors tackle this issue by introducing a method for truncating importance weights and implementing a modified trust region optimization strategy while also incorporating the retrace method into the mix. The collaborative application of these strategies produces outcomes on Atari and MuJoCo benchmarks by showcasing improved sample efficiency. A crucial question arises regarding how each technique contributes to the overall enhancement, in performance. Carrying out tests to measure the individual advantages of these approaches would offer valuable knowledge and shed light on their distinct contributions, to the improvements noted. "
        }
    ],
    "editorDocumentId": null
}