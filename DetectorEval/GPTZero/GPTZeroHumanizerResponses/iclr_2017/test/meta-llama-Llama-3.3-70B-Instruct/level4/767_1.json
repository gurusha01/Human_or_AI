{
    "version": "2025-03-13-base",
    "scanId": "69ba44d0-4700-42e0-9418-c4c690540be8",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999973177909851,
                    "sentence": "The writers suggest a method for choosing the best step size in stochastic gradient descent (SGD) viewing the learning rate as a decision in a Markov Decision Process (MDP) where the reward is based on the loss change.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973177909851,
                    "sentence": "They test this technique against known adaptive optimization methods like Adagrad, Adam and RMSProp for training deep neural networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999980926513672,
                    "sentence": "While the results are interesting making a direct comparison is tough due, to varying evaluation criteria.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999980926513672,
                    "sentence": "There are important aspects that need careful thought;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "The actor critic algorithms computational expenses compared to methods are not clearly measured in a direct manner.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "It is quite noticeable that there is a lack of optimization plots related to wall time when considering the success of approaches such, as Adagrad historically relies more on their effectiveness in time taken rather than the number of iterations used.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999975562095642,
                    "sentence": "The choice to use one learning rate for all parameters seems questionable to me.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960660934448,
                    "sentence": "To have a basis for comparison with other adaptive first order techniques that typically adjust learning rates per parameter it would be interesting to develop individual RL models, for each parameter emulating the adaptive approaches used by well known first order methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995768070221,
                    "sentence": "Given that learning's always changing and not constant but reinforcement learning algorithms assume a stable environment it raises doubts, about whether an RL strategy can really figure out the best pace of learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967217445374,
                    "sentence": "The idea that RL depends on things staying the same may not fit well with how learning works since its always evolving and shifting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999996542930603,
                    "sentence": "Figure 6 poses a question about how the proposed approach compares to methods such as early stopping in machine learning models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995768070221,
                    "sentence": "It's possible that the actor critic method shows less overfitting not necessarily due, to regularization techniques but potentially because it might struggle more with optimization tasks leading to underfitting issues.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994695186615,
                    "sentence": "A detailed comparison taking into account these factors would offer an insight into the strengths and weaknesses of the method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999278588744412,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 7.214112555874672e-05,
                        "ai_paraphrased": 0.9999278588744412
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 7.214102555874676e-05,
                            "ai_paraphrased": 0.9999278588744412
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The writers suggest a method for choosing the best step size in stochastic gradient descent (SGD) viewing the learning rate as a decision in a Markov Decision Process (MDP) where the reward is based on the loss change. They test this technique against known adaptive optimization methods like Adagrad, Adam and RMSProp for training deep neural networks. While the results are interesting making a direct comparison is tough due, to varying evaluation criteria. There are important aspects that need careful thought; \nThe actor critic algorithms computational expenses compared to methods are not clearly measured in a direct manner. It is quite noticeable that there is a lack of optimization plots related to wall time when considering the success of approaches such, as Adagrad historically relies more on their effectiveness in time taken rather than the number of iterations used. \nThe choice to use one learning rate for all parameters seems questionable to me. To have a basis for comparison with other adaptive first order techniques that typically adjust learning rates per parameter it would be interesting to develop individual RL models, for each parameter emulating the adaptive approaches used by well known first order methods. \nGiven that learning's always changing and not constant but reinforcement learning algorithms assume a stable environment it raises doubts, about whether an RL strategy can really figure out the best pace of learning. The idea that RL depends on things staying the same may not fit well with how learning works since its always evolving and shifting. \nFigure 6 poses a question about how the proposed approach compares to methods such as early stopping in machine learning models. It's possible that the actor critic method shows less overfitting not necessarily due, to regularization techniques but potentially because it might struggle more with optimization tasks leading to underfitting issues. A detailed comparison taking into account these factors would offer an insight into the strengths and weaknesses of the method. "
        }
    ],
    "editorDocumentId": null
}