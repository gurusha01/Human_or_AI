{
    "version": "2025-03-13-base",
    "scanId": "56d4e9dd-2850-428c-9dce-ea5eac1cb7b9",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999957084655762,
                    "sentence": "This study introduces a perspective on dropout by incorporating a latent variable model in which the dropout variable is considered as an unseen element that is averaged out of the analysis process.The process of estimating the likelihood, within this model is complex; however it demonstrates that traditional dropout can be seen as a straightforward Monte Carlo approximation of the maximum likelihood estimation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958872795105,
                    "sentence": "The writers then create a structure to examine the difference known as the inference gap between the model employed in training (a mix of models or the latent variable model) and the model utilized in testing (where the average activation expectation is usually estimated by a single model with averaged weights).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995231628418,
                    "sentence": "This structure introduces important ideas such, as expectation linearity that allows for the study of transition functions and layers that can reduce the inference gap.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999957084655762,
                    "sentence": "The third theorem sets a limit on the difference, in inference.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961256980896,
                    "sentence": "Gives useful perspectives on how the training and testing models are connected.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999959468841553,
                    "sentence": "Additionally the article suggests a regularization factor aimed at reducing the discrepancy in inference while learning.The empirical findings on the MNIST,CIFAR.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971985816956,
                    "sentence": "10,And CIFAR.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967813491821,
                    "sentence": "100 Datasets showcase the promise of this approach to surpass dropout techniques and achieve results akin, to Monte Carlo Dropout.However this comes at a computational expense as a new hyperparameter is introduced.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999963641166687,
                    "sentence": "The theoretical framework discussed in this research provides a perspective on dropout as a latent variable model where standard dropout is likened to a Monte Carlo approximation method with broad implications, for future dropout studies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999951720237732,
                    "sentence": "The method used to analyze the inference gap is also notable; however its practicality might have some constraints.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999942183494568,
                    "sentence": "The suggested method seems reasonable; however it does come with a drawbacks such as relying on basic datasets and achieving only minor performance improvements along, with the added computational burden during training because of the new hyperparameter introduced.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999926686286926,
                    "sentence": "Furthermore a typo was spotted on page 6,line 8 where \"expecatation\" should be rectified to \"expectation.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999987973704706,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.2026295293310457e-06,
                        "ai_paraphrased": 0.9999987973704706
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.2025295293310757e-06,
                            "ai_paraphrased": 0.9999987973704706
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study introduces a perspective on dropout by incorporating a latent variable model in which the dropout variable is considered as an unseen element that is averaged out of the analysis process.The process of estimating the likelihood, within this model is complex; however it demonstrates that traditional dropout can be seen as a straightforward Monte Carlo approximation of the maximum likelihood estimation. \nThe writers then create a structure to examine the difference known as the inference gap between the model employed in training (a mix of models or the latent variable model) and the model utilized in testing (where the average activation expectation is usually estimated by a single model with averaged weights). This structure introduces important ideas such, as expectation linearity that allows for the study of transition functions and layers that can reduce the inference gap. The third theorem sets a limit on the difference, in inference. Gives useful perspectives on how the training and testing models are connected. \nAdditionally the article suggests a regularization factor aimed at reducing the discrepancy in inference while learning.The empirical findings on the MNIST,CIFAR. 10,And CIFAR. 100 Datasets showcase the promise of this approach to surpass dropout techniques and achieve results akin, to Monte Carlo Dropout.However this comes at a computational expense as a new hyperparameter is introduced. \nThe theoretical framework discussed in this research provides a perspective on dropout as a latent variable model where standard dropout is likened to a Monte Carlo approximation method with broad implications, for future dropout studies. The method used to analyze the inference gap is also notable; however its practicality might have some constraints. \nThe suggested method seems reasonable; however it does come with a drawbacks such as relying on basic datasets and achieving only minor performance improvements along, with the added computational burden during training because of the new hyperparameter introduced. Furthermore a typo was spotted on page 6,line 8 where \"expecatation\" should be rectified to \"expectation.\" "
        }
    ],
    "editorDocumentId": null
}