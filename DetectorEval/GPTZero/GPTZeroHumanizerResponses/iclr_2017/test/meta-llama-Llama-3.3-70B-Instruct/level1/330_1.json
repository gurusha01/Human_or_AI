{
    "version": "2025-03-13-base",
    "scanId": "c19734f2-537e-4756-845a-d130eead765a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999829530715942,
                    "sentence": "The research paper titled \"Document Vector through Corruption (Doc2VecC)\" introduces a method for learning document representations that effectively captures the meaning of texts in a semantic way.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999812841415405,
                    "sentence": "The authors suggest a straightforward model structure that represents each document by averaging word embeddings and incorporates a corruption model that prioritizes uncommon words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999768137931824,
                    "sentence": "This method surpasses cutting edge algorithms, for learning document representations in terms of both efficiency and the richness of the resulting representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999596476554871,
                    "sentence": "After consideration of the paper at hand and weighing two crucial factors in favor of acceptance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999686479568481,
                    "sentence": "Firstly due, to the well founded approach rooted in existing literature and secondly supported by compelling empirical evidence validating its assertions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999769926071167,
                    "sentence": "I have decided to approve it for publication.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999436736106873,
                    "sentence": "The paper presents a explained and organized overview of the methodology with a detailed explanation of the models structure and how errors are introduced intentionally in the process as well as a thorough examination of the outcomes with a talk, on both the positive aspects and drawbacks of their methodological approach.The practical test is thorough and meticulously planned out with a description of how the experiments were set up and what results were obtained.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999526143074036,
                    "sentence": "To enhance the quality of the paper more I recommend that the authors delve deeper into how corruption impacts word embeddings and its mechanism.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999684691429138,
                    "sentence": "For instance they could conduct an analysis of the words that bear the brunt of corruption and explore how this impacts the models overall effectiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999765157699585,
                    "sentence": "Furthermore the authors might want to contemplate offering comparisons, with alternative approaches like those utilizing recurrent neural networks or convolutional neural networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999560117721558,
                    "sentence": "I would appreciate it if the authors could address a couple of queries to help me better grasp the paper; (1a).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999732971191406,
                    "sentence": "How does the corruption rate impact the models performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999732971191406,
                    "sentence": "What is the most effective corruption rate for various tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999740719795227,
                    "sentence": "(2a).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999622702598572,
                    "sentence": "Could the authors offer explanation about the data specific regularization brought in by the corruption process and its impact, on word embeddings?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999960720539093,
                    "sentence": "How is the method dealing with words not found in the list and what impact does it have for tasks that have insufficient training data available?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                }
            ],
            "completely_generated_prob": 0.9999999999999999,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9999999999999999,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9999999999999999,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9999999999999999,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999977250895262,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.2749104737987603e-06,
                        "ai_paraphrased": 0.9999977250895262
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.27481047379879e-06,
                            "ai_paraphrased": 0.9999977250895262
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The research paper titled \"Document Vector through Corruption (Doc2VecC)\" introduces a method for learning document representations that effectively captures the meaning of texts in a semantic way. The authors suggest a straightforward model structure that represents each document by averaging word embeddings and incorporates a corruption model that prioritizes uncommon words. This method surpasses cutting edge algorithms, for learning document representations in terms of both efficiency and the richness of the resulting representations. \nAfter consideration of the paper at hand and weighing two crucial factors in favor of acceptance. Firstly due, to the well founded approach rooted in existing literature and secondly supported by compelling empirical evidence validating its assertions. I have decided to approve it for publication. \nThe paper presents a explained and organized overview of the methodology with a detailed explanation of the models structure and how errors are introduced intentionally in the process as well as a thorough examination of the outcomes with a talk, on both the positive aspects and drawbacks of their methodological approach.The practical test is thorough and meticulously planned out with a description of how the experiments were set up and what results were obtained. \nTo enhance the quality of the paper more I recommend that the authors delve deeper into how corruption impacts word embeddings and its mechanism. For instance they could conduct an analysis of the words that bear the brunt of corruption and explore how this impacts the models overall effectiveness. Furthermore the authors might want to contemplate offering comparisons, with alternative approaches like those utilizing recurrent neural networks or convolutional neural networks. \nI would appreciate it if the authors could address a couple of queries to help me better grasp the paper; (1a). How does the corruption rate impact the models performance. What is the most effective corruption rate for various tasks? (2a). Could the authors offer explanation about the data specific regularization brought in by the corruption process and its impact, on word embeddings? How is the method dealing with words not found in the list and what impact does it have for tasks that have insufficient training data available? "
        }
    ],
    "editorDocumentId": null
}