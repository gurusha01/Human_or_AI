{
    "version": "2025-03-13-base",
    "scanId": "411333c5-82d7-4a02-a181-1bbde9ea1df2",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999960660934448,
                    "sentence": "The paper discusses dropout using a variable model approach in which the dropout variable (representing whether units are dropped with values of 0 or 1) is unseen and then integrated out of the equation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961256980896,
                    "sentence": "Although its challenging to estimate likelihood under this model directly its revealed that standard dropout is essentially a simple Monte Carlo estimation of maximum likelihood, for this latent variable model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961256980896,
                    "sentence": "The authors suggest a framework to examine the difference called the inference gap.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960064888,
                    "sentence": "This refers to the mismatch between the model utilized in training (such as a model ensemble or latent variable model in this instance).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999949336051941,
                    "sentence": "The model employed in testing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995231628418,
                    "sentence": "During testing phase activation over models is estimated by the activation of a single model, with averaged weights.This model presents a number of ideas, like linearity of expectations that help in analyzing the transition functions or layers that can result in a minimal inference gap.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994158744812,
                    "sentence": "Moreoverllylyylyylyyyllllllyyyyyys paper presents a regularization factor aimed at reducing the discrepancy, in inference while training occurs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999963045120239,
                    "sentence": "Trials carried out on datasets like MNIST and CIFAR show that the new technique could surpass dropout methods and deliver results similar to those of Monte Carlo Dropout - a common strategy used to calculate dropout outputs in line, with ensemble training principles but at a much higher computational expense.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958872795105,
                    "sentence": "The research presents a theoretical framework on dropout by viewing it as a latent variable model and likened to a Monte Carlo approximation method which could be widely used in future studies, on dropout rates.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999949932098389,
                    "sentence": "The method for examining the distinction, in conclusions is fascinating; it may have limited relevance in some cases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999952912330627,
                    "sentence": "The suggested model makes an argument; however it is important to consider that the experiments were conducted using rather straightforward datasets and only resulted in minor performance enhancements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999946355819702,
                    "sentence": "Additionally involving a hyperparameter, in the training process leads to increased computational demands.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999886155128479,
                    "sentence": "There seems to be a typo on page 6, at line 8 - \"expecatation\" should be corrected to \"expectation.”",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999971848296667,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.815170333377864e-06,
                        "ai_paraphrased": 0.9999971848296667
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.815070333377894e-06,
                            "ai_paraphrased": 0.9999971848296667
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The paper discusses dropout using a variable model approach in which the dropout variable (representing whether units are dropped with values of 0 or 1) is unseen and then integrated out of the equation. Although its challenging to estimate likelihood under this model directly its revealed that standard dropout is essentially a simple Monte Carlo estimation of maximum likelihood, for this latent variable model. \nThe authors suggest a framework to examine the difference called the inference gap. This refers to the mismatch between the model utilized in training (such as a model ensemble or latent variable model in this instance). The model employed in testing. During testing phase activation over models is estimated by the activation of a single model, with averaged weights.This model presents a number of ideas, like linearity of expectations that help in analyzing the transition functions or layers that can result in a minimal inference gap. \nMoreoverllylyylyylyyyllllllyyyyyys paper presents a regularization factor aimed at reducing the discrepancy, in inference while training occurs. \nTrials carried out on datasets like MNIST and CIFAR show that the new technique could surpass dropout methods and deliver results similar to those of Monte Carlo Dropout – a common strategy used to calculate dropout outputs in line, with ensemble training principles but at a much higher computational expense. \nThe research presents a theoretical framework on dropout by viewing it as a latent variable model and likened to a Monte Carlo approximation method which could be widely used in future studies, on dropout rates. \nThe method for examining the distinction, in conclusions is fascinating; it may have limited relevance in some cases. \nThe suggested model makes an argument; however it is important to consider that the experiments were conducted using rather straightforward datasets and only resulted in minor performance enhancements. Additionally involving a hyperparameter, in the training process leads to increased computational demands. \nThere seems to be a typo on page 6, at line 8 – \"expecatation\" should be corrected to \"expectation.”"
        }
    ],
    "editorDocumentId": null
}