{
    "version": "2025-03-13-base",
    "scanId": "3b8448d7-c8d1-4d2f-9629-da81e905cf17",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9940649271011353,
                    "sentence": "The article presents an approach to predict future frames by transforming the preceding frame instead of directly forecasting individual pixels.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9933702945709229,
                    "sentence": "Many previous studies have suggested methods and approaches, in their discussions The authors contend that earlier works follow a deterministic path but also note that the suggested model does not adequately consider multimodality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9918665885925293,
                    "sentence": "Besides that idea was to try out the approach by using two RGB frames for input and predicting the transformation as the result to see how important it is to apply transformations, in both input and output since thiss the first study to use transformations as input too; however this suggestion was not accepted by the writers who mentioned that \"if we used RGB frames as input and tasked the model with generating future frames it would give really blurry outcomes \" showing a lack of understanding of the proposal.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9899414777755737,
                    "sentence": "The current work doesn't seem to bring anything significant compared to previous studies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999204992503731,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 7.950074962691109e-05,
                        "ai_paraphrased": 0.9999204992503731
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 7.950064962691112e-05,
                            "ai_paraphrased": 0.9999204992503731
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The article presents an approach to predict future frames by transforming the preceding frame instead of directly forecasting individual pixels. \nMany previous studies have suggested methods and approaches, in their discussions The authors contend that earlier works follow a deterministic path but also note that the suggested model does not adequately consider multimodality. \nBesides that idea was to try out the approach by using two RGB frames for input and predicting the transformation as the result to see how important it is to apply transformations, in both input and output since thiss the first study to use transformations as input too; however this suggestion was not accepted by the writers who mentioned that \"if we used RGB frames as input and tasked the model with generating future frames it would give really blurry outcomes \" showing a lack of understanding of the proposal. The current work doesn't seem to bring anything significant compared to previous studies. "
        }
    ],
    "editorDocumentId": null
}