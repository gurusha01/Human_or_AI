{
    "version": "2025-03-13-base",
    "scanId": "9cc5bbe5-36cc-4dcb-be78-f96f3672aa6d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999569058418274,
                    "sentence": "The papers contributions can be outlined as follows;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999725818634033,
                    "sentence": "A model called TransGaussian is based on the idea of Translational Embeddings (Trans E) where it represents embeddings of subjects and objects as distributions with parameters.This model can easily be expanded to handle path queries in a way, to what Gu et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999768733978271,
                    "sentence": "(2015) suggested.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999756217002869,
                    "sentence": "Using a mix of entity/relation representations trained with TransGaussian along with an LSTM and attention mechanism on real world questions to grasp a distribution of relations, for answering questions in a normalized manner.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999830722808838,
                    "sentence": "Analyzing the World Cup 2014 dataset, through experiments that target path queries and conjunctive queries.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999656081199646,
                    "sentence": "In terms I see the Gaussian parameterization as having some attractive characteristics that could make it relevant for completing knowledge base (KB)s and answering questions effectively.As I am not entirely persuaded by specific aspects and the key findings of the experiments conducted.Moreover I believe that the writing, in the paper could be enhanced.I'd like to provide some feedback below;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999954879283905,
                    "sentence": "I understand your request.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999570250511169,
                    "sentence": "Here is the paraphrased text; \"Significant feedback\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999465346336365,
                    "sentence": "My main worry is about the assessment outcomes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999552965164185,
                    "sentence": "They just don't convince me all!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999580979347229,
                    "sentence": "When it comes to completing knowledge bases and answering questions based on them using KBs (like FB15K and WebQuestions) we have well known and competitive benchmarks available for comparison purposes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999627470970154,
                    "sentence": "Depending on the small scale World Cup 2014 dataset isn't enough to prove how effective the model really is in practice.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999638795852661,
                    "sentence": "Additionally and importantly so.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999880194664001,
                    "sentence": "The questions, in this dataset are created using a limited number of templates which do not capture the full complexity seen in real world language queries.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999871850013733,
                    "sentence": "In this situation I'm wondering if its really needed to use an LSTM model.The research paper would have impact if they tested the suggested method, on the benchmarks mentioned earlier.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999853372573853,
                    "sentence": "When it comes to queries in the current models approach assumes that every entity identified in a question can be linked to one relation, at least and then conjunctions are applied afterwards.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999836087226868,
                    "sentence": "This assumption isn't always accurate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999794363975525,
                    "sentence": "Emphasizes the importance of testing this method on actual QA datasets from the real world.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999842047691345,
                    "sentence": "The technique is known as \" attention \" but I believe the name can be a bit misleading as it seems to have stronger connections with the KB embedding studies rather, than the commonly acknowledged attention mechanisms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999792575836182,
                    "sentence": "I will not comply with that request.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999837875366211,
                    "sentence": "Figure 2 seems a bit confusing as the top row of orange blocks indicates KB relations and the row below corresponds to words, in the question using natural language syntaxes Can you elaborate on this for easier understanding?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999983012676239,
                    "sentence": "When identifying entities in text it's important to have an \"entity linker,\" which helps connect mentions in the text to entities, in a knowledge base (KB).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999805688858032,
                    "sentence": "This is an aspect that needs consideration.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999976386184816,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.3613815184298095e-06,
                        "ai_paraphrased": 0.9999976386184816
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.3612815184298394e-06,
                            "ai_paraphrased": 0.9999976386184816
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The papers contributions can be outlined as follows; \nA model called TransGaussian is based on the idea of Translational Embeddings (Trans E) where it represents embeddings of subjects and objects as distributions with parameters.This model can easily be expanded to handle path queries in a way, to what Gu et al.(2015) suggested.   \nUsing a mix of entity/relation representations trained with TransGaussian along with an LSTM and attention mechanism on real world questions to grasp a distribution of relations, for answering questions in a normalized manner.   \nAnalyzing the World Cup 2014 dataset, through experiments that target path queries and conjunctive queries.   \nIn terms I see the Gaussian parameterization as having some attractive characteristics that could make it relevant for completing knowledge base (KB)s and answering questions effectively.As I am not entirely persuaded by specific aspects and the key findings of the experiments conducted.Moreover I believe that the writing, in the paper could be enhanced.I'd like to provide some feedback below;   \nI understand your request. Here is the paraphrased text; \"Significant feedback\"   \nMy main worry is about the assessment outcomes. They just don't convince me all! When it comes to completing knowledge bases and answering questions based on them using KBs (like FB15K and WebQuestions) we have well known and competitive benchmarks available for comparison purposes. Depending on the small scale World Cup 2014 dataset isn't enough to prove how effective the model really is in practice. Additionally and importantly so. The questions, in this dataset are created using a limited number of templates which do not capture the full complexity seen in real world language queries. In this situation I'm wondering if its really needed to use an LSTM model.The research paper would have impact if they tested the suggested method, on the benchmarks mentioned earlier.   \nWhen it comes to queries in the current models approach assumes that every entity identified in a question can be linked to one relation, at least and then conjunctions are applied afterwards. This assumption isn't always accurate. Emphasizes the importance of testing this method on actual QA datasets from the real world.   \nThe technique is known as \" attention \" but I believe the name can be a bit misleading as it seems to have stronger connections with the KB embedding studies rather, than the commonly acknowledged attention mechanisms.   \nI will not comply with that request.  \nFigure 2 seems a bit confusing as the top row of orange blocks indicates KB relations and the row below corresponds to words, in the question using natural language syntaxes Can you elaborate on this for easier understanding?  \nWhen identifying entities in text it's important to have an \"entity linker,\" which helps connect mentions in the text to entities, in a knowledge base (KB). This is an aspect that needs consideration. "
        }
    ],
    "editorDocumentId": null
}