{
    "version": "2025-03-13-base",
    "scanId": "36de0905-fb43-43ac-b7f5-57eb6e6a2e3c",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999963045120239,
                    "sentence": "The article presents Topic RNN as a recurrent neural network ( RNN ) language model that combines latent topic modeling to capture both local grammatical relationships and overall semantic consistency in text without the need for pre trained topic features like previous contextual RNN models do.The authors showcase its efficiency through findings on word forecasting ( Penn TreeBank dataset ) and sentiment analysis ( IMDB dataset ) demonstrating strong performance, in both tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958872795105,
                    "sentence": "Furthermore Topic Recurrent Neural Network (Topic RNN) produces themes and cohesive text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955892562866,
                    "sentence": "This feature positions it as a substitute for conventional document frameworks such, as Latent Dirichlet Allocation ( LDA ).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948143959045,
                    "sentence": "Sure I'd be happy to help with that!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "Just let me know what text you'd like me to paraphrase to pass as human written.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999938011169434,
                    "sentence": "Top factors contributing to approval;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999953508377075,
                    "sentence": "The combination of RNN with latent topic modeling in an end to end setup is an addition to language modeling efforts outlined in the study.The research highlights the importance of addressing the issue of capturing semantic connections while ensuring accurate syntax - a common concern, in the field of natural language processing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999951124191284,
                    "sentence": "The findings show that Topic based RNN performs better than contextual RNN models, in predicting words (lower perplexity).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995768070221,
                    "sentence": "It also achieves nearly top notch results in sentiment analysis affirms the assertions put forth by the authors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994158744812,
                    "sentence": "Supporting points;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999947547912598,
                    "sentence": "A strong rationale is presented by the authors as they highlight the drawbacks of RNN based and topic models while introducing Topic RNN as a blend that capitalizes on their respective advantages effectively.The innovative integration of topics to influence RNN predictions is both fresh and well supported.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999951124191284,
                    "sentence": "The evaluation process is thorough and detailed as it includes a range of tests that delve into both theoretical aspects like perplexity and real world applications such as sentiment analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999939799308777,
                    "sentence": "The comparison, with models is just and the outcomes are persuasive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999952912330627,
                    "sentence": "The study emphasizes how Topic RNN can create topics and coherent text that goes beyond just performance measures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958276748657,
                    "sentence": "Here are some ideas, for how you can make things better.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960064888,
                    "sentence": "Ensuring the model description is clear is important.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960660934448,
                    "sentence": "Although the architecture is detailed enough in its explanation; certain parts, like how the stop word indicator functions and why Gaussian priors are chosen for topics could be intuitively elaborated for readers who aren't well versed in these methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999968409538269,
                    "sentence": "The paper talks about how it takes to train Topic RNN (for example 78 hours, for IMDB) but it could be improved by discussing scalability and ways to optimize for bigger datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999806880950928,
                    "sentence": "The findings show promise; however the paper would have an impact with further comparisons to newer cutting edge models, like transformer based architectures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999765753746033,
                    "sentence": "The generated text samples are coherent.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999792575836182,
                    "sentence": "They lack diversity and depth in content richness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999791383743286,
                    "sentence": "An in depth evaluation by humans or a qualitative analysis could offer insights, into the models ability to generate content effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999784231185913,
                    "sentence": "Queries, for the Writers;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999783039093018,
                    "sentence": "How does Topic RNN perform in relation to the number of topics (referred to as K)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999762773513794,
                    "sentence": "Have you explored the impact of stop word selection, on its sensitivity level.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999797344207764,
                    "sentence": "Have you tried dynamically identifying stop words during the training process?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999786019325256,
                    "sentence": "Could you explain further why you chose to use a Gaussian for the topic vector instead of the typical Dirichlet prior, in topic models?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999757409095764,
                    "sentence": "How well does Topic RNN work on datasets that contain a variety of text types, like conversations or social media posts?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999749660491943,
                    "sentence": "In summary the paper adds insights to language modeling through a unique hybrid method that is supported by theory and evidence It could become a notable conference contribution, with some minor revisions and more comparisons",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999955133366586,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 4.48666334134832e-06,
                        "ai_paraphrased": 0.9999955133366586
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 4.48656334134835e-06,
                            "ai_paraphrased": 0.9999955133366586
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The article presents Topic RNN as a recurrent neural network ( RNN ) language model that combines latent topic modeling to capture both local grammatical relationships and overall semantic consistency in text without the need for pre trained topic features like previous contextual RNN models do.The authors showcase its efficiency through findings on word forecasting ( Penn TreeBank dataset ) and sentiment analysis ( IMDB dataset ) demonstrating strong performance, in both tasks. Furthermore Topic Recurrent Neural Network (Topic RNN) produces themes and cohesive text. This feature positions it as a substitute for conventional document frameworks such, as Latent Dirichlet Allocation ( LDA ).\nSure I'd be happy to help with that! Just let me know what text you'd like me to paraphrase to pass as human written.\nTop factors contributing to approval; \nThe combination of RNN with latent topic modeling in an end to end setup is an addition to language modeling efforts outlined in the study.The research highlights the importance of addressing the issue of capturing semantic connections while ensuring accurate syntax â€“ a common concern, in the field of natural language processing. \nThe findings show that Topic based RNN performs better than contextual RNN models, in predicting words (lower perplexity). It also achieves nearly top notch results in sentiment analysis affirms the assertions put forth by the authors. \nSupporting points; \nA strong rationale is presented by the authors as they highlight the drawbacks of RNN based and topic models while introducing Topic RNN as a blend that capitalizes on their respective advantages effectively.The innovative integration of topics to influence RNN predictions is both fresh and well supported. \nThe evaluation process is thorough and detailed as it includes a range of tests that delve into both theoretical aspects like perplexity and real world applications such as sentiment analysis. The comparison, with models is just and the outcomes are persuasive. \nThe study emphasizes how Topic RNN can create topics and coherent text that goes beyond just performance measures. \nHere are some ideas, for how you can make things better.\nEnsuring the model description is clear is important. Although the architecture is detailed enough in its explanation; certain parts, like how the stop word indicator functions and why Gaussian priors are chosen for topics could be intuitively elaborated for readers who aren't well versed in these methods. \nThe paper talks about how it takes to train Topic RNN (for example 78 hours, for IMDB) but it could be improved by discussing scalability and ways to optimize for bigger datasets. \nThe findings show promise; however the paper would have an impact with further comparisons to newer cutting edge models, like transformer based architectures. \nThe generated text samples are coherent. They lack diversity and depth in content richness. An in depth evaluation by humans or a qualitative analysis could offer insights, into the models ability to generate content effectively. \nQueries, for the Writers; \nHow does Topic RNN perform in relation to the number of topics (referred to as K)? Have you explored the impact of stop word selection, on its sensitivity level. Have you tried dynamically identifying stop words during the training process? \nCould you explain further why you chose to use a Gaussian for the topic vector instead of the typical Dirichlet prior, in topic models? \nHow well does Topic RNN work on datasets that contain a variety of text types, like conversations or social media posts? \nIn summary the paper adds insights to language modeling through a unique hybrid method that is supported by theory and evidence It could become a notable conference contribution, with some minor revisions and more comparisons"
        }
    ],
    "editorDocumentId": null
}