{
    "version": "2025-03-13-base",
    "scanId": "7490f58a-2823-4d3c-a261-6752be1a1fd6",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998422265052795,
                    "sentence": "This article presents a model for understanding written text using neural networks that focuses on reading a passage and answering related questions effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997508525848389,
                    "sentence": "The innovative method shares similarities, with models but stands out for its capability to anticipate answers of different lengths instead of just single words or entities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997930526733398,
                    "sentence": "The researchers test their model using the Stanford Question Answering Dataset ( SQuAD ) and show enhancements compared with methods; however the model seems noticeably below the best performance currently documented on the SQuAD leaderboard.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997885227203369,
                    "sentence": "The main idea behind the approach is its ability to recognize response phrases of different lengths effectively.This method employs two strategies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998234510421753,
                    "sentence": "One involves utilizing a POS pattern trie tree to screen word sequences based on observed POS tag patterns during training while the other entails exhaustively listing all possible phrases up to a set maximum length; however these strategies may seem somewhat detached from the overarching concept of \"end to end learning”, for extracting answer sections.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998568892478943,
                    "sentence": "Also mentioned by reviewers is the significant impact of linguistic characteristics in attaining the stated accuracy levels (as shown in Table 3).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998724460601807,
                    "sentence": "One might argue that obtaining features is quite simple, with standard tagging tools; however their dependence diminishes the idea of a completely \"end to end trained\" system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997744560241699,
                    "sentence": "The paper is written well overall; however; some crucial parts that detail the model are a bit hard to grasp fully..",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998060464859009,
                    "sentence": "In particular; the explanation of the attention mechanism seems a bit unclear..",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997565150260925,
                    "sentence": "The mechanism seems to be quite common in the realm of sequence to sequence models without any new design features compared to something like the Gated Attentive Reader..",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997498393058777,
                    "sentence": "Even after some clarification provided later; I still find it challenging to understand how this attention mechanism sets itself apart from implementations, in seq2seq models..",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997178912162781,
                    "sentence": "Finally and most importantly despite showing advancements compared to the SQuAD papers results it is currently positioned 12th among 15 systems listed on the leaderboard.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9999999999999999,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9999999999999999,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9999999999999999,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9999999999999999,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999970759690509,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.9240309491823084e-06,
                        "ai_paraphrased": 0.9999970759690509
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.9239309491823382e-06,
                            "ai_paraphrased": 0.9999970759690509
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This article presents a model for understanding written text using neural networks that focuses on reading a passage and answering related questions effectively. The innovative method shares similarities, with models but stands out for its capability to anticipate answers of different lengths instead of just single words or entities. The researchers test their model using the Stanford Question Answering Dataset ( SQuAD ) and show enhancements compared with methods; however the model seems noticeably below the best performance currently documented on the SQuAD leaderboard. \nThe main idea behind the approach is its ability to recognize response phrases of different lengths effectively.This method employs two strategies. One involves utilizing a POS pattern trie tree to screen word sequences based on observed POS tag patterns during training while the other entails exhaustively listing all possible phrases up to a set maximum length; however these strategies may seem somewhat detached from the overarching concept of \"end to end learning”, for extracting answer sections. Also mentioned by reviewers is the significant impact of linguistic characteristics in attaining the stated accuracy levels (as shown in Table 3). One might argue that obtaining features is quite simple, with standard tagging tools; however their dependence diminishes the idea of a completely \"end to end trained\" system. \nThe paper is written well overall; however; some crucial parts that detail the model are a bit hard to grasp fully.. In particular; the explanation of the attention mechanism seems a bit unclear.. The mechanism seems to be quite common in the realm of sequence to sequence models without any new design features compared to something like the Gated Attentive Reader.. Even after some clarification provided later; I still find it challenging to understand how this attention mechanism sets itself apart from implementations, in seq2seq models..\nFinally and most importantly despite showing advancements compared to the SQuAD papers results it is currently positioned 12th among 15 systems listed on the leaderboard. "
        }
    ],
    "editorDocumentId": null
}