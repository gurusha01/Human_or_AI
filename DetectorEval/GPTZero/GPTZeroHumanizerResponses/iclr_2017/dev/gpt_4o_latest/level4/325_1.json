{
    "version": "2025-03-13-base",
    "scanId": "390d8c42-8ad2-4f6d-8cc4-86c0d2d9a573",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9995343685150146,
                    "sentence": "This study introduces a model that transforms noise into model samples by gradually removing noise in a process known as progressive denoising technique The method has some resemblances to generative models based on diffusion but differs from the diffusion framework, in certain aspects;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995563626289368,
                    "sentence": "It uses a few denoise procedures which helps to boost its computational efficiency by a lot.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992913603782654,
                    "sentence": "Of going backwards along a different path as the generative model does in reverse chronological order like a mirror image of itself) the conditional chain for the estimated posterior q(z| x starts directly at z and moves forward in the same direction as the generative model to guide it towards the data by acting as a perturbation around it; this strategy seems to share some similarities, with ladder networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994393587112427,
                    "sentence": "Finding an upper limit, on the log likelihood proves to be complex and challenging.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991761445999146,
                    "sentence": "The main concept was fascinating.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999130129814148,
                    "sentence": "I was really impressed by how well the visual examples turned out using a short chain of processes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994269609451294,
                    "sentence": "The results of inpainting were particularly outstanding because achieving one shot inpainting is usually not possible, in generative modeling systems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999017059803009,
                    "sentence": "To make the research more interesting and persuasive It would be great to have a log likelihood comparison that doesn't depend on Parzen likelihood methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999113917350769,
                    "sentence": "Section 2;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987536668777466,
                    "sentence": "Theta 1 is the factor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998272180557251,
                    "sentence": "\"The function theta(t)\" > \"The function theta(t)\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987110495567322,
                    "sentence": "\"We will be doing what we will be using.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988995790481567,
                    "sentence": "The method of deducing q(z^(zero)| x ) and conducting inference in sync, with the process resonates with me bringing to mind ladder networks to some extent.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999021053314209,
                    "sentence": "Section 2 paragraph 4;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991725087165833,
                    "sentence": "Section 4;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982670545578003,
                    "sentence": "\"For each experiment\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986518025398254,
                    "sentence": "How much do the outcomes change based on the speed of the infusion?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979224801063538,
                    "sentence": "Section 5;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985836744308472,
                    "sentence": "\"I'm not convinced that this has been proven as there hasn't been a comparison made with the Sohl Dickstein paper.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9981438517570496,
                    "sentence": "\"Nice!\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997932945046397,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046397,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046397,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046397,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999942720343715,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 5.727965628461539e-06,
                        "ai_paraphrased": 0.9999942720343715
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 5.7278656284615695e-06,
                            "ai_paraphrased": 0.9999942720343715
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study introduces a model that transforms noise into model samples by gradually removing noise in a process known as progressive denoising technique The method has some resemblances to generative models based on diffusion but differs from the diffusion framework, in certain aspects; \nIt uses a few denoise procedures which helps to boost its computational efficiency by a lot. \nOf going backwards along a different path as the generative model does in reverse chronological order like a mirror image of itself) the conditional chain for the estimated posterior q(z| x starts directly at z and moves forward in the same direction as the generative model to guide it towards the data by acting as a perturbation around it; this strategy seems to share some similarities, with ladder networks. \nFinding an upper limit, on the log likelihood proves to be complex and challenging. \nThe main concept was fascinating. I was really impressed by how well the visual examples turned out using a short chain of processes. The results of inpainting were particularly outstanding because achieving one shot inpainting is usually not possible, in generative modeling systems. To make the research more interesting and persuasive It would be great to have a log likelihood comparison that doesn't depend on Parzen likelihood methods. \n\nSection 2;   \nTheta 1 is the factor.  \n\"The function theta(t)\" > \"The function theta(t)\"   \n\"We will be doing what we will be using.\"   \nThe method of deducing q(z^(zero)| x ) and conducting inference in sync, with the process resonates with me bringing to mind ladder networks to some extent.   \n  \nSection 2 paragraph 4;   \n  \nSection 4;   \n\"For each experiment\"   \nHow much do the outcomes change based \ton the speed of the infusion?   \nSection 5;   \n\"I'm not convinced that this has been proven as there hasn't been a comparison made with the Sohl Dickstein paper.\"  \n  \n\"Nice!\""
        }
    ],
    "editorDocumentId": null
}