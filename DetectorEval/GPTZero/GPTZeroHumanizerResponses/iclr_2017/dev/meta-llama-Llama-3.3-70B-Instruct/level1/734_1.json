{
    "version": "2025-03-13-base",
    "scanId": "4f236eef-1d31-4c42-af5d-ab5eaf2cb4a8",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999060034751892,
                    "sentence": "The key.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999876856803894,
                    "sentence": "Contributions highlighted in the papers summary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998981356620789,
                    "sentence": "The study introduces an advanced multi view learning approach known as the Variational Canonical Correlation Analysis (VCCA).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998756051063538,
                    "sentence": "This model expands upon the idea of linear Canonical Correlation Analysis (CCA) interpreting it within a variable framework and utilizing deep neural networks (DNNs) for nonlinear observation models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999017715454102,
                    "sentence": "Additionally the authors present a modified version of VCCA called VCCA private that's capable of identifying shared as well as individual variables, across different perspectives.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999129772186279,
                    "sentence": "The research paper showcases how VCCA and VCCA private excel in multi view representation learning assignments such as pairing images with images or speech with articulation and matching images, with text datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998731017112732,
                    "sentence": "Main Factors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999009370803833,
                    "sentence": "After reviewing the feedback given to me about the paper, in consideration I have made the decision to accept it for publication.I have two reasons contributing to this conclusion;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998700022697449,
                    "sentence": "The article addresses a particular.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998764395713806,
                    "sentence": "Thoughtfully considered issue in multi view representation learning and its approach is appropriately situated within existing literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998863339424133,
                    "sentence": "The research paper presents an detailed explanation of the VCCA and VCCA private models while showcasing how effective these models are through various datasets, in the experiments conducted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998837113380432,
                    "sentence": "Points, in favor",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998655319213867,
                    "sentence": "The paper presents an brief overview of the challenges in multi view representation learning and the shortcomings of current methods available.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9788027405738831,
                    "sentence": "The authors argue for the importance of a generative model that can grasp the fundamental patterns in the data and they offer a detailed explanation of the VCCA and VCCA private models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.972578227519989,
                    "sentence": "The results from experiments showcase how well these models perform across datasets such, as image image pairs,speech articulation data and image text combinations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9816482663154602,
                    "sentence": "The findings indicate that VCCA and VCCA private are capable of acquiring distinct data representations and surpassing current approaches in various tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9691777229309082,
                    "sentence": "Further.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9164342880249023,
                    "sentence": "Queries",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9665567874908447,
                    "sentence": "I believe the paper could benefit from an extensive exploration of the following aspects;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9759621620178223,
                    "sentence": "How do the writers intend to expand the VCCA and VCCA private models to handle intricate datasets like ones, with various modalities or sequential information?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9639150500297546,
                    "sentence": "How are the writers intending to integrate background information or limitations into the VCCA and VCCA private models, like domain expertise or real world constraints?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9310520887374878,
                    "sentence": "Could the writers offer explanation on how the learned representations can be understood and applied to practical tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9426127672195435,
                    "sentence": "I have some inquiries that I would appreciate the authors addressing;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9870234727859497,
                    "sentence": "How did the authors decide on the settings, for the VCCA and VCCA private models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9586163759231567,
                    "sentence": "How do these settings affect the outcomes?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9788516759872437,
                    "sentence": "Could the authors offer information regarding how the VCCA and VCCA private models are implemented.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.980499267578125,
                    "sentence": "Like describing the neural network architectures and optimization algorithms utilized?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9933702349662781,
                    "sentence": "\"How do the authors intend to assess the effectiveness of the VCCA and VCCA private models, on intricate datasets and which criteria do they aim to employ for assessment?\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9991781598830806,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9991781598830806,
                "mixed": 0.0008218401169194299
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9991781598830806,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9991781598830806,
                    "human": 0,
                    "mixed": 0.0008218401169194299
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The key. Contributions highlighted in the papers summary.\nThe study introduces an advanced multi view learning approach known as the Variational Canonical Correlation Analysis (VCCA). This model expands upon the idea of linear Canonical Correlation Analysis (CCA) interpreting it within a variable framework and utilizing deep neural networks (DNNs) for nonlinear observation models. Additionally the authors present a modified version of VCCA called VCCA private that's capable of identifying shared as well as individual variables, across different perspectives. The research paper showcases how VCCA and VCCA private excel in multi view representation learning assignments such as pairing images with images or speech with articulation and matching images, with text datasets. \nMain Factors\nAfter reviewing the feedback given to me about the paper, in consideration I have made the decision to accept it for publication.I have two reasons contributing to this conclusion; \nThe article addresses a particular. Thoughtfully considered issue in multi view representation learning and its approach is appropriately situated within existing literature. \nThe research paper presents an detailed explanation of the VCCA and VCCA private models while showcasing how effective these models are through various datasets, in the experiments conducted. \nPoints, in favor \nThe paper presents an brief overview of the challenges in multi view representation learning and the shortcomings of current methods available. The authors argue for the importance of a generative model that can grasp the fundamental patterns in the data and they offer a detailed explanation of the VCCA and VCCA private models. The results from experiments showcase how well these models perform across datasets such, as image image pairs,speech articulation data and image text combinations. The findings indicate that VCCA and VCCA private are capable of acquiring distinct data representations and surpassing current approaches in various tasks. \nFurther. Queries\nI believe the paper could benefit from an extensive exploration of the following aspects; \nHow do the writers intend to expand the VCCA and VCCA private models to handle intricate datasets like ones, with various modalities or sequential information? \nHow are the writers intending to integrate background information or limitations into the VCCA and VCCA private models, like domain expertise or real world constraints? \nCould the writers offer explanation on how the learned representations can be understood and applied to practical tasks? \nI have some inquiries that I would appreciate the authors addressing; \nHow did the authors decide on the settings, for the VCCA and VCCA private models. How do these settings affect the outcomes? \nCould the authors offer information regarding how the VCCA and VCCA private models are implemented. Like describing the neural network architectures and optimization algorithms utilized? \n\"How do the authors intend to assess the effectiveness of the VCCA and VCCA private models, on intricate datasets and which criteria do they aim to employ for assessment?\""
        }
    ],
    "editorDocumentId": null
}