{
    "version": "2025-03-13-base",
    "scanId": "6c212bfd-1bff-4acf-803b-14c98ae30718",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999966621398926,
                    "sentence": "The research paper titled \"Autoencoded Variational Inference For Topic Models\" introduces a method for topic modeling by utilizing autoencoding variational Bayes (AEVB).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999964237213135,
                    "sentence": "The authors introduce an inference technique known as AVITM that allows for the application across different topic models, without the need to develop a new mathematical inference algorithm for each model individually.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999996542930603,
                    "sentence": "The research paper suggests advancing the area of topic modeling by presenting an effective inference technique that maintains a level of accuracy comparable to conventional methods but with decreased computational expenses.The authors also introduce a topic model known as ProdLDA that substitutes the blend model in LDA with a product of experts, for clearer interpretation of topics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999956488609314,
                    "sentence": "After consideration and analysis of the paper in question for two main reasons.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955296516418,
                    "sentence": "Firstly due to its well thought out approach in tackling the obstacles of implementing AEVB in topic models and secondly because the results from experiments effectively showcase the proposed methods success, in enhancing topic coherence and computational efficiency.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999946355819702,
                    "sentence": "I have decided to approve it.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958276748657,
                    "sentence": "The document presents an organized explanation of the suggested technique with detailed mathematical derivations and the setup for experiments outlined within it The authors also delve into a comprehensive examination of previous research and highlight the benefits of their approach compared to other methods in use The results from experiments are compelling as they demonstrate that AVITM attains comparable topic consistency, to conventional methods but with a much quicker training process",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999957084655762,
                    "sentence": "I recommend that the authors enhance the paper by offering insights, into how they designed the inference network structure - like specifying the layers and types of activation functions employed in it.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999954104423523,
                    "sentence": "Moreover it'd be beneficial to showcase visual representations of the topics grasped by varying models to aid in comprehending the outcomes better.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999964833259583,
                    "sentence": "Could you please explain how the authors selected the hyperparameters for the Laplace approximation of the Dirichlet prior and share some insights on why ProdLDA shows topic coherence, than LDA?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999959468841553,
                    "sentence": "The paper makes a valuable addition to the topic modeling field and with a tweaks could become a noteworthy publication, in its own right.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999952935943651,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 4.7064056348984236e-06,
                        "ai_paraphrased": 0.9999952935943651
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 4.706305634898454e-06,
                            "ai_paraphrased": 0.9999952935943651
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The research paper titled \"Autoencoded Variational Inference For Topic Models\" introduces a method for topic modeling by utilizing autoencoding variational Bayes (AEVB). The authors introduce an inference technique known as AVITM that allows for the application across different topic models, without the need to develop a new mathematical inference algorithm for each model individually. \nThe research paper suggests advancing the area of topic modeling by presenting an effective inference technique that maintains a level of accuracy comparable to conventional methods but with decreased computational expenses.The authors also introduce a topic model known as ProdLDA that substitutes the blend model in LDA with a product of experts, for clearer interpretation of topics. \nAfter consideration and analysis of the paper in question for two main reasons. Firstly due to its well thought out approach in tackling the obstacles of implementing AEVB in topic models and secondly because the results from experiments effectively showcase the proposed methods success, in enhancing topic coherence and computational efficiency. I have decided to approve it. \nThe document presents an organized explanation of the suggested technique with detailed mathematical derivations and the setup for experiments outlined within it The authors also delve into a comprehensive examination of previous research and highlight the benefits of their approach compared to other methods in use The results from experiments are compelling as they demonstrate that AVITM attains comparable topic consistency, to conventional methods but with a much quicker training process\nI recommend that the authors enhance the paper by offering insights, into how they designed the inference network structure – like specifying the layers and types of activation functions employed in it. Moreover it'd be beneficial to showcase visual representations of the topics grasped by varying models to aid in comprehending the outcomes better. \nCould you please explain how the authors selected the hyperparameters for the Laplace approximation of the Dirichlet prior and share some insights on why ProdLDA shows topic coherence, than LDA?  \nThe paper makes a valuable addition to the topic modeling field and with a tweaks could become a noteworthy publication, in its own right. "
        }
    ],
    "editorDocumentId": null
}