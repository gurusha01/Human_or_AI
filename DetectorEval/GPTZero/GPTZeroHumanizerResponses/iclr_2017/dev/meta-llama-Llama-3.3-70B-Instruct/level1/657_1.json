{
    "version": "2025-03-13-base",
    "scanId": "72b50805-b7b1-48d2-8ab6-274c304cfaee",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999910593032837,
                    "sentence": "In short",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940991401672,
                    "sentence": "The research paper suggests a way to decrease the memory requirements of text classification models without compromising their precision levels.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999932050704956,
                    "sentence": "Authors enhance the fastText library by implementing product quantization for storing word embeddings which leads to a decrease in memory consumption.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999918341636658,
                    "sentence": "They also introduce a method for trimming down crucial elements to further reduce the models size.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999919533729553,
                    "sentence": "The authors exhibit the efficiency of their technique on standard tests and manage to compress the data by up to 1000 times with only a minimal drop, in accuracy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999805688858032,
                    "sentence": "Choice",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999863505363464,
                    "sentence": "Sure thing!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999901056289673,
                    "sentence": "Here's the revised text; \"I have chosen to approve this paper because the method is well justified and backed by experiments.The authors present an elaborate description of their approach and the findings indicate a notable enhancement, in memory efficiency while maintaining accuracy.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999896883964539,
                    "sentence": "Points, in favor",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999907612800598,
                    "sentence": "The research paper addresses an important issue within natural language processing; how to decrease the memory requirements of text classification models effectively The authors offer a detailed examination of previous studies and justify their method by pointing out the drawbacks of current techniques The suggested approach is clearly outlined and the experimental setup is thorough and carefully planned The findings show a notable decrease in memory consumption without compromising accuracy This makes their approach a valuable addition, to the field",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999839663505554,
                    "sentence": "Extra Input Appreciated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999856352806091,
                    "sentence": "In order to enhance the paper further I recommend that the authors delve deeper into the expenses associated with their approach especially focusing on the time required for training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999892711639404,
                    "sentence": "Additionally it would be valuable to observe a comparison with cutting edge techniques like neural networks evaluating their performance in accuracy and memory utilization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999873042106628,
                    "sentence": "The authors could also explore shedding light on the compromises involved in adjusting various hyperparameters, such as the quantity of subquantizers and the threshold, for pruning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999862313270569,
                    "sentence": "Dear Authors, Some Queries, for You.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999812841415405,
                    "sentence": "Could you please provide some clarity, on the paper by addressing the questions?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999843239784241,
                    "sentence": "How do the writers intend to address words not found in the vocabulary in their suggested approach?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999979555606842,
                    "sentence": "Could the writers offer information regarding how they applied the pruning method in practice when using the online parallelizable greedy strategy?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999784231185913,
                    "sentence": "How do the writers imagine using the suggested approach in other tasks within natural language processing field, like developing language models or translating languages through machines?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.999997305698935,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.69430106495204e-06,
                        "ai_paraphrased": 0.999997305698935
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.6942010649520697e-06,
                            "ai_paraphrased": 0.999997305698935
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "In short\nThe research paper suggests a way to decrease the memory requirements of text classification models without compromising their precision levels. Authors enhance the fastText library by implementing product quantization for storing word embeddings which leads to a decrease in memory consumption. They also introduce a method for trimming down crucial elements to further reduce the models size. The authors exhibit the efficiency of their technique on standard tests and manage to compress the data by up to 1000 times with only a minimal drop, in accuracy. \nChoice\nSure thing! Here’s the revised text; \"I have chosen to approve this paper because the method is well justified and backed by experiments.The authors present an elaborate description of their approach and the findings indicate a notable enhancement, in memory efficiency while maintaining accuracy.\"\nPoints, in favor \nThe research paper addresses an important issue within natural language processing; how to decrease the memory requirements of text classification models effectively The authors offer a detailed examination of previous studies and justify their method by pointing out the drawbacks of current techniques The suggested approach is clearly outlined and the experimental setup is thorough and carefully planned The findings show a notable decrease in memory consumption without compromising accuracy This makes their approach a valuable addition, to the field\nExtra Input Appreciated.\nIn order to enhance the paper further I recommend that the authors delve deeper into the expenses associated with their approach especially focusing on the time required for training. Additionally it would be valuable to observe a comparison with cutting edge techniques like neural networks evaluating their performance in accuracy and memory utilization. The authors could also explore shedding light on the compromises involved in adjusting various hyperparameters, such as the quantity of subquantizers and the threshold, for pruning. \nDear Authors, Some Queries, for You.\nCould you please provide some clarity, on the paper by addressing the questions ?\nHow do the writers intend to address words not found in the vocabulary in their suggested approach? \nCould the writers offer information regarding how they applied the pruning method in practice when using the online parallelizable greedy strategy? \nHow do the writers imagine using the suggested approach in other tasks within natural language processing field, like developing language models or translating languages through machines? "
        }
    ],
    "editorDocumentId": null
}