{
    "version": "2025-03-13-base",
    "scanId": "017e4662-8b7e-4a5c-9f55-866ab941e2ec",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9878330826759338,
                    "sentence": "This document introduces a method based on neural networks, for understanding written text that can forecast responses of different lengths as opposed to current models restricted to individual words or items.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9937641620635986,
                    "sentence": "The suggested technique is tested using the Stanford Question Answering Dataset (SQuAD) and shows enhancements compared to models; however it falls short of the top notch outcomes showcased in the SQuAD leaderboard.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9906355142593384,
                    "sentence": "The main highlight of this technique is its capacity to recognize phrases of lengths as potential solutions; however the two methods used.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.991649866104126,
                    "sentence": "Leveraging a POS pattern trie tree to sift through word sequences according to POS tags and exhaustively listing phrases up to a certain length N. Seem somewhat detached from the idea of \"end to end learning”, for extracting answer chunks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9888133406639099,
                    "sentence": "Furthermore as mentioned by reviewers the language aspects play a vital role in achieving accurate results (refer to Table 3) and these can be readily acquired through conventional taggers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9855250716209412,
                    "sentence": "This also serves to reduce the idea of a system trained from start, to finish.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9865046143531799,
                    "sentence": "The paper is well crafted in general; however some parts that explain the model can be a bit hard to grasp at times.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9913029074668884,
                    "sentence": "In particular the attention mechanism appears to be a setup in a sequence to sequence scenario without any distinctive architectural innovations like those found in models such as the Gated Attentive Reader.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9929523468017578,
                    "sentence": "Even after further explanation provided about this attention mechanism and how it differs from the one used in sequence, to sequence models remains somewhat unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.990381121635437,
                    "sentence": "In the end although the technique surpasses the standard method outlined in the SQuAD dataset publication, its current placement as 12th out of 15 systems on the ranking board sparks worries regarding its competitiveness, against established cutting edge models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.8377984952181409,
            "class_probabilities": {
                "human": 0.1620258834353131,
                "ai": 0.8377984952181409,
                "mixed": 0.00017562134654616467
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8377984952181409,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8377984952181409,
                    "human": 0.1620258834353131,
                    "mixed": 0.00017562134654616467
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999304741848966,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 6.952581510343203e-05,
                        "ai_paraphrased": 0.9999304741848966
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 6.952571510343206e-05,
                            "ai_paraphrased": 0.9999304741848966
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This document introduces a method based on neural networks, for understanding written text that can forecast responses of different lengths as opposed to current models restricted to individual words or items. The suggested technique is tested using the Stanford Question Answering Dataset (SQuAD) and shows enhancements compared to models; however it falls short of the top notch outcomes showcased in the SQuAD leaderboard. \nThe main highlight of this technique is its capacity to recognize phrases of lengths as potential solutions; however the two methods used. Leveraging a POS pattern trie tree to sift through word sequences according to POS tags and exhaustively listing phrases up to a certain length N. Seem somewhat detached from the idea of \"end to end learning”, for extracting answer chunks. Furthermore​ as mentioned by reviewers​ the language aspects play a vital role in achieving accurate results (refer to Table 3)​ and these can be readily acquired through conventional taggers​. This also serves to reduce the idea of a system trained from start, to finish. \nThe paper is well crafted in general; however some parts that explain the model can be a bit hard to grasp at times. In particular the attention mechanism appears to be a setup in a sequence to sequence scenario without any distinctive architectural innovations like those found in models such as the Gated Attentive Reader. Even after further explanation provided about this attention mechanism and how it differs from the one used in sequence, to sequence models remains somewhat unclear. \nIn the end although the technique surpasses the standard method outlined in the SQuAD dataset publication, its current placement as 12th out of 15 systems on the ranking board sparks worries regarding its competitiveness, against established cutting edge models. "
        }
    ],
    "editorDocumentId": null
}