{
    "version": "2025-03-13-base",
    "scanId": "4aa7efac-cc55-493b-95e3-9ff779e6bbaa",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999983310699463,
                    "sentence": "This study suggests methods for shrinking a wide and shallow text sorting model that uses n gram characteristics such as optimizing product quantization to compress embedding weights and removing certain vocabulary components through pruning techniques while also implementing hashing to minimize vocabulary storage spaceᅳthough this is considered a minor factor in the overall process focuses mainly on models with extensive vocabularies and shows a substantial decrease in model size with only a slight impact, on accuracy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986290931702,
                    "sentence": "The topic of compressing models is quite interesting and important to discuss about here!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986886978149,
                    "sentence": "I noticed that the methods section is nicely organized with a mix of explanations and references that make sense in the context of the study being done here..",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999988079071045,
                    "sentence": "In my opinion though and as I see things differently compared to others maybe...",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986886978149,
                    "sentence": "I feel like the contributions made by this paper to the field of machine learning are a bit lacking in some areas maybe?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999987483024597,
                    "sentence": "Especially when looking at the results that focus more towards benchmarks that are not commonly used; they don't seem entirely convincing, to me.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999988079071045,
                    "sentence": "Also not really sure what impact these results have for cutting edge RNN text classification models - still trying to figure that out.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999989867210388,
                    "sentence": "The use of optimized product quantization to estimate products isn't revolutionary since similar methods have been studied before in past research endeavors.The reduction in model size mostly comes from trimming down elements; this technique operates on the belief that embeddings, with higher L2 norms are more significant and includes a coverage heuristic as well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999990463256836,
                    "sentence": "When looking at this matter through the lens of machine learning techniques perspective it might be more fitting to start by setting up a flexible set of binary factors for every embedding vector.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999990463256836,
                    "sentence": "These factors should be learned along, with the weights.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992847442627,
                    "sentence": "Could benefit from adding an L1 regularization method to encourage sparsity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991059303284,
                    "sentence": "Another crucial aspect that seems to be overlooked is examining the results obtained by using a vocabulary specifically those derived from subword components.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999315123648234,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 6.848763517658418e-05,
                        "ai_paraphrased": 0.9999315123648234
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 6.848753517658421e-05,
                            "ai_paraphrased": 0.9999315123648234
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study suggests methods for shrinking a wide and shallow text sorting model that uses n gram characteristics such as optimizing product quantization to compress embedding weights and removing certain vocabulary components through pruning techniques while also implementing hashing to minimize vocabulary storage space—though this is considered a minor factor in the overall process focuses mainly on models with extensive vocabularies and shows a substantial decrease in model size with only a slight impact, on accuracy. \nThe topic of compressing models is quite interesting and important to discuss about here! I noticed that the methods section is nicely organized with a mix of explanations and references that make sense in the context of the study being done here.. In my opinion though and as I see things differently compared to others maybe... I feel like the contributions made by this paper to the field of machine learning are a bit lacking in some areas maybe? Especially when looking at the results that focus more towards benchmarks that are not commonly used; they don't seem entirely convincing, to me. Also not really sure what impact these results have for cutting edge RNN text classification models – still trying to figure that out. \nThe use of optimized product quantization to estimate products isn't revolutionary since similar methods have been studied before in past research endeavors.The reduction in model size mostly comes from trimming down elements; this technique operates on the belief that embeddings, with higher L2 norms are more significant and includes a coverage heuristic as well. When looking at this matter through the lens of machine learning techniques perspective it might be more fitting to start by setting up a flexible set of binary factors for every embedding vector. These factors should be learned along, with the weights. Could benefit from adding an L1 regularization method to encourage sparsity. Another crucial aspect that seems to be overlooked is examining the results obtained by using a vocabulary specifically those derived from subword components. "
        }
    ],
    "editorDocumentId": null
}