{
    "version": "2025-03-13-base",
    "scanId": "49c20a60-61cc-4992-9a98-08b8bebd1e4c",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999856352806091,
                    "sentence": "This study discusses the problem of understanding feature embeddings deeply and addresses the challenge of uneven distribution, within the feature space that makes hard negative mining less effective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999868273735046,
                    "sentence": "To solve this issue effectively with an approach inspired by reference [35] the authors introduce a unique component that considers both the average location of a pair of features and their dissimilarities to calculate similarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999864101409912,
                    "sentence": "By incorporating data into this component design is able to handle the uneven distribution within the feature space efficiently.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999832510948181,
                    "sentence": "Moreover this innovative component is differentiable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999828338623047,
                    "sentence": "Can be smoothly integrated into current CNN structures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999871253967285,
                    "sentence": "Introducing a hinge loss mechanism that considers both similarities and differences in pairs before and after the unit doubling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999876022338867,
                    "sentence": "Utilizing hard quadruplet mining to find pairs with low similarity as positive and those with high similarity as negative, during each SGD minibatch.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999855160713196,
                    "sentence": "The method was tested using image search tasks with the CUB 200 2011 and CARS196 datasets and also in tasks involving transfer and zero shot learning with the ImageNet 10k and ImageNet 2010 datasets.The outcomes show enhancements in all areas of study along with improvements in training efficiency.This research paper tackles an well known issue, in computer vision.The suggested approach seems unique based on what I know.The document is well crafted including sources and the results are convincing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999853372573853,
                    "sentence": "The collaboration, between the PDDM unit and quadruple hard mining appears to offer benefits.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999900460243225,
                    "sentence": "I have a things I'd like to clarify regarding Table 1 in your explanation; Firstly I'm not sure about the difference between the \"PDDM score\" and the \"Quadruplet + PDDM.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999924898147583,
                    "sentence": "Are they related to retrieving data using the PDDM modules output (the former) as opposed to the learned embedding before the PDDM module using distance (the latter)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999909996986389,
                    "sentence": "Also I'm interested, in understanding the significance of embedding loss \\( E_e \\) in Equation (4).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999932050704956,
                    "sentence": "How crucial is it considering that gradients can move through the PDDM modules?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999938607215881,
                    "sentence": "Are the features, for negative mining recalculated for each minibatch or are they stored and updated periodically to save computation time?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                }
            ],
            "completely_generated_prob": 0.9997938739332977,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997938739332977,
                "mixed": 0.00020612606670239523
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997938739332977,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997938739332977,
                    "human": 0,
                    "mixed": 0.00020612606670239523
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999973362767635,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.6637232364691013e-06,
                        "ai_paraphrased": 0.9999973362767635
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.663623236469131e-06,
                            "ai_paraphrased": 0.9999973362767635
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study discusses the problem of understanding feature embeddings deeply and addresses the challenge of uneven distribution, within the feature space that makes hard negative mining less effective. To solve this issue effectively with an approach inspired by reference [35] the authors introduce a unique component that considers both the average location of a pair of features and their dissimilarities to calculate similarity. By incorporating data into this component design is able to handle the uneven distribution within the feature space efficiently. Moreover this innovative component is differentiable. Can be smoothly integrated into current CNN structures. Introducing a hinge loss mechanism that considers both similarities and differences in pairs before and after the unit doubling. Utilizing hard quadruplet mining to find pairs with low similarity as positive and those with high similarity as negative, during each SGD minibatch. \nThe method was tested using image search tasks with the CUB 200 2011 and CARS196 datasets and also in tasks involving transfer and zero shot learning with the ImageNet 10k and ImageNet 2010 datasets.The outcomes show enhancements in all areas of study along with improvements in training efficiency.This research paper tackles an well known issue, in computer vision.The suggested approach seems unique based on what I know.The document is well crafted including sources and the results are convincing. The collaboration, between the PDDM unit and quadruple hard mining appears to offer benefits. \nI have a things I'd like to clarify regarding Table 1 in your explanation; Firstly I'm not sure about the difference between the \"PDDM score\" and the \"Quadruplet + PDDM.\" Are they related to retrieving data using the PDDM modules output (the former) as opposed to the learned embedding before the PDDM module using distance (the latter)? Also I'm interested, in understanding the significance of embedding loss \\( E_e \\) in Equation (4). How crucial is it considering that gradients can move through the PDDM modules? Are the features, for negative mining recalculated for each minibatch or are they stored and updated periodically to save computation time? "
        }
    ]
}