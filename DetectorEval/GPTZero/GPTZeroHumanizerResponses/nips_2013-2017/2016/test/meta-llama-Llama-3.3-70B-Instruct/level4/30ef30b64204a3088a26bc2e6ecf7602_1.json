{
    "version": "2025-03-13-base",
    "scanId": "1fed9980-f75f-4f44-8709-1bd90fe362fe",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999614357948303,
                    "sentence": "This study explores how using data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999604821205139,
                    "sentence": "In particular modified versions of the training dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999684691429138,
                    "sentence": "Can improve the overall performance of convolutional neural networks (CNNs).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999958336353302,
                    "sentence": "It introduces a loss function that can be applied alongside regular supervised training to reduce the fluctuations when a data point goes through the network multiple times due to randomization methods, like dropout and stochastic pooling or different augmentation techniques.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999665021896362,
                    "sentence": "The goal of the loss function is to reduce the variations in predictions between transformed points without relying on label data and ensuring that the values, in the prediction output vector are mutually exclusive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999535083770752,
                    "sentence": "Most of the research involves conducting experiments and comparing performance using two network structures as benchmarks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999532699584961,
                    "sentence": "The findings indicate that incorporating loss consistently boosts performance levelsᅳespecially in scenarios with small datasetsᅳand leads to top notch results on CIFAR10 and CIFAR100 datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999582171440125,
                    "sentence": "This study makes a contribution, to the realm of semi supervised learning through suggesting the integration of an unsupervised loss component to enhance the regularization capabilities of Convolutional Neural Networks (CNNs).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999200701713562,
                    "sentence": "The concept is straightforward in essence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999606013298035,
                    "sentence": "It aims to ensure stability by minimizing the variation, between predictions linked to an input data point.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999411702156067,
                    "sentence": "The paper excels, in detailing its findings extensively when introducing the new loss to traditional supervised CNN models as its key strength However it falls short in providing adequate baseline comparisons and theoretical explanations or discussions as areas needing improvement The unique contribution of the study is seen in implementing an unsupervised loss factor to regulate prediction stability amidst transformations or random variations although the idea of a mutual exclusivity term stems from prior research",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999219179153442,
                    "sentence": "The paper shows technical expertise with thorough testing conducted using established vision datasets with two distinct models and perturbation types examined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999395608901978,
                    "sentence": "It also compares results with another method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999204277992249,
                    "sentence": "Suggests enhancing baseline performance further through supervised training with augmented labeled data using similar transformations as in the unsupervised scenario.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999290704727173,
                    "sentence": "The research holds promise as it advocates for integrating data alongside supervised training to ensure stability through a straightforward loss approach, in predictions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998829960823059,
                    "sentence": "On the hand there are several queries that stem from the analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999910831451416,
                    "sentence": "The suggested loss has not been examined theoretically.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998928308486938,
                    "sentence": "Deliberated within the framework of the fundamental disturbances, which could potentially serve as extra regularization or augmentation of data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999315738677979,
                    "sentence": "Some discourse would be beneficial in elucidating the similarity, between dropout and model averaging.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999517202377319,
                    "sentence": "Moreover there is uncertainty surrounding the effectiveness of the suggested loss function as it relates to expanding in tandem with the quantity of alterations in the increased collection; exploring through research the impact of varying transformations and weighing their effect, on both performance and efficiency could provide valuable insights.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999833703041077,
                    "sentence": "The way the loss is calculated while training isn't very clear and its uncertain if making mini batches with transformed data could lead to bias in gradients or impact convergence outcomes.My concern is that comparing it directly to methods, like Ladder networks might not be entirely fair; instead it might make more sense to look at how much error decreases compared to the supervised network being used as a baseline for all models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999976634979248,
                    "sentence": "When looking at clarity and how the information is presented in the text discussing supervised learning methods in CNNs and stochastic regularization techniques could use some tweaks to bring more focus to these topics.Some suggestions to enhance this section include making the network parameter description more concise and descriptive and possibly combining or condensing sections like Sec 4..",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999061822891235,
                    "sentence": "The experimentation feedback also points out the importance of introducing baseline measures,such as training on the complete augmented labeled set and providing thorough comparisons, with alternative approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997798800468445,
                    "sentence": "There are a small errors and typos in the document that should be addressed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998874068260193,
                    "sentence": "For example missing information or only listing dates in several references and the need to update the ImageNet reference is important to consider As for line 69 specifically it could use some revision for better understanding and minor typos like changing \"more stable generalization\" to \"stability is generalization\" could be fixed The paper as a whole makes a valuable contribution, to the field of semi supervised learning; however it could be enhanced by providing more theoretical justification improving baseline comparisons and presenting more clearly in general",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9997932945046396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046396,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046396,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9996789164211245,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.0003210835788755368,
                        "ai_paraphrased": 0.9996789164211245
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.00032108347887553683,
                            "ai_paraphrased": 0.9996789164211245
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study explores how using data. In particular modified versions of the training dataset. Can improve the overall performance of convolutional neural networks (CNNs). It introduces a loss function that can be applied alongside regular supervised training to reduce the fluctuations when a data point goes through the network multiple times due to randomization methods, like dropout and stochastic pooling or different augmentation techniques. The goal of the loss function is to reduce the variations in predictions between transformed points without relying on label data and ensuring that the values, in the prediction output vector are mutually exclusive.  \nMost of the research involves conducting experiments and comparing performance using two network structures as benchmarks. The findings indicate that incorporating loss consistently boosts performance levels—especially in scenarios with small datasets—and leads to top notch results on CIFAR10 and CIFAR100 datasets. This study makes a contribution, to the realm of semi supervised learning through suggesting the integration of an unsupervised loss component to enhance the regularization capabilities of Convolutional Neural Networks (CNNs). The concept is straightforward in essence. It aims to ensure stability by minimizing the variation, between predictions linked to an input data point. \nThe paper excels, in detailing its findings extensively when introducing the new loss to traditional supervised CNN models as its key strength However it falls short in providing adequate baseline comparisons and theoretical explanations or discussions as areas needing improvement The unique contribution of the study is seen in implementing an unsupervised loss factor to regulate prediction stability amidst transformations or random variations although the idea of a mutual exclusivity term stems from prior research \nThe paper shows technical expertise with thorough testing conducted using established vision datasets with two distinct models and perturbation types examined. It also compares results with another method. Suggests enhancing baseline performance further through supervised training with augmented labeled data using similar transformations as in the unsupervised scenario. The research holds promise as it advocates for integrating data alongside supervised training to ensure stability through a straightforward loss approach, in predictions. \nOn the hand there are several queries that stem from the analysis. The suggested loss has not been examined theoretically. Deliberated within the framework of the fundamental disturbances, which could potentially serve as extra regularization or augmentation of data. Some discourse would be beneficial in elucidating the similarity, between dropout and model averaging. Moreover there is uncertainty surrounding the effectiveness of the suggested loss function as it relates to expanding in tandem with the quantity of alterations in the increased collection; exploring through research the impact of varying transformations and weighing their effect, on both performance and efficiency could provide valuable insights. \nThe way the loss is calculated while training isn't very clear and its uncertain if making mini batches with transformed data could lead to bias in gradients or impact convergence outcomes.My concern is that comparing it directly to methods, like Ladder networks might not be entirely fair; instead it might make more sense to look at how much error decreases compared to the supervised network being used as a baseline for all models. \nWhen looking at clarity and how the information is presented in the text discussing supervised learning methods in CNNs and stochastic regularization techniques could use some tweaks to bring more focus to these topics.Some suggestions to enhance this section include making the network parameter description more concise and descriptive and possibly combining or condensing sections like Sec 4.. The experimentation feedback also points out the importance of introducing baseline measures,such as training on the complete augmented labeled set and providing thorough comparisons, with alternative approaches. \nThere are a small errors and typos in the document that should be addressed. For example missing information or only listing dates in several references and the need to update the ImageNet reference is important to consider As for line 69 specifically it could use some revision for better understanding and minor typos like changing \"more stable generalization\" to \"stability is generalization\" could be fixed The paper as a whole makes a valuable contribution, to the field of semi supervised learning; however it could be enhanced by providing more theoretical justification improving baseline comparisons and presenting more clearly in general"
        }
    ],
    "editorDocumentId": null
}