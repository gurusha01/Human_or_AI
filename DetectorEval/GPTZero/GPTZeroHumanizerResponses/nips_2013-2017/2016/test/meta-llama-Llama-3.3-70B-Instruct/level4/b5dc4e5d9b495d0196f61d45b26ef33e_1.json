{
    "version": "2025-03-13-base",
    "scanId": "08a28b1a-86dd-4d5e-a5d0-37f7adfd2a03",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999979138374329,
                    "sentence": "This analysis delves into strategies, for tackling a group of problem solving techniques focused on minimizing the function $F(x)$ defined as the maximum of $Ax \\cdot u.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "\\Phi(u)+ g(x)$ for $ x \\in \\Omega ̂ { )$ where $ g $ is convex and $\\Omega ̂ { may be closed and convex while $\\Omega ̂ { is closed and bounded whereas the collection of best solutions $\\Omega^*$ lies within a convex and compact subset of $\\Omega _ { } which is also non empty.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999984502792358,
                    "sentence": "The authors believe that it is possible to compute a proximal mapping for $ g $.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999984502792358,
                    "sentence": "This framework is versatile.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986886978149,
                    "sentence": "Can encompass a range of applications such as problems involving minimizing regularized empirical losses, in machine learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998927116394,
                    "sentence": "Expanding on the work of Nesterov by combining a method that offers an approximation with accelerated proximal gradient descent led to achieving $O(\\frac{1}{\\epsilon}) $iterations for an $\\epsilon $ precise solution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999990463256836,
                    "sentence": "This submission presents an approach by incorporating a \"Local Error Bound\" (LEBTM)\" condition on F. The LEB condition guarantees that the distance between any x and an optimal solution decreases at a rate proportional to a constant power of the difference, between F(x ) and the optimal value F(x*).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999988675117493,
                    "sentence": "One major breakthrough involves initiating the process with a smoothing parameter $\\mu$ and then gradually reducing it to achieve a more seamless approximation as the algorithm nears an optimal solution without impacting the rate of convergence significantly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999989867210388,
                    "sentence": "Additionally a variant of the algorithm in dual form is introduced that eliminates the need, for manual adjustment of parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "The entry showcases how well the suggested algorithm works in situations by applying it to certain types of problems and presenting results from experiments in three distinct areas that indicate a much better iteration speed, for small values of $\\epsilon$, when compared to the standard Accelerated Proxima Gradient Descent and a primary primal dual method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973773956299,
                    "sentence": "The theoretical breakdown seems clear and easy to understand as it presents the analysis of this smoothing method for the first time.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999976754188538,
                    "sentence": "The experiments indicate that the method brings about an enhancement particularly for low values of $\\epsilon$.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999980330467224,
                    "sentence": "Yet it is highlighted that the benefit of this new method is most evident at lower $\\epsilon$ prompting consideration on whether merging it with other methods, for an initial rough solution could be advantageous.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999976754188538,
                    "sentence": "In some cases like when dealing with minimizing loss using $ L ₁ $ or $ L ∞ $ norms and non smooth loss functions in the algorithm of the submission achieves about O(log ( ɛ ₀ / ɛ)) iterations which could be a significant advancement over previous methods that needed O ( ₁ / ɛ ) iterations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969601631165,
                    "sentence": "Emphasizing this point more and testing how well the algorithm performs compared to existing methods, across a range of ɛ values would be valuable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973177909851,
                    "sentence": "Questions arise regarding the need for the \"backtracking trick\" when calculating $L_\\mu$ in use and the specifics of timing measurements for the primal dual method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "Does it consider both primal and dual updates?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971389770508,
                    "sentence": "In sum this submission offers a contribution through its analysis and experimental efficiency that could have a meaningful impact, on addressing a range of significant optimization problems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999900237234637,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 9.976276536266956e-06,
                        "ai_paraphrased": 0.9999900237234637
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 9.976176536266986e-06,
                            "ai_paraphrased": 0.9999900237234637
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This analysis delves into strategies, for tackling a group of problem solving techniques focused on minimizing the function $F(x)$ defined as the maximum of $Ax \\cdot u. \\Phi(u)+ g(x)$ for $ x \\in \\Omega ̂ { )$ where $ g $ is convex and $\\Omega ̂ { may be closed and convex while $\\Omega ̂ { is closed and bounded whereas the collection of best solutions $\\Omega^*$ lies within a convex and compact subset of $\\Omega _ { } which is also non empty. The authors believe that it is possible to compute a proximal mapping for $ g $. This framework is versatile. Can encompass a range of applications such as problems involving minimizing regularized empirical losses, in machine learning.  \nExpanding on the work of Nesterov by combining a method that offers an approximation with accelerated proximal gradient descent led to achieving $O(\\frac{1}{\\epsilon}) $iterations for an $\\epsilon $ precise solution. This submission presents an approach by incorporating a \"Local Error Bound\" (LEBTM)\" condition on F. The LEB condition guarantees that the distance between any x and an optimal solution decreases at a rate proportional to a constant power of the difference, between F(x ) and the optimal value F(x*). \nOne major breakthrough involves initiating the process with a smoothing parameter $\\mu$ and then gradually reducing it to achieve a more seamless approximation as the algorithm nears an optimal solution without impacting the rate of convergence significantly. Additionally a variant of the algorithm in dual form is introduced that eliminates the need, for manual adjustment of parameters. The entry showcases how well the suggested algorithm works in situations by applying it to certain types of problems and presenting results from experiments in three distinct areas that indicate a much better iteration speed, for small values of $\\epsilon$, when compared to the standard Accelerated Proxima Gradient Descent and a primary primal dual method. \nThe theoretical breakdown seems clear and easy to understand as it presents the analysis of this smoothing method for the first time. The experiments indicate that the method brings about an enhancement particularly for low values of $\\epsilon$. Yet it is highlighted that the benefit of this new method is most evident at lower $\\epsilon$ prompting consideration on whether merging it with other methods, for an initial rough solution could be advantageous.  \nIn some cases like when dealing with minimizing loss using $ L ₁ $ or $ L ∞ $ norms and non smooth loss functions in the algorithm of the submission achieves about O(log ( ε ₀ / ε)) iterations which could be a significant advancement over previous methods that needed O ( ₁ / ε ) iterations. Emphasizing this point more and testing how well the algorithm performs compared to existing methods, across a range of ε values would be valuable.\nQuestions arise regarding the need for the \"backtracking trick\" when calculating $L_\\mu$ in use and the specifics of timing measurements for the primal dual method. Does it consider both primal and dual updates? In sum this submission offers a contribution through its analysis and experimental efficiency that could have a meaningful impact, on addressing a range of significant optimization problems. "
        }
    ],
    "editorDocumentId": null
}