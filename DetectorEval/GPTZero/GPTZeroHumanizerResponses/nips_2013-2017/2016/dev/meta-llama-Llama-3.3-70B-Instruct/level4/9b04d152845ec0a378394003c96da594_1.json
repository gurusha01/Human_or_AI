{
    "version": "2025-03-13-base",
    "scanId": "ee4a7621-aa31-40b7-9c59-2c927fddb56d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998713135719299,
                    "sentence": "This document combines a network to residual networks with a fusion of linguistic and visual features to address the Visual Question Answering task and has shown significant improvements on well known benchmarks, in the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997625946998596,
                    "sentence": "While the uniqueness of this method is somewhat constrained in scope its adoption could support the progression of studies by leveraging existing cutting edge results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998493194580078,
                    "sentence": "The writers suggest an approach that blends residual like architecture and multimodal fusion for visual question answering by creatively using multiplicative interactions to merge visual features with word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998050332069397,
                    "sentence": "A key innovation in their studys success which outperforms the current leading methods by a notable margin.However due to the use of pre trained models and embeddings a deeper examination is needed to understand their impact, on the overall performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992554187774658,
                    "sentence": "To improve the paper further it would help to include the equations mentioned in the cited papers of just mentioning them in section 3 Moreover there's a point raised in section 5 that ponders what results we'd get by using sigmoid(W_q*q) as the visual mask, for attention.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999193838912664,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 8.061610873360453e-05,
                        "ai_paraphrased": 0.9999193838912664
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 8.061600873360457e-05,
                            "ai_paraphrased": 0.9999193838912664
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This document combines a network to residual networks with a fusion of linguistic and visual features to address the Visual Question Answering task and has shown significant improvements on well known benchmarks, in the field. While the uniqueness of this method is somewhat constrained in scope its adoption could support the progression of studies by leveraging existing cutting edge results. The writers suggest an approach that blends residual like architecture and multimodal fusion for visual question answering by creatively using multiplicative interactions to merge visual features with word embeddings. A key innovation in their studys success which outperforms the current leading methods by a notable margin.However due to the use of pre trained models and embeddings a deeper examination is needed to understand their impact, on the overall performance. To improve the paper further it would help to include the equations mentioned in the cited papers of just mentioning them in section 3 Moreover there's a point raised in section 5 that ponders what results we'd get by using sigmoid(W_q*q) as the visual mask, for attention. "
        }
    ],
    "editorDocumentId": null
}