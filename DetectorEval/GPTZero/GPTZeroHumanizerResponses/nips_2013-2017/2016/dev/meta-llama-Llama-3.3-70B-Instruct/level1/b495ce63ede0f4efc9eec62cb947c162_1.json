{
    "version": "2025-03-13-base",
    "scanId": "20d906a5-3c0b-456b-87ea-31a3121a1928",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999958872795105,
                    "sentence": "This research paper introduces a deep learning model for determining visual correspondence\"\"a key challenge in computer vision tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960660934448,
                    "sentence": "The authors introduce the Universal Correspondence Network (UCS) a convolutional architecture designed to capture either geometric or semantic similarities in a feature space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999946355819702,
                    "sentence": "Of optimizing a surrogate patch similarity objective, like traditional methods do the UCS employs deep metric learning to directly learn a mapping that maintains similarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999944567680359,
                    "sentence": "The writers also suggest a method called a unique comparison contrast loss function and utilize active selection of challenging examples to improve the process of patch normalization by introducing a spatial transformer in the convolutional model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999932050704956,
                    "sentence": "The paper is nicely.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940395355225,
                    "sentence": "Effectively conveys the reasons behind the studys motivations and methodology used in the experiments outlined by the authors.The related work is extensively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999942779541016,
                    "sentence": "Their approachs effectiveness is illustrated through various benchmark datasets like KITTI,PASCAL and CUB 2011.It is evident, from the results that UCN surpasses previous cutting edge techniques in both geometric and semantic correspondence assignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999957084655762,
                    "sentence": "The papers notable aspects are as follows;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995768070221,
                    "sentence": "A new deep learning model has been suggested for estimating correspondences.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955296516418,
                    "sentence": "Utilizing metric learning to explicitly grasp a transformation that maintains similarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999939799308777,
                    "sentence": "Innovatively combining an approach involving contrastive loss and proactive hard negative mining in a novel communication framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999954104423523,
                    "sentence": "Showcasing performance, on various standard datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955892562866,
                    "sentence": "The papers shortcomings are as follows;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998801946640015,
                    "sentence": "The intricacy of the suggested design and the loss function could pose challenges in terms of execution and replication.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999024271965027,
                    "sentence": "The absence of references to deep learning techniques, for estimating visual correspondence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998692870140076,
                    "sentence": "The analysis of the transformers convolutional limited scope and how it influences the outcomes",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997631311416626,
                    "sentence": "Reasons supporting approval;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998445510864258,
                    "sentence": "The article introduces an compelling method, for determining visual correspondences.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998425245285034,
                    "sentence": "The findings showcase performance levels, on various standard datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999626874923706,
                    "sentence": "The article is nicely.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994786381721497,
                    "sentence": "Effectively outlines the approach and tests conducted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995530247688293,
                    "sentence": "Reasons, for not agreeing;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992247819900513,
                    "sentence": "The intricacy of the suggested design and the loss function could pose challenges in terms of execution and replication.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991222620010376,
                    "sentence": "The papers influence could be restricted by not comparing it to deep learning based approaches, for estimating visual correspondence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986878633499146,
                    "sentence": "The minimal examination of the spatial transformer and how it influences the outcomes might prompt inquiries regarding the effectiveness of the method employed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972823262214661,
                    "sentence": "In general I think the paper should be accepted because it introduces an well supported method for estimating visual correspondence and shows excellent performance on various standard datasets.However I believe the authors should work on improving the areas of concern highlighted such as conducting an, in depth analysis of the convolutional spatial transformer and comparing their method with other deep learning approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997932945046397,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046397,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046397,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046397,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999582778378429,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 4.17221621571427e-05,
                        "ai_paraphrased": 0.9999582778378429
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 4.1722062157142726e-05,
                            "ai_paraphrased": 0.9999582778378429
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This research paper introduces a deep learning model for determining visual correspondence\"\"a key challenge in computer vision tasks. The authors introduce the Universal Correspondence Network (UCS) a convolutional architecture designed to capture either geometric or semantic similarities in a feature space. Of optimizing a surrogate patch similarity objective, like traditional methods do the UCS employs deep metric learning to directly learn a mapping that maintains similarity. The writers also suggest a method called a unique comparison contrast loss function and utilize active selection of challenging examples to improve the process of patch normalization by introducing a spatial transformer in the convolutional model. \nThe paper is nicely. Effectively conveys the reasons behind the studys motivations and methodology used in the experiments outlined by the authors.The related work is extensively. Their approachs effectiveness is illustrated through various benchmark datasets like KITTI,PASCAL and CUB 2011.It is evident, from the results that UCN surpasses previous cutting edge techniques in both geometric and semantic correspondence assignments. \nThe papers notable aspects are as follows; \nA new deep learning model has been suggested for estimating correspondences.\nUtilizing metric learning to explicitly grasp a transformation that maintains similarity. \nInnovatively combining an approach involving contrastive loss and proactive hard negative mining in a novel communication framework.\nShowcasing performance, on various standard datasets.\nThe papers shortcomings are as follows; \nThe intricacy of the suggested design and the loss function could pose challenges in terms of execution and replication.\nThe absence of references to deep learning techniques, for estimating visual correspondence.\nThe analysis of the transformers convolutional limited scope and how it influences the outcomes\nReasons supporting approval; \nThe article introduces an compelling method, for determining visual correspondences.\nThe findings showcase performance levels, on various standard datasets. \nThe article is nicely. Effectively outlines the approach and tests conducted.\nReasons, for not agreeing; \nThe intricacy of the suggested design and the loss function could pose challenges in terms of execution and replication.\nThe papers influence could be restricted by not comparing it to deep learning based approaches, for estimating visual correspondence.\nThe minimal examination of the spatial transformer and how it influences the outcomes might prompt inquiries regarding the effectiveness of the method employed. \nIn general I think the paper should be accepted because it introduces an well supported method for estimating visual correspondence and shows excellent performance on various standard datasets.However I believe the authors should work on improving the areas of concern highlighted such as conducting an, in depth analysis of the convolutional spatial transformer and comparing their method with other deep learning approaches. "
        }
    ]
}