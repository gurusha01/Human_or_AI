{
    "version": "2025-03-13-base",
    "scanId": "f95efd82-7fd3-4e77-b523-79987f73f9ab",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999966621398926,
                    "sentence": "This study examines a way to estimate the \"discrepancy string kernel,\" as outlined by Leslie and colleagues (2017).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967217445374,
                    "sentence": "This kernel measures the distance between two strings by counting matching k mers while permitting a number of mismatches (up to m).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958872795105,
                    "sentence": "Due, to the complexity of the algorithms computation requirements being onerous high_ the authors suggest an alternative approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958872795105,
                    "sentence": "The suggested method comprises two parts; ( Ì²a ) figuring out the size of m mismatch areas for two sets of k mers ( Ì²b ) estimating the count of k. Mer pairs falling within a Hamming distance of d.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961853027344,
                    "sentence": "The authors propose a method to solve the issue by using an O(mÂ³) algorithm to precalculate all m mismatch neighbors and tackle the second problem through the use of locality sensitive hashing techniques.Their suggested approach offers assurances, on the solutions quality in a manner and provides analytical estimates on its execution time.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999968409538269,
                    "sentence": "The researchers showed through computer tests on artificial data that their new method effectively estimates the precise calculation and delivers much faster processing times especially for higher values of m.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999944567680359,
                    "sentence": "The paper is nicely.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995768070221,
                    "sentence": "Lays out the suggested algorithm in a clear manner with each step explained thoroughly along, with necessary validations included.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958872795105,
                    "sentence": "Just some small notes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955296516418,
                    "sentence": "According to the authors remarks mentioned on in the text (Theorem 3;13) it seems that the proposed limit is not very precise in its current form.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955892562866,
                    "sentence": "Do you think there's room, for enhancing this limit or is it inherently difficult to do so?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995231628418,
                    "sentence": "There are an incomplete sentences, in Section 4.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999980734817446,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.926518255338293e-06,
                        "ai_paraphrased": 0.9999980734817446
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.926418255338323e-06,
                            "ai_paraphrased": 0.9999980734817446
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study examines a way to estimate the \"discrepancy string kernel,\" as outlined by Leslie and colleagues (2017). This kernel measures the distance between two strings by counting matching k mers while permitting a number of mismatches (up to m). Due, to the complexity of the algorithms computation requirements being onerous high_ the authors suggest an alternative approach. \nThe suggested method comprises two parts; ( Ì²a ) figuring out the size of m mismatch areas for two sets of k mers ( Ì²b ) estimating the count of k. Mer pairs falling within a Hamming distance of d. \nThe authors propose a method to solve the issue by using an O(mÂ³) algorithm to precalculate all m mismatch neighbors and tackle the second problem through the use of locality sensitive hashing techniques.Their suggested approach offers assurances, on the solutions quality in a manner and provides analytical estimates on its execution time. \nThe researchers showed through computer tests on artificial data that their new method effectively estimates the precise calculation and delivers much faster processing times especially for higher values of m. \nThe paper is nicely. Lays out the suggested algorithm in a clear manner with each step explained thoroughly along, with necessary validations included. \nJust some small notes.\nAccording to the authors remarks mentioned on in the text (Theorem 3;13) it seems that the proposed limit is not very precise in its current form. Do you think there's room, for enhancing this limit or is it inherently difficult to do so? \nThere are an incomplete sentences, in Section 4. "
        }
    ]
}