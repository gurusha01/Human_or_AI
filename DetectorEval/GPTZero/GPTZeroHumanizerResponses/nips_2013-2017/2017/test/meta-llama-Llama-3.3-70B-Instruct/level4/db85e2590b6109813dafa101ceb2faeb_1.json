{
    "version": "2025-03-13-base",
    "scanId": "af6dfb0f-f25a-4336-81c5-f5818568b1f8",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999986469745636,
                    "sentence": "The writers suggest a method to encourage weight matrices with rank while training the network to improve compression and allow for explicit rank reduction in subsequent processing stages which ultimately reduces the amount of operations needed for inference purposes.The article seems to be well crafted and delves, into a subject matter.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999796748161316,
                    "sentence": "I do have a concerns about the content provided in the text you shared with me.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999861717224121,
                    "sentence": "Firstly and foremostly the writers seem to overlook inference as a method that effectively compresses networks by reducing description length and can later be employed to trim weights after training as illustrated in 'Practical Variational Inference for Neural Networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999884366989136,
                    "sentence": "Generally nearly any form of regularizer serves as a type of 'compression aware training' implicitly aiding in the creation of simpler models that generalize better and are often suitable, for pruning networks post training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999987483024597,
                    "sentence": "In some cases as an example networks trained with l1 or l1^ll or l₂ l_22 regularization often lead to weights nearing 0 that could be eliminated without significantly affecting performance outcomes It's important to clear up this point especially as the authors incorporate an l₂ l_22 term along with their regularizer, in the training process Additionally there appears to be no comparison made by them on how effective their regularizer is when compared to earlier low rank post processing methods or other regularization approaches seen in previous studiesIt might help to include baseline findings in the experimental section to show how training with this particular regularizer can improve the balance between accuracy and compression better than other methods do.It was a bit tricky for me to fully grasp the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971389770508,
                    "sentence": "I might have missed some details.I would prefer seeing a series of curves that compare accuracy, with compression ratio (based on parameters or MACs) than just looking at accuracy as a function of regularization strength.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977350234985,
                    "sentence": "This chart allows for a comparison, between the new method and the traditional regularizers and compressors used in the past.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                }
            ],
            "completely_generated_prob": 0.9997938739332978,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997938739332978,
                "mixed": 0.00020612606670239523
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997938739332978,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997938739332978,
                    "human": 0,
                    "mixed": 0.00020612606670239523
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999936305611618,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 6.36943883820768e-06,
                        "ai_paraphrased": 0.9999936305611618
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 6.36933883820771e-06,
                            "ai_paraphrased": 0.9999936305611618
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The writers suggest a method to encourage weight matrices with rank while training the network to improve compression and allow for explicit rank reduction in subsequent processing stages which ultimately reduces the amount of operations needed for inference purposes.The article seems to be well crafted and delves, into a subject matter. I do have a concerns about the content provided in the text you shared with me. Firstly and foremostly the writers seem to overlook inference as a method that effectively compresses networks by reducing description length and can later be employed to trim weights after training as illustrated in 'Practical Variational Inference for Neural Networks. Generally nearly any form of regularizer serves as a type of 'compression aware training' implicitly aiding in the creation of simpler models that generalize better and are often suitable, for pruning networks post training. In some cases as an example networks trained with l1 or l​​​​​ℓ^l​l​ or l₂ l_22 ​ regularization often lead to weights nearing 0 that could be eliminated without significantly affecting performance outcomes It's important to clear up this point especially as the authors incorporate an l₂ l_22 ​ term along with their regularizer, in the training process Additionally there appears to be no comparison made by them on how effective their regularizer is when compared to earlier low rank post processing methods or other regularization approaches seen in previous studiesIt might help to include baseline findings in the experimental section to show how training with this particular regularizer can improve the balance between accuracy and compression better than other methods do.It was a bit tricky for me to fully grasp the results. I might have missed some details.I would prefer seeing a series of curves that compare accuracy, with compression ratio (based on parameters or MACs) than just looking at accuracy as a function of regularization strength. This chart allows for a comparison, between the new method and the traditional regularizers and compressors used in the past. "
        }
    ],
    "editorDocumentId": null
}