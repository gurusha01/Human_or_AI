{
    "version": "2025-03-13-base",
    "scanId": "5545895f-ef99-4666-b589-de2f4e970995",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999949932098389,
                    "sentence": "The research paper being examined delves into the topic of bandits with N arms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999943971633911,
                    "sentence": "In each round of the experiment or scenario discussed in the paper review process - a context $x_{ti}$ is presented for each arm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999953508377075,
                    "sentence": "The learner then makes a decision on which arm to choose and subsequently receives a reward $ r_{ti}$ related to that choice.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999939203262329,
                    "sentence": "A central focus of this study revolves around the framework imposed on these rewards.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999947547912598,
                    "sentence": "The authors highlight two options as follows; $ E[r_{ti}] =$ < x_{ti} θ > and $ E[r_{ti}] =$ < x_{ti} θ_i >.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999951720237732,
                    "sentence": "The first option allows for learning with some constraints on capacity; whereas the second option provides greater capacity but with a trade off of slower learning speed.This study aims to strike a balance, between the two by implementing kernelization techniques.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999954104423523,
                    "sentence": "The main concept discussed is about expanding the context area to include $( z_{ti} x_{ti})$, where $ z_{ti}$'s part of another space called Z.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994158744812,
                    "sentence": "A kernel function operating on this area assesses the likeness of contexts and decides how much arms share, in common.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999945759773254,
                    "sentence": "The primary focus of this paper seems to be the idea of expanding the context space in this way and incorporating it into regret analysis using methods.There is a level of originality, in this methodology; however it does not introduce groundbreaking ideas since Valko et al.s analysis could potentially be applied to augmented contexts with assurances.Thus the real innovation lies within the concept of augmentation itself.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999950528144836,
                    "sentence": "It's quite tricky to evaluate the significance of this paper.The theoretical aspect doesn't offer novelty.The practical experiments are decently.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998067021369934,
                    "sentence": "Not particularly impressive especially without comparisons, like linear Thompson sampling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995685815811157,
                    "sentence": "Upon glancing through the material proofs of the paper briefly and superficially assessing its accuracy level seems acceptable to me at first glance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993680119514465,
                    "sentence": "Nevertheless I find the overall impact of the paper to be somewhat questionable or on the edge.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996968507766724,
                    "sentence": "If the authors were to present a compelling case that their theoretical findings go beyond what was discussed in Valkó et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994797110557556,
                    "sentence": "'scited paper it is possible that this could lead to an improvement, in the evaluation score.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996707439422607,
                    "sentence": "Some interesting points to mention are the assumption about $ n_{ a t }= t/N $ at time t that weakens the validity of the deductions made from it.It's unfortunate that there was a lack of analysis, on Algorithm 1 and it might be beneficial to compare the regret of the \"sup \" algorithms to Algorithm 1 in a graph.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997494220733643,
                    "sentence": "Some small ways to make it better are to capitalize variables like $At$ fix grammatical mistakes like changing \"one estimate\" to \"one estimates\" define the domain of $za as $za \\in \\mathcal Z$ explain the meaning of $ta and the use of prime symbols more clearly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995364546775818,
                    "sentence": "Moreover it appears essential to introduce a noise assumption about the rewards even though it was not explicitly mentioned.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993582963943481,
                    "sentence": "Consistency in notation for the augmented context is also important;, for instance using $(A x_A)$ consistently would be preferable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9988893008845894,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9988893008845894,
                "mixed": 0.0011106991154105066
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9988893008845894,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9988893008845894,
                    "human": 0,
                    "mixed": 0.0011106991154105066
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999931370176438,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 6.862982356241102e-06,
                        "ai_paraphrased": 0.9999931370176438
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 6.862882356241132e-06,
                            "ai_paraphrased": 0.9999931370176438
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The research paper being examined delves into the topic of bandits with N arms. In each round of the experiment or scenario discussed in the paper review process – a context $x_{ti}$ is presented for each arm. The learner then makes a decision on which arm to choose and subsequently receives a reward $ r_{ti}$ related to that choice. A central focus of this study revolves around the framework imposed on these rewards. The authors highlight two options as follows; $ E[r_{ti}] =$ < x_{ti} θ > and $ E[r_{ti}] =$ < x_{ti} θ_i >. The first option allows for learning with some constraints on capacity; whereas the second option provides greater capacity but with a trade off of slower learning speed.This study aims to strike a balance, between the two by implementing kernelization techniques. \nThe main concept discussed is about expanding the context area to include $( z_{ti} x_{ti})$, where $ z_{ti}$'s part of another space called Z. A kernel function operating on this area assesses the likeness of contexts and decides how much arms share, in common. \nThe primary focus of this paper seems to be the idea of expanding the context space in this way and incorporating it into regret analysis using methods.There is a level of originality, in this methodology; however it does not introduce groundbreaking ideas since Valko et al.s analysis could potentially be applied to augmented contexts with assurances.Thus the real innovation lies within the concept of augmentation itself. \nIt's quite tricky to evaluate the significance of this paper.The theoretical aspect doesn't offer novelty.The practical experiments are decently. Not particularly impressive especially without comparisons, like linear Thompson sampling. \nUpon glancing through the material proofs of the paper briefly and superficially assessing its accuracy level seems acceptable to me at first glance. Nevertheless I find the overall impact of the paper to be somewhat questionable or on the edge. If the authors were to present a compelling case that their theoretical findings go beyond what was discussed in Valkó et al.'scited paper it is possible that this could lead to an improvement, in the evaluation score. \nSome interesting points to mention are the assumption about $ n_{ a t }= t/N $ at time t that weakens the validity of the deductions made from it.It's unfortunate that there was a lack of analysis, on Algorithm 1 and it might be beneficial to compare the regret of the \"sup \" algorithms to Algorithm 1 in a graph. \nSome small ways to make it better are to capitalize variables like $At$ fix grammatical mistakes like changing \"one estimate\" to \"one estimates\" define the domain of $za as $za \\in \\mathcal Z$ explain the meaning of $ta and the use of prime symbols more clearly. Moreover it appears essential to introduce a noise assumption about the rewards even though it was not explicitly mentioned. Consistency in notation for the augmented context is also important;, for instance using $(A x_A)$ consistently would be preferable. "
        }
    ],
    "editorDocumentId": null
}