{
    "version": "2025-03-13-base",
    "scanId": "fdde16e2-53c7-4918-8b1d-4a6364aed8be",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999919056892395,
                    "sentence": "This document introduces a method for storing familiar contexts without using fixed parameters in order to improve language modeling accuracy.In this approach the main idea is to access the similar past states at each moment and apply a kernel density estimation method to create a probability distribution across an unlimited vocabulary.This distinguishes the cache model from techniques like pointer networks or continuous caches.The outcomes demonstrate the effectiveness of this strategy in language modeling situations marked by changes, in time and subject matter surpassing RNN language models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998758435249329,
                    "sentence": "The document is nicely organized and easy to understand; the idea of a cache is quite fascinating!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999035000801086,
                    "sentence": "The approach suggested could be applied to tasks beyond just language modeling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998846054077148,
                    "sentence": "However comparing it to parametric or local cache methods, like the pointer generator network would have made the paper stronger.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998390674591064,
                    "sentence": "Moreover it would be helpful to evaluate the models efficiency in terms of inference time as computing 1024 neighbors for estimating \\(p_{cache}\\) seems computationally demanding and may affect how practically applicable the model is.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.999997379919768,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.620080231954763e-06,
                        "ai_paraphrased": 0.999997379919768
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.6199802319547928e-06,
                            "ai_paraphrased": 0.999997379919768
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This document introduces a method for storing familiar contexts without using fixed parameters in order to improve language modeling accuracy.In this approach the main idea is to access the similar past states at each moment and apply a kernel density estimation method to create a probability distribution across an unlimited vocabulary.This distinguishes the cache model from techniques like pointer networks or continuous caches.The outcomes demonstrate the effectiveness of this strategy in language modeling situations marked by changes, in time and subject matter surpassing RNN language models. \nThe document is nicely organized and easy to understand; the idea of a cache is quite fascinating! The approach suggested could be applied to tasks beyond just language modeling. \nHowever  comparing it to parametric or local cache methods, like the pointer generator network would have made the paper stronger. Moreover it would be helpful to evaluate the models efficiency in terms of inference time as computing 1024 neighbors for estimating \\(p_{cache}\\) seems computationally demanding and may affect how practically applicable the model is."
        }
    ],
    "editorDocumentId": null
}