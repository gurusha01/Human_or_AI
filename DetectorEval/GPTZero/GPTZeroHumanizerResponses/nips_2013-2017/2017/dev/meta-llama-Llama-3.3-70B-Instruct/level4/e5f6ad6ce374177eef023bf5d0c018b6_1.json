{
    "version": "2025-03-13-base",
    "scanId": "fd27e36b-9666-4077-8a2f-4d2b99ee77fd",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999688863754272,
                    "sentence": "This study introduces a design for creating videos based on convolutional LSTMs with a focus, on conditions that drive the generation process forward.The main idea is that the typical hierarchical structure in which each level represents abstract scene features might not work well for video creation because generative models need to maintain object positions even in the final output layer.The suggested PredRNN model enhances LSTMs by introducing two memory cells one that moves across time within the same layer and another that moves vertically through the layers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999791979789734,
                    "sentence": "The researchers test their model on MNIST digits and KTH action recognition datasets and show better mean squared error (MSQD performance, than previous studies did.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999599456787109,
                    "sentence": "The article is well crafted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999646544456482,
                    "sentence": "Offers a comprehensive examination of pertinent literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999510645866394,
                    "sentence": "The new design is really interesting and different from what we've seen !",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999422430992126,
                    "sentence": "However if we compare it deeply with previous research work it could help us better grasp the real impact of the results achieved here.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99994957447052,
                    "sentence": "For example Kalchrennners study from 2016 showcases perfect performance on the MNIST digit task but strangely isn't used as a basis here for comparison.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999508261680603,
                    "sentence": "Past research on MNIST, like Kalchrennner (2016) and Srivastava (2015) focused on cross entropy loss while the authors of this study emphasized likelihood output instead.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999191164970398,
                    "sentence": "For a thorough comparison with previous research results it would be helpful to include the probability as well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999366998672485,
                    "sentence": "Furthermore although the computational complexity is noted as a benefit, a detailed examination or comparison, with research is missing making it difficult to evaluate the models computational effectiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999427199363708,
                    "sentence": "One small suggestion for notation is to replace \"cell state in equation 3 to clarify the connection with equation 4, with \" M \".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999292492866516,
                    "sentence": "I think the paper is better now with the comparison to work in the works; I expect it to be in the final version too.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999566674232483,
                    "sentence": "Still not sure about using MSE as the training loss in this model though.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999657869338989,
                    "sentence": "Could give an advantage when comparing performance, with other methods that use different losses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9997932945046396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046396,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046396,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999979416182012,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 2.0583817988900185e-06,
                        "ai_paraphrased": 0.9999979416182012
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 2.0582817988900483e-06,
                            "ai_paraphrased": 0.9999979416182012
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This study introduces a design for creating videos based on convolutional LSTMs with a focus, on conditions that drive the generation process forward.The main idea is that the typical hierarchical structure in which each level represents abstract scene features might not work well for video creation because generative models need to maintain object positions even in the final output layer.The suggested PredRNN model enhances LSTMs by introducing two memory cells one that moves across time within the same layer and another that moves vertically through the layers. The researchers test their model on MNIST digits and KTH action recognition datasets and show better mean squared error (MSQD performance, than previous studies did. The article is well crafted. Offers a comprehensive examination of pertinent literature. \nThe new design is really interesting and different from what we've seen ! However if we compare it deeply with previous research work it could help us better grasp the real impact of the results achieved here. For example Kalchrennners study from 2016 showcases perfect performance on the MNIST digit task but strangely isn't used as a basis here for comparison. Past research on MNIST, like Kalchrennner (2016) and Srivastava (2015) focused on cross entropy loss while the authors of this study emphasized likelihood output instead. For a thorough comparison with previous research results it would be helpful to include the probability as well. Furthermore although the computational complexity is noted as a benefit, a detailed examination or comparison, with research is missing making it difficult to evaluate the models computational effectiveness. \nOne small suggestion for notation is to replace \"cell state in equation 3 to clarify the connection with equation 4, with \" M \". \nI think the paper is better now with the comparison to work in the works; I expect it to be in the final version too. Still not sure about using MSE as the training loss in this model though. Could give an advantage when comparing performance, with other methods that use different losses. "
        }
    ],
    "editorDocumentId": null
}