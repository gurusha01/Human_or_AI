{
    "version": "2025-03-13-base",
    "scanId": "d7a470f1-452e-4a3f-ac58-dcdf42ad8b05",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998818635940552,
                    "sentence": "The writers present a method known as variational memory addressing that improves generative models by integrating external memory and hard attention features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998542666435242,
                    "sentence": "A key aspect of this technique is the creation of read and write mechanisms that mirror those seen in probabilistic graphical models instead of the more intricate mechanisms used in models such, as neural Turing machines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998766183853149,
                    "sentence": "In this setup external memory works like a variable in topic models and mixture models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998679161071777,
                    "sentence": "The strict focus selects a \"membership \" then produces variables z and data x based on the memory associated with this membership.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998301267623901,
                    "sentence": "This view of memory in variable models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999900221824646,
                    "sentence": "Where writing is similar, to inference.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998515844345093,
                    "sentence": "Offers valuable understanding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998282194137573,
                    "sentence": "The authors admit that the algorithms ability to scale is restricted by the memorys size as highlighted in lines 175 186 of their work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998614192008972,
                    "sentence": "This restriction can be linked mathematically to the rising disparity in the black box gradients concerning the q(a) parameters as as size increases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998717308044434,
                    "sentence": "While VIMCO may not effectively address this problem the authors have still managed to produce outcomes using external memory capacities of up, to 1024 which is quite impressive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999858163631331,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.4183636866916833e-05,
                        "ai_paraphrased": 0.9999858163631331
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.4183536866916864e-05,
                            "ai_paraphrased": 0.9999858163631331
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The writers present a method known as variational memory addressing that improves generative models by integrating external memory and hard attention features. A key aspect of this technique is the creation of read and write mechanisms that mirror those seen in probabilistic graphical models instead of the more intricate mechanisms used in models such, as neural Turing machines. \nIn this setup external memory works like a variable in topic models and mixture models. The strict focus selects a \"membership \" then produces variables z and data x based on the memory associated with this membership. This view of memory in variable models. Where writing is similar, to inference. Offers valuable understanding. \nThe authors admit that the algorithms ability to scale is restricted by the memorys size as highlighted in lines 175 186 of their work. This restriction can be linked mathematically to the rising disparity in the black box gradients concerning the q(a) parameters as as size increases. While VIMCO may not effectively address this problem the authors have still managed to produce outcomes using external memory capacities of up, to 1024 which is quite impressive. "
        }
    ],
    "editorDocumentId": null
}