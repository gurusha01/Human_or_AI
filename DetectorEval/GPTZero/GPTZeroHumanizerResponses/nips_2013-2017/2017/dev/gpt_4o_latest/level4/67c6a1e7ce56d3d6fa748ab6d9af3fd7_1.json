{
    "version": "2025-03-13-base",
    "scanId": "de009aa7-05a3-42e6-9cc0-cff6a9230dc4",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998200535774231,
                    "sentence": "This research paper presents a model focused on recognizing actions and interactions between humans and objects using attention mechanisms.The model can function with or, without guidance and shows enhanced accuracy without significantly increasing network complexity or computational demands.The authors conducted thorough analytical assessments of the attention module and suggested a novel approach involving bottom up and top down attention as simplified versions of bilinear pooling techniques commonly seen in detailed classification assignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999616801738739,
                    "sentence": "The proposed method has been proven effective through experiments conducted on three datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999726414680481,
                    "sentence": "One impressive aspect of this project is its capability to comprehend attention maps autonomously.The attention maps provide clarity as they showcase the networks points and utilize a combination of both foundational salience and higher level attention operations.This removes the necessity for bounding box identification that's usually essential, in rigorous attention techniques.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993095397949219,
                    "sentence": "In general this study is quite intriguing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990623593330383,
                    "sentence": "Well thought out The paper is nicely written and the ideas are presented clearly Yet there is one significant worry that lingers",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989757537841797,
                    "sentence": "In second order pooling technique involves approximating the connected weight matrix by multiplying two rank one vectors together What are the implications of this approximation on retaining information compared to the original matrix Is there a decrease, in information quality that affects the attention maps Further theoretical examination and deeper exploration of this aspect would enhance the papers content greatly",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9946538805961609,
                    "sentence": "There are some problems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9567899703979492,
                    "sentence": "On page 3 the network design includes this attention module.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8410162329673767,
                    "sentence": "Investigates a pose.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8857147097587585,
                    "sentence": "On page 5 there is a stack of modules.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5724881887435913,
                    "sentence": "Page 8 asks, \"Was the image resized to 450px at input time?\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.9806051202482545,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9806051202482545,
                "mixed": 0.01939487975174554
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9806051202482545,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9806051202482545,
                    "human": 0,
                    "mixed": 0.01939487975174554
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9963543954094697,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.0036456045905302434,
                        "ai_paraphrased": 0.9963543954094697
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.0036456044905302434,
                            "ai_paraphrased": 0.9963543954094697
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This research paper presents a model focused on recognizing actions and interactions between humans and objects using attention mechanisms.The model can function with or, without guidance and shows enhanced accuracy without significantly increasing network complexity or computational demands.The authors conducted thorough analytical assessments of the attention module and suggested a novel approach involving bottom up and top down attention as simplified versions of bilinear pooling techniques commonly seen in detailed classification assignments. The proposed method has been proven effective through experiments conducted on three datasets. \nOne impressive aspect of this project is its capability to comprehend attention maps autonomously.The attention maps provide clarity as they showcase the networks points and utilize a combination of both foundational salience and higher level attention operations.This removes the necessity for bounding box identification that's usually essential, in rigorous attention techniques. \nIn general this study is quite intriguing. Well thought out The paper is nicely written and the ideas are presented clearly Yet there is one significant worry that lingers\nIn second order pooling technique involves approximating the connected weight matrix by multiplying two rank one vectors together What are the implications of this approximation on retaining information compared to the original matrix Is there a decrease, in information quality that affects the attention maps Further theoretical examination and deeper exploration of this aspect would enhance the papers content greatly\nThere are some problems. \nOn page 3 the network design includes this attention module. Investigates a pose. \nOn page 5 there is a stack of modules. \nPage 8 asks, \"Was the image resized to 450px at input time?\""
        }
    ]
}