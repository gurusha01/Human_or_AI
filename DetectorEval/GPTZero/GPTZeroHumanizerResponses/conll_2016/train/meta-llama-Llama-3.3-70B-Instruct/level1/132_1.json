{
    "version": "2025-03-13-base",
    "scanId": "0f911c40-4f8e-438b-b084-9be572855ea3",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999924898147583,
                    "sentence": "This article introduces lda3vec.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999912977218628,
                    "sentence": "A model that merges the benefits of word embeddings and topic models, for creating document representations that can be easily understood.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999892115592957,
                    "sentence": "The key highlights of this research include;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999895095825195,
                    "sentence": "Simultaneous learning of word vectors and document vectors in lda4vec enables the exploration of semantic connections, between words and documents.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999868869781494,
                    "sentence": "The model employs a Dirichlet likelihood term to promote document to topic proportions, for more easily understandable learned representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999875426292419,
                    "sentence": "Implementating ldavec in differentiation frameworks like Chainer is straightforward and user friendly, for researchers and practitioners alike.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999990701675415,
                    "sentence": "One of the advantages of this paper is its strengths.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999901056289673,
                    "sentence": "The lda vec model effectively combines word and document representations by showcasing its capability to comprehend topics and depict semantic connections among wordsᅳa promising method, for analyzing text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999877214431763,
                    "sentence": "Impressive outcomes regarding topic coherence have been achieved by the model as shown in the Twenty Newsgroups corpus analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999904036521912,
                    "sentence": "Suggesting that the topics learned are both significant and easily understood by users.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999910593032837,
                    "sentence": "Flexibility and simplicity in application are strengths of lda·vec; it can be tailored to various datasets and purposes with ease thanks to its integration, into automatic differentiation frameworks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999886155128479,
                    "sentence": "The papers limitations are as follows;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990535974502563,
                    "sentence": "Limited assessment has been done using one dataset; even though the findings from the Twenty Newsgroups corpus show potential promise for lda^vec performance evaluation could be enhanced by testing it across various datasets to showcase its versatility and applicability, in different scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994105100631714,
                    "sentence": "The paper would be more informative with a comparison, to other methods of topic modeling like Latent Dirichlet Allocation ( LDA ) and Non Negative Matrix Factorization (NMF).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991199374198914,
                    "sentence": "scalability to handle large datasets is not entirely certain, with lda4vec despite its efficiency design; further testing is required to prove its ability to scale effectively with extensive datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992755651473999,
                    "sentence": "Questions, for writers;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991382956504822,
                    "sentence": "How do the writers intend to handle the scalability of lda3vec for datasets and what enhancements can be implemented to enhance its efficiency?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993533492088318,
                    "sentence": "Can the writers offer a thorough comparison, between lda3vec and other methods of topic modeling like LDA and NMF to showcase the advantages and limitations of lda3vec?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992249608039856,
                    "sentence": "How do the writers intend to expand lda vec for text analysis functions, like categorizing text and gathering information and what changes would be needed to adjust the model for these tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997932945046397,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046397,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046397,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046397,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9998840753398756,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.00011592466012435637,
                        "ai_paraphrased": 0.9998840753398756
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.0001159245601243564,
                            "ai_paraphrased": 0.9998840753398756
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This article introduces lda3vec. A model that merges the benefits of word embeddings and topic models, for creating document representations that can be easily understood. The key highlights of this research include; \nSimultaneous learning of word vectors and document vectors in lda4vec enables the exploration of semantic connections, between words and documents. \nThe model employs a Dirichlet likelihood term to promote document to topic proportions, for more easily understandable learned representations. \nImplementating ldavec in differentiation frameworks like Chainer is straightforward and user friendly, for researchers and practitioners alike. \nOne of the advantages of this paper is its strengths.\nThe lda vec model effectively combines word and document representations by showcasing its capability to comprehend topics and depict semantic connections among words—a promising method, for analyzing text. \nImpressive outcomes regarding topic coherence have been achieved by the model as shown in the Twenty Newsgroups corpus analysis. Suggesting that the topics learned are both significant and easily understood by users. \nFlexibility and simplicity in application are strengths of lda•vec; it can be tailored to various datasets and purposes with ease thanks to its integration, into automatic differentiation frameworks. \nThe papers limitations are as follows; \nLimited assessment has been done using one dataset; even though the findings from the Twenty Newsgroups corpus show potential promise for lda^vec performance evaluation could be enhanced by testing it across various datasets to showcase its versatility and applicability, in different scenarios. \nThe paper would be more informative with a comparison, to other methods of topic modeling like Latent Dirichlet Allocation ( LDA ) and Non Negative Matrix Factorization (NMF).\nscalability to handle large datasets is not entirely certain, with lda4vec despite its efficiency design; further testing is required to prove its ability to scale effectively with extensive datasets. \nQuestions, for writers; \nHow do the writers intend to handle the scalability of lda3vec for datasets and what enhancements can be implemented to enhance its efficiency? \nCan the writers offer a thorough comparison, between lda3vec and other methods of topic modeling like LDA and NMF to showcase the advantages and limitations of lda3vec? \nHow do the writers intend to expand lda vec for text analysis functions, like categorizing text and gathering information and what changes would be needed to adjust the model for these tasks? "
        }
    ],
    "editorDocumentId": null
}