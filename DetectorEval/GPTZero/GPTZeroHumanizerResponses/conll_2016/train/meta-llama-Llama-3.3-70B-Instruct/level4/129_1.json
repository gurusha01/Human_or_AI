{
    "version": "2025-03-13-base",
    "scanId": "25e85432-eea5-4e0d-8c0c-729315b47748",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9974064230918884,
                    "sentence": "The article introduces a method for choosing training data for machine translation (MT) using a convolutional neural network (CNN) classifier to assess and prioritize general domain sentences instead of continuous or ngram language models as done in previous studies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997955858707428,
                    "sentence": "It is comprehensive in comparing with existing research on bilingual data selection techniques, like the sum of difference of cross entropies although it is not clear if it evaluates against those methods well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980180859565735,
                    "sentence": "At glance it wasn't clear why opting for a CNN instead of an RNN or LSTM was preferred in the paper but the argument put forth effectively highlights that specific parts of a text or sentence hold more significance which a CNN can capture well..",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987671971321106,
                    "sentence": "The paper lacks empirical evidence to ascertain the superiority, between a bag of word (BoW) sequential (SEQ) or a blend of both representations and their respective importance reasons.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985510110855103,
                    "sentence": "The explanation of the CNN is well written and thorough regarding the utilization of one encoding or pre trained embeddings, with limited supervision; it effectively emphasizes the crucial components of the models operations.However a graphical depiction of the layers demonstrating how inputs are merged would greatly enhance comprehension.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987938404083252,
                    "sentence": "The paper is nicely written in general; however there are some citations in brackets that seem unnecessary and the usage of \\citet versus \\citep is inconsistent (for example, at line 385).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986658692359924,
                    "sentence": "The research and assessment up the assertions in the paper; however there are some reservations about how the quantity of selected sentences from the same domain (line 443) determined using a distinct validation set is identified.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999946355819702,
                    "sentence": "It's not clear what kind of validation information is utilized or how the hyperparameters for the CNN models are selected.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999942183494568,
                    "sentence": "The impact of these hyperparameters, on the models isn't addressed either.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999939203262329,
                    "sentence": "Table 2 ought to display the performance of methods using an equal number of selected sentences for comparison purposes; the method outlined in the paper continues to excel over the baseline approaches, in this particular case study as evidenced by Figure 1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940395355225,
                    "sentence": "Additional remarks also consist of;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995231628418,
                    "sentence": "An experiment that compares the method described in the paper to baseline methods using additional data specific, to the field could be intriguing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999952912330627,
                    "sentence": "The results or discussion section would be enhanced by incorporating sample sentences chosen using methods to substantiate the points put forth in section 5 e four.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958276748657,
                    "sentence": "In section 5 point 4 discussion about moving from surface details and considering Axelrods study in 2015 that substitutes words with part of speech tags to overcome language model sparsity could shed light on the potential benefits of, word2vec embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955892562866,
                    "sentence": "The method of combining the scores for source and target classification resembles the approach used in the Lewis Moore language model data selection technique that computes the sum of cross entropy differences of absolute values as noted at, around line 435 in prior research references.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961256980896,
                    "sentence": "It might be interesting to consider if we can train the CNN model in an parallel setup to learn weights, for combining the source and target classification scores together.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9999999999999999,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9999999999999999,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9999999999999999,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9999999999999999,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999853221390508,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 1.4677860949284555e-05,
                        "ai_paraphrased": 0.9999853221390508
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 1.4677760949284585e-05,
                            "ai_paraphrased": 0.9999853221390508
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "The article introduces a method for choosing training data for machine translation (MT) using a convolutional neural network (CNN) classifier to assess and prioritize general domain sentences instead of continuous or ngram language models as done in previous studies. It is comprehensive in comparing with existing research on bilingual data selection techniques, like the sum of difference of cross entropies although it is not clear if it evaluates against those methods well. \nAt glance​ it wasn't clear why opting for a CNN instead of an RNN or LSTM was preferred​ in the paper​ but the argument put forth effectively highlights that specific parts of a text or sentence hold more significance which a CNN can capture well​.. The paper lacks empirical evidence to ascertain the superiority, between a bag of word (BoW) sequential (SEQ) or a blend of both representations and their respective importance reasons. \nThe explanation of the CNN is well written and thorough regarding the utilization of one encoding or pre trained embeddings, with limited supervision; it effectively emphasizes the crucial components of the models operations.However a graphical depiction of the layers demonstrating how inputs are merged would greatly enhance comprehension. \nThe paper is nicely written in general; however there are some citations in brackets that seem unnecessary and the usage of \\citet versus \\citep is inconsistent (for example, at line 385).\nThe research and assessment up the assertions in the paper; however there are some reservations about how the quantity of selected sentences from the same domain (line 443) determined using a distinct validation set is identified. It's not clear what kind of validation information is utilized or how the hyperparameters for the CNN models are selected. The impact of these hyperparameters, on the models isn't addressed either. \nTable 2 ought to display the performance of methods using an equal number of selected sentences for comparison purposes; the method outlined in the paper continues to excel over the baseline approaches, in this particular case study as evidenced by Figure 1. \nAdditional remarks also consist of; \nAn experiment that compares the method described in the paper to baseline methods using additional data specific, to the field could be intriguing. \nThe results or discussion section would be enhanced by incorporating sample sentences chosen using methods to substantiate the points put forth in section 5 e four. \nIn section 5 point 4 discussion about moving from surface details and considering Axelrods study in 2015 that substitutes words with part of speech tags to overcome language model sparsity could shed light on the potential benefits of, word2vec embeddings. \nThe method of combining the scores for source and target classification resembles the approach used in the Lewis Moore language model data selection technique that computes the sum of cross entropy differences of absolute values as noted at, around line 435 in prior research references. \nIt might be interesting to consider if we can train the CNN model in an parallel setup to learn weights, for combining the source and target classification scores together. "
        }
    ],
    "editorDocumentId": null
}