{
    "version": "2025-03-13-base",
    "scanId": "313badb1-d9c8-472b-be2e-06f2a970d9c5",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999923706054688,
                    "sentence": "This article presents a method for evaluating the quality of topics using word embeddings to measure similarity either directly or through matrix factorization techniques The approach shows impressive results, on common datasets and achieves significant outcomes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999856948852539,
                    "sentence": "The suggested method marks an meaningful advancement in the research path of evaluating topics.However a major issue with the findings is the irregularities noted among datasets.Even though top notch performance is attained for all three datasets the approaches differ significantly in efficiency some falling short of the cutting edge technology.Notably no proposed techniques consistently excel compared with the methods, across all datasets and the SVD based techniques fare especially poorly on the genomics dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999901652336121,
                    "sentence": "Practitioners considering using this method on datasets may be worried about the inconsistency observed in its performance across various scenarios and datasets like genomics data where Singular Value Decomposition (SVD) seems less effective due to the presence of out of vocabulary (OOV) terms (more details below).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999989926815033,
                    "sentence": "A potential approach could involve predicting the suitable method by assessing vocabulary alignment with GloVe embeddings or comparable measures; however this aspect remains unexplored, in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999889135360718,
                    "sentence": "Other factors to consider;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999921917915344,
                    "sentence": "The suggested approach shares similarities, with the methods mentioned in the lexical chaining research field and it would be beneficial for the authors to delve into this area of study and integrate pertinent findings into upcoming versions of the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997236132621765,
                    "sentence": "The authors highlight that their approach doesn't rely on parameters; however one should recognize that the word embedding techniques used actually incorporate a significant amount of parameters by nature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997389316558838,
                    "sentence": "While this doesn't pose a concern it's important to acknowledge this fact explicitly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995125532150269,
                    "sentence": "The paper doesn't explain what happens with OOV terms in the method when dealing with the genomics dataset (terms that're n't in the pretrained GloVe embeddings).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995226860046387,
                    "sentence": "Are these terms just left out.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996060729026794,
                    "sentence": "How does this affect the methods performance?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995661377906799,
                    "sentence": "Here are some small concerns.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999175667762756,
                    "sentence": "In Section 2 of the documents chapter discussing word embeddings analysis relies heavily on cosine similarity to gauge vector similarity without considering vector length significance upfrontᅳthough suitable for unit vectors but word2vec may not always yield unit vectors (pretrained vectors are adjusted post training to be unit length but self generated ones can differ).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999387860298157,
                    "sentence": "This may seem trivial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999294877052307,
                    "sentence": "Holds key significance, in understanding the concept thoroughly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999890923500061,
                    "sentence": "The charts, in Figure 1 are difficult to read because they are too small.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997932945046398,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997932945046398,
                "mixed": 0.00020670549536025372
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997932945046398,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997932945046398,
                    "human": 0,
                    "mixed": 0.00020670549536025372
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.7,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.9126934656800181
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9999920801693328,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 7.919830667059942e-06,
                        "ai_paraphrased": 0.9999920801693328
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 7.919730667059973e-06,
                            "ai_paraphrased": 0.9999920801693328
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.7,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-03-13-base",
            "language": "en",
            "pageNumber": 0,
            "inputText": "This article presents a method for evaluating the quality of topics using word embeddings to measure similarity either directly or through matrix factorization techniques The approach shows impressive results, on common datasets and achieves significant outcomes. \nThe suggested method marks an meaningful advancement in the research path of evaluating topics.However a major issue with the findings is the irregularities noted among datasets.Even though top notch performance is attained for all three datasets the approaches differ significantly in efficiency some falling short of the cutting edge technology.Notably no proposed techniques consistently excel compared with the methods, across all datasets and the SVD based techniques fare especially poorly on the genomics dataset. Practitioners considering using this method on datasets may be worried about the inconsistency observed in its performance across various scenarios and datasets like genomics data where Singular Value Decomposition (SVD) seems less effective due to the presence of out of vocabulary (OOV) terms (more details below). A potential approach could involve predicting the suitable method by assessing vocabulary alignment with GloVe embeddings or comparable measures; however this aspect remains unexplored, in the paper. \nOther factors to consider; \nThe suggested approach shares similarities, with the methods mentioned in the lexical chaining research field and it would be beneficial for the authors to delve into this area of study and integrate pertinent findings into upcoming versions of the paper. \nThe authors highlight that their approach doesn't rely on parameters; however one should recognize that the word embedding techniques used actually incorporate a significant amount of parameters by nature. While this doesn't pose a concern it's important to acknowledge this fact explicitly. \nThe paper doesn't explain what happens with OOV terms in the method when dealing with the genomics dataset (terms that're n't in the pretrained GloVe embeddings). Are these terms just left out. How does this affect the methods performance? \nHere are some small concerns.\nIn Section 2 of the documents chapter discussing word embeddings analysis relies heavily on cosine similarity to gauge vector similarity without considering vector length significance upfront—though suitable for unit vectors but word2vec may not always yield unit vectors (pretrained vectors are adjusted post training to be unit length but self generated ones can differ). This may seem trivial. Holds key significance, in understanding the concept thoroughly. \nThe charts, in Figure 1 are difficult to read because they are too small. "
        }
    ],
    "editorDocumentId": null
}