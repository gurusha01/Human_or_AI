{
    "version": "2025-01-09-base",
    "scanId": "7767e5b6-18a5-4265-a843-cc62fbf2245a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999946355819702,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991059303284,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999980330467224,
                    "sentence": "This paper addresses the challenge of part-of-speech (POS) tagging in low-resource languages by proposing a novel neural network model that explicitly accounts for noise in cross-lingual projected annotations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971985816956,
                    "sentence": "The authors introduce a bidirectional Long Short-Term Memory (BiLSTM) network with a noise layer that models the transformation between clean gold-standard tags and noisy projected tags.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999949932098389,
                    "sentence": "The model is trained jointly on a small corpus of gold-standard annotations and a larger corpus of noisy projected annotations, enabling it to learn when and how to trust the noisy data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999953508377075,
                    "sentence": "The paper demonstrates state-of-the-art performance on eight simulated low-resource languages and two real low-resource languages, Malagasy and Kinyarwanda.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999984860420227,
                    "sentence": "The primary contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999834895133972,
                    "sentence": "1. The introduction of a noise layer to explicitly model and mitigate the noise in cross-lingual projected annotations, which is a novel approach in the context of sequence tagging.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999979555606842,
                    "sentence": "2. The integration of this noise-aware model into a deep neural network framework, enabling joint training on both annotated and projected data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999870657920837,
                    "sentence": "3. Empirical validation of the approach, showing consistent improvements over prior methods across both simulated and real-world low-resource languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955296516418,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948143959045,
                    "sentence": "1. Novelty and Technical Contribution: The explicit modeling of noise in projected annotations is a significant contribution, addressing a well-known limitation of cross-lingual projection methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999953508377075,
                    "sentence": "The use of a noise layer is innovative and well-motivated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955296516418,
                    "sentence": "2. Empirical Results: The model achieves state-of-the-art results on both simulated and real low-resource languages, demonstrating its effectiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999966025352478,
                    "sentence": "The results are robust across multiple languages and datasets, providing strong evidence of the model's generalizability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994158744812,
                    "sentence": "3. Practical Relevance: The approach is highly relevant for low-resource language NLP, where annotated data is scarce but parallel corpora are more readily available.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999949932098389,
                    "sentence": "The method's reliance on minimal gold-standard data (e.g., 1000 tokens) makes it practical for real-world applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999956488609314,
                    "sentence": "4. Clarity of Presentation: The paper is well-written and provides a clear explanation of the problem, methodology, and experimental setup.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999900460243225,
                    "sentence": "The inclusion of noise transformation matrices offers valuable insights into the model's behavior.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999902844429016,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999988317489624,
                    "sentence": "1. Artificial Cost Assumptions: One concern raised by another reviewer is the artificial setup regarding the costs of finding versus paying annotators for low-resource languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9814651012420654,
                    "sentence": "While this does not detract from the technical contribution, it would be helpful for the authors to clarify these assumptions in the writeup.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9714235663414001,
                    "sentence": "2. Related Work Coverage: The paper does not discuss some relevant prior work on annotation projection for low-resource languages, such as Johannsen et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9782679677009583,
                    "sentence": "(2016) and Agi√Ñ et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9386381506919861,
                    "sentence": "(2015).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9545456767082214,
                    "sentence": "Including these references would enhance the completeness of the related work section.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9851773381233215,
                    "sentence": "3. Data Availability: It is unclear whether the data used in the experiments is publicly available.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9822909832000732,
                    "sentence": "While the reviewer assumes it is, this should be explicitly clarified in the paper to ensure reproducibility.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9745693802833557,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.975354790687561,
                    "sentence": "1. Can you clarify the assumptions made regarding the costs of finding versus paying annotators for low-resource languages?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9880395531654358,
                    "sentence": "How do these assumptions impact the generalizability of your approach?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9804093837738037,
                    "sentence": "2. Is the data used in your experiments (e.g., annotated datasets for Malagasy and Kinyarwanda, parallel corpora) publicly available?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9728960990905762,
                    "sentence": "If not, do you plan to release it to facilitate reproducibility?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9678705930709839,
                    "sentence": "Recommendation",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9688087701797485,
                    "sentence": "I consider this paper a valuable contribution to the field of low-resource language processing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9822522401809692,
                    "sentence": "The novel noise-aware approach and the strong empirical results justify its acceptance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9589437246322632,
                    "sentence": "Addressing the concerns about cost assumptions, related work, and data availability would further strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9667596817016602,
                    "sentence": "I vote to accept this submission.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                }
            ],
            "completely_generated_prob": 0.7995711047355243,
            "class_probabilities": {
                "human": 0.19427518060025914,
                "ai": 0.7995711047355243,
                "mixed": 0.006153714664216576
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.7995711047355243,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.7995711047355243,
                    "human": 0.19427518060025914,
                    "mixed": 0.006153714664216576
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary and Contributions\nThis paper addresses the challenge of part-of-speech (POS) tagging in low-resource languages by proposing a novel neural network model that explicitly accounts for noise in cross-lingual projected annotations. The authors introduce a bidirectional Long Short-Term Memory (BiLSTM) network with a noise layer that models the transformation between clean gold-standard tags and noisy projected tags. The model is trained jointly on a small corpus of gold-standard annotations and a larger corpus of noisy projected annotations, enabling it to learn when and how to trust the noisy data. The paper demonstrates state-of-the-art performance on eight simulated low-resource languages and two real low-resource languages, Malagasy and Kinyarwanda.\nThe primary contributions of this work are:\n1. The introduction of a noise layer to explicitly model and mitigate the noise in cross-lingual projected annotations, which is a novel approach in the context of sequence tagging.\n2. The integration of this noise-aware model into a deep neural network framework, enabling joint training on both annotated and projected data.\n3. Empirical validation of the approach, showing consistent improvements over prior methods across both simulated and real-world low-resource languages.\nStrengths\n1. Novelty and Technical Contribution: The explicit modeling of noise in projected annotations is a significant contribution, addressing a well-known limitation of cross-lingual projection methods. The use of a noise layer is innovative and well-motivated.\n2. Empirical Results: The model achieves state-of-the-art results on both simulated and real low-resource languages, demonstrating its effectiveness. The results are robust across multiple languages and datasets, providing strong evidence of the model's generalizability.\n3. Practical Relevance: The approach is highly relevant for low-resource language NLP, where annotated data is scarce but parallel corpora are more readily available. The method's reliance on minimal gold-standard data (e.g., 1000 tokens) makes it practical for real-world applications.\n4. Clarity of Presentation: The paper is well-written and provides a clear explanation of the problem, methodology, and experimental setup. The inclusion of noise transformation matrices offers valuable insights into the model's behavior.\nWeaknesses\n1. Artificial Cost Assumptions: One concern raised by another reviewer is the artificial setup regarding the costs of finding versus paying annotators for low-resource languages. While this does not detract from the technical contribution, it would be helpful for the authors to clarify these assumptions in the writeup.\n2. Related Work Coverage: The paper does not discuss some relevant prior work on annotation projection for low-resource languages, such as Johannsen et al. (2016) and Agi√Ñ et al. (2015). Including these references would enhance the completeness of the related work section.\n3. Data Availability: It is unclear whether the data used in the experiments is publicly available. While the reviewer assumes it is, this should be explicitly clarified in the paper to ensure reproducibility.\nQuestions to Authors\n1. Can you clarify the assumptions made regarding the costs of finding versus paying annotators for low-resource languages? How do these assumptions impact the generalizability of your approach?\n2. Is the data used in your experiments (e.g., annotated datasets for Malagasy and Kinyarwanda, parallel corpora) publicly available? If not, do you plan to release it to facilitate reproducibility?\nRecommendation\nI consider this paper a valuable contribution to the field of low-resource language processing. The novel noise-aware approach and the strong empirical results justify its acceptance. Addressing the concerns about cost assumptions, related work, and data availability would further strengthen the paper. I vote to accept this submission."
        }
    ]
}