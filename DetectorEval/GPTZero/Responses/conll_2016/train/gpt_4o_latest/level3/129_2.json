{
    "version": "2025-01-09-base",
    "scanId": "29d9e571-d9c3-40bd-b7db-c7d26b5574bd",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999613761901855,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999897480010986,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999943375587463,
                    "sentence": "This paper proposes a semi-supervised convolutional neural network (CNN) approach for in-domain data selection to improve statistical machine translation (SMT) systems, particularly in scenarios with limited in-domain data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999898076057434,
                    "sentence": "The method leverages word embeddings trained on large general-domain datasets and combines them with one-hot vectors to classify sentences as in-domain or out-of-domain.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999855160713196,
                    "sentence": "The proposed approach outperforms state-of-the-art language model (LM)-based methods, achieving up to a 3.1 BLEU improvement, with notable robustness when the in-domain dataset is as small as 100 sentence pairs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999886751174927,
                    "sentence": "The authors demonstrate the method's effectiveness across four language pairs and three test domains, including challenging social media text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999487996101379,
                    "sentence": "Key contributions of the paper include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999690651893616,
                    "sentence": "1. A novel semi-supervised CNN-based framework for domain classification and data selection, which outperforms LM-based methods by 0.5 BLEU points on average.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999838471412659,
                    "sentence": "2. Demonstration of the method's robustness in scenarios with extremely limited in-domain data, achieving significant improvements even with only 100 in-domain sentence pairs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999908804893494,
                    "sentence": "3. Empirical evidence that the proposed method can select highly relevant in-domain data even when no domain-specific knowledge is available.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999849796295166,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999759793281555,
                    "sentence": "1. Performance and Robustness: The proposed semi-supervised CNN achieves consistent improvements over strong baselines, including LM-based methods, particularly in low-resource settings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999706745147705,
                    "sentence": "The robustness with as few as 100 in-domain sentences is a significant strength, as it addresses a critical gap in domain adaptation for SMT.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999774098396301,
                    "sentence": "2. Novelty and Practicality: The use of CNNs for domain classification in SMT is novel and practical, especially given the increasing need for domain-specific adaptation in low-resource scenarios like social media translation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999971330165863,
                    "sentence": "3. Comprehensive Evaluation: The authors evaluate their method on four language pairs and multiple test domains, providing strong empirical evidence of its effectiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999781847000122,
                    "sentence": "The inclusion of a detailed comparison with LM-based methods and an analysis of performance under varying in-domain data sizes further strengthens the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999827146530151,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999880790710449,
                    "sentence": "1. Limited Generalizability: While the method is shown to be effective for the zh2en task, its generalizability to other language pairs and domains is less clear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999902844429016,
                    "sentence": "Results for all tasks with only 100 in-domain sentence pairs would strengthen the claim of robustness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998283982276917,
                    "sentence": "2. Comparison with Interpolation Methods: The paper does not compare the proposed method with systems that interpolate in-domain and out-of-domain models, which could provide additional insights into its relative effectiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997148513793945,
                    "sentence": "3. Lack of Preprocessing Details: Social media text presents unique challenges, such as high out-of-vocabulary (OOV) rates and special character sequences.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996415972709656,
                    "sentence": "The paper does not clarify preprocessing or filtering steps, which are critical for reproducibility and understanding the method's applicability to noisy text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993906021118164,
                    "sentence": "4. Architectural Clarity: Section 3.2 lacks sufficient detail on how word embeddings, segment vectors, and one-hot vectors are combined in the CNN architecture.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9973381757736206,
                    "sentence": "An illustration of the architecture and clarification of the \"designated loss function\" would improve the paper's clarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7640048861503601,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.77511066198349,
                    "sentence": "1. Can you provide results for all language pairs with only 100 in-domain sentence pairs to further validate the robustness of your method?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7773632407188416,
                    "sentence": "2. How does your method compare to systems that interpolate in-domain and out-of-domain models?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8550024628639221,
                    "sentence": "3. What preprocessing or filtering steps were applied to handle the unique challenges of social media text, such as OOV words and special character sequences?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8234173059463501,
                    "sentence": "4. Could you clarify how word embeddings, segment vectors, and one-hot vectors are combined in the CNN architecture?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.715556800365448,
                    "sentence": "An illustration would be helpful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7221226692199707,
                    "sentence": "5. What \"designated loss function\" was used in the CNN model, and how were system weights tuned in Section 5.2?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8716322183609009,
                    "sentence": "Recommendation",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7258477210998535,
                    "sentence": "While the paper presents a novel and effective method for in-domain data selection, the lack of clarity in certain methodological details and limited generalizability evidence slightly detracts from its impact.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6774913668632507,
                    "sentence": "Addressing these concerns during the author response period could significantly strengthen the submission.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.9841954571483108,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9841954571483108,
                "mixed": 0.015804542851689255
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9841954571483108,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9841954571483108,
                    "human": 0,
                    "mixed": 0.015804542851689255
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary and Contributions\nThis paper proposes a semi-supervised convolutional neural network (CNN) approach for in-domain data selection to improve statistical machine translation (SMT) systems, particularly in scenarios with limited in-domain data. The method leverages word embeddings trained on large general-domain datasets and combines them with one-hot vectors to classify sentences as in-domain or out-of-domain. The proposed approach outperforms state-of-the-art language model (LM)-based methods, achieving up to a 3.1 BLEU improvement, with notable robustness when the in-domain dataset is as small as 100 sentence pairs. The authors demonstrate the method's effectiveness across four language pairs and three test domains, including challenging social media text. \nKey contributions of the paper include:\n1. A novel semi-supervised CNN-based framework for domain classification and data selection, which outperforms LM-based methods by 0.5 BLEU points on average.\n2. Demonstration of the method's robustness in scenarios with extremely limited in-domain data, achieving significant improvements even with only 100 in-domain sentence pairs.\n3. Empirical evidence that the proposed method can select highly relevant in-domain data even when no domain-specific knowledge is available.\nStrengths\n1. Performance and Robustness: The proposed semi-supervised CNN achieves consistent improvements over strong baselines, including LM-based methods, particularly in low-resource settings. The robustness with as few as 100 in-domain sentences is a significant strength, as it addresses a critical gap in domain adaptation for SMT.\n2. Novelty and Practicality: The use of CNNs for domain classification in SMT is novel and practical, especially given the increasing need for domain-specific adaptation in low-resource scenarios like social media translation.\n3. Comprehensive Evaluation: The authors evaluate their method on four language pairs and multiple test domains, providing strong empirical evidence of its effectiveness. The inclusion of a detailed comparison with LM-based methods and an analysis of performance under varying in-domain data sizes further strengthens the paper.\nWeaknesses\n1. Limited Generalizability: While the method is shown to be effective for the zh2en task, its generalizability to other language pairs and domains is less clear. Results for all tasks with only 100 in-domain sentence pairs would strengthen the claim of robustness.\n2. Comparison with Interpolation Methods: The paper does not compare the proposed method with systems that interpolate in-domain and out-of-domain models, which could provide additional insights into its relative effectiveness.\n3. Lack of Preprocessing Details: Social media text presents unique challenges, such as high out-of-vocabulary (OOV) rates and special character sequences. The paper does not clarify preprocessing or filtering steps, which are critical for reproducibility and understanding the method's applicability to noisy text.\n4. Architectural Clarity: Section 3.2 lacks sufficient detail on how word embeddings, segment vectors, and one-hot vectors are combined in the CNN architecture. An illustration of the architecture and clarification of the \"designated loss function\" would improve the paper's clarity.\nQuestions to Authors\n1. Can you provide results for all language pairs with only 100 in-domain sentence pairs to further validate the robustness of your method?\n2. How does your method compare to systems that interpolate in-domain and out-of-domain models?\n3. What preprocessing or filtering steps were applied to handle the unique challenges of social media text, such as OOV words and special character sequences?\n4. Could you clarify how word embeddings, segment vectors, and one-hot vectors are combined in the CNN architecture? An illustration would be helpful.\n5. What \"designated loss function\" was used in the CNN model, and how were system weights tuned in Section 5.2?\nRecommendation\nWhile the paper presents a novel and effective method for in-domain data selection, the lack of clarity in certain methodological details and limited generalizability evidence slightly detracts from its impact. Addressing these concerns during the author response period could significantly strengthen the submission."
        }
    ]
}