{
    "version": "2025-01-09-base",
    "scanId": "37bc88f0-83cc-44c0-ac62-32d25f633cdd",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999996542930603,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992847442627,
                    "sentence": "Summary and Contributions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991059303284,
                    "sentence": "This paper proposes a novel method for data selection in statistical machine translation (SMT) using semi-supervised convolutional neural networks (CNNs).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998927116394,
                    "sentence": "The method addresses the challenge of domain adaptation when only a tiny amount of in-domain data is available.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983310699463,
                    "sentence": "The proposed approach trains a CNN-based data selection model using in-domain and general-domain data, leveraging word embeddings learned from large unlabeled corpora.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999985694885254,
                    "sentence": "The model assigns domain relevance scores to general-domain sentences, selecting the most relevant ones to train an adapted SMT system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999985098838806,
                    "sentence": "Experiments on four language pairs and three test domains demonstrate significant improvements over state-of-the-art language model-based methods, achieving up to 3.1 BLEU improvement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999984502792358,
                    "sentence": "Notably, the method performs well even with as few as 100 in-domain sentences, making it suitable for fine-grained and resource-constrained domain adaptation tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973773956299,
                    "sentence": "The primary contributions of the paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999984502792358,
                    "sentence": "1. Semi-supervised CNN for Data Selection: The introduction of a semi-supervised CNN framework for domain classification and data selection, which outperforms existing language model-based methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973177909851,
                    "sentence": "2. Effective Use of Limited In-Domain Data: Demonstration of the method's robustness with minimal in-domain data (as few as 100 sentences), achieving significant BLEU improvements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977946281433,
                    "sentence": "3. Practical Applicability: Validation of the method across multiple language pairs and social media domains, showcasing its generalizability and effectiveness in real-world scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999982118606567,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983906745911,
                    "sentence": "1. Significant Performance Gains: The proposed method consistently outperforms strong baselines, including state-of-the-art n-gram and RNN-based language model methods, with improvements of up to 3.1 BLEU.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998152256012,
                    "sentence": "The results are statistically significant and robust across diverse tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999987483024597,
                    "sentence": "2. Innovative Use of CNNs: The use of CNNs for domain classification is novel in the context of SMT data selection.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999988675117493,
                    "sentence": "The combination of convolutional and pooling layers effectively captures domain-specific features, while word embeddings enhance generalization to unseen words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999984502792358,
                    "sentence": "3. Practical Relevance: The method addresses a critical challenge in SMT\"\"domain adaptation with limited in-domain data\"\"making it highly relevant for applications like social media translation, where labeled data is scarce.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999982714653015,
                    "sentence": "4. Comprehensive Evaluation: The experiments are thorough, covering multiple language pairs, domains, and baseline comparisons.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999987483024597,
                    "sentence": "The authors also explore the impact of varying in-domain data sizes, providing valuable insights into the method's scalability and robustness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979138374329,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999518394470215,
                    "sentence": "1. Limited Discussion of Computational Costs: While the method demonstrates strong performance, the paper does not provide sufficient details on the computational overhead of training CNNs compared to traditional language model-based methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998088479042053,
                    "sentence": "This could be a concern for large-scale applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999331831932068,
                    "sentence": "2. Generalizability Beyond SMT: Although the method is effective for SMT, its applicability to other NLP tasks requiring domain adaptation is not explored.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998782873153687,
                    "sentence": "A broader discussion of potential extensions would strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999592304229736,
                    "sentence": "3. Ablation Studies: The paper lacks detailed ablation studies to isolate the contributions of different components, such as the impact of word embeddings versus CNN architecture.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999560713768005,
                    "sentence": "This would clarify which aspects of the method drive its success.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999222159385681,
                    "sentence": "Questions to Authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998391270637512,
                    "sentence": "1. How does the computational cost of training the semi-supervised CNN compare to the baseline language model-based methods, particularly for large-scale datasets?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998173117637634,
                    "sentence": "2. Could the proposed method be extended to other NLP tasks, such as domain-specific sentiment analysis or question answering?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997831583023071,
                    "sentence": "If so, what modifications would be needed?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988531470298767,
                    "sentence": "Additional Comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997748732566833,
                    "sentence": "Overall, this paper presents a significant advancement in SMT domain adaptation, particularly for low-resource scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998067617416382,
                    "sentence": "While some aspects, such as computational efficiency and broader applicability, could be further explored, the method's strong empirical results and practical relevance make it a valuable contribution to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998450875282288,
                    "sentence": "Encouraging further exploration of its potential applications and optimizations would enhance its impact.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9997862822885396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997862822885396,
                "mixed": 0.00021371771146045916
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997862822885396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997862822885396,
                    "human": 0,
                    "mixed": 0.00021371771146045916
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary and Contributions:\nThis paper proposes a novel method for data selection in statistical machine translation (SMT) using semi-supervised convolutional neural networks (CNNs). The method addresses the challenge of domain adaptation when only a tiny amount of in-domain data is available. The proposed approach trains a CNN-based data selection model using in-domain and general-domain data, leveraging word embeddings learned from large unlabeled corpora. The model assigns domain relevance scores to general-domain sentences, selecting the most relevant ones to train an adapted SMT system. Experiments on four language pairs and three test domains demonstrate significant improvements over state-of-the-art language model-based methods, achieving up to 3.1 BLEU improvement. Notably, the method performs well even with as few as 100 in-domain sentences, making it suitable for fine-grained and resource-constrained domain adaptation tasks.\nThe primary contributions of the paper are:\n1. Semi-supervised CNN for Data Selection: The introduction of a semi-supervised CNN framework for domain classification and data selection, which outperforms existing language model-based methods.\n2. Effective Use of Limited In-Domain Data: Demonstration of the method's robustness with minimal in-domain data (as few as 100 sentences), achieving significant BLEU improvements.\n3. Practical Applicability: Validation of the method across multiple language pairs and social media domains, showcasing its generalizability and effectiveness in real-world scenarios.\nStrengths:\n1. Significant Performance Gains: The proposed method consistently outperforms strong baselines, including state-of-the-art n-gram and RNN-based language model methods, with improvements of up to 3.1 BLEU. The results are statistically significant and robust across diverse tasks.\n2. Innovative Use of CNNs: The use of CNNs for domain classification is novel in the context of SMT data selection. The combination of convolutional and pooling layers effectively captures domain-specific features, while word embeddings enhance generalization to unseen words.\n3. Practical Relevance: The method addresses a critical challenge in SMT\"\"domain adaptation with limited in-domain data\"\"making it highly relevant for applications like social media translation, where labeled data is scarce.\n4. Comprehensive Evaluation: The experiments are thorough, covering multiple language pairs, domains, and baseline comparisons. The authors also explore the impact of varying in-domain data sizes, providing valuable insights into the method's scalability and robustness.\nWeaknesses:\n1. Limited Discussion of Computational Costs: While the method demonstrates strong performance, the paper does not provide sufficient details on the computational overhead of training CNNs compared to traditional language model-based methods. This could be a concern for large-scale applications.\n2. Generalizability Beyond SMT: Although the method is effective for SMT, its applicability to other NLP tasks requiring domain adaptation is not explored. A broader discussion of potential extensions would strengthen the paper.\n3. Ablation Studies: The paper lacks detailed ablation studies to isolate the contributions of different components, such as the impact of word embeddings versus CNN architecture. This would clarify which aspects of the method drive its success.\nQuestions to Authors:\n1. How does the computational cost of training the semi-supervised CNN compare to the baseline language model-based methods, particularly for large-scale datasets?\n2. Could the proposed method be extended to other NLP tasks, such as domain-specific sentiment analysis or question answering? If so, what modifications would be needed?\nAdditional Comments:\nOverall, this paper presents a significant advancement in SMT domain adaptation, particularly for low-resource scenarios. While some aspects, such as computational efficiency and broader applicability, could be further explored, the method's strong empirical results and practical relevance make it a valuable contribution to the field. Encouraging further exploration of its potential applications and optimizations would enhance its impact."
        }
    ]
}