{
    "version": "2025-01-09-base",
    "scanId": "64a3db57-4944-4176-8fb3-2a6d68a4807b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.962341845035553,
                    "sentence": "This paper introduces a novel approach for evaluating topic models by clustering the top n words of each topic into groups or \"buckets\" based on the cosine similarity of their corresponding word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9003004431724548,
                    "sentence": "In its simplest form, the method processes words sequentially, assigning each word to an existing \"bucket\" if its cosine similarity with the other words in that bucket exceeds a predefined threshold, or creating a new bucket otherwise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8411633372306824,
                    "sentence": "Additionally, the authors propose two more advanced variations of the method, incorporating eigenvectors and reorganization techniques.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6512042284011841,
                    "sentence": "The approach is tested on three standard datasets as well as in a weakly supervised text classification scenario, where it either matches or outperforms the current state-of-the-art method (RÃ¶der et al., 2015).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.39424625039100647,
                    "sentence": "While the core idea of the paper is straightforward, it has an ad hoc quality to it.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4439137876033783,
                    "sentence": "The authors do not provide a theoretical justification for why topic quality should be assessed based on word\"\"word similarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18789750337600708,
                    "sentence": "This is not immediately intuitive, given that topics and word embeddings are derived from two distinct notions of context (document-level versus sequential context).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.22959649562835693,
                    "sentence": "Nonetheless, the proposed method appears to perform effectively in practice.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.20823024213314056,
                    "sentence": "(I would, however, recommend including significance tests for the results presented in Table 1.)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.26081666350364685,
                    "sentence": "On the whole, the paper is well-written, though there are some minor language issues.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2988905608654022,
                    "sentence": "I found the explanation of the techniques in Section 3 somewhat difficult to follow, likely because the authors frequently use the passive voice (e.g., \"the threshold is computed as\") when describing what are essentially design decisions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4448413848876953,
                    "sentence": "I suggest the authors clarify the descriptions of the different methods, perhaps by dedicating a separate subsection to each.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.490003377199173,
                    "sentence": "There seems to be sufficient space for this, as the authors have not fully utilized the 8-page limit, and the relatively uninformative \"trace\" of the method on page 3 could be condensed or removed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.46818456053733826,
                    "sentence": "One question I had concerns the sensitivity of the proposed method to different word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4073575735092163,
                    "sentence": "For instance, how would the results change if the authors used word2vec instead of GloVe?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.5710657228372709
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.3298042666671994,
            "class_probabilities": {
                "human": 0.6035798923689981,
                "ai": 0.3298042666671994,
                "mixed": 0.06661584096380246
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.6035798923689981,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.3298042666671994,
                    "human": 0.6035798923689981,
                    "mixed": 0.06661584096380246
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper introduces a novel approach for evaluating topic models by clustering the top n words of each topic into groups or \"buckets\" based on the cosine similarity of their corresponding word embeddings. In its simplest form, the method processes words sequentially, assigning each word to an existing \"bucket\" if its cosine similarity with the other words in that bucket exceeds a predefined threshold, or creating a new bucket otherwise. Additionally, the authors propose two more advanced variations of the method, incorporating eigenvectors and reorganization techniques. The approach is tested on three standard datasets as well as in a weakly supervised text classification scenario, where it either matches or outperforms the current state-of-the-art method (RÃ¶der et al., 2015).\nWhile the core idea of the paper is straightforward, it has an ad hoc quality to it. The authors do not provide a theoretical justification for why topic quality should be assessed based on word\"\"word similarity. This is not immediately intuitive, given that topics and word embeddings are derived from two distinct notions of context (document-level versus sequential context). Nonetheless, the proposed method appears to perform effectively in practice. (I would, however, recommend including significance tests for the results presented in Table 1.)\nOn the whole, the paper is well-written, though there are some minor language issues. I found the explanation of the techniques in Section 3 somewhat difficult to follow, likely because the authors frequently use the passive voice (e.g., \"the threshold is computed as\") when describing what are essentially design decisions. I suggest the authors clarify the descriptions of the different methods, perhaps by dedicating a separate subsection to each. There seems to be sufficient space for this, as the authors have not fully utilized the 8-page limit, and the relatively uninformative \"trace\" of the method on page 3 could be condensed or removed.\nOne question I had concerns the sensitivity of the proposed method to different word embeddings. For instance, how would the results change if the authors used word2vec instead of GloVe?"
        }
    ]
}