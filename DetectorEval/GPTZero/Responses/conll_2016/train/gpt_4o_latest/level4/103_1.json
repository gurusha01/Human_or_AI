{
    "version": "2025-01-09-base",
    "scanId": "6a74fe2b-0bdc-4605-9f0a-bd86ff4e25b2",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9981186389923096,
                    "sentence": "This paper introduces a novel approach for assessing topic quality by leveraging word embeddings to compute similarity, either directly or through matrix factorization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980839490890503,
                    "sentence": "The method demonstrates strong performance across standard datasets, achieving notable results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9956676363945007,
                    "sentence": "The proposed approach represents a logical and significant progression in the research trajectory of topic evaluation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9934566020965576,
                    "sentence": "However, a key concern with the results lies in the inconsistencies observed across datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9900932312011719,
                    "sentence": "While state-of-the-art performance is achieved for all three datasets, the methods vary considerably in effectiveness, with some performing below the state of the art.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.973387598991394,
                    "sentence": "Specifically, none of the proposed methods consistently outperforms the state of the art across all datasets, and the SVD-based methods perform particularly poorly on the genomics dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9826986193656921,
                    "sentence": "This inconsistency raises concerns for practitioners seeking to adopt the method for arbitrary datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5407962203025818,
                    "sentence": "It seems plausible that the lower performance of SVD on the genomics dataset is linked to the proportion of out-of-vocabulary (OOV) terms (see further comments below).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5482227206230164,
                    "sentence": "It might be possible to predict which method would perform best based on vocabulary overlap with GloVe embeddings or similar metrics, but this issue is not addressed in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.34993597865104675,
                    "sentence": "Additional points of concern:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.29078033566474915,
                    "sentence": "- The proposed method bears significant resemblance to techniques discussed in the lexical chaining literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990254044532776,
                    "sentence": "The authors are encouraged to explore this body of work and incorporate relevant insights in future iterations of the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990663528442383,
                    "sentence": "- While the authors emphasize that their method is parameter-free, it should be noted that the word embedding models employed inherently involve a substantial number of parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998570442199707,
                    "sentence": "Although this is not a critical issue, it is worth acknowledging explicitly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9952009320259094,
                    "sentence": "- The paper does not clarify how the method handles OOV terms, such as those found in the genomics dataset (i.e., terms absent from the pretrained GloVe embeddings).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.995269238948822,
                    "sentence": "Are these terms simply excluded?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975201487541199,
                    "sentence": "If so, what implications does this have for the method's performance?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9949838519096375,
                    "sentence": "Minor issues:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.992094099521637,
                    "sentence": "- In Section 2.1, the description of word embeddings implicitly assumes that vector length is irrelevant, as cosine similarity is used to measure similarity between vectors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9823266267776489,
                    "sentence": "While this is valid for unit-length vectors, it is worth noting that word2vec does not inherently produce unit-length vectors (pretrained vectors are normalized post hoc, but vectors generated independently may vary in length).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9935977458953857,
                    "sentence": "This is a small but important detail.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9814643859863281,
                    "sentence": "- The graphs in Figure 1 are too small to be legible.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.4355389046213882
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9658502932045533,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9658502932045533,
                "mixed": 0.034149706795446697
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9658502932045533,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9658502932045533,
                    "human": 0,
                    "mixed": 0.034149706795446697
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper introduces a novel approach for assessing topic quality by leveraging word embeddings to compute similarity, either directly or through matrix factorization. The method demonstrates strong performance across standard datasets, achieving notable results.\nThe proposed approach represents a logical and significant progression in the research trajectory of topic evaluation. However, a key concern with the results lies in the inconsistencies observed across datasets. While state-of-the-art performance is achieved for all three datasets, the methods vary considerably in effectiveness, with some performing below the state of the art. Specifically, none of the proposed methods consistently outperforms the state of the art across all datasets, and the SVD-based methods perform particularly poorly on the genomics dataset. This inconsistency raises concerns for practitioners seeking to adopt the method for arbitrary datasets. It seems plausible that the lower performance of SVD on the genomics dataset is linked to the proportion of out-of-vocabulary (OOV) terms (see further comments below). It might be possible to predict which method would perform best based on vocabulary overlap with GloVe embeddings or similar metrics, but this issue is not addressed in the paper.\nAdditional points of concern:\n- The proposed method bears significant resemblance to techniques discussed in the lexical chaining literature. The authors are encouraged to explore this body of work and incorporate relevant insights in future iterations of the paper.\n- While the authors emphasize that their method is parameter-free, it should be noted that the word embedding models employed inherently involve a substantial number of parameters. Although this is not a critical issue, it is worth acknowledging explicitly.\n- The paper does not clarify how the method handles OOV terms, such as those found in the genomics dataset (i.e., terms absent from the pretrained GloVe embeddings). Are these terms simply excluded? If so, what implications does this have for the method's performance?\nMinor issues:\n- In Section 2.1, the description of word embeddings implicitly assumes that vector length is irrelevant, as cosine similarity is used to measure similarity between vectors. While this is valid for unit-length vectors, it is worth noting that word2vec does not inherently produce unit-length vectors (pretrained vectors are normalized post hoc, but vectors generated independently may vary in length). This is a small but important detail.\n- The graphs in Figure 1 are too small to be legible."
        }
    ]
}