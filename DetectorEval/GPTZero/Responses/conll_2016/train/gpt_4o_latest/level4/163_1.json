{
    "version": "2025-01-09-base",
    "scanId": "e61b3a73-8a73-4cc3-9568-88d53a338256",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.025576381012797356,
                    "sentence": "The objective of this paper is to demonstrate that distributional information encoded in word vector models carries information about POS labels.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0062818401493132114,
                    "sentence": "The authors utilize a version of the BNC annotated with UD POS, where words have been replaced by their lemmas.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0072147599421441555,
                    "sentence": "They train word embeddings on this corpus and subsequently use the resulting vectors to train a logistic classifier for predicting word POS.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004024050664156675,
                    "sentence": "Evaluations are conducted on the same corpus (via cross-validation) as well as on additional corpora.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012920518405735493,
                    "sentence": "The results are clearly presented, thoroughly discussed, and analyzed in detail.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009379095397889614,
                    "sentence": "The paper is well-written and easy to follow.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010548720136284828,
                    "sentence": "However, the primary limitation of this work is the lack of novelty in terms of NLP or ML contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01485072448849678,
                    "sentence": "It describes a series of straightforward experiments without introducing any new ideas or methods in these domains.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01939570903778076,
                    "sentence": "That said, the results are indeed interesting as they provide empirical support for the concept of POS.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009363744407892227,
                    "sentence": "In this respect, the paper is certainly suitable for publication in a venue focused on quantitative or empirical linguistics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008652296848595142,
                    "sentence": "Additionally, the authors should provide a more comprehensive review of the literature on POS tagging and POS induction using word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0056724767200648785,
                    "sentence": "Relevant works to consider include Lin, Ammar, Duer, and Levin (2015); Ling et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011574347503483295,
                    "sentence": "(2015, EMNLP); and Plank, SÃ,gaard, and Goldberg (2016), among others.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                }
            ],
            "completely_generated_prob": 0.017486769031531738,
            "class_probabilities": {
                "human": 0.9825132309684682,
                "ai": 0.017486769031531738,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9825132309684682,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.017486769031531738,
                    "human": 0.9825132309684682,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The objective of this paper is to demonstrate that distributional information encoded in word vector models carries information about POS labels. The authors utilize a version of the BNC annotated with UD POS, where words have been replaced by their lemmas. They train word embeddings on this corpus and subsequently use the resulting vectors to train a logistic classifier for predicting word POS. Evaluations are conducted on the same corpus (via cross-validation) as well as on additional corpora. The results are clearly presented, thoroughly discussed, and analyzed in detail.\nThe paper is well-written and easy to follow. However, the primary limitation of this work is the lack of novelty in terms of NLP or ML contributions. It describes a series of straightforward experiments without introducing any new ideas or methods in these domains. That said, the results are indeed interesting as they provide empirical support for the concept of POS. In this respect, the paper is certainly suitable for publication in a venue focused on quantitative or empirical linguistics.\nAdditionally, the authors should provide a more comprehensive review of the literature on POS tagging and POS induction using word embeddings. Relevant works to consider include Lin, Ammar, Duer, and Levin (2015); Ling et al. (2015, EMNLP); and Plank, SÃ¸gaard, and Goldberg (2016), among others."
        }
    ]
}