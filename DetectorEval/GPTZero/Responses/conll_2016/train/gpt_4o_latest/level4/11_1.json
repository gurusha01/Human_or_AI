{
    "version": "2025-01-09-base",
    "scanId": "2215ed8e-8f65-454e-9b51-1bee85d5f437",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9996654987335205,
                    "sentence": "The authors introduce a novel variation of the coreference resolution task, specifically adapted for Wikipedia.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997185468673706,
                    "sentence": "The task involves identifying the coreference chain corresponding to the entity that the Wikipedia article primarily discusses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994272589683533,
                    "sentence": "To this end, the authors annotate 30 documents with all coreference chains, noting that approximately 25% of the mentions pertain to the \"main concept\" of the article.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993484616279602,
                    "sentence": "They proceed to describe simple baselines and a basic classifier, which demonstrates superior performance compared to these baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993032217025757,
                    "sentence": "Furthermore, they integrate their classifier into the Stanford rule-based coreference system, achieving significant improvements over all state-of-the-art systems on Wikipedia.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990094304084778,
                    "sentence": "This paper presents an intriguing modification to the coreference task, which is both logical from an information extraction (IE) standpoint and has the potential to invigorate coreference research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996311068534851,
                    "sentence": "It also offers a compelling bridge between coreference and entity linking literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996071457862854,
                    "sentence": "While I am often skeptical of papers that define a new task where standard systems underperform and then propose minor adjustments to improve results, this work feels different.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999438464641571,
                    "sentence": "The task itself is highly motivating, and rather than artificially creating a new domain, the authors convincingly demonstrate that standard systems struggle in a setting that is genuinely important.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991652965545654,
                    "sentence": "THE TASK: Main concept resolution is a fascinating task from an IE perspective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988165497779846,
                    "sentence": "There are numerous scenarios where documents center around a specific entity (e.g., biographical articles, dossiers, clinical records), and the relevant information to extract pertains to that entity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977508783340454,
                    "sentence": "Traditional coreference tasks often include a large number of mentions that are irrelevant for most IE applications (e.g., generic mentions), whereas this task focuses exclusively on mentions that are directly meaningful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997753381729126,
                    "sentence": "From a methodological perspective, the concept of a \"main concept\" serves as a useful discourse anchor for coreference.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.995859682559967,
                    "sentence": "However, there is still considerable room for improvement beyond the baselines, particularly for non-pronominal mentions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9945991635322571,
                    "sentence": "Additionally, conducting coreference directly on Wikipedia introduces opportunities for leveraging knowledge in innovative ways, as illustrated by the authors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9935893416404724,
                    "sentence": "Consequently, this domain has the potential to serve as an exciting testbed for developing ideas that could enhance coreference resolution more broadly, even though achieving robust improvements in the general setting remains challenging due to the complexity of the problem.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9961552619934082,
                    "sentence": "Unlike prior work that isolates specific aspects of coreference (e.g., the Winograd schema), this paper has a notable impact on the overall coreference problem, particularly in a domain like Wikipedia, which is of significant interest to the ACL community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9508239030838013,
                    "sentence": "THE TECHNIQUES: While the techniques are not the standout feature of this paper, they are effective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9419915676116943,
                    "sentence": "The features employed are reasonable, though exploring additional feature conjunctions might yield further improvements (it is unclear if the authors experimented in this direction).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9497168064117432,
                    "sentence": "The authors should also clarify earlier in the paper that their primary main concept resolution system is a binary classifier, as this is not explicitly stated early on, leaving the model undefined during the description of feature engineering.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9529668688774109,
                    "sentence": "MINOR DETAILS:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9577656984329224,
                    "sentence": "- Organization: I suggest introducing the dataset immediately after the \"Related Works\" section (making it the new Section 3).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9363813400268555,
                    "sentence": "This would allow the results in the \"Baselines\" section to be presented earlier, providing stronger motivation for the \"Approach\" section.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9279835820198059,
                    "sentence": "- Terminology: When referring to Dcoref and Scoref in Section 4, the authors should cite the relevant Stanford coreference system papers or explicitly state that these refer to the Stanford system, as many readers may not be familiar with these names.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9534463286399841,
                    "sentence": "- Candidate List: The term \"candidate list\" is somewhat unclear, particularly in the following passage:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9461308121681213,
                    "sentence": "\"We leverage the hyperlink structure of the article in order to enrich the list of mentions with shallow semantic attributes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9442542791366577,
                    "sentence": "For each link found within the article under consideration, we look through the candidate list for all mentions that match the surface string of the link.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.903054416179657,
                    "sentence": "Please clarify that the \"candidate list\" refers to the set of mentions in the article that could potentially be coreferent with the main concept.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9068723917007446,
                    "sentence": "Most readers will understand that this module imports semantic information from Wikipedia's hyperlink structure (e.g., if a mention is hyperlinked to an article identified as female in Freebase, that mention is female), so maintaining clear terminology is important.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9415340423583984,
                    "sentence": "- Section 6.1: The statement \"we consider the union of WCR mentions and all mentions predicted by the method described in (Raghunathan et al., 2010)\" is confusing, as Section 4.1 implies these are the same.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9324455857276917,
                    "sentence": "It is unclear where additional WCR mentions would be extracted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9462478756904602,
                    "sentence": "This discrepancy should be addressed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.8923758534658037,
            "class_probabilities": {
                "human": 0.10626570421472266,
                "ai": 0.8923758534658037,
                "mixed": 0.00135844231947367
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8923758534658037,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8923758534658037,
                    "human": 0.10626570421472266,
                    "mixed": 0.00135844231947367
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The authors introduce a novel variation of the coreference resolution task, specifically adapted for Wikipedia. The task involves identifying the coreference chain corresponding to the entity that the Wikipedia article primarily discusses. To this end, the authors annotate 30 documents with all coreference chains, noting that approximately 25% of the mentions pertain to the \"main concept\" of the article. They proceed to describe simple baselines and a basic classifier, which demonstrates superior performance compared to these baselines. Furthermore, they integrate their classifier into the Stanford rule-based coreference system, achieving significant improvements over all state-of-the-art systems on Wikipedia.\nThis paper presents an intriguing modification to the coreference task, which is both logical from an information extraction (IE) standpoint and has the potential to invigorate coreference research. It also offers a compelling bridge between coreference and entity linking literature. While I am often skeptical of papers that define a new task where standard systems underperform and then propose minor adjustments to improve results, this work feels different. The task itself is highly motivating, and rather than artificially creating a new domain, the authors convincingly demonstrate that standard systems struggle in a setting that is genuinely important.\nTHE TASK: Main concept resolution is a fascinating task from an IE perspective. There are numerous scenarios where documents center around a specific entity (e.g., biographical articles, dossiers, clinical records), and the relevant information to extract pertains to that entity. Traditional coreference tasks often include a large number of mentions that are irrelevant for most IE applications (e.g., generic mentions), whereas this task focuses exclusively on mentions that are directly meaningful. \nFrom a methodological perspective, the concept of a \"main concept\" serves as a useful discourse anchor for coreference. However, there is still considerable room for improvement beyond the baselines, particularly for non-pronominal mentions. Additionally, conducting coreference directly on Wikipedia introduces opportunities for leveraging knowledge in innovative ways, as illustrated by the authors. Consequently, this domain has the potential to serve as an exciting testbed for developing ideas that could enhance coreference resolution more broadly, even though achieving robust improvements in the general setting remains challenging due to the complexity of the problem.\nUnlike prior work that isolates specific aspects of coreference (e.g., the Winograd schema), this paper has a notable impact on the overall coreference problem, particularly in a domain like Wikipedia, which is of significant interest to the ACL community.\nTHE TECHNIQUES: While the techniques are not the standout feature of this paper, they are effective. The features employed are reasonable, though exploring additional feature conjunctions might yield further improvements (it is unclear if the authors experimented in this direction). The authors should also clarify earlier in the paper that their primary main concept resolution system is a binary classifier, as this is not explicitly stated early on, leaving the model undefined during the description of feature engineering.\nMINOR DETAILS:\n- Organization: I suggest introducing the dataset immediately after the \"Related Works\" section (making it the new Section 3). This would allow the results in the \"Baselines\" section to be presented earlier, providing stronger motivation for the \"Approach\" section.\n- Terminology: When referring to Dcoref and Scoref in Section 4, the authors should cite the relevant Stanford coreference system papers or explicitly state that these refer to the Stanford system, as many readers may not be familiar with these names.\n- Candidate List: The term \"candidate list\" is somewhat unclear, particularly in the following passage: \n \"We leverage the hyperlink structure of the article in order to enrich the list of mentions with shallow semantic attributes. For each link found within the article under consideration, we look through the candidate list for all mentions that match the surface string of the link.\" \n Please clarify that the \"candidate list\" refers to the set of mentions in the article that could potentially be coreferent with the main concept. Most readers will understand that this module imports semantic information from Wikipedia's hyperlink structure (e.g., if a mention is hyperlinked to an article identified as female in Freebase, that mention is female), so maintaining clear terminology is important.\n- Section 6.1: The statement \"we consider the union of WCR mentions and all mentions predicted by the method described in (Raghunathan et al., 2010)\" is confusing, as Section 4.1 implies these are the same. It is unclear where additional WCR mentions would be extracted. This discrepancy should be addressed."
        }
    ]
}