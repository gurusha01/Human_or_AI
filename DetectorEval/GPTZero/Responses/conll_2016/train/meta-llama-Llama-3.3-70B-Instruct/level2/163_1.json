{
    "version": "2025-01-09-base",
    "scanId": "85ea9fb2-9227-400d-a895-eb10f2a67523",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9983021020889282,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9948707222938538,
                    "sentence": "This paper explores the relationship between word embeddings and part-of-speech (PoS) boundaries in the British National Corpus.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9942396879196167,
                    "sentence": "The authors train a classifier to predict PoS tags for words based on their embeddings and analyze the errors to identify words with distributional patterns different from other words of the same part of speech.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9955504536628723,
                    "sentence": "The results show that word embeddings contain information about PoS affiliation, and the classifier achieves a high accuracy in predicting PoS tags.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972018003463745,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9971367716789246,
                    "sentence": "1. Word embeddings contain PoS information: The paper demonstrates that word embeddings trained on a large corpus contain information about the part of speech of words, which can be used to predict PoS tags with high accuracy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9978383779525757,
                    "sentence": "2. Distributional models can detect annotation errors: The authors show that distributional models can detect systematic errors or inconsistencies in PoS tags, whether they be automatic or manual, by analyzing the errors made by the classifier.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990871548652649,
                    "sentence": "3. PoS affiliation is distributed among multiple components: The paper finds that the knowledge about PoS affiliation is distributed among at least a hundred components of the word embeddings, rather than being concentrated in one or two specific features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9976366758346558,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983400106430054,
                    "sentence": "1. Novel approach to PoS tagging: The paper proposes a novel approach to PoS tagging using distributional models, which can be used to improve the accuracy of PoS tagging and detect annotation errors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999833703041077,
                    "sentence": "2. High accuracy: The classifier achieves a high accuracy in predicting PoS tags, demonstrating the effectiveness of the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999845623970032,
                    "sentence": "3. Insights into PoS boundaries: The paper provides insights into the nature of PoS boundaries, showing that they are not strict and can be considered a non-categorical linguistic phenomenon.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999603629112244,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999513626098633,
                    "sentence": "1. Limited to English: The paper only explores the relationship between word embeddings and PoS boundaries in English, and it is unclear whether the results generalize to other languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999584555625916,
                    "sentence": "2. Dependence on corpus quality: The accuracy of the classifier depends on the quality of the corpus used to train the distributional model, which may not always be available or reliable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998767971992493,
                    "sentence": "3. Need for further research: The paper highlights the need for further research into the correspondence between particular embedding components and part of speech affiliation, as well as the influence of hyperparameters on the performance of distributional models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997096061706543,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995688796043396,
                    "sentence": "1. How do the results generalize to other languages, and what are the implications for PoS tagging in languages with different grammatical structures?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994230270385742,
                    "sentence": "2. Can the approach be used to improve the accuracy of PoS tagging in low-resource languages, where annotated corpora may not be available?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992381930351257,
                    "sentence": "3. How do the hyperparameters used to train the distributional model affect the performance of the classifier, and what are the optimal hyperparameters for PoS tagging?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThis paper explores the relationship between word embeddings and part-of-speech (PoS) boundaries in the British National Corpus. The authors train a classifier to predict PoS tags for words based on their embeddings and analyze the errors to identify words with distributional patterns different from other words of the same part of speech. The results show that word embeddings contain information about PoS affiliation, and the classifier achieves a high accuracy in predicting PoS tags.\nMain Contributions\n1. Word embeddings contain PoS information: The paper demonstrates that word embeddings trained on a large corpus contain information about the part of speech of words, which can be used to predict PoS tags with high accuracy.\n2. Distributional models can detect annotation errors: The authors show that distributional models can detect systematic errors or inconsistencies in PoS tags, whether they be automatic or manual, by analyzing the errors made by the classifier.\n3. PoS affiliation is distributed among multiple components: The paper finds that the knowledge about PoS affiliation is distributed among at least a hundred components of the word embeddings, rather than being concentrated in one or two specific features.\nStrengths\n1. Novel approach to PoS tagging: The paper proposes a novel approach to PoS tagging using distributional models, which can be used to improve the accuracy of PoS tagging and detect annotation errors.\n2. High accuracy: The classifier achieves a high accuracy in predicting PoS tags, demonstrating the effectiveness of the approach.\n3. Insights into PoS boundaries: The paper provides insights into the nature of PoS boundaries, showing that they are not strict and can be considered a non-categorical linguistic phenomenon.\nWeaknesses\n1. Limited to English: The paper only explores the relationship between word embeddings and PoS boundaries in English, and it is unclear whether the results generalize to other languages.\n2. Dependence on corpus quality: The accuracy of the classifier depends on the quality of the corpus used to train the distributional model, which may not always be available or reliable.\n3. Need for further research: The paper highlights the need for further research into the correspondence between particular embedding components and part of speech affiliation, as well as the influence of hyperparameters on the performance of distributional models.\nQuestions to Authors\n1. How do the results generalize to other languages, and what are the implications for PoS tagging in languages with different grammatical structures?\n2. Can the approach be used to improve the accuracy of PoS tagging in low-resource languages, where annotated corpora may not be available?\n3. How do the hyperparameters used to train the distributional model affect the performance of the classifier, and what are the optimal hyperparameters for PoS tagging?"
        }
    ]
}