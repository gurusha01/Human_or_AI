{
    "version": "2025-01-09-base",
    "scanId": "e51abbb2-c2ec-42c5-bf8a-ecde200779e7",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999894499778748,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999957263469696,
                    "sentence": "The paper proposes a language-independent method for Named Entity Recognition (NER) using a cross-lingual wikifier to disambiguate words and phrases to the English Wikipedia.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999855160713196,
                    "sentence": "The model works on all languages in Wikipedia and requires only a Wikipedia dump.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999854564666748,
                    "sentence": "The authors evaluate their model on a wide range of languages in both monolingual and cross-lingual settings, showing significant improvements over strong baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999532103538513,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999150633811951,
                    "sentence": "1. Language-independent features: The paper introduces a novel approach to generating language-independent features for NER using a cross-lingual wikifier, which grounds words and phrases to the English Wikipedia.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998518228530884,
                    "sentence": "2. Cross-lingual NER model: The authors propose a cross-lingual NER model that can be applied to all languages in Wikipedia, outperforming comparable approaches on the standard CoNLL datasets and low-resource languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999051094055176,
                    "sentence": "3. Improved performance on low-resource languages: The model achieves state-of-the-art results on low-resource languages, including Turkish, Tagalog, Yoruba, Bengali, and Tamil, with limited annotated training data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999226331710815,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998993873596191,
                    "sentence": "1. Effective use of Wikipedia: The paper leverages Wikipedia as a source of information for each language, allowing for the creation of language-independent features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998505115509033,
                    "sentence": "2. Improved performance on low-resource languages: The model's ability to perform well on low-resource languages is a significant strength, as these languages often lack annotated training data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998166561126709,
                    "sentence": "3. Flexibility: The model can be trained on multiple source languages, allowing for the augmentation of training data from other languages' annotated documents.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999383091926575,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996101260185242,
                    "sentence": "1. Dependence on Wikipedia size: The quality of wikifier features depends on the size of the Wikipedia for the test language, which may limit the model's performance on languages with small Wikipedia sizes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997192025184631,
                    "sentence": "2. Limited use of Wikipedia information: The paper only uses Wikipedia categories and FreeBase types as features, leaving other information from Wikipedia, such as words from documents and relations between titles, unexplored.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997406005859375,
                    "sentence": "3. Need for further research: The authors acknowledge the need for additional research to incorporate other techniques for multilingual NER, such as parallel projection and generating training data from Wikipedia automatically.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996156096458435,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999172687530518,
                    "sentence": "1. How do you plan to address the limitation of dependence on Wikipedia size, particularly for languages with small Wikipedia sizes?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998873472213745,
                    "sentence": "2. Can you elaborate on the potential benefits and challenges of incorporating other Wikipedia information, such as words from documents and relations between titles, into the model?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999130964279175,
                    "sentence": "3. How do you envision the model being used in real-world applications, and what potential impact could it have on the field of NLP?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper proposes a language-independent method for Named Entity Recognition (NER) using a cross-lingual wikifier to disambiguate words and phrases to the English Wikipedia. The model works on all languages in Wikipedia and requires only a Wikipedia dump. The authors evaluate their model on a wide range of languages in both monolingual and cross-lingual settings, showing significant improvements over strong baselines.\nMain Contributions\n1. Language-independent features: The paper introduces a novel approach to generating language-independent features for NER using a cross-lingual wikifier, which grounds words and phrases to the English Wikipedia.\n2. Cross-lingual NER model: The authors propose a cross-lingual NER model that can be applied to all languages in Wikipedia, outperforming comparable approaches on the standard CoNLL datasets and low-resource languages.\n3. Improved performance on low-resource languages: The model achieves state-of-the-art results on low-resource languages, including Turkish, Tagalog, Yoruba, Bengali, and Tamil, with limited annotated training data.\nStrengths\n1. Effective use of Wikipedia: The paper leverages Wikipedia as a source of information for each language, allowing for the creation of language-independent features.\n2. Improved performance on low-resource languages: The model's ability to perform well on low-resource languages is a significant strength, as these languages often lack annotated training data.\n3. Flexibility: The model can be trained on multiple source languages, allowing for the augmentation of training data from other languages' annotated documents.\nWeaknesses\n1. Dependence on Wikipedia size: The quality of wikifier features depends on the size of the Wikipedia for the test language, which may limit the model's performance on languages with small Wikipedia sizes.\n2. Limited use of Wikipedia information: The paper only uses Wikipedia categories and FreeBase types as features, leaving other information from Wikipedia, such as words from documents and relations between titles, unexplored.\n3. Need for further research: The authors acknowledge the need for additional research to incorporate other techniques for multilingual NER, such as parallel projection and generating training data from Wikipedia automatically.\nQuestions to Authors\n1. How do you plan to address the limitation of dependence on Wikipedia size, particularly for languages with small Wikipedia sizes?\n2. Can you elaborate on the potential benefits and challenges of incorporating other Wikipedia information, such as words from documents and relations between titles, into the model?\n3. How do you envision the model being used in real-world applications, and what potential impact could it have on the field of NLP?"
        }
    ]
}