{
    "version": "2025-01-09-base",
    "scanId": "9e409cde-2536-463c-bab6-a000c33c43f1",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998076558113098,
                    "sentence": "This paper explores the interaction between word embeddings and part of speech (PoS) boundaries, leveraging the British National Corpus to train classifiers for predicting PoS tags based on word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999012351036072,
                    "sentence": "The main contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999167323112488,
                    "sentence": "1. Demonstrating that word embeddings contain information about PoS affiliation: The authors show that distributional vectors can be used to predict PoS tags with high accuracy, indicating that word embeddings capture meaningful information about PoS classes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999174475669861,
                    "sentence": "2. Identifying word groups with distributional patterns different from other words of the same part of speech: The authors discover \"outliers\" that exhibit behavior similar to another part of speech, revealing hidden inconsistencies in the annotation process or guidelines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999307990074158,
                    "sentence": "3. Showing that information about PoS is distributed among dozens of vector components: The authors find that PoS affiliation is not limited to one or two specific features, but rather is distributed among many components of the word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999284148216248,
                    "sentence": "The strengths of this paper include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999467134475708,
                    "sentence": "1. Robust experimental design: The authors use a large and well-studied corpus, and employ a range of evaluation metrics to demonstrate the effectiveness of their approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998640418052673,
                    "sentence": "2. Interesting insights into PoS boundaries: The authors provide a detailed analysis of the \"outliers\" and errors in the classification results, shedding light on the complexities of PoS boundaries and the limitations of current annotation schemes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99980628490448,
                    "sentence": "3. Potential applications: The authors suggest that their approach could be used to improve the performance of PoS taggers, particularly in resource-poor languages, and to detect annotation errors in corpora.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999145269393921,
                    "sentence": "The weaknesses of this paper include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999314546585083,
                    "sentence": "1. Limited generalizability: The authors only experiment with English, and it is unclear whether their findings will generalize to other languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999946653842926,
                    "sentence": "2. Dependence on specific hyperparameters: The authors use a specific set of hyperparameters for training the distributional model, and it is unclear how sensitive their results are to these choices.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999564290046692,
                    "sentence": "3. Lack of comparison to state-of-the-art PoS taggers: The authors do not compare their approach to current state-of-the-art PoS taggers, making it difficult to evaluate the relative effectiveness of their approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984480738639832,
                    "sentence": "Questions to authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996945261955261,
                    "sentence": "1. How do the authors plan to extend their approach to other languages, and what challenges do they anticipate in doing so?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991218447685242,
                    "sentence": "2. Can the authors provide more details on the specific hyperparameters used for training the distributional model, and how they were chosen?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993034601211548,
                    "sentence": "3. How do the authors plan to address the issue of annotation errors in corpora, and what strategies do they propose for improving the accuracy of PoS tagging?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper explores the interaction between word embeddings and part of speech (PoS) boundaries, leveraging the British National Corpus to train classifiers for predicting PoS tags based on word embeddings. The main contributions of this work are:\n1. Demonstrating that word embeddings contain information about PoS affiliation: The authors show that distributional vectors can be used to predict PoS tags with high accuracy, indicating that word embeddings capture meaningful information about PoS classes.\n2. Identifying word groups with distributional patterns different from other words of the same part of speech: The authors discover \"outliers\" that exhibit behavior similar to another part of speech, revealing hidden inconsistencies in the annotation process or guidelines.\n3. Showing that information about PoS is distributed among dozens of vector components: The authors find that PoS affiliation is not limited to one or two specific features, but rather is distributed among many components of the word embeddings.\nThe strengths of this paper include:\n1. Robust experimental design: The authors use a large and well-studied corpus, and employ a range of evaluation metrics to demonstrate the effectiveness of their approach.\n2. Interesting insights into PoS boundaries: The authors provide a detailed analysis of the \"outliers\" and errors in the classification results, shedding light on the complexities of PoS boundaries and the limitations of current annotation schemes.\n3. Potential applications: The authors suggest that their approach could be used to improve the performance of PoS taggers, particularly in resource-poor languages, and to detect annotation errors in corpora.\nThe weaknesses of this paper include:\n1. Limited generalizability: The authors only experiment with English, and it is unclear whether their findings will generalize to other languages.\n2. Dependence on specific hyperparameters: The authors use a specific set of hyperparameters for training the distributional model, and it is unclear how sensitive their results are to these choices.\n3. Lack of comparison to state-of-the-art PoS taggers: The authors do not compare their approach to current state-of-the-art PoS taggers, making it difficult to evaluate the relative effectiveness of their approach.\nQuestions to authors:\n1. How do the authors plan to extend their approach to other languages, and what challenges do they anticipate in doing so?\n2. Can the authors provide more details on the specific hyperparameters used for training the distributional model, and how they were chosen?\n3. How do the authors plan to address the issue of annotation errors in corpora, and what strategies do they propose for improving the accuracy of PoS tagging?"
        }
    ]
}