{
    "version": "2025-01-09-base",
    "scanId": "0b7069a0-6e9c-4414-a254-7b2aaf9af291",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999391436576843,
                    "sentence": "This paper proposes a language-independent model for cross-lingual Named Entity Recognition (NER) using a cross-lingual wikifier to disambiguate every n-gram.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999535083770752,
                    "sentence": "The model works on all languages in Wikipedia and requires only a Wikipedia dump.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999487996101379,
                    "sentence": "The authors evaluate their model on a wide range of languages in both monolingual and cross-lingual settings, showing significant improvements over strong baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999591112136841,
                    "sentence": "The main contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999313950538635,
                    "sentence": "1. Language-independent features: The authors introduce a novel approach to generating language-independent features for NER using a cross-lingual wikifier, which grounds words and phrases to the English Wikipedia.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999480843544006,
                    "sentence": "2. Cross-lingual NER model: The proposed model can be applied to all languages in Wikipedia, outperforming comparable approaches on the standard CoNLL datasets and low-resource languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999369382858276,
                    "sentence": "3. Improved monolingual NER: The authors demonstrate that their language-independent features can also enhance monolingual NER systems, yielding improved results for all 9 languages evaluated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999271035194397,
                    "sentence": "The strengths of this paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999276995658875,
                    "sentence": "1. Effective use of Wikipedia: The authors leverage Wikipedia as a source of information for each language, allowing them to develop a language-independent model that can be applied to a wide range of languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99998939037323,
                    "sentence": "2. State-of-the-art results: The proposed model achieves state-of-the-art results on several languages, including low-resource languages, demonstrating its effectiveness in cross-lingual NER.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999978244304657,
                    "sentence": "3. Flexibility and scalability: The model can be trained on multiple source languages, and the authors show that this can further improve results, making it a flexible and scalable approach to cross-lingual NER.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986379742622375,
                    "sentence": "The weaknesses of this paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999118447303772,
                    "sentence": "1. Dependence on Wikipedia size: The quality of the wikifier features depends on the size of the Wikipedia for the target language, which can be a limitation for low-resource languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987969994544983,
                    "sentence": "2. Limited use of Wikipedia information: The authors only use Wikipedia categories and FreeBase types as features, leaving other potentially useful information from Wikipedia unexplored.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993036389350891,
                    "sentence": "3. No comparison to other multilingual NER approaches: The authors do not compare their model to other multilingual NER approaches, such as parallel projection or generating training data from Wikipedia automatically, which could provide a more comprehensive evaluation of their model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.992225706577301,
                    "sentence": "Questions to authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992432594299316,
                    "sentence": "1. How do the authors plan to address the limitation of dependence on Wikipedia size for low-resource languages?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998191773891449,
                    "sentence": "2. Can the authors explore other techniques for multilingual NER, such as parallel projection or generating training data from Wikipedia automatically, and integrate them into their model?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994438886642456,
                    "sentence": "3. How do the authors plan to evaluate their model on a larger set of languages, including those with limited Wikipedia presence?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a language-independent model for cross-lingual Named Entity Recognition (NER) using a cross-lingual wikifier to disambiguate every n-gram. The model works on all languages in Wikipedia and requires only a Wikipedia dump. The authors evaluate their model on a wide range of languages in both monolingual and cross-lingual settings, showing significant improvements over strong baselines.\nThe main contributions of this work are:\n1. Language-independent features: The authors introduce a novel approach to generating language-independent features for NER using a cross-lingual wikifier, which grounds words and phrases to the English Wikipedia.\n2. Cross-lingual NER model: The proposed model can be applied to all languages in Wikipedia, outperforming comparable approaches on the standard CoNLL datasets and low-resource languages.\n3. Improved monolingual NER: The authors demonstrate that their language-independent features can also enhance monolingual NER systems, yielding improved results for all 9 languages evaluated.\nThe strengths of this paper are:\n1. Effective use of Wikipedia: The authors leverage Wikipedia as a source of information for each language, allowing them to develop a language-independent model that can be applied to a wide range of languages.\n2. State-of-the-art results: The proposed model achieves state-of-the-art results on several languages, including low-resource languages, demonstrating its effectiveness in cross-lingual NER.\n3. Flexibility and scalability: The model can be trained on multiple source languages, and the authors show that this can further improve results, making it a flexible and scalable approach to cross-lingual NER.\nThe weaknesses of this paper are:\n1. Dependence on Wikipedia size: The quality of the wikifier features depends on the size of the Wikipedia for the target language, which can be a limitation for low-resource languages.\n2. Limited use of Wikipedia information: The authors only use Wikipedia categories and FreeBase types as features, leaving other potentially useful information from Wikipedia unexplored.\n3. No comparison to other multilingual NER approaches: The authors do not compare their model to other multilingual NER approaches, such as parallel projection or generating training data from Wikipedia automatically, which could provide a more comprehensive evaluation of their model.\nQuestions to authors:\n1. How do the authors plan to address the limitation of dependence on Wikipedia size for low-resource languages?\n2. Can the authors explore other techniques for multilingual NER, such as parallel projection or generating training data from Wikipedia automatically, and integrate them into their model?\n3. How do the authors plan to evaluate their model on a larger set of languages, including those with limited Wikipedia presence?"
        }
    ]
}