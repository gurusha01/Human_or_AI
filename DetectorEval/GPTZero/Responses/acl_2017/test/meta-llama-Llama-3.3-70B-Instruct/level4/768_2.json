{
    "version": "2025-01-09-base",
    "scanId": "d1008732-b28a-42c8-8daf-74b8408290e9",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9901283979415894,
                    "sentence": "This paper presents a methodology for identifying lexical entailment, specifically hypernymy, within a given context.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.985055148601532,
                    "sentence": "The approach involves representing each context through the averaging, min-pooling, and max-pooling of word embeddings, which are then combined with the target word's embedding via element-wise multiplication.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.983413815498352,
                    "sentence": "The contextual representations of the left-hand-side and right-hand-side arguments are concatenated, forming a single vectorial input that is subsequently fed into a logistic regression classifier.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9907252788543701,
                    "sentence": "However, I have identified two significant weaknesses in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9898016452789307,
                    "sentence": "Firstly, the classification model employed, which involves concatenating the input vectors and using a linear classifier, has been previously demonstrated to be inherently incapable of learning lexical inference relations, as shown in \"Do Supervised Distributional Methods Really Learn Lexical Inference Relations?\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9903789162635803,
                    "sentence": "by Levy et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9768036603927612,
                    "sentence": "(2015).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9849556088447571,
                    "sentence": "Secondly, the paper makes claims of superiority that are not substantiated by the quantitative results, and there are several issues related to clarity and experimental setup that suggest the paper is not yet fully developed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9858465194702148,
                    "sentence": "Regarding the classification model, it is worth noting that concatenating two word vectors as input for a linear classifier has been mathematically proven to be incapable of learning a relation between words, as demonstrated by Levy et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9684147834777832,
                    "sentence": "(2015).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9826502203941345,
                    "sentence": "The motivation behind using this model in the contextual setting is unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9874793887138367,
                    "sentence": "Although the addition of similarity features may partially mitigate this limitation, these features are symmetric, which raises questions about their ability to detect entailment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9857296347618103,
                    "sentence": "I remain unconvinced that this is a suitable classification model for the task at hand.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9867993593215942,
                    "sentence": "The authors' claims of superiority over context2vec are also not supported by the evidence presented.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993559122085571,
                    "sentence": "The best results in tables 3 and 4 (excluding PPDB features) are achieved by a variant that does not utilize the proposed contextual representation, but rather employs the context2vec representation for the word type.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992977380752563,
                    "sentence": "Furthermore, the comparison between the two methods is flawed due to the use of pre-trained embeddings and parameters that were tuned on different datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991151094436646,
                    "sentence": "The largest performance gain appears to come from the addition of similarity features, rather than the proposed context representation, which is not discussed in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985345005989075,
                    "sentence": "In addition to these concerns, I have several miscellaneous comments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996960163116455,
                    "sentence": "I appreciate the use of the WordNet dataset and the inclusion of example sentences.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994462728500366,
                    "sentence": "However, I am unclear about the relevance and reasonableness of the task of cross-lingual lexical entailment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987294673919678,
                    "sentence": "The paper lacks basic baselines, such as the \"all true\" baseline, and the context-agnostic symmetric cosine similarity of the two target words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994515180587769,
                    "sentence": "The tables are difficult to read and lack clear captions, making it challenging to understand the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998448371887207,
                    "sentence": "The description of the PPDB-specific features is unclear, and I was unable to comprehend section 8.1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992510676383972,
                    "sentence": "Table 4 is overly dense, and there are errors, such as the F1 score of the \"random\" baseline, which should be 0.25.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983646869659424,
                    "sentence": "In response to the authors' rebuttal, I appreciate their efforts to address my comments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9968852400779724,
                    "sentence": "However, I still have concerns regarding the base model, which I believe is mathematically incapable of learning a relation, rather than simply learning prototypical hypernyms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9946163892745972,
                    "sentence": "I also have a different interpretation of tables 3 and 4, which may be a clarity issue, but it prevents me from understanding how the claim of improved performance using contextual representations is supported.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9958130121231079,
                    "sentence": "Other factors, such as similarity features, appear to have a greater impact on performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a methodology for identifying lexical entailment, specifically hypernymy, within a given context. The approach involves representing each context through the averaging, min-pooling, and max-pooling of word embeddings, which are then combined with the target word's embedding via element-wise multiplication. The contextual representations of the left-hand-side and right-hand-side arguments are concatenated, forming a single vectorial input that is subsequently fed into a logistic regression classifier.\nHowever, I have identified two significant weaknesses in the paper. Firstly, the classification model employed, which involves concatenating the input vectors and using a linear classifier, has been previously demonstrated to be inherently incapable of learning lexical inference relations, as shown in \"Do Supervised Distributional Methods Really Learn Lexical Inference Relations?\" by Levy et al. (2015). Secondly, the paper makes claims of superiority that are not substantiated by the quantitative results, and there are several issues related to clarity and experimental setup that suggest the paper is not yet fully developed.\nRegarding the classification model, it is worth noting that concatenating two word vectors as input for a linear classifier has been mathematically proven to be incapable of learning a relation between words, as demonstrated by Levy et al. (2015). The motivation behind using this model in the contextual setting is unclear. Although the addition of similarity features may partially mitigate this limitation, these features are symmetric, which raises questions about their ability to detect entailment. I remain unconvinced that this is a suitable classification model for the task at hand.\nThe authors' claims of superiority over context2vec are also not supported by the evidence presented. The best results in tables 3 and 4 (excluding PPDB features) are achieved by a variant that does not utilize the proposed contextual representation, but rather employs the context2vec representation for the word type. Furthermore, the comparison between the two methods is flawed due to the use of pre-trained embeddings and parameters that were tuned on different datasets. The largest performance gain appears to come from the addition of similarity features, rather than the proposed context representation, which is not discussed in the paper.\nIn addition to these concerns, I have several miscellaneous comments. I appreciate the use of the WordNet dataset and the inclusion of example sentences. However, I am unclear about the relevance and reasonableness of the task of cross-lingual lexical entailment. The paper lacks basic baselines, such as the \"all true\" baseline, and the context-agnostic symmetric cosine similarity of the two target words. The tables are difficult to read and lack clear captions, making it challenging to understand the results. The description of the PPDB-specific features is unclear, and I was unable to comprehend section 8.1. Table 4 is overly dense, and there are errors, such as the F1 score of the \"random\" baseline, which should be 0.25.\nIn response to the authors' rebuttal, I appreciate their efforts to address my comments. However, I still have concerns regarding the base model, which I believe is mathematically incapable of learning a relation, rather than simply learning prototypical hypernyms. I also have a different interpretation of tables 3 and 4, which may be a clarity issue, but it prevents me from understanding how the claim of improved performance using contextual representations is supported. Other factors, such as similarity features, appear to have a greater impact on performance."
        }
    ]
}