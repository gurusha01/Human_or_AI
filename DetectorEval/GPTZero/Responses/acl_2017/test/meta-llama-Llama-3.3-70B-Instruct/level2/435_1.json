{
    "version": "2025-01-09-base",
    "scanId": "30b19286-72ac-45ca-a2d1-72d090f971ea",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999797940254211,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999973714351654,
                    "sentence": "The paper proposes a neural network architecture for causality classification, a task that involves identifying whether there is a causal relation between two events.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999962568283081,
                    "sentence": "The authors argue that the encoding of the meaning of the two events is required for the disambiguation of their causal meaning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999716281890869,
                    "sentence": "They evaluate their system on the AltLex corpus, which is a dataset of sentences with annotated causal relations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999890327453613,
                    "sentence": "The results show that their system outperforms the state-of-the-art system on this corpus.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999906420707703,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999984085559845,
                    "sentence": "1. Neural Network Architecture: The authors propose a neural network architecture with two inputs, one for the first event and the other for the lexical marker and the second event.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999639987945557,
                    "sentence": "The architecture uses Long Short-Term Memory (LSTM) recurrent neural networks to encode the sequential information of the input text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999783635139465,
                    "sentence": "2. Encoding of Causal Meaning: The authors demonstrate that the encoding of the causal meaning of the two events is required for a suitable disambiguation of causality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999741315841675,
                    "sentence": "They show that their system can correctly disambiguate the causal meaning of sentences with ambiguous lexical markers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999818801879883,
                    "sentence": "3. State-of-the-Art Results: The authors report state-of-the-art results on the AltLex corpus, outperforming the previous best system by 2.13% in terms of F1 score.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999624490737915,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999730587005615,
                    "sentence": "1. Effective Use of LSTM: The authors effectively use LSTM to encode the sequential information of the input text, which is suitable for the task of causality classification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999478459358215,
                    "sentence": "2. Robustness to Ambiguity: The authors demonstrate that their system is robust to ambiguity in lexical markers, which is a common challenge in causality classification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99997878074646,
                    "sentence": "3. State-of-the-Art Results: The authors report state-of-the-art results on the AltLex corpus, which demonstrates the effectiveness of their system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999879002571106,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997880458831787,
                    "sentence": "1. Limited Corpus: The authors only evaluate their system on the AltLex corpus, which may not be representative of all types of causal relations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999703764915466,
                    "sentence": "2. Lack of Interpretability: The authors do not provide any interpretation of the results, which makes it difficult to understand why their system is effective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999502897262573,
                    "sentence": "3. Dependence on Word Embeddings: The authors rely on pre-trained word embeddings, which may not capture the nuances of causal relations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995419979095459,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998278617858887,
                    "sentence": "1. How do the authors plan to extend their system to handle more complex causal relations, such as those involving multiple events or entities?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998807311058044,
                    "sentence": "2. Can the authors provide more insight into why their system is effective, such as by analyzing the learned representations or the attention mechanisms?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998746514320374,
                    "sentence": "3. How do the authors plan to address the limitation of relying on pre-trained word embeddings, such as by using more advanced embedding techniques or training their own embeddings?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper proposes a neural network architecture for causality classification, a task that involves identifying whether there is a causal relation between two events. The authors argue that the encoding of the meaning of the two events is required for the disambiguation of their causal meaning. They evaluate their system on the AltLex corpus, which is a dataset of sentences with annotated causal relations. The results show that their system outperforms the state-of-the-art system on this corpus.\nMain Contributions\n1. Neural Network Architecture: The authors propose a neural network architecture with two inputs, one for the first event and the other for the lexical marker and the second event. The architecture uses Long Short-Term Memory (LSTM) recurrent neural networks to encode the sequential information of the input text.\n2. Encoding of Causal Meaning: The authors demonstrate that the encoding of the causal meaning of the two events is required for a suitable disambiguation of causality. They show that their system can correctly disambiguate the causal meaning of sentences with ambiguous lexical markers.\n3. State-of-the-Art Results: The authors report state-of-the-art results on the AltLex corpus, outperforming the previous best system by 2.13% in terms of F1 score.\nStrengths\n1. Effective Use of LSTM: The authors effectively use LSTM to encode the sequential information of the input text, which is suitable for the task of causality classification.\n2. Robustness to Ambiguity: The authors demonstrate that their system is robust to ambiguity in lexical markers, which is a common challenge in causality classification.\n3. State-of-the-Art Results: The authors report state-of-the-art results on the AltLex corpus, which demonstrates the effectiveness of their system.\nWeaknesses\n1. Limited Corpus: The authors only evaluate their system on the AltLex corpus, which may not be representative of all types of causal relations.\n2. Lack of Interpretability: The authors do not provide any interpretation of the results, which makes it difficult to understand why their system is effective.\n3. Dependence on Word Embeddings: The authors rely on pre-trained word embeddings, which may not capture the nuances of causal relations.\nQuestions to Authors\n1. How do the authors plan to extend their system to handle more complex causal relations, such as those involving multiple events or entities?\n2. Can the authors provide more insight into why their system is effective, such as by analyzing the learned representations or the attention mechanisms?\n3. How do the authors plan to address the limitation of relying on pre-trained word embeddings, such as by using more advanced embedding techniques or training their own embeddings?"
        }
    ]
}