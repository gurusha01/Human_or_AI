{
    "version": "2025-01-09-base",
    "scanId": "6dad242f-de5c-4107-bda9-19ce2d9a95d3",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999836087226868,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999512434005737,
                    "sentence": "The paper proposes a novel approach to detecting lexical entailment in context, where the meaning of words is grounded by exemplar sentences.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999768733978271,
                    "sentence": "The authors introduce two new datasets, CONTEXT-PPDB and CONTEXT-WN, to evaluate their approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999736547470093,
                    "sentence": "They propose contextualized word representations and similarity features to capture the nuances of word meaning in context.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999706149101257,
                    "sentence": "The experiments demonstrate significant improvements over context-agnostic baselines on both monolingual and cross-lingual datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999700784683228,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999913573265076,
                    "sentence": "1. Contextualized Word Representations: The authors propose a method to transform context-agnostic word representations into contextualized representations using convolutional filters and element-wise product.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999898672103882,
                    "sentence": "2. Similarity Features: They introduce similarity features to capture the relationships between words and contexts, including cosine similarity, dot product, and Euclidean distance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999843835830688,
                    "sentence": "3. New Datasets: The authors introduce two new datasets, CONTEXT-PPDB and CONTEXT-WN, to evaluate their approach and provide a benchmark for future research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999664425849915,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999828338623047,
                    "sentence": "1. Significant Improvements: The proposed approach achieves significant improvements over context-agnostic baselines on both monolingual and cross-lingual datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999740719795227,
                    "sentence": "2. Robustness to Context: The experiments demonstrate that the proposed features are sensitive to changes in context and capture the directionality of entailment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999982476234436,
                    "sentence": "3. Generalizability: The approach is shown to be effective in both monolingual and cross-lingual settings, demonstrating its potential for real-world applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999593496322632,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999647736549377,
                    "sentence": "1. Limited Analysis: The paper could benefit from a more in-depth analysis of the results, including a detailed examination of the errors and limitations of the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999651908874512,
                    "sentence": "2. Comparison to State-of-the-Art: While the paper compares its results to previous work, a more comprehensive comparison to state-of-the-art models and approaches would be beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999532103538513,
                    "sentence": "3. Lack of Theoretical Justification: The paper could benefit from a more detailed theoretical justification of the proposed approach, including a discussion of the underlying linguistic and cognitive assumptions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9899337887763977,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977793097496033,
                    "sentence": "1. How do the proposed contextualized word representations and similarity features capture the nuances of word meaning in context?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9956429600715637,
                    "sentence": "2. Can the authors provide more details on the annotation process for the new datasets and the inter-annotator agreement?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975263476371765,
                    "sentence": "3. How do the authors plan to address the limitations of the approach, including the potential for overfitting and the need for more robust evaluation metrics?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984930238596827,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984930238596827,
                "mixed": 0.001506976140317253
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984930238596827,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984930238596827,
                    "human": 0,
                    "mixed": 0.001506976140317253
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper proposes a novel approach to detecting lexical entailment in context, where the meaning of words is grounded by exemplar sentences. The authors introduce two new datasets, CONTEXT-PPDB and CONTEXT-WN, to evaluate their approach. They propose contextualized word representations and similarity features to capture the nuances of word meaning in context. The experiments demonstrate significant improvements over context-agnostic baselines on both monolingual and cross-lingual datasets.\nMain Contributions\n1. Contextualized Word Representations: The authors propose a method to transform context-agnostic word representations into contextualized representations using convolutional filters and element-wise product.\n2. Similarity Features: They introduce similarity features to capture the relationships between words and contexts, including cosine similarity, dot product, and Euclidean distance.\n3. New Datasets: The authors introduce two new datasets, CONTEXT-PPDB and CONTEXT-WN, to evaluate their approach and provide a benchmark for future research.\nStrengths\n1. Significant Improvements: The proposed approach achieves significant improvements over context-agnostic baselines on both monolingual and cross-lingual datasets.\n2. Robustness to Context: The experiments demonstrate that the proposed features are sensitive to changes in context and capture the directionality of entailment.\n3. Generalizability: The approach is shown to be effective in both monolingual and cross-lingual settings, demonstrating its potential for real-world applications.\nWeaknesses\n1. Limited Analysis: The paper could benefit from a more in-depth analysis of the results, including a detailed examination of the errors and limitations of the approach.\n2. Comparison to State-of-the-Art: While the paper compares its results to previous work, a more comprehensive comparison to state-of-the-art models and approaches would be beneficial.\n3. Lack of Theoretical Justification: The paper could benefit from a more detailed theoretical justification of the proposed approach, including a discussion of the underlying linguistic and cognitive assumptions.\nQuestions to Authors\n1. How do the proposed contextualized word representations and similarity features capture the nuances of word meaning in context?\n2. Can the authors provide more details on the annotation process for the new datasets and the inter-annotator agreement?\n3. How do the authors plan to address the limitations of the approach, including the potential for overfitting and the need for more robust evaluation metrics?"
        }
    ]
}