{
    "version": "2025-01-09-base",
    "scanId": "ca383c62-f6c0-410e-9e93-f5fe6a585bc4",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9991261959075928,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999076724052429,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999324083328247,
                    "sentence": "This paper addresses the task of lexical entailment in context, introducing a novel dataset derived from WordNet (CONTEXT-WN) and proposing a \"context relevance mask\" to contextualize word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999352097511292,
                    "sentence": "The authors combine these masked representations with similarity features (e.g., cosine similarity, Euclidean distance) to improve entailment detection.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999493360519409,
                    "sentence": "The paper also evaluates its approach on a cross-lingual entailment dataset and establishes a new state-of-the-art on the CONTEXT-PPDB dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996616244316101,
                    "sentence": "The main contributions, as I see them, are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996365308761597,
                    "sentence": "1. Dataset Creation: The CONTEXT-WN dataset is a significant contribution, as it provides a controlled and scalable benchmark for lexical entailment in context.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992974400520325,
                    "sentence": "Its focus on hypernymy and the use of exemplar sentences from WordNet make it a valuable resource for future research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994887709617615,
                    "sentence": "2. Contextualized Representations: The \"context relevance mask\" is a novel adaptation of existing sequence encoding techniques, which highlights salient contextual dimensions for word representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995826482772827,
                    "sentence": "3. Empirical Results: The paper demonstrates marginal improvements over state-of-the-art methods on CONTEXT-PPDB and introduces a cross-lingual evaluation, which is a less-explored direction in lexical entailment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992854595184326,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989950060844421,
                    "sentence": "1. Novel Dataset: The CONTEXT-WN dataset is well-motivated and fills a gap in evaluating lexical entailment in context.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980692267417908,
                    "sentence": "Its design ensures sensitivity to word sense and entailment directionality, making it a robust benchmark.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994279146194458,
                    "sentence": "2. Comprehensive Evaluation: The authors evaluate their method on multiple datasets (CONTEXT-PPDB, CONTEXT-WN, and a cross-lingual dataset), demonstrating the generalizability of their approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992749691009521,
                    "sentence": "3. Combination of Features: The inclusion of similarity features (cosine, Euclidean, etc.)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999194860458374,
                    "sentence": "alongside contextualized representations is effective, yielding consistent performance gains across datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992749691009521,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991136193275452,
                    "sentence": "1. Ambiguity in Contributions: The contribution of the masked representations is unclear, as the best results often rely on similarity features rather than the masked vectors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997173547744751,
                    "sentence": "No results are presented without similarity features, making it difficult to isolate the impact of the proposed masking method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996778964996338,
                    "sentence": "2. Dataset Quality Concerns: The CONTEXT-WN dataset may suffer from potential false negatives due to semantically close synsets being permuted into negative examples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993538856506348,
                    "sentence": "This could affect the reliability of the evaluation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9926491975784302,
                    "sentence": "3. Cross-Lingual Task Motivation: The cross-lingual evaluation is poorly motivated and lacks sufficient explanation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9928199052810669,
                    "sentence": "The choice of bilingual embeddings and their alignment with the task is not adequately justified.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9900678396224976,
                    "sentence": "4. Clarity on Novelty: The paper does not clearly distinguish which similarity measures and features are novel versus borrowed from prior work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9933216571807861,
                    "sentence": "This lack of clarity undermines the originality of the methodological contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9926417469978333,
                    "sentence": "5. Metric Interpretation: The intuition behind the Macro-F1 metric and its relation to context sensitivity is not well-explained, leaving its significance unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.837894856929779,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7098976969718933,
                    "sentence": "1. Could you provide results isolating the performance of the masked representations without similarity features?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.81590735912323,
                    "sentence": "This would clarify their contribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5600643157958984,
                    "sentence": "2. How do you address the potential issue of false negatives in the CONTEXT-WN dataset caused by semantically close synsets being treated as negatives?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5016781687736511,
                    "sentence": "3. What is the rationale for including the cross-lingual task, and how do you justify the use of the specific bilingual embeddings (BiVec and BiCVM)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5085599422454834,
                    "sentence": "4. Can you elaborate on the novelty of the similarity features used?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.2731403708457947,
                    "sentence": "Are any of them introduced for the first time in this work?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.45069512724876404,
                    "sentence": "Recommendation",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.4103374779224396,
                    "sentence": "While the paper introduces a valuable dataset and demonstrates empirical improvements, the methodological contributions are somewhat unclear, and the dataset has potential quality issues.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.24566583335399628,
                    "sentence": "I recommend acceptance with minor revisions, contingent on clarifying the contributions of the masked representations and addressing concerns about dataset quality and cross-lingual task motivation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.9658502932045533,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9658502932045533,
                "mixed": 0.034149706795446697
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9658502932045533,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9658502932045533,
                    "human": 0,
                    "mixed": 0.034149706795446697
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary and Contributions\nThis paper addresses the task of lexical entailment in context, introducing a novel dataset derived from WordNet (CONTEXT-WN) and proposing a \"context relevance mask\" to contextualize word embeddings. The authors combine these masked representations with similarity features (e.g., cosine similarity, Euclidean distance) to improve entailment detection. The paper also evaluates its approach on a cross-lingual entailment dataset and establishes a new state-of-the-art on the CONTEXT-PPDB dataset. The main contributions, as I see them, are:\n1. Dataset Creation: The CONTEXT-WN dataset is a significant contribution, as it provides a controlled and scalable benchmark for lexical entailment in context. Its focus on hypernymy and the use of exemplar sentences from WordNet make it a valuable resource for future research.\n2. Contextualized Representations: The \"context relevance mask\" is a novel adaptation of existing sequence encoding techniques, which highlights salient contextual dimensions for word representations.\n3. Empirical Results: The paper demonstrates marginal improvements over state-of-the-art methods on CONTEXT-PPDB and introduces a cross-lingual evaluation, which is a less-explored direction in lexical entailment.\nStrengths\n1. Novel Dataset: The CONTEXT-WN dataset is well-motivated and fills a gap in evaluating lexical entailment in context. Its design ensures sensitivity to word sense and entailment directionality, making it a robust benchmark.\n2. Comprehensive Evaluation: The authors evaluate their method on multiple datasets (CONTEXT-PPDB, CONTEXT-WN, and a cross-lingual dataset), demonstrating the generalizability of their approach.\n3. Combination of Features: The inclusion of similarity features (cosine, Euclidean, etc.) alongside contextualized representations is effective, yielding consistent performance gains across datasets.\nWeaknesses\n1. Ambiguity in Contributions: The contribution of the masked representations is unclear, as the best results often rely on similarity features rather than the masked vectors. No results are presented without similarity features, making it difficult to isolate the impact of the proposed masking method.\n2. Dataset Quality Concerns: The CONTEXT-WN dataset may suffer from potential false negatives due to semantically close synsets being permuted into negative examples. This could affect the reliability of the evaluation.\n3. Cross-Lingual Task Motivation: The cross-lingual evaluation is poorly motivated and lacks sufficient explanation. The choice of bilingual embeddings and their alignment with the task is not adequately justified.\n4. Clarity on Novelty: The paper does not clearly distinguish which similarity measures and features are novel versus borrowed from prior work. This lack of clarity undermines the originality of the methodological contributions.\n5. Metric Interpretation: The intuition behind the Macro-F1 metric and its relation to context sensitivity is not well-explained, leaving its significance unclear.\nQuestions to Authors\n1. Could you provide results isolating the performance of the masked representations without similarity features? This would clarify their contribution.\n2. How do you address the potential issue of false negatives in the CONTEXT-WN dataset caused by semantically close synsets being treated as negatives?\n3. What is the rationale for including the cross-lingual task, and how do you justify the use of the specific bilingual embeddings (BiVec and BiCVM)?\n4. Can you elaborate on the novelty of the similarity features used? Are any of them introduced for the first time in this work?\nRecommendation\nWhile the paper introduces a valuable dataset and demonstrates empirical improvements, the methodological contributions are somewhat unclear, and the dataset has potential quality issues. I recommend acceptance with minor revisions, contingent on clarifying the contributions of the masked representations and addressing concerns about dataset quality and cross-lingual task motivation."
        }
    ]
}