{
    "version": "2025-01-09-base",
    "scanId": "575105f8-4079-4118-9f44-f49a68a1f51a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999263882637024,
                    "sentence": "Review of the Submission",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999700784683228,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999442100524902,
                    "sentence": "This paper introduces a novel approach to lexical entailment in context, addressing the limitations of traditional context-agnostic methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998893141746521,
                    "sentence": "The authors propose contextualized word representations derived from existing embeddings and combine them with word-context similarity features to detect entailment relationships.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999623894691467,
                    "sentence": "The primary contributions of the paper are as follows:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999391436576843,
                    "sentence": "1. Contextualized Representations for Lexical Entailment: The paper demonstrates an effective transformation of context-agnostic embeddings into contextualized representations using innovative masking techniques and convolutional operations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999333620071411,
                    "sentence": "These representations significantly outperform context-agnostic baselines in entailment tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998844861984253,
                    "sentence": "2. Novel Datasets and Evaluation Framework: The authors introduce two datasets, CONTEXT-PPDB and CONTEXT-WN, designed to evaluate lexical entailment in context.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998608827590942,
                    "sentence": "These datasets are well-constructed, with CONTEXT-WN offering a challenging benchmark for assessing sensitivity to context and entailment directionality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998902678489685,
                    "sentence": "A cross-lingual dataset further highlights the generalizability of the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998841881752014,
                    "sentence": "3. Improved State-of-the-Art: The proposed features improve performance on CONTEXT-PPDB and CONTEXT-WN, as well as on a related semantic relation detection task, achieving a new state-of-the-art.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998888969421387,
                    "sentence": "The cross-lingual experiments further demonstrate the robustness of the approach across languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999216794967651,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998383522033691,
                    "sentence": "1. Well-Defined Approach: The methodology is clearly articulated, with strong motivations for design choices, such as the use of masked representations and similarity features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997139573097229,
                    "sentence": "The experimental design, including the split for CONTEXT-PPDB, is rigorous and reproducible.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998507499694824,
                    "sentence": "2. Generalizability: The paper convincingly demonstrates cross-dataset and cross-linguistic generalizability, with experiments on English and English-French datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999215006828308,
                    "sentence": "This highlights the robustness of the proposed features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998744130134583,
                    "sentence": "3. Innovative Negative Example Generation: The use of WordNet to automatically generate challenging negative examples is a novel and effective contribution, ensuring the datasets are both large-scale and contextually nuanced.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99985671043396,
                    "sentence": "4. Comprehensive Analysis: The paper provides detailed analyses of context sensitivity and entailment directionality, showcasing the strengths of the proposed features and their ability to capture nuanced semantic relationships.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999914824962616,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999881386756897,
                    "sentence": "1. Insufficient Clarity in Key Areas: While the paper is generally well-written, certain details are insufficiently described.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999045133590698,
                    "sentence": "For example, table captions lack descriptive clarity, and the explanation of word type features could be more explicit.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991627335548401,
                    "sentence": "2. Class Weights in CONTEXT-WN: The authors mention using class weights to address data imbalance but do not provide sufficient details on how these weights were determined or their impact on comparisons with prior work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990407228469849,
                    "sentence": "3. Figure Completeness: Figure 1 could be improved by including the \"mask\" representation for completeness, as it is a key component of the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994561076164246,
                    "sentence": "4. Minor Terminology Issues: There are minor errors in terminology, such as \"directionality 4,\" which should be corrected for clarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992743730545044,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999932050704956,
                    "sentence": "1. Could you clarify how class weights were determined for the CONTEXT-WN experiments and whether they influenced the comparison with prior baselines?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999833345413208,
                    "sentence": "2. Can you provide more details on the word type features used in the experiments?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998292922973633,
                    "sentence": "How do these features interact with the contextualized representations?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999730110168457,
                    "sentence": "3. Would adding richer features, such as second-order comparisons or asymmetric scoring functions, further improve performance?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992567300796509,
                    "sentence": "Have you considered these extensions?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983677864074707,
                    "sentence": "Additional Comments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997597336769104,
                    "sentence": "The paper is a strong contribution to the field of lexical entailment and contextualized word representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994577765464783,
                    "sentence": "Addressing the minor weaknesses and clarifying the experimental details would further strengthen the submission.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998751878738403,
                    "sentence": "The proposed approach has significant potential for broader applications in NLP tasks requiring fine-grained semantic understanding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9984800378301695,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984800378301695,
                "mixed": 0.0015199621698304396
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984800378301695,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984800378301695,
                    "human": 0,
                    "mixed": 0.0015199621698304396
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Submission\nSummary and Contributions\nThis paper introduces a novel approach to lexical entailment in context, addressing the limitations of traditional context-agnostic methods. The authors propose contextualized word representations derived from existing embeddings and combine them with word-context similarity features to detect entailment relationships. The primary contributions of the paper are as follows:\n1. Contextualized Representations for Lexical Entailment: The paper demonstrates an effective transformation of context-agnostic embeddings into contextualized representations using innovative masking techniques and convolutional operations. These representations significantly outperform context-agnostic baselines in entailment tasks.\n \n2. Novel Datasets and Evaluation Framework: The authors introduce two datasets, CONTEXT-PPDB and CONTEXT-WN, designed to evaluate lexical entailment in context. These datasets are well-constructed, with CONTEXT-WN offering a challenging benchmark for assessing sensitivity to context and entailment directionality. A cross-lingual dataset further highlights the generalizability of the proposed approach.\n3. Improved State-of-the-Art: The proposed features improve performance on CONTEXT-PPDB and CONTEXT-WN, as well as on a related semantic relation detection task, achieving a new state-of-the-art. The cross-lingual experiments further demonstrate the robustness of the approach across languages.\nStrengths\n1. Well-Defined Approach: The methodology is clearly articulated, with strong motivations for design choices, such as the use of masked representations and similarity features. The experimental design, including the split for CONTEXT-PPDB, is rigorous and reproducible.\n2. Generalizability: The paper convincingly demonstrates cross-dataset and cross-linguistic generalizability, with experiments on English and English-French datasets. This highlights the robustness of the proposed features.\n3. Innovative Negative Example Generation: The use of WordNet to automatically generate challenging negative examples is a novel and effective contribution, ensuring the datasets are both large-scale and contextually nuanced.\n4. Comprehensive Analysis: The paper provides detailed analyses of context sensitivity and entailment directionality, showcasing the strengths of the proposed features and their ability to capture nuanced semantic relationships.\nWeaknesses\n1. Insufficient Clarity in Key Areas: While the paper is generally well-written, certain details are insufficiently described. For example, table captions lack descriptive clarity, and the explanation of word type features could be more explicit.\n2. Class Weights in CONTEXT-WN: The authors mention using class weights to address data imbalance but do not provide sufficient details on how these weights were determined or their impact on comparisons with prior work.\n3. Figure Completeness: Figure 1 could be improved by including the \"mask\" representation for completeness, as it is a key component of the proposed approach.\n4. Minor Terminology Issues: There are minor errors in terminology, such as \"directionality 4,\" which should be corrected for clarity.\nQuestions to Authors\n1. Could you clarify how class weights were determined for the CONTEXT-WN experiments and whether they influenced the comparison with prior baselines?\n2. Can you provide more details on the word type features used in the experiments? How do these features interact with the contextualized representations?\n3. Would adding richer features, such as second-order comparisons or asymmetric scoring functions, further improve performance? Have you considered these extensions?\nAdditional Comments\nThe paper is a strong contribution to the field of lexical entailment and contextualized word representations. Addressing the minor weaknesses and clarifying the experimental details would further strengthen the submission. The proposed approach has significant potential for broader applications in NLP tasks requiring fine-grained semantic understanding."
        }
    ]
}