{
    "version": "2025-01-09-base",
    "scanId": "f4d95efb-4cce-4f01-9aa0-fc7fde8b673f",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0044238208793103695,
                    "sentence": "- Summary",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0039852941408753395,
                    "sentence": "This paper introduces chunk-level architecture for existing NMT models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011834402568638325,
                    "sentence": "Three",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006125644315034151,
                    "sentence": "models are proposed to model the correlation between word and chunk modelling",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00798538327217102,
                    "sentence": "on the target side in the existing NMT models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0085367476567626,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007615906186401844,
                    "sentence": "The paper is well-written and clear about the proposed models and its",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011594794690608978,
                    "sentence": "contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016434846445918083,
                    "sentence": "The proposed models to incorporating chunk information into NMT models are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0061601074412465096,
                    "sentence": "novel and well-motivated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014036680571734905,
                    "sentence": "I think such models can be generally applicable for",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011247675865888596,
                    "sentence": "many other language pairs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03345568850636482,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02154008112847805,
                    "sentence": "There are some minor points, listed as follows:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016267526894807816,
                    "sentence": "1) Figure 1: I am a bit surprised that the function words dominate the content",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012432186864316463,
                    "sentence": "ones in a Japanese sentence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009788911789655685,
                    "sentence": "Sorry I may not understand Japanese.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00798356905579567,
                    "sentence": "2) In all equations, sequences/vectors (like matrices) should be represented",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011877726763486862,
                    "sentence": "as bold texts to distinguish from scalars, e.g., hi, xi, c, s,...",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.027129041031003,
                    "sentence": "3) Equation 12: sj-1 instead of sj.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017702408134937286,
                    "sentence": "4) Line 244: all encoder states should be referred to bidirectional RNN states.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006265738047659397,
                    "sentence": "5) Line 285: a bit confused about the phrase \"non-sequential information such",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004525760188698769,
                    "sentence": "as chunks\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018520306795835495,
                    "sentence": "Is chunk still sequential information??",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02295510098338127,
                    "sentence": "?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012037413194775581,
                    "sentence": "6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005388542078435421,
                    "sentence": "to indicate the word in a chunk.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015909342095255852,
                    "sentence": "7) Some questions for the experiments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013958366587758064,
                    "sentence": "Table 1: source language statistics?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0056911613792181015,
                    "sentence": "For the baselines, why not running a baseline (without using any chunk",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021646104753017426,
                    "sentence": "information) instead of using (Li et al., 2016) baseline (\"V_src\" is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03374559432268143,
                    "sentence": "different)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023729322478175163,
                    "sentence": "It would be easy to see the effect of chunk-based models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03501460701227188,
                    "sentence": "Did (Li",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.029867233708500862,
                    "sentence": "et al., 2016) and other baselines use the same pre-processing and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02864208072423935,
                    "sentence": "post-processing steps?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.030345868319272995,
                    "sentence": "Other baselines are not very comparable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03269263356924057,
                    "sentence": "After authors's",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.030678527429699898,
                    "sentence": "response, I still think that (Li et al., 2016) baseline can be a reference but",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02659965306520462,
                    "sentence": "the baseline from the existing model should be shown.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.026408642530441284,
                    "sentence": "Figure 5: baseline result will be useful for comparison?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021054107695817947,
                    "sentence": "chunks in the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022622613236308098,
                    "sentence": "translated examples are generated automatically by the model or manually by",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03649454191327095,
                    "sentence": "the authors?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023627303540706635,
                    "sentence": "Is it possible to compare the no.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022786526009440422,
                    "sentence": "of chunks generated by the model",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013064345344901085,
                    "sentence": "and by the bunsetsu-chunking toolkit?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.024570278823375702,
                    "sentence": "In that case, the chunk information for",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01727115735411644,
                    "sentence": "Dev and Test in Table 1 will be required.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021082881838083267,
                    "sentence": "BTW, the authors's response did not",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013656802475452423,
                    "sentence": "address my point here.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03040197864174843,
                    "sentence": "8) I am bit surprised about the beam size 20 used in the decoding process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017352934926748276,
                    "sentence": "I",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02845659665763378,
                    "sentence": "suppose large beam size is likely to make the model prefer shorter generated",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.019606130197644234,
                    "sentence": "sentences.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02507733553647995,
                    "sentence": "9) Past tenses should be used in the experiments, e.g.,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04633466154336929,
                    "sentence": "Line 558: We use (used)...",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03236128389835358,
                    "sentence": "Line 579-584: we perform (performed)... use (used)...",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014529703184962273,
                    "sentence": "...",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.028229843825101852,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022877398878335953,
                    "sentence": "Overall, this is a solid work - the first one tackling the chunk-based NMT;",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02092074416577816,
                    "sentence": "and it well deserves a slot at ACL.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 38,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 40,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 42,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 43,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 48,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 51,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 54,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 55,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 56,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 57,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 58,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 59,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 60,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 61,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.024461651786716186,
            "class_probabilities": {
                "human": 0.9754981327244504,
                "ai": 0.024461651786716186,
                "mixed": 4.0215488833363626e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9754981327244504,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.024461651786716186,
                    "human": 0.9754981327244504,
                    "mixed": 4.0215488833363626e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "- Summary\nThis paper introduces chunk-level architecture for existing NMT models. Three\nmodels are proposed to model the correlation between word and chunk modelling\non the target side in the existing NMT models. \n- Strengths:\nThe paper is well-written and clear about the proposed models and its\ncontributions. \nThe proposed models to incorporating chunk information into NMT models are\nnovel and well-motivated. I think such models can be generally applicable for\nmany other language pairs. \n- Weaknesses:\nThere are some minor points, listed as follows:\n1) Figure 1: I am a bit surprised that the function words dominate the content\nones in a Japanese sentence. Sorry I may not understand Japanese. \n2) In all equations, sequences/vectors (like matrices) should be represented\nas bold texts to distinguish from scalars, e.g., hi, xi, c, s, ...\n3) Equation 12: sj-1 instead of sj.\n4) Line 244: all encoder states should be referred to bidirectional RNN states.\n5) Line 285: a bit confused about the phrase \"non-sequential information such\nas chunks\". Is chunk still sequential information???\n6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k)\nto indicate the word in a chunk. \n7) Some questions for the experiments:\nTable 1: source language statistics? \nFor the baselines, why not running a baseline (without using any chunk\ninformation) instead of using (Li et al., 2016) baseline (\"V_src\" is\ndifferent)? It would be easy to see the effect of chunk-based models. Did (Li\net al., 2016) and other baselines use the same pre-processing and\npost-processing steps? Other baselines are not very comparable. After authors's\nresponse, I still think that (Li et al., 2016) baseline can be a reference but\nthe baseline from the existing model should be shown. \nFigure 5: baseline result will be useful for comparison? chunks in the\ntranslated examples are generated automatically by the model or manually by\nthe authors? Is it possible to compare the no. of chunks generated by the model\nand by the bunsetsu-chunking toolkit? In that case, the chunk information for\nDev and Test in Table 1 will be required. BTW, the authors's response did not\naddress my point here. \n8) I am bit surprised about the beam size 20 used in the decoding process. I\nsuppose large beam size is likely to make the model prefer shorter generated\nsentences. \n9) Past tenses should be used in the experiments, e.g.,\nLine 558: We use (used) ...\nLine 579-584: we perform (performed) ... use (used) ...\n...\n- General Discussion:\nOverall, this is a solid work - the first one tackling the chunk-based NMT;\nand it well deserves a slot at ACL."
        }
    ]
}