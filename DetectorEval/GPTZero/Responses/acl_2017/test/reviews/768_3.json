{
    "version": "2025-01-09-base",
    "scanId": "c418c98c-8d18-49b7-b55d-07901caa8c10",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.004303629510104656,
                    "sentence": "This paper addresses the task of lexical entailment detection in context, e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0024233246222138405,
                    "sentence": "is \"chess\" a kind of \"game\" given a sentence containing each of the words --",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006278387736529112,
                    "sentence": "relevant for QA.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003355005057528615,
                    "sentence": "The major contributions are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0035130032338202,
                    "sentence": "(1) a new dataset derived from WordNet using synset exemplar sentences, and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0023135789670050144,
                    "sentence": "(2) a \"context relevance mask\" for a word vector, accomplished by elementwise",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004162615165114403,
                    "sentence": "multiplication with feature vectors derived from the context sentence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0029041951056569815,
                    "sentence": "Fed to a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006021018140017986,
                    "sentence": "logistic regression classifier, the masked word vectors just beat state of the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004366620909422636,
                    "sentence": "art on entailment prediction on a PPDB-derived dataset from previous",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0022408636286854744,
                    "sentence": "literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004960228223353624,
                    "sentence": "Combined with other existing features, they beat state of the art",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004641382023692131,
                    "sentence": "by a few points.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0037667332217097282,
                    "sentence": "They also beats the baseline on the new WN-derived dataset,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003897283226251602,
                    "sentence": "although the best-scoring method on that dataset doesn't use the masked",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0025855989661067724,
                    "sentence": "representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0034217271022498608,
                    "sentence": "The paper also introduces some simple word similarity features (cosine,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0027690171264111996,
                    "sentence": "euclidean distance) which accompany other cross-context similarity features",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001902999123558402,
                    "sentence": "from previous literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004283200018107891,
                    "sentence": "All of the similarity features, together, improve the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005403080489486456,
                    "sentence": "classification results by a large amount, but the features in the present paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004068084992468357,
                    "sentence": "are a relatively small contribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00770589942112565,
                    "sentence": "The task is interesting, and the work seems to be correct as far as it goes,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004674556199461222,
                    "sentence": "but incremental.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005386511329561472,
                    "sentence": "The method of producing the mask vectors is taken from",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00676429457962513,
                    "sentence": "existing literature on encoding variable-length sequences into min/max/mean",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009520319290459156,
                    "sentence": "vectors, but I don't think they've been used as masks before, so this is novel.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013395313173532486,
                    "sentence": "However, excluding the PPDB features it looks like the best result does not use",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011443591676652431,
                    "sentence": "the representation introduced in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01707703061401844,
                    "sentence": "A few more specific points:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007774763740599155,
                    "sentence": "In the creation of the new Context-WN dataset, are there a lot of false",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004230458755046129,
                    "sentence": "negatives resulting from similar synsets in the \"permuted\" examples?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00441347761079669,
                    "sentence": "If you",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006667575798928738,
                    "sentence": "take word w, with synsets i and j, is it guaranteed that the exemplar context",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007564802188426256,
                    "sentence": "for a hypernym synset of j is a bad entailment context for i?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0021149669773876667,
                    "sentence": "What if i and j",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004478787537664175,
                    "sentence": "are semantically close?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003764509689062834,
                    "sentence": "Why does the masked representation hurt classification with the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005591697059571743,
                    "sentence": "context-agnostic word vectors (rows 3, 5 in Table 3) when row 1 does so well?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004454484209418297,
                    "sentence": "Wouldn't the classifier learn to ignore the context-agnostic features?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005823651794344187,
                    "sentence": "The paper should make clearer which similarity measures are new and which are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006809800397604704,
                    "sentence": "from previous literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00760967843234539,
                    "sentence": "It currently says that previous lit used the \"most",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008413122035562992,
                    "sentence": "salient\" similarity features, but that's not informative to the reader.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008547664619982243,
                    "sentence": "The paper should be clearer about the contribution of the masked vectors vs the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011396360583603382,
                    "sentence": "similarity features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010889772325754166,
                    "sentence": "It seems like similarity is doing most of the work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009678998962044716,
                    "sentence": "I don't understand the intuition behind the Macro-F1 measure, or how it relates",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010462788864970207,
                    "sentence": "to \"how sensitive are our models to changes in context\" -- what changes?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02139425277709961,
                    "sentence": "How do",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010951317846775055,
                    "sentence": "we expect Macro-F1 to compare with F1?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01264230813831091,
                    "sentence": "The cross-language task is not well motivated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02649540826678276,
                    "sentence": "Missing a relevant citation: Learning to Distinguish Hypernyms and Co-Hyponyms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022091787308454514,
                    "sentence": "Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09282129257917404,
                    "sentence": "COLING",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.030831899493932724,
                    "sentence": "2014. ==",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02135670743882656,
                    "sentence": "I have read the author response.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011391582898795605,
                    "sentence": "As noted in the original reviews, a quick",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009454199112951756,
                    "sentence": "examination of the tables shows that the similarity features make the largest",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010539836250245571,
                    "sentence": "contribution to the improvement in F-score on the two datasets (aside from PPDB",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008675504475831985,
                    "sentence": "features).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009389497339725494,
                    "sentence": "The author response makes the point that similarities include",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005425166804343462,
                    "sentence": "contextualized representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0061331759206950665,
                    "sentence": "However, the similarity features are a mixed",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0060887024737894535,
                    "sentence": "bag, including both contextualized and non-contextualized representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010903454385697842,
                    "sentence": "This",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010326656512916088,
                    "sentence": "would need to be teased out more (as acknowledged in the response).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01485492568463087,
                    "sentence": "Neither Table 3 nor 4 gives results using only the masked representations",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007148883305490017,
                    "sentence": "without the similarity features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012348486110568047,
                    "sentence": "This makes the contribution of the masked",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008214936591684818,
                    "sentence": "representations difficult to isolate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 36,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 38,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 40,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 41,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 43,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 44,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 45,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 48,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 51,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 55,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 56,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 58,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 59,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 60,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 62,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 64,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 66,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 67,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 68,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 70,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.0375246461941804,
            "class_probabilities": {
                "human": 0.9624347023880775,
                "ai": 0.0375246461941804,
                "mixed": 4.065141774218333e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9624347023880775,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.0375246461941804,
                    "human": 0.9624347023880775,
                    "mixed": 4.065141774218333e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper addresses the task of lexical entailment detection in context, e.g.\nis \"chess\" a kind of \"game\" given a sentence containing each of the words --\nrelevant for QA. The major contributions are:\n(1) a new dataset derived from WordNet using synset exemplar sentences, and \n(2) a \"context relevance mask\" for a word vector, accomplished by elementwise\nmultiplication with feature vectors derived from the context sentence. Fed to a\nlogistic regression classifier, the masked word vectors just beat state of the\nart on entailment prediction on a PPDB-derived dataset from previous\nliterature. Combined with other existing features, they beat state of the art\nby a few points. They also beats the baseline on the new WN-derived dataset,\nalthough the best-scoring method on that dataset doesn't use the masked\nrepresentations.\nThe paper also introduces some simple word similarity features (cosine,\neuclidean distance) which accompany other cross-context similarity features\nfrom previous literature. All of the similarity features, together, improve the\nclassification results by a large amount, but the features in the present paper\nare a relatively small contribution.\nThe task is interesting, and the work seems to be correct as far as it goes,\nbut incremental. The method of producing the mask vectors is taken from\nexisting literature on encoding variable-length sequences into min/max/mean\nvectors, but I don't think they've been used as masks before, so this is novel.\nHowever, excluding the PPDB features it looks like the best result does not use\nthe representation introduced in the paper.\nA few more specific points:\nIn the creation of the new Context-WN dataset, are there a lot of false\nnegatives resulting from similar synsets in the \"permuted\" examples? If you\ntake word w, with synsets i and j, is it guaranteed that the exemplar context\nfor a hypernym synset of j is a bad entailment context for i? What if i and j\nare semantically close?\nWhy does the masked representation hurt classification with the\ncontext-agnostic word vectors (rows 3, 5 in Table 3) when row 1 does so well?\nWouldn't the classifier learn to ignore the context-agnostic features?\nThe paper should make clearer which similarity measures are new and which are\nfrom previous literature. It currently says that previous lit used the \"most\nsalient\" similarity features, but that's not informative to the reader.\nThe paper should be clearer about the contribution of the masked vectors vs the\nsimilarity features. It seems like similarity is doing most of the work.\nI don't understand the intuition behind the Macro-F1 measure, or how it relates\nto \"how sensitive are our models to changes in context\" -- what changes? How do\nwe expect Macro-F1 to compare with F1?\nThe cross-language task is not well motivated.\nMissing a relevant citation: Learning to Distinguish Hypernyms and Co-Hyponyms.\nJulie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller. COLING\n2014.\n==\nI have read the author response. As noted in the original reviews, a quick\nexamination of the tables shows that the similarity features make the largest\ncontribution to the improvement in F-score on the two datasets (aside from PPDB\nfeatures). The author response makes the point that similarities include\ncontextualized representations. However, the similarity features are a mixed\nbag, including both contextualized and non-contextualized representations. This\nwould need to be teased out more (as acknowledged in the response).\nNeither Table 3 nor 4 gives results using only the masked representations\nwithout the similarity features. This makes the contribution of the masked\nrepresentations difficult to isolate."
        }
    ]
}