{
    "version": "2025-01-09-base",
    "scanId": "df64344c-674f-44dc-92b9-72257abaae9d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.00723921786993742,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018390396609902382,
                    "sentence": "- This paper describes experiments that aim to address a crucial",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015543223358690739,
                    "sentence": "problem for NMT: understanding what does the model learn about morphology and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01880897581577301,
                    "sentence": "syntax, etc..",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015869053080677986,
                    "sentence": "- Very clear objectives and experiments effectively laid down.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0042703100480139256,
                    "sentence": "Good",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01554087270051241,
                    "sentence": "state",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01107516884803772,
                    "sentence": "of the art review and comparison.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01096135750412941,
                    "sentence": "In general, this paper is a pleasure to read.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013613834045827389,
                    "sentence": "- Sound experimentation framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.024738715961575508,
                    "sentence": "Encoder/Decoder Recurrent layer",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022441495209932327,
                    "sentence": "outputs are used to train POS/morphological classifiers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01981067657470703,
                    "sentence": "They show the effect",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.020310552790760994,
                    "sentence": "of certain changes in the framework on the classifier accuracy (e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021800128743052483,
                    "sentence": "use",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0940004289150238,
                    "sentence": "characters instead of words).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.035330113023519516,
                    "sentence": "- Experimentation is carried out on many language pairs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05121568962931633,
                    "sentence": "- Interesting conclusions derived from this work, and not all agree with",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01919722557067871,
                    "sentence": "intuition.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.061320625245571136,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.037338659167289734,
                    "sentence": "- The contrast of character-based vs word-based representations is slightly",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05473752319812775,
                    "sentence": "lacking: NMT with byte-pair encoding is showing v. strong performance in the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06177588552236557,
                    "sentence": "literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05339798331260681,
                    "sentence": "It would have been more relevant to have BPE in the mix, or replace",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02315223217010498,
                    "sentence": "word-based representations if three is too many.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0294107124209404,
                    "sentence": "- Section 1: \"\"¦ while higher layers are more focused on word meaning\";",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02088285982608795,
                    "sentence": "similar sentence in Section 7.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013461772352457047,
                    "sentence": "I am ready to agree with this intuition, but I",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02488456293940544,
                    "sentence": "think the experiments in this paper do not support this particular sentence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017002560198307037,
                    "sentence": "Therefore it should not be included, or it should be clearly stressed that this",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021689005196094513,
                    "sentence": "is a reasonable hypothesis based on indirect evidence (translation performance",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.042013950645923615,
                    "sentence": "improves but morphology on higher layers does not).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08551929146051407,
                    "sentence": "Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01832273043692112,
                    "sentence": "This is a fine paper that presents a thorough and systematic analysis of the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.031990669667720795,
                    "sentence": "NMT model, and derives several interesting conclusions based on many data",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.030875099822878838,
                    "sentence": "points across several language pairs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005876225419342518,
                    "sentence": "I find particularly interesting that (a)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.020621899515390396,
                    "sentence": "the target language affects the quality of the encoding on the source side; in",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017827024683356285,
                    "sentence": "particular, when the target side is a morphologically-poor language (English)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05209081619977951,
                    "sentence": "the pos tagger accuracy for the encoder improves.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03446142002940178,
                    "sentence": "(b) increasing the depth of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04999886080622673,
                    "sentence": "the encoder does not improve pos accuracy (more experiments needed to determine",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05699155852198601,
                    "sentence": "what does it improve); (c) the attention layer hurts the quality of the decoder",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03813308849930763,
                    "sentence": "representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.026777125895023346,
                    "sentence": "I wonder if (a) and (c) are actually related?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07759484648704529,
                    "sentence": "The attention",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03100452944636345,
                    "sentence": "hurts the decoder representation, which is more difficult to learn for a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0419563464820385,
                    "sentence": "morphologically rich language; in turn, the encoders learn based on the global",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05314163118600845,
                    "sentence": "objective, and this backpropagates through the decoder.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03726408630609512,
                    "sentence": "Would this not be a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04229322075843811,
                    "sentence": "strong",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07312238216400146,
                    "sentence": "indication that we need separate objectives to govern the encoder/decoder",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05276818200945854,
                    "sentence": "modules of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09888370335102081,
                    "sentence": "the NMT model?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 38,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 41,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 42,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 43,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 48,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 51,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.039837804045504216,
            "class_probabilities": {
                "human": 0.9601216422360306,
                "ai": 0.039837804045504216,
                "mixed": 4.055371846526579e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9601216422360306,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.039837804045504216,
                    "human": 0.9601216422360306,
                    "mixed": 4.055371846526579e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Strengths:\n- This paper describes experiments that aim to address a crucial\nproblem for NMT: understanding what does the model learn about morphology and\nsyntax, etc..\n- Very clear objectives and experiments effectively laid down. Good\nstate\nof the art review and comparison. In general, this paper is a pleasure to read.\n- Sound experimentation framework. Encoder/Decoder Recurrent layer\noutputs are used to train POS/morphological classifiers. They show the effect\nof certain changes in the framework on the classifier accuracy (e.g. use\ncharacters instead of words).\n- Experimentation is carried out on many language pairs.\n- Interesting conclusions derived from this work, and not all agree with\nintuition.\nWeaknesses:\n - The contrast of character-based vs word-based representations is slightly\nlacking: NMT with byte-pair encoding is showing v. strong performance in the\nliterature. It would have been more relevant to have BPE in the mix, or replace\nword-based representations if three is too many.\n - Section 1: \"\"¦ while higher layers are more focused on word meaning\";\nsimilar sentence in Section 7. I am ready to agree with this intuition, but I\nthink the experiments in this paper do not support this particular sentence.\nTherefore it should not be included, or it should be clearly stressed that this\nis a reasonable hypothesis based on indirect evidence (translation performance\nimproves but morphology on higher layers does not).\nDiscussion:\nThis is a fine paper that presents a thorough and systematic analysis of the\nNMT model, and derives several interesting conclusions based on many data\npoints across several language pairs. I find particularly interesting that (a)\nthe target language affects the quality of the encoding on the source side; in\nparticular, when the target side is a morphologically-poor language (English)\nthe pos tagger accuracy for the encoder improves. (b) increasing the depth of\nthe encoder does not improve pos accuracy (more experiments needed to determine\nwhat does it improve); (c) the attention layer hurts the quality of the decoder\nrepresentations. I wonder if (a) and (c) are actually related? The attention\nhurts the decoder representation, which is more difficult to learn for a\nmorphologically rich language; in turn, the encoders learn based on the global\nobjective, and this backpropagates through the decoder. Would this not be a\nstrong\nindication that we need separate objectives to govern the encoder/decoder\nmodules of\nthe NMT model?"
        }
    ]
}