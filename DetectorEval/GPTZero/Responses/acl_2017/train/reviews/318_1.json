{
    "version": "2025-01-09-base",
    "scanId": "308afcdd-80f4-4680-90f4-a6916177466d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.4457608759403229,
                    "sentence": "This work showed that word representation learning can benefit from sememes",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.47616976499557495,
                    "sentence": "when used in an appropriate attention scheme.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5258046388626099,
                    "sentence": "Authors hypothesized that sememes",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3614116907119751,
                    "sentence": "can act as an essential regularizer for WRL and WSI tasks and proposed SE-WL",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4402880370616913,
                    "sentence": "model which detects word senses and learn representations simultaneously.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2312658131122589,
                    "sentence": "Though experimental results indicate that WRL benefits, exact gains for WSI are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.28709515929222107,
                    "sentence": "unclear since a qualitative case study of a couple of examples has only been",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.416072279214859,
                    "sentence": "done.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3544098138809204,
                    "sentence": "Overall, paper is well-written and well-structured.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2933274805545807,
                    "sentence": "In the last paragraph of introduction section, authors tried to tell three",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2251463383436203,
                    "sentence": "contributions of this work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2921217679977417,
                    "sentence": "(1) and (2) are more of novelties of the work",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3103640079498291,
                    "sentence": "rather than contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17856821417808533,
                    "sentence": "I see the main contribution of the work to be the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21413251757621765,
                    "sentence": "results which show that we can learn better word representations (unsure about",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2612653374671936,
                    "sentence": "WSI) by modeling sememe information than other competitive baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21392975747585297,
                    "sentence": "(3) is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.24789685010910034,
                    "sentence": "neither a contribution nor a novelty.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.20194339752197266,
                    "sentence": "The three strategies tried for SE-WRL modeling makes sense and can be",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.26246798038482666,
                    "sentence": "intuitively ranked in terms of how well they will work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.14379800856113434,
                    "sentence": "Authors did a good job",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13188935816287994,
                    "sentence": "explaining that and experimental results supported the intuition but the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13255834579467773,
                    "sentence": "reviewer also sees MST as a fourth strategy rather than a baseline inspired by",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3267616927623749,
                    "sentence": "Chen et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18570572137832642,
                    "sentence": "2014 (many WSI systems assume one sense per word given a context).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.23531511425971985,
                    "sentence": "MST many times performed better than SSA and SAC.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2519298493862152,
                    "sentence": "Unless authors missed to",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17592549324035645,
                    "sentence": "clarify otherwise, MST seems to be exactly like SAT with a difference that",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.20403273403644562,
                    "sentence": "target word is represented by the most probable sense rather than taking an",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.301379531621933,
                    "sentence": "attention weighted average over all its senses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2769050896167755,
                    "sentence": "MST is still an attention based",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3075241446495056,
                    "sentence": "scheme where sense with maximum attention weight is chosen though it has not",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.029030613601207733,
                    "sentence": "been clearly mentioned if target word is represented by chosen sense embedding",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013153353706002235,
                    "sentence": "or some function of it.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013741860166192055,
                    "sentence": "Authors did not explain the selection of datasets for training and evaluation",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01145542785525322,
                    "sentence": "tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016055453568696976,
                    "sentence": "Reference page to Sogou-T text corpus did not help as reviewer does not",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023401878774166107,
                    "sentence": "know Chinese language.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009114655666053295,
                    "sentence": "It was unclear which exact dataset was used as there are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012585420161485672,
                    "sentence": "several datasets mentioned on that page.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013927859254181385,
                    "sentence": "Why two word similarity datasets were",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006984994281083345,
                    "sentence": "used and how they are different (like does one has more rare words than",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00683478731662035,
                    "sentence": "another) since different models performed differently on these datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004694747272878885,
                    "sentence": "The",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01074673980474472,
                    "sentence": "choice of these datasets did not allow evaluating against results of other",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006725626066327095,
                    "sentence": "works which makes the reviewer wonder about next question.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014386598020792007,
                    "sentence": "Are proposed SAT model results state of the art for Chinese word similarity?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010109159164130688,
                    "sentence": "E.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0314348004758358,
                    "sentence": "Schnabel et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01844071038067341,
                    "sentence": "(2015) report a score of 0.640 on WordSim-353 data by",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018626023083925247,
                    "sentence": "using CBOW word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006917137652635574,
                    "sentence": "Reviewer needs clarification on some model parameters like vocabulary sizes for",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015281926840543747,
                    "sentence": "words (Does Sogou-T contains 2.7 billion unique words) and word senses (how",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02110210806131363,
                    "sentence": "many word types from HowNet).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008660911582410336,
                    "sentence": "Because of the notation used it is not clear if",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015419398434460163,
                    "sentence": "embeddings for senses and sememes for different words were shared.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011518603190779686,
                    "sentence": "Reviewer",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012568959966301918,
                    "sentence": "hopes that is the case but then why 200 dimensional embeddings were used for",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014953112229704857,
                    "sentence": "only 1889 sememes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012279764749109745,
                    "sentence": "It would be better if complexity of model parameters can",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008484666235744953,
                    "sentence": "also be discussed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014944225549697876,
                    "sentence": "May be due to lack of space but experiment results discussion lack insight into",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010842306539416313,
                    "sentence": "observations other than SAT performing the best.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018426477909088135,
                    "sentence": "Also, authors claimed that",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018713966012001038,
                    "sentence": "words with lower frequency were learned better with sememes without evaluating",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.026910975575447083,
                    "sentence": "on a rare words dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02120191976428032,
                    "sentence": "I have read author's response.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 41,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 42,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 44,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 45,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 51,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 55,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 57,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 58,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 60,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 61,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 62,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 64,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 65,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 66,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.06631762963797466,
            "class_probabilities": {
                "human": 0.9333949326468816,
                "ai": 0.06631762963797466,
                "mixed": 0.0002874377151437875
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9333949326468816,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.06631762963797466,
                    "human": 0.9333949326468816,
                    "mixed": 0.0002874377151437875
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This work showed that word representation learning can benefit from sememes\nwhen used in an appropriate attention scheme. Authors hypothesized that sememes\ncan act as an essential regularizer for WRL and WSI tasks and proposed SE-WL\nmodel which detects word senses and learn representations simultaneously.\nThough experimental results indicate that WRL benefits, exact gains for WSI are\nunclear since a qualitative case study of a couple of examples has only been\ndone. Overall, paper is well-written and well-structured.\nIn the last paragraph of introduction section, authors tried to tell three\ncontributions of this work. (1) and (2) are more of novelties of the work\nrather than contributions. I see the main contribution of the work to be the\nresults which show that we can learn better word representations (unsure about\nWSI) by modeling sememe information than other competitive baselines. (3) is\nneither a contribution nor a novelty.\nThe three strategies tried for SE-WRL modeling makes sense and can be\nintuitively ranked in terms of how well they will work. Authors did a good job\nexplaining that and experimental results supported the intuition but the\nreviewer also sees MST as a fourth strategy rather than a baseline inspired by\nChen et al. 2014 (many WSI systems assume one sense per word given a context).\nMST many times performed better than SSA and SAC. Unless authors missed to\nclarify otherwise, MST seems to be exactly like SAT with a difference that\ntarget word is represented by the most probable sense rather than taking an\nattention weighted average over all its senses. MST is still an attention based\nscheme where sense with maximum attention weight is chosen though it has not\nbeen clearly mentioned if target word is represented by chosen sense embedding\nor some function of it.\nAuthors did not explain the selection of datasets for training and evaluation\ntasks. Reference page to Sogou-T text corpus did not help as reviewer does not\nknow Chinese language. It was unclear which exact dataset was used as there are\nseveral datasets mentioned on that page. Why two word similarity datasets were\nused and how they are different (like does one has more rare words than\nanother) since different models performed differently on these datasets. The\nchoice of these datasets did not allow evaluating against results of other\nworks which makes the reviewer wonder about next question.\nAre proposed SAT model results state of the art for Chinese word similarity? \nE.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by\nusing CBOW word embeddings.\nReviewer needs clarification on some model parameters like vocabulary sizes for\nwords (Does Sogou-T contains 2.7 billion unique words) and word senses (how\nmany word types from HowNet). Because of the notation used it is not clear if\nembeddings for senses and sememes for different words were shared. Reviewer\nhopes that is the case but then why 200 dimensional embeddings were used for\nonly 1889 sememes. It would be better if complexity of model parameters can\nalso be discussed.\nMay be due to lack of space but experiment results discussion lack insight into\nobservations other than SAT performing the best. Also, authors claimed that\nwords with lower frequency were learned better with sememes without evaluating\non a rare words dataset.\nI have read author's response."
        }
    ]
}