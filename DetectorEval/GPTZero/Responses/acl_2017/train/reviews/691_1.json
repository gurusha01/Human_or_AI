{
    "version": "2025-01-09-base",
    "scanId": "74ddd5ce-c8e9-4ab2-98b1-9558b17db29b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.06480147689580917,
                    "sentence": "- Overview:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04590272903442383,
                    "sentence": "The paper proposes a new model for training sense embeddings grounded in a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.028894321992993355,
                    "sentence": "lexical-semantic resource (in this case WordNet).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.032108061015605927,
                    "sentence": "There is no direct evaluation",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03213976323604584,
                    "sentence": "that the learned sense vectors are meaningful; instead, the sense vectors are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05174047872424126,
                    "sentence": "combined back into word embeddings, which are evaluated in a downstream task:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05195135250687599,
                    "sentence": "PP attachment prediction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06582125276327133,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07068347185850143,
                    "sentence": "PP attachment results seem solid.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10162139683961868,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.040176086127758026,
                    "sentence": "Whether the sense embeddings are meaningful remains uninvestigated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07164325565099716,
                    "sentence": "The probabilistic model has some details that are hard to understand.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016679419204592705,
                    "sentence": "Are the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.030643761157989502,
                    "sentence": "\\lambdawi hyperparameters or trained?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021686309948563576,
                    "sentence": "Where does \"rank\" come from, is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.048325058072805405,
                    "sentence": "this taken from the sense ranks in WordNet?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02465592883527279,
                    "sentence": "Related work: the idea of expressing embeddings of words as a convex",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.024440879002213478,
                    "sentence": "combination of sense embeddings has been proposed a number of times previously.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.027488093823194504,
                    "sentence": "For instance, Johansson and Nieto PiÂçòa \"Embedding a semantic network in a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.042042508721351624,
                    "sentence": "word space\" (NAACL, 2015) decomposed word embeddings into ontology-grounded",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02200685255229473,
                    "sentence": "sense embeddings based on this idea.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016272014006972313,
                    "sentence": "Also in unsupervised sense vector training",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01956792362034321,
                    "sentence": "this idea has been used, for instance by Arora et al \"Linear Algebraic",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06077718734741211,
                    "sentence": "Structure of Word Senses, with Applications to Polysemy\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016936451196670532,
                    "sentence": "Minor comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012485231272876263,
                    "sentence": "no need to define types and tokens, this is standard terminology",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01365720946341753,
                    "sentence": "why is the first \\lambawi in equation 4 needed if the probability is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017218541353940964,
                    "sentence": "unnormalized?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08089099824428558,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.059261636754220776,
            "class_probabilities": {
                "human": 0.9406968808522652,
                "ai": 0.059261636754220776,
                "mixed": 4.148239351388549e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9406968808522652,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.059261636754220776,
                    "human": 0.9406968808522652,
                    "mixed": 4.148239351388549e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "- Overview:\nThe paper proposes a new model for training sense embeddings grounded in a\nlexical-semantic resource (in this case WordNet). There is no direct evaluation\nthat the learned sense vectors are meaningful; instead, the sense vectors are\ncombined back into word embeddings, which are evaluated in a downstream task:\nPP attachment prediction.\n- Strengths:\nPP attachment results seem solid.\n- Weaknesses:\nWhether the sense embeddings are meaningful remains uninvestigated. \nThe probabilistic model has some details that are hard to understand. Are the\n\\lambdawi hyperparameters or trained? Where does \"rank\" come from, is\nthis taken from the sense ranks in WordNet?\nRelated work: the idea of expressing embeddings of words as a convex\ncombination of sense embeddings has been proposed a number of times previously.\nFor instance, Johansson and Nieto PiÂçòa \"Embedding a semantic network in a\nword space\" (NAACL, 2015) decomposed word embeddings into ontology-grounded\nsense embeddings based on this idea. Also in unsupervised sense vector training\nthis idea has been used, for instance by Arora et al \"Linear Algebraic\nStructure of Word Senses, with Applications to Polysemy\".\nMinor comments:\nno need to define types and tokens, this is standard terminology\nwhy is the first \\lambawi in equation 4 needed if the probability is\nunnormalized?\n- General Discussion:"
        }
    ]
}