{
    "version": "2025-01-09-base",
    "scanId": "b911068a-42b8-4fbb-b12f-424b946c2df1",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.771407425403595,
                    "sentence": "This paper proposes a neural network architecture that represent structural",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8659981489181519,
                    "sentence": "linguistic knowledge in a memory network for sequence tagging tasks (in",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8393420577049255,
                    "sentence": "particular, slot-filling of the natural language understanding unit in",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9056996703147888,
                    "sentence": "conversation systems).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5724799633026123,
                    "sentence": "Substructures (e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8479899764060974,
                    "sentence": "a node in the parse tree) is encoded",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.826367199420929,
                    "sentence": "as a vector (a memory slot) and a weighted sum of the substructure embeddings",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.792919933795929,
                    "sentence": "are fed in a RNN at each time step as additional context for labeling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.676209032535553,
                    "sentence": "-----Strengths-----",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7506769895553589,
                    "sentence": "I think the main contribution of this paper is a simple way to \"flatten\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6919776201248169,
                    "sentence": "structured information to an array of vectors (the memory), which is then",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7898585796356201,
                    "sentence": "connected to the tagger as additional knowledge.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8379814624786377,
                    "sentence": "The idea is similar to",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6454618573188782,
                    "sentence": "structured / syntax-based attention (i.e.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8309758901596069,
                    "sentence": "attention over nodes from treeLSTM);",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8950111269950867,
                    "sentence": "related work includes Zhao et al on textual entailment, Liu et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8626458048820496,
                    "sentence": "on natural",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8412848114967346,
                    "sentence": "language inference, and Eriguchi et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.834993302822113,
                    "sentence": "for machine translation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.900873064994812,
                    "sentence": "The proposed",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.771080493927002,
                    "sentence": "substructure encoder is similar to DCNN (Ma et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5372536778450012,
                    "sentence": "): each node is embedded from",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5453941226005554,
                    "sentence": "a sequence of ancestor words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7942359447479248,
                    "sentence": "The architecture does not look entirely novel,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6274555325508118,
                    "sentence": "but I kind of like the simple and practical approach compared to prior work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7754227519035339,
                    "sentence": "-----Weaknesses-----",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6515610218048096,
                    "sentence": "I'm not very convinced by the empirical results, mostly due to the lack of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8539661765098572,
                    "sentence": "details of the baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8834229707717896,
                    "sentence": "Comments below are ranked by decreasing importance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8983793258666992,
                    "sentence": "- The proposed model has two main parts: sentence embedding and substructure",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9182649254798889,
                    "sentence": "embedding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8608683943748474,
                    "sentence": "In Table 1, the baseline models are TreeRNN and DCNN, they are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8813450336456299,
                    "sentence": "originally used for sentence embedding but one can easily take the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7775136828422546,
                    "sentence": "node/substructure embedding from them too.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7505939602851868,
                    "sentence": "It's not clear how they are used to",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7633712887763977,
                    "sentence": "compute the two parts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8404041528701782,
                    "sentence": "- The model uses two RNNs: a chain-based one and a knowledge guided one.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9037343859672546,
                    "sentence": "The",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7859975695610046,
                    "sentence": "only difference in the knowledge-guided RNN is the addition of a \"knowledge\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8710011839866638,
                    "sentence": "vector from the memory in the RNN input (Eqn 5 and 8).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8085092306137085,
                    "sentence": "It seems completely",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.938112735748291,
                    "sentence": "unnecessary to me to have separate weights for the two RNNs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8439748883247375,
                    "sentence": "The only advantage",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7821659445762634,
                    "sentence": "of using two is an increase of model capacity, i.e.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8478687405586243,
                    "sentence": "more parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8507675528526306,
                    "sentence": "Furthermore, what are the hyper-parameters / size of the baseline neural",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.87356036901474,
                    "sentence": "networks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.871027946472168,
                    "sentence": "They should have comparable numbers of parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9026602506637573,
                    "sentence": "- I also think it is reasonable to include a baseline that just input",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7781721949577332,
                    "sentence": "additional knowledge as features to the RNN, e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8452547192573547,
                    "sentence": "the head of each word, NER",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.903951108455658,
                    "sentence": "results etc.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9013097882270813,
                    "sentence": "- Any comments / results on the model's sensitivity to parser errors?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0022920880001038313,
                    "sentence": "Comments on the model:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001908917911350727,
                    "sentence": "- After computing the substructure embeddings, it seems very natural to compute",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0011132201179862022,
                    "sentence": "an attention over them at each word.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0010079334024339914,
                    "sentence": "Is there any reason to use a static",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001111594494432211,
                    "sentence": "attention for all words?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0014432196039706469,
                    "sentence": "I guess as it is, the \"knowledge\" is acting more like",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0012137903831899166,
                    "sentence": "a filter to mark important words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0018733229953795671,
                    "sentence": "Then it is reasonable to include the baseline",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0010032709687948227,
                    "sentence": "suggest above, i.e.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0013785650953650475,
                    "sentence": "input additional features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002142704324796796,
                    "sentence": "- Since the weight on a word is computed by inner product of the sentence",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001432796474546194,
                    "sentence": "embedding and the substructure embedding, and the two embeddings are computed",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0014723694184795022,
                    "sentence": "by the same RNN/CNN, doesn't it means nodes / phrases similar to the whole",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0009589726105332375,
                    "sentence": "sentence gets higher weights, i.e.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0018412304343655705,
                    "sentence": "all leaf nodes?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001456287456676364,
                    "sentence": "- The paper claims the model generalizes to different knowledge but I think the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0009444866445846856,
                    "sentence": "substructure has to be represented as a sequence of words, e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0020214577671140432,
                    "sentence": "it doesn't seem",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0021475087851285934,
                    "sentence": "straightforward for me to use constituent parse as knowledge here.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001674140803515911,
                    "sentence": "Finally, I'm hesitating to call it \"knowledge\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0007909332634881139,
                    "sentence": "This is misleading as usually",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0012882021255791187,
                    "sentence": "it is used to refer to world / external knowledge such as a knowledge base of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002191036706790328,
                    "sentence": "entities, whereas here it is really just syntax, or arguably semantics if AMR",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001302339369431138,
                    "sentence": "parsing is used.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0018389109754934907,
                    "sentence": "-----General Discussion-----",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001300569623708725,
                    "sentence": "This paper proposes a practical model which seems working well on one dataset,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0011110457126051188,
                    "sentence": "but the main ideas are not very novel (see comments in Strengths).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0010607104049995542,
                    "sentence": "I think as",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0017295386642217636,
                    "sentence": "an ACL paper there should be more takeaways.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0018518231809139252,
                    "sentence": "More importantly, the experiments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001324604731053114,
                    "sentence": "are not convincing as it is presented now.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0016599444206804037,
                    "sentence": "Will need some clarification to",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0010767931817099452,
                    "sentence": "better judge the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0021483865566551685,
                    "sentence": "-----Post-rebuttal-----",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0009083314798772335,
                    "sentence": "The authors did not address my main concern, which is whether the baselines",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0008815677720122039,
                    "sentence": "(e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0017174233216792345,
                    "sentence": "TreeRNN) are used to compute substructure embeddings independent of the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0024493348319083452,
                    "sentence": "sentence embedding and the joint tagger.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0014134853845462203,
                    "sentence": "Another major concern is the use of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0024082728195935488,
                    "sentence": "two separate RNNs which gives the proposed model more parameters than the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0016852469416335225,
                    "sentence": "baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003753668162971735,
                    "sentence": "Therefore I'm not changing my scores.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.45887534985363754
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 36,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 38,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 41,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 43,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 45,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 48,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 49,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 51,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 54,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 55,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 57,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 59,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 61,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 63,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 64,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 65,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 66,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 68,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 69,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 71,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 72,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 74,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 75,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 76,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 77,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 78,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 79,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 81,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 83,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 85,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 86,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 87,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 88,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 90,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 92,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 93,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.2425855513307985,
            "class_probabilities": {
                "human": 0.7574144486692015,
                "ai": 0.2425855513307985,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.7574144486692015,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.2425855513307985,
                    "human": 0.7574144486692015,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a neural network architecture that represent structural\nlinguistic knowledge in a memory network for sequence tagging tasks (in\nparticular, slot-filling of the natural language understanding unit in\nconversation systems). Substructures (e.g. a node in the parse tree) is encoded\nas a vector (a memory slot) and a weighted sum of the substructure embeddings\nare fed in a RNN at each time step as additional context for labeling.\n-----Strengths-----\nI think the main contribution of this paper is a simple way to \"flatten\"\nstructured information to an array of vectors (the memory), which is then\nconnected to the tagger as additional knowledge. The idea is similar to\nstructured / syntax-based attention (i.e. attention over nodes from treeLSTM);\nrelated work includes Zhao et al on textual entailment, Liu et al. on natural\nlanguage inference, and Eriguchi et al. for machine translation. The proposed\nsubstructure encoder is similar to DCNN (Ma et al.): each node is embedded from\na sequence of ancestor words. The architecture does not look entirely novel,\nbut I kind of like the simple and practical approach compared to prior work.\n-----Weaknesses-----\nI'm not very convinced by the empirical results, mostly due to the lack of\ndetails of the baselines. Comments below are ranked by decreasing importance.\n- The proposed model has two main parts: sentence embedding and substructure\nembedding. In Table 1, the baseline models are TreeRNN and DCNN, they are\noriginally used for sentence embedding but one can easily take the\nnode/substructure embedding from them too. It's not clear how they are used to\ncompute the two parts.\n- The model uses two RNNs: a chain-based one and a knowledge guided one. The\nonly difference in the knowledge-guided RNN is the addition of a \"knowledge\"\nvector from the memory in the RNN input (Eqn 5 and 8). It seems completely\nunnecessary to me to have separate weights for the two RNNs. The only advantage\nof using two is an increase of model capacity, i.e. more parameters.\nFurthermore, what are the hyper-parameters / size of the baseline neural\nnetworks? They should have comparable numbers of parameters.\n- I also think it is reasonable to include a baseline that just input\nadditional knowledge as features to the RNN, e.g. the head of each word, NER\nresults etc.\n- Any comments / results on the model's sensitivity to parser errors?\nComments on the model:\n- After computing the substructure embeddings, it seems very natural to compute\nan attention over them at each word. Is there any reason to use a static\nattention for all words? I guess as it is, the \"knowledge\" is acting more like\na filter to mark important words. Then it is reasonable to include the baseline\nsuggest above, i.e. input additional features.\n- Since the weight on a word is computed by inner product of the sentence\nembedding and the substructure embedding, and the two embeddings are computed\nby the same RNN/CNN, doesn't it means nodes / phrases similar to the whole\nsentence gets higher weights, i.e. all leaf nodes?\n- The paper claims the model generalizes to different knowledge but I think the\nsubstructure has to be represented as a sequence of words, e.g. it doesn't seem\nstraightforward for me to use constituent parse as knowledge here.\nFinally, I'm hesitating to call it \"knowledge\". This is misleading as usually\nit is used to refer to world / external knowledge such as a knowledge base of\nentities, whereas here it is really just syntax, or arguably semantics if AMR\nparsing is used.\n-----General Discussion-----\nThis paper proposes a practical model which seems working well on one dataset,\nbut the main ideas are not very novel (see comments in Strengths). I think as\nan ACL paper there should be more takeaways. More importantly, the experiments\nare not convincing as it is presented now. Will need some clarification to\nbetter judge the results.\n-----Post-rebuttal-----\nThe authors did not address my main concern, which is whether the baselines\n(e.g. TreeRNN) are used to compute substructure embeddings independent of the\nsentence embedding and the joint tagger. Another major concern is the use of\ntwo separate RNNs which gives the proposed model more parameters than the\nbaselines. Therefore I'm not changing my scores."
        }
    ]
}