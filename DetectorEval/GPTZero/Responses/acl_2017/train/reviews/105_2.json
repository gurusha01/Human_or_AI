{
    "version": "2025-01-09-base",
    "scanId": "d0448461-9d38-4ae5-ab17-d424568069db",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0615982711315155,
                    "sentence": "- Strengths: A new encoder-decoder model is proposed that explicitly takes",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12848161160945892,
                    "sentence": "into account monotonicity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04516126587986946,
                    "sentence": "- Weaknesses: Maybe the model is just an ordinary BiRNN with alignments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06790199875831604,
                    "sentence": "de-coupled.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05597982555627823,
                    "sentence": "Only evaluated on morphology, no other monotone Seq2Seq tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04508735612034798,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05359157919883728,
                    "sentence": "The authors propose a novel encoder-decoder neural network architecture with",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04617238789796829,
                    "sentence": "\"hard monotonic attention\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.051922209560871124,
                    "sentence": "They evaluate it on three morphology datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006493817083537579,
                    "sentence": "This paper is a tough one.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0049514067359268665,
                    "sentence": "One the one hand it is well-written, mostly very",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0037469074595719576,
                    "sentence": "clear and also presents a novel idea, namely including monotonicity in",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005182195920497179,
                    "sentence": "morphology tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0035706772468984127,
                    "sentence": "The reason for including such monotonicity is pretty obvious: Unlike machine",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00441592326387763,
                    "sentence": "translation, many seq2seq tasks are monotone, and therefore general",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0034763372968882322,
                    "sentence": "encoder-decoder models should not be used in the first place.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0016238139942288399,
                    "sentence": "That they still",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0038264922332018614,
                    "sentence": "perform reasonably well should be considered a strong argument for neural",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005414511077105999,
                    "sentence": "techniques, in general.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003389132209122181,
                    "sentence": "The idea of this paper is now to explicity enforce a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0027189592365175486,
                    "sentence": "monotonic output character generation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00139431597199291,
                    "sentence": "They do this by decoupling alignment and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0015616107266396284,
                    "sentence": "transduction and first aligning input-output sequences monotonically and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0022339720744639635,
                    "sentence": "then training to generate outputs in agreement with the monotone alignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0016397458966821432,
                    "sentence": "However, the authors are unclear on this point.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001467385794967413,
                    "sentence": "I have a few questions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0021919505670666695,
                    "sentence": "1) How do your alignments look like?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001863539800979197,
                    "sentence": "On the one hand, the alignments seem to",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0013589876471087337,
                    "sentence": "be of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0015814794460311532,
                    "sentence": "character can be aligned with zero, 1, or several output characters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0008624311303719878,
                    "sentence": "However,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0013261954300105572,
                    "sentence": "this seems to contrast with the description given in lines 311-312 where the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001173785189166665,
                    "sentence": "authors speak of several input characters aligned to 1 output character.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0010653965873643756,
                    "sentence": "That",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001345139811746776,
                    "sentence": "is, do you use 1-to-many, many-to-1 or many-to-many alignments?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001300826552323997,
                    "sentence": "2) Actually, there is a quite simple approach to monotone Seq2Seq.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0012989204842597246,
                    "sentence": "In a first",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0020721631590276957,
                    "sentence": "stage, align input and output characters monotonically with a 1-to-many",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0014636634150519967,
                    "sentence": "constraint (one can use any monotone aligner, such as the toolkit of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0027296256739646196,
                    "sentence": "Jiampojamarn and Kondrak).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003356076078489423,
                    "sentence": "Then one trains a standard sequence tagger(!)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008395503275096416,
                    "sentence": "to",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0034242584370076656,
                    "sentence": "predict exactly these 1-to-many alignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002132429974153638,
                    "sentence": "For example, flog->fliege (your",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0033591778483241796,
                    "sentence": "example on l.613): First align as in \"f-l-o-g / f-l-ie-ge\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0026434040628373623,
                    "sentence": "Now use any tagger",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003157684113830328,
                    "sentence": "(could use an LSTM, if you like) to predict \"f-l-ie-ge\" (sequence of length 4)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012875624001026154,
                    "sentence": "from \"f-l-o-g\" (sequence of length 4).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02583957277238369,
                    "sentence": "Such an approach may have been suggested",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.019625792279839516,
                    "sentence": "in multiple papers, one reference could be [*, Section 4.2] below.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008659924380481243,
                    "sentence": "My two questions here are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0074957432225346565,
                    "sentence": "2a) How does your approach differ from this rather simple idea?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006724856793880463,
                    "sentence": "2b) Why did you not include it as a baseline?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006111764814704657,
                    "sentence": "Further issues:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00812017172574997,
                    "sentence": "3) It's really a pitty that you only tested on morphology, because there are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007603493519127369,
                    "sentence": "many other interesting monotonic seq2seq tasks, and you could have shown your",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006361284293234348,
                    "sentence": "system's superiority by evaluating on these, given that you explicitly model",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007244250737130642,
                    "sentence": "monotonicity (cf.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013585638254880905,
                    "sentence": "also [*]).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01046054158359766,
                    "sentence": "4) You perform \"on par or better\" (l.791).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009545844979584217,
                    "sentence": "There seems to be a general",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004944232292473316,
                    "sentence": "cognitive bias among NLP researchers to map instances where they perform worse",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0335954949259758,
                    "sentence": "to",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0030781885143369436,
                    "sentence": "\"on par\" and all the rest to \"better\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007329941261559725,
                    "sentence": "I think this wording should be",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006655986420810223,
                    "sentence": "corrected, but otherwise I'm fine with the experimental results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010267162695527077,
                    "sentence": "5) You say little about your linguistic features: From Fig.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007502523250877857,
                    "sentence": "1, I infer that",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007609059568494558,
                    "sentence": "they include POS, etc.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0048871650360524654,
                    "sentence": "5a) Where did you take these features from?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005172597244381905,
                    "sentence": "5b) Is it possible that these are responsible for your better performance in",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00382950808852911,
                    "sentence": "some cases, rather than the monotonicity constraints?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006072944961488247,
                    "sentence": "Minor points:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007256106473505497,
                    "sentence": "6) Equation (3): please re-write $NN$ as $\\text{NN}$ or similar",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007554787676781416,
                    "sentence": "7) l.231 \"Where\" should be lower case",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0059586176648736,
                    "sentence": "8) l.237 and many more: $x1\\ldots xn$.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006402753293514252,
                    "sentence": "As far as I know, the math community",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007397249806672335,
                    "sentence": "recommends to write $x1,\\ldots,xn$ but $x1\\cdots xn$.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007215515710413456,
                    "sentence": "That is, dots should",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009276933036744595,
                    "sentence": "be on the same level as surrounding symbols.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010546700097620487,
                    "sentence": "9) Figure 1: is it really necessary to use cyrillic font?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006775735877454281,
                    "sentence": "I can't even address",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013109952211380005,
                    "sentence": "your example here, because I don't have your fonts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02299843356013298,
                    "sentence": "10) l.437: should be \"these\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04001014679670334,
                    "sentence": "[*]",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05315544456243515,
                    "sentence": "@InProceedings{schnober-EtAl:2016:COLING,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04109776392579079,
                    "sentence": "author = {Schnober, Carsten and Eger, Steffen and Do Dinh,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.038638077676296234,
                    "sentence": "Erik-L\\^{a}n and Gurevych, Iryna},",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.024585938081145287,
                    "sentence": "title = {Still not there?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04724317416548729,
                    "sentence": "Comparing Traditional Sequence-to-Sequence",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09646205604076385,
                    "sentence": "Models to Encoder-Decoder Neural Networks on Monotone String Translation",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03595694899559021,
                    "sentence": "Tasks},",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10615761578083038,
                    "sentence": "booktitle = {Proceedings of COLING 2016, the 26th International Conference on",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15454241633415222,
                    "sentence": "Computational Linguistics: Technical Papers},",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1278773695230484,
                    "sentence": "month = {December},",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06675226241350174,
                    "sentence": "year = {2016},",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08965789526700974,
                    "sentence": "address = {Osaka, Japan},",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06300706416368484,
                    "sentence": "publisher = {The COLING 2016 Organizing Committee},",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09681109338998795,
                    "sentence": "pages = {1703--1714},",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.14388419687747955,
                    "sentence": "url =",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15690407156944275,
                    "sentence": "{http://aclweb.org/anthology/C16-1160}",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3621786832809448,
                    "sentence": "}",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3162720799446106,
                    "sentence": "AFTER AUTHOR RESPONSE",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1438806653022766,
                    "sentence": "Thanks for the clarifications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21143615245819092,
                    "sentence": "I think your alignments got mixed up in the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17849378287792206,
                    "sentence": "response somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.279229998588562,
                    "sentence": "1-1, and later make many-to-many alignments from these.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13675279915332794,
                    "sentence": "I know that you compare to Nicolai, Cherry and Kondrak (2015) but my question",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21490910649299622,
                    "sentence": "would have rather been: why not use 1-x (x in 0,1,2) alignments as in Schnober",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2394920289516449,
                    "sentence": "et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1682056337594986,
                    "sentence": "and then train a neural tagger on these (e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2999134659767151,
                    "sentence": "BiLSTM).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12389686703681946,
                    "sentence": "I wonder how much",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.22151605784893036,
                    "sentence": "your results would have differed from such a rather simple baseline.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.16706465184688568,
                    "sentence": "(A tagger",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.19004280865192413,
                    "sentence": "is a monotone model to start with and given the monotone alignments, everything",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18664050102233887,
                    "sentence": "stays monotone.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09950918704271317,
                    "sentence": "In contrast, you start out with a more general model and then",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1412762999534607,
                    "sentence": "put hard monotonicity constraints on this...)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18086908757686615,
                    "sentence": "NOTES FROM AC",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08006656169891357,
                    "sentence": "Also quite relevant is Cohn et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.14560842514038086,
                    "sentence": "(2016),",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07852339744567871,
                    "sentence": "http://www.aclweb.org/anthology/N16-1102.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0968342125415802,
                    "sentence": "Isn't your architecture also related to methods like the Stack LSTM, which",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1728542447090149,
                    "sentence": "similarly predicts a sequence of actions that modify or annotate an input?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.139694944024086,
                    "sentence": "Do you think you lose anything by using a greedy alignment, in contrast to",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10226442664861679,
                    "sentence": "Rastogi et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09235627949237823,
                    "sentence": "(2016), which also has hard monotonic attention but sums over",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.16408886015415192,
                    "sentence": "all alignments?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 38,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 42,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 44,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 49,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 51,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 54,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 55,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 56,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 57,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 59,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 61,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 62,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 63,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 65,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 66,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 68,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 69,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 70,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 71,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 72,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 73,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 74,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 75,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 77,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 79,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 80,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 82,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 83,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 84,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 85,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 86,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 87,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 88,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 90,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 91,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 92,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 93,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 94,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 95,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 96,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 97,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 98,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 99,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 100,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 101,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 102,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 103,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 105,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 106,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 107,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 108,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 109,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 113,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 115,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 116,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 118,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 119,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 120,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 122,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 123,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 124,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 125,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 126,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 128,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.06421350303038222,
            "class_probabilities": {
                "human": 0.9354984114909908,
                "ai": 0.06421350303038222,
                "mixed": 0.0002880854786270212
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9354984114909908,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.06421350303038222,
                    "human": 0.9354984114909908,
                    "mixed": 0.0002880854786270212
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "- Strengths: A new encoder-decoder model is proposed that explicitly takes \ninto account monotonicity.\n- Weaknesses: Maybe the model is just an ordinary BiRNN with alignments\nde-coupled.\nOnly evaluated on morphology, no other monotone Seq2Seq tasks.\n- General Discussion:\nThe authors propose a novel encoder-decoder neural network architecture with\n\"hard monotonic attention\". They evaluate it on three morphology datasets.\nThis paper is a tough one. One the one hand it is well-written, mostly very\nclear and also presents a novel idea, namely including monotonicity in\nmorphology tasks. \nThe reason for including such monotonicity is pretty obvious: Unlike machine\ntranslation, many seq2seq tasks are monotone, and therefore general\nencoder-decoder models should not be used in the first place. That they still\nperform reasonably well should be considered a strong argument for neural\ntechniques, in general. The idea of this paper is now to explicity enforce a\nmonotonic output character generation. They do this by decoupling alignment and\ntransduction and first aligning input-output sequences monotonically and\nthen training to generate outputs in agreement with the monotone alignments.\nHowever, the authors are unclear on this point. I have a few questions:\n1) How do your alignments look like? On the one hand, the alignments seem to\nbe of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input\ncharacter can be aligned with zero, 1, or several output characters. However,\nthis seems to contrast with the description given in lines 311-312 where the\nauthors speak of several input characters aligned to 1 output character. That\nis, do you use 1-to-many, many-to-1 or many-to-many alignments?\n2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first\nstage, align input and output characters monotonically with a 1-to-many\nconstraint (one can use any monotone aligner, such as the toolkit of\nJiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to\npredict exactly these 1-to-many alignments. For example, flog->fliege (your\nexample on l.613): First align as in \"f-l-o-g / f-l-ie-ge\". Now use any tagger\n(could use an LSTM, if you like) to predict \"f-l-ie-ge\" (sequence of length 4)\nfrom \"f-l-o-g\" (sequence of length 4). Such an approach may have been suggested\nin multiple papers, one reference could be [*, Section 4.2] below. \nMy two questions here are: \n2a) How does your approach differ from this rather simple idea?\n2b) Why did you not include it as a baseline?\nFurther issues:\n3) It's really a pitty that you only tested on morphology, because there are\nmany other interesting monotonic seq2seq tasks, and you could have shown your\nsystem's superiority by evaluating on these, given that you explicitly model\nmonotonicity (cf. also [*]).\n4) You perform \"on par or better\" (l.791). There seems to be a general\ncognitive bias among NLP researchers to map instances where they perform worse\nto\n\"on par\" and all the rest to \"better\". I think this wording should be\ncorrected, but otherwise I'm fine with the experimental results.\n5) You say little about your linguistic features: From Fig. 1, I infer that\nthey include POS, etc. \n5a) Where did you take these features from?\n5b) Is it possible that these are responsible for your better performance in\nsome cases, rather than the monotonicity constraints?\nMinor points:\n6) Equation (3): please re-write $NN$ as $\\text{NN}$ or similar\n7) l.231 \"Where\" should be lower case\n8) l.237 and many more: $x1\\ldots xn$. As far as I know, the math community\nrecommends to write $x1,\\ldots,xn$ but $x1\\cdots xn$. That is, dots should\nbe on the same level as surrounding symbols.\n9) Figure 1: is it really necessary to use cyrillic font? I can't even address\nyour example here, because I don't have your fonts.\n10) l.437: should be \"these\"\n[*] \n@InProceedings{schnober-EtAl:2016:COLING, \n author = {Schnober, Carsten and Eger, Steffen and Do Dinh,\nErik-L\\^{a}n and Gurevych, Iryna},\n title = {Still not there? Comparing Traditional Sequence-to-Sequence\nModels to Encoder-Decoder Neural Networks on Monotone String Translation\nTasks},\n booktitle = {Proceedings of COLING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers},\n month = {December},\n year = {2016},\n address = {Osaka, Japan},\n publisher = {The COLING 2016 Organizing Committee},\n pages = {1703--1714},\n url =\n{http://aclweb.org/anthology/C16-1160}\n}\nAFTER AUTHOR RESPONSE\nThanks for the clarifications. I think your alignments got mixed up in the\nresponse somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1,\n1-1, and later make many-to-many alignments from these. \nI know that you compare to Nicolai, Cherry and Kondrak (2015) but my question\nwould have rather been: why not use 1-x (x in 0,1,2) alignments as in Schnober\net al. and then train a neural tagger on these (e.g. BiLSTM). I wonder how much\nyour results would have differed from such a rather simple baseline. (A tagger\nis a monotone model to start with and given the monotone alignments, everything\nstays monotone. In contrast, you start out with a more general model and then\nput hard monotonicity constraints on this ...)\nNOTES FROM AC\nAlso quite relevant is Cohn et al. (2016),\nhttp://www.aclweb.org/anthology/N16-1102 .\nIsn't your architecture also related to methods like the Stack LSTM, which\nsimilarly predicts a sequence of actions that modify or annotate an input? \nDo you think you lose anything by using a greedy alignment, in contrast to\nRastogi et al. (2016), which also has hard monotonic attention but sums over\nall alignments?"
        }
    ]
}