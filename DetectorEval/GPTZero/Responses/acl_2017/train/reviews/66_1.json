{
    "version": "2025-01-09-base",
    "scanId": "1562dd3c-d75e-453d-b1cd-8d8424330516",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.027280563488602638,
                    "sentence": "This paper describes several ways to encode arbitrarily long sequences of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03337790071964264,
                    "sentence": "digits using something called the major system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02778497152030468,
                    "sentence": "In the major system, each digit",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023223616182804108,
                    "sentence": "is mapped to one or more characters representing consonantal phonemes; the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021430352702736855,
                    "sentence": "possible mappings between digit and phoneme are predefined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017269639298319817,
                    "sentence": "The output of an",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014608696103096008,
                    "sentence": "encoding is typically a sequence of words constrained such that digits in the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016918513923883438,
                    "sentence": "original sequence correspond to characters or digraphs in the output sequence",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014638998545706272,
                    "sentence": "of words; vowels added surrounding the consonant phonemes to form words are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009860650636255741,
                    "sentence": "unconstrained.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018956182524561882,
                    "sentence": "This paper describes several ways to encode your sequence of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015091595239937305,
                    "sentence": "digits such that the output sequence of words is more memorable, generally by",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021409720182418823,
                    "sentence": "applying syntactic constraints and heuristics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012253513559699059,
                    "sentence": "I found this application of natural language processing concepts somewhat",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010813286527991295,
                    "sentence": "interesting, as I have not read an ACL paper on this topic before.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012149525806307793,
                    "sentence": "However, I",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01207283977419138,
                    "sentence": "found the paper and ideas presented here to have a rather old-school feel.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012095510959625244,
                    "sentence": "With",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009994096122682095,
                    "sentence": "much of the focus on n-gram models for generation, frequent POS-tag sequences,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009154001250863075,
                    "sentence": "and other heuristics, this paper really could have been written 15-20 years",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016049541532993317,
                    "sentence": "ago.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010389940813183784,
                    "sentence": "I am not sure that there is enough novelty in the ideas here to warrant",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014821555465459824,
                    "sentence": "publication in ACL in 2017.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00857795774936676,
                    "sentence": "There is no contribution to NLP itself, e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0038567762821912766,
                    "sentence": "in",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011102757416665554,
                    "sentence": "terms of modeling or search, and not a convincing contribution to the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009205419570207596,
                    "sentence": "application area which is just an instance of constrained generation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010964157991111279,
                    "sentence": "Since you start with one sequence and output another sequence with a very",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010765112936496735,
                    "sentence": "straightforward monotonic mapping, it seems like a character-based",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018119383603334427,
                    "sentence": "sequence-to-sequence encoder-decoder model (Sequence to Sequence Learning with",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.028567735105752945,
                    "sentence": "Neural Networks; Sutskever et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0108432462438941,
                    "sentence": "2014) would work rather well here, very",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010481297969818115,
                    "sentence": "likely with very fluent output and fewer moving parts (e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011981670744717121,
                    "sentence": "trigram models and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0121248634532094,
                    "sentence": "POS tag and scoring heuristics and postprocessing with a bigram model).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014268394559621811,
                    "sentence": "You can",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016210680827498436,
                    "sentence": "use large amounts of training from an arbitrary genre and do not need to rely",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02310064807534218,
                    "sentence": "on an already-tagged corpus like in this paper, or worry about a parser.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03218142315745354,
                    "sentence": "This",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.028544224798679352,
                    "sentence": "would be a 2017 paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 36,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.025041194076269924,
            "class_probabilities": {
                "human": 0.974671313226954,
                "ai": 0.025041194076269924,
                "mixed": 0.00028749269677611
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.974671313226954,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.025041194076269924,
                    "human": 0.974671313226954,
                    "mixed": 0.00028749269677611
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper describes several ways to encode arbitrarily long sequences of\ndigits using something called the major system. In the major system, each digit\nis mapped to one or more characters representing consonantal phonemes; the\npossible mappings between digit and phoneme are predefined. The output of an\nencoding is typically a sequence of words constrained such that digits in the\noriginal sequence correspond to characters or digraphs in the output sequence\nof words; vowels added surrounding the consonant phonemes to form words are\nunconstrained. This paper describes several ways to encode your sequence of\ndigits such that the output sequence of words is more memorable, generally by\napplying syntactic constraints and heuristics.\nI found this application of natural language processing concepts somewhat\ninteresting, as I have not read an ACL paper on this topic before. However, I\nfound the paper and ideas presented here to have a rather old-school feel. With\nmuch of the focus on n-gram models for generation, frequent POS-tag sequences,\nand other heuristics, this paper really could have been written 15-20 years\nago. I am not sure that there is enough novelty in the ideas here to warrant\npublication in ACL in 2017. There is no contribution to NLP itself, e.g. in\nterms of modeling or search, and not a convincing contribution to the\napplication area which is just an instance of constrained generation. \nSince you start with one sequence and output another sequence with a very\nstraightforward monotonic mapping, it seems like a character-based\nsequence-to-sequence encoder-decoder model (Sequence to Sequence Learning with\nNeural Networks; Sutskever et al. 2014) would work rather well here, very\nlikely with very fluent output and fewer moving parts (e.g. trigram models and\nPOS tag and scoring heuristics and postprocessing with a bigram model). You can\nuse large amounts of training from an arbitrary genre and do not need to rely\non an already-tagged corpus like in this paper, or worry about a parser. This\nwould be a 2017 paper."
        }
    ]
}