{
    "version": "2025-01-09-base",
    "scanId": "8f92d416-d193-4cb7-b0e4-680d04a6b428",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.7001990079879761,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8686648607254028,
                    "sentence": "1. The presentation of the paper, up until the final few sections, is excellent",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9407785534858704,
                    "sentence": "and the paper reads very well at the start.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9105009436607361,
                    "sentence": "The paper has a clear structure and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9161010980606079,
                    "sentence": "the argumentation is, for the most part, good.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8152323961257935,
                    "sentence": "2. The paper addresses an important problem by attempting to incorporate word",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8115374445915222,
                    "sentence": "order information into word (and sense) embeddings and the proposed solution is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8039980530738831,
                    "sentence": "interesting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9292289018630981,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8435097932815552,
                    "sentence": "1. Unfortunately, the results are rather inconsistent and one is not left",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9015909433364868,
                    "sentence": "entirely convinced that the proposed models are better than the alternatives,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8499638438224792,
                    "sentence": "especially given the added complexity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8972299695014954,
                    "sentence": "Negative results are fine, but there is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9539397954940796,
                    "sentence": "insufficient analysis to learn from them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9441965818405151,
                    "sentence": "Moreover, no results are reported on",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9521316289901733,
                    "sentence": "the word analogy task, besides being told that the proposed models were not",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.889367401599884,
                    "sentence": "competitive - this could have been interesting and analyzed further.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8367875218391418,
                    "sentence": "2. Some aspects of the experimental setup were unclear or poorly motivated, for",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9054456353187561,
                    "sentence": "instance w.r.t.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.91907799243927,
                    "sentence": "to corpora and datasets (see details below).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9202649593353271,
                    "sentence": "3. Unfortunately, the quality of the paper deteriorates towards the end and the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9084051847457886,
                    "sentence": "reader is left a little disappointed, not only w.r.t.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.91949862241745,
                    "sentence": "to the results but with",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8424367904663086,
                    "sentence": "the quality of the presentation and the argumentation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.948604941368103,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8556380867958069,
                    "sentence": "1. The authors aim \"to learn representations for both words and senses in a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9431878328323364,
                    "sentence": "shared emerging space\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9354892373085022,
                    "sentence": "This is only done in the LSTMEmbed_SW version, which",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8440380096435547,
                    "sentence": "rather consisently performs worse than the alternatives.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9181270003318787,
                    "sentence": "In any case, what is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9099457859992981,
                    "sentence": "the motivation for learning representations for words and senses in a shared",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9413981437683105,
                    "sentence": "semantic space?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9170748591423035,
                    "sentence": "This is not entirely clear and never really discussed in the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9740493893623352,
                    "sentence": "paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8941859602928162,
                    "sentence": "2. The motivation for, or intuition behind, predicting pre-trained embeddings",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9207279086112976,
                    "sentence": "is not explicitly stated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.874613344669342,
                    "sentence": "Also, are the pre-trained embeddings in the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.16013170778751373,
                    "sentence": "LSTMEmbed_SW model representations for words or senses, or is a sum of these",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05980102717876434,
                    "sentence": "used again?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1763690561056137,
                    "sentence": "If different alternatives are possible, which setup is used in the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13679835200309753,
                    "sentence": "experiments?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11315356194972992,
                    "sentence": "3. The importance of learning sense embeddings is well recognized and also",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.231146901845932,
                    "sentence": "stressed by the authors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.183641254901886,
                    "sentence": "Unfortunately, however, it seems that these are never",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15336331725120544,
                    "sentence": "really evaluated; if they are, this remains unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.25274792313575745,
                    "sentence": "Most or all of the word",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09144610166549683,
                    "sentence": "similarity datasets considers words independent of context.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15848039090633392,
                    "sentence": "4. What is the size of the training corpora?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11920072138309479,
                    "sentence": "For instance, using different",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21094918251037598,
                    "sentence": "proportions of BabelWiki and SEW is shown in Figure 4; however, the comparison",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15521632134914398,
                    "sentence": "is somewhat problematic if the sizes are substantially different.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21853847801685333,
                    "sentence": "The size of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.19575491547584534,
                    "sentence": "SemCor is moreover really small and one would typically not use such a small",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1120152398943901,
                    "sentence": "corpus for learning embeddings with, e.g., word2vec.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15267027914524078,
                    "sentence": "If the proposed models",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10656791925430298,
                    "sentence": "favor small corpora, this should be stated and evaluated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.16238726675510406,
                    "sentence": "5. Some of the test sets are not independent, i.e.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2409919947385788,
                    "sentence": "WS353, WSSim and WSRel,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1330753117799759,
                    "sentence": "which makes comparisons problematic, in this case giving three \"wins\" as",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2818128168582916,
                    "sentence": "opposed to one.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17407217621803284,
                    "sentence": "6. The proposed models are said to be faster to train by using pre-trained",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.16462013125419617,
                    "sentence": "embeddings in the output layer.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10246114432811737,
                    "sentence": "However, no evidence to support this claim is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.077080138027668,
                    "sentence": "provided.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10852251946926117,
                    "sentence": "This would strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.20876286923885345,
                    "sentence": "7. Table 4: why not use the same dimensionality for a fair(er) comparison?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2502523362636566,
                    "sentence": "8. A section on synonym identification is missing under similarity measurement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1636219471693039,
                    "sentence": "that would describe how the multiple-choice task is approached.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2888204753398895,
                    "sentence": "9. A reference to Table 2 is missing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17114512622356415,
                    "sentence": "10. There is no description of any training for the word analogy task, which is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08845914155244827,
                    "sentence": "mentioned when describing the corresponding dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 38,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 40,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 41,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 42,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 44,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 49,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 55,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 56,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 58,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 59,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 60,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 61,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 63,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 65,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 66,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 67,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 68,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 69,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 70,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.4973606023242839,
            "class_probabilities": {
                "human": 0.5018362882844575,
                "ai": 0.4973606023242839,
                "mixed": 0.0008031093912586005
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5018362882844575,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.4973606023242839,
                    "human": 0.5018362882844575,
                    "mixed": 0.0008031093912586005
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "- Strengths:\n1. The presentation of the paper, up until the final few sections, is excellent\nand the paper reads very well at the start. The paper has a clear structure and\nthe argumentation is, for the most part, good.\n2. The paper addresses an important problem by attempting to incorporate word\norder information into word (and sense) embeddings and the proposed solution is\ninteresting.\n- Weaknesses:\n 1. Unfortunately, the results are rather inconsistent and one is not left\nentirely convinced that the proposed models are better than the alternatives,\nespecially given the added complexity. Negative results are fine, but there is\ninsufficient analysis to learn from them. Moreover, no results are reported on\nthe word analogy task, besides being told that the proposed models were not\ncompetitive - this could have been interesting and analyzed further.\n2. Some aspects of the experimental setup were unclear or poorly motivated, for\ninstance w.r.t. to corpora and datasets (see details below).\n3. Unfortunately, the quality of the paper deteriorates towards the end and the\nreader is left a little disappointed, not only w.r.t. to the results but with\nthe quality of the presentation and the argumentation.\n- General Discussion:\n1. The authors aim \"to learn representations for both words and senses in a\nshared emerging space\". This is only done in the LSTMEmbed_SW version, which\nrather consisently performs worse than the alternatives. In any case, what is\nthe motivation for learning representations for words and senses in a shared\nsemantic space? This is not entirely clear and never really discussed in the\npaper.\n2. The motivation for, or intuition behind, predicting pre-trained embeddings\nis not explicitly stated. Also, are the pre-trained embeddings in the\nLSTMEmbed_SW model representations for words or senses, or is a sum of these\nused again? If different alternatives are possible, which setup is used in the\nexperiments?\n3. The importance of learning sense embeddings is well recognized and also\nstressed by the authors. Unfortunately, however, it seems that these are never\nreally evaluated; if they are, this remains unclear. Most or all of the word\nsimilarity datasets considers words independent of context.\n4. What is the size of the training corpora? For instance, using different\nproportions of BabelWiki and SEW is shown in Figure 4; however, the comparison\nis somewhat problematic if the sizes are substantially different. The size of\nSemCor is moreover really small and one would typically not use such a small\ncorpus for learning embeddings with, e.g., word2vec. If the proposed models\nfavor small corpora, this should be stated and evaluated.\n5. Some of the test sets are not independent, i.e. WS353, WSSim and WSRel,\nwhich makes comparisons problematic, in this case giving three \"wins\" as\nopposed to one.\n6. The proposed models are said to be faster to train by using pre-trained\nembeddings in the output layer. However, no evidence to support this claim is\nprovided. This would strengthen the paper.\n7. Table 4: why not use the same dimensionality for a fair(er) comparison?\n8. A section on synonym identification is missing under similarity measurement\nthat would describe how the multiple-choice task is approached.\n9. A reference to Table 2 is missing.\n10. There is no description of any training for the word analogy task, which is\nmentioned when describing the corresponding dataset."
        }
    ]
}