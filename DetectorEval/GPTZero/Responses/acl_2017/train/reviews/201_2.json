{
    "version": "2025-01-09-base",
    "scanId": "17ccb8f3-e2f0-4774-994f-ad71147baa9a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.00765230180695653,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006684544496238232,
                    "sentence": "Evaluating bag of words and \"bound\" contexts from either dependencies or",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007947520352900028,
                    "sentence": "sentence ordering is important, and will be a useful reference to the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006391383707523346,
                    "sentence": "community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009853623807430267,
                    "sentence": "The experiments were relatively thorough (though some choices could",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010856430977582932,
                    "sentence": "use further justification), and the authors used downstream tasks instead of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0077581568621098995,
                    "sentence": "just intrinsic evaluations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.036381132900714874,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009053241461515427,
                    "sentence": "The authors change the objective function of GBOW from p(c\"\\sum w_i) to",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010134955868124962,
                    "sentence": "p(w\"\\sum c_i).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013647543266415596,
                    "sentence": "This is somewhat justified as dependency-based context with a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01532244123518467,
                    "sentence": "bound representation only has one word available for predicting the context,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008612976409494877,
                    "sentence": "but it's unclear exactly why that is the case and deserves more discussion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006687551271170378,
                    "sentence": "Presumably the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0077680554240942,
                    "sentence": "non-dependency context with a bound representation would also suffer from this",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01199561357498169,
                    "sentence": "drawback?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006172780878841877,
                    "sentence": "If so, how did Ling et al., 2015 do it?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004937240853905678,
                    "sentence": "Unfortunately, the authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006150467321276665,
                    "sentence": "don't compare any results against the original objective, which is a definite",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008888829499483109,
                    "sentence": "weakness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00794158224016428,
                    "sentence": "In addition, the authors change GSG to match GBOW, again without",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0060794176533818245,
                    "sentence": "comparing to the original objective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012846228666603565,
                    "sentence": "Adding results from word vectors trained",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011399113573133945,
                    "sentence": "using the original GBOW and GSG objective functions would justify these changes",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010350270196795464,
                    "sentence": "(assuming the results don't show large changes).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005016372539103031,
                    "sentence": "The hyperparameter settings should be discussed further.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007573904935270548,
                    "sentence": "This played a large",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007430126424878836,
                    "sentence": "role in Levy et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006103207357227802,
                    "sentence": "(2015), so you should consider trying different",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00516621395945549,
                    "sentence": "hyperparameter values.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005093103274703026,
                    "sentence": "These depend pretty heavily on the task, so simply",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007123175077140331,
                    "sentence": "taking good values from another task may not work well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005331805907189846,
                    "sentence": "In addition, the authors are unclear on exactly what model is trained in",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006795445457100868,
                    "sentence": "section 3.4.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00349484127946198,
                    "sentence": "They say only that it is a \"simple linear classifier\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005095216445624828,
                    "sentence": "In section",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0046857125125825405,
                    "sentence": "3.5, they use logistic regression with the average of the word vectors as",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0038491017185151577,
                    "sentence": "input, but call it a Neural Bag-of-Words model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004361789673566818,
                    "sentence": "Technically previous work also",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005305126309394836,
                    "sentence": "used this name, but I find it misleading, since it's just logistic regression",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0028023328632116318,
                    "sentence": "(and hence a linear model, which is not something I would call \"Neural\").",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0012504233745858073,
                    "sentence": "It is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0037619243375957012,
                    "sentence": "important to know if the model trained in section 3.4 is the same as the model",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004302919842302799,
                    "sentence": "trained in 3.5, so we know if the different conclusions are the result of the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005672641098499298,
                    "sentence": "task or the model changing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018735263496637344,
                    "sentence": "- General Discussion: This paper evaluates context taken from dependency parses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010596660897135735,
                    "sentence": "vs context taken from word position in a given sentence, and bag-of-words vs",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.019030146300792694,
                    "sentence": "tokens with relative position indicators.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007898243144154549,
                    "sentence": "This paper is useful to the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01595436967909336,
                    "sentence": "community, as they show when and where researchers should use word vectors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.019007433205842972,
                    "sentence": "trained using these different decisions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014720810577273369,
                    "sentence": "- Emphasis to improve:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.20724958181381226,
                    "sentence": "The main takeaway from this paper that future researchers will use is given at",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18440426886081696,
                    "sentence": "the end of 3.4 and 3.5, but really should be summarized at the start of the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.30030733346939087,
                    "sentence": "paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3560059070587158,
                    "sentence": "Specifically, the authors should put in the abstract that for POS,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3452044129371643,
                    "sentence": "chunking, and NER, bound representations outperform bag-of-words",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5352487564086914,
                    "sentence": "representations, and that dependency contexts work better than linear contexts",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5354927778244019,
                    "sentence": "in most cases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5744422674179077,
                    "sentence": "In addition, for a simple text classification model, bound",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4458366930484772,
                    "sentence": "representations perform worse than bag-of-words representations, and there",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.43041908740997314,
                    "sentence": "seemed to be no major difference between the different models or context types.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.20345519483089447,
                    "sentence": "- Small points of improvement:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06589177995920181,
                    "sentence": "Should call \"unbounded\" context \"bag of words\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07955034077167511,
                    "sentence": "This may lead to some confusion",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08188939094543457,
                    "sentence": "as one of the techniques you use is Generalized Bag-Of-Words, but this can be",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11427079886198044,
                    "sentence": "clarified easily.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07328487932682037,
                    "sentence": "043: it's the \"distributional hypothesis\", not the \"Distributed Hypothesis\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05526868253946304,
                    "sentence": "069: citations should have a comma instead of semicolon separating them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.053173698484897614,
                    "sentence": "074: \"DEPS\" should be capitalized consistently throughout the paper (usually it",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.043060582131147385,
                    "sentence": "appears as \"Deps\").",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10149416327476501,
                    "sentence": "Also should be introduced as something like dependency",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0803350955247879,
                    "sentence": "parse tree context (Deps).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.050534334033727646,
                    "sentence": "085: typo: \"How different contexts affect model's performances...\" Should have",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023686688393354416,
                    "sentence": "the word \"do\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 36,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 40,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 42,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 43,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 44,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 45,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 49,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 51,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 54,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 56,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 57,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 58,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 60,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 61,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 62,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 63,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 65,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 66,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 67,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 68,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 69,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 70,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 72,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 73,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 74,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.041461834619104215,
            "class_probabilities": {
                "human": 0.9571272979089567,
                "ai": 0.041461834619104215,
                "mixed": 0.001410867471939271
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9571272979089567,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.041461834619104215,
                    "human": 0.9571272979089567,
                    "mixed": 0.001410867471939271
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "- Strengths: \nEvaluating bag of words and \"bound\" contexts from either dependencies or\nsentence ordering is important, and will be a useful reference to the\ncommunity. The experiments were relatively thorough (though some choices could\nuse further justification), and the authors used downstream tasks instead of\njust intrinsic evaluations.\n- Weaknesses: \nThe authors change the objective function of GBOW from p(c\"\\sum w_i) to\np(w\"\\sum c_i). This is somewhat justified as dependency-based context with a\nbound representation only has one word available for predicting the context,\nbut it's unclear exactly why that is the case and deserves more discussion.\nPresumably the\nnon-dependency context with a bound representation would also suffer from this\ndrawback? If so, how did Ling et al., 2015 do it? Unfortunately, the authors\ndon't compare any results against the original objective, which is a definite\nweakness. In addition, the authors change GSG to match GBOW, again without\ncomparing to the original objective. Adding results from word vectors trained\nusing the original GBOW and GSG objective functions would justify these changes\n(assuming the results don't show large changes).\nThe hyperparameter settings should be discussed further. This played a large\nrole in Levy et al. (2015), so you should consider trying different\nhyperparameter values. These depend pretty heavily on the task, so simply\ntaking good values from another task may not work well.\nIn addition, the authors are unclear on exactly what model is trained in\nsection 3.4. They say only that it is a \"simple linear classifier\". In section\n3.5, they use logistic regression with the average of the word vectors as\ninput, but call it a Neural Bag-of-Words model. Technically previous work also\nused this name, but I find it misleading, since it's just logistic regression\n(and hence a linear model, which is not something I would call \"Neural\"). It is\nimportant to know if the model trained in section 3.4 is the same as the model\ntrained in 3.5, so we know if the different conclusions are the result of the\ntask or the model changing. \n- General Discussion: This paper evaluates context taken from dependency parses\nvs context taken from word position in a given sentence, and bag-of-words vs\ntokens with relative position indicators. This paper is useful to the\ncommunity, as they show when and where researchers should use word vectors\ntrained using these different decisions. \n- Emphasis to improve:\nThe main takeaway from this paper that future researchers will use is given at\nthe end of 3.4 and 3.5, but really should be summarized at the start of the\npaper. Specifically, the authors should put in the abstract that for POS,\nchunking, and NER, bound representations outperform bag-of-words\nrepresentations, and that dependency contexts work better than linear contexts\nin most cases. In addition, for a simple text classification model, bound\nrepresentations perform worse than bag-of-words representations, and there\nseemed to be no major difference between the different models or context types.\n- Small points of improvement: \nShould call \"unbounded\" context \"bag of words\". This may lead to some confusion\nas one of the techniques you use is Generalized Bag-Of-Words, but this can be\nclarified easily.\n043: it's the \"distributional hypothesis\", not the \"Distributed Hypothesis\". \n069: citations should have a comma instead of semicolon separating them.\n074: \"DEPS\" should be capitalized consistently throughout the paper (usually it\nappears as \"Deps\"). Also should be introduced as something like dependency\nparse tree context (Deps).\n085: typo: \"How different contexts affect model's performances...\" Should have\nthe word \"do\"."
        }
    ]
}