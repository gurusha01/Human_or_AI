{
    "version": "2025-01-09-base",
    "scanId": "cf844773-42fa-4ca0-9bd0-f39ffcf51e5c",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0066948398016393185,
                    "sentence": "This paper introduces a new approach to semantic parsing in which the model is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004937358666211367,
                    "sentence": "equipped with a neural sequence to sequence (seq2seq) model (referred to as the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0023787289392203093,
                    "sentence": "\"programmer\") which encodes a natural language question and produces a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006710145156830549,
                    "sentence": "program.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0044555384665727615,
                    "sentence": "The programmer is also equipped with a 'key variable' memory",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003884710604324937,
                    "sentence": "component which stores (a) entities in the questions (b) values of intermediate",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004363981541246176,
                    "sentence": "variables formed during execution of intermediate programs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009914725087583065,
                    "sentence": "These variables are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005255833733826876,
                    "sentence": "referred to further build the program.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00616821926087141,
                    "sentence": "The model is also equipped",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004937953315675259,
                    "sentence": "with",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013226461596786976,
                    "sentence": "certain",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005133265163749456,
                    "sentence": "discrete operations (such as argmax or 'hop to next edges in a KB').",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00840817391872406,
                    "sentence": "A separate",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0037111330311745405,
                    "sentence": "component (\"interpreter/computer\") executes these operations and stores",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004090390633791685,
                    "sentence": "intermediate values (as explained before).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0030948601197451353,
                    "sentence": "Since the 'programmer' is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003507721470668912,
                    "sentence": "inherently a seq2seq model, the \"interpreter/computer\" also acts as a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00629613408818841,
                    "sentence": "syntax/type checker only allowing the decoder to generate valid tokens.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0034267303999513388,
                    "sentence": "For",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005545472260564566,
                    "sentence": "example, the second argument to the \"hop\" operation has to be a KB",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004798153415322304,
                    "sentence": "predicate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0068198819644749165,
                    "sentence": "Finally the model is trained with weak supervision and directly",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006685792934149504,
                    "sentence": "optimizes the metric which is used to evaluate the performance (F score).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004916063509881496,
                    "sentence": "Because of the discrete operations and the non differentiable reward functions,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007085893303155899,
                    "sentence": "the model is trained with policy gradients (REINFORCE).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0056554898619651794,
                    "sentence": "Since gradients",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005467806477099657,
                    "sentence": "obtained through REINFORCE have high variance, it is common to first pretrain",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009260425344109535,
                    "sentence": "the model with a max-likelihood objective or find some good sequences of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0052704536356031895,
                    "sentence": "actions trained through some auxiliary objective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008520377799868584,
                    "sentence": "This paper takes a latter",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01810462400317192,
                    "sentence": "approach in which it finds good sequences via an iterative maximum likelihood",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012934370897710323,
                    "sentence": "approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014665581285953522,
                    "sentence": "The results and discussion sections are presented in a very nice way",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021302685141563416,
                    "sentence": "and the model achieves SOTA results on the WebQuestions dataset when compared",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.029212908819317818,
                    "sentence": "to other weakly supervised model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016504935920238495,
                    "sentence": "The paper is written clearly and is very easy to follow.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016201892867684364,
                    "sentence": "This paper presents a new and exciting direction and there is scope for a lot",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02336430735886097,
                    "sentence": "of future research in this direction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016431936994194984,
                    "sentence": "I would definitely love to see this",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01921568252146244,
                    "sentence": "presented in the conference.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02018902450799942,
                    "sentence": "Questions for the authors (important ones first)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017631536349654198,
                    "sentence": "1. Another alternative way of training the model would be to bootstrap the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023535732179880142,
                    "sentence": "parameters (\\theta) from the iterative ML method instead of adding pseudo gold",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02443373017013073,
                    "sentence": "programs in the beam (Line 510 would be deleted).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01415042020380497,
                    "sentence": "Did you try that and if so",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0131695456802845,
                    "sentence": "why do you think it didn't work?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02021441049873829,
                    "sentence": "2. What was the baseline model in REINFORCE.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014156734570860863,
                    "sentence": "Did you have a separate network",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01252111978828907,
                    "sentence": "which predicts the value function.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013173054903745651,
                    "sentence": "This must be discussed in the paper in",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01676712930202484,
                    "sentence": "detail.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01452787034213543,
                    "sentence": "3. Were there programs which required multiple hop operations?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011662102304399014,
                    "sentence": "Or were they",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.028914468362927437,
                    "sentence": "limited to single hops.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010585012845695019,
                    "sentence": "If there were, can you provide an example?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014970875345170498,
                    "sentence": "(I will",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017237426713109016,
                    "sentence": "understand if you are bound by word limit of the response)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013987980782985687,
                    "sentence": "4. Can you give an example where the filter operation would be used?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014158214442431927,
                    "sentence": "5. I did not follow the motivation behind replacing the entities in the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018247829750180244,
                    "sentence": "question with special ENT symbol",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022460272535681725,
                    "sentence": "Minor comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.048002831637859344,
                    "sentence": "Line 161 describe -> describing",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023202521726489067,
                    "sentence": "Line 318 decoder reads ')' -> decoder generates ')'",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 36,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 38,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 40,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 41,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 42,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 43,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 44,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 49,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 51,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 54,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 57,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 58,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 59,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 60,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 61,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 62,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 63,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.021596770642208406,
            "class_probabilities": {
                "human": 0.9781216349765232,
                "ai": 0.021596770642208406,
                "mixed": 0.00028159438126836863
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9781216349765232,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.021596770642208406,
                    "human": 0.9781216349765232,
                    "mixed": 0.00028159438126836863
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper introduces a new approach to semantic parsing in which the model is\nequipped with a neural sequence to sequence (seq2seq) model (referred to as the\n\"programmer\") which encodes a natural language question and produces a\nprogram. The programmer is also equipped with a 'key variable' memory\ncomponent which stores (a) entities in the questions (b) values of intermediate\nvariables formed during execution of intermediate programs. These variables are\nreferred to further build the program. The model is also equipped\nwith\ncertain\ndiscrete operations (such as argmax or 'hop to next edges in a KB'). A separate\ncomponent (\"interpreter/computer\") executes these operations and stores\nintermediate values (as explained before). Since the 'programmer' is\ninherently a seq2seq model, the \"interpreter/computer\" also acts as a\nsyntax/type checker only allowing the decoder to generate valid tokens. For\nexample, the second argument to the \"hop\" operation has to be a KB\npredicate. Finally the model is trained with weak supervision and directly\noptimizes the metric which is used to evaluate the performance (F score).\nBecause of the discrete operations and the non differentiable reward functions,\nthe model is trained with policy gradients (REINFORCE). Since gradients\nobtained through REINFORCE have high variance, it is common to first pretrain\nthe model with a max-likelihood objective or find some good sequences of\nactions trained through some auxiliary objective. This paper takes a latter\napproach in which it finds good sequences via an iterative maximum likelihood\napproach. The results and discussion sections are presented in a very nice way\nand the model achieves SOTA results on the WebQuestions dataset when compared\nto other weakly supervised model.\nThe paper is written clearly and is very easy to follow.\nThis paper presents a new and exciting direction and there is scope for a lot\nof future research in this direction. I would definitely love to see this\npresented in the conference.\nQuestions for the authors (important ones first)\n1. Another alternative way of training the model would be to bootstrap the\nparameters (\\theta) from the iterative ML method instead of adding pseudo gold\nprograms in the beam (Line 510 would be deleted). Did you try that and if so\nwhy do you think it didn't work?\n2. What was the baseline model in REINFORCE. Did you have a separate network\nwhich predicts the value function. This must be discussed in the paper in\ndetail.\n3. Were there programs which required multiple hop operations? Or were they\nlimited to single hops. If there were, can you provide an example? (I will\nunderstand if you are bound by word limit of the response)\n4. Can you give an example where the filter operation would be used?\n5. I did not follow the motivation behind replacing the entities in the\nquestion with special ENT symbol\nMinor comments:\nLine 161 describe -> describing\nLine 318 decoder reads ')' -> decoder generates ')'"
        }
    ]
}