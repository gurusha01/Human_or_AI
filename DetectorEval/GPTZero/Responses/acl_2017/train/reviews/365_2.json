{
    "version": "2025-01-09-base",
    "scanId": "6c249367-6d09-40bf-aa57-3ddb1cf6c852",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.12631197273731232,
                    "sentence": "Summary:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05781549587845802,
                    "sentence": "The paper applies a sequence to sequence (seq2seq) approach for German",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08928580582141876,
                    "sentence": "historical text normalization, and showed that using a grapheme-to-phoneme",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.058942534029483795,
                    "sentence": "generation as an auxiliary task in a multi-task learning (MTL) seq2seq",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03498503565788269,
                    "sentence": "framework improves performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12129228562116623,
                    "sentence": "The authors argue that the MTL approach",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0673544779419899,
                    "sentence": "replaces the need for an attention menchanism, showing experimentally that the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04505613073706627,
                    "sentence": "attention mechanism harms the MTL performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.274911493062973,
                    "sentence": "The authors also tried to show",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06101417914032936,
                    "sentence": "statistical correlation between the weights of an MTL normalizer and an",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09703531116247177,
                    "sentence": "attention-based one.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06194175034761429,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.054263923317193985,
                    "sentence": "1) Novel application of seq2seq to historical text correction, although it has",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05376140773296356,
                    "sentence": "been applied recently to sentence grammatical error identification [1].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09197185188531876,
                    "sentence": "2) Showed that using grapheme-to-phoneme as an auxiliary task in a MTL setting",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.042625755071640015,
                    "sentence": "improves text normalization accuracy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09424377977848053,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.058616526424884796,
                    "sentence": "1) Instead of arguing that the MTL approach replaces the attention mechanism, I",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04894197732210159,
                    "sentence": "think the authors should investigate why attention did not work on MTL, and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.058098141103982925,
                    "sentence": "perhaps modify the attention mechanism so that it would not harm performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.061271779239177704,
                    "sentence": "2) I think the authors should reference past seq2seq MTL work, such as [2] and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03352903202176094,
                    "sentence": "[3].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.060702357441186905,
                    "sentence": "The MTL work in [2] also worked on non-attention seq2seq models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10889644920825958,
                    "sentence": "3) This paper only tested on one German historical text data set of 44",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03143629431724548,
                    "sentence": "documents.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09356628358364105,
                    "sentence": "It would be interesting if the authors can evaluate the same",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13234157860279083,
                    "sentence": "approach in another language or data set.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2534857392311096,
                    "sentence": "References:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0725179985165596,
                    "sentence": "[1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09010109305381775,
                    "sentence": "2016.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12239179760217667,
                    "sentence": "Sentence-level grammatical error identification as sequence-to-sequence",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10193058103322983,
                    "sentence": "correction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.19155174493789673,
                    "sentence": "In Proceedings of the 11th Workshop on Innovative Use of NLP for",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09675823152065277,
                    "sentence": "Building Educational Applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10723131895065308,
                    "sentence": "[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11505438387393951,
                    "sentence": "Kaiser.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09794317185878754,
                    "sentence": "Multi-task Sequence to Sequence Learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03978239744901657,
                    "sentence": "ICLR'16.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.16003279387950897,
                    "sentence": "[3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2583712339401245,
                    "sentence": "Multi-task learning for multiple language translation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04182523488998413,
                    "sentence": "ACL'15",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6024643778800964,
                    "sentence": "---------------------------",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.32626664638519287,
                    "sentence": "Here is my reply to the authors' rebuttal:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.34510526061058044,
                    "sentence": "I am keeping my review score of 3, which means I do not object to accepting the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5008230209350586,
                    "sentence": "paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4151984453201294,
                    "sentence": "However, I am not raising my score for 2 reasons:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4689929187297821,
                    "sentence": "* the authors did not respond to my questions about other papers on seq2seq",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.28134194016456604,
                    "sentence": "MTL, which also avoided using attention mechanism.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2636774778366089,
                    "sentence": "So in terms of novelty, the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5232776403427124,
                    "sentence": "main novelty lies in applying it to text normalization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.46847084164619446,
                    "sentence": "* it is always easier to show something (i.e.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5057032108306885,
                    "sentence": "attention in seq2seq MTL) is not",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6714317202568054,
                    "sentence": "working, but the value would lie in finding out why it fails and changing the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5289474129676819,
                    "sentence": "attention mechanism so that it works.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 38,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 41,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 42,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 43,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 44,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 49,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.1276676162807097,
            "class_probabilities": {
                "human": 0.8722807660846754,
                "ai": 0.1276676162807097,
                "mixed": 5.161763461484218e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.8722807660846754,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.1276676162807097,
                    "human": 0.8722807660846754,
                    "mixed": 5.161763461484218e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary:\nThe paper applies a sequence to sequence (seq2seq) approach for German\nhistorical text normalization, and showed that using a grapheme-to-phoneme\ngeneration as an auxiliary task in a multi-task learning (MTL) seq2seq\nframework improves performance. The authors argue that the MTL approach\nreplaces the need for an attention menchanism, showing experimentally that the\nattention mechanism harms the MTL performance. The authors also tried to show\nstatistical correlation between the weights of an MTL normalizer and an\nattention-based one.\nStrengths:\n1) Novel application of seq2seq to historical text correction, although it has\nbeen applied recently to sentence grammatical error identification [1]. \n2) Showed that using grapheme-to-phoneme as an auxiliary task in a MTL setting\nimproves text normalization accuracy.\nWeaknesses:\n1) Instead of arguing that the MTL approach replaces the attention mechanism, I\nthink the authors should investigate why attention did not work on MTL, and\nperhaps modify the attention mechanism so that it would not harm performance.\n2) I think the authors should reference past seq2seq MTL work, such as [2] and\n[3]. The MTL work in [2] also worked on non-attention seq2seq models.\n3) This paper only tested on one German historical text data set of 44\ndocuments. It would be interesting if the authors can evaluate the same\napproach in another language or data set.\nReferences:\n[1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016.\nSentence-level grammatical error identification as sequence-to-sequence\ncorrection. In Proceedings of the 11th Workshop on Innovative Use of NLP for\nBuilding Educational Applications.\n[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz\nKaiser. Multi-task Sequence to Sequence Learning. ICLR'16. \n[3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng. \nMulti-task learning for multiple language translation. ACL'15\n---------------------------\nHere is my reply to the authors' rebuttal:\nI am keeping my review score of 3, which means I do not object to accepting the\npaper. However, I am not raising my score for 2 reasons:\n* the authors did not respond to my questions about other papers on seq2seq\nMTL, which also avoided using attention mechanism. So in terms of novelty, the\nmain novelty lies in applying it to text normalization.\n* it is always easier to show something (i.e. attention in seq2seq MTL) is not\nworking, but the value would lie in finding out why it fails and changing the\nattention mechanism so that it works."
        }
    ]
}