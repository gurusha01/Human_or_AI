{
    "version": "2025-01-09-base",
    "scanId": "10863f29-f26c-4b9e-b765-3fdf00309a23",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.4775319993495941,
                    "sentence": "[update after reading author response: the alignment of the hidden units does",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.560565710067749,
                    "sentence": "not match with my intuition and experience, but I'm willing to believe I'm",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7020179033279419,
                    "sentence": "wrong in this case.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.49018266797065735,
                    "sentence": "Discussing the alignment in the paper is important (and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.48106691241264343,
                    "sentence": "maybe just sanity-checking that the alignment goes away if you initialize with",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6610015034675598,
                    "sentence": "a different seed).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.29114267230033875,
                    "sentence": "If what you're saying about how the new model is very",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21570011973381042,
                    "sentence": "different but only a little better performing -- a 10% error reduction -- then",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.22520557045936584,
                    "sentence": "I wonder about an ensemble of the new model and the old one.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.20212198793888092,
                    "sentence": "Seems like",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21685656905174255,
                    "sentence": "ensembling would provide a nice boost if the failures across models are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2447594702243805,
                    "sentence": "distinct, right?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2517867982387543,
                    "sentence": "Anyhow this is a solid paper and I appreciate the author",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2026410847902298,
                    "sentence": "response, I raise my review score to a 4.]",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2781558334827423,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2364272177219391,
                    "sentence": "1) Evidence of the attention-MTL connection is interesting",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.25422143936157227,
                    "sentence": "2) Methods are appropriate, models perform well relative to state-of-the-art",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2086746096611023,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3716736435890198,
                    "sentence": "1) Critical detail is not provided in the paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.24072930216789246,
                    "sentence": "2) Models are not particularly novel",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5089165568351746,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6237369775772095,
                    "sentence": "This paper presents a new method for historical text normalization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7734014987945557,
                    "sentence": "The model",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4551059901714325,
                    "sentence": "performs well, but the primary contribution of the paper ends up being a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.44452688097953796,
                    "sentence": "hypothesis that attention mechanisms in the task can be learned via multi-task",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5393802523612976,
                    "sentence": "learning, where the auxiliary task is a pronunciation task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.33267301321029663,
                    "sentence": "This connection",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5588250160217285,
                    "sentence": "between attention and MTL is interesting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5315640568733215,
                    "sentence": "There are two major areas for improvement in this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5088186264038086,
                    "sentence": "The first is that we",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.46572843194007874,
                    "sentence": "are given almost no explanation as to why the pronunciation task would somehow",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6005712747573853,
                    "sentence": "require an attention mechanism similar to that used for the normalization task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3729398250579834,
                    "sentence": "Why the two tasks (normalization and pronunciation) are related is mentioned",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5557223558425903,
                    "sentence": "in the paper: spelling variation often stems from variation in pronunciation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6413655877113342,
                    "sentence": "But why would doing MTL on both tasks result in an implicit attention mechanism",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.44915148615837097,
                    "sentence": "(and in fact, one that is then only hampered by the inclusion of an explicit",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8515149354934692,
                    "sentence": "attention mechanism?).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4597161114215851,
                    "sentence": "This remains a mystery.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.36312171816825867,
                    "sentence": "The paper can",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5090516805648804,
                    "sentence": "leave some",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.45739951729774475,
                    "sentence": "questions unanswered, but at least a suggestion of an answer to this one would",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5505331158638,
                    "sentence": "strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.31199514865875244,
                    "sentence": "The other concern is clarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.46741268038749695,
                    "sentence": "While the writing in this paper is clear, a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4525758624076843,
                    "sentence": "number of details are omitted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.30078956484794617,
                    "sentence": "The most important one is the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5957137942314148,
                    "sentence": "description",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3691796660423279,
                    "sentence": "of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5185877680778503,
                    "sentence": "the attention mechanism itself.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.32207491993904114,
                    "sentence": "Given the central role that method plays, it",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.38212838768959045,
                    "sentence": "should be described in detail in the paper rather than referring to previous",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13214819133281708,
                    "sentence": "work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.32351967692375183,
                    "sentence": "I did not understand the paragraph about this in Sec 3.4.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.38690146803855896,
                    "sentence": "Other questions included why you can compare the output vectors of two models",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.38328903913497925,
                    "sentence": "(Figure 4), while the output dimensions are the same I don't understand why the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6457083225250244,
                    "sentence": "hidden layer dimensions of two models would ever be comparable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5865188241004944,
                    "sentence": "Usually how",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.45955806970596313,
                    "sentence": "the hidden states are \"organized\" is completely different for every model, at",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4736277461051941,
                    "sentence": "the very least it is permuted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3997945487499237,
                    "sentence": "So I really did not understand",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4081406891345978,
                    "sentence": "Figure 4.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3839350938796997,
                    "sentence": "The Kappa statistic for attention vs. MTL needs to be compared to the same",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4422420263290405,
                    "sentence": "statistic for each of those models vs. the base model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3759338855743408,
                    "sentence": "At the end of Sec 5, is that row < 0.21 an upper bound across all data sets?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.29806748032569885,
                    "sentence": "Lastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4317089319229126,
                    "sentence": "approaches make large changes to the model (comparing e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.36206749081611633,
                    "sentence": "Fig 5) but the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6754696369171143,
                    "sentence": "experimental improvements in accuracy for either model are quite small (2%),",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5233341455459595,
                    "sentence": "which seems like a bit of a contradiction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 36,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.07332528267997859
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 40,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 41,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 42,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 44,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 48,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 51,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 54,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 55,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 57,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 58,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 60,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 61,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 62,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 63,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 64,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 65,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 67,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 68,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.24226556879047087,
            "class_probabilities": {
                "human": 0.7564153809257333,
                "ai": 0.24226556879047087,
                "mixed": 0.0013190502837956669
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.7564153809257333,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.24226556879047087,
                    "human": 0.7564153809257333,
                    "mixed": 0.0013190502837956669
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "[update after reading author response: the alignment of the hidden units does\nnot match with my intuition and experience, but I'm willing to believe I'm\nwrong in this case. Discussing the alignment in the paper is important (and\nmaybe just sanity-checking that the alignment goes away if you initialize with\na different seed). If what you're saying about how the new model is very\ndifferent but only a little better performing -- a 10% error reduction -- then\nI wonder about an ensemble of the new model and the old one. Seems like\nensembling would provide a nice boost if the failures across models are\ndistinct, right? Anyhow this is a solid paper and I appreciate the author\nresponse, I raise my review score to a 4.]\n- Strengths:\n 1) Evidence of the attention-MTL connection is interesting\n 2) Methods are appropriate, models perform well relative to state-of-the-art\n- Weaknesses:\n 1) Critical detail is not provided in the paper\n 2) Models are not particularly novel\n- General Discussion:\nThis paper presents a new method for historical text normalization. The model\nperforms well, but the primary contribution of the paper ends up being a\nhypothesis that attention mechanisms in the task can be learned via multi-task\nlearning, where the auxiliary task is a pronunciation task. This connection\nbetween attention and MTL is interesting.\nThere are two major areas for improvement in this paper. The first is that we\nare given almost no explanation as to why the pronunciation task would somehow\nrequire an attention mechanism similar to that used for the normalization task.\n Why the two tasks (normalization and pronunciation) are related is mentioned\nin the paper: spelling variation often stems from variation in pronunciation. \nBut why would doing MTL on both tasks result in an implicit attention mechanism\n(and in fact, one that is then only hampered by the inclusion of an explicit\nattention mechanism?). This remains a mystery. The paper can\nleave some\nquestions unanswered, but at least a suggestion of an answer to this one would\nstrengthen the paper.\nThe other concern is clarity. While the writing in this paper is clear, a\nnumber of details are omitted. The most important one is the\ndescription\nof\nthe attention mechanism itself. Given the central role that method plays, it\nshould be described in detail in the paper rather than referring to previous\nwork. I did not understand the paragraph about this in Sec 3.4.\nOther questions included why you can compare the output vectors of two models\n(Figure 4), while the output dimensions are the same I don't understand why the\nhidden layer dimensions of two models would ever be comparable. Usually how\nthe hidden states are \"organized\" is completely different for every model, at\nthe very least it is permuted. So I really did not understand\nFigure 4.\nThe Kappa statistic for attention vs. MTL needs to be compared to the same\nstatistic for each of those models vs. the base model.\nAt the end of Sec 5, is that row < 0.21 an upper bound across all data sets?\nLastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL\napproaches make large changes to the model (comparing e.g. Fig 5) but the\nexperimental improvements in accuracy for either model are quite small (2%),\nwhich seems like a bit of a contradiction."
        }
    ]
}