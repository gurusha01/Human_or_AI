{
    "version": "2025-01-09-base",
    "scanId": "3b3ae99b-f607-4795-9c00-a0264e976bdc",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.09295044094324112,
                    "sentence": "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10111092031002045,
                    "sentence": "advantages in training and decoding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09839630126953125,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08046075701713562,
                    "sentence": "It provides a solid work of hybrid CTC-attention framework in training and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13736900687217712,
                    "sentence": "decoding, and the experimental results showed that the proposed method could",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13926038146018982,
                    "sentence": "provide an improvement in Japanese CSJ and Mandarin Chinese telephone speech",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.14690381288528442,
                    "sentence": "recognition task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18248827755451202,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08727748692035675,
                    "sentence": "The only problem is that the paper sounds too similar with Ref [Kim et al.,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07121822983026505,
                    "sentence": "2016] which will be officially published in the coming IEEE International",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05858346074819565,
                    "sentence": "Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.041514940559864044,
                    "sentence": "Kim at al., 2016, proposes joint CTC-attention using MTL for English ASR task,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04760441184043884,
                    "sentence": "and this paper proposes joint CTC-attention using MTL+joint decoding for",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02784249559044838,
                    "sentence": "Japanese and Chinese ASR tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021445121616125107,
                    "sentence": "I guess the difference is on joint decoding and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005593476351350546,
                    "sentence": "the application to Japanese/Chinese ASR tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014552153646945953,
                    "sentence": "However, the difference is not",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01887660287320614,
                    "sentence": "clearly explained by the authors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.026536891236901283,
                    "sentence": "So it took sometimes to figure out the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017124006524682045,
                    "sentence": "original contribution of this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00579490652307868,
                    "sentence": "(a) Title:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007158681750297546,
                    "sentence": "The title in Ref [Kim et al., 2016] is \"Joint CTC- Attention Based End-to-End",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006281752604991198,
                    "sentence": "Speech Recognition Using Multi-task Learning\", while the title of this paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006768417079001665,
                    "sentence": "is \"Joint CTC-attention End-to-end Speech Recognition\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005811023525893688,
                    "sentence": "I think the title",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006065039429813623,
                    "sentence": "is too general.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007161814719438553,
                    "sentence": "If this is the first paper about \"Joint CTC-attention\" than it",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0053864442743361,
                    "sentence": "is absolutely OK. Or if Ref [Kim et al., 2016] will remain only as",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010460923425853252,
                    "sentence": "pre-published arXiv, then it might be still acceptable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005546370521187782,
                    "sentence": "But since [Kim et al.,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009039557538926601,
                    "sentence": "2016] will officially publish in IEEE conference, much earlier than this paper,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018285002559423447,
                    "sentence": "then a more specified title that represents the main contribution of this paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.030553819611668587,
                    "sentence": "in contrast with the existing publication would be necessary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0052883257158100605,
                    "sentence": "(b) Introduction:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01772395148873329,
                    "sentence": "The author claims that \"We propose to take advantage of the constrained CTC",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015859287232160568,
                    "sentence": "alignment in a hybrid CTC-attention based system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009572221897542477,
                    "sentence": "During training, we attach a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01600578986108303,
                    "sentence": "CTC objective to an attention-based encoder network as a regularization, as",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008997517637908459,
                    "sentence": "proposed by [Kim at al., 2016].\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03419611230492592,
                    "sentence": "Taking advantage of the constrained CTC",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013402869924902916,
                    "sentence": "alignment in a hybrid CTC-attention is the original idea from [Kim at al.,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0026554465293884277,
                    "sentence": "2016].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013686705380678177,
                    "sentence": "So the whole argument about attention-based end-to-end ASR versus",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01783028617501259,
                    "sentence": "CTC-based ASR, and the necessary of CTC-attention combination is not novel.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01951802894473076,
                    "sentence": "Furthermore, the statement \"we propose \"¦ as proposed by [Kim et al,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014183927327394485,
                    "sentence": "2016]\" is somewhat weird.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8608613014221191,
                    "sentence": "We can build upon someone proposal with additional",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9430520534515381,
                    "sentence": "extensions, but not just re-propose other people's proposal.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9316348433494568,
                    "sentence": "Therefore, what",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9719677567481995,
                    "sentence": "would be important here is to state clearly the original contribution of this",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9708616733551025,
                    "sentence": "paper and the position of the proposed method with respect to existing",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9193912744522095,
                    "sentence": "literature",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9149344563484192,
                    "sentence": "(c) Experimental Results:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9388993978500366,
                    "sentence": "Kim at al., 2016 applied the proposed method on English task, while this paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8959786891937256,
                    "sentence": "applied the proposed method on Japanese and Mandarin Chinese tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9185861945152283,
                    "sentence": "I think it",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9187039732933044,
                    "sentence": "would be interesting if the paper could explain in more details about the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9337578415870667,
                    "sentence": "specific problems in Japanese and Mandarin Chinese tasks that may not appear in",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9910383224487305,
                    "sentence": "English task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9493798613548279,
                    "sentence": "For example, how the system could address multiple possible",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9088799357414246,
                    "sentence": "outputs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9127070903778076,
                    "sentence": "i.e., Kanji, Hiragana, and Katakana given Japanese speech input",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8857839703559875,
                    "sentence": "without using any linguistic resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9561049342155457,
                    "sentence": "This could be one of the important",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9292234182357788,
                    "sentence": "contributions from this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9689406752586365,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.938072681427002,
                    "sentence": "I think it would be better to cite Ref [Kim et al., 2016] from",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9547474384307861,
                    "sentence": "the official IEEE ICASSP conference, rather than pre-published arXiv:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9510564208030701,
                    "sentence": "Kim, S., Hori, T., Watanabe, S., \"Joint CTC- Attention Based End-to-End Speech",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9587056636810303,
                    "sentence": "Recognition Using Multi-task Learning\", IEEE International Conference on",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9628315567970276,
                    "sentence": "Acoustics, Speech, and Signal Processing (ICASSP), March 2017, pp.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9642757773399353,
                    "sentence": "to appear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 38,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 40,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 41,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 43,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 44,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 45,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 49,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 51,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 54,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 56,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 57,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 58,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 60,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 62,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 64,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 65,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 66,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 67,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 68,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 69,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 70,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.39724073960248324,
            "class_probabilities": {
                "human": 0.6019043255234381,
                "ai": 0.39724073960248324,
                "mixed": 0.0008549348740787098
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.6019043255234381,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.39724073960248324,
                    "human": 0.6019043255234381,
                    "mixed": 0.0008549348740787098
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. \n- Strengths:\nIt provides a solid work of hybrid CTC-attention framework in training and\ndecoding, and the experimental results showed that the proposed method could\nprovide an improvement in Japanese CSJ and Mandarin Chinese telephone speech\nrecognition task. \n- Weaknesses:\nThe only problem is that the paper sounds too similar with Ref [Kim et al.,\n2016] which will be officially published in the coming IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017.\nKim at al., 2016, proposes joint CTC-attention using MTL for English ASR task,\nand this paper proposes joint CTC-attention using MTL+joint decoding for\nJapanese and Chinese ASR tasks. I guess the difference is on joint decoding and\nthe application to Japanese/Chinese ASR tasks. However, the difference is not\nclearly explained by the authors. So it took sometimes to figure out the\noriginal contribution of this paper.\n(a) Title: \nThe title in Ref [Kim et al., 2016] is \"Joint CTC- Attention Based End-to-End\nSpeech Recognition Using Multi-task Learning\", while the title of this paper\nis \"Joint CTC-attention End-to-end Speech Recognition\". I think the title\nis too general. If this is the first paper about \"Joint CTC-attention\" than it\nis absolutely OK. Or if Ref [Kim et al., 2016] will remain only as\npre-published arXiv, then it might be still acceptable. But since [Kim et al.,\n2016] will officially publish in IEEE conference, much earlier than this paper,\nthen a more specified title that represents the main contribution of this paper\nin contrast with the existing publication would be necessary. \n(b) Introduction:\nThe author claims that \"We propose to take advantage of the constrained CTC\nalignment in a hybrid CTC-attention based system. During training, we attach a\nCTC objective to an attention-based encoder network as a regularization, as\nproposed by [Kim at al., 2016].\" Taking advantage of the constrained CTC\nalignment in a hybrid CTC-attention is the original idea from [Kim at al.,\n2016]. So the whole argument about attention-based end-to-end ASR versus\nCTC-based ASR, and the necessary of CTC-attention combination is not novel.\nFurthermore, the statement \"we propose \"¦ as proposed by [Kim et al,\n2016]\" is somewhat weird. We can build upon someone proposal with additional\nextensions, but not just re-propose other people's proposal. Therefore, what\nwould be important here is to state clearly the original contribution of this\npaper and the position of the proposed method with respect to existing\nliterature\n(c) Experimental Results:\nKim at al., 2016 applied the proposed method on English task, while this paper\napplied the proposed method on Japanese and Mandarin Chinese tasks. I think it\nwould be interesting if the paper could explain in more details about the\nspecific problems in Japanese and Mandarin Chinese tasks that may not appear in\nEnglish task. For example, how the system could address multiple possible\noutputs. i.e., Kanji, Hiragana, and Katakana given Japanese speech input\nwithout using any linguistic resources. This could be one of the important\ncontributions from this paper.\n- General Discussion:\nI think it would be better to cite Ref [Kim et al., 2016] from\nthe official IEEE ICASSP conference, rather than pre-published arXiv:\nKim, S., Hori, T., Watanabe, S., \"Joint CTC- Attention Based End-to-End Speech\nRecognition Using Multi-task Learning\", IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear."
        }
    ]
}