{
    "version": "2025-01-09-base",
    "scanId": "0442e44e-c788-4bce-a6b4-64a3d9cda312",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.02094501443207264,
                    "sentence": "The paper presents a new neural approach for summarization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.020250216126441956,
                    "sentence": "They build on a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.025693656876683235,
                    "sentence": "standard encoder-decoder with attention framework but add a network that gates",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02491588518023491,
                    "sentence": "every encoded hidden state based on summary vectors from initial encoding",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02071312628686428,
                    "sentence": "stages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015656569972634315,
                    "sentence": "Overall, the method seems to outperform standard seq2seq methods by 1-2",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.031402841210365295,
                    "sentence": "points on three different evaluation sets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02414824068546295,
                    "sentence": "Overall, the technical sections of the paper are reasonably clear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05257207527756691,
                    "sentence": "Equation 16",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04305349662899971,
                    "sentence": "needs more explanation, I could not understand the notation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04985826835036278,
                    "sentence": "The specific",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04815007373690605,
                    "sentence": "contribution, the selective mechanism, seems novel and could potentially be",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04412781819701195,
                    "sentence": "used in other contexts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.025056300684809685,
                    "sentence": "The evaluation is extensive and does demonstrate consistent improvement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05048006400465965,
                    "sentence": "One",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018715867772698402,
                    "sentence": "would imagine that adding an additional encoder layer instead of the selective",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0241974089294672,
                    "sentence": "layer is the most reasonable baseline (given the GRU baseline uses only one",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0316283218562603,
                    "sentence": "bi-GRU, this adds expressivity), and this seems to be implemented Luong-NMT.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018609648570418358,
                    "sentence": "My",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02368733659386635,
                    "sentence": "one concern is LSTM/GRU mismatch.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015525948256254196,
                    "sentence": "Is the benefit coming from just GRU switch?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01118842326104641,
                    "sentence": "The quality of the writing, especially in the intro/abstract/related work is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015716014429926872,
                    "sentence": "quite bad.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014871887862682343,
                    "sentence": "This paper does not make a large departure from previous work, and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01097581721842289,
                    "sentence": "therefore a related work nearby the introduction seems more appropriate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0046057384461164474,
                    "sentence": "In",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018108898773789406,
                    "sentence": "related work, one common good approach is highlighting similarities and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01956583373248577,
                    "sentence": "differences between your work and previous work, in words before they are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.019911011680960655,
                    "sentence": "presented in equations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014271091669797897,
                    "sentence": "Simply listing works without relating them to your work",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009579089470207691,
                    "sentence": "is not that useful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01082826592028141,
                    "sentence": "Placement of the related work near the intro will allow you",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017554208636283875,
                    "sentence": "to relieve the intro of significant background detail and instead focus on more",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04332704842090607,
                    "sentence": "high level.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.0375246461941804,
            "class_probabilities": {
                "human": 0.9624347023880775,
                "ai": 0.0375246461941804,
                "mixed": 4.065141774218333e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9624347023880775,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.0375246461941804,
                    "human": 0.9624347023880775,
                    "mixed": 4.065141774218333e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper presents a new neural approach for summarization. They build on a\nstandard encoder-decoder with attention framework but add a network that gates\nevery encoded hidden state based on summary vectors from initial encoding\nstages. Overall, the method seems to outperform standard seq2seq methods by 1-2\npoints on three different evaluation sets.\nOverall, the technical sections of the paper are reasonably clear. Equation 16\nneeds more explanation, I could not understand the notation. The specific\ncontribution, the selective mechanism, seems novel and could potentially be\nused in other contexts. \nThe evaluation is extensive and does demonstrate consistent improvement. One\nwould imagine that adding an additional encoder layer instead of the selective\nlayer is the most reasonable baseline (given the GRU baseline uses only one\nbi-GRU, this adds expressivity), and this seems to be implemented Luong-NMT. My\none concern is LSTM/GRU mismatch. Is the benefit coming from just GRU switch? \nThe quality of the writing, especially in the intro/abstract/related work is\nquite bad. This paper does not make a large departure from previous work, and\ntherefore a related work nearby the introduction seems more appropriate. In\nrelated work, one common good approach is highlighting similarities and\ndifferences between your work and previous work, in words before they are\npresented in equations. Simply listing works without relating them to your work\nis not that useful. Placement of the related work near the intro will allow you\nto relieve the intro of significant background detail and instead focus on more\nhigh level."
        }
    ]
}