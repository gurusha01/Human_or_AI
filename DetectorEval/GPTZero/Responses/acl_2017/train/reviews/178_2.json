{
    "version": "2025-01-09-base",
    "scanId": "a9d24093-0583-4efb-88ab-2f3614ac7645",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.004785711877048016,
                    "sentence": "Summary: This paper presents a model for embedding words, phrases and concepts",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009340496733784676,
                    "sentence": "into vector spaces.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0036230599507689476,
                    "sentence": "To do so, it uses an ontology of concepts, each of which is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0058107576332986355,
                    "sentence": "mapped to phrases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005224958527833223,
                    "sentence": "These phrases are found in text corpora and treated as",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005486810579895973,
                    "sentence": "atomic symbols.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005094700492918491,
                    "sentence": "Using this, the paper uses what is essentially the skip-gram",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004858330823481083,
                    "sentence": "method to train embeddings for words, the now atomic phrases and also the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003983831033110619,
                    "sentence": "concepts associated with them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004043211229145527,
                    "sentence": "The proposed work is evaluated on the task of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00522608682513237,
                    "sentence": "concept similarity and relatedness using UMLS and Yago to act as the backing",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008405627682805061,
                    "sentence": "ontologies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006327561102807522,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0028061538469046354,
                    "sentence": "The key question addressed by the paper is that phrases that are not lexically",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003940866328775883,
                    "sentence": "similar can be semantically close and, furthermore, not all phrases are",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0055778976529836655,
                    "sentence": "compositional in nature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0035384877119213343,
                    "sentence": "To this end, the paper proposes a plausible model to",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004406931344419718,
                    "sentence": "train phrase embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00614809338003397,
                    "sentence": "The trained embeddings are shown to be competitive or",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005722430534660816,
                    "sentence": "better at identifying similarity between concepts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006391837261617184,
                    "sentence": "The software released with the paper could be useful for biomedical NLP",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007287384010851383,
                    "sentence": "researchers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014230877161026001,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006110866088420153,
                    "sentence": "The primary weakness of the paper is that the model is not too novel.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013763923197984695,
                    "sentence": "It is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013613933697342873,
                    "sentence": "essentially a tweak to skip-gram.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008471280336380005,
                    "sentence": "Furthermore, the full model presented by the paper doesn't seem to be the best",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004333315882831812,
                    "sentence": "one in the results (in Table 4).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00874926894903183,
                    "sentence": "On the two Mayo datasets, the Choi baseline is",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011821385473012924,
                    "sentence": "substantially better.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009816773235797882,
                    "sentence": "A similar trend seems to dominate Table 6 too.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02045907825231552,
                    "sentence": "On the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009518688544631004,
                    "sentence": "larger UMNSRS data, the proposed model is at best competitive with previous",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003701204899698496,
                    "sentence": "simpler models (Chiu).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014701436273753643,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011275751516222954,
                    "sentence": "The paper says that it is uses known phrases as distant supervision to train",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008153710514307022,
                    "sentence": "embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03445916250348091,
                    "sentence": "However, it is not clear what the \"supervision\" here is.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03224923089146614,
                    "sentence": "If I",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0250552948564291,
                    "sentence": "understand the paper correctly, every occurrence of a phrase associated with a",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04085038974881172,
                    "sentence": "concept provides the context to train word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021897433325648308,
                    "sentence": "But this is not",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.029085755348205566,
                    "sentence": "supervision in the traditional sense (say for identifying the concept in the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05160022899508476,
                    "sentence": "text or other such predictive tasks).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.041413478553295135,
                    "sentence": "So the terminology is a bit confusing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08852767199277878,
                    "sentence": "The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08314940333366394,
                    "sentence": "the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06678768247365952,
                    "sentence": "The use of \\beta to control for compositionality of phrases by words is quite",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06384032964706421,
                    "sentence": "surprising.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08226674050092697,
                    "sentence": "Essentially, this is equivalent to saying that there is a single",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08064354211091995,
                    "sentence": "global constant that decides \"how compositional\" any phrase should be.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08939856290817261,
                    "sentence": "The",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12201745063066483,
                    "sentence": "surprising part here is that the actual values of \\beta chosen by cross",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06156289950013161,
                    "sentence": "validation from Table 3 are odd.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12386497110128403,
                    "sentence": "For PM+CL and WikiNYT, it is zero, which",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09725706279277802,
                    "sentence": "basically argues against compositionality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11192060261964798,
                    "sentence": "The experimental setup for table 4 needs some explanation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18207255005836487,
                    "sentence": "The paper says that",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10687507688999176,
                    "sentence": "the data labels similarity/relatedness of concepts (or entities).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.044824060052633286,
                    "sentence": "However, if",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07874027639627457,
                    "sentence": "the concepts-phrases mapping is really many-to-many, then how are the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07078559696674347,
                    "sentence": "phrase/word vectors used to compute the similarities?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04588472470641136,
                    "sentence": "It seems that we can only",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08463452011346817,
                    "sentence": "use the concept vectors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07687146216630936,
                    "sentence": "In table 5, the approximate phr method (which approximate concepts with the",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07393432408571243,
                    "sentence": "average of the phrases in them) is best performing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1376894861459732,
                    "sentence": "So it is not clear why we",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09664930403232574,
                    "sentence": "need the concept ontology.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1356145292520523,
                    "sentence": "Instead, we could have just started with a seed set",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11863939464092255,
                    "sentence": "of phrases to get the same results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 36,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 39,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 40,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 42,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 43,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 45,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 46,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 47,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 48,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 50,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 52,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 53,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 55,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 56,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 58,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 60,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 61,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 63,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 64,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 65,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 67,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 69,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.03751552122182913,
            "class_probabilities": {
                "human": 0.9622006644706045,
                "ai": 0.03751552122182913,
                "mixed": 0.00028381430756647936
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9622006644706045,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.03751552122182913,
                    "human": 0.9622006644706045,
                    "mixed": 0.00028381430756647936
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary: This paper presents a model for embedding words, phrases and concepts\ninto vector spaces. To do so, it uses an ontology of concepts, each of which is\nmapped to phrases. These phrases are found in text corpora and treated as\natomic symbols. Using this, the paper uses what is essentially the skip-gram\nmethod to train embeddings for words, the now atomic phrases and also the\nconcepts associated with them. The proposed work is evaluated on the task of\nconcept similarity and relatedness using UMLS and Yago to act as the backing\nontologies.\nStrengths:\nThe key question addressed by the paper is that phrases that are not lexically\nsimilar can be semantically close and, furthermore, not all phrases are\ncompositional in nature. To this end, the paper proposes a plausible model to\ntrain phrase embeddings. The trained embeddings are shown to be competitive or\nbetter at identifying similarity between concepts.\nThe software released with the paper could be useful for biomedical NLP\nresearchers.\n- Weaknesses:\nThe primary weakness of the paper is that the model is not too novel. It is\nessentially a tweak to skip-gram. \nFurthermore, the full model presented by the paper doesn't seem to be the best\none in the results (in Table 4). On the two Mayo datasets, the Choi baseline is\nsubstantially better. A similar trend seems to dominate Table 6 too. On the\nlarger UMNSRS data, the proposed model is at best competitive with previous\nsimpler models (Chiu).\n- General Discussion:\nThe paper says that it is uses known phrases as distant supervision to train\nembeddings. However, it is not clear what the \"supervision\" here is. If I\nunderstand the paper correctly, every occurrence of a phrase associated with a\nconcept provides the context to train word embeddings. But this is not\nsupervision in the traditional sense (say for identifying the concept in the\ntext or other such predictive tasks). So the terminology is a bit confusing.\n The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of\nthe paper.\nThe use of \\beta to control for compositionality of phrases by words is quite\nsurprising. Essentially, this is equivalent to saying that there is a single\nglobal constant that decides \"how compositional\" any phrase should be. The\nsurprising part here is that the actual values of \\beta chosen by cross\nvalidation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which\nbasically argues against compositionality. \nThe experimental setup for table 4 needs some explanation. The paper says that\nthe data labels similarity/relatedness of concepts (or entities). However, if\nthe concepts-phrases mapping is really many-to-many, then how are the\nphrase/word vectors used to compute the similarities? It seems that we can only\nuse the concept vectors.\nIn table 5, the approximate phr method (which approximate concepts with the\naverage of the phrases in them) is best performing. So it is not clear why we\nneed the concept ontology. Instead, we could have just started with a seed set\nof phrases to get the same results."
        }
    ]
}