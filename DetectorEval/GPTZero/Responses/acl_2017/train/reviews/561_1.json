{
    "version": "2025-01-09-base",
    "scanId": "746d320b-4009-4758-b9df-33a45149e202",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.17940445244312286,
                    "sentence": "The paper introduces a general method for improving NLP tasks using embeddings",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17134273052215576,
                    "sentence": "from language models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11798335611820221,
                    "sentence": "Context independent word representations have been very",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09282686561346054,
                    "sentence": "useful, and this paper proposes a nice extension by using context-dependent",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13029301166534424,
                    "sentence": "word representations obtained from the hidden states of neural language models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08150394260883331,
                    "sentence": "They show significant improvements in tagging and chunking tasks from including",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1438160240650177,
                    "sentence": "embeddings from large language models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05641740933060646,
                    "sentence": "There is also interesting analysis which",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05473116412758827,
                    "sentence": "answers several natural questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04147075116634369,
                    "sentence": "Overall this is a very good paper, but I have several suggestions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09296274930238724,
                    "sentence": "- Too many experiments are carried out on the test set.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07279618829488754,
                    "sentence": "Please change Tables 5",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04381602630019188,
                    "sentence": "and 6 to use development data",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04232483729720116,
                    "sentence": "- It would be really nice to see results on some more tasks - NER tagging and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.038835834711790085,
                    "sentence": "chunking don't have many interesting long range dependencies, and the language",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.030376167967915535,
                    "sentence": "model might really help in those cases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.046092793345451355,
                    "sentence": "I'd love to see results on SRL or CCG",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02756621316075325,
                    "sentence": "supertagging.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04612210392951965,
                    "sentence": "- The paper claims that using a task specific RNN is necessary because a CRF on",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03574194014072418,
                    "sentence": "top of language model embeddings performs poorly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.027076182886958122,
                    "sentence": "It wasn't clear to me if they",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.033694468438625336,
                    "sentence": "were backpropagating into the language model in this experiment - but if not,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03555656224489212,
                    "sentence": "it certainly seems like there is potential for that to make a task specific RNN",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08494167774915695,
                    "sentence": "unnecessary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.03751552122182913,
            "class_probabilities": {
                "human": 0.9622006644706045,
                "ai": 0.03751552122182913,
                "mixed": 0.00028381430756647936
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9622006644706045,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.03751552122182913,
                    "human": 0.9622006644706045,
                    "mixed": 0.00028381430756647936
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper introduces a general method for improving NLP tasks using embeddings\nfrom language models. Context independent word representations have been very\nuseful, and this paper proposes a nice extension by using context-dependent\nword representations obtained from the hidden states of neural language models.\nThey show significant improvements in tagging and chunking tasks from including\nembeddings from large language models. There is also interesting analysis which\nanswers several natural questions.\nOverall this is a very good paper, but I have several suggestions:\n- Too many experiments are carried out on the test set. Please change Tables 5\nand 6 to use development data\n- It would be really nice to see results on some more tasks - NER tagging and\nchunking don't have many interesting long range dependencies, and the language\nmodel might really help in those cases. I'd love to see results on SRL or CCG\nsupertagging.\n- The paper claims that using a task specific RNN is necessary because a CRF on\ntop of language model embeddings performs poorly. It wasn't clear to me if they\nwere backpropagating into the language model in this experiment - but if not,\nit certainly seems like there is potential for that to make a task specific RNN\nunnecessary."
        }
    ]
}