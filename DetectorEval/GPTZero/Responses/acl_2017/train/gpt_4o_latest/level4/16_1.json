{
    "version": "2025-01-09-base",
    "scanId": "0ae10384-30c1-4029-9e0a-493d9a06107d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9992262125015259,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991379380226135,
                    "sentence": "This paper seeks to leverage information from arguments, which is often overlooked but holds significant importance, to enhance event detection performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9973087906837463,
                    "sentence": "The proposed framework is straightforward and well-structured.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984359741210938,
                    "sentence": "By incorporating the supervised attention mechanism\"\"a widely used and impactful method in various tasks like machine translation\"\"the system achieves a notable improvement over the baseline.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992560744285583,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9963605999946594,
                    "sentence": "The attention vector is constructed by simply summing the two attention vectors from each part.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9960060119628906,
                    "sentence": "A more refined approach to computing the attention vector might yield better results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9969903230667114,
                    "sentence": "Regarding the supervised attention mechanism, the authors propose two strategies, both of which are relatively simple.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990636706352234,
                    "sentence": "Exploring more sophisticated strategies could potentially lead to improved performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992583394050598,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975920915603638,
                    "sentence": "While there are areas that could benefit from further refinement, this paper presents an effective framework with strong performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9931444525718689,
                    "sentence": "The experimental results are robust, and the work is deserving of acceptance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "- Strengths: \nThis paper seeks to leverage information from arguments, which is often overlooked but holds significant importance, to enhance event detection performance. The proposed framework is straightforward and well-structured. By incorporating the supervised attention mechanism\"\"a widely used and impactful method in various tasks like machine translation\"\"the system achieves a notable improvement over the baseline.\n- Weaknesses: \nThe attention vector is constructed by simply summing the two attention vectors from each part. A more refined approach to computing the attention vector might yield better results. Regarding the supervised attention mechanism, the authors propose two strategies, both of which are relatively simple. Exploring more sophisticated strategies could potentially lead to improved performance.\n- General Discussion: \nWhile there are areas that could benefit from further refinement, this paper presents an effective framework with strong performance. The experimental results are robust, and the work is deserving of acceptance."
        }
    ]
}