{
    "version": "2025-01-09-base",
    "scanId": "d30f1bce-5e25-47f4-a277-434c2cc70865",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999821126461029,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995690584182739,
                    "sentence": "The evaluation of bag-of-words and \"bound\" contexts derived from either dependencies or sentence ordering is a significant contribution and will serve as a valuable reference for the research community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994609951972961,
                    "sentence": "The experiments conducted were relatively comprehensive (though certain choices warrant additional justification), and the authors opted for downstream task evaluations rather than solely relying on intrinsic evaluations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998265504837036,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992000460624695,
                    "sentence": "The authors modify the GBOW objective function from \\( p(c\"\\sum wi) \\) to \\( p(w\"\\sum ci) \\).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993972778320312,
                    "sentence": "While this adjustment is somewhat justified by the fact that dependency-based contexts with bound representations only have a single word available for predicting the context, the reasoning behind this is not entirely clear and requires further elaboration.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988623857498169,
                    "sentence": "For instance, would non-dependency contexts with bound representations also face this limitation?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988151788711548,
                    "sentence": "If so, how was this addressed in Ling et al., 2015?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988191723823547,
                    "sentence": "Unfortunately, the paper does not include comparisons against results obtained using the original objective function, which is a notable shortcoming.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989527463912964,
                    "sentence": "Similarly, the authors modify GSG to align with GBOW but again fail to compare it to the original objective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992777109146118,
                    "sentence": "Including results from word vectors trained using the original GBOW and GSG objective functions would help validate these changes, assuming the results do not show significant deviations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979742765426636,
                    "sentence": "The discussion of hyperparameter settings is also insufficient.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985247850418091,
                    "sentence": "Hyperparameters played a critical role in Levy et al., 2015, and exploring different values would strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999343991279602,
                    "sentence": "These settings are often highly task-dependent, so simply adopting values from another task may not yield optimal results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993882179260254,
                    "sentence": "Additionally, the description of the model trained in Section 3.4 is vague, with the authors referring to it only as a \"simple linear classifier.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996816515922546,
                    "sentence": "In Section 3.5, they employ logistic regression with the average of word vectors as input but label it as a Neural Bag-of-Words model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996683597564697,
                    "sentence": "While this terminology has been used in prior work, it is misleading here, as logistic regression is a linear model and not something typically described as \"Neural.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996398687362671,
                    "sentence": "It is crucial to clarify whether the model in Section 3.4 is the same as the one in Section 3.5, as this distinction is necessary to determine whether the differing conclusions arise from changes in the task or the model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999800443649292,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996136426925659,
                    "sentence": "This paper compares contexts derived from dependency parses with those based on word position within a sentence, as well as bag-of-words representations with tokens that include relative position indicators.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9914155602455139,
                    "sentence": "The findings are valuable to the community, as they provide guidance on when and where to use word vectors trained with these different design choices.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9960057735443115,
                    "sentence": "- Emphasis to Improve:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9920336008071899,
                    "sentence": "The primary insights from this paper, which future researchers are likely to reference, are presented at the end of Sections 3.4 and 3.5 but should be summarized earlier in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9924265146255493,
                    "sentence": "Specifically, the abstract should highlight that for tasks like POS tagging, chunking, and NER, bound representations outperform bag-of-words representations, and dependency contexts generally perform better than linear contexts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9957953691482544,
                    "sentence": "Conversely, for simple text classification tasks, bound representations underperform compared to bag-of-words representations, with no significant differences observed between the models or context types.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9968491792678833,
                    "sentence": "- Small Points of Improvement:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9976972341537476,
                    "sentence": "1. The term \"unbounded\" context should be replaced with \"bag-of-words\" to avoid confusion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9969650506973267,
                    "sentence": "While one of the techniques used is Generalized Bag-of-Words, this distinction can be clarified easily.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972190856933594,
                    "sentence": "2. Line 043: The correct term is \"distributional hypothesis,\" not \"Distributed Hypothesis.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982125759124756,
                    "sentence": "3. Line 069: Citations should use a comma instead of a semicolon for separation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977033138275146,
                    "sentence": "4. Line 074: \"DEPS\" should be consistently capitalized throughout the paper (it often appears as \"Deps\").",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9970748424530029,
                    "sentence": "It should also be introduced as \"dependency parse tree context (Deps)\" or a similar phrase for clarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9966849088668823,
                    "sentence": "5. Line 085: Typo\"\"\"How different contexts affect model's performances...\" should include the word \"do\" (e.g., \"How do different contexts affect model's performances...\").",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "- Strengths: \nThe evaluation of bag-of-words and \"bound\" contexts derived from either dependencies or sentence ordering is a significant contribution and will serve as a valuable reference for the research community. The experiments conducted were relatively comprehensive (though certain choices warrant additional justification), and the authors opted for downstream task evaluations rather than solely relying on intrinsic evaluations.\n- Weaknesses: \nThe authors modify the GBOW objective function from \\( p(c\"\\sum wi) \\) to \\( p(w\"\\sum ci) \\). While this adjustment is somewhat justified by the fact that dependency-based contexts with bound representations only have a single word available for predicting the context, the reasoning behind this is not entirely clear and requires further elaboration. For instance, would non-dependency contexts with bound representations also face this limitation? If so, how was this addressed in Ling et al., 2015? Unfortunately, the paper does not include comparisons against results obtained using the original objective function, which is a notable shortcoming. Similarly, the authors modify GSG to align with GBOW but again fail to compare it to the original objective. Including results from word vectors trained using the original GBOW and GSG objective functions would help validate these changes, assuming the results do not show significant deviations. \nThe discussion of hyperparameter settings is also insufficient. Hyperparameters played a critical role in Levy et al., 2015, and exploring different values would strengthen the paper. These settings are often highly task-dependent, so simply adopting values from another task may not yield optimal results. \nAdditionally, the description of the model trained in Section 3.4 is vague, with the authors referring to it only as a \"simple linear classifier.\" In Section 3.5, they employ logistic regression with the average of word vectors as input but label it as a Neural Bag-of-Words model. While this terminology has been used in prior work, it is misleading here, as logistic regression is a linear model and not something typically described as \"Neural.\" It is crucial to clarify whether the model in Section 3.4 is the same as the one in Section 3.5, as this distinction is necessary to determine whether the differing conclusions arise from changes in the task or the model.\n- General Discussion: \nThis paper compares contexts derived from dependency parses with those based on word position within a sentence, as well as bag-of-words representations with tokens that include relative position indicators. The findings are valuable to the community, as they provide guidance on when and where to use word vectors trained with these different design choices.\n- Emphasis to Improve: \nThe primary insights from this paper, which future researchers are likely to reference, are presented at the end of Sections 3.4 and 3.5 but should be summarized earlier in the paper. Specifically, the abstract should highlight that for tasks like POS tagging, chunking, and NER, bound representations outperform bag-of-words representations, and dependency contexts generally perform better than linear contexts. Conversely, for simple text classification tasks, bound representations underperform compared to bag-of-words representations, with no significant differences observed between the models or context types.\n- Small Points of Improvement: \n1. The term \"unbounded\" context should be replaced with \"bag-of-words\" to avoid confusion. While one of the techniques used is Generalized Bag-of-Words, this distinction can be clarified easily. \n2. Line 043: The correct term is \"distributional hypothesis,\" not \"Distributed Hypothesis.\" \n3. Line 069: Citations should use a comma instead of a semicolon for separation. \n4. Line 074: \"DEPS\" should be consistently capitalized throughout the paper (it often appears as \"Deps\"). It should also be introduced as \"dependency parse tree context (Deps)\" or a similar phrase for clarity. \n5. Line 085: Typo\"\"\"How different contexts affect model's performances...\" should include the word \"do\" (e.g., \"How do different contexts affect model's performances...\")."
        }
    ]
}