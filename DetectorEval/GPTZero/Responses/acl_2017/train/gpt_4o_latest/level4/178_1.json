{
    "version": "2025-01-09-base",
    "scanId": "24590a5d-e084-490b-aeb6-5c41684d3c63",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9892453551292419,
                    "sentence": "The paper presents an extension of word embedding methods to generate representations not only for words but also for phrases and concepts associated with those words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9905137419700623,
                    "sentence": "The proposed approach involves assigning a unique identifier to groups of phrases, words, and the overarching concept they represent.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.984000563621521,
                    "sentence": "These identifiers are used to replace occurrences of the corresponding phrases and words in the training corpus, creating a \"tagged\" corpus.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9610241651535034,
                    "sentence": "The tagged corpus is then combined with the original corpus for training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9842198491096497,
                    "sentence": "The sets of concepts, phrases, and words are derived from an ontology.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9779446125030518,
                    "sentence": "Given the biomedical focus of the study, relevant corpora and ontologies from this domain are utilized.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9797819256782532,
                    "sentence": "Additionally, the authors introduce a novel test dataset for evaluating word similarity and relatedness in the context of real-world entities, which is an innovative aspect of the work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9794600009918213,
                    "sentence": "Overall, the paper is well-written.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9828140139579773,
                    "sentence": "The proposed technique is intuitive, though its contribution is not particularly groundbreaking.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9854108095169067,
                    "sentence": "The scope of the work is somewhat narrow due to the evaluation being confined to the biomedical domain.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9800282120704651,
                    "sentence": "A more detailed discussion of the newly generated test resource would enhance the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9915903806686401,
                    "sentence": "This resource might represent the most significant and interesting contribution of the study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9968677163124084,
                    "sentence": "There is a minor technical issue, likely related to the mathematical formulation rather than the implementation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989702701568604,
                    "sentence": "Technical issue:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979599714279175,
                    "sentence": "Equation 8: The authors aim to define the MAP calculation, which is a worthwhile endeavor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975470900535583,
                    "sentence": "However, it might be more appropriate to define a natural cut-off rather than ranking the entire vocabulary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986422657966614,
                    "sentence": "Additionally, Equation 8 does not represent a probability, and this can be demonstrated even when assuming an infinite vocabulary size.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9973216652870178,
                    "sentence": "The explanation should be revised to remove references to probability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982824325561523,
                    "sentence": "Minor corrections:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9969253540039062,
                    "sentence": "Line 556: \"most concepts has\" â ' \"most concepts have\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper presents an extension of word embedding methods to generate representations not only for words but also for phrases and concepts associated with those words. The proposed approach involves assigning a unique identifier to groups of phrases, words, and the overarching concept they represent. These identifiers are used to replace occurrences of the corresponding phrases and words in the training corpus, creating a \"tagged\" corpus. The tagged corpus is then combined with the original corpus for training. The sets of concepts, phrases, and words are derived from an ontology. Given the biomedical focus of the study, relevant corpora and ontologies from this domain are utilized. Additionally, the authors introduce a novel test dataset for evaluating word similarity and relatedness in the context of real-world entities, which is an innovative aspect of the work.\nOverall, the paper is well-written. The proposed technique is intuitive, though its contribution is not particularly groundbreaking. The scope of the work is somewhat narrow due to the evaluation being confined to the biomedical domain.\nA more detailed discussion of the newly generated test resource would enhance the paper. This resource might represent the most significant and interesting contribution of the study.\nThere is a minor technical issue, likely related to the mathematical formulation rather than the implementation.\nTechnical issue: \nEquation 8: The authors aim to define the MAP calculation, which is a worthwhile endeavor. However, it might be more appropriate to define a natural cut-off rather than ranking the entire vocabulary. Additionally, Equation 8 does not represent a probability, and this can be demonstrated even when assuming an infinite vocabulary size. The explanation should be revised to remove references to probability.\nMinor corrections: \nLine 556: \"most concepts has\" â ' \"most concepts have\""
        }
    ]
}