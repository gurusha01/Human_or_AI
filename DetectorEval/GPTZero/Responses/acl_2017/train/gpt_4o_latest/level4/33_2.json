{
    "version": "2025-01-09-base",
    "scanId": "73850d0d-001d-4ecf-a188-9e9923ce5bf8",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9996668696403503,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991549253463745,
                    "sentence": "This paper introduces an effective approach to integrating a neural model (LSTM) with linguistic knowledge, including a sentiment lexicon, negation, and intensity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991807341575623,
                    "sentence": "The proposed method is both straightforward and impactful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988797307014465,
                    "sentence": "It achieves state-of-the-art results on the Movie Review dataset and performs competitively with leading models on the SST dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999790608882904,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992817044258118,
                    "sentence": "A similar concept has been explored in (Teng et al., 2016).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993712306022644,
                    "sentence": "While this work demonstrates a more refined framework design and mathematical representation, the experimental comparison with (Teng et al., 2016) is less compelling than the comparisons with other methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996476173400879,
                    "sentence": "Specifically, the authors only provide re-implementation results for sentence-level experiments on SST and omit their own phrase-level results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996808171272278,
                    "sentence": "Certain details lack sufficient explanation, as discussed below.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998167753219604,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996388554573059,
                    "sentence": "The reviewer raises the following questions and suggestions regarding this work:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992388486862183,
                    "sentence": "1. Given that the SST dataset includes phrase-level annotations, it would be beneficial to present statistics on how often negation or intensity words influence sentiment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9976000785827637,
                    "sentence": "For instance, how frequently does the word \"nothing\" appear, and in how many cases does it alter the contextual polarity?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977025985717773,
                    "sentence": "2. In Section 4.5, the bi-LSTM is mentioned as being used for the regularizers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9978405237197876,
                    "sentence": "Is the bi-LSTM also employed to predict sentiment labels?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997715950012207,
                    "sentence": "3. The authors state that \"we only use the sentence-level annotation since one of our goals is to avoid expensive phrase-level annotation.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9960148334503174,
                    "sentence": "Nonetheless, the reviewer recommends including phrase-level results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9959151148796082,
                    "sentence": "If feasible, please provide these results during the rebuttal phase.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987255334854126,
                    "sentence": "4. The paper mentions that \"sc is a parameter to be optimized but could also be set fixed with prior knowledge.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9981628060340881,
                    "sentence": "However, the reviewer could not locate a clear definition of sc in the experimental section.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9970405697822571,
                    "sentence": "Is this parameter learned or fixed?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9953996539115906,
                    "sentence": "If fixed, what value is used?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.996623158454895,
                    "sentence": "If learned, what is the resulting value?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9946405291557312,
                    "sentence": "5. In Sections 5.4 and 5.5, the reviewer suggests conducting an additional experiment using a subset of the SST dataset that includes only phrases containing negation or intensity words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977297186851501,
                    "sentence": "Reporting the results on this subset, both with and without the corresponding regularizer, would strengthen the case for the proposed method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "- Strengths: \nThis paper introduces an effective approach to integrating a neural model (LSTM) with linguistic knowledge, including a sentiment lexicon, negation, and intensity. The proposed method is both straightforward and impactful. It achieves state-of-the-art results on the Movie Review dataset and performs competitively with leading models on the SST dataset.\n- Weaknesses: \nA similar concept has been explored in (Teng et al., 2016). While this work demonstrates a more refined framework design and mathematical representation, the experimental comparison with (Teng et al., 2016) is less compelling than the comparisons with other methods. Specifically, the authors only provide re-implementation results for sentence-level experiments on SST and omit their own phrase-level results.\nCertain details lack sufficient explanation, as discussed below.\n- General Discussion: \nThe reviewer raises the following questions and suggestions regarding this work:\n1. Given that the SST dataset includes phrase-level annotations, it would be beneficial to present statistics on how often negation or intensity words influence sentiment. For instance, how frequently does the word \"nothing\" appear, and in how many cases does it alter the contextual polarity?\n2. In Section 4.5, the bi-LSTM is mentioned as being used for the regularizers. Is the bi-LSTM also employed to predict sentiment labels?\n3. The authors state that \"we only use the sentence-level annotation since one of our goals is to avoid expensive phrase-level annotation.\" Nonetheless, the reviewer recommends including phrase-level results. If feasible, please provide these results during the rebuttal phase.\n4. The paper mentions that \"sc is a parameter to be optimized but could also be set fixed with prior knowledge.\" However, the reviewer could not locate a clear definition of sc in the experimental section. Is this parameter learned or fixed? If fixed, what value is used? If learned, what is the resulting value?\n5. In Sections 5.4 and 5.5, the reviewer suggests conducting an additional experiment using a subset of the SST dataset that includes only phrases containing negation or intensity words. Reporting the results on this subset, both with and without the corresponding regularizer, would strengthen the case for the proposed method."
        }
    ]
}