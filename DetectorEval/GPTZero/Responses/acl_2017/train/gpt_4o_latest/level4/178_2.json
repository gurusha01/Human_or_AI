{
    "version": "2025-01-09-base",
    "scanId": "aa2d006e-588e-47b1-ab6c-9bb46839a406",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999421238899231,
                    "sentence": "Review - Summary:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979137778282166,
                    "sentence": "This paper introduces a model for embedding words, phrases, and concepts into vector spaces.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986293911933899,
                    "sentence": "The approach leverages an ontology of concepts, each linked to specific phrases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9968442916870117,
                    "sentence": "These phrases, extracted from text corpora, are treated as atomic units.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983384609222412,
                    "sentence": "The model employs a variation of the skip-gram method to train embeddings for words, atomic phrases, and their associated concepts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986853003501892,
                    "sentence": "The proposed approach is evaluated on concept similarity and relatedness tasks, using UMLS and Yago as the underlying ontologies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998050928115845,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994491338729858,
                    "sentence": "The paper addresses an important problem: semantically similar phrases may not always be lexically similar, and not all phrases are compositional.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995498657226562,
                    "sentence": "To tackle this, the paper proposes a reasonable model for training phrase embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997467994689941,
                    "sentence": "The resulting embeddings demonstrate competitive or superior performance in identifying concept similarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995685815811157,
                    "sentence": "Additionally, the software accompanying the paper has potential utility for researchers in biomedical NLP.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998038411140442,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996746182441711,
                    "sentence": "The main limitation of the paper lies in the lack of novelty.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996720552444458,
                    "sentence": "The model is essentially a modified version of the skip-gram method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993131160736084,
                    "sentence": "Moreover, the full model proposed in the paper does not consistently outperform baselines in the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999678373336792,
                    "sentence": "For instance, in Table 4, the Choi baseline performs significantly better on the two Mayo datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997713565826416,
                    "sentence": "A similar trend is observed in Table 6, where the proposed model is, at best, competitive with simpler prior models (e.g., Chiu) on the larger UMNSRS dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999528527259827,
                    "sentence": "General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999064207077026,
                    "sentence": "The paper claims to use known phrases as distant supervision for training embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982883334159851,
                    "sentence": "However, the term \"supervision\" is somewhat misleading.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974799752235413,
                    "sentence": "If the interpretation is correct, each occurrence of a phrase linked to a concept serves as context for training word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980665445327759,
                    "sentence": "This does not align with traditional notions of supervision, such as predicting concepts in text, making the terminology unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9954533576965332,
                    "sentence": "The notation introduced in Section 3.2 (e.g., \\(E_W\\)) is not utilized elsewhere in the paper, which reduces its utility.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.995722770690918,
                    "sentence": "The use of \\(\\beta\\) to control the compositionality of phrases is unexpected.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975571036338806,
                    "sentence": "This implies a single global constant determines the compositionality of all phrases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9976664185523987,
                    "sentence": "Surprisingly, the cross-validated values of \\(\\beta\\) in Table 3 are unusual.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9978270530700684,
                    "sentence": "For PM+CL and WikiNYT, \\(\\beta\\) is zero, suggesting no compositionality, which contradicts the paper's premise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9971339702606201,
                    "sentence": "The experimental setup for Table 4 requires clarification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977957606315613,
                    "sentence": "The paper states that the data labels concept similarity/relatedness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974451065063477,
                    "sentence": "However, if the mapping between concepts and phrases is many-to-many, it is unclear how phrase or word vectors are used to compute similarities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9948868751525879,
                    "sentence": "It seems only concept vectors would be applicable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977735280990601,
                    "sentence": "In Table 5, the approximate phr method (which approximates concepts using the average of their associated phrase vectors) achieves the best performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9951795935630798,
                    "sentence": "This raises the question of whether the concept ontology is necessary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9950078129768372,
                    "sentence": "Starting with a seed set of phrases might yield similar results without requiring the ontology.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9984930238596827,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984930238596827,
                "mixed": 0.001506976140317253
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984930238596827,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984930238596827,
                    "human": 0,
                    "mixed": 0.001506976140317253
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review - Summary: \nThis paper introduces a model for embedding words, phrases, and concepts into vector spaces. The approach leverages an ontology of concepts, each linked to specific phrases. These phrases, extracted from text corpora, are treated as atomic units. The model employs a variation of the skip-gram method to train embeddings for words, atomic phrases, and their associated concepts. The proposed approach is evaluated on concept similarity and relatedness tasks, using UMLS and Yago as the underlying ontologies.\nStrengths: \nThe paper addresses an important problem: semantically similar phrases may not always be lexically similar, and not all phrases are compositional. To tackle this, the paper proposes a reasonable model for training phrase embeddings. The resulting embeddings demonstrate competitive or superior performance in identifying concept similarity. \nAdditionally, the software accompanying the paper has potential utility for researchers in biomedical NLP.\nWeaknesses: \nThe main limitation of the paper lies in the lack of novelty. The model is essentially a modified version of the skip-gram method. \nMoreover, the full model proposed in the paper does not consistently outperform baselines in the results. For instance, in Table 4, the Choi baseline performs significantly better on the two Mayo datasets. A similar trend is observed in Table 6, where the proposed model is, at best, competitive with simpler prior models (e.g., Chiu) on the larger UMNSRS dataset.\nGeneral Discussion: \nThe paper claims to use known phrases as distant supervision for training embeddings. However, the term \"supervision\" is somewhat misleading. If the interpretation is correct, each occurrence of a phrase linked to a concept serves as context for training word embeddings. This does not align with traditional notions of supervision, such as predicting concepts in text, making the terminology unclear. \nThe notation introduced in Section 3.2 (e.g., \\(E_W\\)) is not utilized elsewhere in the paper, which reduces its utility. \nThe use of \\(\\beta\\) to control the compositionality of phrases is unexpected. This implies a single global constant determines the compositionality of all phrases. Surprisingly, the cross-validated values of \\(\\beta\\) in Table 3 are unusual. For PM+CL and WikiNYT, \\(\\beta\\) is zero, suggesting no compositionality, which contradicts the paper's premise. \nThe experimental setup for Table 4 requires clarification. The paper states that the data labels concept similarity/relatedness. However, if the mapping between concepts and phrases is many-to-many, it is unclear how phrase or word vectors are used to compute similarities. It seems only concept vectors would be applicable. \nIn Table 5, the approximate phr method (which approximates concepts using the average of their associated phrase vectors) achieves the best performance. This raises the question of whether the concept ontology is necessary. Starting with a seed set of phrases might yield similar results without requiring the ontology."
        }
    ]
}