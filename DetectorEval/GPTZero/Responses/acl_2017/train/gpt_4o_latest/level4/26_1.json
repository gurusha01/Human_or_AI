{
    "version": "2025-01-09-base",
    "scanId": "2cd56bf8-39a1-4ada-b91a-9a33bed2ed95",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998574256896973,
                    "sentence": "Review-26: An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9997945427894592,
                    "sentence": "This paper proposes a neural model for factoid question answering over a knowledge graph (Freebase).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9997587203979492,
                    "sentence": "The model aims to establish a semantic correspondence between various \"aspects\" of candidate answers (e.g., answer type, relation to the question entity, answer semantics, etc.)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9997749328613281,
                    "sentence": "and specific subsets of the question's words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.999377965927124,
                    "sentence": "Each \"aspect\" of the candidate answers is modeled with a distinct correspondence component.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9993557333946228,
                    "sentence": "The paper's two primary contributions are: (1) the introduction of separate components to capture different aspects of candidate answers, as opposed to relying on a unified semantic representation, and (2) the integration of global context (from the KB) for candidate answers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9988479018211365,
                    "sentence": "The most compelling aspect of this work, in my view, is the decomposition of candidate answer representation into distinct aspects.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9991161823272705,
                    "sentence": "This approach provides neural model developers with greater control, enabling them to guide the neural network toward more relevant information for decision-making.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9988260269165039,
                    "sentence": "It evokes the spirit of traditional algorithms that depend on feature engineering, though here the \"feature engineering\" (i.e., aspects) is more nuanced and less burdensome.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9985183477401733,
                    "sentence": "I encourage the authors to continue refining this approach in future iterations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9995746612548828,
                    "sentence": "While the overarching idea is relatively clear to an informed reader, some details may be challenging for certain audiences to fully grasp.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9996207356452942,
                    "sentence": "Specific areas of the paper could benefit from additional clarification, including:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9995517730712891,
                    "sentence": "(1) The context aspect of candidate answers (e_c) is insufficiently explained.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9993823766708374,
                    "sentence": "The final two sentences of Section 3.2.2, in particular, are unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9996213316917419,
                    "sentence": "(2) The mention of OOV in the abstract and introduction requires further elaboration.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9997336864471436,
                    "sentence": "The current presentation seems to assume a deep familiarity with prior work, which may not be accessible to all readers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9997498989105225,
                    "sentence": "(3) The experimental comparisons focus on IR-based systems, which is a reasonable choice.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9993297457695007,
                    "sentence": "However, the inclusion of Yang et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.999533474445343,
                    "sentence": "(2014)\"\"described as SP-based\"\"raises questions about the consistency of the comparison criteria.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9994746446609497,
                    "sentence": "While I support including more systems in the evaluation, the rationale for what is and isn't compared should be clarified.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9993002414703369,
                    "sentence": "Additionally, it would be helpful to include performance numbers for the best SP-based systems for context.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9994919896125793,
                    "sentence": "The paper notes that embeddings are learned entirely from the training data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9995546936988831,
                    "sentence": "I am curious about the impact of random initialization on the final performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9992828369140625,
                    "sentence": "It would be informative to report any observed variance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17126181721687317,
                    "sentence": "Furthermore, it would be interesting to explore whether starting with pre-trained embeddings (e.g., from word2vec) instead of random initialization would influence the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.19079163670539856,
                    "sentence": "One potential direction for future work that came to mind while reading the paper is the inclusion of structured queries (from SP-based methods) as part of the cross-attention mechanism.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2475660890340805,
                    "sentence": "In addition to leveraging the various aspects of candidate answers as features, structured queries that generate the candidate answers could be incorporated as an additional aspect.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.30553627014160156,
                    "sentence": "An attention mechanism could then focus on different parts of the structured query and its semantic alignment with the input question, providing another signal for the neural model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3502264618873596,
                    "sentence": "This is merely a suggestion for exploration.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.30648642778396606,
                    "sentence": "Regarding the paper's positioning:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2521604895591736,
                    "sentence": "I am hesitant to label the proposed model as an \"attention\" model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.32868635654449463,
                    "sentence": "My understanding of attention mechanisms is that they typically apply in \"encoder-decoder\" frameworks, where information in one structured form (e.g., an image, a sentence in one language, a natural language question, etc.)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.279787540435791,
                    "sentence": "is encoded into an abstract representation and then decoded into another structured form (e.g., a caption, a sentence in another language, a structured query, etc.).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3456480801105499,
                    "sentence": "Attention mechanisms allow the encoder to selectively focus on different parts of the input as the decoder generates the output.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.23591473698616028,
                    "sentence": "This paper does not seem to align with that definition, and using the term \"attention\" may cause confusion for a broader audience.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.45215946435928345,
                    "sentence": "------",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.19877324998378754,
                    "sentence": "Thank you for the clarifications provided in the author response.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.5710657228372709
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 36,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.493689680772086,
            "class_probabilities": {
                "human": 0.5042687453600593,
                "ai": 0.493689680772086,
                "mixed": 0.002041573867854491
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5042687453600593,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.493689680772086,
                    "human": 0.5042687453600593,
                    "mixed": 0.002041573867854491
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review-26: An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge\nThis paper proposes a neural model for factoid question answering over a knowledge graph (Freebase). The model aims to establish a semantic correspondence between various \"aspects\" of candidate answers (e.g., answer type, relation to the question entity, answer semantics, etc.) and specific subsets of the question's words. Each \"aspect\" of the candidate answers is modeled with a distinct correspondence component. The paper's two primary contributions are: (1) the introduction of separate components to capture different aspects of candidate answers, as opposed to relying on a unified semantic representation, and (2) the integration of global context (from the KB) for candidate answers.\nThe most compelling aspect of this work, in my view, is the decomposition of candidate answer representation into distinct aspects. This approach provides neural model developers with greater control, enabling them to guide the neural network toward more relevant information for decision-making. It evokes the spirit of traditional algorithms that depend on feature engineering, though here the \"feature engineering\" (i.e., aspects) is more nuanced and less burdensome. I encourage the authors to continue refining this approach in future iterations.\nWhile the overarching idea is relatively clear to an informed reader, some details may be challenging for certain audiences to fully grasp. Specific areas of the paper could benefit from additional clarification, including:\n(1) The context aspect of candidate answers (e_c) is insufficiently explained. The final two sentences of Section 3.2.2, in particular, are unclear.\n(2) The mention of OOV in the abstract and introduction requires further elaboration. The current presentation seems to assume a deep familiarity with prior work, which may not be accessible to all readers.\n(3) The experimental comparisons focus on IR-based systems, which is a reasonable choice. However, the inclusion of Yang et al. (2014)\"\"described as SP-based\"\"raises questions about the consistency of the comparison criteria. While I support including more systems in the evaluation, the rationale for what is and isn't compared should be clarified. Additionally, it would be helpful to include performance numbers for the best SP-based systems for context.\nThe paper notes that embeddings are learned entirely from the training data. I am curious about the impact of random initialization on the final performance. It would be informative to report any observed variance. Furthermore, it would be interesting to explore whether starting with pre-trained embeddings (e.g., from word2vec) instead of random initialization would influence the results.\nOne potential direction for future work that came to mind while reading the paper is the inclusion of structured queries (from SP-based methods) as part of the cross-attention mechanism. In addition to leveraging the various aspects of candidate answers as features, structured queries that generate the candidate answers could be incorporated as an additional aspect. An attention mechanism could then focus on different parts of the structured query and its semantic alignment with the input question, providing another signal for the neural model. This is merely a suggestion for exploration.\nRegarding the paper's positioning:\nI am hesitant to label the proposed model as an \"attention\" model. My understanding of attention mechanisms is that they typically apply in \"encoder-decoder\" frameworks, where information in one structured form (e.g., an image, a sentence in one language, a natural language question, etc.) is encoded into an abstract representation and then decoded into another structured form (e.g., a caption, a sentence in another language, a structured query, etc.). Attention mechanisms allow the encoder to selectively focus on different parts of the input as the decoder generates the output. This paper does not seem to align with that definition, and using the term \"attention\" may cause confusion for a broader audience.\n------\nThank you for the clarifications provided in the author response."
        }
    ]
}