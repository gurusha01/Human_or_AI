{
    "version": "2025-01-09-base",
    "scanId": "2d9aa9ca-4272-46d7-9771-f87b803f0b07",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.020751696079969406,
                    "sentence": "Paraphrased Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03148999437689781,
                    "sentence": "- Contents:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05660255253314972,
                    "sentence": "This paper introduces a novel task and presents a corresponding dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03673578053712845,
                    "sentence": "The task involves predicting masked named entities within a text by leveraging an external definitional resource, specifically FreeBase.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02520032599568367,
                    "sentence": "These named entities are generally rare, meaning they appear infrequently in the corpus, making it impractical to train models tailored to each entity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.020727165043354034,
                    "sentence": "The paper makes a compelling case for why this is an important problem to investigate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.034146055579185486,
                    "sentence": "In addition to several baselines, the authors propose two neural network models that utilize the external resource, with one model also aggregating evidence from multiple contexts within the same text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03640270605683327,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05932953208684921,
                    "sentence": "The task's design is well thought out and has the potential to push the field forward: predicting masked named entities, a challenge already demonstrated to be interesting in the CNN/Daily Mail dataset, but framed here in a way that makes it particularly difficult for language models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05795983225107193,
                    "sentence": "Additionally, the emphasis on rare entities is likely to encourage the development of more sophisticated models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03272681683301926,
                    "sentence": "The selection of baselines is appropriate and effectively demonstrates that neither a neural network model without external knowledge nor a simple cosine similarity-based model leveraging external knowledge performs well on this task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023457564413547516,
                    "sentence": "The two primary models proposed in the paper are well-suited to the problem.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.037821799516677856,
                    "sentence": "The writing is clear, and the arguments are well-structured.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05470091849565506,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04721669852733612,
                    "sentence": "I found it somewhat surprising that expanding the context beyond the sentences containing the blanks did not improve model performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06633137166500092,
                    "sentence": "This is especially intriguing given that the HierEnc model, which aggregates information from other contexts, does utilize additional context in a different way.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07865791767835617,
                    "sentence": "There are two potential explanations for this: either the sentences containing the blanks are consistently more informative for the task than those without, as suggested in the paper, though this seems somewhat counterintuitive; or the method of incorporating additional context in the HierEnc model, via the temporal network, is significantly more effective than simply expanding the individual context \\(C\\) and feeding the larger \\(C\\) into the recurrent network.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12829849123954773,
                    "sentence": "Could this latter explanation be a contributing factor?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6798660755157471,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07938876003026962,
                    "sentence": "I am particularly impressed by the task and dataset proposed in this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06984435766935349,
                    "sentence": "I believe this setup has the potential to significantly advance the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1324559450149536,
                    "sentence": "In my view, this is the paper's primary contribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                }
            ],
            "completely_generated_prob": 0.06633669730667867,
            "class_probabilities": {
                "human": 0.9336633026933213,
                "ai": 0.06633669730667867,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9336633026933213,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.06633669730667867,
                    "human": 0.9336633026933213,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Paraphrased Review\n- Contents: \nThis paper introduces a novel task and presents a corresponding dataset. The task involves predicting masked named entities within a text by leveraging an external definitional resource, specifically FreeBase. These named entities are generally rare, meaning they appear infrequently in the corpus, making it impractical to train models tailored to each entity. The paper makes a compelling case for why this is an important problem to investigate. In addition to several baselines, the authors propose two neural network models that utilize the external resource, with one model also aggregating evidence from multiple contexts within the same text. \n- Strengths: \nThe task's design is well thought out and has the potential to push the field forward: predicting masked named entities, a challenge already demonstrated to be interesting in the CNN/Daily Mail dataset, but framed here in a way that makes it particularly difficult for language models. Additionally, the emphasis on rare entities is likely to encourage the development of more sophisticated models. \nThe selection of baselines is appropriate and effectively demonstrates that neither a neural network model without external knowledge nor a simple cosine similarity-based model leveraging external knowledge performs well on this task. \nThe two primary models proposed in the paper are well-suited to the problem. \nThe writing is clear, and the arguments are well-structured. \n- Weaknesses: \nI found it somewhat surprising that expanding the context beyond the sentences containing the blanks did not improve model performance. This is especially intriguing given that the HierEnc model, which aggregates information from other contexts, does utilize additional context in a different way. There are two potential explanations for this: either the sentences containing the blanks are consistently more informative for the task than those without, as suggested in the paper, though this seems somewhat counterintuitive; or the method of incorporating additional context in the HierEnc model, via the temporal network, is significantly more effective than simply expanding the individual context \\(C\\) and feeding the larger \\(C\\) into the recurrent network. Could this latter explanation be a contributing factor? \n- General Discussion: \nI am particularly impressed by the task and dataset proposed in this paper. I believe this setup has the potential to significantly advance the field. In my view, this is the paper's primary contribution."
        }
    ]
}