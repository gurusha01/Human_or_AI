{
    "version": "2025-01-09-base",
    "scanId": "06bf2d31-c755-44ba-b854-344a15ba23da",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9997410774230957,
                    "sentence": "This paper introduces a joint CTC-attention end-to-end ASR framework that leverages the strengths of both approaches in training and decoding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998056292533875,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996724128723145,
                    "sentence": "The study presents a robust implementation of the hybrid CTC-attention framework for both training and decoding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995886087417603,
                    "sentence": "The experimental results demonstrate that the proposed method achieves performance improvements in Japanese CSJ and Mandarin Chinese telephone speech recognition tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997309446334839,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996348023414612,
                    "sentence": "The primary concern is the similarity of this work to Ref [Kim et al., 2016], which is set to be officially published at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) in March 2017.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990772008895874,
                    "sentence": "While [Kim et al., 2016] introduces joint CTC-attention with multi-task learning (MTL) for English ASR, this paper extends the approach by incorporating joint decoding and applying it to Japanese and Chinese ASR tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987488985061646,
                    "sentence": "However, the distinction between the two works is not clearly articulated by the authors, making it challenging to pinpoint the novel contributions of this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998207688331604,
                    "sentence": "(a) Title:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9907932281494141,
                    "sentence": "The title of Ref [Kim et al., 2016] is \"Joint CTC-Attention Based End-to-End Speech Recognition Using Multi-task Learning,\" while the title of this paper is \"Joint CTC-attention End-to-end Speech Recognition.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9940223097801208,
                    "sentence": "The title here is overly broad.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9932398200035095,
                    "sentence": "If this were the first work on \"Joint CTC-attention,\" the title would be appropriate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9880874752998352,
                    "sentence": "Alternatively, if Ref [Kim et al., 2016] remained a pre-published arXiv paper, the title might still be acceptable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9923502802848816,
                    "sentence": "However, since [Kim et al., 2016] will be officially published earlier, a more specific title highlighting this paper's unique contributions compared to the existing work is necessary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9905287623405457,
                    "sentence": "(b) Introduction:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9946313500404358,
                    "sentence": "The authors state, \"We propose to take advantage of the constrained CTC alignment in a hybrid CTC-attention based system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974210858345032,
                    "sentence": "During training, we attach a CTC objective to an attention-based encoder network as a regularization, as proposed by [Kim et al., 2016].\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9964964985847473,
                    "sentence": "However, leveraging constrained CTC alignment in a hybrid CTC-attention system is an idea originally introduced by [Kim et al., 2016].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9976201057434082,
                    "sentence": "Consequently, the discussion on the advantages of combining attention-based and CTC-based ASR is not novel.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9949876070022583,
                    "sentence": "Additionally, the phrasing \"we propose \"¦ as proposed by [Kim et al., 2016]\" is problematic.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9971156120300293,
                    "sentence": "While it is valid to build upon prior work with extensions, it is not appropriate to re-propose an existing idea.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9917206168174744,
                    "sentence": "Therefore, it is crucial for the authors to clearly delineate the original contributions of this paper and clarify its position relative to the existing literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9918597340583801,
                    "sentence": "(c) Experimental Results:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9807960391044617,
                    "sentence": "While [Kim et al., 2016] applied the joint CTC-attention framework to English ASR tasks, this paper focuses on Japanese and Mandarin Chinese tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9724867343902588,
                    "sentence": "It would be valuable for the authors to elaborate on the specific challenges posed by Japanese and Mandarin Chinese ASR that may not arise in English ASR.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9554187655448914,
                    "sentence": "For instance, the paper could discuss how the system handles multiple possible outputs (e.g., Kanji, Hiragana, and Katakana) for Japanese speech input without relying on linguistic resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9738329648971558,
                    "sentence": "Addressing such language-specific challenges could represent a significant contribution of this work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5304451584815979,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.22848308086395264,
                    "sentence": "It is recommended to cite Ref [Kim et al., 2016] from its official IEEE ICASSP publication rather than the pre-published arXiv version:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.3575321137905121,
                    "sentence": "Kim, S., Hori, T., Watanabe, S., \"Joint CTC-Attention Based End-to-End Speech Recognition Using Multi-task Learning,\" IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017, pp.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.2769831120967865,
                    "sentence": "to appear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.9658502932045533,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9658502932045533,
                "mixed": 0.034149706795446697
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9658502932045533,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9658502932045533,
                    "human": 0,
                    "mixed": 0.034149706795446697
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper introduces a joint CTC-attention end-to-end ASR framework that leverages the strengths of both approaches in training and decoding.\n- Strengths: \nThe study presents a robust implementation of the hybrid CTC-attention framework for both training and decoding. The experimental results demonstrate that the proposed method achieves performance improvements in Japanese CSJ and Mandarin Chinese telephone speech recognition tasks.\n- Weaknesses: \nThe primary concern is the similarity of this work to Ref [Kim et al., 2016], which is set to be officially published at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) in March 2017. While [Kim et al., 2016] introduces joint CTC-attention with multi-task learning (MTL) for English ASR, this paper extends the approach by incorporating joint decoding and applying it to Japanese and Chinese ASR tasks. However, the distinction between the two works is not clearly articulated by the authors, making it challenging to pinpoint the novel contributions of this paper.\n(a) Title: \nThe title of Ref [Kim et al., 2016] is \"Joint CTC-Attention Based End-to-End Speech Recognition Using Multi-task Learning,\" while the title of this paper is \"Joint CTC-attention End-to-end Speech Recognition.\" The title here is overly broad. If this were the first work on \"Joint CTC-attention,\" the title would be appropriate. Alternatively, if Ref [Kim et al., 2016] remained a pre-published arXiv paper, the title might still be acceptable. However, since [Kim et al., 2016] will be officially published earlier, a more specific title highlighting this paper's unique contributions compared to the existing work is necessary.\n(b) Introduction: \nThe authors state, \"We propose to take advantage of the constrained CTC alignment in a hybrid CTC-attention based system. During training, we attach a CTC objective to an attention-based encoder network as a regularization, as proposed by [Kim et al., 2016].\" However, leveraging constrained CTC alignment in a hybrid CTC-attention system is an idea originally introduced by [Kim et al., 2016]. Consequently, the discussion on the advantages of combining attention-based and CTC-based ASR is not novel. Additionally, the phrasing \"we propose \"¦ as proposed by [Kim et al., 2016]\" is problematic. While it is valid to build upon prior work with extensions, it is not appropriate to re-propose an existing idea. Therefore, it is crucial for the authors to clearly delineate the original contributions of this paper and clarify its position relative to the existing literature.\n(c) Experimental Results: \nWhile [Kim et al., 2016] applied the joint CTC-attention framework to English ASR tasks, this paper focuses on Japanese and Mandarin Chinese tasks. It would be valuable for the authors to elaborate on the specific challenges posed by Japanese and Mandarin Chinese ASR that may not arise in English ASR. For instance, the paper could discuss how the system handles multiple possible outputs (e.g., Kanji, Hiragana, and Katakana) for Japanese speech input without relying on linguistic resources. Addressing such language-specific challenges could represent a significant contribution of this work.\n- General Discussion: \nIt is recommended to cite Ref [Kim et al., 2016] from its official IEEE ICASSP publication rather than the pre-published arXiv version: \nKim, S., Hori, T., Watanabe, S., \"Joint CTC-Attention Based End-to-End Speech Recognition Using Multi-task Learning,\" IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear."
        }
    ]
}