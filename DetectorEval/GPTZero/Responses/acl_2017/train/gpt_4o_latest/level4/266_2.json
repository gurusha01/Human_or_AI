{
    "version": "2025-01-09-base",
    "scanId": "d5f3518f-fa75-4282-96ea-28bd2caa1802",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9916051030158997,
                    "sentence": "- Strengths: This paper provides an engaging and detailed investigation into the impact of leveraging domain-specific corpora for training word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9926237463951111,
                    "sentence": "The authors offer a clear and well-structured explanation of their assumptions, contributions, methodology, and findings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9901992082595825,
                    "sentence": "Additionally, the evaluation thoroughly examines multiple facets of the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.996147096157074,
                    "sentence": "- Weaknesses: Some of the conclusions drawn by the authors are not entirely supported by the numerical results presented.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9879599213600159,
                    "sentence": "For instance, the claim that the improvements from using specialized corpora for training word vectors are more significant for Catalan than for English seems questionable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9928414225578308,
                    "sentence": "Specifically, in Table 6, none of the combination methods surpass the baseline performance for the 300-dimensional vectors, which raises doubts about this conclusion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9817134141921997,
                    "sentence": "- General Discussion: The paper conducts a series of straightforward yet compelling experiments, demonstrating that word vectors (trained here using the skip-gram model) can significantly benefit from the use of relevant, domain-specific corpora.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9637693166732788,
                    "sentence": "The work addresses key questions that are highly relevant to practitioners in the field of natural language processing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9720597863197327,
                    "sentence": "Furthermore, the paper is exceptionally well-written and systematically organized, making it easy to follow and understand.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.8128347701425863,
            "class_probabilities": {
                "human": 0.18698804302060604,
                "ai": 0.8128347701425863,
                "mixed": 0.00017718683680779119
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8128347701425863,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8128347701425863,
                    "human": 0.18698804302060604,
                    "mixed": 0.00017718683680779119
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "- Strengths: This paper provides an engaging and detailed investigation into the impact of leveraging domain-specific corpora for training word embeddings. The authors offer a clear and well-structured explanation of their assumptions, contributions, methodology, and findings. Additionally, the evaluation thoroughly examines multiple facets of the proposed approach.\n- Weaknesses: Some of the conclusions drawn by the authors are not entirely supported by the numerical results presented. For instance, the claim that the improvements from using specialized corpora for training word vectors are more significant for Catalan than for English seems questionable. Specifically, in Table 6, none of the combination methods surpass the baseline performance for the 300-dimensional vectors, which raises doubts about this conclusion.\n- General Discussion: The paper conducts a series of straightforward yet compelling experiments, demonstrating that word vectors (trained here using the skip-gram model) can significantly benefit from the use of relevant, domain-specific corpora. The work addresses key questions that are highly relevant to practitioners in the field of natural language processing. Furthermore, the paper is exceptionally well-written and systematically organized, making it easy to follow and understand."
        }
    ]
}