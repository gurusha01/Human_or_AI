{
    "version": "2025-01-09-base",
    "scanId": "dc1f730d-1630-4d24-aa84-c2c63723f5b0",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.921026885509491,
                    "sentence": "Paraphrased Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9306754469871521,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9036657810211182,
                    "sentence": "1. The proposed models demonstrate significant and consistent improvements over reasonable baselines across two distinct tasks (word similarity and word analogy).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9013683199882507,
                    "sentence": "This not only underscores the effectiveness of the models but also highlights the potential benefits of leveraging sememe information from existing knowledge bases to enhance word representation learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7243637442588806,
                    "sentence": "2. The paper makes a meaningful contribution to the ongoing research on addressing polysemy in word representation learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.689017653465271,
                    "sentence": "It builds effectively on prior work while introducing novel ideas and enhancements, such as the use of an attention mechanism to incorporate a form of soft word sense disambiguation into the learning process, which could be of interest to the research community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8484569191932678,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6419991850852966,
                    "sentence": "1. Presentation and clarity: Key details regarding the proposed models are either omitted or insufficiently explained (see further comments below).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7492716312408447,
                    "sentence": "While the paper is generally well-written, the manuscript would require improvements in clarity if accepted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.663183331489563,
                    "sentence": "2. The evaluation on the word analogy task appears somewhat biased, given that the semantic relations are explicitly encoded through sememes, as acknowledged by the authors themselves (see further comments below).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6697981357574463,
                    "sentence": "General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5350849628448486,
                    "sentence": "1. The authors emphasize the importance of addressing polysemy and learning sense-specific representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.40413135290145874,
                    "sentence": "While polysemy is accounted for by calculating sense distributions for words in specific contexts during the learning process, the evaluation tasks are entirely context-independent.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.414797306060791,
                    "sentence": "This implies that, ultimately, only a single vector per word is evaluated\"\"or at least this is how the evaluation is conducted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2770102322101593,
                    "sentence": "Instead, word sense disambiguation and sememe information are utilized to enhance word representation learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.34088581800460815,
                    "sentence": "This distinction needs to be clarified in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.22798025608062744,
                    "sentence": "2. The process for learning sememe embeddings is not clearly explained, and the description of the SSA model seems to assume the prior existence of sememe embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5026770234107971,
                    "sentence": "This is a critical detail for understanding the subsequent models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.42819395661354065,
                    "sentence": "Additionally, it is unclear whether the SAC and SAT models require pre-trained sememe embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5189473628997803,
                    "sentence": "3. The comparison between the proposed models and those that consider word senses but not sememes is unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7135879993438721,
                    "sentence": "For example, is the MST baseline representative of such a model?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5642441511154175,
                    "sentence": "If so, this is not adequately described, as the emphasis is placed on soft versus hard word sense disambiguation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6517176628112793,
                    "sentence": "Including additional baselines from related work would strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5728480815887451,
                    "sentence": "4. The authors reasonably argue that the proposed models are particularly beneficial for learning representations of low-frequency words (by mapping words to a smaller set of sememes shared across words).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9316044449806213,
                    "sentence": "However, no empirical evidence is provided to support this claim.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9546588659286499,
                    "sentence": "It would have been valuable for the authors to explore this aspect further.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8691871762275696,
                    "sentence": "Moreover, this hypothesis does not appear to account for the observed improvements, as the word similarity datasets primarily involve frequent word pairs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8191742300987244,
                    "sentence": "5. Related to the previous point, the observed improvements seem to stem more from the integration of sememe information than from word sense disambiguation within the learning process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8445009589195251,
                    "sentence": "As noted earlier, the evaluation relies solely on context-independent word representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.855964720249176,
                    "sentence": "Even if the method enables the learning of sememe- and sense-specific representations, these representations would need to be aggregated for the evaluation tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8533101677894592,
                    "sentence": "6. The example illustrating HowNet (Figure 1) is somewhat unclear, particularly with respect to the modifiers of \"computer.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9321589469909668,
                    "sentence": "7. The paper states that the models are trained using their optimal parameters, but it does not specify how these parameters are determined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9212068915367126,
                    "sentence": "Additionally, it is unclear how the value of K is set\"\"is it optimized for each model or randomly chosen for each target word observation?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8763856887817383,
                    "sentence": "Lastly, the motivation for setting K' to 2 is not explained.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.5710657228372709
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.42261306532663323,
            "class_probabilities": {
                "human": 0.5683417085427136,
                "ai": 0.42261306532663323,
                "mixed": 0.009045226130653266
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5683417085427136,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.42261306532663323,
                    "human": 0.5683417085427136,
                    "mixed": 0.009045226130653266
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Paraphrased Review\nStrengths:\n1. The proposed models demonstrate significant and consistent improvements over reasonable baselines across two distinct tasks (word similarity and word analogy). This not only underscores the effectiveness of the models but also highlights the potential benefits of leveraging sememe information from existing knowledge bases to enhance word representation learning. \n2. The paper makes a meaningful contribution to the ongoing research on addressing polysemy in word representation learning. It builds effectively on prior work while introducing novel ideas and enhancements, such as the use of an attention mechanism to incorporate a form of soft word sense disambiguation into the learning process, which could be of interest to the research community.\nWeaknesses:\n1. Presentation and clarity: Key details regarding the proposed models are either omitted or insufficiently explained (see further comments below). While the paper is generally well-written, the manuscript would require improvements in clarity if accepted. \n2. The evaluation on the word analogy task appears somewhat biased, given that the semantic relations are explicitly encoded through sememes, as acknowledged by the authors themselves (see further comments below).\nGeneral Discussion:\n1. The authors emphasize the importance of addressing polysemy and learning sense-specific representations. While polysemy is accounted for by calculating sense distributions for words in specific contexts during the learning process, the evaluation tasks are entirely context-independent. This implies that, ultimately, only a single vector per word is evaluated\"\"or at least this is how the evaluation is conducted. Instead, word sense disambiguation and sememe information are utilized to enhance word representation learning. This distinction needs to be clarified in the paper. \n2. The process for learning sememe embeddings is not clearly explained, and the description of the SSA model seems to assume the prior existence of sememe embeddings. This is a critical detail for understanding the subsequent models. Additionally, it is unclear whether the SAC and SAT models require pre-trained sememe embeddings. \n3. The comparison between the proposed models and those that consider word senses but not sememes is unclear. For example, is the MST baseline representative of such a model? If so, this is not adequately described, as the emphasis is placed on soft versus hard word sense disambiguation. Including additional baselines from related work would strengthen the paper. \n4. The authors reasonably argue that the proposed models are particularly beneficial for learning representations of low-frequency words (by mapping words to a smaller set of sememes shared across words). However, no empirical evidence is provided to support this claim. It would have been valuable for the authors to explore this aspect further. Moreover, this hypothesis does not appear to account for the observed improvements, as the word similarity datasets primarily involve frequent word pairs. \n5. Related to the previous point, the observed improvements seem to stem more from the integration of sememe information than from word sense disambiguation within the learning process. As noted earlier, the evaluation relies solely on context-independent word representations. Even if the method enables the learning of sememe- and sense-specific representations, these representations would need to be aggregated for the evaluation tasks. \n6. The example illustrating HowNet (Figure 1) is somewhat unclear, particularly with respect to the modifiers of \"computer.\" \n7. The paper states that the models are trained using their optimal parameters, but it does not specify how these parameters are determined. Additionally, it is unclear how the value of K is set\"\"is it optimized for each model or randomly chosen for each target word observation? Lastly, the motivation for setting K' to 2 is not explained."
        }
    ]
}