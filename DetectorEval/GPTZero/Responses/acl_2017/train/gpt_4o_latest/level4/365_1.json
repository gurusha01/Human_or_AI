{
    "version": "2025-01-09-base",
    "scanId": "e757cd36-61fb-4f40-a27b-f866f696b612",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.98493891954422,
                    "sentence": "[Updated after reading the authors' response: While the alignment of the hidden units does not align with my intuition or prior experience, I am open to the possibility that I may be mistaken in this case.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9712826013565063,
                    "sentence": "It is important to discuss the alignment in the paper, and it might also be worth sanity-checking whether this alignment disappears when using a different initialization seed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9843883514404297,
                    "sentence": "If the authors' claim that the new model is significantly different but only marginally better-performing \"\" with a 10% error reduction \"\" is accurate, I wonder whether combining the new model with the old one in an ensemble might yield better results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.984852135181427,
                    "sentence": "If the errors across the models are complementary, ensembling could provide a meaningful performance boost.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9579916000366211,
                    "sentence": "Overall, this is a solid paper, and I appreciate the authors' response.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9848140478134155,
                    "sentence": "I am raising my review score to a 4.]",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997812509536743,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993637204170227,
                    "sentence": "1) The evidence supporting the connection between attention mechanisms and multi-task learning (MTL) is compelling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993014335632324,
                    "sentence": "2) The methods are well-suited to the task, and the models perform competitively compared to the state-of-the-art.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999790608882904,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988421201705933,
                    "sentence": "1) The paper lacks critical details in some areas.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991396069526672,
                    "sentence": "2) The proposed models are not particularly novel.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993329644203186,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990876913070679,
                    "sentence": "This paper introduces a new approach to historical text normalization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9958658814430237,
                    "sentence": "While the model achieves strong performance, the primary contribution lies in the hypothesis that attention mechanisms for this task can be learned through multi-task learning, with pronunciation modeling as an auxiliary task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9935817718505859,
                    "sentence": "The proposed connection between attention mechanisms and MTL is intriguing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9919803738594055,
                    "sentence": "However, there are two significant areas where the paper could be improved.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9967398047447205,
                    "sentence": "First, the paper provides little explanation as to why the pronunciation task would necessitate an attention mechanism similar to the one used for normalization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9843831062316895,
                    "sentence": "The authors mention that spelling variation often arises from differences in pronunciation, which explains the relationship between the two tasks (normalization and pronunciation).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9773549437522888,
                    "sentence": "However, it remains unclear why performing MTL on these tasks would result in an implicit attention mechanism \"\" and why this implicit mechanism would perform better without the inclusion of an explicit attention mechanism.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9388634562492371,
                    "sentence": "This remains an open question.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9766039252281189,
                    "sentence": "While it is acceptable for some questions to remain unanswered, offering at least a plausible hypothesis here would strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9167543649673462,
                    "sentence": "The second issue concerns clarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9878644943237305,
                    "sentence": "Although the writing is generally clear, several important details are omitted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9755751490592957,
                    "sentence": "The most critical omission is the detailed description of the attention mechanism itself.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9642851948738098,
                    "sentence": "Given its central role in the paper, this method should be explained in detail rather than relying on references to prior work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9342882633209229,
                    "sentence": "For instance, I found the explanation in Section 3.4 to be unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8877594470977783,
                    "sentence": "Additionally, I had difficulty understanding Figure 4.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9187952280044556,
                    "sentence": "While the output dimensions of the two models are the same, it is unclear why their hidden layer dimensions would be directly comparable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9152171611785889,
                    "sentence": "Typically, the organization of hidden states differs significantly across models and may even be permuted, making such comparisons non-trivial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.956595778465271,
                    "sentence": "The Kappa statistic comparing attention and MTL should also be evaluated against the same statistic for each of these models relative to the baseline model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9191600680351257,
                    "sentence": "In Section 5, does the row labeled \"< 0.21\" represent an upper bound across all datasets?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9236371517181396,
                    "sentence": "Clarifying this point would be helpful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9177055954933167,
                    "sentence": "Finally, the analysis in Section 5 suggests that the attention and MTL approaches result in substantial changes to the model (as evidenced by Figure 5).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9223036766052246,
                    "sentence": "However, the observed experimental improvements in accuracy are relatively modest (around 2%), which seems somewhat contradictory.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9984800378301695,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984800378301695,
                "mixed": 0.0015199621698304396
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984800378301695,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984800378301695,
                    "human": 0,
                    "mixed": 0.0015199621698304396
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "[Updated after reading the authors' response: While the alignment of the hidden units does not align with my intuition or prior experience, I am open to the possibility that I may be mistaken in this case. It is important to discuss the alignment in the paper, and it might also be worth sanity-checking whether this alignment disappears when using a different initialization seed. If the authors' claim that the new model is significantly different but only marginally better-performing \"\" with a 10% error reduction \"\" is accurate, I wonder whether combining the new model with the old one in an ensemble might yield better results. If the errors across the models are complementary, ensembling could provide a meaningful performance boost. Overall, this is a solid paper, and I appreciate the authors' response. I am raising my review score to a 4.]\n- Strengths:\n 1) The evidence supporting the connection between attention mechanisms and multi-task learning (MTL) is compelling.\n 2) The methods are well-suited to the task, and the models perform competitively compared to the state-of-the-art.\n- Weaknesses:\n 1) The paper lacks critical details in some areas.\n 2) The proposed models are not particularly novel.\n- General Discussion:\nThis paper introduces a new approach to historical text normalization. While the model achieves strong performance, the primary contribution lies in the hypothesis that attention mechanisms for this task can be learned through multi-task learning, with pronunciation modeling as an auxiliary task. The proposed connection between attention mechanisms and MTL is intriguing.\nHowever, there are two significant areas where the paper could be improved. First, the paper provides little explanation as to why the pronunciation task would necessitate an attention mechanism similar to the one used for normalization. The authors mention that spelling variation often arises from differences in pronunciation, which explains the relationship between the two tasks (normalization and pronunciation). However, it remains unclear why performing MTL on these tasks would result in an implicit attention mechanism \"\" and why this implicit mechanism would perform better without the inclusion of an explicit attention mechanism. This remains an open question. While it is acceptable for some questions to remain unanswered, offering at least a plausible hypothesis here would strengthen the paper.\nThe second issue concerns clarity. Although the writing is generally clear, several important details are omitted. The most critical omission is the detailed description of the attention mechanism itself. Given its central role in the paper, this method should be explained in detail rather than relying on references to prior work. For instance, I found the explanation in Section 3.4 to be unclear.\nAdditionally, I had difficulty understanding Figure 4. While the output dimensions of the two models are the same, it is unclear why their hidden layer dimensions would be directly comparable. Typically, the organization of hidden states differs significantly across models and may even be permuted, making such comparisons non-trivial.\nThe Kappa statistic comparing attention and MTL should also be evaluated against the same statistic for each of these models relative to the baseline model.\nIn Section 5, does the row labeled \"< 0.21\" represent an upper bound across all datasets? Clarifying this point would be helpful.\nFinally, the analysis in Section 5 suggests that the attention and MTL approaches result in substantial changes to the model (as evidenced by Figure 5). However, the observed experimental improvements in accuracy are relatively modest (around 2%), which seems somewhat contradictory."
        }
    ]
}