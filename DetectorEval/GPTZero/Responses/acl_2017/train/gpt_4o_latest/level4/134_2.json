{
    "version": "2025-01-09-base",
    "scanId": "134c7f96-0544-479f-ac78-24f81397bae7",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999710321426392,
                    "sentence": "The paper presents a joint neural framework for argumentation mining, exploring multiple approaches, including:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999501705169678,
                    "sentence": "1) framing the task as a dependency parsing problem (evaluating several parsers),",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998807311058044,
                    "sentence": "2) framing the task as a sequence labeling problem,",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999222755432129,
                    "sentence": "3) employing multitask learning (built on a sequence labeling model),",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999370574951172,
                    "sentence": "4) utilizing an off-the-shelf neural model for entity and relation labeling (LSTM-ER), and",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999902606010437,
                    "sentence": "5) leveraging ILP-based state-of-the-art models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999479651451111,
                    "sentence": "All approaches are evaluated using F1 scores for concepts and relations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999449849128723,
                    "sentence": "Dependency-based methods show limited effectiveness, while sequence labeling approaches perform well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999225735664368,
                    "sentence": "The LSTM-ER model achieves strong results, particularly at the paragraph level.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998159408569336,
                    "sentence": "Both the sequence labeling and LSTM-ER models outperform the ILP-based approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998798966407776,
                    "sentence": "The paper provides a detailed supplement, covering training procedures, hyperparameter optimization, and other technical aspects.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997659921646118,
                    "sentence": "Additionally, the authors demonstrate that multitask learning can significantly enhance sequence labeling models, with the claim task contributing more than the relation task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997811317443848,
                    "sentence": "Overall, this work offers a thorough exploration of neural approaches to end-to-end argumentation mining.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999366998672485,
                    "sentence": "Major Remarks",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999875545501709,
                    "sentence": "- One concern pertains to the dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999900758266449,
                    "sentence": "It is unclear whether essays in the training and test sets might cover the same topics, potentially leading to information leakage if writers reuse arguments across essays.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999115467071533,
                    "sentence": "This could result in overly optimistic performance estimates.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998999238014221,
                    "sentence": "However, since the same issue likely affects the ILP models, the proposed models do not appear to have an unfair advantage.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999290108680725,
                    "sentence": "Nevertheless, this is worth discussing in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9950337409973145,
                    "sentence": "- Another concern is that one of the top-performing models, LSTM-ER, is essentially an out-of-the-box application of a pre-existing model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.994594156742096,
                    "sentence": "That said, the success of sequence-based models, combined with the extensive experiments and insights provided, makes this work worthy of publication.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993869662284851,
                    "sentence": "Minor Remarks and Questions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986507892608643,
                    "sentence": "- Lines 222-226: Are you suggesting that it is possible to reconstruct the full graph from a tree output?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9961258172988892,
                    "sentence": "This section is somewhat unclear and could benefit from clarification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999396026134491,
                    "sentence": "- Lines 443-444: The order of presentation in this section (sequence tagging dependency-based multitask learning with sequence tagging) is slightly confusing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983323216438293,
                    "sentence": "Reversing the first two would improve readability, as readers may have forgotten the meaning of STag_T by this point.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992729425430298,
                    "sentence": "- Line 455: The claim that the system \"de-couples\" tasks while \"jointly modeling\" them is unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993311166763306,
                    "sentence": "Isn't coupling necessary for joint modeling?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9951672554016113,
                    "sentence": "I also reviewed Miwa and Bansal but could not find this concept explained there.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993404150009155,
                    "sentence": "- Lines 477-479: The statement about decoupling relation and entity information is confusing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989498853683472,
                    "sentence": "My interpretation is that the system treats some tasks as \"tree edges\" and others as \"labels on those edges,\" effectively decoupling them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975192546844482,
                    "sentence": "If this is correct, please clarify this explanation in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992032647132874,
                    "sentence": "- Are the F1 scores for paragraph- and essay-level settings directly comparable, particularly for relation tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990507960319519,
                    "sentence": "It seems possible that paragraph-based models might inherently miss cross-paragraph relations, as they are not designed to account for them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper presents a joint neural framework for argumentation mining, exploring multiple approaches, including: \n1) framing the task as a dependency parsing problem (evaluating several parsers), \n2) framing the task as a sequence labeling problem, \n3) employing multitask learning (built on a sequence labeling model), \n4) utilizing an off-the-shelf neural model for entity and relation labeling (LSTM-ER), and \n5) leveraging ILP-based state-of-the-art models. \nAll approaches are evaluated using F1 scores for concepts and relations. Dependency-based methods show limited effectiveness, while sequence labeling approaches perform well. The LSTM-ER model achieves strong results, particularly at the paragraph level. Both the sequence labeling and LSTM-ER models outperform the ILP-based approach. The paper provides a detailed supplement, covering training procedures, hyperparameter optimization, and other technical aspects. Additionally, the authors demonstrate that multitask learning can significantly enhance sequence labeling models, with the claim task contributing more than the relation task. Overall, this work offers a thorough exploration of neural approaches to end-to-end argumentation mining. \nMajor Remarks \n- One concern pertains to the dataset. It is unclear whether essays in the training and test sets might cover the same topics, potentially leading to information leakage if writers reuse arguments across essays. This could result in overly optimistic performance estimates. However, since the same issue likely affects the ILP models, the proposed models do not appear to have an unfair advantage. Nevertheless, this is worth discussing in the paper. \n- Another concern is that one of the top-performing models, LSTM-ER, is essentially an out-of-the-box application of a pre-existing model. That said, the success of sequence-based models, combined with the extensive experiments and insights provided, makes this work worthy of publication. \nMinor Remarks and Questions \n- Lines 222–226: Are you suggesting that it is possible to reconstruct the full graph from a tree output? This section is somewhat unclear and could benefit from clarification. \n- Lines 443–444: The order of presentation in this section (sequence tagging dependency-based multitask learning with sequence tagging) is slightly confusing. Reversing the first two would improve readability, as readers may have forgotten the meaning of STag_T by this point. \n- Line 455: The claim that the system \"de-couples\" tasks while \"jointly modeling\" them is unclear. Isn't coupling necessary for joint modeling? I also reviewed Miwa and Bansal but could not find this concept explained there. \n- Lines 477–479: The statement about decoupling relation and entity information is confusing. My interpretation is that the system treats some tasks as \"tree edges\" and others as \"labels on those edges,\" effectively decoupling them. If this is correct, please clarify this explanation in the paper. \n- Are the F1 scores for paragraph- and essay-level settings directly comparable, particularly for relation tasks? It seems possible that paragraph-based models might inherently miss cross-paragraph relations, as they are not designed to account for them."
        }
    ]
}