{
    "version": "2025-01-09-base",
    "scanId": "d9ba7194-f634-4073-82e9-8018a4965409",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9990856647491455,
                    "sentence": "This study demonstrates that word representation learning (WRL) can be enhanced by incorporating sememes within an appropriate attention mechanism.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9973678588867188,
                    "sentence": "The authors hypothesize that sememes serve as a critical regularizer for WRL and word sense induction (WSI) tasks, and they propose the SE-WRL model, which simultaneously detects word senses and learns representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9759978652000427,
                    "sentence": "While experimental results suggest improvements in WRL, the specific gains for WSI remain unclear, as the evaluation is limited to a qualitative case study of a few examples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9229162335395813,
                    "sentence": "Overall, the paper is well-written and well-structured.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.035047583281993866,
                    "sentence": "In the final paragraph of the introduction, the authors outline three contributions of their work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.020001938566565514,
                    "sentence": "However, points (1) and (2) appear to be novelties of the approach rather than true contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.023240340873599052,
                    "sentence": "The primary contribution, in the reviewer's view, lies in the results, which demonstrate that sememe modeling can yield better word representations compared to competitive baselines (though the impact on WSI remains uncertain).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.044325172901153564,
                    "sentence": "Point (3), however, does not qualify as a contribution or a novelty.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.03868008032441139,
                    "sentence": "The three strategies proposed for SE-WRL modeling are logical and can be intuitively ranked based on their expected performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.03396584093570709,
                    "sentence": "The authors explain these strategies well, and the experimental results align with these intuitions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.046579498797655106,
                    "sentence": "However, the reviewer considers MST to be a fourth strategy rather than a baseline, as it appears to be inspired by Chen et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.015559665858745575,
                    "sentence": "(2014), where many WSI systems assume one sense per word given a context.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.012567451223731041,
                    "sentence": "MST often outperformed SSA and SAC.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.02687264047563076,
                    "sentence": "Unless clarified otherwise, MST seems nearly identical to SAT, differing only in that the target word is represented by its most probable sense rather than an attention-weighted average of all senses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.04974735900759697,
                    "sentence": "MST still employs an attention-based scheme, where the sense with the highest attention weight is selected, though it is unclear whether the target word is represented solely by the chosen sense embedding or by some function of it.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.11040390282869339,
                    "sentence": "The authors did not adequately justify their selection of datasets for training and evaluation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.0757543295621872,
                    "sentence": "The reference to the Sogou-T text corpus was unhelpful, as the reviewer is unfamiliar with the Chinese language, and it was unclear which specific dataset was used, given the multiple datasets mentioned on the referenced page.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.0411306694149971,
                    "sentence": "Additionally, the rationale for using two word similarity datasets and how they differ (e.g., whether one contains more rare words) was not explained, despite the models performing differently on these datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.036612339317798615,
                    "sentence": "This choice of datasets also precluded direct comparison with results from other works, raising further questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9480206966400146,
                    "sentence": "It is unclear whether the proposed SAT model achieves state-of-the-art results for Chinese word similarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9669554233551025,
                    "sentence": "For instance, Schnabel et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9496341943740845,
                    "sentence": "(2015) report a score of 0.640 on the WordSim-353 dataset using CBOW word embeddings, but no such comparison is provided here.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9318284392356873,
                    "sentence": "Clarifications are needed regarding certain model parameters, such as the vocabulary sizes for words (e.g., does Sogou-T contain 2.7 billion unique words?)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9421713948249817,
                    "sentence": "and word senses (e.g., how many word types are derived from HowNet?).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9376366138458252,
                    "sentence": "The notation used in the paper makes it unclear whether embeddings for senses and sememes across different words were shared.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9344302415847778,
                    "sentence": "While the reviewer assumes this to be the case, it raises the question of why 200-dimensional embeddings were used for only 1,889 sememes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8423939347267151,
                    "sentence": "A discussion on the complexity of model parameters would also be beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8902285099029541,
                    "sentence": "Perhaps due to space constraints, the discussion of experimental results lacks depth, offering little insight beyond the observation that SAT performed best.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9185926914215088,
                    "sentence": "Additionally, while the authors claim that sememes help in learning representations for low-frequency words, this claim is not substantiated by evaluation on a rare-words dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8028792142868042,
                    "sentence": "The reviewer has read the authors' response.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 7,
                    "completely_generated_prob": 2.1228438805416278e-06
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.6901468531159954,
            "class_probabilities": {
                "human": 0.3045187780479302,
                "ai": 0.6901468531159954,
                "mixed": 0.005334368836074327
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.6901468531159954,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.6901468531159954,
                    "human": 0.3045187780479302,
                    "mixed": 0.005334368836074327
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This study demonstrates that word representation learning (WRL) can be enhanced by incorporating sememes within an appropriate attention mechanism. The authors hypothesize that sememes serve as a critical regularizer for WRL and word sense induction (WSI) tasks, and they propose the SE-WRL model, which simultaneously detects word senses and learns representations. While experimental results suggest improvements in WRL, the specific gains for WSI remain unclear, as the evaluation is limited to a qualitative case study of a few examples. Overall, the paper is well-written and well-structured.\nIn the final paragraph of the introduction, the authors outline three contributions of their work. However, points (1) and (2) appear to be novelties of the approach rather than true contributions. The primary contribution, in the reviewer's view, lies in the results, which demonstrate that sememe modeling can yield better word representations compared to competitive baselines (though the impact on WSI remains uncertain). Point (3), however, does not qualify as a contribution or a novelty.\nThe three strategies proposed for SE-WRL modeling are logical and can be intuitively ranked based on their expected performance. The authors explain these strategies well, and the experimental results align with these intuitions. However, the reviewer considers MST to be a fourth strategy rather than a baseline, as it appears to be inspired by Chen et al. (2014), where many WSI systems assume one sense per word given a context. MST often outperformed SSA and SAC. Unless clarified otherwise, MST seems nearly identical to SAT, differing only in that the target word is represented by its most probable sense rather than an attention-weighted average of all senses. MST still employs an attention-based scheme, where the sense with the highest attention weight is selected, though it is unclear whether the target word is represented solely by the chosen sense embedding or by some function of it.\nThe authors did not adequately justify their selection of datasets for training and evaluation. The reference to the Sogou-T text corpus was unhelpful, as the reviewer is unfamiliar with the Chinese language, and it was unclear which specific dataset was used, given the multiple datasets mentioned on the referenced page. Additionally, the rationale for using two word similarity datasets and how they differ (e.g., whether one contains more rare words) was not explained, despite the models performing differently on these datasets. This choice of datasets also precluded direct comparison with results from other works, raising further questions.\nIt is unclear whether the proposed SAT model achieves state-of-the-art results for Chinese word similarity. For instance, Schnabel et al. (2015) report a score of 0.640 on the WordSim-353 dataset using CBOW word embeddings, but no such comparison is provided here.\nClarifications are needed regarding certain model parameters, such as the vocabulary sizes for words (e.g., does Sogou-T contain 2.7 billion unique words?) and word senses (e.g., how many word types are derived from HowNet?). The notation used in the paper makes it unclear whether embeddings for senses and sememes across different words were shared. While the reviewer assumes this to be the case, it raises the question of why 200-dimensional embeddings were used for only 1,889 sememes. A discussion on the complexity of model parameters would also be beneficial.\nPerhaps due to space constraints, the discussion of experimental results lacks depth, offering little insight beyond the observation that SAT performed best. Additionally, while the authors claim that sememes help in learning representations for low-frequency words, this claim is not substantiated by evaluation on a rare-words dataset.\nThe reviewer has read the authors' response."
        }
    ]
}