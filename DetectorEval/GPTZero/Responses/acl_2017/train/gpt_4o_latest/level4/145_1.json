{
    "version": "2025-01-09-base",
    "scanId": "a42d2c6e-8d6b-40f9-956b-0d35f57294e1",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9973873496055603,
                    "sentence": "Review: Multimodal Word Distributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9921078681945801,
                    "sentence": "- Strengths: Overall, this is a very solid and well-executed paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.979957103729248,
                    "sentence": "- Weaknesses: The comparisons with related approaches could be expanded.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9951848983764648,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9888834357261658,
                    "sentence": "This paper primarily focuses on presenting a novel model for learning multimodal word distributions, where multiple word meanings are represented using Gaussian mixtures\"\"essentially modeling a word as a collection of Gaussian distributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9807953834533691,
                    "sentence": "The proposed method builds upon the work of Vilnis and McCallum (2014), which represented words as unimodal Gaussian distributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9776853322982788,
                    "sentence": "By adopting a multimodal approach, the authors effectively address the challenge of polysemy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9656209349632263,
                    "sentence": "Overall, this is a strong and well-structured paper with clear exposition.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.981005847454071,
                    "sentence": "The experiments are conducted appropriately, and the qualitative analysis in Table 1 demonstrates results consistent with the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9539579749107361,
                    "sentence": "There are very few issues to critique, and the following comments are intended to further enhance the paper's clarity and comprehensiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8946679830551147,
                    "sentence": "Some comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.901843786239624,
                    "sentence": "_ It might be helpful to include a brief explanation of how the current approach differs from that of Tian et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9686207175254822,
                    "sentence": "(2014).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9723147749900818,
                    "sentence": "Both methods decompose single-word representations into multiple prototypes using a mixture model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7923047542572021,
                    "sentence": "_ There are a few missing citations that could be added to the related work section, such as:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9934101104736328,
                    "sentence": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space - Neelakantan, A., Shankar, J., Passos, A., McCallum.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9920718669891357,
                    "sentence": "EMNLP 2014",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9961346387863159,
                    "sentence": "Do Multi-Sense Embeddings Improve Natural Language Understanding?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9964313507080078,
                    "sentence": "- Li and Jurafsky, EMNLP 2015",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972495436668396,
                    "sentence": "Topical Word Embeddings - Liu Y., Liu Z., Chua T., Sun M. AAAI 2015",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9927079081535339,
                    "sentence": "_ Including the results from these approaches in Tables 3 and 4 could also be valuable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9945868253707886,
                    "sentence": "_ A question for the authors: What do you believe causes the performance drop of w2gm compared to w2g in the SWCS analysis?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.98969966173172,
                    "sentence": "I have reviewed the authors' response.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.6372010217509094,
            "class_probabilities": {
                "human": 0.22486260546481923,
                "ai": 0.6372010217509094,
                "mixed": 0.13793637278427123
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.6372010217509094,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.6372010217509094,
                    "human": 0.22486260546481923,
                    "mixed": 0.13793637278427123
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review: Multimodal Word Distributions\n- Strengths: Overall, this is a very solid and well-executed paper.\n- Weaknesses: The comparisons with related approaches could be expanded.\n- General Discussion:\nThis paper primarily focuses on presenting a novel model for learning multimodal word distributions, where multiple word meanings are represented using Gaussian mixtures\"\"essentially modeling a word as a collection of Gaussian distributions. The proposed method builds upon the work of Vilnis and McCallum (2014), which represented words as unimodal Gaussian distributions. By adopting a multimodal approach, the authors effectively address the challenge of polysemy.\nOverall, this is a strong and well-structured paper with clear exposition. The experiments are conducted appropriately, and the qualitative analysis in Table 1 demonstrates results consistent with the proposed approach. There are very few issues to critique, and the following comments are intended to further enhance the paper's clarity and comprehensiveness.\nSome comments:\n_ It might be helpful to include a brief explanation of how the current approach differs from that of Tian et al. (2014). Both methods decompose single-word representations into multiple prototypes using a mixture model.\n_ There are a few missing citations that could be added to the related work section, such as:\nEfficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space - Neelakantan, A., Shankar, J., Passos, A., McCallum. EMNLP 2014 \nDo Multi-Sense Embeddings Improve Natural Language Understanding? - Li and Jurafsky, EMNLP 2015 \nTopical Word Embeddings - Liu Y., Liu Z., Chua T., Sun M. AAAI 2015 \n_ Including the results from these approaches in Tables 3 and 4 could also be valuable.\n_ A question for the authors: What do you believe causes the performance drop of w2gm compared to w2g in the SWCS analysis?\nI have reviewed the authors' response."
        }
    ]
}