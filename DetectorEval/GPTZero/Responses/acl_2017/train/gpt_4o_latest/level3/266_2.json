{
    "version": "2025-01-09-base",
    "scanId": "a3629496-6b27-4cb3-8bd8-9b4d6d3b5594",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999843835830688,
                    "sentence": "Review of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999980330467224,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999947547912598,
                    "sentence": "This paper investigates the impact of task-specific corpora on training word embeddings for sentiment analysis, offering a detailed study on how subjectivity in corpora affects embedding quality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960064888,
                    "sentence": "The authors propose methods to quantify subjectivity in datasets, explore techniques for combining task-specific and generic corpora, and demonstrate the utility of these methods for under-resourced languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999943375587463,
                    "sentence": "The primary contributions of the paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999966025352478,
                    "sentence": "1. Quantification of Subjectivity: The paper introduces a method to measure the subjectivity of a corpus, providing a novel metric to assess its suitability for sentiment analysis tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999953508377075,
                    "sentence": "2. Combination Techniques: It evaluates three strategies\"\"appending, splicing, and concatenation\"\"for combining generic and task-specific datasets, with concatenation emerging as the most effective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999963641166687,
                    "sentence": "3. Application to Under-Resourced Languages: The study extends its findings to Catalan, showcasing the potential of these methods for languages with limited resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999930262565613,
                    "sentence": "4. Approximation of Task-Specific Data: The authors propose extracting subjective portions from generic corpora as a proxy for task-specific datasets, which is particularly useful for under-resourced scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999938607215881,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999915957450867,
                    "sentence": "1. Comprehensive Study: The paper conducts a thorough investigation into the effects of task-specific corpora on word embeddings, supported by clear explanations and robust experimental evaluations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999875426292419,
                    "sentence": "2. Practical Relevance: The proposed methods address a critical challenge in NLP\"\"how to optimize embeddings when task-specific data is scarce\"\"making the work highly relevant to the community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999817609786987,
                    "sentence": "3. Clarity and Organization: The paper is well-written and logically structured, making it accessible to readers from diverse backgrounds.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999987006187439,
                    "sentence": "4. Impactful Experiments: The experiments are simple yet impactful, answering key questions about the trade-offs between generic and task-specific data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999643564224243,
                    "sentence": "5. Under-Resourced Language Focus: The application to Catalan demonstrates the broader applicability of the methods and highlights their potential for addressing linguistic inequities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999852180480957,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999703168869019,
                    "sentence": "1. Insufficient Numerical Evidence for Claims: While the authors claim a pronounced improvement for Catalan over English, this assertion is not fully supported by numerical results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999589323997498,
                    "sentence": "The differences in performance metrics are not substantial enough to justify this conclusion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999602437019348,
                    "sentence": "2. Limited Exploration of Alternatives: The paper focuses heavily on subjectivity as a metric but does not explore other potential indicators, such as polarity, which could also influence embedding quality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999951958656311,
                    "sentence": "3. Lexical Overlap Analysis: While the lexical overlap analysis is insightful, it could benefit from a deeper exploration of how missing words impact specific sentiment classes (e.g., strong positive vs. strong negative).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998844265937805,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999012351036072,
                    "sentence": "1. Could you provide additional numerical evidence or statistical significance tests to support the claim of pronounced improvement for Catalan over English?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997541904449463,
                    "sentence": "2. Have you considered using polarity or other sentiment-specific metrics as an alternative to subjectivity for filtering corpora?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999573826789856,
                    "sentence": "If so, how do these compare to your current approach?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995765686035156,
                    "sentence": "3. How do the proposed methods generalize to other NLP tasks beyond sentiment analysis?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988160133361816,
                    "sentence": "Have you conducted any preliminary experiments in this direction?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999630868434906,
                    "sentence": "Conclusion",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993590116500854,
                    "sentence": "This paper makes a valuable contribution to the field of NLP by addressing the underexplored area of task-specific corpora for word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995649456977844,
                    "sentence": "Its methods are practical and impactful, particularly for under-resourced languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998229444026947,
                    "sentence": "However, some claims require stronger numerical support, and the exploration of alternative metrics could enhance the study's robustness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9964575171470642,
                    "sentence": "Overall, the paper is a strong candidate for acceptance, with minor revisions to address the identified weaknesses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                }
            ],
            "completely_generated_prob": 0.9961636828644501,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9961636828644501,
                "mixed": 0.003836317135549872
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9961636828644501,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9961636828644501,
                    "human": 0,
                    "mixed": 0.003836317135549872
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Paper\nSummary and Contributions\nThis paper investigates the impact of task-specific corpora on training word embeddings for sentiment analysis, offering a detailed study on how subjectivity in corpora affects embedding quality. The authors propose methods to quantify subjectivity in datasets, explore techniques for combining task-specific and generic corpora, and demonstrate the utility of these methods for under-resourced languages. The primary contributions of the paper are:\n1. Quantification of Subjectivity: The paper introduces a method to measure the subjectivity of a corpus, providing a novel metric to assess its suitability for sentiment analysis tasks.\n2. Combination Techniques: It evaluates three strategies\"\"appending, splicing, and concatenation\"\"for combining generic and task-specific datasets, with concatenation emerging as the most effective.\n3. Application to Under-Resourced Languages: The study extends its findings to Catalan, showcasing the potential of these methods for languages with limited resources.\n4. Approximation of Task-Specific Data: The authors propose extracting subjective portions from generic corpora as a proxy for task-specific datasets, which is particularly useful for under-resourced scenarios.\nStrengths\n1. Comprehensive Study: The paper conducts a thorough investigation into the effects of task-specific corpora on word embeddings, supported by clear explanations and robust experimental evaluations.\n2. Practical Relevance: The proposed methods address a critical challenge in NLP\"\"how to optimize embeddings when task-specific data is scarce\"\"making the work highly relevant to the community.\n3. Clarity and Organization: The paper is well-written and logically structured, making it accessible to readers from diverse backgrounds.\n4. Impactful Experiments: The experiments are simple yet impactful, answering key questions about the trade-offs between generic and task-specific data.\n5. Under-Resourced Language Focus: The application to Catalan demonstrates the broader applicability of the methods and highlights their potential for addressing linguistic inequities.\nWeaknesses\n1. Insufficient Numerical Evidence for Claims: While the authors claim a pronounced improvement for Catalan over English, this assertion is not fully supported by numerical results. The differences in performance metrics are not substantial enough to justify this conclusion.\n2. Limited Exploration of Alternatives: The paper focuses heavily on subjectivity as a metric but does not explore other potential indicators, such as polarity, which could also influence embedding quality.\n3. Lexical Overlap Analysis: While the lexical overlap analysis is insightful, it could benefit from a deeper exploration of how missing words impact specific sentiment classes (e.g., strong positive vs. strong negative).\nQuestions to Authors\n1. Could you provide additional numerical evidence or statistical significance tests to support the claim of pronounced improvement for Catalan over English?\n2. Have you considered using polarity or other sentiment-specific metrics as an alternative to subjectivity for filtering corpora? If so, how do these compare to your current approach?\n3. How do the proposed methods generalize to other NLP tasks beyond sentiment analysis? Have you conducted any preliminary experiments in this direction?\nConclusion\nThis paper makes a valuable contribution to the field of NLP by addressing the underexplored area of task-specific corpora for word embeddings. Its methods are practical and impactful, particularly for under-resourced languages. However, some claims require stronger numerical support, and the exploration of alternative metrics could enhance the study's robustness. Overall, the paper is a strong candidate for acceptance, with minor revisions to address the identified weaknesses."
        }
    ]
}