{
    "version": "2025-01-09-base",
    "scanId": "2e89aaa8-4bad-4643-8d7c-165494c5d226",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.994556188583374,
                    "sentence": "Review of the Submission",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9941404461860657,
                    "sentence": "Summary and Contributions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9832124710083008,
                    "sentence": "This paper proposes an attention-based two-step deep neural model combining Bidirectional Long Short-Term Memory (BiLSTM) and Convolutional Neural Networks (CNN) for event factuality identification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9899824261665344,
                    "sentence": "The model is evaluated on the FactBank corpus and demonstrates statistically significant improvements over state-of-the-art systems, particularly in the identification of three challenging factuality classes: CT-, PR+, and PS+.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9869917631149292,
                    "sentence": "The authors claim three primary contributions: (1) a two-step supervised framework for event factuality identification, (2) the use of an attention-based CNN to detect source introducing predicates (SIPs), and (3) the integration of BiLSTM and CNN for learning effective representations from syntactic paths and words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9831503629684448,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9829750061035156,
                    "sentence": "1. Empirical Performance: The proposed model achieves strong empirical results, with statistically significant improvements over existing baselines, particularly for underrepresented factuality classes (CT-, PR+, PS+).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9755101203918457,
                    "sentence": "This demonstrates the model's ability to handle challenging cases effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9390388131141663,
                    "sentence": "2. Attention Mechanism: The use of attention mechanisms enhances the model's ability to focus on critical syntactic and lexical features, as evidenced by improved performance when compared to non-attention-based baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.921846330165863,
                    "sentence": "3. Framework Design: The two-step framework, which separates Uu/Non-Uu classification from cue-based factuality identification, is well-motivated and addresses class imbalance issues effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9571559429168701,
                    "sentence": "4. Comprehensive Evaluation: The paper provides detailed experimental results, including micro- and macro-averaged F1 scores, and compares the proposed model against several baselines, demonstrating its robustness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9903168678283691,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9930959343910217,
                    "sentence": "1. Lack of Novelty: While the combination of BiLSTM and CNN is effective, the paper does not clearly establish whether this is the first neural network-based approach for event factuality identification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9952218532562256,
                    "sentence": "The novelty of the proposed architecture is not sufficiently differentiated from prior work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9930023550987244,
                    "sentence": "2. Error Analysis: The paper lacks a detailed error analysis, particularly for the low performance on Uu events with embedded sources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9938704967498779,
                    "sentence": "Understanding the dominant error sources would provide valuable insights into the model's limitations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9937350749969482,
                    "sentence": "3. Feature Comparison: The comparison with prior work is shallow and omits relevant studies from 2014 and 2015.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9919078350067139,
                    "sentence": "A more comprehensive discussion of features used in previous systems versus the proposed model would strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9843690991401672,
                    "sentence": "4. Clarity Issues: Several sections suffer from unclear explanations, intertwined descriptions, and typographical errors, which reduce readability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9973580837249756,
                    "sentence": "For instance, the benefits of the attention mechanism are not adequately described, and auxiliary words' roles are ambiguously presented.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9922794103622437,
                    "sentence": "5. Underspecified Modality/Polarity: The paper does not provide sufficient examples or explanations to illustrate the challenges of detecting underspecified modality (U) and polarity (u), which are critical for factuality identification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9963926076889038,
                    "sentence": "Questions to Authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9958681464195251,
                    "sentence": "1. Can you clarify whether this is the first neural network-based approach for event factuality identification?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9964421987533569,
                    "sentence": "If not, how does your model differ from prior neural approaches?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984565377235413,
                    "sentence": "2. Could you provide a detailed error analysis for the low performance on Uu events with embedded sources?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9969046115875244,
                    "sentence": "What are the dominant error sources, and how might they be addressed?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9932673573493958,
                    "sentence": "3. Why were the studies from 2014 and 2015 omitted from the feature comparison?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9955679774284363,
                    "sentence": "How do the features used in these studies compare to those in your model?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9951703548431396,
                    "sentence": "4. Could you elaborate on the specific benefits of the attention mechanism in your architecture?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9954190254211426,
                    "sentence": "How does it improve performance compared to non-attention-based models?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992169141769409,
                    "sentence": "Recommendation:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980247020721436,
                    "sentence": "While the paper demonstrates strong empirical results and provides a well-designed framework, the lack of clarity on novelty, insufficient error analysis, and shallow feature comparison are significant concerns.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980490803718567,
                    "sentence": "These issues should be addressed during the author response period.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982873797416687,
                    "sentence": "Pending satisfactory clarifications, the paper has potential for acceptance due to its strong empirical contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9841954571483108,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9841954571483108,
                "mixed": 0.015804542851689255
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9841954571483108,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9841954571483108,
                    "human": 0,
                    "mixed": 0.015804542851689255
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Submission\nSummary and Contributions: \nThis paper proposes an attention-based two-step deep neural model combining Bidirectional Long Short-Term Memory (BiLSTM) and Convolutional Neural Networks (CNN) for event factuality identification. The model is evaluated on the FactBank corpus and demonstrates statistically significant improvements over state-of-the-art systems, particularly in the identification of three challenging factuality classes: CT-, PR+, and PS+. The authors claim three primary contributions: (1) a two-step supervised framework for event factuality identification, (2) the use of an attention-based CNN to detect source introducing predicates (SIPs), and (3) the integration of BiLSTM and CNN for learning effective representations from syntactic paths and words. \nStrengths: \n1. Empirical Performance: The proposed model achieves strong empirical results, with statistically significant improvements over existing baselines, particularly for underrepresented factuality classes (CT-, PR+, PS+). This demonstrates the model's ability to handle challenging cases effectively. \n2. Attention Mechanism: The use of attention mechanisms enhances the model's ability to focus on critical syntactic and lexical features, as evidenced by improved performance when compared to non-attention-based baselines. \n3. Framework Design: The two-step framework, which separates Uu/Non-Uu classification from cue-based factuality identification, is well-motivated and addresses class imbalance issues effectively. \n4. Comprehensive Evaluation: The paper provides detailed experimental results, including micro- and macro-averaged F1 scores, and compares the proposed model against several baselines, demonstrating its robustness. \nWeaknesses: \n1. Lack of Novelty: While the combination of BiLSTM and CNN is effective, the paper does not clearly establish whether this is the first neural network-based approach for event factuality identification. The novelty of the proposed architecture is not sufficiently differentiated from prior work. \n2. Error Analysis: The paper lacks a detailed error analysis, particularly for the low performance on Uu events with embedded sources. Understanding the dominant error sources would provide valuable insights into the model's limitations. \n3. Feature Comparison: The comparison with prior work is shallow and omits relevant studies from 2014 and 2015. A more comprehensive discussion of features used in previous systems versus the proposed model would strengthen the paper. \n4. Clarity Issues: Several sections suffer from unclear explanations, intertwined descriptions, and typographical errors, which reduce readability. For instance, the benefits of the attention mechanism are not adequately described, and auxiliary words' roles are ambiguously presented. \n5. Underspecified Modality/Polarity: The paper does not provide sufficient examples or explanations to illustrate the challenges of detecting underspecified modality (U) and polarity (u), which are critical for factuality identification. \nQuestions to Authors: \n1. Can you clarify whether this is the first neural network-based approach for event factuality identification? If not, how does your model differ from prior neural approaches? \n2. Could you provide a detailed error analysis for the low performance on Uu events with embedded sources? What are the dominant error sources, and how might they be addressed? \n3. Why were the studies from 2014 and 2015 omitted from the feature comparison? How do the features used in these studies compare to those in your model? \n4. Could you elaborate on the specific benefits of the attention mechanism in your architecture? How does it improve performance compared to non-attention-based models? \nRecommendation: \nWhile the paper demonstrates strong empirical results and provides a well-designed framework, the lack of clarity on novelty, insufficient error analysis, and shallow feature comparison are significant concerns. These issues should be addressed during the author response period. Pending satisfactory clarifications, the paper has potential for acceptance due to its strong empirical contributions."
        }
    ]
}