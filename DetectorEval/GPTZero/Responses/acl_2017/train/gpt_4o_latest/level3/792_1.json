{
    "version": "2025-01-09-base",
    "scanId": "c1056bb3-4cf1-4897-bdc0-f65be9acb25d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999993622303009,
                    "sentence": "Review of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999987483024597,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998152256012,
                    "sentence": "This paper introduces LSTMEmbed, a novel model leveraging bidirectional LSTMs to jointly learn word and sense embeddings in a shared vector space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "The authors claim that their approach addresses limitations of classical embedding models like word2vec and GloVe by incorporating word order and enriching representations with semantic knowledge.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999968409538269,
                    "sentence": "The primary contributions of the paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999988079071045,
                    "sentence": "1. LSTMEmbed Model: A bidirectional LSTM-based architecture that jointly learns word and sense embeddings, outperforming classical approaches on certain benchmarks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999984502792358,
                    "sentence": "2. Semantic Enrichment: A novel method for injecting semantic knowledge into embeddings using pre-trained representations, which purportedly improves training efficiency and embedding quality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999984502792358,
                    "sentence": "3. Sense-based Representations: The model provides competitive sense embeddings while addressing the lack of word order in prior approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999978542327881,
                    "sentence": "1. Clear Problem Statement and Motivation: The paper identifies the limitations of existing embedding models (e.g., lack of word order and poor handling of word senses) and proposes a well-motivated solution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999963641166687,
                    "sentence": "2. Innovative Approach: The incorporation of bidirectional LSTMs and the use of pre-trained embeddings as a learning objective are interesting and novel contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "3. Comprehensive Evaluation: The authors evaluate their model on a wide range of tasks, including word similarity, synonym identification, and word analogy, using multiple datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940395355225,
                    "sentence": "4. Semantic Enrichment: The idea of leveraging richer pre-trained embeddings to enhance representations is compelling and demonstrates potential for further exploration.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999970197677612,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999968409538269,
                    "sentence": "1. Inconsistent Results: The model's performance is inconsistent across tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960064888,
                    "sentence": "While it shows improvements in word similarity and synonym identification, it underperforms in word analogy tasks, raising questions about its generalizability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "2. Experimental Setup Issues: The paper lacks clarity in describing the experimental setup, particularly regarding the choice of corpora, training details, and hyperparameter selection.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999963045120239,
                    "sentence": "For example, the rationale behind using specific datasets and corpus proportions is not well-justified.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971389770508,
                    "sentence": "3. Decline in Quality Towards the End: The paper's presentation and argumentation weaken in later sections.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999966025352478,
                    "sentence": "For instance, Table 4 lacks proper dimensionality alignment, and the results discussion is fragmented, making it difficult to draw clear conclusions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979138374329,
                    "sentence": "4. Unclear Motivation for Sense Representations: While sense embeddings are a focus, their evaluation is limited, and the motivation for learning shared word and sense representations is not well-articulated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9960215091705322,
                    "sentence": "5. Ambiguous Claims: The claim of faster training due to pre-trained embeddings is not substantiated with quantitative evidence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.995399534702301,
                    "sentence": "Additionally, synonym identification lacks a detailed description, and some test sets are not independent, complicating the validity of results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9889296889305115,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.993774950504303,
                    "sentence": "1. Can you provide more details about the training process for the word analogy task?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9940962195396423,
                    "sentence": "Why do you think the model underperforms in this task compared to others?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9845420718193054,
                    "sentence": "2. What is the motivation behind jointly learning word and sense embeddings in a shared space?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9770970344543457,
                    "sentence": "How does this improve downstream tasks compared to separate embeddings?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9873278737068176,
                    "sentence": "3. Can you clarify the dimensionality inconsistencies in Table 4 and provide a fair comparison across models?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9422492384910583,
                    "sentence": "4. How does the use of pre-trained embeddings quantitatively speed up training?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9644641280174255,
                    "sentence": "Can you provide runtime comparisons or ablation studies?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9415397644042969,
                    "sentence": "Recommendation",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9421746730804443,
                    "sentence": "While the paper addresses an important problem and proposes an interesting solution, the inconsistencies in results, lack of clarity in the experimental setup, and weak justification for certain claims make it difficult to fully endorse.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.88523930311203,
                    "sentence": "The paper would benefit from a more thorough analysis of negative outcomes, clearer motivation for sense embeddings, and stronger evidence for its claims.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9378842711448669,
                    "sentence": "I recommend major revisions before acceptance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9984930238596827,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984930238596827,
                "mixed": 0.001506976140317253
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984930238596827,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984930238596827,
                    "human": 0,
                    "mixed": 0.001506976140317253
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Paper\nSummary and Contributions\nThis paper introduces LSTMEmbed, a novel model leveraging bidirectional LSTMs to jointly learn word and sense embeddings in a shared vector space. The authors claim that their approach addresses limitations of classical embedding models like word2vec and GloVe by incorporating word order and enriching representations with semantic knowledge. The primary contributions of the paper are:\n1. LSTMEmbed Model: A bidirectional LSTM-based architecture that jointly learns word and sense embeddings, outperforming classical approaches on certain benchmarks.\n2. Semantic Enrichment: A novel method for injecting semantic knowledge into embeddings using pre-trained representations, which purportedly improves training efficiency and embedding quality.\n3. Sense-based Representations: The model provides competitive sense embeddings while addressing the lack of word order in prior approaches.\nStrengths\n1. Clear Problem Statement and Motivation: The paper identifies the limitations of existing embedding models (e.g., lack of word order and poor handling of word senses) and proposes a well-motivated solution.\n2. Innovative Approach: The incorporation of bidirectional LSTMs and the use of pre-trained embeddings as a learning objective are interesting and novel contributions.\n3. Comprehensive Evaluation: The authors evaluate their model on a wide range of tasks, including word similarity, synonym identification, and word analogy, using multiple datasets.\n4. Semantic Enrichment: The idea of leveraging richer pre-trained embeddings to enhance representations is compelling and demonstrates potential for further exploration.\nWeaknesses\n1. Inconsistent Results: The model's performance is inconsistent across tasks. While it shows improvements in word similarity and synonym identification, it underperforms in word analogy tasks, raising questions about its generalizability.\n2. Experimental Setup Issues: The paper lacks clarity in describing the experimental setup, particularly regarding the choice of corpora, training details, and hyperparameter selection. For example, the rationale behind using specific datasets and corpus proportions is not well-justified.\n3. Decline in Quality Towards the End: The paper's presentation and argumentation weaken in later sections. For instance, Table 4 lacks proper dimensionality alignment, and the results discussion is fragmented, making it difficult to draw clear conclusions.\n4. Unclear Motivation for Sense Representations: While sense embeddings are a focus, their evaluation is limited, and the motivation for learning shared word and sense representations is not well-articulated.\n5. Ambiguous Claims: The claim of faster training due to pre-trained embeddings is not substantiated with quantitative evidence. Additionally, synonym identification lacks a detailed description, and some test sets are not independent, complicating the validity of results.\nQuestions to Authors\n1. Can you provide more details about the training process for the word analogy task? Why do you think the model underperforms in this task compared to others?\n2. What is the motivation behind jointly learning word and sense embeddings in a shared space? How does this improve downstream tasks compared to separate embeddings?\n3. Can you clarify the dimensionality inconsistencies in Table 4 and provide a fair comparison across models?\n4. How does the use of pre-trained embeddings quantitatively speed up training? Can you provide runtime comparisons or ablation studies?\nRecommendation\nWhile the paper addresses an important problem and proposes an interesting solution, the inconsistencies in results, lack of clarity in the experimental setup, and weak justification for certain claims make it difficult to fully endorse. The paper would benefit from a more thorough analysis of negative outcomes, clearer motivation for sense embeddings, and stronger evidence for its claims. I recommend major revisions before acceptance."
        }
    ]
}