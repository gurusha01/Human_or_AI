{
    "version": "2025-01-09-base",
    "scanId": "ac697349-dbf6-40b2-b2f7-de36a092e575",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999998152256012,
                    "sentence": "Review of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999993443489075,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999995231628418,
                    "sentence": "This paper tackles the challenging task of open-domain question answering (QA) using Wikipedia as the sole knowledge source.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999995231628418,
                    "sentence": "The authors propose a system, DrWiki, which integrates a document retrieval module and a machine comprehension module.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999996423721313,
                    "sentence": "The system achieves state-of-the-art results on the SQuAD dataset and demonstrates competitive performance across multiple QA benchmarks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999989867210388,
                    "sentence": "The primary contributions of the paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999993443489075,
                    "sentence": "1. State-of-the-Art Results on SQuAD: The proposed Document Reader achieves state-of-the-art performance on the SQuAD dataset, showcasing the efficacy of the model's feature-rich representation and multi-layer bi-directional LSTM architecture.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999994039535522,
                    "sentence": "2. Comprehensive Evaluation: The paper evaluates the system across multiple QA datasets (SQuAD, CuratedTREC, WebQuestions, and WikiMovies), highlighting the generalizability of the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999999463558197,
                    "sentence": "3. Multitask Learning and Distant Supervision: The authors demonstrate the benefits of multitask learning and distant supervision in improving performance on open-domain QA tasks, particularly when training on multiple datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999997615814209,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999994039535522,
                    "sentence": "1. Novel Problem Setting: The paper addresses a critical and underexplored problem\"\"answering open-domain questions using only Wikipedia, without relying on external knowledge bases or redundant resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999999463558197,
                    "sentence": "This focus on a single knowledge source is both challenging and impactful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999999463558197,
                    "sentence": "2. Strong Empirical Results: The system achieves state-of-the-art results on SQuAD and performs competitively on other datasets, demonstrating the robustness of the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991059303284,
                    "sentence": "3. Comprehensive System Design: The modular design of DrWiki, with a well-defined Document Retriever and Document Reader, is clear and effective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991655349731,
                    "sentence": "The use of bigram hashing and TF-IDF for retrieval, combined with neural network-based comprehension, is a strong combination.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992847442627,
                    "sentence": "4. Detailed Evaluation: The paper includes thorough experiments, including feature ablation studies and comparisons with existing systems like YodaQA.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999995827674866,
                    "sentence": "This provides valuable insights into the system's strengths and limitations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999993443489075,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999993443489075,
                    "sentence": "1. Limited Error Analysis: The paper lacks a detailed analysis of errors, particularly the significant performance drop when transitioning from paragraph-level comprehension to full Wikipedia QA.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992847442627,
                    "sentence": "For example, while the Document Retriever achieves 77.8% retrieval accuracy, the overall system's performance on SQuAD drops to 26.7%, which warrants deeper investigation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999135732650757,
                    "sentence": "2. Training-Testing Inconsistency: The performance drop when using fetched articles instead of the best paragraph suggests potential issues with the training process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999021291732788,
                    "sentence": "The authors should explore ways to better align training and testing conditions, such as end-to-end training of the retrieval and comprehension modules.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999582767486572,
                    "sentence": "3. Missed Opportunities for External Knowledge: While the paper focuses on Wikipedia as the sole knowledge source, incorporating external resources like Freebase for entity typing could address errors related to topical relevance and improve overall accuracy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998891353607178,
                    "sentence": "4. Lack of Comparison to Related Work: The paper does not reference some relevant prior work, such as QuASE (Sun et al., 2015), which also tackles open-domain QA using fetched passages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991344809532166,
                    "sentence": "Including such comparisons would strengthen the positioning of the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9811674952507019,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9845656156539917,
                    "sentence": "1. Can the authors provide a more detailed error analysis to explain the large performance drop between paragraph-level comprehension and full Wikipedia QA?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9567692279815674,
                    "sentence": "What types of questions or errors are most challenging for the system?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9643036127090454,
                    "sentence": "2. Have the authors considered end-to-end training of the Document Retriever and Document Reader modules?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9541080594062805,
                    "sentence": "If so, what challenges were encountered?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9692806005477905,
                    "sentence": "3. Could external resources like Freebase or DBpedia be incorporated to improve entity typing and reduce errors related to topical relevance?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9460188746452332,
                    "sentence": "If not, why was this design choice made?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9198102951049805,
                    "sentence": "Recommendation",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9124639630317688,
                    "sentence": "The paper addresses an important problem and presents a strong system with state-of-the-art results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9149131178855896,
                    "sentence": "However, the lack of detailed error analysis and limited exploration of training-testing consistency are notable weaknesses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7460881471633911,
                    "sentence": "I recommend acceptance with minor revisions, provided the authors address these concerns and clarify the questions raised.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.45887534985363754
                }
            ],
            "completely_generated_prob": 0.8817996762583483,
            "class_probabilities": {
                "human": 0.11138460808424448,
                "ai": 0.8817996762583483,
                "mixed": 0.006815715657407152
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8817996762583483,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8817996762583483,
                    "human": 0.11138460808424448,
                    "mixed": 0.006815715657407152
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Paper\nSummary and Contributions\nThis paper tackles the challenging task of open-domain question answering (QA) using Wikipedia as the sole knowledge source. The authors propose a system, DrWiki, which integrates a document retrieval module and a machine comprehension module. The system achieves state-of-the-art results on the SQuAD dataset and demonstrates competitive performance across multiple QA benchmarks. The primary contributions of the paper are:\n1. State-of-the-Art Results on SQuAD: The proposed Document Reader achieves state-of-the-art performance on the SQuAD dataset, showcasing the efficacy of the model's feature-rich representation and multi-layer bi-directional LSTM architecture.\n2. Comprehensive Evaluation: The paper evaluates the system across multiple QA datasets (SQuAD, CuratedTREC, WebQuestions, and WikiMovies), highlighting the generalizability of the approach.\n3. Multitask Learning and Distant Supervision: The authors demonstrate the benefits of multitask learning and distant supervision in improving performance on open-domain QA tasks, particularly when training on multiple datasets.\nStrengths\n1. Novel Problem Setting: The paper addresses a critical and underexplored problem\"\"answering open-domain questions using only Wikipedia, without relying on external knowledge bases or redundant resources. This focus on a single knowledge source is both challenging and impactful.\n2. Strong Empirical Results: The system achieves state-of-the-art results on SQuAD and performs competitively on other datasets, demonstrating the robustness of the approach.\n3. Comprehensive System Design: The modular design of DrWiki, with a well-defined Document Retriever and Document Reader, is clear and effective. The use of bigram hashing and TF-IDF for retrieval, combined with neural network-based comprehension, is a strong combination.\n4. Detailed Evaluation: The paper includes thorough experiments, including feature ablation studies and comparisons with existing systems like YodaQA. This provides valuable insights into the system's strengths and limitations.\nWeaknesses\n1. Limited Error Analysis: The paper lacks a detailed analysis of errors, particularly the significant performance drop when transitioning from paragraph-level comprehension to full Wikipedia QA. For example, while the Document Retriever achieves 77.8% retrieval accuracy, the overall system's performance on SQuAD drops to 26.7%, which warrants deeper investigation.\n2. Training-Testing Inconsistency: The performance drop when using fetched articles instead of the best paragraph suggests potential issues with the training process. The authors should explore ways to better align training and testing conditions, such as end-to-end training of the retrieval and comprehension modules.\n3. Missed Opportunities for External Knowledge: While the paper focuses on Wikipedia as the sole knowledge source, incorporating external resources like Freebase for entity typing could address errors related to topical relevance and improve overall accuracy.\n4. Lack of Comparison to Related Work: The paper does not reference some relevant prior work, such as QuASE (Sun et al., 2015), which also tackles open-domain QA using fetched passages. Including such comparisons would strengthen the positioning of the proposed approach.\nQuestions to Authors\n1. Can the authors provide a more detailed error analysis to explain the large performance drop between paragraph-level comprehension and full Wikipedia QA? What types of questions or errors are most challenging for the system?\n2. Have the authors considered end-to-end training of the Document Retriever and Document Reader modules? If so, what challenges were encountered?\n3. Could external resources like Freebase or DBpedia be incorporated to improve entity typing and reduce errors related to topical relevance? If not, why was this design choice made?\nRecommendation\nThe paper addresses an important problem and presents a strong system with state-of-the-art results. However, the lack of detailed error analysis and limited exploration of training-testing consistency are notable weaknesses. I recommend acceptance with minor revisions, provided the authors address these concerns and clarify the questions raised."
        }
    ]
}