{
    "version": "2025-01-09-base",
    "scanId": "98b0ae6a-3cb6-433f-80fc-c29693ede2d3",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999735355377197,
                    "sentence": "Review of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999935030937195,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999964833259583,
                    "sentence": "This paper investigates the use of discourse structure, as defined by Rhetorical Structure Theory (RST), for text categorization tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973773956299,
                    "sentence": "The authors propose a recursive neural network model that incorporates a novel attention mechanism to weight the importance of discourse units (EDUs) based on their positions and relations in a discourse dependency tree.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969005584717,
                    "sentence": "The key contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969601631165,
                    "sentence": "1. Empirical Demonstration of Discourse Utility: The paper demonstrates that discourse structure, even when derived from an imperfect parser, can improve performance on text categorization tasks across multiple datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973773956299,
                    "sentence": "Notably, the \"UNLABELED\" model, which uses only the tree structure without relation labels, outperforms the \"FULL\" model on most datasets, raising questions about the utility of relation labeling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999964237213135,
                    "sentence": "2. Novel Attention Mechanism: The authors introduce an unnormalized attention mechanism tailored to the hierarchical nature of discourse trees.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955296516418,
                    "sentence": "This mechanism avoids competition among sibling nodes, aligning with the theoretical underpinnings of RST.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999959468841553,
                    "sentence": "3. Comprehensive Evaluation: The study evaluates the proposed models on five datasets spanning diverse tasks and genres, providing insights into the strengths and limitations of discourse-informed models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999820590019226,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999711513519287,
                    "sentence": "1. Strong Empirical Results: The \"UNLABELED\" model achieves state-of-the-art performance on four out of five datasets, demonstrating the practical utility of discourse structure for text categorization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999790787696838,
                    "sentence": "The results also highlight the robustness of the approach across genres like reviews, debates, and news articles.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999892115592957,
                    "sentence": "2. Theoretical Motivation: The unnormalized attention mechanism is well-motivated by the properties of RST, distinguishing it from prior work in machine translation and syntactic parsing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999836087226868,
                    "sentence": "This design decision is empirically validated, as the normalized attention variant underperforms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999959468841553,
                    "sentence": "3. Analysis of Discourse Parsing Quality: The authors explore the impact of discourse parser quality on model performance, showing that improvements in parsing accuracy could lead to further gains in text categorization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999947547912598,
                    "sentence": "This analysis is a valuable contribution to the discourse parsing and text categorization communities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999952912330627,
                    "sentence": "4. Comparison with Simpler Baselines: The inclusion of simpler baselines (e.g., \"ROOT\" and \"ADDITIVE\") strengthens the paper by isolating the benefits of discourse structure and the proposed attention mechanism.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999858140945435,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999988853931427,
                    "sentence": "1. Limited Dataset Scope: While the paper evaluates its models on five datasets, some of these (e.g., legislative bills) are less commonly used in the broader NLP community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999605655670166,
                    "sentence": "Evaluating on more widely-used datasets, such as IMDB or Amazon reviews, would facilitate better comparison with prior work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999338388442993,
                    "sentence": "2. Overparameterization of the FULL Model: The \"FULL\" model underperforms on smaller datasets, likely due to overparameterization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999005794525146,
                    "sentence": "This raises concerns about the scalability and generalizability of the approach to low-resource settings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998805522918701,
                    "sentence": "3. Loss of Original RST Tree Structure: The transformation of RST trees into dependency structures may discard valuable hierarchical information.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997797608375549,
                    "sentence": "Retaining the original tree structure and training a hierarchical model could potentially yield better results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994160532951355,
                    "sentence": "4. Genre-Specific Limitations: The approach struggles with legislative bills, a genre with discourse conventions that diverge from the training data of the discourse parser.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996810555458069,
                    "sentence": "This highlights the need for genre-specific adaptations or more diverse training data for discourse parsing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9804345965385437,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9849437475204468,
                    "sentence": "1. Could you provide insights into why the \"UNLABELED\" model outperforms the \"FULL\" model on most datasets?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9720950126647949,
                    "sentence": "Does this suggest that relation labels are noisy or less informative for text categorization?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9914960861206055,
                    "sentence": "2. Have you considered retaining the original RST tree structure instead of converting it to a dependency structure?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9847964644432068,
                    "sentence": "If so, what challenges did you encounter?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9733902812004089,
                    "sentence": "3. Would evaluating the model on more commonly used datasets (e.g., IMDB, Amazon reviews) yield similar improvements?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9659218192100525,
                    "sentence": "Are there plans to expand the evaluation?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9813286066055298,
                    "sentence": "Recommendation",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9629071950912476,
                    "sentence": "This paper makes significant contributions to the use of discourse structure in text categorization and introduces a well-motivated attention mechanism.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9057057499885559,
                    "sentence": "However, the limited dataset scope and the underperformance of the \"FULL\" model on smaller datasets suggest areas for improvement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8532229065895081,
                    "sentence": "I recommend acceptance, provided the authors address the evaluation and modeling concerns.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.8753704990852369,
            "class_probabilities": {
                "human": 0.11057250597192597,
                "ai": 0.8753704990852369,
                "mixed": 0.014056994942837287
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8753704990852369,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8753704990852369,
                    "human": 0.11057250597192597,
                    "mixed": 0.014056994942837287
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Paper\nSummary and Contributions\nThis paper investigates the use of discourse structure, as defined by Rhetorical Structure Theory (RST), for text categorization tasks. The authors propose a recursive neural network model that incorporates a novel attention mechanism to weight the importance of discourse units (EDUs) based on their positions and relations in a discourse dependency tree. The key contributions of this work are:\n1. Empirical Demonstration of Discourse Utility: The paper demonstrates that discourse structure, even when derived from an imperfect parser, can improve performance on text categorization tasks across multiple datasets. Notably, the \"UNLABELED\" model, which uses only the tree structure without relation labels, outperforms the \"FULL\" model on most datasets, raising questions about the utility of relation labeling.\n \n2. Novel Attention Mechanism: The authors introduce an unnormalized attention mechanism tailored to the hierarchical nature of discourse trees. This mechanism avoids competition among sibling nodes, aligning with the theoretical underpinnings of RST.\n3. Comprehensive Evaluation: The study evaluates the proposed models on five datasets spanning diverse tasks and genres, providing insights into the strengths and limitations of discourse-informed models.\nStrengths\n1. Strong Empirical Results: The \"UNLABELED\" model achieves state-of-the-art performance on four out of five datasets, demonstrating the practical utility of discourse structure for text categorization. The results also highlight the robustness of the approach across genres like reviews, debates, and news articles.\n2. Theoretical Motivation: The unnormalized attention mechanism is well-motivated by the properties of RST, distinguishing it from prior work in machine translation and syntactic parsing. This design decision is empirically validated, as the normalized attention variant underperforms.\n3. Analysis of Discourse Parsing Quality: The authors explore the impact of discourse parser quality on model performance, showing that improvements in parsing accuracy could lead to further gains in text categorization. This analysis is a valuable contribution to the discourse parsing and text categorization communities.\n4. Comparison with Simpler Baselines: The inclusion of simpler baselines (e.g., \"ROOT\" and \"ADDITIVE\") strengthens the paper by isolating the benefits of discourse structure and the proposed attention mechanism.\nWeaknesses\n1. Limited Dataset Scope: While the paper evaluates its models on five datasets, some of these (e.g., legislative bills) are less commonly used in the broader NLP community. Evaluating on more widely-used datasets, such as IMDB or Amazon reviews, would facilitate better comparison with prior work.\n2. Overparameterization of the FULL Model: The \"FULL\" model underperforms on smaller datasets, likely due to overparameterization. This raises concerns about the scalability and generalizability of the approach to low-resource settings.\n3. Loss of Original RST Tree Structure: The transformation of RST trees into dependency structures may discard valuable hierarchical information. Retaining the original tree structure and training a hierarchical model could potentially yield better results.\n4. Genre-Specific Limitations: The approach struggles with legislative bills, a genre with discourse conventions that diverge from the training data of the discourse parser. This highlights the need for genre-specific adaptations or more diverse training data for discourse parsing.\nQuestions to Authors\n1. Could you provide insights into why the \"UNLABELED\" model outperforms the \"FULL\" model on most datasets? Does this suggest that relation labels are noisy or less informative for text categorization?\n2. Have you considered retaining the original RST tree structure instead of converting it to a dependency structure? If so, what challenges did you encounter?\n3. Would evaluating the model on more commonly used datasets (e.g., IMDB, Amazon reviews) yield similar improvements? Are there plans to expand the evaluation?\nRecommendation\nThis paper makes significant contributions to the use of discourse structure in text categorization and introduces a well-motivated attention mechanism. However, the limited dataset scope and the underperformance of the \"FULL\" model on smaller datasets suggest areas for improvement. I recommend acceptance, provided the authors address the evaluation and modeling concerns."
        }
    ]
}