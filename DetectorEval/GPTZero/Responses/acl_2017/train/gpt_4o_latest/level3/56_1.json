{
    "version": "2025-01-09-base",
    "scanId": "8b35fa61-4ca8-43d2-8d28-24e59f9f3727",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999785423278809,
                    "sentence": "Review of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999982714653015,
                    "sentence": "Summary and Contributions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "This paper extends popular word representation models, such as SGNS, GloVe, PPMI, and SVD, to incorporate n-gram-based co-occurrence statistics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999980330467224,
                    "sentence": "The authors propose a novel method for efficiently computing n-gram embeddings, which reduces the computational burden associated with n-gram co-occurrence matrices.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973773956299,
                    "sentence": "The paper demonstrates the effectiveness of the proposed n-gram embeddings through experiments on word similarity and analogy tasks, showing significant improvements over baseline methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961256980896,
                    "sentence": "Additionally, qualitative evaluations reveal that the embeddings capture semantic meanings and syntactic patterns of n-grams, making them potentially useful for various downstream NLP tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999882578849792,
                    "sentence": "The primary contributions of the paper are as follows:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999945759773254,
                    "sentence": "1. Extension of Word Representation Models to N-grams: The paper introduces n-grams into four widely used word representation methods, demonstrating their utility in capturing richer semantic and syntactic information.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994158744812,
                    "sentence": "2. Efficient Co-occurrence Matrix Construction: A novel algorithm is proposed to reduce the hardware and computational costs of building n-gram co-occurrence matrices, enabling scalability even on resource-constrained systems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999915957450867,
                    "sentence": "3. Empirical Validation: The paper provides strong experimental evidence of the effectiveness of n-gram embeddings on similarity and analogy tasks, achieving notable improvements over baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940991401672,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999942183494568,
                    "sentence": "1. Innovative Extension to N-grams: The extension of existing word representation methods to incorporate n-gram statistics is a significant and intuitive idea that builds on well-established models while addressing their limitations in capturing multi-word contexts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999879598617554,
                    "sentence": "2. Strong Experimental Results: The proposed n-gram embeddings achieve substantial improvements on analogy and similarity benchmarks, particularly in semantic tasks, demonstrating the practical value of the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999994158744812,
                    "sentence": "3. Efficient Implementation: The proposed method for constructing n-gram co-occurrence matrices is both novel and practical, reducing computational overhead and making the approach accessible to researchers with limited hardware resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999924302101135,
                    "sentence": "4. Qualitative Analysis: The qualitative evaluations provide compelling evidence that the embeddings capture meaningful semantic and syntactic patterns, such as antonyms, passive voice, and phrasal verbs, which are valuable for downstream tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999922513961792,
                    "sentence": "5. Open Source Contribution: The release of the ngram2vec toolkit enhances the reproducibility and accessibility of the research, fostering further exploration and adoption by the community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999783635139465,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999898672103882,
                    "sentence": "1. Lack of Real-World Task Evaluation: While the paper demonstrates strong results on intrinsic tasks (e.g., similarity and analogy), it does not evaluate the proposed embeddings on real-world downstream tasks such as text classification, sentiment analysis, or machine translation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999939203262329,
                    "sentence": "This limits the practical applicability and generalizability of the findings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999943375587463,
                    "sentence": "2. Limited Exploration of Higher-Order N-grams: The paper focuses primarily on uni-grams and bi-grams, with little discussion or experimentation on higher-order n-grams.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999873638153076,
                    "sentence": "While the authors cite sparsity as a potential issue, this aspect could have been explored further.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999867081642151,
                    "sentence": "3. Hyperparameter Sensitivity: The paper mentions that default hyperparameter settings were used, which may not be optimal for n-gram-based models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999819993972778,
                    "sentence": "A more thorough exploration of hyperparameter tuning could provide additional insights into the robustness of the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995617866516113,
                    "sentence": "Questions to Authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995925426483154,
                    "sentence": "1. Have you considered evaluating the proposed n-gram embeddings on downstream tasks, such as text classification or question answering?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997743964195251,
                    "sentence": "If not, could you provide insights into potential challenges or limitations in applying these embeddings to such tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996776580810547,
                    "sentence": "2. How do the embeddings perform when incorporating higher-order n-grams (e.g., tri-grams or beyond)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998044967651367,
                    "sentence": "Are there specific strategies to mitigate sparsity issues for higher-order n-grams?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999313235282898,
                    "sentence": "3. Could the proposed co-occurrence matrix construction method be extended to other types of embeddings, such as contextualized embeddings (e.g., BERT)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991559386253357,
                    "sentence": "Final Recommendation:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979512095451355,
                    "sentence": "Despite the lack of evaluation on real-world tasks, the paper makes a valuable contribution to the field by extending word representation methods to n-grams and proposing an efficient computational framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982362985610962,
                    "sentence": "The strong results on intrinsic tasks and the release of the ngram2vec toolkit make this work a significant step forward.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9978039860725403,
                    "sentence": "I recommend acceptance with the suggestion to include real-world task evaluations in future work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9997862822885396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997862822885396,
                "mixed": 0.00021371771146045916
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997862822885396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997862822885396,
                    "human": 0,
                    "mixed": 0.00021371771146045916
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Paper\nSummary and Contributions:\nThis paper extends popular word representation models, such as SGNS, GloVe, PPMI, and SVD, to incorporate n-gram-based co-occurrence statistics. The authors propose a novel method for efficiently computing n-gram embeddings, which reduces the computational burden associated with n-gram co-occurrence matrices. The paper demonstrates the effectiveness of the proposed n-gram embeddings through experiments on word similarity and analogy tasks, showing significant improvements over baseline methods. Additionally, qualitative evaluations reveal that the embeddings capture semantic meanings and syntactic patterns of n-grams, making them potentially useful for various downstream NLP tasks.\nThe primary contributions of the paper are as follows:\n1. Extension of Word Representation Models to N-grams: The paper introduces n-grams into four widely used word representation methods, demonstrating their utility in capturing richer semantic and syntactic information.\n2. Efficient Co-occurrence Matrix Construction: A novel algorithm is proposed to reduce the hardware and computational costs of building n-gram co-occurrence matrices, enabling scalability even on resource-constrained systems.\n3. Empirical Validation: The paper provides strong experimental evidence of the effectiveness of n-gram embeddings on similarity and analogy tasks, achieving notable improvements over baselines.\nStrengths:\n1. Innovative Extension to N-grams: The extension of existing word representation methods to incorporate n-gram statistics is a significant and intuitive idea that builds on well-established models while addressing their limitations in capturing multi-word contexts.\n2. Strong Experimental Results: The proposed n-gram embeddings achieve substantial improvements on analogy and similarity benchmarks, particularly in semantic tasks, demonstrating the practical value of the approach.\n3. Efficient Implementation: The proposed method for constructing n-gram co-occurrence matrices is both novel and practical, reducing computational overhead and making the approach accessible to researchers with limited hardware resources.\n4. Qualitative Analysis: The qualitative evaluations provide compelling evidence that the embeddings capture meaningful semantic and syntactic patterns, such as antonyms, passive voice, and phrasal verbs, which are valuable for downstream tasks.\n5. Open Source Contribution: The release of the ngram2vec toolkit enhances the reproducibility and accessibility of the research, fostering further exploration and adoption by the community.\nWeaknesses:\n1. Lack of Real-World Task Evaluation: While the paper demonstrates strong results on intrinsic tasks (e.g., similarity and analogy), it does not evaluate the proposed embeddings on real-world downstream tasks such as text classification, sentiment analysis, or machine translation. This limits the practical applicability and generalizability of the findings.\n2. Limited Exploration of Higher-Order N-grams: The paper focuses primarily on uni-grams and bi-grams, with little discussion or experimentation on higher-order n-grams. While the authors cite sparsity as a potential issue, this aspect could have been explored further.\n3. Hyperparameter Sensitivity: The paper mentions that default hyperparameter settings were used, which may not be optimal for n-gram-based models. A more thorough exploration of hyperparameter tuning could provide additional insights into the robustness of the proposed approach.\nQuestions to Authors:\n1. Have you considered evaluating the proposed n-gram embeddings on downstream tasks, such as text classification or question answering? If not, could you provide insights into potential challenges or limitations in applying these embeddings to such tasks?\n2. How do the embeddings perform when incorporating higher-order n-grams (e.g., tri-grams or beyond)? Are there specific strategies to mitigate sparsity issues for higher-order n-grams?\n3. Could the proposed co-occurrence matrix construction method be extended to other types of embeddings, such as contextualized embeddings (e.g., BERT)?\nFinal Recommendation:\nDespite the lack of evaluation on real-world tasks, the paper makes a valuable contribution to the field by extending word representation methods to n-grams and proposing an efficient computational framework. The strong results on intrinsic tasks and the release of the ngram2vec toolkit make this work a significant step forward. I recommend acceptance with the suggestion to include real-world task evaluations in future work."
        }
    ]
}