{
    "version": "2025-01-09-base",
    "scanId": "2947703d-d7f9-4d15-9340-6a1fb6fdaa7f",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9986225366592407,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999345541000366,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998161792755127,
                    "sentence": "This paper proposes an innovative approach to sentence-level sentiment classification by integrating linguistic knowledge\"\"sentiment lexicons, negation words, and intensity words\"\"into Long Short-Term Memory (LSTM) models via linguistic-inspired regularizers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996347427368164,
                    "sentence": "The primary contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988670349121094,
                    "sentence": "1. Linguistic Regularization: The introduction of linguistic regularizers (non-sentiment, sentiment, negation, and intensity) to model the linguistic role of sentiment-related words in a mathematically principled way.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993685483932495,
                    "sentence": "2. Efficiency and Simplicity: The proposed models avoid the need for expensive phrase-level annotations and parsing tree structures, offering a simpler and more efficient alternative while achieving competitive performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989560842514038,
                    "sentence": "3. Empirical Validation: The paper provides a thorough experimental evaluation, demonstrating the effectiveness of the regularizers in capturing linguistic phenomena such as sentiment intensification and negation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997393488883972,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989734888076782,
                    "sentence": "1. Innovative Regularization Framework: The idea of incorporating linguistic knowledge through regularization is novel and addresses a critical gap in sentiment classification by leveraging linguistic resources effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988287091255188,
                    "sentence": "2. Technically Sound Experiments: The experiments are well-designed, with ablation studies and subset evaluations (e.g., negation and intensity subsets) providing strong evidence for the utility of the proposed regularizers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979841113090515,
                    "sentence": "3. In-depth Analysis: The paper provides detailed insights into the model's behavior, such as how negation and intensity words shift sentiment distributions, supported by visualizations and quantitative results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992150068283081,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9981802105903625,
                    "sentence": "1. Similarity to Distant Supervision: The proposed approach closely resembles distant supervision, raising concerns about the novelty of the method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983718991279602,
                    "sentence": "Alternative methods for integrating lexical information (e.g., prior linguistic regularization attempts like [YOG14]) are overlooked in the related work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9971852898597717,
                    "sentence": "2. Weak Baselines: The baselines used for comparison lack lexical information, making them less competitive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998237133026123,
                    "sentence": "A vanilla LSTM with appended lexical features would have been a stronger baseline for fairer comparisons.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991108775138855,
                    "sentence": "3. Ambiguity and Repetition: The explanation of the regularizers is overly lengthy, repetitive, and suffers from inconsistent notation (e.g., \"position\" t and overloaded usage of p_t).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985464811325073,
                    "sentence": "This detracts from the clarity of the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985300302505493,
                    "sentence": "4. Fairness in Comparisons: The comparison with the Neural Context-Sensitive Lexicon (NCSL) model raises fairness concerns, particularly regarding the use of lexicons and dataset configurations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9884779453277588,
                    "sentence": "5. Handling of OOV Words: The paper does not adequately address how the model handles out-of-vocabulary (OOV) words, which is a critical issue in real-world applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8456363081932068,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8380276560783386,
                    "sentence": "1. How does the proposed model handle OOV words, especially when they are sentiment, negation, or intensity words?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7562816143035889,
                    "sentence": "2. Could you clarify the distinction between your approach and distant supervision?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8616696000099182,
                    "sentence": "Are there specific advantages of your method over existing distant supervision techniques?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8569579124450684,
                    "sentence": "3. Why were baselines without lexical information chosen, and how would the results compare to a vanilla LSTM with appended lexical features?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8560745716094971,
                    "sentence": "Overall Assessment",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8640085458755493,
                    "sentence": "The paper presents a reasonable research direction with an innovative idea of linguistic regularization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8819668292999268,
                    "sentence": "However, the approach's novelty is somewhat diminished due to its resemblance to distant supervision, and the experimental comparisons are weakened by the choice of baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9509178400039673,
                    "sentence": "While the model demonstrates the ability to capture linguistic phenomena like intensification and negation, the lack of clarity on handling OOV words and the absence of significance tests for marginal improvements limit the paper's impact.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9321468472480774,
                    "sentence": "Addressing these issues and conducting additional experiments (e.g., on OOV words) would significantly enhance the paper's contribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.976751983165741,
                    "sentence": "Recommendation: Weak Accept.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.8119739381064363,
            "class_probabilities": {
                "human": 0.18679001347792185,
                "ai": 0.8119739381064363,
                "mixed": 0.0012360484156418805
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8119739381064363,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8119739381064363,
                    "human": 0.18679001347792185,
                    "mixed": 0.0012360484156418805
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary and Contributions\nThis paper proposes an innovative approach to sentence-level sentiment classification by integrating linguistic knowledge\"\"sentiment lexicons, negation words, and intensity words\"\"into Long Short-Term Memory (LSTM) models via linguistic-inspired regularizers. The primary contributions of this work are: \n1. Linguistic Regularization: The introduction of linguistic regularizers (non-sentiment, sentiment, negation, and intensity) to model the linguistic role of sentiment-related words in a mathematically principled way. \n2. Efficiency and Simplicity: The proposed models avoid the need for expensive phrase-level annotations and parsing tree structures, offering a simpler and more efficient alternative while achieving competitive performance. \n3. Empirical Validation: The paper provides a thorough experimental evaluation, demonstrating the effectiveness of the regularizers in capturing linguistic phenomena such as sentiment intensification and negation. \nStrengths\n1. Innovative Regularization Framework: The idea of incorporating linguistic knowledge through regularization is novel and addresses a critical gap in sentiment classification by leveraging linguistic resources effectively. \n2. Technically Sound Experiments: The experiments are well-designed, with ablation studies and subset evaluations (e.g., negation and intensity subsets) providing strong evidence for the utility of the proposed regularizers. \n3. In-depth Analysis: The paper provides detailed insights into the model's behavior, such as how negation and intensity words shift sentiment distributions, supported by visualizations and quantitative results. \nWeaknesses\n1. Similarity to Distant Supervision: The proposed approach closely resembles distant supervision, raising concerns about the novelty of the method. Alternative methods for integrating lexical information (e.g., prior linguistic regularization attempts like [YOG14]) are overlooked in the related work. \n2. Weak Baselines: The baselines used for comparison lack lexical information, making them less competitive. A vanilla LSTM with appended lexical features would have been a stronger baseline for fairer comparisons. \n3. Ambiguity and Repetition: The explanation of the regularizers is overly lengthy, repetitive, and suffers from inconsistent notation (e.g., \"position\" t and overloaded usage of p_t). This detracts from the clarity of the paper. \n4. Fairness in Comparisons: The comparison with the Neural Context-Sensitive Lexicon (NCSL) model raises fairness concerns, particularly regarding the use of lexicons and dataset configurations. \n5. Handling of OOV Words: The paper does not adequately address how the model handles out-of-vocabulary (OOV) words, which is a critical issue in real-world applications. \nQuestions to Authors\n1. How does the proposed model handle OOV words, especially when they are sentiment, negation, or intensity words? \n2. Could you clarify the distinction between your approach and distant supervision? Are there specific advantages of your method over existing distant supervision techniques? \n3. Why were baselines without lexical information chosen, and how would the results compare to a vanilla LSTM with appended lexical features? \nOverall Assessment\nThe paper presents a reasonable research direction with an innovative idea of linguistic regularization. However, the approach's novelty is somewhat diminished due to its resemblance to distant supervision, and the experimental comparisons are weakened by the choice of baselines. While the model demonstrates the ability to capture linguistic phenomena like intensification and negation, the lack of clarity on handling OOV words and the absence of significance tests for marginal improvements limit the paper's impact. Addressing these issues and conducting additional experiments (e.g., on OOV words) would significantly enhance the paper's contribution. \nRecommendation: Weak Accept."
        }
    ]
}