{
    "version": "2025-01-09-base",
    "scanId": "15a0f3cd-69a7-4129-a74e-06b073d2ced9",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999974370002747,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999999463558197,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999993443489075,
                    "sentence": "This paper presents a novel neural encoder-decoder transition-based parser for full-coverage semantic graph parsing of Minimal Recursion Semantics (MRS).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999990463256836,
                    "sentence": "The parser is notable for its ability to predict linguistically deep semantic graphs incrementally, without relying on the underlying English Resource Grammar (ERG) or syntactic structures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992251396179,
                    "sentence": "The authors propose a stack-based model with hard attention and demonstrate its effectiveness on MRS-derived representations (DMRS and EDS) as well as Abstract Meaning Representation (AMR).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991059303284,
                    "sentence": "The parser achieves state-of-the-art performance on MRS parsing and competitive results on AMR parsing, while being significantly faster than grammar-based parsers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979138374329,
                    "sentence": "The main contributions of the paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983310699463,
                    "sentence": "1. A transition-based parser for MRS that outperforms attention-based baselines and achieves an 86.69% Smatch score, surpassing the upper bound for AMR parsing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999980926513672,
                    "sentence": "2. A stack-based architecture that incorporates hard attention and improves parsing accuracy while enabling GPU batch processing for significant speed gains.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986886978149,
                    "sentence": "3. A demonstration of the parser's generalizability by applying it to AMR parsing and achieving competitive results without relying on extensive external resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971985816956,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983310699463,
                    "sentence": "1. Novelty and Significance: The paper addresses a gap in semantic parsing by developing the first robust, full-coverage parser for MRS, a linguistically expressive representation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999985098838806,
                    "sentence": "The work is novel and provides a significant improvement over existing approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999982118606567,
                    "sentence": "2. Performance and Efficiency: The parser achieves competitive accuracy while being an order of magnitude faster than grammar-based parsers like ACE.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986886978149,
                    "sentence": "This makes it highly practical for real-world applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999988675117493,
                    "sentence": "3. Generalizability: The model's ability to generalize to AMR parsing demonstrates its versatility and potential for broader adoption in semantic parsing tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998152256012,
                    "sentence": "4. Thorough Evaluation: The authors provide extensive experiments and comparisons, including ablation studies, to validate their claims.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979138374329,
                    "sentence": "The results are robust and well-supported by metrics like EDM and Smatch.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "5. Reproducibility: The paper provides sufficient implementation details, including model architectures, training setups, and datasets, which enhances reproducibility.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999980330467224,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986886978149,
                    "sentence": "1. Limited AMR Performance: While the parser performs well on MRS, its performance on AMR parsing lags behind state-of-the-art parsers that leverage external resources like syntax trees and semantic role labeling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999985694885254,
                    "sentence": "This limits its competitiveness in AMR-specific tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996989369392395,
                    "sentence": "2. Sparse Discussion of Limitations: The paper does not sufficiently discuss the limitations of the proposed approach, such as its reliance on high-quality alignments for MRS parsing or its challenges in handling non-compositional representations like AMR.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996926188468933,
                    "sentence": "3. Interpretability of Results: The paper could benefit from a more detailed analysis of error cases, particularly for AMR parsing, to better understand the model's weaknesses and guide future improvements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998756051063538,
                    "sentence": "4. Scalability to Larger Datasets: While the parser is fast, the paper does not explore its scalability to larger or more diverse datasets, which could provide insights into its robustness in real-world scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.1003790870308876,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.254595547914505,
                    "sentence": "1. How does the parser handle ambiguous or noisy alignments in AMR parsing, and could this be a factor in its lower performance compared to state-of-the-art parsers?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.17406675219535828,
                    "sentence": "2. Could the proposed stack-based architecture be extended to incorporate external resources like syntax trees or semantic role labels to improve AMR parsing accuracy?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.3486000597476959,
                    "sentence": "3. What are the primary sources of error in the parser's predictions, and how might these be addressed in future work?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5457479357719421,
                    "sentence": "Recommendation",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.23750707507133484,
                    "sentence": "This paper makes a significant contribution to semantic parsing, particularly for MRS, and introduces a fast, robust, and generalizable parser.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.10056150704622269,
                    "sentence": "While there are some limitations in AMR parsing and the discussion of weaknesses, the strengths of the work outweigh these concerns.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.14385317265987396,
                    "sentence": "I recommend acceptance with minor revisions to address the interpretability and scalability aspects.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                }
            ],
            "completely_generated_prob": 0.958904109589041,
            "class_probabilities": {
                "human": 0,
                "ai": 0.958904109589041,
                "mixed": 0.041095890410958895
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.958904109589041,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.958904109589041,
                    "human": 0,
                    "mixed": 0.041095890410958895
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary and Contributions\nThis paper presents a novel neural encoder-decoder transition-based parser for full-coverage semantic graph parsing of Minimal Recursion Semantics (MRS). The parser is notable for its ability to predict linguistically deep semantic graphs incrementally, without relying on the underlying English Resource Grammar (ERG) or syntactic structures. The authors propose a stack-based model with hard attention and demonstrate its effectiveness on MRS-derived representations (DMRS and EDS) as well as Abstract Meaning Representation (AMR). The parser achieves state-of-the-art performance on MRS parsing and competitive results on AMR parsing, while being significantly faster than grammar-based parsers. The main contributions of the paper are:\n1. A transition-based parser for MRS that outperforms attention-based baselines and achieves an 86.69% Smatch score, surpassing the upper bound for AMR parsing.\n2. A stack-based architecture that incorporates hard attention and improves parsing accuracy while enabling GPU batch processing for significant speed gains.\n3. A demonstration of the parser's generalizability by applying it to AMR parsing and achieving competitive results without relying on extensive external resources.\nStrengths\n1. Novelty and Significance: The paper addresses a gap in semantic parsing by developing the first robust, full-coverage parser for MRS, a linguistically expressive representation. The work is novel and provides a significant improvement over existing approaches.\n2. Performance and Efficiency: The parser achieves competitive accuracy while being an order of magnitude faster than grammar-based parsers like ACE. This makes it highly practical for real-world applications.\n3. Generalizability: The model's ability to generalize to AMR parsing demonstrates its versatility and potential for broader adoption in semantic parsing tasks.\n4. Thorough Evaluation: The authors provide extensive experiments and comparisons, including ablation studies, to validate their claims. The results are robust and well-supported by metrics like EDM and Smatch.\n5. Reproducibility: The paper provides sufficient implementation details, including model architectures, training setups, and datasets, which enhances reproducibility.\nWeaknesses\n1. Limited AMR Performance: While the parser performs well on MRS, its performance on AMR parsing lags behind state-of-the-art parsers that leverage external resources like syntax trees and semantic role labeling. This limits its competitiveness in AMR-specific tasks.\n2. Sparse Discussion of Limitations: The paper does not sufficiently discuss the limitations of the proposed approach, such as its reliance on high-quality alignments for MRS parsing or its challenges in handling non-compositional representations like AMR.\n3. Interpretability of Results: The paper could benefit from a more detailed analysis of error cases, particularly for AMR parsing, to better understand the model's weaknesses and guide future improvements.\n4. Scalability to Larger Datasets: While the parser is fast, the paper does not explore its scalability to larger or more diverse datasets, which could provide insights into its robustness in real-world scenarios.\nQuestions to Authors\n1. How does the parser handle ambiguous or noisy alignments in AMR parsing, and could this be a factor in its lower performance compared to state-of-the-art parsers?\n2. Could the proposed stack-based architecture be extended to incorporate external resources like syntax trees or semantic role labels to improve AMR parsing accuracy?\n3. What are the primary sources of error in the parser's predictions, and how might these be addressed in future work?\nRecommendation\nThis paper makes a significant contribution to semantic parsing, particularly for MRS, and introduces a fast, robust, and generalizable parser. While there are some limitations in AMR parsing and the discussion of weaknesses, the strengths of the work outweigh these concerns. I recommend acceptance with minor revisions to address the interpretability and scalability aspects."
        }
    ]
}