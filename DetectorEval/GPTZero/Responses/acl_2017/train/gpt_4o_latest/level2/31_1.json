{
    "version": "2025-01-09-base",
    "scanId": "5fad1682-8088-453a-b91b-29c9122078e5",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999628067016602,
                    "sentence": "Review of the Submission",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999744892120361,
                    "sentence": "Summary and Contributions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999759793281555,
                    "sentence": "This paper addresses the task of event factuality identification, a critical component in various NLP applications such as opinion detection, question answering, and rumor identification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999972403049469,
                    "sentence": "The authors propose a two-step supervised framework that first extracts key factors (e.g., events, Source Introducing Predicates (SIPs), cues) and then uses an attention-based neural network combining BiLSTM and CNN to identify event factuality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999783039093018,
                    "sentence": "The paper claims three main contributions: (1) the introduction of a two-step framework for event factuality identification, (2) the use of an attention-based CNN for SIP detection, and (3) the development of an attention-based BiLSTM-CNN model for factuality classification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999679327011108,
                    "sentence": "The experimental results on the FactBank dataset demonstrate significant improvements over state-of-the-art baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999691247940063,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999343156814575,
                    "sentence": "1. Novel Architecture Design: The combination of BiLSTM and CNN with attention mechanisms is well-motivated and effectively leverages both syntactic paths and lexical features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998127222061157,
                    "sentence": "The two-output design for factuality classification is particularly innovative and addresses class imbalance issues effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999115467071533,
                    "sentence": "2. Comprehensive Evaluation: The paper provides a thorough evaluation on the FactBank dataset, including detailed analysis of the effects of different inputs and ablation studies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997873306274414,
                    "sentence": "The results convincingly demonstrate the superiority of the proposed model, particularly in handling speculative and negative factuality values.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998897910118103,
                    "sentence": "3. Attention Mechanism: The use of attention mechanisms to focus on critical factors (e.g., SIPs, cues) in syntactic paths is a strong methodological contribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999748706817627,
                    "sentence": "The experiments show that attention significantly improves performance over average pooling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999300241470337,
                    "sentence": "4. Reproducibility: The paper provides sufficient details about the architecture, hyperparameters, and experimental setup, making it easier for others to replicate the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999217987060547,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999571442604065,
                    "sentence": "1. Limited Discussion of Limitations: While the paper acknowledges challenges in identifying Uu events with embedded sources, it does not explore potential solutions or provide sufficient analysis of why the model struggles with these cases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999749660491943,
                    "sentence": "2. Dataset Dependency: The model heavily relies on the FactBank dataset, which is relatively small and domain-specific.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999969482421875,
                    "sentence": "The generalizability of the approach to other datasets or domains is not discussed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999765753746033,
                    "sentence": "3. Baseline Comparisons: Although the paper compares its model to several baselines, some comparisons (e.g., with more recent transformer-based models) are missing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999635219573975,
                    "sentence": "This omission limits the claim of state-of-the-art performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9774311184883118,
                    "sentence": "4. Interpretability: While the attention mechanism is a key component, the paper does not provide qualitative examples or visualizations of attention weights to demonstrate how the model focuses on important factors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.983374834060669,
                    "sentence": "Questions to Authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9903985857963562,
                    "sentence": "1. How does the model perform on datasets other than FactBank?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9941390752792358,
                    "sentence": "Can it generalize to other domains or languages?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.985243022441864,
                    "sentence": "2. Have you considered using transformer-based architectures (e.g., BERT) for this task?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9723363518714905,
                    "sentence": "How do you expect your model to compare?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9879655838012695,
                    "sentence": "3. Can you provide examples or visualizations of attention weights to illustrate how the model identifies critical factors like SIPs and cues?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9932785630226135,
                    "sentence": "Recommendation:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9722136855125427,
                    "sentence": "The paper presents a novel and well-executed approach to event factuality identification, with strong empirical results and methodological contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9368860721588135,
                    "sentence": "However, the lack of discussion on limitations and generalizability, as well as the absence of comparisons with transformer-based models, slightly weakens the impact.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8702831268310547,
                    "sentence": "I recommend acceptance, provided the authors address these concerns during the rebuttal phase.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.7995711047355243,
            "class_probabilities": {
                "human": 0.19427518060025914,
                "ai": 0.7995711047355243,
                "mixed": 0.006153714664216576
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.7995711047355243,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.7995711047355243,
                    "human": 0.19427518060025914,
                    "mixed": 0.006153714664216576
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Submission\nSummary and Contributions: \nThis paper addresses the task of event factuality identification, a critical component in various NLP applications such as opinion detection, question answering, and rumor identification. The authors propose a two-step supervised framework that first extracts key factors (e.g., events, Source Introducing Predicates (SIPs), cues) and then uses an attention-based neural network combining BiLSTM and CNN to identify event factuality. The paper claims three main contributions: (1) the introduction of a two-step framework for event factuality identification, (2) the use of an attention-based CNN for SIP detection, and (3) the development of an attention-based BiLSTM-CNN model for factuality classification. The experimental results on the FactBank dataset demonstrate significant improvements over state-of-the-art baselines.\nStrengths: \n1. Novel Architecture Design: The combination of BiLSTM and CNN with attention mechanisms is well-motivated and effectively leverages both syntactic paths and lexical features. The two-output design for factuality classification is particularly innovative and addresses class imbalance issues effectively. \n2. Comprehensive Evaluation: The paper provides a thorough evaluation on the FactBank dataset, including detailed analysis of the effects of different inputs and ablation studies. The results convincingly demonstrate the superiority of the proposed model, particularly in handling speculative and negative factuality values. \n3. Attention Mechanism: The use of attention mechanisms to focus on critical factors (e.g., SIPs, cues) in syntactic paths is a strong methodological contribution. The experiments show that attention significantly improves performance over average pooling. \n4. Reproducibility: The paper provides sufficient details about the architecture, hyperparameters, and experimental setup, making it easier for others to replicate the results. \nWeaknesses: \n1. Limited Discussion of Limitations: While the paper acknowledges challenges in identifying Uu events with embedded sources, it does not explore potential solutions or provide sufficient analysis of why the model struggles with these cases. \n2. Dataset Dependency: The model heavily relies on the FactBank dataset, which is relatively small and domain-specific. The generalizability of the approach to other datasets or domains is not discussed. \n3. Baseline Comparisons: Although the paper compares its model to several baselines, some comparisons (e.g., with more recent transformer-based models) are missing. This omission limits the claim of state-of-the-art performance. \n4. Interpretability: While the attention mechanism is a key component, the paper does not provide qualitative examples or visualizations of attention weights to demonstrate how the model focuses on important factors. \nQuestions to Authors: \n1. How does the model perform on datasets other than FactBank? Can it generalize to other domains or languages? \n2. Have you considered using transformer-based architectures (e.g., BERT) for this task? How do you expect your model to compare? \n3. Can you provide examples or visualizations of attention weights to illustrate how the model identifies critical factors like SIPs and cues? \nRecommendation: \nThe paper presents a novel and well-executed approach to event factuality identification, with strong empirical results and methodological contributions. However, the lack of discussion on limitations and generalizability, as well as the absence of comparisons with transformer-based models, slightly weakens the impact. I recommend acceptance, provided the authors address these concerns during the rebuttal phase."
        }
    ]
}