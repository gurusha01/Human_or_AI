{
    "version": "2025-01-09-base",
    "scanId": "6931a4e4-f918-426b-bed3-3cac69725281",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999982714653015,
                    "sentence": "Review of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999994039535522,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986886978149,
                    "sentence": "This paper presents a novel method for jointly embedding concepts, phrases, and words into a shared vector space using distant supervision from ontologies and unannotated corpora.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999990463256836,
                    "sentence": "The authors propose a model that leverages representative phrases for ontology concepts as supervision, eliminating the need for manual annotation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983310699463,
                    "sentence": "The embeddings are trained using a modified skip-gram architecture, incorporating hyperparameters to balance compositionality between words, phrases, and concepts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999985694885254,
                    "sentence": "The authors evaluate their embeddings on biomedical and general-domain similarity and relatedness tasks, demonstrating competitive performance with state-of-the-art methods that rely on human-annotated data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998927116394,
                    "sentence": "Additionally, the paper introduces a novel dataset for evaluating similarity and relatedness of real-world entities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "The primary contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999978542327881,
                    "sentence": "1. A method for jointly embedding concepts, phrases, and words without requiring manual annotations, achieving competitive performance on similarity and relatedness tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997615814209,
                    "sentence": "2. A scalable approach that significantly increases vocabulary coverage compared to prior methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977946281433,
                    "sentence": "3. A novel dataset for evaluating similarity and relatedness of real-world entities, which is made publicly available.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999987483024597,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999982714653015,
                    "sentence": "1. Novelty and Practicality: The proposed method introduces an innovative way to leverage distant supervision for embedding concepts, phrases, and words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974370002747,
                    "sentence": "This eliminates the reliance on expensive manual annotations, making the approach more scalable and practical for large datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "2. Comprehensive Evaluation: The authors evaluate their embeddings on multiple datasets, including both biomedical and general-domain tasks, providing evidence of the method's broad applicability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979138374329,
                    "sentence": "The inclusion of a new dataset for real-world entities is a valuable contribution to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974370002747,
                    "sentence": "3. Scalability: The method demonstrates more than 3x vocabulary coverage compared to prior approaches, addressing a significant limitation in existing embedding methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971389770508,
                    "sentence": "4. Reproducibility: The authors provide a software implementation (cui2vec) and make their dataset publicly available, supporting reproducibility and further research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999988675117493,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999980926513672,
                    "sentence": "1. Limited Downstream Applications: While the embeddings are evaluated on similarity and relatedness tasks, the paper does not provide evidence of their utility in downstream NLP applications such as entity linking or information retrieval.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977946281433,
                    "sentence": "This limits the practical impact of the proposed method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "2. Ontology Structure Underutilized: The method does not fully leverage the hierarchical or relational structure of the ontologies, which could potentially enhance the embeddings' semantic richness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999959468841553,
                    "sentence": "3. Hyperparameter Sensitivity: The performance of the method appears highly sensitive to hyperparameter tuning, as noted in the supplementary material.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999926686286926,
                    "sentence": "This may limit its ease of use for practitioners.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.730478048324585,
                    "sentence": "4. Limited Analysis of Limitations: While the authors acknowledge some limitations, such as the noisiness of distant supervision, a more detailed discussion of potential failure cases and how they might be addressed would strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9959248304367065,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983094334602356,
                    "sentence": "1. Have you evaluated the embeddings on any downstream tasks, such as entity linking or question answering?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995584487915039,
                    "sentence": "If not, do you plan to explore this in future work?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992048740386963,
                    "sentence": "2. How does the method handle polysemy or ambiguous phrases that map to multiple concepts?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992839097976685,
                    "sentence": "3. Could incorporating the hierarchical structure of ontologies (e.g., parent-child relationships) improve the embeddings?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992319941520691,
                    "sentence": "Conclusion",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982655048370361,
                    "sentence": "This paper presents a promising approach for embedding concepts, phrases, and words using distant supervision, achieving competitive performance without manual annotations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998928964138031,
                    "sentence": "While the method demonstrates strong potential, its practical impact would be enhanced by evaluations on downstream tasks and better utilization of ontology structures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990527033805847,
                    "sentence": "Nonetheless, the scalability and reproducibility of the approach make it a valuable contribution to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9903847575187683,
                    "sentence": "I recommend acceptance, provided the authors address the identified weaknesses and clarify the questions raised.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                }
            ],
            "completely_generated_prob": 0.9954476479514417,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9954476479514417,
                "mixed": 0.004552352048558421
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9954476479514417,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9954476479514417,
                    "human": 0,
                    "mixed": 0.004552352048558421
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Paper\nSummary and Contributions \nThis paper presents a novel method for jointly embedding concepts, phrases, and words into a shared vector space using distant supervision from ontologies and unannotated corpora. The authors propose a model that leverages representative phrases for ontology concepts as supervision, eliminating the need for manual annotation. The embeddings are trained using a modified skip-gram architecture, incorporating hyperparameters to balance compositionality between words, phrases, and concepts. The authors evaluate their embeddings on biomedical and general-domain similarity and relatedness tasks, demonstrating competitive performance with state-of-the-art methods that rely on human-annotated data. Additionally, the paper introduces a novel dataset for evaluating similarity and relatedness of real-world entities.\nThe primary contributions of this work are: \n1. A method for jointly embedding concepts, phrases, and words without requiring manual annotations, achieving competitive performance on similarity and relatedness tasks. \n2. A scalable approach that significantly increases vocabulary coverage compared to prior methods. \n3. A novel dataset for evaluating similarity and relatedness of real-world entities, which is made publicly available. \nStrengths \n1. Novelty and Practicality: The proposed method introduces an innovative way to leverage distant supervision for embedding concepts, phrases, and words. This eliminates the reliance on expensive manual annotations, making the approach more scalable and practical for large datasets. \n2. Comprehensive Evaluation: The authors evaluate their embeddings on multiple datasets, including both biomedical and general-domain tasks, providing evidence of the method's broad applicability. The inclusion of a new dataset for real-world entities is a valuable contribution to the field. \n3. Scalability: The method demonstrates more than 3x vocabulary coverage compared to prior approaches, addressing a significant limitation in existing embedding methods. \n4. Reproducibility: The authors provide a software implementation (cui2vec) and make their dataset publicly available, supporting reproducibility and further research. \nWeaknesses \n1. Limited Downstream Applications: While the embeddings are evaluated on similarity and relatedness tasks, the paper does not provide evidence of their utility in downstream NLP applications such as entity linking or information retrieval. This limits the practical impact of the proposed method. \n2. Ontology Structure Underutilized: The method does not fully leverage the hierarchical or relational structure of the ontologies, which could potentially enhance the embeddings' semantic richness. \n3. Hyperparameter Sensitivity: The performance of the method appears highly sensitive to hyperparameter tuning, as noted in the supplementary material. This may limit its ease of use for practitioners. \n4. Limited Analysis of Limitations: While the authors acknowledge some limitations, such as the noisiness of distant supervision, a more detailed discussion of potential failure cases and how they might be addressed would strengthen the paper. \nQuestions to Authors \n1. Have you evaluated the embeddings on any downstream tasks, such as entity linking or question answering? If not, do you plan to explore this in future work? \n2. How does the method handle polysemy or ambiguous phrases that map to multiple concepts? \n3. Could incorporating the hierarchical structure of ontologies (e.g., parent-child relationships) improve the embeddings? \nConclusion \nThis paper presents a promising approach for embedding concepts, phrases, and words using distant supervision, achieving competitive performance without manual annotations. While the method demonstrates strong potential, its practical impact would be enhanced by evaluations on downstream tasks and better utilization of ontology structures. Nonetheless, the scalability and reproducibility of the approach make it a valuable contribution to the field. I recommend acceptance, provided the authors address the identified weaknesses and clarify the questions raised."
        }
    ]
}