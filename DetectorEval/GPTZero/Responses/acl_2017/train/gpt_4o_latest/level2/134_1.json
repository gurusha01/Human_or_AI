{
    "version": "2025-01-09-base",
    "scanId": "74859f39-c891-4caf-bbff-5bfef97a1823",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9463323950767517,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9847497344017029,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7517721056938171,
                    "sentence": "This paper investigates neural approaches for end-to-end computational argumentation mining (AM), presenting several novel framings of the task: dependency parsing, sequence tagging, multi-task learning (MTL), and a hybrid model combining sequential and tree structure information (LSTM-ER).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5767813920974731,
                    "sentence": "The authors claim the following contributions: (1) the first neural end-to-end solutions for AM, (2) evidence that neural approaches outperform state-of-the-art feature-based ILP models, (3) a demonstration that token-based dependency parsing is ineffective for AM, (4) robust performance of a sequence tagging model encoding distance information, and (5) improved performance through multi-task learning with auxiliary subtasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8812903761863708,
                    "sentence": "The work is evaluated on the PE dataset, achieving new state-of-the-art results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9794446229934692,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997177124023438,
                    "sentence": "1. Comprehensive Exploration of Neural Framings: The paper systematically explores multiple neural approaches to AM, offering a detailed comparison of their strengths and weaknesses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999244213104248,
                    "sentence": "This breadth of experimentation provides valuable insights into the suitability of different framings for AM tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999772310256958,
                    "sentence": "2. State-of-the-Art Results: The proposed models achieve significant improvements over the ILP baseline, particularly for sequence tagging and multi-task learning setups.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999668002128601,
                    "sentence": "This demonstrates the practical utility of neural approaches in AM.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999762177467346,
                    "sentence": "3. Novel Multi-Task Learning Setup: The inclusion of auxiliary subtasks (e.g., component detection and relation classification) in the MTL framework is a notable contribution, showing measurable performance gains and providing a clear direction for future work in AM.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999815821647644,
                    "sentence": "4. Critical Analysis of Dependency Parsing: The authors provide a thorough analysis of why dependency parsing underperforms for AM, offering valuable lessons for researchers considering similar approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999534487724304,
                    "sentence": "5. Practical Recommendations: The paper offers actionable insights, such as the preference for simpler sequence tagging models for long documents and the decoupling of component and relation detection tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999983549118042,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999865889549255,
                    "sentence": "1. Limited Generalization Beyond Dataset: The experiments are conducted solely on the PE dataset, which may limit the generalizability of the findings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999446272850037,
                    "sentence": "The paper would benefit from evaluations on additional datasets with different argumentation structures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999269843101501,
                    "sentence": "2. Insufficient Error Analysis: While the authors discuss performance trends, a deeper qualitative error analysis (e.g., common failure cases for relation detection) would enhance understanding of the models' limitations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9904871582984924,
                    "sentence": "3. Lack of Encoder-Decoder Comparison: The paper mentions encoder-decoder models as a potential framing but does not include them in the experiments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9777994751930237,
                    "sentence": "This omission leaves a gap in the exploration of neural architectures for AM.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9743970632553101,
                    "sentence": "4. Complexity of Label Encoding: The sequence tagging approach uses a highly complex label set (BIO encoding with component type, distance, and stance).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9814004302024841,
                    "sentence": "While effective, this complexity may hinder interpretability and scalability to other datasets or tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8311593532562256,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9211006164550781,
                    "sentence": "1. How well do the proposed models generalize to other AM datasets with different argumentation structures or domains (e.g., legal or scientific texts)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8745604157447815,
                    "sentence": "2. Could you provide more details on the computational efficiency of the models, particularly for longer documents like essays?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9107168912887573,
                    "sentence": "3. How sensitive are the models to hyperparameter choices, especially for multi-task learning setups?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9660058617591858,
                    "sentence": "Overall Assessment",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8562054634094238,
                    "sentence": "This paper makes a strong contribution to the field of computational argumentation mining by introducing and rigorously evaluating neural end-to-end approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8644349575042725,
                    "sentence": "The work is well-motivated, methodologically sound, and achieves state-of-the-art results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9323098659515381,
                    "sentence": "However, the lack of evaluation on diverse datasets and the omission of encoder-decoder models slightly limit the scope of the findings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9022855162620544,
                    "sentence": "Addressing these issues in future work could further strengthen the impact of this research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.86301589012146,
                    "sentence": "Recommendation: Accept with minor revisions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.07332528267997859
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.4937288086193664,
            "class_probabilities": {
                "human": 0.31246660384140834,
                "ai": 0.4937288086193664,
                "mixed": 0.1938045875392253
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.4937288086193664,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.4937288086193664,
                    "human": 0.31246660384140834,
                    "mixed": 0.1938045875392253
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary and Contributions\nThis paper investigates neural approaches for end-to-end computational argumentation mining (AM), presenting several novel framings of the task: dependency parsing, sequence tagging, multi-task learning (MTL), and a hybrid model combining sequential and tree structure information (LSTM-ER). The authors claim the following contributions: (1) the first neural end-to-end solutions for AM, (2) evidence that neural approaches outperform state-of-the-art feature-based ILP models, (3) a demonstration that token-based dependency parsing is ineffective for AM, (4) robust performance of a sequence tagging model encoding distance information, and (5) improved performance through multi-task learning with auxiliary subtasks. The work is evaluated on the PE dataset, achieving new state-of-the-art results.\nStrengths\n1. Comprehensive Exploration of Neural Framings: The paper systematically explores multiple neural approaches to AM, offering a detailed comparison of their strengths and weaknesses. This breadth of experimentation provides valuable insights into the suitability of different framings for AM tasks.\n2. State-of-the-Art Results: The proposed models achieve significant improvements over the ILP baseline, particularly for sequence tagging and multi-task learning setups. This demonstrates the practical utility of neural approaches in AM.\n3. Novel Multi-Task Learning Setup: The inclusion of auxiliary subtasks (e.g., component detection and relation classification) in the MTL framework is a notable contribution, showing measurable performance gains and providing a clear direction for future work in AM.\n4. Critical Analysis of Dependency Parsing: The authors provide a thorough analysis of why dependency parsing underperforms for AM, offering valuable lessons for researchers considering similar approaches.\n5. Practical Recommendations: The paper offers actionable insights, such as the preference for simpler sequence tagging models for long documents and the decoupling of component and relation detection tasks.\nWeaknesses\n1. Limited Generalization Beyond Dataset: The experiments are conducted solely on the PE dataset, which may limit the generalizability of the findings. The paper would benefit from evaluations on additional datasets with different argumentation structures.\n2. Insufficient Error Analysis: While the authors discuss performance trends, a deeper qualitative error analysis (e.g., common failure cases for relation detection) would enhance understanding of the models' limitations.\n3. Lack of Encoder-Decoder Comparison: The paper mentions encoder-decoder models as a potential framing but does not include them in the experiments. This omission leaves a gap in the exploration of neural architectures for AM.\n4. Complexity of Label Encoding: The sequence tagging approach uses a highly complex label set (BIO encoding with component type, distance, and stance). While effective, this complexity may hinder interpretability and scalability to other datasets or tasks.\nQuestions to Authors\n1. How well do the proposed models generalize to other AM datasets with different argumentation structures or domains (e.g., legal or scientific texts)?\n2. Could you provide more details on the computational efficiency of the models, particularly for longer documents like essays?\n3. How sensitive are the models to hyperparameter choices, especially for multi-task learning setups?\nOverall Assessment\nThis paper makes a strong contribution to the field of computational argumentation mining by introducing and rigorously evaluating neural end-to-end approaches. The work is well-motivated, methodologically sound, and achieves state-of-the-art results. However, the lack of evaluation on diverse datasets and the omission of encoder-decoder models slightly limit the scope of the findings. Addressing these issues in future work could further strengthen the impact of this research. \nRecommendation: Accept with minor revisions."
        }
    ]
}