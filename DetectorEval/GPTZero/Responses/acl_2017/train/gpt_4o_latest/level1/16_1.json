{
    "version": "2025-01-09-base",
    "scanId": "7e9c0af2-383d-41b6-a719-d26567621abf",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999964237213135,
                    "sentence": "Review of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986290931702,
                    "sentence": "Summary and Contributions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992251396179,
                    "sentence": "This paper addresses the task of event detection (ED) by explicitly leveraging argument information through supervised attention mechanisms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986290931702,
                    "sentence": "The authors argue that while arguments provide significant clues for ED, existing methods either ignore them or use them indirectly, leading to suboptimal results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999990463256836,
                    "sentence": "The proposed approach systematically investigates different supervised attention strategies to explicitly incorporate argument information into ED.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999988079071045,
                    "sentence": "Experimental results on the ACE 2005 dataset demonstrate that the proposed method achieves state-of-the-art performance, with significant improvements in F1 scores.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999978542327881,
                    "sentence": "The main contributions of the paper, as I interpret them, are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998927116394,
                    "sentence": "1. Explicit Use of Argument Information for ED: The paper identifies a gap in existing methods, where argument information is underutilized for ED.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992847442627,
                    "sentence": "It proposes a novel supervised attention mechanism to address this issue.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983310699463,
                    "sentence": "2. Supervised Attention Strategies: The authors systematically explore two strategies (S1 and S2) to construct gold attention vectors using annotated arguments, providing a clear framework for leveraging argument information.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977350234985,
                    "sentence": "3. State-of-the-Art Results: The proposed approach achieves the best F1 score on the ACE 2005 dataset, outperforming both pipeline and joint methods, and demonstrates robustness when augmented with external data from FrameNet.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999975562095642,
                    "sentence": "---",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999982118606567,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986886978149,
                    "sentence": "1. Novelty in Leveraging Argument Information: The explicit modeling of argument information through supervised attention mechanisms is a significant and novel contribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999985694885254,
                    "sentence": "The paper clearly demonstrates how arguments can disambiguate triggers and improve ED performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991655349731,
                    "sentence": "2. Comprehensive Experimental Evaluation: The authors conduct extensive experiments on the ACE 2005 dataset, systematically comparing their approach with state-of-the-art methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983310699463,
                    "sentence": "The inclusion of external data from FrameNet further strengthens the evaluation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999989867210388,
                    "sentence": "3. Clear Analysis of Results: The paper provides a detailed analysis of the impact of supervised attention strategies (S1 and S2), highlighting their respective strengths in precision and recall.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999982714653015,
                    "sentence": "This analysis adds depth to the experimental findings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986290931702,
                    "sentence": "4. Addressing Data Sparsity: The use of external data from FrameNet to mitigate data sparsity issues is a practical and effective solution, further validating the robustness of the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971985816956,
                    "sentence": "---",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974370002747,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979138374329,
                    "sentence": "1. Limited Generalization Beyond ACE 2005: While the proposed approach performs well on ACE 2005, its generalizability to other datasets or domains is not explored.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971389770508,
                    "sentence": "This limits the broader applicability of the method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999857485294342,
                    "sentence": "2. Dependency on Annotated Arguments: The reliance on annotated arguments for constructing gold attention vectors may restrict the approach's applicability in scenarios where such annotations are unavailable or incomplete.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999863862991333,
                    "sentence": "3. Complexity of Supervised Attention: The supervised attention mechanism introduces additional complexity to the model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998894929885864,
                    "sentence": "The paper does not provide a detailed analysis of the computational overhead or scalability of the approach, which could be a concern for larger datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997583031654358,
                    "sentence": "---",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991357326507568,
                    "sentence": "Questions to Authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99920254945755,
                    "sentence": "1. How does the proposed approach perform on datasets other than ACE 2005?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996916055679321,
                    "sentence": "Have you considered evaluating it on more diverse or low-resource datasets?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996448755264282,
                    "sentence": "2. What is the impact of noisy or incomplete argument annotations on the performance of the supervised attention mechanism?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988550543785095,
                    "sentence": "3. Could the proposed supervised attention mechanism be adapted for unsupervised or semi-supervised settings to reduce reliance on annotated arguments?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993464350700378,
                    "sentence": "---",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988003969192505,
                    "sentence": "Conclusion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988493919372559,
                    "sentence": "Overall, this paper makes a strong contribution to the field of event detection by explicitly leveraging argument information through supervised attention mechanisms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994378089904785,
                    "sentence": "The proposed approach is well-motivated, rigorously evaluated, and achieves state-of-the-art results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991645812988281,
                    "sentence": "However, the dependency on annotated arguments and the lack of exploration beyond ACE 2005 limit its broader applicability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987812042236328,
                    "sentence": "Addressing these issues in future work could further enhance the impact of this research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.945429265499115,
                    "sentence": "I recommend acceptance, contingent on addressing the outlined weaknesses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Paper\nSummary and Contributions:\nThis paper addresses the task of event detection (ED) by explicitly leveraging argument information through supervised attention mechanisms. The authors argue that while arguments provide significant clues for ED, existing methods either ignore them or use them indirectly, leading to suboptimal results. The proposed approach systematically investigates different supervised attention strategies to explicitly incorporate argument information into ED. Experimental results on the ACE 2005 dataset demonstrate that the proposed method achieves state-of-the-art performance, with significant improvements in F1 scores.\nThe main contributions of the paper, as I interpret them, are:\n1. Explicit Use of Argument Information for ED: The paper identifies a gap in existing methods, where argument information is underutilized for ED. It proposes a novel supervised attention mechanism to address this issue.\n2. Supervised Attention Strategies: The authors systematically explore two strategies (S1 and S2) to construct gold attention vectors using annotated arguments, providing a clear framework for leveraging argument information.\n3. State-of-the-Art Results: The proposed approach achieves the best F1 score on the ACE 2005 dataset, outperforming both pipeline and joint methods, and demonstrates robustness when augmented with external data from FrameNet.\n---\nStrengths:\n1. Novelty in Leveraging Argument Information: The explicit modeling of argument information through supervised attention mechanisms is a significant and novel contribution. The paper clearly demonstrates how arguments can disambiguate triggers and improve ED performance.\n2. Comprehensive Experimental Evaluation: The authors conduct extensive experiments on the ACE 2005 dataset, systematically comparing their approach with state-of-the-art methods. The inclusion of external data from FrameNet further strengthens the evaluation.\n3. Clear Analysis of Results: The paper provides a detailed analysis of the impact of supervised attention strategies (S1 and S2), highlighting their respective strengths in precision and recall. This analysis adds depth to the experimental findings.\n4. Addressing Data Sparsity: The use of external data from FrameNet to mitigate data sparsity issues is a practical and effective solution, further validating the robustness of the proposed approach.\n---\nWeaknesses:\n1. Limited Generalization Beyond ACE 2005: While the proposed approach performs well on ACE 2005, its generalizability to other datasets or domains is not explored. This limits the broader applicability of the method.\n2. Dependency on Annotated Arguments: The reliance on annotated arguments for constructing gold attention vectors may restrict the approach's applicability in scenarios where such annotations are unavailable or incomplete.\n3. Complexity of Supervised Attention: The supervised attention mechanism introduces additional complexity to the model. The paper does not provide a detailed analysis of the computational overhead or scalability of the approach, which could be a concern for larger datasets.\n---\nQuestions to Authors:\n1. How does the proposed approach perform on datasets other than ACE 2005? Have you considered evaluating it on more diverse or low-resource datasets?\n2. What is the impact of noisy or incomplete argument annotations on the performance of the supervised attention mechanism?\n3. Could the proposed supervised attention mechanism be adapted for unsupervised or semi-supervised settings to reduce reliance on annotated arguments?\n---\nConclusion:\nOverall, this paper makes a strong contribution to the field of event detection by explicitly leveraging argument information through supervised attention mechanisms. The proposed approach is well-motivated, rigorously evaluated, and achieves state-of-the-art results. However, the dependency on annotated arguments and the lack of exploration beyond ACE 2005 limit its broader applicability. Addressing these issues in future work could further enhance the impact of this research. I recommend acceptance, contingent on addressing the outlined weaknesses."
        }
    ]
}