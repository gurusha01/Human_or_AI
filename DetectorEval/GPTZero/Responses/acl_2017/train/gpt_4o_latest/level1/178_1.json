{
    "version": "2025-01-09-base",
    "scanId": "39cac865-53a1-4760-9d51-3e7d883fb9ce",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999946355819702,
                    "sentence": "Review of the Submitted Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999989867210388,
                    "sentence": "Summary and Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969005584717,
                    "sentence": "This paper introduces a novel approach for jointly learning embeddings of concepts, phrases, and words using distant supervision from ontology-linked phrases in unannotated corpora.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974370002747,
                    "sentence": "The proposed method leverages structured knowledge from ontologies (e.g., UMLS and YAGO) to train embeddings without requiring manual annotation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960660934448,
                    "sentence": "The authors demonstrate the utility of their embeddings in both biomedical and general domains, achieving competitive performance on concept similarity and relatedness tasks while significantly increasing vocabulary coverage.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999966621398926,
                    "sentence": "Additionally, the paper contributes a novel dataset for evaluating the similarity and relatedness of real-world entities, as well as a software implementation of the method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977946281433,
                    "sentence": "The primary contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999948740005493,
                    "sentence": "1. Joint Embedding Framework: The proposed method integrates concepts, phrases, and words into a shared embedding space, enabling representation learning without manual annotation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999963045120239,
                    "sentence": "This is a significant contribution as it addresses the challenge of modeling multi-word expressions and their underlying concepts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999980330467224,
                    "sentence": "2. Scalability and Coverage: The method achieves over 3x vocabulary coverage compared to prior approaches, demonstrating its scalability across large corpora and diverse ontologies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999982714653015,
                    "sentence": "3. Evaluation Dataset: The introduction of a novel dataset for evaluating real-world entity similarity and relatedness is a valuable resource for the research community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999964237213135,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "1. Scalability and Automation: The method eliminates the need for manual annotation, which is a major bottleneck in embedding learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "By relying on distant supervision, the approach is scalable to large corpora and diverse domains.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999947547912598,
                    "sentence": "2. Competitive Performance: The embeddings achieve results comparable to or better than state-of-the-art methods on several benchmark datasets, particularly in the biomedical domain, without requiring human-annotated training data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997079372406,
                    "sentence": "3. Comprehensive Evaluation: The paper provides extensive evaluations, including similarity and relatedness tasks, compositionality analysis, and semantic type clustering.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999951720237732,
                    "sentence": "These analyses demonstrate the robustness and versatility of the proposed embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999938011169434,
                    "sentence": "4. Resource Contribution: The release of the cui2vec software and the novel evaluation dataset enhances the reproducibility and utility of the work for future research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999899864196777,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999990701675415,
                    "sentence": "1. Limited Novelty in Methodology: While the joint embedding framework is well-executed, it builds heavily on existing techniques like word2vec and distant supervision.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999807476997375,
                    "sentence": "The methodological novelty is somewhat incremental.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997566342353821,
                    "sentence": "2. Evaluation Scope: The evaluation focuses primarily on intrinsic tasks (e.g., similarity and relatedness) and lacks extrinsic validation on downstream NLP applications, such as entity linking or information retrieval, which would further demonstrate the practical utility of the embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999114871025085,
                    "sentence": "3. Ontology Dependency: The method relies on the availability of high-quality ontologies with phrase-to-concept mappings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998909831047058,
                    "sentence": "This dependency may limit its applicability in domains where such resources are incomplete or unavailable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997856020927429,
                    "sentence": "4. Compositionality Trade-offs: The authors note that their embeddings struggle to capture non-lexical information about concepts, as evidenced by the high similarity between concepts and their representative phrases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996985197067261,
                    "sentence": "This suggests that the model may not fully leverage the hierarchical structure of ontologies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9744279384613037,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9654009938240051,
                    "sentence": "1. How does the method perform on downstream tasks such as entity linking or document classification?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9735077619552612,
                    "sentence": "Could you provide additional results to demonstrate its practical utility?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9389061331748962,
                    "sentence": "2. Have you explored incorporating ontology structure (e.g., hierarchical relationships) directly into the training objective to address the compositionality limitations?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9324899315834045,
                    "sentence": "3. How sensitive is the model to the quality and completeness of the input ontology?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9402331113815308,
                    "sentence": "Could the method be adapted for domains with less comprehensive ontologies?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9088678956031799,
                    "sentence": "Overall Assessment",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8365863561630249,
                    "sentence": "The paper presents a well-executed and scalable approach for embedding concepts, phrases, and words, with strong empirical results and valuable resource contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8538049459457397,
                    "sentence": "However, the methodological novelty is somewhat limited, and the lack of extrinsic evaluations leaves questions about the broader applicability of the embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8427793383598328,
                    "sentence": "Addressing these concerns in the author response or future work would strengthen the impact of this submission.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9961636828644501,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9961636828644501,
                "mixed": 0.003836317135549872
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9961636828644501,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9961636828644501,
                    "human": 0,
                    "mixed": 0.003836317135549872
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Submitted Paper\nSummary and Contributions \nThis paper introduces a novel approach for jointly learning embeddings of concepts, phrases, and words using distant supervision from ontology-linked phrases in unannotated corpora. The proposed method leverages structured knowledge from ontologies (e.g., UMLS and YAGO) to train embeddings without requiring manual annotation. The authors demonstrate the utility of their embeddings in both biomedical and general domains, achieving competitive performance on concept similarity and relatedness tasks while significantly increasing vocabulary coverage. Additionally, the paper contributes a novel dataset for evaluating the similarity and relatedness of real-world entities, as well as a software implementation of the method.\nThe primary contributions of this work are: \n1. Joint Embedding Framework: The proposed method integrates concepts, phrases, and words into a shared embedding space, enabling representation learning without manual annotation. This is a significant contribution as it addresses the challenge of modeling multi-word expressions and their underlying concepts. \n2. Scalability and Coverage: The method achieves over 3x vocabulary coverage compared to prior approaches, demonstrating its scalability across large corpora and diverse ontologies. \n3. Evaluation Dataset: The introduction of a novel dataset for evaluating real-world entity similarity and relatedness is a valuable resource for the research community. \nStrengths \n1. Scalability and Automation: The method eliminates the need for manual annotation, which is a major bottleneck in embedding learning. By relying on distant supervision, the approach is scalable to large corpora and diverse domains. \n2. Competitive Performance: The embeddings achieve results comparable to or better than state-of-the-art methods on several benchmark datasets, particularly in the biomedical domain, without requiring human-annotated training data. \n3. Comprehensive Evaluation: The paper provides extensive evaluations, including similarity and relatedness tasks, compositionality analysis, and semantic type clustering. These analyses demonstrate the robustness and versatility of the proposed embeddings. \n4. Resource Contribution: The release of the cui2vec software and the novel evaluation dataset enhances the reproducibility and utility of the work for future research. \nWeaknesses \n1. Limited Novelty in Methodology: While the joint embedding framework is well-executed, it builds heavily on existing techniques like word2vec and distant supervision. The methodological novelty is somewhat incremental. \n2. Evaluation Scope: The evaluation focuses primarily on intrinsic tasks (e.g., similarity and relatedness) and lacks extrinsic validation on downstream NLP applications, such as entity linking or information retrieval, which would further demonstrate the practical utility of the embeddings. \n3. Ontology Dependency: The method relies on the availability of high-quality ontologies with phrase-to-concept mappings. This dependency may limit its applicability in domains where such resources are incomplete or unavailable. \n4. Compositionality Trade-offs: The authors note that their embeddings struggle to capture non-lexical information about concepts, as evidenced by the high similarity between concepts and their representative phrases. This suggests that the model may not fully leverage the hierarchical structure of ontologies. \nQuestions to Authors \n1. How does the method perform on downstream tasks such as entity linking or document classification? Could you provide additional results to demonstrate its practical utility? \n2. Have you explored incorporating ontology structure (e.g., hierarchical relationships) directly into the training objective to address the compositionality limitations? \n3. How sensitive is the model to the quality and completeness of the input ontology? Could the method be adapted for domains with less comprehensive ontologies? \nOverall Assessment \nThe paper presents a well-executed and scalable approach for embedding concepts, phrases, and words, with strong empirical results and valuable resource contributions. However, the methodological novelty is somewhat limited, and the lack of extrinsic evaluations leaves questions about the broader applicability of the embeddings. Addressing these concerns in the author response or future work would strengthen the impact of this submission."
        }
    ]
}