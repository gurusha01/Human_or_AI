{
    "version": "2025-01-09-base",
    "scanId": "faa79e2b-18d5-4b6a-9307-3277d9bb381d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999867081642151,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961853027344,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999954104423523,
                    "sentence": "This paper investigates the impact of task-specific corpora and subjectivity filtering on word embeddings for sentiment analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999915361404419,
                    "sentence": "The authors propose a method to quantify the subjectivity of a corpus and demonstrate that embeddings trained on task-specific datasets outperform those trained on large, generic datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999990701675415,
                    "sentence": "They explore methods to combine generic and task-specific embeddings, including concatenation, appending, and splicing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999993085861206,
                    "sentence": "Additionally, they propose extracting subjective information from generic corpora as a way to approximate task-specific data, which is particularly useful for under-resourced languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999926090240479,
                    "sentence": "The paper concludes by showing that the concatenation approach improves sentiment classification performance for Catalan, an under-resourced language.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999817609786987,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999809861183167,
                    "sentence": "1. Subjectivity as a Metric for Corpus Appropriateness: The paper introduces a novel method to quantify corpus subjectivity and demonstrates its utility in predicting the effectiveness of corpora for sentiment analysis tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999819397926331,
                    "sentence": "2. Combination of Generic and Task-Specific Embeddings: The authors show that concatenating embeddings trained separately on generic and task-specific corpora consistently outperforms other methods (e.g., appending, splicing) for sentiment classification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999852776527405,
                    "sentence": "3. Application to Under-Resourced Languages: The study extends its findings to Catalan, demonstrating that the proposed techniques are particularly beneficial for languages with limited task-specific resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999784231185913,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999884366989136,
                    "sentence": "1. Novelty of Subjectivity Metric: The introduction of subjectivity as a metric for corpus evaluation is a significant contribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999797940254211,
                    "sentence": "The authors provide empirical evidence that subjectivity scores correlate with the effectiveness of embeddings for sentiment analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999683499336243,
                    "sentence": "2. Comprehensive Evaluation: The paper evaluates multiple methods for combining embeddings (concatenation, appending, splicing) and provides clear comparisons, highlighting the advantages of concatenation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999845027923584,
                    "sentence": "3. Relevance to Under-Resourced Languages: The application of the proposed techniques to Catalan is timely and impactful, addressing a critical gap in NLP research for low-resource languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999843239784241,
                    "sentence": "4. Practical Implications: The findings have practical implications for researchers and practitioners working on sentiment analysis, particularly in domains or languages with limited task-specific data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999729990959167,
                    "sentence": "5. Thorough Experiments: The experiments are well-designed, with evaluations on multiple datasets (Rotten Tomatoes, OpeNER, Catalan Aspect-level Sentiment Dataset) and metrics (accuracy, macro F1), lending credibility to the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999045729637146,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999171495437622,
                    "sentence": "1. Limited Exploration of Alternatives to Subjectivity: While the subjectivity metric is novel, the paper does not explore alternative metrics, such as polarity or domain relevance, which could also predict corpus utility for sentiment analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998424053192139,
                    "sentence": "2. Generalizability Beyond Sentiment Analysis: The paper focuses exclusively on sentiment analysis, leaving open the question of whether the proposed methods generalize to other NLP tasks (e.g., named entity recognition, topic modeling).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998701214790344,
                    "sentence": "3. Lexical Overlap Analysis: While the paper discusses lexical overlap between training and test corpora, it does not provide a clear strategy for addressing out-of-vocabulary issues, which remain a limitation for under-resourced languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999893307685852,
                    "sentence": "4. Scalability of Subjectivity Filtering: The computational cost of filtering large generic corpora for subjectivity is not discussed, which could be a concern for scaling the approach to larger datasets or other languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9919961094856262,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9957081079483032,
                    "sentence": "1. Have you considered using polarity instead of subjectivity as a metric for corpus evaluation?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9943529963493347,
                    "sentence": "If so, how would it compare to subjectivity in terms of effectiveness?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9885556101799011,
                    "sentence": "2. Could the concatenation approach be extended to tasks beyond sentiment analysis?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9893008470535278,
                    "sentence": "Have you conducted any preliminary experiments on other tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9906815886497498,
                    "sentence": "3. What are the computational costs of filtering large generic corpora for subjectivity, and how scalable is this approach for high-resource languages?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9911925196647644,
                    "sentence": "Additional Comments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9905150532722473,
                    "sentence": "The paper is well-structured and addresses an important problem in sentiment analysis and low-resource NLP.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9758162498474121,
                    "sentence": "However, exploring alternative metrics and extending the findings to other tasks could further strengthen the impact of the work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary of the Paper\nThis paper investigates the impact of task-specific corpora and subjectivity filtering on word embeddings for sentiment analysis. The authors propose a method to quantify the subjectivity of a corpus and demonstrate that embeddings trained on task-specific datasets outperform those trained on large, generic datasets. They explore methods to combine generic and task-specific embeddings, including concatenation, appending, and splicing. Additionally, they propose extracting subjective information from generic corpora as a way to approximate task-specific data, which is particularly useful for under-resourced languages. The paper concludes by showing that the concatenation approach improves sentiment classification performance for Catalan, an under-resourced language.\nMain Contributions\n1. Subjectivity as a Metric for Corpus Appropriateness: The paper introduces a novel method to quantify corpus subjectivity and demonstrates its utility in predicting the effectiveness of corpora for sentiment analysis tasks.\n2. Combination of Generic and Task-Specific Embeddings: The authors show that concatenating embeddings trained separately on generic and task-specific corpora consistently outperforms other methods (e.g., appending, splicing) for sentiment classification.\n3. Application to Under-Resourced Languages: The study extends its findings to Catalan, demonstrating that the proposed techniques are particularly beneficial for languages with limited task-specific resources.\nStrengths\n1. Novelty of Subjectivity Metric: The introduction of subjectivity as a metric for corpus evaluation is a significant contribution. The authors provide empirical evidence that subjectivity scores correlate with the effectiveness of embeddings for sentiment analysis.\n2. Comprehensive Evaluation: The paper evaluates multiple methods for combining embeddings (concatenation, appending, splicing) and provides clear comparisons, highlighting the advantages of concatenation.\n3. Relevance to Under-Resourced Languages: The application of the proposed techniques to Catalan is timely and impactful, addressing a critical gap in NLP research for low-resource languages.\n4. Practical Implications: The findings have practical implications for researchers and practitioners working on sentiment analysis, particularly in domains or languages with limited task-specific data.\n5. Thorough Experiments: The experiments are well-designed, with evaluations on multiple datasets (Rotten Tomatoes, OpeNER, Catalan Aspect-level Sentiment Dataset) and metrics (accuracy, macro F1), lending credibility to the results.\nWeaknesses\n1. Limited Exploration of Alternatives to Subjectivity: While the subjectivity metric is novel, the paper does not explore alternative metrics, such as polarity or domain relevance, which could also predict corpus utility for sentiment analysis.\n2. Generalizability Beyond Sentiment Analysis: The paper focuses exclusively on sentiment analysis, leaving open the question of whether the proposed methods generalize to other NLP tasks (e.g., named entity recognition, topic modeling).\n3. Lexical Overlap Analysis: While the paper discusses lexical overlap between training and test corpora, it does not provide a clear strategy for addressing out-of-vocabulary issues, which remain a limitation for under-resourced languages.\n4. Scalability of Subjectivity Filtering: The computational cost of filtering large generic corpora for subjectivity is not discussed, which could be a concern for scaling the approach to larger datasets or other languages.\nQuestions to Authors\n1. Have you considered using polarity instead of subjectivity as a metric for corpus evaluation? If so, how would it compare to subjectivity in terms of effectiveness?\n2. Could the concatenation approach be extended to tasks beyond sentiment analysis? Have you conducted any preliminary experiments on other tasks?\n3. What are the computational costs of filtering large generic corpora for subjectivity, and how scalable is this approach for high-resource languages?\nAdditional Comments\nThe paper is well-structured and addresses an important problem in sentiment analysis and low-resource NLP. However, exploring alternative metrics and extending the findings to other tasks could further strengthen the impact of the work."
        }
    ]
}