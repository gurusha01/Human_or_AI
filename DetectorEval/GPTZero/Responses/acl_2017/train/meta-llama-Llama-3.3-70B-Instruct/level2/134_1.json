{
    "version": "2025-01-09-base",
    "scanId": "84478c21-9552-4419-833b-dba55b0546af",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999918341636658,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999975323677063,
                    "sentence": "The paper presents a study on neural end-to-end computational argumentation mining (AM), which involves identifying argumentative structures in text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999766945838928,
                    "sentence": "The authors investigate several neural network framings for AM, including dependency parsing, sequence tagging, and multi-task learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999719262123108,
                    "sentence": "They evaluate the performance of these models on a dataset of persuasive essays and compare them to a feature-based ILP model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999688267707825,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999459385871887,
                    "sentence": "1. The paper presents the first neural end-to-end solutions to computational AM.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999549984931946,
                    "sentence": "2. The authors show that several neural models perform better than the state-of-the-art joint ILP model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999412894248962,
                    "sentence": "3. They demonstrate that a standard neural sequence tagging model performs robustly in different environments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999958872795105,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999645352363586,
                    "sentence": "1. The paper provides a comprehensive evaluation of different neural network architectures for AM, including dependency parsing, sequence tagging, and multi-task learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999097585678101,
                    "sentence": "2. The authors demonstrate the effectiveness of neural models in eliminating the need for manual feature engineering and costly ILP constraint designing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999345541000366,
                    "sentence": "3. The paper provides new state-of-the-art results in end-to-end AM on the PE dataset from Stab and Gurevych (2016).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999871253967285,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999799728393555,
                    "sentence": "1. The paper does not provide a detailed analysis of the errors made by the models, which could help to identify areas for improvement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999760389328003,
                    "sentence": "2. The authors do not compare their results to other neural models that have been proposed for AM, such as encoder-decoder models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999622106552124,
                    "sentence": "3. The paper could benefit from a more detailed discussion of the implications of the results for the field of AM and natural language processing more broadly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999864399433136,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999725222587585,
                    "sentence": "1. Can you provide more details on the error analysis of the models, including the types of errors that were made and the frequency of these errors?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999557733535767,
                    "sentence": "2. How do you think the results of this study could be applied to other tasks in natural language processing, such as text classification or sentiment analysis?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999691843986511,
                    "sentence": "3. Are there any plans to extend this work to other datasets or domains, such as legal or scientific texts?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper presents a study on neural end-to-end computational argumentation mining (AM), which involves identifying argumentative structures in text. The authors investigate several neural network framings for AM, including dependency parsing, sequence tagging, and multi-task learning. They evaluate the performance of these models on a dataset of persuasive essays and compare them to a feature-based ILP model.\nMain Contributions\n1. The paper presents the first neural end-to-end solutions to computational AM.\n2. The authors show that several neural models perform better than the state-of-the-art joint ILP model.\n3. They demonstrate that a standard neural sequence tagging model performs robustly in different environments.\nStrengths\n1. The paper provides a comprehensive evaluation of different neural network architectures for AM, including dependency parsing, sequence tagging, and multi-task learning.\n2. The authors demonstrate the effectiveness of neural models in eliminating the need for manual feature engineering and costly ILP constraint designing.\n3. The paper provides new state-of-the-art results in end-to-end AM on the PE dataset from Stab and Gurevych (2016).\nWeaknesses\n1. The paper does not provide a detailed analysis of the errors made by the models, which could help to identify areas for improvement.\n2. The authors do not compare their results to other neural models that have been proposed for AM, such as encoder-decoder models.\n3. The paper could benefit from a more detailed discussion of the implications of the results for the field of AM and natural language processing more broadly.\nQuestions to Authors\n1. Can you provide more details on the error analysis of the models, including the types of errors that were made and the frequency of these errors?\n2. How do you think the results of this study could be applied to other tasks in natural language processing, such as text classification or sentiment analysis?\n3. Are there any plans to extend this work to other datasets or domains, such as legal or scientific texts?"
        }
    ]
}