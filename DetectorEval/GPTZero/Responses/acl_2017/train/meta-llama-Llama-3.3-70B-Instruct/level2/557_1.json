{
    "version": "2025-01-09-base",
    "scanId": "ccfe82a6-c266-40b0-b853-3ad4914a7efa",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998202323913574,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988318681716919,
                    "sentence": "The paper proposes a novel neural network model for end-to-end relation extraction, which jointly recognizes entities and relations in unstructured text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990987181663513,
                    "sentence": "The model is based on a table-filling framework, where the task is cast as an end-to-end problem, solved incrementally using a local classifier.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994678497314453,
                    "sentence": "The authors introduce novel LSTM features to better learn representations and propose a method to integrate syntactic information into the model without the need for parser outputs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997191429138184,
                    "sentence": "The model is optimized globally, using beam search and early-update, to make use of structural information more effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9967732429504395,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999250590801239,
                    "sentence": "1. Novel LSTM Features: The authors propose a new method to learn representations using LSTM structures, which includes segmental features to reflect entities in a sentence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999271810054779,
                    "sentence": "2. Syntactic Features: The authors introduce a simple method to integrate syntactic information into the model, using the encoder source representations of state-of-the-art parsers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996015429496765,
                    "sentence": "3. Global Optimization: The authors apply global optimization to the model, using beam search and early-update, to make use of structural information more effectively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991000890731812,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996858835220337,
                    "sentence": "1. State-of-the-art Results: The model achieves the best performances on two benchmark datasets, ACE05 and CONLL04.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996181130409241,
                    "sentence": "2. Effective Use of Syntactic Features: The proposed syntactic features improve the relation performances significantly, demonstrating the effectiveness of the method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997111558914185,
                    "sentence": "3. Global Optimization: The global optimization strategy improves the sentence-level accuracies, especially for longer sentences.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998602867126465,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999724805355072,
                    "sentence": "1. Computational Cost: The global optimization strategy increases the computational cost, making the model slower than the local optimization model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997360110282898,
                    "sentence": "2. Limited Analysis: The analysis of the model's performance is limited to the ACE05 test dataset, and more analysis on other datasets would be beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995148777961731,
                    "sentence": "3. Comparison to Other Models: The comparison to other models is limited, and more comparison to other state-of-the-art models would be beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9951595067977905,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997162222862244,
                    "sentence": "1. How does the model perform on other datasets, such as the NYT dataset?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994483590126038,
                    "sentence": "2. Can the model be applied to other NLP tasks, such as sentiment analysis or question answering?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996054172515869,
                    "sentence": "3. How does the model handle out-of-vocabulary words and entities?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper proposes a novel neural network model for end-to-end relation extraction, which jointly recognizes entities and relations in unstructured text. The model is based on a table-filling framework, where the task is cast as an end-to-end problem, solved incrementally using a local classifier. The authors introduce novel LSTM features to better learn representations and propose a method to integrate syntactic information into the model without the need for parser outputs. The model is optimized globally, using beam search and early-update, to make use of structural information more effectively.\nMain Contributions\n1. Novel LSTM Features: The authors propose a new method to learn representations using LSTM structures, which includes segmental features to reflect entities in a sentence.\n2. Syntactic Features: The authors introduce a simple method to integrate syntactic information into the model, using the encoder source representations of state-of-the-art parsers.\n3. Global Optimization: The authors apply global optimization to the model, using beam search and early-update, to make use of structural information more effectively.\nStrengths\n1. State-of-the-art Results: The model achieves the best performances on two benchmark datasets, ACE05 and CONLL04.\n2. Effective Use of Syntactic Features: The proposed syntactic features improve the relation performances significantly, demonstrating the effectiveness of the method.\n3. Global Optimization: The global optimization strategy improves the sentence-level accuracies, especially for longer sentences.\nWeaknesses\n1. Computational Cost: The global optimization strategy increases the computational cost, making the model slower than the local optimization model.\n2. Limited Analysis: The analysis of the model's performance is limited to the ACE05 test dataset, and more analysis on other datasets would be beneficial.\n3. Comparison to Other Models: The comparison to other models is limited, and more comparison to other state-of-the-art models would be beneficial.\nQuestions to Authors\n1. How does the model perform on other datasets, such as the NYT dataset?\n2. Can the model be applied to other NLP tasks, such as sentiment analysis or question answering?\n3. How does the model handle out-of-vocabulary words and entities?"
        }
    ]
}