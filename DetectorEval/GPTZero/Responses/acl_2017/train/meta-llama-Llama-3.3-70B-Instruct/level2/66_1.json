{
    "version": "2025-01-09-base",
    "scanId": "e218456c-c579-4789-a74d-bf1438cb5bff",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999770522117615,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998832941055298,
                    "sentence": "The paper presents a method to automatically generate sentences that encode a given number using the major system, a mnemonic device that maps each digit of a number to a consonant phoneme.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999836802482605,
                    "sentence": "The authors propose several encoding models, including baseline models, preliminary models, and a final sentence encoder model, which combines part-of-speech sentence templates with an n-gram language model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998781085014343,
                    "sentence": "The results of a password memorability study show that the sentence encoder model produces the most memorable password representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998968243598938,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999111294746399,
                    "sentence": "1. The development of a sentence encoder model that generates memorable mnemonic encodings of numbers using the major system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998621940612793,
                    "sentence": "2. The evaluation of the sentence encoder model through a user study on password memorability, which shows that it produces more memorable encodings than the n-gram encoder.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998053312301636,
                    "sentence": "3. The comparison of the sentence encoder model with other encoding models, including baseline models and preliminary models, to identify the strengths and weaknesses of each approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999918520450592,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998345971107483,
                    "sentence": "1. The paper presents a novel approach to generating memorable mnemonic encodings of numbers using the major system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999105930328369,
                    "sentence": "2. The sentence encoder model is shown to be effective in producing memorable encodings, as demonstrated by the user study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999280571937561,
                    "sentence": "3. The paper provides a thorough evaluation of the sentence encoder model, including a comparison with other encoding models and a discussion of its limitations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999628067016602,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999919056892395,
                    "sentence": "1. The user study has some limitations, including the unexpectedly short amount of time spent on the distraction page and the removal of fraudulent responses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999428987503052,
                    "sentence": "2. The paper could benefit from a more detailed discussion of the potential applications and implications of the sentence encoder model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999963641166687,
                    "sentence": "3. The evaluation of the sentence encoder model is limited to a specific context (password memorability), and it would be interesting to see how it performs in other contexts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995623826980591,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999686479568481,
                    "sentence": "1. How do you plan to address the limitations of the user study, such as the short amount of time spent on the distraction page?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999637007713318,
                    "sentence": "2. Can you provide more details on the potential applications and implications of the sentence encoder model, beyond password memorability?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999526739120483,
                    "sentence": "3. How do you plan to extend the sentence encoder model to handle longer numbers and more complex encoding tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper presents a method to automatically generate sentences that encode a given number using the major system, a mnemonic device that maps each digit of a number to a consonant phoneme. The authors propose several encoding models, including baseline models, preliminary models, and a final sentence encoder model, which combines part-of-speech sentence templates with an n-gram language model. The results of a password memorability study show that the sentence encoder model produces the most memorable password representations.\nMain Contributions\n1. The development of a sentence encoder model that generates memorable mnemonic encodings of numbers using the major system.\n2. The evaluation of the sentence encoder model through a user study on password memorability, which shows that it produces more memorable encodings than the n-gram encoder.\n3. The comparison of the sentence encoder model with other encoding models, including baseline models and preliminary models, to identify the strengths and weaknesses of each approach.\nStrengths\n1. The paper presents a novel approach to generating memorable mnemonic encodings of numbers using the major system.\n2. The sentence encoder model is shown to be effective in producing memorable encodings, as demonstrated by the user study.\n3. The paper provides a thorough evaluation of the sentence encoder model, including a comparison with other encoding models and a discussion of its limitations.\nWeaknesses\n1. The user study has some limitations, including the unexpectedly short amount of time spent on the distraction page and the removal of fraudulent responses.\n2. The paper could benefit from a more detailed discussion of the potential applications and implications of the sentence encoder model.\n3. The evaluation of the sentence encoder model is limited to a specific context (password memorability), and it would be interesting to see how it performs in other contexts.\nQuestions to Authors\n1. How do you plan to address the limitations of the user study, such as the short amount of time spent on the distraction page?\n2. Can you provide more details on the potential applications and implications of the sentence encoder model, beyond password memorability?\n3. How do you plan to extend the sentence encoder model to handle longer numbers and more complex encoding tasks?"
        }
    ]
}