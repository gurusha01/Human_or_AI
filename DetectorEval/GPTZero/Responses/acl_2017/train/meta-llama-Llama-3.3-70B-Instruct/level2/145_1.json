{
    "version": "2025-01-09-base",
    "scanId": "9d03c7e3-cc60-4f61-a10d-f6d3c77d2a43",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999600052833557,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999030828475952,
                    "sentence": "The paper introduces a novel approach to word embeddings, representing words as multimodal distributions formed from Gaussian mixtures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999430179595947,
                    "sentence": "This allows for the capture of multiple distinct meanings, entailment, and uncertainty information.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999048709869385,
                    "sentence": "The authors propose an energy-based max-margin objective to learn the parameters of the Gaussian mixture model, using an expected likelihood kernel as the energy function.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998785257339478,
                    "sentence": "The model is evaluated on several benchmark datasets, including word similarity and entailment tasks, and is shown to outperform existing methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998640418052673,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999861717224121,
                    "sentence": "1. Multimodal Word Embeddings: The paper proposes a novel approach to word embeddings, representing words as multimodal distributions formed from Gaussian mixtures, allowing for the capture of multiple distinct meanings and uncertainty information.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999984085559845,
                    "sentence": "2. Energy-Based Max-Margin Objective: The authors propose an energy-based max-margin objective to learn the parameters of the Gaussian mixture model, using an expected likelihood kernel as the energy function.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999924898147583,
                    "sentence": "3. State-of-the-Art Results: The model is evaluated on several benchmark datasets, including word similarity and entailment tasks, and is shown to outperform existing methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999836683273315,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999914169311523,
                    "sentence": "1. Expressive Power: The multimodal word embeddings provide a more expressive representation of words, allowing for the capture of multiple distinct meanings and uncertainty information.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999842643737793,
                    "sentence": "2. Improved Performance: The model is shown to outperform existing methods on several benchmark datasets, including word similarity and entailment tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999988317489624,
                    "sentence": "3. Scalability: The authors provide a scalable learning procedure, capable of training on large datasets with billions of words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999852776527405,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999849200248718,
                    "sentence": "1. Computational Complexity: The model requires the computation of the expected likelihood kernel, which can be computationally expensive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999821186065674,
                    "sentence": "2. Hyperparameter Tuning: The model has several hyperparameters that need to be tuned, which can be time-consuming and require significant computational resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999828934669495,
                    "sentence": "3. Limited Interpretability: The multimodal word embeddings can be difficult to interpret, making it challenging to understand the underlying semantics of the words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997656345367432,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999115467071533,
                    "sentence": "1. How do the authors plan to address the computational complexity of the model, particularly for large-scale datasets?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999387860298157,
                    "sentence": "2. Can the authors provide more insights into the hyperparameter tuning process and how to select the optimal hyperparameters for the model?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999222755432129,
                    "sentence": "3. How do the authors plan to improve the interpretability of the multimodal word embeddings, making it easier to understand the underlying semantics of the words?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997862822885396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997862822885396,
                "mixed": 0.00021371771146045916
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997862822885396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997862822885396,
                    "human": 0,
                    "mixed": 0.00021371771146045916
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper introduces a novel approach to word embeddings, representing words as multimodal distributions formed from Gaussian mixtures. This allows for the capture of multiple distinct meanings, entailment, and uncertainty information. The authors propose an energy-based max-margin objective to learn the parameters of the Gaussian mixture model, using an expected likelihood kernel as the energy function. The model is evaluated on several benchmark datasets, including word similarity and entailment tasks, and is shown to outperform existing methods.\nMain Contributions\n1. Multimodal Word Embeddings: The paper proposes a novel approach to word embeddings, representing words as multimodal distributions formed from Gaussian mixtures, allowing for the capture of multiple distinct meanings and uncertainty information.\n2. Energy-Based Max-Margin Objective: The authors propose an energy-based max-margin objective to learn the parameters of the Gaussian mixture model, using an expected likelihood kernel as the energy function.\n3. State-of-the-Art Results: The model is evaluated on several benchmark datasets, including word similarity and entailment tasks, and is shown to outperform existing methods.\nStrengths\n1. Expressive Power: The multimodal word embeddings provide a more expressive representation of words, allowing for the capture of multiple distinct meanings and uncertainty information.\n2. Improved Performance: The model is shown to outperform existing methods on several benchmark datasets, including word similarity and entailment tasks.\n3. Scalability: The authors provide a scalable learning procedure, capable of training on large datasets with billions of words.\nWeaknesses\n1. Computational Complexity: The model requires the computation of the expected likelihood kernel, which can be computationally expensive.\n2. Hyperparameter Tuning: The model has several hyperparameters that need to be tuned, which can be time-consuming and require significant computational resources.\n3. Limited Interpretability: The multimodal word embeddings can be difficult to interpret, making it challenging to understand the underlying semantics of the words.\nQuestions to Authors\n1. How do the authors plan to address the computational complexity of the model, particularly for large-scale datasets?\n2. Can the authors provide more insights into the hyperparameter tuning process and how to select the optimal hyperparameters for the model?\n3. How do the authors plan to improve the interpretability of the multimodal word embeddings, making it easier to understand the underlying semantics of the words?"
        }
    ]
}