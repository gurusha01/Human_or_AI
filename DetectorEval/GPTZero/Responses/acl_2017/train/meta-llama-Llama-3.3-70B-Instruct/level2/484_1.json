{
    "version": "2025-01-09-base",
    "scanId": "778cfb21-b44f-4d8f-b2a3-6d650ccf0ee2",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999678134918213,
                    "sentence": "Summary of the Paper:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999340176582336,
                    "sentence": "The paper proposes a joint CTC-attention end-to-end automatic speech recognition (ASR) system, which combines the benefits of connectionist temporal classification (CTC) and attention-based methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999151825904846,
                    "sentence": "The system uses a shared encoder network and trains the model using a multi-task learning framework with both CTC and attention objectives.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997494220733643,
                    "sentence": "The proposed method is applied to Japanese and Mandarin Chinese ASR tasks and achieves comparable performance to state-of-the-art conventional systems without using linguistic resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999732971191406,
                    "sentence": "Main Contributions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999963045120239,
                    "sentence": "1. Joint CTC-Attention Model: The paper proposes a joint CTC-attention model that combines the benefits of CTC and attention-based methods, allowing for more accurate and efficient ASR.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967813491821,
                    "sentence": "2. Multi-Task Learning Framework: The paper uses a multi-task learning framework to train the model with both CTC and attention objectives, which improves the performance of the system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999964833259583,
                    "sentence": "3. State-of-the-Art Performance: The proposed method achieves comparable performance to state-of-the-art conventional systems on Japanese and Mandarin Chinese ASR tasks without using linguistic resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973773956299,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977350234985,
                    "sentence": "1. Improved Accuracy: The joint CTC-attention model improves the accuracy of ASR by reducing misalignment issues.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999714493751526,
                    "sentence": "2. Simplified ASR Building Process: The proposed method simplifies the ASR building process by eliminating the need for linguistic resources, GMM-HMM construction, and complex search in decoding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999859929084778,
                    "sentence": "3. Reduced Computational Cost: The method reduces the computational cost of training the network, making it more efficient.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999660849571228,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999069571495056,
                    "sentence": "1. Limited Language Support: The paper only evaluates the proposed method on Japanese and Mandarin Chinese ASR tasks, and it is unclear how well it will perform on other languages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999678134918213,
                    "sentence": "2. Dependence on Hyperparameters: The method relies on hyperparameters such as the weight parameter 位, which may require careful tuning for optimal performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999560713768005,
                    "sentence": "3. Limited Analysis of Errors: The paper does not provide a detailed analysis of the errors made by the proposed method, which could help identify areas for improvement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988383054733276,
                    "sentence": "Questions to Authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995145797729492,
                    "sentence": "1. How do you plan to extend the proposed method to support other languages, such as English?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990352988243103,
                    "sentence": "2. Can you provide more details on the hyperparameter tuning process and how to select the optimal value for 位?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987377524375916,
                    "sentence": "3. How do you plan to address the issue of long sequence lengths in ASR, which can make it difficult to train a decoder network?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper:\nThe paper proposes a joint CTC-attention end-to-end automatic speech recognition (ASR) system, which combines the benefits of connectionist temporal classification (CTC) and attention-based methods. The system uses a shared encoder network and trains the model using a multi-task learning framework with both CTC and attention objectives. The proposed method is applied to Japanese and Mandarin Chinese ASR tasks and achieves comparable performance to state-of-the-art conventional systems without using linguistic resources.\nMain Contributions:\n1. Joint CTC-Attention Model: The paper proposes a joint CTC-attention model that combines the benefits of CTC and attention-based methods, allowing for more accurate and efficient ASR.\n2. Multi-Task Learning Framework: The paper uses a multi-task learning framework to train the model with both CTC and attention objectives, which improves the performance of the system.\n3. State-of-the-Art Performance: The proposed method achieves comparable performance to state-of-the-art conventional systems on Japanese and Mandarin Chinese ASR tasks without using linguistic resources.\nStrengths:\n1. Improved Accuracy: The joint CTC-attention model improves the accuracy of ASR by reducing misalignment issues.\n2. Simplified ASR Building Process: The proposed method simplifies the ASR building process by eliminating the need for linguistic resources, GMM-HMM construction, and complex search in decoding.\n3. Reduced Computational Cost: The method reduces the computational cost of training the network, making it more efficient.\nWeaknesses:\n1. Limited Language Support: The paper only evaluates the proposed method on Japanese and Mandarin Chinese ASR tasks, and it is unclear how well it will perform on other languages.\n2. Dependence on Hyperparameters: The method relies on hyperparameters such as the weight parameter 位, which may require careful tuning for optimal performance.\n3. Limited Analysis of Errors: The paper does not provide a detailed analysis of the errors made by the proposed method, which could help identify areas for improvement.\nQuestions to Authors:\n1. How do you plan to extend the proposed method to support other languages, such as English?\n2. Can you provide more details on the hyperparameter tuning process and how to select the optimal value for 位?\n3. How do you plan to address the issue of long sequence lengths in ASR, which can make it difficult to train a decoder network?"
        }
    ]
}