{
    "version": "2025-01-09-base",
    "scanId": "f5b05b54-83d8-44fa-8f06-793e9a2745bd",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999650120735168,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999305605888367,
                    "sentence": "The paper proposes a novel knowledge embedding model, ITransF, which enables knowledge transfer by learning to discover shared regularities between relations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999602437019348,
                    "sentence": "The model uses a sparse attention mechanism to compose shared concept matrices into relation-specific projection matrices, leading to better generalization properties.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999662637710571,
                    "sentence": "The authors evaluate ITransF on two benchmark datasets, WN18 and FB15k, and show that it outperforms previous models without external information on both mean rank and Hits@10 metrics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999154806137085,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999969482421875,
                    "sentence": "1. Novel Knowledge Embedding Model: ITransF proposes a new knowledge embedding model that enables knowledge transfer by learning to discover shared regularities between relations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999447464942932,
                    "sentence": "2. Sparse Attention Mechanism: The model uses a sparse attention mechanism to compose shared concept matrices into relation-specific projection matrices, leading to better generalization properties.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999144077301025,
                    "sentence": "3. Interpretable Representation: The learned sparse attention vectors provide an interpretable representation of how knowledge is shared between relations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998870491981506,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998980760574341,
                    "sentence": "1. Improved Performance: ITransF outperforms previous models without external information on both mean rank and Hits@10 metrics on two benchmark datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999936044216156,
                    "sentence": "2. Interpretable Representation: The model provides an interpretable representation of how knowledge is shared between relations, which can be useful for understanding the relationships between entities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999673366546631,
                    "sentence": "3. Efficient Optimization: The block iterative optimization algorithm used in ITransF is efficient and scalable, making it suitable for large-scale knowledge graphs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999401569366455,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999219179153442,
                    "sentence": "1. Limited Scalability: While ITransF is efficient, its scalability may be limited by the number of concept matrices and the size of the knowledge graph.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998801946640015,
                    "sentence": "2. Sensitivity to Hyperparameters: The model's performance may be sensitive to hyperparameters, such as the number of concept matrices and the learning rate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998708963394165,
                    "sentence": "3. Lack of Multi-Step Inference: ITransF currently only performs single-step inference, which may limit its ability to capture complex relationships between entities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9909752607345581,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980692267417908,
                    "sentence": "1. How do you plan to extend ITransF to perform multi-step inference, and what benefits do you expect this to bring?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9978232383728027,
                    "sentence": "2. Can you provide more details on the block iterative optimization algorithm used in ITransF, and how it compares to other optimization algorithms?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998068630695343,
                    "sentence": "3. How do you plan to apply ITransF to multi-task learning, and what benefits do you expect this to bring?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984930238596827,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984930238596827,
                "mixed": 0.001506976140317253
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984930238596827,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984930238596827,
                    "human": 0,
                    "mixed": 0.001506976140317253
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper proposes a novel knowledge embedding model, ITransF, which enables knowledge transfer by learning to discover shared regularities between relations. The model uses a sparse attention mechanism to compose shared concept matrices into relation-specific projection matrices, leading to better generalization properties. The authors evaluate ITransF on two benchmark datasets, WN18 and FB15k, and show that it outperforms previous models without external information on both mean rank and Hits@10 metrics.\nMain Contributions\n1. Novel Knowledge Embedding Model: ITransF proposes a new knowledge embedding model that enables knowledge transfer by learning to discover shared regularities between relations.\n2. Sparse Attention Mechanism: The model uses a sparse attention mechanism to compose shared concept matrices into relation-specific projection matrices, leading to better generalization properties.\n3. Interpretable Representation: The learned sparse attention vectors provide an interpretable representation of how knowledge is shared between relations.\nStrengths\n1. Improved Performance: ITransF outperforms previous models without external information on both mean rank and Hits@10 metrics on two benchmark datasets.\n2. Interpretable Representation: The model provides an interpretable representation of how knowledge is shared between relations, which can be useful for understanding the relationships between entities.\n3. Efficient Optimization: The block iterative optimization algorithm used in ITransF is efficient and scalable, making it suitable for large-scale knowledge graphs.\nWeaknesses\n1. Limited Scalability: While ITransF is efficient, its scalability may be limited by the number of concept matrices and the size of the knowledge graph.\n2. Sensitivity to Hyperparameters: The model's performance may be sensitive to hyperparameters, such as the number of concept matrices and the learning rate.\n3. Lack of Multi-Step Inference: ITransF currently only performs single-step inference, which may limit its ability to capture complex relationships between entities.\nQuestions to Authors\n1. How do you plan to extend ITransF to perform multi-step inference, and what benefits do you expect this to bring?\n2. Can you provide more details on the block iterative optimization algorithm used in ITransF, and how it compares to other optimization algorithms?\n3. How do you plan to apply ITransF to multi-task learning, and what benefits do you expect this to bring?"
        }
    ]
}