{
    "version": "2025-01-09-base",
    "scanId": "5c4a8b1e-c16c-4a73-9959-3c1a99108bf0",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9746023416519165,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9841973781585693,
                    "sentence": "The paper introduces a new multi-modal task for computer systems, called Dual Machine Comprehension (DMC), which aims to identify the most suitable text describing a scene from several similar options.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9591212868690491,
                    "sentence": "The task requires a deep understanding of both visual and linguistic elements and their dependencies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7210352420806885,
                    "sentence": "The authors propose an effective and extensible mechanism for generating decoys from human-created image captions and create a large-scale machine comprehension dataset, MCIC, based on the COCO images and captions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5476648211479187,
                    "sentence": "They also conduct human evaluation on this dataset to establish a performance upper-bound and propose several baseline and competitive learning approaches to illustrate the utility of the proposed task and dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9673002362251282,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997067451477051,
                    "sentence": "1. An effective and extensible algorithm for generating decoys from human-created image captions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998958110809326,
                    "sentence": "2. A large-scale dual machine comprehension dataset, MCIC, based on the COCO images and captions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999300837516785,
                    "sentence": "3. Human evaluation results on the MCIC dataset, providing an upper-bound on performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999322295188904,
                    "sentence": "4. Several baseline and competitive learning approaches, including a Vec2seq+FFNN model that achieves state-of-the-art results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999414086341858,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999837279319763,
                    "sentence": "1. The paper proposes a novel and challenging task that requires a deep understanding of both visual and linguistic elements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999820351600647,
                    "sentence": "2. The authors provide a comprehensive evaluation of the proposed task and dataset, including human evaluation and several baseline and competitive learning approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998903870582581,
                    "sentence": "3. The Vec2seq+FFNN model achieves state-of-the-art results on the DMC task and demonstrates the potential of multi-task learning for improving performance on related vision-language tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996009469032288,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997663497924805,
                    "sentence": "1. The paper assumes that the decoy captions are sufficiently similar to the target captions, which may not always be the case.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998700022697449,
                    "sentence": "2. The authors do not provide a detailed analysis of the errors made by the models, which could provide insights into the challenges of the task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997444748878479,
                    "sentence": "3. The paper does not explore the potential applications of the DMC task beyond image captioning and visual question answering.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972760677337646,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996261596679688,
                    "sentence": "1. How do the authors plan to address the issue of decoy captions that are not sufficiently similar to the target captions?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992232322692871,
                    "sentence": "2. Can the authors provide a more detailed analysis of the errors made by the models, including the types of errors and the frequency of each type?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992353916168213,
                    "sentence": "3. How do the authors envision the DMC task being used in real-world applications, and what potential benefits or challenges do they see in using this task in practice?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.15144553742985709
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.8797569601877308,
            "class_probabilities": {
                "human": 0,
                "ai": 0.8797569601877308,
                "mixed": 0.12024303981226925
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8797569601877308,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8797569601877308,
                    "human": 0,
                    "mixed": 0.12024303981226925
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper introduces a new multi-modal task for computer systems, called Dual Machine Comprehension (DMC), which aims to identify the most suitable text describing a scene from several similar options. The task requires a deep understanding of both visual and linguistic elements and their dependencies. The authors propose an effective and extensible mechanism for generating decoys from human-created image captions and create a large-scale machine comprehension dataset, MCIC, based on the COCO images and captions. They also conduct human evaluation on this dataset to establish a performance upper-bound and propose several baseline and competitive learning approaches to illustrate the utility of the proposed task and dataset.\nMain Contributions\n1. An effective and extensible algorithm for generating decoys from human-created image captions.\n2. A large-scale dual machine comprehension dataset, MCIC, based on the COCO images and captions.\n3. Human evaluation results on the MCIC dataset, providing an upper-bound on performance.\n4. Several baseline and competitive learning approaches, including a Vec2seq+FFNN model that achieves state-of-the-art results.\nStrengths\n1. The paper proposes a novel and challenging task that requires a deep understanding of both visual and linguistic elements.\n2. The authors provide a comprehensive evaluation of the proposed task and dataset, including human evaluation and several baseline and competitive learning approaches.\n3. The Vec2seq+FFNN model achieves state-of-the-art results on the DMC task and demonstrates the potential of multi-task learning for improving performance on related vision-language tasks.\nWeaknesses\n1. The paper assumes that the decoy captions are sufficiently similar to the target captions, which may not always be the case.\n2. The authors do not provide a detailed analysis of the errors made by the models, which could provide insights into the challenges of the task.\n3. The paper does not explore the potential applications of the DMC task beyond image captioning and visual question answering.\nQuestions to Authors\n1. How do the authors plan to address the issue of decoy captions that are not sufficiently similar to the target captions?\n2. Can the authors provide a more detailed analysis of the errors made by the models, including the types of errors and the frequency of each type?\n3. How do the authors envision the DMC task being used in real-world applications, and what potential benefits or challenges do they see in using this task in practice?"
        }
    ]
}