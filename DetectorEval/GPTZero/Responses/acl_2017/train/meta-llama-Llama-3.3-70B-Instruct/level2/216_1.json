{
    "version": "2025-01-09-base",
    "scanId": "7ab4a492-f7e7-4da2-94b4-0ea81a9960bd",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999977946281433,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "The paper presents a novel LDA-based model, called segLDAcop, which generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995768070221,
                    "sentence": "The model uses Frank's copula to bind the topics associated with the words of a segment, ensuring coherence between topics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973773956299,
                    "sentence": "Additionally, the model relies on both document and segment-specific topic distributions to capture fine-grained differences in topic assignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999943971633911,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999966025352478,
                    "sentence": "1. Novel LDA-based model: The paper introduces a new LDA-based model that jointly segments documents and assigns topics to their words, ensuring topically coherent segments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960660934448,
                    "sentence": "2. Use of Frank's copula: The model uses Frank's copula to bind the topics associated with the words of a segment, ensuring coherence between topics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999963045120239,
                    "sentence": "3. Document and segment-specific topic distributions: The model relies on both document and segment-specific topic distributions to capture fine-grained differences in topic assignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983310699463,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999996542930603,
                    "sentence": "1. Improved perplexity: The model outperforms other state-of-the-art LDA-based models in terms of perplexity on six publicly available datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955892562866,
                    "sentence": "2. Better topic coherence: The model produces more coherent topics, as measured by the Normalized Pointwise Mutual Information (NPMI) score.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999955296516418,
                    "sentence": "3. Flexible segmentation: The model can discover flexible segmentations, which is not possible with fixed segmentations based on sentences or NPs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967813491821,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967813491821,
                    "sentence": "1. Computational complexity: The model's computational complexity may be higher than other LDA-based models due to the use of Frank's copula and the joint segmentation and topic assignment process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973177909851,
                    "sentence": "2. Hyperparameter tuning: The model has several hyperparameters that need to be tuned, which can be time-consuming and may require significant computational resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999932646751404,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999975562095642,
                    "sentence": "1. How did you choose the value of the hyperparameter 位 for Frank's copula, and what is the sensitivity of the model to this parameter?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999976754188538,
                    "sentence": "2. Can you provide more details on the computational complexity of the model and how it compares to other LDA-based models?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973177909851,
                    "sentence": "3. How do you plan to extend the model to handle larger datasets and more complex topic structures?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper presents a novel LDA-based model, called segLDAcop, which generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The model uses Frank's copula to bind the topics associated with the words of a segment, ensuring coherence between topics. Additionally, the model relies on both document and segment-specific topic distributions to capture fine-grained differences in topic assignments.\nMain Contributions\n1. Novel LDA-based model: The paper introduces a new LDA-based model that jointly segments documents and assigns topics to their words, ensuring topically coherent segments.\n2. Use of Frank's copula: The model uses Frank's copula to bind the topics associated with the words of a segment, ensuring coherence between topics.\n3. Document and segment-specific topic distributions: The model relies on both document and segment-specific topic distributions to capture fine-grained differences in topic assignments.\nStrengths\n1. Improved perplexity: The model outperforms other state-of-the-art LDA-based models in terms of perplexity on six publicly available datasets.\n2. Better topic coherence: The model produces more coherent topics, as measured by the Normalized Pointwise Mutual Information (NPMI) score.\n3. Flexible segmentation: The model can discover flexible segmentations, which is not possible with fixed segmentations based on sentences or NPs.\nWeaknesses\n1. Computational complexity: The model's computational complexity may be higher than other LDA-based models due to the use of Frank's copula and the joint segmentation and topic assignment process.\n2. Hyperparameter tuning: The model has several hyperparameters that need to be tuned, which can be time-consuming and may require significant computational resources.\nQuestions to Authors\n1. How did you choose the value of the hyperparameter 位 for Frank's copula, and what is the sensitivity of the model to this parameter?\n2. Can you provide more details on the computational complexity of the model and how it compares to other LDA-based models?\n3. How do you plan to extend the model to handle larger datasets and more complex topic structures?"
        }
    ]
}