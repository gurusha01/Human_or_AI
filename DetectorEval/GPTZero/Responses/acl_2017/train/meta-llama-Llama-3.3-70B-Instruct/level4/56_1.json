{
    "version": "2025-01-09-base",
    "scanId": "5a4dd98f-96db-4752-a163-45b331861c18",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9995023608207703,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987940788269043,
                    "sentence": "This manuscript proposes an innovative extension of established methods for learning vector representations of text, building upon techniques such as skip-gram with negative sampling, Glove, and other PMI-based approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998885452747345,
                    "sentence": "By transitioning from word co-occurrence statistics to n-gram based statistics, the authors acknowledge the potential increase in complexity due to the expansion of both the embedding vocabulary and context space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993249773979187,
                    "sentence": "The paper successfully addresses this challenge by introducing a method for efficiently computing n-gram embeddings with n-gram context, yielding impressive results on similarity and analogy tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997904300689697,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982488751411438,
                    "sentence": "A notable limitation of the paper is the lack of experiments demonstrating the application of these embeddings in real-world tasks beyond the presented simulations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975505471229553,
                    "sentence": "Incorporating such experiments would significantly enhance the manuscript's strength and practical relevance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9973062872886658,
                    "sentence": "General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991971850395203,
                    "sentence": "Despite this limitation, the paper presents a valuable contribution to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992159008979797,
                    "sentence": "The authors' work is a worthwhile addition to the ACL conference, offering a meaningful exploration of n-gram based embedding methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984602928161621,
                    "sentence": "Having reviewed the author response, the overall quality and potential impact of the research are further reinforced.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Strengths:\nThis manuscript proposes an innovative extension of established methods for learning vector representations of text, building upon techniques such as skip-gram with negative sampling, Glove, and other PMI-based approaches. By transitioning from word co-occurrence statistics to n-gram based statistics, the authors acknowledge the potential increase in complexity due to the expansion of both the embedding vocabulary and context space. The paper successfully addresses this challenge by introducing a method for efficiently computing n-gram embeddings with n-gram context, yielding impressive results on similarity and analogy tasks.\nWeaknesses:\nA notable limitation of the paper is the lack of experiments demonstrating the application of these embeddings in real-world tasks beyond the presented simulations. Incorporating such experiments would significantly enhance the manuscript's strength and practical relevance.\nGeneral Discussion:\nDespite this limitation, the paper presents a valuable contribution to the field. The authors' work is a worthwhile addition to the ACL conference, offering a meaningful exploration of n-gram based embedding methods. Having reviewed the author response, the overall quality and potential impact of the research are further reinforced."
        }
    ]
}