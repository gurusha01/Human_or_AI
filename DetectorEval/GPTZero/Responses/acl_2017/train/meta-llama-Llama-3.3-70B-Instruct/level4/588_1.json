{
    "version": "2025-01-09-base",
    "scanId": "4e2fa28a-baab-4a96-b18b-7e61f01611a8",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.98162841796875,
                    "sentence": "This paper introduces a novel task and accompanying dataset, focusing on predicting omitted named entities from text using an external definitional resource, specifically FreeBase.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.975610613822937,
                    "sentence": "The task is challenging due to the rarity of these entities, making it impractical to train entity-specific models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9652157425880432,
                    "sentence": "The authors convincingly argue the importance of exploring this setting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9770362973213196,
                    "sentence": "Alongside multiple baselines, the paper presents two neural network models that leverage the external resource, including one that accumulates evidence across contexts within the same text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9852865934371948,
                    "sentence": "The paper's strengths include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.988764762878418,
                    "sentence": "- A well-curated set of requirements for the task, which has the potential to advance the field by predicting blanked-out named entities in a manner that poses difficulties for language models, with a particular emphasis on rare entities that should drive the development of more sophisticated models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9899221062660217,
                    "sentence": "- A judicious selection of baselines that effectively demonstrate the inadequacy of both neural network models without external knowledge and simple cosine similarity-based models with external knowledge.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9865551590919495,
                    "sentence": "- The two primary models are appropriately chosen.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.989946186542511,
                    "sentence": "- The text is clear and well-argued.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9959350228309631,
                    "sentence": "However, there are some weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9971343874931335,
                    "sentence": "- The finding that using larger contexts beyond the sentences containing blanks did not improve model performance was somewhat puzzling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9957341551780701,
                    "sentence": "Given that the HierEnc model, which accumulates knowledge from other contexts, was used, it seems counterintuitive that additional context would not be beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9981223940849304,
                    "sentence": "There are two potential explanations: either sentences with blanks are consistently more informative for the task, as suggested in the paper, although this seems somewhat counterintuitive, or the method of utilizing additional context in HierEnc, via the temporal network, is significantly more effective than enlarging individual contexts and feeding them into the recurrent network.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974879622459412,
                    "sentence": "It would be worth considering whether the latter could be a contributing factor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9947232007980347,
                    "sentence": "In general discussion, the proposed task and data are particularly noteworthy, as they have the potential to significantly advance the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9944968223571777,
                    "sentence": "In my opinion, this contribution is the paper's most substantial offering.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.8944041024838034,
            "class_probabilities": {
                "human": 0.10559589751619658,
                "ai": 0.8944041024838034,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8944041024838034,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8944041024838034,
                    "human": 0.10559589751619658,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper introduces a novel task and accompanying dataset, focusing on predicting omitted named entities from text using an external definitional resource, specifically FreeBase. The task is challenging due to the rarity of these entities, making it impractical to train entity-specific models. The authors convincingly argue the importance of exploring this setting. Alongside multiple baselines, the paper presents two neural network models that leverage the external resource, including one that accumulates evidence across contexts within the same text.\nThe paper's strengths include:\n- A well-curated set of requirements for the task, which has the potential to advance the field by predicting blanked-out named entities in a manner that poses difficulties for language models, with a particular emphasis on rare entities that should drive the development of more sophisticated models.\n- A judicious selection of baselines that effectively demonstrate the inadequacy of both neural network models without external knowledge and simple cosine similarity-based models with external knowledge.\n- The two primary models are appropriately chosen.\n- The text is clear and well-argued.\nHowever, there are some weaknesses:\n- The finding that using larger contexts beyond the sentences containing blanks did not improve model performance was somewhat puzzling. Given that the HierEnc model, which accumulates knowledge from other contexts, was used, it seems counterintuitive that additional context would not be beneficial. There are two potential explanations: either sentences with blanks are consistently more informative for the task, as suggested in the paper, although this seems somewhat counterintuitive, or the method of utilizing additional context in HierEnc, via the temporal network, is significantly more effective than enlarging individual contexts and feeding them into the recurrent network. It would be worth considering whether the latter could be a contributing factor.\nIn general discussion, the proposed task and data are particularly noteworthy, as they have the potential to significantly advance the field. In my opinion, this contribution is the paper's most substantial offering."
        }
    ]
}