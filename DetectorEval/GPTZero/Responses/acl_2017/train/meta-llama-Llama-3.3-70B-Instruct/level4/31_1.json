{
    "version": "2025-01-09-base",
    "scanId": "2c569488-2cfb-44a6-af61-d81bc6a82357",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998220801353455,
                    "sentence": "Update after author response:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997195601463318,
                    "sentence": "1. My primary concern regarding the optimization of the model's numerous hyperparameters remains unaddressed, which is crucial given the results are obtained from folded cross-validation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994492530822754,
                    "sentence": "2. The experimental confirmation of the method's benefits, citing a 2% difference through 5-fold CV on 200 examples, is unconvincing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998597264289856,
                    "sentence": "========================================================================",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992377758026123,
                    "sentence": "Summary:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989445209503174,
                    "sentence": "This paper presents a complex neural model for detecting the factuality of event mentions in text, combining traditional classifiers for event mentions, factuality sources, and source introducing predicates (SIPs), with a bidirectional attention-based LSTM model and a CNN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986000061035156,
                    "sentence": "The LSTM learns latent representations for elements on different dependency paths, while the CNN uses these representations for two output predictions: detecting specific from underspecified cases and predicting the actual factuality class.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989473819732666,
                    "sentence": "Methodologically, the authors combine familiar methods (att-BiLSTM and CNN) into a complex model, but it relies on hand-crafted features rather than raw text input.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982872605323792,
                    "sentence": "The evaluation is tainted by the lack of hyperparameter optimization reporting, given the results are from folded cross-validation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988246560096741,
                    "sentence": "The results, showing a 2% macro-average gain over a rule-based baseline and 44% overall performance, are modest considering the model's complexity and preprocessing requirements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989901781082153,
                    "sentence": "The paper is well-written but not suitable for a top-tier conference in its current form.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987557530403137,
                    "sentence": "Remarks:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9959644675254822,
                    "sentence": "1. The claim of \"proper\" combination of LSTM and CNN lacks clarity; what constitutes \"properness,\" and how does it manifest?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9963361024856567,
                    "sentence": "2. The motivation for the two-output design is weak, as it allows for manual feature addition, contradicting the deep model's advantage of learning representations, and lacks experimental support for addressing training set imbalance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9956483840942383,
                    "sentence": "3. There's a discrepancy between motivating the complex DL architecture for learning latent representations and using manually designed features as input.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9960912466049194,
                    "sentence": "4. The detailed description of the attention-based bidirectional LSTM is unnecessary, given its standard use in NLP tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99039626121521,
                    "sentence": "5. The baseline in Section 3 is part of the model, generating input, and should not be referred to as a baseline to maintain paper clarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9907516241073608,
                    "sentence": "6. The results from 5-fold CV do not account for hyperparameter optimization, which is crucial for the model's numerous hyperparameters; reporting these results without optimization or optimizing on the test set is unfair.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9871023893356323,
                    "sentence": "7. The limitation of underspecification to one dimension (polarity or certainty) is unclear, as real-world cases may require specification in one aspect but not the other.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9834991097450256,
                    "sentence": "Language & style:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9749341011047363,
                    "sentence": "1. \"to a great degree\" should be revised to \"to a great extent\" or \"to a large degree\" for clarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9672750234603882,
                    "sentence": "2. \"events that can not\" should be corrected to \"events that cannot\" or \"events that do not.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9737004041671753,
                    "sentence": "3. \"describes out networks...in details shown in Figure 3\" should be rephrased to \"...shown in Figure 3 in details\" for better readability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Update after author response:\n1. My primary concern regarding the optimization of the model's numerous hyperparameters remains unaddressed, which is crucial given the results are obtained from folded cross-validation.\n2. The experimental confirmation of the method's benefits, citing a 2% difference through 5-fold CV on 200 examples, is unconvincing.\n========================================================================\nSummary:\nThis paper presents a complex neural model for detecting the factuality of event mentions in text, combining traditional classifiers for event mentions, factuality sources, and source introducing predicates (SIPs), with a bidirectional attention-based LSTM model and a CNN. The LSTM learns latent representations for elements on different dependency paths, while the CNN uses these representations for two output predictions: detecting specific from underspecified cases and predicting the actual factuality class.\nMethodologically, the authors combine familiar methods (att-BiLSTM and CNN) into a complex model, but it relies on hand-crafted features rather than raw text input. The evaluation is tainted by the lack of hyperparameter optimization reporting, given the results are from folded cross-validation. The results, showing a 2% macro-average gain over a rule-based baseline and 44% overall performance, are modest considering the model's complexity and preprocessing requirements.\nThe paper is well-written but not suitable for a top-tier conference in its current form.\nRemarks:\n1. The claim of \"proper\" combination of LSTM and CNN lacks clarity; what constitutes \"properness,\" and how does it manifest?\n2. The motivation for the two-output design is weak, as it allows for manual feature addition, contradicting the deep model's advantage of learning representations, and lacks experimental support for addressing training set imbalance.\n3. There's a discrepancy between motivating the complex DL architecture for learning latent representations and using manually designed features as input.\n4. The detailed description of the attention-based bidirectional LSTM is unnecessary, given its standard use in NLP tasks.\n5. The baseline in Section 3 is part of the model, generating input, and should not be referred to as a baseline to maintain paper clarity.\n6. The results from 5-fold CV do not account for hyperparameter optimization, which is crucial for the model's numerous hyperparameters; reporting these results without optimization or optimizing on the test set is unfair.\n7. The limitation of underspecification to one dimension (polarity or certainty) is unclear, as real-world cases may require specification in one aspect but not the other.\nLanguage & style:\n1. \"to a great degree\" should be revised to \"to a great extent\" or \"to a large degree\" for clarity.\n2. \"events that can not\" should be corrected to \"events that cannot\" or \"events that do not.\"\n3. \"describes out networks...in details shown in Figure 3\" should be rephrased to \"...shown in Figure 3 in details\" for better readability."
        }
    ]
}