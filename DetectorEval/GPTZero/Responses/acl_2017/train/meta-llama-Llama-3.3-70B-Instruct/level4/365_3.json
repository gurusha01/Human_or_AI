{
    "version": "2025-01-09-base",
    "scanId": "66f0d74b-25a5-4327-a40d-6a01a8ebb3db",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9446001648902893,
                    "sentence": "The paper exhibits several strengths, including its clear writing style, robust experimental design, and engaging qualitative analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9358676671981812,
                    "sentence": "However, one of its weaknesses is that, aside from the qualitative analysis, the content may be more suited to the applications domain, as the models employed are not particularly innovative, but the application itself is where the novelty lies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9373171925544739,
                    "sentence": "This study presents a sequence-to-sequence model that incorporates attention mechanisms and an auxiliary phonetic prediction task to address historical text normalization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9156436920166016,
                    "sentence": "Although the individual models and techniques used are not new, their application to this specific problem appears to be unprecedented, resulting in an improvement over the current state-of-the-art.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8080163598060608,
                    "sentence": "A significant portion of the paper seems more appropriate for the applications track, with the exception of the final analysis, where the authors explore the connection between attention and multi-task learning, suggesting that these two approaches produce similar effects.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7266749143600464,
                    "sentence": "This hypothesis is intriguing and is supported by substantial evidence, at least for the task at hand.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5319520831108093,
                    "sentence": "Nevertheless, I have some questions regarding this analysis:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5030690431594849,
                    "sentence": "1. In Section 5.1, is the assumption that the hidden layer spaces of the two models are aligned justified?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5568302869796753,
                    "sentence": "Is it safe to make this assumption?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.613849401473999,
                    "sentence": "2. In Section 5.2, the description of errors that each model resolves independently is unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6475797891616821,
                    "sentence": "Does this refer to a symmetric difference, implying that combining the two models would not resolve these errors?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6512413024902344,
                    "sentence": "3. Why is there no comparison with Azawi's model?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6221833825111389,
                    "sentence": "After reviewing the authors' response, I have become more concerned about the claims of alignment in the hidden space of the two models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.501604437828064,
                    "sentence": "If the paper is accepted, I strongly encourage the authors to explicitly discuss in the paper the rationale behind their assumption that alignment holds in practice, as shared in their response.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.07332528267997859
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.6927998989311738,
            "class_probabilities": {
                "human": 0.3043821056707332,
                "ai": 0.6927998989311738,
                "mixed": 0.002817995398093028
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.6927998989311738,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.6927998989311738,
                    "human": 0.3043821056707332,
                    "mixed": 0.002817995398093028
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper exhibits several strengths, including its clear writing style, robust experimental design, and engaging qualitative analysis. However, one of its weaknesses is that, aside from the qualitative analysis, the content may be more suited to the applications domain, as the models employed are not particularly innovative, but the application itself is where the novelty lies.\nThis study presents a sequence-to-sequence model that incorporates attention mechanisms and an auxiliary phonetic prediction task to address historical text normalization. Although the individual models and techniques used are not new, their application to this specific problem appears to be unprecedented, resulting in an improvement over the current state-of-the-art.\nA significant portion of the paper seems more appropriate for the applications track, with the exception of the final analysis, where the authors explore the connection between attention and multi-task learning, suggesting that these two approaches produce similar effects. This hypothesis is intriguing and is supported by substantial evidence, at least for the task at hand. Nevertheless, I have some questions regarding this analysis:\n1. In Section 5.1, is the assumption that the hidden layer spaces of the two models are aligned justified? Is it safe to make this assumption?\n2. In Section 5.2, the description of errors that each model resolves independently is unclear. Does this refer to a symmetric difference, implying that combining the two models would not resolve these errors?\n3. Why is there no comparison with Azawi's model?\nAfter reviewing the authors' response, I have become more concerned about the claims of alignment in the hidden space of the two models. If the paper is accepted, I strongly encourage the authors to explicitly discuss in the paper the rationale behind their assumption that alignment holds in practice, as shared in their response."
        }
    ]
}