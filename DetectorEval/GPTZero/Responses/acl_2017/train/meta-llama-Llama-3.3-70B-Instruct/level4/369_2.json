{
    "version": "2025-01-09-base",
    "scanId": "f322f289-108a-484e-b508-c5eeb5d089de",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9986825585365295,
                    "sentence": "The paper presents a deep learning-based approach to enhance two-step translation, with results demonstrated for Chinese->Spanish translation, and the method appears to be generally language-agnostic.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.996433436870575,
                    "sentence": "The setup is standard for two-step machine translation, where the first step translates into a morphologically underspecified version of the target language, and the second step utilizes machine learning to fill in the missing morphological categories, producing the final output by inflecting the underspecified forms using a morphological generator.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9934183359146118,
                    "sentence": "The key innovation in this work is the use of deep neural networks as classifiers in the second step, along with a proposed rescoring step that employs a language model to select the optimal variant.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9946302175521851,
                    "sentence": "Overall, this is a solid piece of work with good empirical results, as the classifier models achieve high accuracy, clearly outperforming baseline methods such as support vector machines, and the improvement is evident in the final translation quality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9976553320884705,
                    "sentence": "However, a major concern with the paper is the lack of comparison with straightforward deep learning baselines, particularly for a structured prediction problem addressed through independent local decisions followed by a rescoring step, which is essentially a sequence labeling task well-suited for recurrent neural networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99828040599823,
                    "sentence": "It would be beneficial to see how a bidirectional LSTM network performs when trained and applied in the standard sequence labeling setting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984466433525085,
                    "sentence": "The author's response did not adequately clarify whether baselines, including the standard LSTM, were run in the same framework, i.e., independently for each local label, which is a significant issue because the paper does not use RNNs in the conventional manner nor justify the alternative approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9961692094802856,
                    "sentence": "The final rescoring step lacks clarity, specifically whether it involves rescoring n-best sentences, the features used, or searching a weighted graph for the single optimal path.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9969891309738159,
                    "sentence": "This needs more explicit explanation in the paper, as the current description suggests producing a graph, finding K best paths, generating inflected sentences from these paths, and then using a language model to select the best variant, but this interpretation is uncertain.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9938817024230957,
                    "sentence": "The paper mentions that larger word embeddings lead to longer training times but does not specify if they also impact the final results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9940040707588196,
                    "sentence": "Additionally, it is counterintuitive that adding information from the source sentence is detrimental, and a more thorough discussion with convincing examples would be appreciated to explain this phenomenon, such as whether the number information sometimes gets lost due to this approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9969642162322998,
                    "sentence": "The paper contains several typos and the English level may not be sufficient for presentation at a major conference like ACL.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9689281582832336,
                    "sentence": "Minor corrections include technical and grammatical errors such as \"context of the application of MT\" to \"context of application for MT\", \"In this cases\" to \"In this case\", \"markov\" to \"Markov\", \"CFR\" to \"CRF\", and numerous other corrections to improve clarity and accuracy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984535574913025,
                    "sentence": "Furthermore, there are specific technical inaccuracies, such as the description of the Sigmoid function's output range and the suggestion to use a tanh layer for outputs in the range (-1, 1), indicating a need for careful review to ensure technical precision.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9960442781448364,
                    "sentence": "Lastly, several phrases and sentences require rephrasing for better readability and understanding, including \"information of a word consists in itself\" to \"information of a word consists of itself\", \"this $A$ set\" to \"the set $A$\", and \"coverage raises the 99%\" to \"coverage exceeds 99%\", among others, to enhance the overall quality of the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper presents a deep learning-based approach to enhance two-step translation, with results demonstrated for Chinese->Spanish translation, and the method appears to be generally language-agnostic.\nThe setup is standard for two-step machine translation, where the first step translates into a morphologically underspecified version of the target language, and the second step utilizes machine learning to fill in the missing morphological categories, producing the final output by inflecting the underspecified forms using a morphological generator. The key innovation in this work is the use of deep neural networks as classifiers in the second step, along with a proposed rescoring step that employs a language model to select the optimal variant.\nOverall, this is a solid piece of work with good empirical results, as the classifier models achieve high accuracy, clearly outperforming baseline methods such as support vector machines, and the improvement is evident in the final translation quality.\nHowever, a major concern with the paper is the lack of comparison with straightforward deep learning baselines, particularly for a structured prediction problem addressed through independent local decisions followed by a rescoring step, which is essentially a sequence labeling task well-suited for recurrent neural networks. It would be beneficial to see how a bidirectional LSTM network performs when trained and applied in the standard sequence labeling setting. The author's response did not adequately clarify whether baselines, including the standard LSTM, were run in the same framework, i.e., independently for each local label, which is a significant issue because the paper does not use RNNs in the conventional manner nor justify the alternative approach.\nThe final rescoring step lacks clarity, specifically whether it involves rescoring n-best sentences, the features used, or searching a weighted graph for the single optimal path. This needs more explicit explanation in the paper, as the current description suggests producing a graph, finding K best paths, generating inflected sentences from these paths, and then using a language model to select the best variant, but this interpretation is uncertain.\nThe paper mentions that larger word embeddings lead to longer training times but does not specify if they also impact the final results. Additionally, it is counterintuitive that adding information from the source sentence is detrimental, and a more thorough discussion with convincing examples would be appreciated to explain this phenomenon, such as whether the number information sometimes gets lost due to this approach.\nThe paper contains several typos and the English level may not be sufficient for presentation at a major conference like ACL. Minor corrections include technical and grammatical errors such as \"context of the application of MT\" to \"context of application for MT\", \"In this cases\" to \"In this case\", \"markov\" to \"Markov\", \"CFR\" to \"CRF\", and numerous other corrections to improve clarity and accuracy.\nFurthermore, there are specific technical inaccuracies, such as the description of the Sigmoid function's output range and the suggestion to use a tanh layer for outputs in the range (-1, 1), indicating a need for careful review to ensure technical precision. \nLastly, several phrases and sentences require rephrasing for better readability and understanding, including \"information of a word consists in itself\" to \"information of a word consists of itself\", \"this $A$ set\" to \"the set $A$\", and \"coverage raises the 99%\" to \"coverage exceeds 99%\", among others, to enhance the overall quality of the paper."
        }
    ]
}