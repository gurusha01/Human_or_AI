{
    "version": "2025-01-09-base",
    "scanId": "1c9bc52a-40c8-407b-bc7c-8ee7d27b288b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9938862919807434,
                    "sentence": "The paper proposes a novel encoder-decoder architecture that incorporates \"hard monotonic attention\" to explicitly model monotonicity in sequence-to-sequence tasks, which is a significant strength.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9858354330062866,
                    "sentence": "However, a potential weakness is that the model may be equivalent to a standard BiRNN with decoupled alignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9858711957931519,
                    "sentence": "Furthermore, the evaluation is limited to morphology tasks, without exploring other monotonic sequence-to-sequence tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9819689393043518,
                    "sentence": "The authors present a well-written and clear paper that introduces a new idea, namely, enforcing monotonicity in morphology tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9792073965072632,
                    "sentence": "The motivation for this approach is clear, as many sequence-to-sequence tasks are monotone, and general encoder-decoder models may not be suitable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9698957204818726,
                    "sentence": "The paper's idea is to explicitly enforce monotonic output character generation by decoupling alignment and transduction, first aligning input-output sequences monotonically and then training to generate outputs in agreement with the monotone alignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9784646034240723,
                    "sentence": "However, the authors' description of the alignment process is unclear, raising several questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9745244383811951,
                    "sentence": "Firstly, the nature of the alignments used in the model is ambiguous, with the paper seeming to describe both 1-to-many and many-to-1 alignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9675205945968628,
                    "sentence": "It is essential to clarify whether the model uses 1-to-many, many-to-1, or many-to-many alignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9707956314086914,
                    "sentence": "Secondly, a simpler approach to monotone sequence-to-sequence tasks involves using a monotone aligner to align input and output characters and then training a standard sequence tagger to predict these alignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9732096195220947,
                    "sentence": "The authors' approach differs from this simple idea, but it is unclear how.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972769021987915,
                    "sentence": "Moreover, it would be beneficial to include this simpler approach as a baseline for comparison.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999069333076477,
                    "sentence": "Additional concerns include the limited evaluation of the model on only morphology tasks, which makes it difficult to assess its performance on other monotonic sequence-to-sequence tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9876043796539307,
                    "sentence": "The experimental results show that the model performs \"on par or better\" than other models, but this wording may be misleading, as it maps instances where the model performs worse to \"on par.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977744817733765,
                    "sentence": "The paper also lacks information about the linguistic features used, such as their source and potential impact on the model's performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9702672958374023,
                    "sentence": "Minor issues include the notation used in Equation (3), formatting errors in the text, and the use of Cyrillic font in Figure 1, which may not be necessary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6945794820785522,
                    "sentence": "After the author response, it is still unclear how the model's alignments are generated, and it would be beneficial to compare the results to a simpler baseline using 1-x alignments and a neural tagger.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.95164555311203,
                    "sentence": "The architecture proposed in the paper is related to other methods, such as the Stack LSTM, which predicts a sequence of actions that modify or annotate an input.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9620236754417419,
                    "sentence": "The use of greedy alignment in the model may also be a limitation, as it does not consider all possible alignments, unlike other models that sum over all alignments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9719721674919128,
                    "sentence": "Overall, while the paper presents a novel idea, it requires further clarification and evaluation to demonstrate its effectiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.992329959936083,
            "class_probabilities": {
                "human": 0,
                "ai": 0.992329959936083,
                "mixed": 0.007670040063917
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.992329959936083,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.992329959936083,
                    "human": 0,
                    "mixed": 0.007670040063917
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper proposes a novel encoder-decoder architecture that incorporates \"hard monotonic attention\" to explicitly model monotonicity in sequence-to-sequence tasks, which is a significant strength. However, a potential weakness is that the model may be equivalent to a standard BiRNN with decoupled alignments. Furthermore, the evaluation is limited to morphology tasks, without exploring other monotonic sequence-to-sequence tasks.\nThe authors present a well-written and clear paper that introduces a new idea, namely, enforcing monotonicity in morphology tasks. The motivation for this approach is clear, as many sequence-to-sequence tasks are monotone, and general encoder-decoder models may not be suitable. The paper's idea is to explicitly enforce monotonic output character generation by decoupling alignment and transduction, first aligning input-output sequences monotonically and then training to generate outputs in agreement with the monotone alignments. However, the authors' description of the alignment process is unclear, raising several questions.\nFirstly, the nature of the alignments used in the model is ambiguous, with the paper seeming to describe both 1-to-many and many-to-1 alignments. It is essential to clarify whether the model uses 1-to-many, many-to-1, or many-to-many alignments. Secondly, a simpler approach to monotone sequence-to-sequence tasks involves using a monotone aligner to align input and output characters and then training a standard sequence tagger to predict these alignments. The authors' approach differs from this simple idea, but it is unclear how. Moreover, it would be beneficial to include this simpler approach as a baseline for comparison.\nAdditional concerns include the limited evaluation of the model on only morphology tasks, which makes it difficult to assess its performance on other monotonic sequence-to-sequence tasks. The experimental results show that the model performs \"on par or better\" than other models, but this wording may be misleading, as it maps instances where the model performs worse to \"on par.\" The paper also lacks information about the linguistic features used, such as their source and potential impact on the model's performance.\nMinor issues include the notation used in Equation (3), formatting errors in the text, and the use of Cyrillic font in Figure 1, which may not be necessary. After the author response, it is still unclear how the model's alignments are generated, and it would be beneficial to compare the results to a simpler baseline using 1-x alignments and a neural tagger.\nThe architecture proposed in the paper is related to other methods, such as the Stack LSTM, which predicts a sequence of actions that modify or annotate an input. The use of greedy alignment in the model may also be a limitation, as it does not consider all possible alignments, unlike other models that sum over all alignments. Overall, while the paper presents a novel idea, it requires further clarification and evaluation to demonstrate its effectiveness."
        }
    ]
}