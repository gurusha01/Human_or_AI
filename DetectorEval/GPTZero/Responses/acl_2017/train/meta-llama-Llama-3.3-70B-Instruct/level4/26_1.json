{
    "version": "2025-01-09-base",
    "scanId": "128deea2-1a9a-461a-948b-d4374851a673",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9971724152565002,
                    "sentence": "Review- 26: An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9935488700866699,
                    "sentence": "This paper proposes a novel approach to factoid question answering over a knowledge graph (Freebase) by employing a neural model that learns to establish semantic relationships between various aspects of candidate answers (such as answer type, relation to the question entity, and answer semantics) and a subset of question words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9823200702667236,
                    "sentence": "Notably, a separate correspondence component is learned for each aspect of the candidate answers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9752347469329834,
                    "sentence": "The two primary contributions of this work are: (1) the development of distinct components to capture different aspects of candidate answers, rather than relying on a single semantic representation, and (2) the incorporation of global context from the knowledge base (KB) into the candidate answers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9511623978614807,
                    "sentence": "One of the most intriguing aspects of this work is the decomposition of candidate answer representation into distinct aspects, providing developers with greater control over guiding neural network (NN) models towards more beneficial information for decision-making.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8977288007736206,
                    "sentence": "This approach bears some resemblance to traditional algorithms that rely on feature engineering, but with more subtle and less onerous \"feature engineering\" (i.e., aspects).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8739942312240601,
                    "sentence": "I encourage the authors to further refine this system along these lines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9465355277061462,
                    "sentence": "While the high-level concept is relatively clear to informed readers, the details may pose challenges for some audiences to immediately grasp key insights.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9570790529251099,
                    "sentence": "Certain sections of the paper could benefit from additional explanation, specifically:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8732794523239136,
                    "sentence": "(1) The context aspect of candidate answers (e_c) is not clearly explained, leading to unclear sentences in Section 3.2.2.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8716641664505005,
                    "sentence": "(2) The mention of out-of-vocabulary (OOV) terms in the abstract and introduction requires more explanation, as the current exposition assumes a deep understanding of prior work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9548399448394775,
                    "sentence": "(3) The experiments primarily compare the proposed system to information retrieval (IR)-based systems, which is reasonable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8403456807136536,
                    "sentence": "However, the inclusion of Yang et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9011440277099609,
                    "sentence": "(2014) in the comparison, described as a semantic parsing (SP)-based system, seems inconsistent.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8992705345153809,
                    "sentence": "It would be beneficial to include comparable performance numbers for top-performing SP-based systems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6453797817230225,
                    "sentence": "I notice that the embeddings are learned solely from the training data, and I wonder about the impact of random initialization on the final performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5301364064216614,
                    "sentence": "Investigating and reporting the variance, if any, would be interesting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7343010902404785,
                    "sentence": "Additionally, using pre-trained embeddings (e.g., from word2vec) instead of random initialization may have an impact, which could be explored.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7363370656967163,
                    "sentence": "As I read the paper, a potential future direction that occurred to me was incorporating structured queries (from SP-based methods) into the cross-attention mechanism.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7784337997436523,
                    "sentence": "This could involve using structured queries that generate the candidate answer as an additional aspect of the candidate answer, allowing the attention mechanism to focus on various parts of the structured query and its semantic matches to the input question.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7550580501556396,
                    "sentence": "Regarding the paper's positioning, I hesitate to categorize the proposed model as an \"attention\" model, as attention mechanisms typically apply to encoder-decoder situations where semantics are encoded into an abstract representation and generated into another structured form.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6823987364768982,
                    "sentence": "The attention mechanism enables the encoder to attend to different parts of the input as the output is being generated by the decoder.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5586565136909485,
                    "sentence": "This paper's approach may not fit this notion, potentially causing confusion for a broader audience.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7482390999794006,
                    "sentence": "------",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.4974864423274994,
                    "sentence": "I appreciate the clarifications provided in the author response.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.8942312479578237,
            "class_probabilities": {
                "human": 0.1055754898181998,
                "ai": 0.8942312479578237,
                "mixed": 0.0001932622239765565
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8942312479578237,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8942312479578237,
                    "human": 0.1055754898181998,
                    "mixed": 0.0001932622239765565
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review- 26: An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge\nThis paper proposes a novel approach to factoid question answering over a knowledge graph (Freebase) by employing a neural model that learns to establish semantic relationships between various aspects of candidate answers (such as answer type, relation to the question entity, and answer semantics) and a subset of question words. Notably, a separate correspondence component is learned for each aspect of the candidate answers. The two primary contributions of this work are: (1) the development of distinct components to capture different aspects of candidate answers, rather than relying on a single semantic representation, and (2) the incorporation of global context from the knowledge base (KB) into the candidate answers.\nOne of the most intriguing aspects of this work is the decomposition of candidate answer representation into distinct aspects, providing developers with greater control over guiding neural network (NN) models towards more beneficial information for decision-making. This approach bears some resemblance to traditional algorithms that rely on feature engineering, but with more subtle and less onerous \"feature engineering\" (i.e., aspects). I encourage the authors to further refine this system along these lines.\nWhile the high-level concept is relatively clear to informed readers, the details may pose challenges for some audiences to immediately grasp key insights. Certain sections of the paper could benefit from additional explanation, specifically: \n(1) The context aspect of candidate answers (e_c) is not clearly explained, leading to unclear sentences in Section 3.2.2.\n(2) The mention of out-of-vocabulary (OOV) terms in the abstract and introduction requires more explanation, as the current exposition assumes a deep understanding of prior work.\n(3) The experiments primarily compare the proposed system to information retrieval (IR)-based systems, which is reasonable. However, the inclusion of Yang et al. (2014) in the comparison, described as a semantic parsing (SP)-based system, seems inconsistent. It would be beneficial to include comparable performance numbers for top-performing SP-based systems.\nI notice that the embeddings are learned solely from the training data, and I wonder about the impact of random initialization on the final performance. Investigating and reporting the variance, if any, would be interesting. Additionally, using pre-trained embeddings (e.g., from word2vec) instead of random initialization may have an impact, which could be explored.\nAs I read the paper, a potential future direction that occurred to me was incorporating structured queries (from SP-based methods) into the cross-attention mechanism. This could involve using structured queries that generate the candidate answer as an additional aspect of the candidate answer, allowing the attention mechanism to focus on various parts of the structured query and its semantic matches to the input question.\nRegarding the paper's positioning, I hesitate to categorize the proposed model as an \"attention\" model, as attention mechanisms typically apply to encoder-decoder situations where semantics are encoded into an abstract representation and generated into another structured form. The attention mechanism enables the encoder to attend to different parts of the input as the output is being generated by the decoder. This paper's approach may not fit this notion, potentially causing confusion for a broader audience.\n------\nI appreciate the clarifications provided in the author response."
        }
    ]
}