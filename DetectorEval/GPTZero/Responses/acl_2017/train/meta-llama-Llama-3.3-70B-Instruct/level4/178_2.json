{
    "version": "2025-01-09-base",
    "scanId": "a35f2015-9502-4272-9791-ddb50e19327f",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998806118965149,
                    "sentence": "This paper proposes a vector space model for representing words, phrases, and concepts, leveraging an ontology that maps concepts to phrases found in text corpora.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997880458831787,
                    "sentence": "The approach employs a methodology akin to the skip-gram technique to generate embeddings for words, phrases, and their associated concepts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997884035110474,
                    "sentence": "The model's performance is evaluated on concept similarity and relatedness tasks, utilizing UMLS and Yago as the underlying ontologies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997658133506775,
                    "sentence": "The paper's strengths include addressing the crucial issue of phrases that are not lexically similar but semantically close, and proposing a plausible model for training phrase embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998128414154053,
                    "sentence": "The resulting embeddings demonstrate competitiveness or superiority in identifying concept similarities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997433423995972,
                    "sentence": "Additionally, the accompanying software release may prove beneficial for biomedical NLP researchers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997937083244324,
                    "sentence": "However, the paper also exhibits weaknesses, primarily that the proposed model is not particularly novel, essentially being a variation of the skip-gram method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997254014015198,
                    "sentence": "Furthermore, the full model presented does not consistently outperform baseline models, such as the Choi baseline on the Mayo datasets, and is at best competitive with simpler models, like Chiu's, on the larger UMNSRS data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999671220779419,
                    "sentence": "A general discussion of the paper reveals some areas of concern.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997096657752991,
                    "sentence": "The terminology used to describe the model's training process as \"distant supervision\" is misleading, as it does not conform to the traditional understanding of supervision in predictive tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997325539588928,
                    "sentence": "The notation introduced in Section 3.2 is not utilized elsewhere in the paper, which may cause confusion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996306300163269,
                    "sentence": "The use of the hyperparameter Î² to control phrase compositionality is surprising, as it implies a global constant governing compositionality, with cross-validated values from Table 3 yielding unexpected results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995977878570557,
                    "sentence": "The experimental setup for Table 4 requires clarification, particularly regarding the computation of similarities between concepts or entities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996328353881836,
                    "sentence": "Finally, the superior performance of the approximate phr method in Table 5 raises questions about the necessity of the concept ontology, as comparable results could potentially be achieved using a seed set of phrases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a vector space model for representing words, phrases, and concepts, leveraging an ontology that maps concepts to phrases found in text corpora. The approach employs a methodology akin to the skip-gram technique to generate embeddings for words, phrases, and their associated concepts. The model's performance is evaluated on concept similarity and relatedness tasks, utilizing UMLS and Yago as the underlying ontologies.\nThe paper's strengths include addressing the crucial issue of phrases that are not lexically similar but semantically close, and proposing a plausible model for training phrase embeddings. The resulting embeddings demonstrate competitiveness or superiority in identifying concept similarities. Additionally, the accompanying software release may prove beneficial for biomedical NLP researchers.\nHowever, the paper also exhibits weaknesses, primarily that the proposed model is not particularly novel, essentially being a variation of the skip-gram method. Furthermore, the full model presented does not consistently outperform baseline models, such as the Choi baseline on the Mayo datasets, and is at best competitive with simpler models, like Chiu's, on the larger UMNSRS data.\nA general discussion of the paper reveals some areas of concern. The terminology used to describe the model's training process as \"distant supervision\" is misleading, as it does not conform to the traditional understanding of supervision in predictive tasks. The notation introduced in Section 3.2 is not utilized elsewhere in the paper, which may cause confusion. The use of the hyperparameter Î² to control phrase compositionality is surprising, as it implies a global constant governing compositionality, with cross-validated values from Table 3 yielding unexpected results. The experimental setup for Table 4 requires clarification, particularly regarding the computation of similarities between concepts or entities. Finally, the superior performance of the approximate phr method in Table 5 raises questions about the necessity of the concept ontology, as comparable results could potentially be achieved using a seed set of phrases."
        }
    ]
}