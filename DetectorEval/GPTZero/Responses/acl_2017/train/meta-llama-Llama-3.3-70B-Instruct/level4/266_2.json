{
    "version": "2025-01-09-base",
    "scanId": "5fabe4cc-7f8a-4f45-aed5-91f324862946",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9993755221366882,
                    "sentence": "This study presents a compelling and exhaustive examination of the impact of utilizing specialized-domain corpora for training word embeddings, with a clear articulation of its underlying assumptions, key contributions, methodological approach, and resultant findings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991391897201538,
                    "sentence": "A thorough assessment of the proposal's various facets is also provided.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989122748374939,
                    "sentence": "However, certain conclusions drawn by the authors appear to lack sufficient numerical evidence to support them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989894032478333,
                    "sentence": "For instance, the claim that the benefits of using domain-specific corpora for training word vectors are more pronounced in Catalan than in English is not entirely substantiated by the data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983272552490234,
                    "sentence": "Specifically, Table 6 indicates that none of the combination methods surpass the baseline for 300-dimension vectors, which raises questions about the basis for this conclusion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986855387687683,
                    "sentence": "Overall, the paper offers a series of straightforward yet intriguing experiments that demonstrate the significant advantages of using relevant, in-domain corpora for training word vectors, as evidenced by the skip-gram method employed in this research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986305832862854,
                    "sentence": "The study addresses crucial questions that hold considerable value for natural language processing practitioners.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993790984153748,
                    "sentence": "Furthermore, the paper is distinguished by its clarity, organization, and overall quality of writing, making it a notable contribution to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This study presents a compelling and exhaustive examination of the impact of utilizing specialized-domain corpora for training word embeddings, with a clear articulation of its underlying assumptions, key contributions, methodological approach, and resultant findings. A thorough assessment of the proposal's various facets is also provided.\nHowever, certain conclusions drawn by the authors appear to lack sufficient numerical evidence to support them. For instance, the claim that the benefits of using domain-specific corpora for training word vectors are more pronounced in Catalan than in English is not entirely substantiated by the data. Specifically, Table 6 indicates that none of the combination methods surpass the baseline for 300-dimension vectors, which raises questions about the basis for this conclusion.\nOverall, the paper offers a series of straightforward yet intriguing experiments that demonstrate the significant advantages of using relevant, in-domain corpora for training word vectors, as evidenced by the skip-gram method employed in this research. The study addresses crucial questions that hold considerable value for natural language processing practitioners. Furthermore, the paper is distinguished by its clarity, organization, and overall quality of writing, making it a notable contribution to the field."
        }
    ]
}