{
    "version": "2025-01-09-base",
    "scanId": "49b1f268-ccce-4543-8805-d80646770cd0",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9992435574531555,
                    "sentence": "This paper presents a novel approach to automatic speech recognition (ASR) by proposing a joint CTC-attention end-to-end framework that leverages the strengths of both techniques during training and decoding phases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993100762367249,
                    "sentence": "- Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995998740196228,
                    "sentence": "The work provides a comprehensive exploration of the hybrid CTC-attention framework, demonstrating its effectiveness in both training and decoding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993432760238647,
                    "sentence": "The experimental results indicate that the proposed method yields improved performance in Japanese CSJ and Mandarin Chinese telephone speech recognition tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995097517967224,
                    "sentence": "- Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986730217933655,
                    "sentence": "A notable concern is the similarity between this paper and the work presented in Ref [Kim et al., 2016], which is scheduled for official publication at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) in March 2017.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9632332921028137,
                    "sentence": "Kim et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9601286053657532,
                    "sentence": "[2016] propose a joint CTC-attention approach using multi-task learning (MTL) for English ASR, whereas this paper extends this concept to Japanese and Chinese ASR tasks by incorporating joint decoding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9517086148262024,
                    "sentence": "However, the distinction between the two works is not clearly articulated by the authors, making it challenging to discern the original contributions of this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9276948571205139,
                    "sentence": "(a) Title:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5975536108016968,
                    "sentence": "The title of Ref [Kim et al., 2016] is \"Joint CTC-Attention Based End-to-End Speech Recognition Using Multi-task Learning\", whereas this paper's title is \"Joint CTC-attention End-to-end Speech Recognition\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.34263721108436584,
                    "sentence": "The title appears overly broad, and a more specific title that highlights the primary contributions of this paper in relation to existing publications would be more suitable, particularly given that [Kim et al., 2016] will be officially published prior to this work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.0690593346953392,
                    "sentence": "(b) Introduction:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5613926649093628,
                    "sentence": "The authors claim to propose leveraging the constrained CTC alignment in a hybrid CTC-attention system, attaching a CTC objective to an attention-based encoder network as a regularization, similar to [Kim et al., 2016].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.36690568923950195,
                    "sentence": "However, this concept is not novel, as it originates from [Kim et al., 2016].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.655879020690918,
                    "sentence": "The discussion on the necessity of combining CTC and attention-based end-to-end ASR is also not new.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.18726859986782074,
                    "sentence": "Furthermore, the statement \"we propose... as proposed by [Kim et al., 2016]\" is awkward, as it implies re-proposing existing work rather than building upon it with additional extensions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.4982004165649414,
                    "sentence": "It is essential to clearly state the original contributions of this paper and its position relative to existing literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.07413879036903381,
                    "sentence": "(c) Experimental Results:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.06595553457736969,
                    "sentence": "While Kim et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.4140782058238983,
                    "sentence": "[2016] applied their method to English tasks, this paper focuses on Japanese and Mandarin Chinese tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.2405654489994049,
                    "sentence": "It would be intriguing to see a more detailed explanation of the specific challenges associated with these languages that may not be present in English, such as addressing multiple possible outputs (e.g., Kanji, Hiragana, and Katakana) given Japanese speech input without relying on linguistic resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.2309432178735733,
                    "sentence": "This could be a significant contribution of this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.85744309425354,
                    "sentence": "- General Discussion:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5130919814109802,
                    "sentence": "It is recommended to cite Ref [Kim et al., 2016] from the official IEEE ICASSP conference proceedings instead of the pre-published arXiv version: Kim, S., Hori, T., Watanabe, S., \"Joint CTC-Attention Based End-to-End Speech Recognition Using Multi-task Learning\", IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017, pp.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7765595316886902,
                    "sentence": "to appear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.7915901668643477,
            "class_probabilities": {
                "human": 0.19554161860041921,
                "ai": 0.7915901668643477,
                "mixed": 0.012868214535233237
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.7915901668643477,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.7915901668643477,
                    "human": 0.19554161860041921,
                    "mixed": 0.012868214535233237
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a novel approach to automatic speech recognition (ASR) by proposing a joint CTC-attention end-to-end framework that leverages the strengths of both techniques during training and decoding phases.\n- Strengths:\nThe work provides a comprehensive exploration of the hybrid CTC-attention framework, demonstrating its effectiveness in both training and decoding. The experimental results indicate that the proposed method yields improved performance in Japanese CSJ and Mandarin Chinese telephone speech recognition tasks.\n- Weaknesses:\nA notable concern is the similarity between this paper and the work presented in Ref [Kim et al., 2016], which is scheduled for official publication at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) in March 2017. Kim et al. [2016] propose a joint CTC-attention approach using multi-task learning (MTL) for English ASR, whereas this paper extends this concept to Japanese and Chinese ASR tasks by incorporating joint decoding. However, the distinction between the two works is not clearly articulated by the authors, making it challenging to discern the original contributions of this paper.\n(a) Title:\nThe title of Ref [Kim et al., 2016] is \"Joint CTC-Attention Based End-to-End Speech Recognition Using Multi-task Learning\", whereas this paper's title is \"Joint CTC-attention End-to-end Speech Recognition\". The title appears overly broad, and a more specific title that highlights the primary contributions of this paper in relation to existing publications would be more suitable, particularly given that [Kim et al., 2016] will be officially published prior to this work.\n(b) Introduction:\nThe authors claim to propose leveraging the constrained CTC alignment in a hybrid CTC-attention system, attaching a CTC objective to an attention-based encoder network as a regularization, similar to [Kim et al., 2016]. However, this concept is not novel, as it originates from [Kim et al., 2016]. The discussion on the necessity of combining CTC and attention-based end-to-end ASR is also not new. Furthermore, the statement \"we propose... as proposed by [Kim et al., 2016]\" is awkward, as it implies re-proposing existing work rather than building upon it with additional extensions. It is essential to clearly state the original contributions of this paper and its position relative to existing literature.\n(c) Experimental Results:\nWhile Kim et al. [2016] applied their method to English tasks, this paper focuses on Japanese and Mandarin Chinese tasks. It would be intriguing to see a more detailed explanation of the specific challenges associated with these languages that may not be present in English, such as addressing multiple possible outputs (e.g., Kanji, Hiragana, and Katakana) given Japanese speech input without relying on linguistic resources. This could be a significant contribution of this paper.\n- General Discussion:\nIt is recommended to cite Ref [Kim et al., 2016] from the official IEEE ICASSP conference proceedings instead of the pre-published arXiv version: Kim, S., Hori, T., Watanabe, S., \"Joint CTC-Attention Based End-to-End Speech Recognition Using Multi-task Learning\", IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear."
        }
    ]
}