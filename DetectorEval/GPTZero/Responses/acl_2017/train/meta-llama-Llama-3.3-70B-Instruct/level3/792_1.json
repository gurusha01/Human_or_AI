{
    "version": "2025-01-09-base",
    "scanId": "75252b1d-51ba-4e57-8325-ea9fb57ea78d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9995538592338562,
                    "sentence": "This paper presents a systematic investigation of different context types and representations for learning word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993507862091064,
                    "sentence": "The authors evaluate various models on intrinsic property analysis, sequence labeling tasks, and text classification tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994485974311829,
                    "sentence": "The paper has a clear structure and good argumentation, making it an enjoyable read, especially in the initial sections.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990385174751282,
                    "sentence": "The authors address an important problem of incorporating word order information into word embeddings and propose an interesting solution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99974524974823,
                    "sentence": "The main contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998703598976135,
                    "sentence": "1. A systematic comparison of different context types (linear and dependency-based) and context representations (bound and unbound) for learning word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998708367347717,
                    "sentence": "2. An evaluation of the effectiveness of different word embedding models (GSG, GBOW, and GloVe) with different contexts on various tasks, including word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text classification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999111890792847,
                    "sentence": "3. The authors provide insights into the importance of context representations and context types for different tasks, highlighting that bound representation is essential for sequence labeling tasks, while unbound representation is more suitable for syntactic word analogy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998744130134583,
                    "sentence": "The strengths of this paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999642968177795,
                    "sentence": "1. The authors provide a comprehensive evaluation of different context types and representations, which sheds light on their importance for various tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999504089355469,
                    "sentence": "2. The paper presents a clear and well-structured argument, making it easy to follow and understand.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990665316581726,
                    "sentence": "3. The authors provide numerical results and visualizations to support their claims, making it easier to compare and analyze the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994550943374634,
                    "sentence": "However, there are some weaknesses in the paper:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995260834693909,
                    "sentence": "1. The results are inconsistent and unconvincing, with insufficient analysis of negative results and unclear experimental setup.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999826967716217,
                    "sentence": "2. The paper's quality deteriorates towards the end, with poor presentation and argumentation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998071193695068,
                    "sentence": "3. The motivation for learning representations for words and senses in a shared space is unclear and not discussed in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997642040252686,
                    "sentence": "4. The comparison in Table 4 is unfair due to different dimensionalities used.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997466802597046,
                    "sentence": "5. The claim that the proposed models are faster to train is unsupported and lacks evidence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9937204718589783,
                    "sentence": "Questions to authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9961926937103271,
                    "sentence": "1. Can you provide more details on the experimental setup and hyper-parameter tuning for each task?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9928340911865234,
                    "sentence": "2. How do you plan to address the inconsistencies in the results and provide a more convincing analysis?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9954450726509094,
                    "sentence": "3. Can you clarify the motivation for learning representations for words and senses in a shared space and its relevance to the paper?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.985008716583252,
                    "sentence": "4. How do you plan to improve the presentation and argumentation in the latter part of the paper?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9961636828644501,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9961636828644501,
                "mixed": 0.003836317135549872
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9961636828644501,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9961636828644501,
                    "human": 0,
                    "mixed": 0.003836317135549872
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a systematic investigation of different context types and representations for learning word embeddings. The authors evaluate various models on intrinsic property analysis, sequence labeling tasks, and text classification tasks. The paper has a clear structure and good argumentation, making it an enjoyable read, especially in the initial sections. The authors address an important problem of incorporating word order information into word embeddings and propose an interesting solution.\nThe main contributions of this work are:\n1. A systematic comparison of different context types (linear and dependency-based) and context representations (bound and unbound) for learning word embeddings.\n2. An evaluation of the effectiveness of different word embedding models (GSG, GBOW, and GloVe) with different contexts on various tasks, including word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text classification.\n3. The authors provide insights into the importance of context representations and context types for different tasks, highlighting that bound representation is essential for sequence labeling tasks, while unbound representation is more suitable for syntactic word analogy.\nThe strengths of this paper are:\n1. The authors provide a comprehensive evaluation of different context types and representations, which sheds light on their importance for various tasks.\n2. The paper presents a clear and well-structured argument, making it easy to follow and understand.\n3. The authors provide numerical results and visualizations to support their claims, making it easier to compare and analyze the results.\nHowever, there are some weaknesses in the paper:\n1. The results are inconsistent and unconvincing, with insufficient analysis of negative results and unclear experimental setup.\n2. The paper's quality deteriorates towards the end, with poor presentation and argumentation.\n3. The motivation for learning representations for words and senses in a shared space is unclear and not discussed in the paper.\n4. The comparison in Table 4 is unfair due to different dimensionalities used.\n5. The claim that the proposed models are faster to train is unsupported and lacks evidence.\nQuestions to authors:\n1. Can you provide more details on the experimental setup and hyper-parameter tuning for each task?\n2. How do you plan to address the inconsistencies in the results and provide a more convincing analysis?\n3. Can you clarify the motivation for learning representations for words and senses in a shared space and its relevance to the paper?\n4. How do you plan to improve the presentation and argumentation in the latter part of the paper?"
        }
    ]
}