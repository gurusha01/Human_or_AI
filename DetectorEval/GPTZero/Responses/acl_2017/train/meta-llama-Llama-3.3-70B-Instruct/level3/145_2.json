{
    "version": "2025-01-09-base",
    "scanId": "56de67ff-227d-4559-ae05-cc087fbf73a4",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.992061972618103,
                    "sentence": "This paper proposes a self-learning framework to learn bilingual word embedding mappings, which can work with as little as 25 word pairs or even an automatically generated list of numerals.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9920840263366699,
                    "sentence": "The method is an extension of existing mapping techniques and can be combined with any dictionary-based mapping technique.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9838355183601379,
                    "sentence": "The authors evaluate their method on bilingual lexicon induction and cross-lingual word similarity tasks, achieving competitive results with state-of-the-art systems that use much richer bilingual resources.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9909222722053528,
                    "sentence": "The main contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9933590292930603,
                    "sentence": "1. A simple self-learning framework that can learn high-quality bilingual word embeddings from limited bilingual evidence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9831016063690186,
                    "sentence": "2. The ability to work with very small seed dictionaries, making it possible to learn bilingual word embeddings without any real bilingual data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9732897281646729,
                    "sentence": "3. Competitive results with state-of-the-art systems on bilingual lexicon induction and cross-lingual word similarity tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9871190190315247,
                    "sentence": "The strengths of this paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9759765863418579,
                    "sentence": "1. The proposed self-learning framework is efficient and can be combined with any embedding mapping and dictionary induction technique.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9458214640617371,
                    "sentence": "2. The method achieves competitive results with state-of-the-art systems on bilingual lexicon induction and cross-lingual word similarity tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992024898529053,
                    "sentence": "3. The authors provide a detailed analysis of the optimization objective and show that the method is implicitly optimizing a meaningful objective function.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987789392471313,
                    "sentence": "The weaknesses of this paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990778565406799,
                    "sentence": "1. The lack of discussion on the selection of the number of Gaussian components (k) and its impact on performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987590909004211,
                    "sentence": "2. The use of spherical covariance matrices, which may not be the most effective choice.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9929533004760742,
                    "sentence": "3. Minor issues, such as a missing reference to Table 4 and an incomplete citation for Luong et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988006353378296,
                    "sentence": "Questions to the authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991094470024109,
                    "sentence": "1. How did you select the number of Gaussian components (k) and what is the impact of this choice on the performance of the method?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9961657524108887,
                    "sentence": "2. Have you considered using general diagonal covariance matrices instead of spherical covariance matrices?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997915506362915,
                    "sentence": "3. Can you provide more details on the optimization objective and how it is related to the self-learning framework?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9971054196357727,
                    "sentence": "Overall, this paper presents a simple yet effective self-learning framework for learning bilingual word embeddings from limited bilingual evidence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998396098613739,
                    "sentence": "The method achieves competitive results with state-of-the-art systems and has the potential to be used in a variety of applications, including machine translation and cross-lingual information retrieval.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.8905251832425527,
            "class_probabilities": {
                "human": 0.10604532311209736,
                "ai": 0.8905251832425527,
                "mixed": 0.00342949364535001
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8905251832425527,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8905251832425527,
                    "human": 0.10604532311209736,
                    "mixed": 0.00342949364535001
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a self-learning framework to learn bilingual word embedding mappings, which can work with as little as 25 word pairs or even an automatically generated list of numerals. The method is an extension of existing mapping techniques and can be combined with any dictionary-based mapping technique. The authors evaluate their method on bilingual lexicon induction and cross-lingual word similarity tasks, achieving competitive results with state-of-the-art systems that use much richer bilingual resources.\nThe main contributions of this work are:\n1. A simple self-learning framework that can learn high-quality bilingual word embeddings from limited bilingual evidence.\n2. The ability to work with very small seed dictionaries, making it possible to learn bilingual word embeddings without any real bilingual data.\n3. Competitive results with state-of-the-art systems on bilingual lexicon induction and cross-lingual word similarity tasks.\nThe strengths of this paper are:\n1. The proposed self-learning framework is efficient and can be combined with any embedding mapping and dictionary induction technique.\n2. The method achieves competitive results with state-of-the-art systems on bilingual lexicon induction and cross-lingual word similarity tasks.\n3. The authors provide a detailed analysis of the optimization objective and show that the method is implicitly optimizing a meaningful objective function.\nThe weaknesses of this paper are:\n1. The lack of discussion on the selection of the number of Gaussian components (k) and its impact on performance.\n2. The use of spherical covariance matrices, which may not be the most effective choice.\n3. Minor issues, such as a missing reference to Table 4 and an incomplete citation for Luong et al.\nQuestions to the authors:\n1. How did you select the number of Gaussian components (k) and what is the impact of this choice on the performance of the method?\n2. Have you considered using general diagonal covariance matrices instead of spherical covariance matrices?\n3. Can you provide more details on the optimization objective and how it is related to the self-learning framework? \nOverall, this paper presents a simple yet effective self-learning framework for learning bilingual word embeddings from limited bilingual evidence. The method achieves competitive results with state-of-the-art systems and has the potential to be used in a variety of applications, including machine translation and cross-lingual information retrieval."
        }
    ]
}