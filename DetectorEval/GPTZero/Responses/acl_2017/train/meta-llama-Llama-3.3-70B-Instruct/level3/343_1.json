{
    "version": "2025-01-09-base",
    "scanId": "23569995-87f4-4e6f-822e-0cfe1f6d270a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999411106109619,
                    "sentence": "This paper proposes a novel approach to zero pronoun resolution by automatically generating large-scale pseudo training data and utilizing a two-step training mechanism with an attention-based neural network model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997798204421997,
                    "sentence": "The paper is well-organized and provides detailed comparisons under various experimental settings, showing state-of-the-art performances.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996768832206726,
                    "sentence": "The main contributions of this paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997990131378174,
                    "sentence": "1. A simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997813105583191,
                    "sentence": "2. A two-step training approach, namely pre-training-then-adaptation, which benefits from both large-scale pseudo training data and task-specific data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997965693473816,
                    "sentence": "3. An attention-based neural network model for zero pronoun resolution, which achieves significant improvements over previous state-of-the-art systems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996603727340698,
                    "sentence": "The strengths of this paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996492266654968,
                    "sentence": "1. The proposed approach significantly outperforms previous state-of-the-art systems, with an absolute improvement of 3.1% F-score on OntoNotes 5.0 data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995618462562561,
                    "sentence": "2. The two-step training approach is effective in leveraging both pseudo training data and task-specific data, and can be easily adapted to other tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996300935745239,
                    "sentence": "3. The attention-based neural network model is well-designed and achieves significant improvements over previous systems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995927810668945,
                    "sentence": "The weaknesses of this paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997091293334961,
                    "sentence": "1. The comparison with previous supervised approaches may not be entirely fair, as the proposed approach uses a large amount of pseudo training data, which may give it an advantage over the baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995485544204712,
                    "sentence": "2. The use of additional \"gold-labeled\" dataset to pretrain character embeddings may also give the proposed method an unfair advantage over the baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999731719493866,
                    "sentence": "3. The paper could benefit from a more detailed analysis of the errors made by the proposed approach, to identify areas for further improvement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993927478790283,
                    "sentence": "Questions to authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997039437294006,
                    "sentence": "1. How do the authors plan to address the issue of unknown words, which is identified as a critical part in comprehending context?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996970295906067,
                    "sentence": "2. Can the authors provide more details on the two-step training approach, and how it can be adapted to other tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997969269752502,
                    "sentence": "3. How do the authors plan to explore other neural network architectures to make it more appropriate for zero pronoun resolution task?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a novel approach to zero pronoun resolution by automatically generating large-scale pseudo training data and utilizing a two-step training mechanism with an attention-based neural network model. The paper is well-organized and provides detailed comparisons under various experimental settings, showing state-of-the-art performances.\nThe main contributions of this paper are:\n1. A simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution.\n2. A two-step training approach, namely pre-training-then-adaptation, which benefits from both large-scale pseudo training data and task-specific data.\n3. An attention-based neural network model for zero pronoun resolution, which achieves significant improvements over previous state-of-the-art systems.\nThe strengths of this paper are:\n1. The proposed approach significantly outperforms previous state-of-the-art systems, with an absolute improvement of 3.1% F-score on OntoNotes 5.0 data.\n2. The two-step training approach is effective in leveraging both pseudo training data and task-specific data, and can be easily adapted to other tasks.\n3. The attention-based neural network model is well-designed and achieves significant improvements over previous systems.\nThe weaknesses of this paper are:\n1. The comparison with previous supervised approaches may not be entirely fair, as the proposed approach uses a large amount of pseudo training data, which may give it an advantage over the baselines.\n2. The use of additional \"gold-labeled\" dataset to pretrain character embeddings may also give the proposed method an unfair advantage over the baselines.\n3. The paper could benefit from a more detailed analysis of the errors made by the proposed approach, to identify areas for further improvement.\nQuestions to authors:\n1. How do the authors plan to address the issue of unknown words, which is identified as a critical part in comprehending context?\n2. Can the authors provide more details on the two-step training approach, and how it can be adapted to other tasks?\n3. How do the authors plan to explore other neural network architectures to make it more appropriate for zero pronoun resolution task?"
        }
    ]
}