{
    "version": "2025-01-09-base",
    "scanId": "06fc07ff-92a3-4993-963e-7ea684e24fd9",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999697804450989,
                    "sentence": "This paper proposes a framework for evaluating word embeddings based on data efficiency and simple supervised tasks, with the motivation of transfer learning and semi-supervised approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999557137489319,
                    "sentence": "The approach uses simple tasks like word similarity and word analogy, but the ranks of embeddings tend to be task-specific and change with the amount of training data used.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999786615371704,
                    "sentence": "The main contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999972403049469,
                    "sentence": "1. The proposal of a Gaussian mixture model to represent words, which can capture multiple distinct meanings and uncertainty information.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999594688415527,
                    "sentence": "2. The introduction of an energy-based max-margin objective to learn the parameters of the Gaussian mixture model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999686479568481,
                    "sentence": "3. The evaluation of the proposed model on several word similarity datasets and word entailment tasks, showing competitive or superior performance compared to existing models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999244809150696,
                    "sentence": "However, there are several weaknesses in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999891459941864,
                    "sentence": "1. A major weakness is that the evaluation approach does not propagate to end tasks like text classification, parsing, or machine translation, making it difficult to trust its usefulness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998270869255066,
                    "sentence": "2. The discussion on injective embeddings seems out-of-topic and does not add to the paper's understanding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999175667762756,
                    "sentence": "3. The experimental section is confusing and lacks clarity, with some results not being clearly explained or justified.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999915599822998,
                    "sentence": "4. The paper's conclusions, such as the suitability of purely unsupervised large-scale pretraining for NLP applications, are too bold and not supported by the evaluation approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999099373817444,
                    "sentence": "5. The use of off-the-shelf pre-trained embeddings limits the validity of the evaluation, and the manuscript needs proofreading to correct errors and improve clarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998365044593811,
                    "sentence": "The strongest arguments against the acceptance of this submission are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999578595161438,
                    "sentence": "1. The lack of extrinsic studies to validate the approach, which makes it difficult to evaluate the effectiveness of the proposed model in real-world applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999944269657135,
                    "sentence": "2. The need for a more controlled experimental setting, which would allow for a more thorough evaluation of the proposed model and its comparison to existing models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999644160270691,
                    "sentence": "3. The lack of clarity and justification in the experimental section, which makes it difficult to understand and replicate the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998908042907715,
                    "sentence": "Questions to the authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999772310256958,
                    "sentence": "1. How do the authors plan to address the lack of extrinsic studies to validate the approach?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99997478723526,
                    "sentence": "2. Can the authors provide more details on the experimental setting and the evaluation metrics used?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999690651893616,
                    "sentence": "3. How do the authors plan to improve the clarity and justification of the experimental section?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a framework for evaluating word embeddings based on data efficiency and simple supervised tasks, with the motivation of transfer learning and semi-supervised approaches. The approach uses simple tasks like word similarity and word analogy, but the ranks of embeddings tend to be task-specific and change with the amount of training data used. \nThe main contributions of this work are: \n1. The proposal of a Gaussian mixture model to represent words, which can capture multiple distinct meanings and uncertainty information.\n2. The introduction of an energy-based max-margin objective to learn the parameters of the Gaussian mixture model.\n3. The evaluation of the proposed model on several word similarity datasets and word entailment tasks, showing competitive or superior performance compared to existing models.\nHowever, there are several weaknesses in the paper. \n1. A major weakness is that the evaluation approach does not propagate to end tasks like text classification, parsing, or machine translation, making it difficult to trust its usefulness.\n2. The discussion on injective embeddings seems out-of-topic and does not add to the paper's understanding.\n3. The experimental section is confusing and lacks clarity, with some results not being clearly explained or justified.\n4. The paper's conclusions, such as the suitability of purely unsupervised large-scale pretraining for NLP applications, are too bold and not supported by the evaluation approach.\n5. The use of off-the-shelf pre-trained embeddings limits the validity of the evaluation, and the manuscript needs proofreading to correct errors and improve clarity.\nThe strongest arguments against the acceptance of this submission are:\n1. The lack of extrinsic studies to validate the approach, which makes it difficult to evaluate the effectiveness of the proposed model in real-world applications.\n2. The need for a more controlled experimental setting, which would allow for a more thorough evaluation of the proposed model and its comparison to existing models.\n3. The lack of clarity and justification in the experimental section, which makes it difficult to understand and replicate the results.\nQuestions to the authors:\n1. How do the authors plan to address the lack of extrinsic studies to validate the approach?\n2. Can the authors provide more details on the experimental setting and the evaluation metrics used?\n3. How do the authors plan to improve the clarity and justification of the experimental section?"
        }
    ]
}