{
    "version": "2025-01-09-base",
    "scanId": "2ab9a602-7cad-42af-b735-6e3f1995f8a6",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998932480812073,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999087452888489,
                    "sentence": "The paper proposes a novel approach to word representation learning (WRL) by incorporating sememe information from the HowNet knowledge base.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999316334724426,
                    "sentence": "The authors introduce a Sememe-Encoded Word Representation Learning (SE-WRL) model, which uses sememes to improve word representation learning and word sense induction tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999920666217804,
                    "sentence": "The model is evaluated on two tasks: word similarity and word analogy, and the results show that the proposed model outperforms competitive baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998893141746521,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998644590377808,
                    "sentence": "1. The paper proposes a novel approach to WRL by incorporating sememe information, which improves word representation learning and word sense induction tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998027682304382,
                    "sentence": "2. The authors introduce a SE-WRL model, which uses sememes to represent various senses of each word and proposes Sememe Attention to automatically select appropriate senses in contexts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997237324714661,
                    "sentence": "3. The model is evaluated on two tasks: word similarity and word analogy, and the results show that the proposed model outperforms competitive baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990382790565491,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997802376747131,
                    "sentence": "1. The paper proposes a novel approach to WRL, which incorporates sememe information to improve word representation learning and word sense induction tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999135732650757,
                    "sentence": "2. The authors provide a detailed evaluation of the proposed model on two tasks: word similarity and word analogy, and the results show that the proposed model outperforms competitive baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999322295188904,
                    "sentence": "3. The paper provides a case study to demonstrate the effectiveness of the proposed model in word sense disambiguation and word representation learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997878670692444,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999345600605011,
                    "sentence": "1. The paper lacks a quantitative evaluation of the proposed model on word sense induction tasks, and only provides a qualitative case study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995479583740234,
                    "sentence": "2. The selection of datasets for training and evaluation tasks is not clearly explained, and the choice of datasets makes it difficult to compare results to other works.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993875026702881,
                    "sentence": "3. The paper does not provide a clear comparison of the proposed model to other state-of-the-art models in WRL, and the results are not clearly presented in tables or figures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9319352507591248,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9867915511131287,
                    "sentence": "1. Can you provide a quantitative evaluation of the proposed model on word sense induction tasks, and compare the results to other state-of-the-art models?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9700570106506348,
                    "sentence": "2. Can you explain the selection of datasets for training and evaluation tasks, and provide a clear comparison of the results to other works?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999797344207764,
                    "sentence": "3. Can you provide a clear comparison of the proposed model to other state-of-the-art models in WRL, and present the results in tables or figures?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper proposes a novel approach to word representation learning (WRL) by incorporating sememe information from the HowNet knowledge base. The authors introduce a Sememe-Encoded Word Representation Learning (SE-WRL) model, which uses sememes to improve word representation learning and word sense induction tasks. The model is evaluated on two tasks: word similarity and word analogy, and the results show that the proposed model outperforms competitive baselines.\nMain Contributions\n1. The paper proposes a novel approach to WRL by incorporating sememe information, which improves word representation learning and word sense induction tasks.\n2. The authors introduce a SE-WRL model, which uses sememes to represent various senses of each word and proposes Sememe Attention to automatically select appropriate senses in contexts.\n3. The model is evaluated on two tasks: word similarity and word analogy, and the results show that the proposed model outperforms competitive baselines.\nStrengths\n1. The paper proposes a novel approach to WRL, which incorporates sememe information to improve word representation learning and word sense induction tasks.\n2. The authors provide a detailed evaluation of the proposed model on two tasks: word similarity and word analogy, and the results show that the proposed model outperforms competitive baselines.\n3. The paper provides a case study to demonstrate the effectiveness of the proposed model in word sense disambiguation and word representation learning.\nWeaknesses\n1. The paper lacks a quantitative evaluation of the proposed model on word sense induction tasks, and only provides a qualitative case study.\n2. The selection of datasets for training and evaluation tasks is not clearly explained, and the choice of datasets makes it difficult to compare results to other works.\n3. The paper does not provide a clear comparison of the proposed model to other state-of-the-art models in WRL, and the results are not clearly presented in tables or figures.\nQuestions to Authors\n1. Can you provide a quantitative evaluation of the proposed model on word sense induction tasks, and compare the results to other state-of-the-art models?\n2. Can you explain the selection of datasets for training and evaluation tasks, and provide a clear comparison of the results to other works?\n3. Can you provide a clear comparison of the proposed model to other state-of-the-art models in WRL, and present the results in tables or figures?"
        }
    ]
}