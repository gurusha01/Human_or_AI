{
    "version": "2025-01-09-base",
    "scanId": "16cdf9b3-531c-4a83-a1ca-f7a98094961e",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999231100082397,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996411204338074,
                    "sentence": "This paper presents a method to automatically generate sentences that encode a given number using the major system, a mnemonic device that maps each digit to a consonant phoneme.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997804164886475,
                    "sentence": "The authors propose several encoding models and compare them in a password memorability study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997534155845642,
                    "sentence": "The results show that a model combining part-of-speech sentence templates with an n-gram language model produces the most memorable password representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997599720954895,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999157786369324,
                    "sentence": "1. Novel Encoding Models: The authors propose several novel encoding models, including a sentence encoder model that combines part-of-speech sentence templates with an n-gram language model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999387860298157,
                    "sentence": "2. Password Memorability Study: The authors conduct a user study to evaluate the memorability of the generated sentences, providing insights into the effectiveness of their approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999111294746399,
                    "sentence": "3. Improvements over Existing Systems: The authors demonstrate that their approach outperforms existing systems for generating mnemonic encodings, including naive encodings and other corpus-based methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99997478723526,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999393224716187,
                    "sentence": "1. Effective Encoding Model: The sentence encoder model is shown to be effective in generating memorable password representations, outperforming other models in the study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999562501907349,
                    "sentence": "2. Well-Designed User Study: The user study is well-designed, with a clear methodology and statistically significant results that support the authors' claims.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999634623527527,
                    "sentence": "3. Comparison to Existing Systems: The authors provide a thorough comparison to existing systems, demonstrating the advantages of their approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999446868896484,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999223947525024,
                    "sentence": "1. Limited Scope: The study only evaluates the memorability of 8-digit numbers, which may not be representative of longer numbers or more complex passwords.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999006390571594,
                    "sentence": "2. Fraudulent Responses: The study suffered from fraudulent responses, which may have affected the results and limited the sample size.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999301433563232,
                    "sentence": "3. Greedy Approach: The sentence encoder model takes a greedy approach, which may not always produce the most optimal encoding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996643662452698,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999014139175415,
                    "sentence": "1. How do you plan to address the issue of fraudulent responses in future user studies?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998644590377808,
                    "sentence": "2. Can you provide more details on the dynamic programming approach you mentioned as a potential alternative to the greedy approach?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999037981033325,
                    "sentence": "3. How do you think the results of the study would change if you were to evaluate the memorability of longer numbers or more complex passwords?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThis paper presents a method to automatically generate sentences that encode a given number using the major system, a mnemonic device that maps each digit to a consonant phoneme. The authors propose several encoding models and compare them in a password memorability study. The results show that a model combining part-of-speech sentence templates with an n-gram language model produces the most memorable password representations.\nMain Contributions\n1. Novel Encoding Models: The authors propose several novel encoding models, including a sentence encoder model that combines part-of-speech sentence templates with an n-gram language model.\n2. Password Memorability Study: The authors conduct a user study to evaluate the memorability of the generated sentences, providing insights into the effectiveness of their approach.\n3. Improvements over Existing Systems: The authors demonstrate that their approach outperforms existing systems for generating mnemonic encodings, including naive encodings and other corpus-based methods.\nStrengths\n1. Effective Encoding Model: The sentence encoder model is shown to be effective in generating memorable password representations, outperforming other models in the study.\n2. Well-Designed User Study: The user study is well-designed, with a clear methodology and statistically significant results that support the authors' claims.\n3. Comparison to Existing Systems: The authors provide a thorough comparison to existing systems, demonstrating the advantages of their approach.\nWeaknesses\n1. Limited Scope: The study only evaluates the memorability of 8-digit numbers, which may not be representative of longer numbers or more complex passwords.\n2. Fraudulent Responses: The study suffered from fraudulent responses, which may have affected the results and limited the sample size.\n3. Greedy Approach: The sentence encoder model takes a greedy approach, which may not always produce the most optimal encoding.\nQuestions to Authors\n1. How do you plan to address the issue of fraudulent responses in future user studies?\n2. Can you provide more details on the dynamic programming approach you mentioned as a potential alternative to the greedy approach?\n3. How do you think the results of the study would change if you were to evaluate the memorability of longer numbers or more complex passwords?"
        }
    ]
}