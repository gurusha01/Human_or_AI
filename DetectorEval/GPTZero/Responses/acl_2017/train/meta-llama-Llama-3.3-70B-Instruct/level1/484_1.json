{
    "version": "2025-01-09-base",
    "scanId": "75c4c0d8-6394-4e8d-922a-5ab2dda75a64",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999898076057434,
                    "sentence": "This paper proposes a joint CTC-attention end-to-end automatic speech recognition (ASR) system, which combines the benefits of connectionist temporal classification (CTC) and attention-based methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999930262565613,
                    "sentence": "The main contributions of this work are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999920725822449,
                    "sentence": "1. Joint CTC-attention model: The authors propose a multi-task learning framework that combines CTC and attention-based objectives to train a single encoder network.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999909400939941,
                    "sentence": "This approach effectively utilizes the strengths of both methods, with CTC providing a monotonic alignment and attention allowing for flexible alignment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999901056289673,
                    "sentence": "2. Improved decoding strategy: The authors introduce a joint CTC-attention decoding strategy that rescores hypotheses using both CTC and attention-based probabilities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999943971633911,
                    "sentence": "This approach eliminates the need for heuristic search techniques, such as length penalties and coverage terms, and improves the overall performance of the system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999887347221375,
                    "sentence": "3. State-of-the-art results on Japanese and Mandarin ASR tasks: The proposed system achieves comparable or superior performance to conventional hybrid ASR systems on the Corpus of Spontaneous Japanese (CSJ) and HKUST Mandarin Chinese conversational telephone speech recognition (MTS) tasks, without using linguistic resources such as pronunciation dictionaries and language models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999849200248718,
                    "sentence": "The strengths of this paper include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999855756759644,
                    "sentence": "1. Effective combination of CTC and attention: The joint CTC-attention model effectively addresses the misalignment issues in attention-based ASR systems, while leveraging the strengths of CTC in providing a monotonic alignment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999878406524658,
                    "sentence": "2. Improved decoding strategy: The proposed decoding strategy eliminates the need for heuristic search techniques and improves the overall performance of the system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999206066131592,
                    "sentence": "3. State-of-the-art results: The system achieves comparable or superior performance to conventional hybrid ASR systems on two challenging ASR tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999409317970276,
                    "sentence": "The weaknesses of this paper include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999622702598572,
                    "sentence": "1. Limited analysis of the joint CTC-attention model: The authors could provide more detailed analysis of the joint CTC-attention model, including the impact of the hyperparameter 位 on the performance of the system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999598860740662,
                    "sentence": "2. Limited comparison to other end-to-end ASR systems: The authors could provide more comprehensive comparison to other end-to-end ASR systems, including those using different attention mechanisms or decoding strategies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999600052833557,
                    "sentence": "3. Limited discussion of the computational complexity: The authors could provide more detailed discussion of the computational complexity of the proposed system, including the training and decoding times.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998628497123718,
                    "sentence": "Questions to authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999716281890869,
                    "sentence": "1. How does the value of 位 affect the performance of the joint CTC-attention model, and what is the optimal value of 位 for different ASR tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999397993087769,
                    "sentence": "2. How does the proposed system perform on other ASR tasks, such as English ASR, and what are the challenges in applying the system to these tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999695420265198,
                    "sentence": "3. What are the potential applications of the proposed system, and how can it be integrated with other speech processing systems, such as speech synthesis or dialogue systems?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997862822885396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997862822885396,
                "mixed": 0.00021371771146045916
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997862822885396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997862822885396,
                    "human": 0,
                    "mixed": 0.00021371771146045916
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a joint CTC-attention end-to-end automatic speech recognition (ASR) system, which combines the benefits of connectionist temporal classification (CTC) and attention-based methods. The main contributions of this work are:\n1. Joint CTC-attention model: The authors propose a multi-task learning framework that combines CTC and attention-based objectives to train a single encoder network. This approach effectively utilizes the strengths of both methods, with CTC providing a monotonic alignment and attention allowing for flexible alignment.\n2. Improved decoding strategy: The authors introduce a joint CTC-attention decoding strategy that rescores hypotheses using both CTC and attention-based probabilities. This approach eliminates the need for heuristic search techniques, such as length penalties and coverage terms, and improves the overall performance of the system.\n3. State-of-the-art results on Japanese and Mandarin ASR tasks: The proposed system achieves comparable or superior performance to conventional hybrid ASR systems on the Corpus of Spontaneous Japanese (CSJ) and HKUST Mandarin Chinese conversational telephone speech recognition (MTS) tasks, without using linguistic resources such as pronunciation dictionaries and language models.\nThe strengths of this paper include:\n1. Effective combination of CTC and attention: The joint CTC-attention model effectively addresses the misalignment issues in attention-based ASR systems, while leveraging the strengths of CTC in providing a monotonic alignment.\n2. Improved decoding strategy: The proposed decoding strategy eliminates the need for heuristic search techniques and improves the overall performance of the system.\n3. State-of-the-art results: The system achieves comparable or superior performance to conventional hybrid ASR systems on two challenging ASR tasks.\nThe weaknesses of this paper include:\n1. Limited analysis of the joint CTC-attention model: The authors could provide more detailed analysis of the joint CTC-attention model, including the impact of the hyperparameter 位 on the performance of the system.\n2. Limited comparison to other end-to-end ASR systems: The authors could provide more comprehensive comparison to other end-to-end ASR systems, including those using different attention mechanisms or decoding strategies.\n3. Limited discussion of the computational complexity: The authors could provide more detailed discussion of the computational complexity of the proposed system, including the training and decoding times.\nQuestions to authors:\n1. How does the value of 位 affect the performance of the joint CTC-attention model, and what is the optimal value of 位 for different ASR tasks?\n2. How does the proposed system perform on other ASR tasks, such as English ASR, and what are the challenges in applying the system to these tasks?\n3. What are the potential applications of the proposed system, and how can it be integrated with other speech processing systems, such as speech synthesis or dialogue systems?"
        }
    ]
}