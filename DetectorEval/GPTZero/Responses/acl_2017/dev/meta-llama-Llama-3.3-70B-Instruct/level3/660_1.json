{
    "version": "2025-01-09-base",
    "scanId": "d5b3fd68-1f10-4c5d-88ad-832e6fce9ef8",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999871850013733,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999362230300903,
                    "sentence": "The paper presents a study on models of referential word meaning, which link visual object representations to lexical representations in a distributional vector space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999950110912323,
                    "sentence": "The authors propose three models: Direct Cross-Modal Mapping (TRANSFER), Lexical Mapping Through Individual Word Classifiers (WAC), and Word Prediction via Cross-Modal Similarity Mapping (SIM-WAP).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999277591705322,
                    "sentence": "The models are evaluated on a standard object naming task and a zero-shot naming task, where the goal is to predict object names without seeing any visual instances of the object during training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998346567153931,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999521374702454,
                    "sentence": "1. The paper introduces a new model, SIM-WAP, which combines ideas from cross-modal mapping and individual word classifiers, and shows that it outperforms the other two models in the zero-shot naming task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999755024909973,
                    "sentence": "2. The authors demonstrate that combining the predictions of different models can lead to improved performance in object naming tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999784827232361,
                    "sentence": "3. The paper provides a detailed analysis of the strengths and weaknesses of each model, highlighting the importance of considering both visual and lexical information when learning referential word meaning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999978244304657,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999943971633911,
                    "sentence": "1. The paper presents a thorough evaluation of the three models, including a comparison of their performance on different tasks and datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999650239944458,
                    "sentence": "2. The authors provide a detailed analysis of the results, highlighting the strengths and weaknesses of each model and discussing the implications for future research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995242357254028,
                    "sentence": "3. The paper introduces a new model, SIM-WAP, which shows promise in the zero-shot naming task and has the potential to be applied to other tasks in natural language processing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985769987106323,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988752603530884,
                    "sentence": "1. The paper could benefit from a more detailed discussion of the limitations of the current models and the potential avenues for future research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991140961647034,
                    "sentence": "2. The authors could provide more insight into the reasons behind the performance differences between the models, and how these differences can be addressed in future work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986494779586792,
                    "sentence": "3. The paper assumes a certain level of background knowledge in natural language processing and computer vision, which may make it difficult for readers without this background to fully understand the content.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9873495101928711,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999138593673706,
                    "sentence": "1. Can you provide more details on the implementation of the SIM-WAP model, and how it differs from the other two models?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984403252601624,
                    "sentence": "2. How do you plan to address the limitations of the current models, and what avenues for future research do you see as most promising?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993389248847961,
                    "sentence": "3. Can you provide more insight into the potential applications of the models presented in the paper, and how they can be used in real-world scenarios?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984930238596827,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984930238596827,
                "mixed": 0.001506976140317253
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984930238596827,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984930238596827,
                    "human": 0,
                    "mixed": 0.001506976140317253
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper presents a study on models of referential word meaning, which link visual object representations to lexical representations in a distributional vector space. The authors propose three models: Direct Cross-Modal Mapping (TRANSFER), Lexical Mapping Through Individual Word Classifiers (WAC), and Word Prediction via Cross-Modal Similarity Mapping (SIM-WAP). The models are evaluated on a standard object naming task and a zero-shot naming task, where the goal is to predict object names without seeing any visual instances of the object during training.\nMain Contributions\n1. The paper introduces a new model, SIM-WAP, which combines ideas from cross-modal mapping and individual word classifiers, and shows that it outperforms the other two models in the zero-shot naming task.\n2. The authors demonstrate that combining the predictions of different models can lead to improved performance in object naming tasks.\n3. The paper provides a detailed analysis of the strengths and weaknesses of each model, highlighting the importance of considering both visual and lexical information when learning referential word meaning.\nStrengths\n1. The paper presents a thorough evaluation of the three models, including a comparison of their performance on different tasks and datasets.\n2. The authors provide a detailed analysis of the results, highlighting the strengths and weaknesses of each model and discussing the implications for future research.\n3. The paper introduces a new model, SIM-WAP, which shows promise in the zero-shot naming task and has the potential to be applied to other tasks in natural language processing.\nWeaknesses\n1. The paper could benefit from a more detailed discussion of the limitations of the current models and the potential avenues for future research.\n2. The authors could provide more insight into the reasons behind the performance differences between the models, and how these differences can be addressed in future work.\n3. The paper assumes a certain level of background knowledge in natural language processing and computer vision, which may make it difficult for readers without this background to fully understand the content.\nQuestions to Authors\n1. Can you provide more details on the implementation of the SIM-WAP model, and how it differs from the other two models?\n2. How do you plan to address the limitations of the current models, and what avenues for future research do you see as most promising?\n3. Can you provide more insight into the potential applications of the models presented in the paper, and how they can be used in real-world scenarios?"
        }
    ]
}