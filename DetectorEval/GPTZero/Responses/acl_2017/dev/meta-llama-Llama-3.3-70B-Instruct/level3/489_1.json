{
    "version": "2025-01-09-base",
    "scanId": "6b3243a6-207b-409d-aa87-60d58e5ba834",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999968409538269,
                    "sentence": "This paper explores the concept of referential word meaning, which is the connection between a word's meaning and the referents it is applied to.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "The authors investigate three models of interfacing visual and distributional information to predict object names in a realistic referring expression generation task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999978542327881,
                    "sentence": "The models are: Direct Cross-Modal Mapping (TRANSFER), Lexical Mapping Through Individual Word Classifiers (WAC), and Word Prediction via Cross-Modal Similarity Mapping (SIM-WAP).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960660934448,
                    "sentence": "The paper's main contributions are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999975562095642,
                    "sentence": "1. The authors propose a new approach to learning referential word meaning by combining visual and lexical information, which improves over previous cross-modal mapping models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974370002747,
                    "sentence": "2. The authors evaluate the performance of the three models in a standard and zero-shot object naming task, and show that the SIM-WAP model outperforms the other two models in the zero-shot setup.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997615814209,
                    "sentence": "3. The authors provide insights into the strengths and weaknesses of each model, and discuss the potential benefits of combining them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999922513961792,
                    "sentence": "The strengths of the paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967813491821,
                    "sentence": "1. The authors tackle an interesting and important problem in the field of natural language processing and computer vision.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998943209648132,
                    "sentence": "2. The paper provides a thorough evaluation of the three models, including a detailed analysis of their performance in different scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997981786727905,
                    "sentence": "3. The authors discuss the potential applications of their approach in referring expression generation and other related tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995880126953125,
                    "sentence": "The weaknesses of the paper are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991528987884521,
                    "sentence": "1. The authors' hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge is not clearly stated and could be confusing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995614886283875,
                    "sentence": "2. The paper uses a small vocabulary and a limited dataset, which may limit the generality of the conclusions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993364214897156,
                    "sentence": "3. The authors could provide more information about the implementation details of the models and the experimental setup.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996622204780579,
                    "sentence": "Overall, the paper provides a valuable contribution to the field of natural language processing and computer vision, and the authors' approach has the potential to improve the performance of referring expression generation systems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8842289447784424,
                    "sentence": "Questions to authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9840987920761108,
                    "sentence": "1. Can you provide more information about the implementation details of the SIM-WAP model and how it is trained?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9747086763381958,
                    "sentence": "2. How do you plan to scale up the approach to larger test sets and more complex scenarios?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9849064350128174,
                    "sentence": "3. Can you provide more insights into the potential applications of the approach in other related tasks, such as image captioning and visual question answering?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9975565308353808,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9975565308353808,
                "mixed": 0.002443469164619287
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9975565308353808,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9975565308353808,
                    "human": 0,
                    "mixed": 0.002443469164619287
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper explores the concept of referential word meaning, which is the connection between a word's meaning and the referents it is applied to. The authors investigate three models of interfacing visual and distributional information to predict object names in a realistic referring expression generation task. The models are: Direct Cross-Modal Mapping (TRANSFER), Lexical Mapping Through Individual Word Classifiers (WAC), and Word Prediction via Cross-Modal Similarity Mapping (SIM-WAP).\nThe paper's main contributions are:\n1. The authors propose a new approach to learning referential word meaning by combining visual and lexical information, which improves over previous cross-modal mapping models.\n2. The authors evaluate the performance of the three models in a standard and zero-shot object naming task, and show that the SIM-WAP model outperforms the other two models in the zero-shot setup.\n3. The authors provide insights into the strengths and weaknesses of each model, and discuss the potential benefits of combining them.\nThe strengths of the paper are:\n1. The authors tackle an interesting and important problem in the field of natural language processing and computer vision.\n2. The paper provides a thorough evaluation of the three models, including a detailed analysis of their performance in different scenarios.\n3. The authors discuss the potential applications of their approach in referring expression generation and other related tasks.\nThe weaknesses of the paper are:\n1. The authors' hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge is not clearly stated and could be confusing.\n2. The paper uses a small vocabulary and a limited dataset, which may limit the generality of the conclusions.\n3. The authors could provide more information about the implementation details of the models and the experimental setup.\nOverall, the paper provides a valuable contribution to the field of natural language processing and computer vision, and the authors' approach has the potential to improve the performance of referring expression generation systems.\nQuestions to authors:\n1. Can you provide more information about the implementation details of the SIM-WAP model and how it is trained?\n2. How do you plan to scale up the approach to larger test sets and more complex scenarios?\n3. Can you provide more insights into the potential applications of the approach in other related tasks, such as image captioning and visual question answering?"
        }
    ]
}