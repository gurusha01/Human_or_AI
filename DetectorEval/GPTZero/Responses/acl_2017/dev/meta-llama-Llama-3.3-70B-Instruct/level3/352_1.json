{
    "version": "2025-01-09-base",
    "scanId": "099f8265-97db-46ea-aac2-c857bb6e8bec",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999995231628418,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999686479568481,
                    "sentence": "This paper explores models of referential word meaning that link visual object representations to lexical representations in a distributional vector space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999454021453857,
                    "sentence": "The authors compare three models: direct cross-modal mapping, lexical mapping through individual word classifiers, and word prediction via cross-modal similarity mapping.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999775886535645,
                    "sentence": "They evaluate these models on a standard object naming task and a zero-shot naming task, where the correct name is not seen during training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998931288719177,
                    "sentence": "The results show that combining the models leads to improved performance, and that the cross-modal similarity mapping model is particularly effective in the zero-shot setting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999819278717041,
                    "sentence": "Main Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999879598617554,
                    "sentence": "1. Introduction of a new model: The paper introduces a new model, SIM-WAP, which combines ideas from cross-modal mapping and individual word classifiers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999833106994629,
                    "sentence": "2. Evaluation of models on zero-shot naming task: The paper evaluates the models on a zero-shot naming task, which is a challenging task that requires the model to generalize to new, unseen object names.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999823570251465,
                    "sentence": "3. Analysis of model strengths and weaknesses: The paper provides a detailed analysis of the strengths and weaknesses of each model, including their ability to capture taxonomic aspects of object names and their performance on singular and plural nouns.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999778270721436,
                    "sentence": "Strengths",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999676942825317,
                    "sentence": "1. Improved performance on zero-shot naming task: The SIM-WAP model shows improved performance on the zero-shot naming task, particularly when evaluated on the full vocabulary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999690651893616,
                    "sentence": "2. Effective combination of models: The paper demonstrates that combining the models leads to improved performance, suggesting that they capture complementary aspects of referential word meaning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999798536300659,
                    "sentence": "3. Detailed analysis of model strengths and weaknesses: The paper provides a thorough analysis of the models' strengths and weaknesses, which can inform future research in this area.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996491074562073,
                    "sentence": "Weaknesses",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999476671218872,
                    "sentence": "1. Limited vocabulary size: The paper notes that the vocabulary size is relatively small, which may limit the generality of the conclusions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999073147773743,
                    "sentence": "2. Lack of contextual information: The paper does not consider contextual information during object naming, which may be an important factor in referential word meaning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999398589134216,
                    "sentence": "3. Need for further evaluation: The paper suggests that further evaluation is needed to establish the effectiveness of the models in different settings and to explore their limitations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997679591178894,
                    "sentence": "Questions to Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988609552383423,
                    "sentence": "1. How do the authors plan to address the limitation of the small vocabulary size in future work?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993048310279846,
                    "sentence": "2. Can the authors provide more details on how the SIM-WAP model is able to capture taxonomic aspects of object names?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99881911277771,
                    "sentence": "3. How do the authors think the models could be improved to capture contextual information during object naming?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997862822885396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997862822885396,
                "mixed": 0.00021371771146045916
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997862822885396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997862822885396,
                    "human": 0,
                    "mixed": 0.00021371771146045916
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThis paper explores models of referential word meaning that link visual object representations to lexical representations in a distributional vector space. The authors compare three models: direct cross-modal mapping, lexical mapping through individual word classifiers, and word prediction via cross-modal similarity mapping. They evaluate these models on a standard object naming task and a zero-shot naming task, where the correct name is not seen during training. The results show that combining the models leads to improved performance, and that the cross-modal similarity mapping model is particularly effective in the zero-shot setting.\nMain Contributions\n1. Introduction of a new model: The paper introduces a new model, SIM-WAP, which combines ideas from cross-modal mapping and individual word classifiers.\n2. Evaluation of models on zero-shot naming task: The paper evaluates the models on a zero-shot naming task, which is a challenging task that requires the model to generalize to new, unseen object names.\n3. Analysis of model strengths and weaknesses: The paper provides a detailed analysis of the strengths and weaknesses of each model, including their ability to capture taxonomic aspects of object names and their performance on singular and plural nouns.\nStrengths\n1. Improved performance on zero-shot naming task: The SIM-WAP model shows improved performance on the zero-shot naming task, particularly when evaluated on the full vocabulary.\n2. Effective combination of models: The paper demonstrates that combining the models leads to improved performance, suggesting that they capture complementary aspects of referential word meaning.\n3. Detailed analysis of model strengths and weaknesses: The paper provides a thorough analysis of the models' strengths and weaknesses, which can inform future research in this area.\nWeaknesses\n1. Limited vocabulary size: The paper notes that the vocabulary size is relatively small, which may limit the generality of the conclusions.\n2. Lack of contextual information: The paper does not consider contextual information during object naming, which may be an important factor in referential word meaning.\n3. Need for further evaluation: The paper suggests that further evaluation is needed to establish the effectiveness of the models in different settings and to explore their limitations.\nQuestions to Authors\n1. How do the authors plan to address the limitation of the small vocabulary size in future work?\n2. Can the authors provide more details on how the SIM-WAP model is able to capture taxonomic aspects of object names?\n3. How do the authors think the models could be improved to capture contextual information during object naming?"
        }
    ]
}