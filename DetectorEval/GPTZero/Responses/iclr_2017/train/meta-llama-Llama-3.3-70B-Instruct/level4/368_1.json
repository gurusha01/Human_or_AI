{
    "version": "2025-01-09-base",
    "scanId": "ddc588a2-0d33-4334-9b96-0644f2322bc5",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999555945396423,
                    "sentence": "This paper presents a quantitative assessment framework for decoder-based generative models utilizing Annealed Importance Sampling (AIS) to estimate log-likelihoods, addressing the need for such evaluations as qualitative assessments of samples are still prevalent for models like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999340772628784,
                    "sentence": "Although methods like Kernel Density Estimation (KDE) exist, the authors demonstrate AIS's superior accuracy and its capability for fine-grained comparisons between generative models, including GANs, GMMNs, and Variational Autoencoders (VAEs).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999929666519165,
                    "sentence": "The authors provide empirical results comparing two decoder architectures trained on the continuous MNIST dataset using VAE, GAN, and GMMN objectives.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999928891658783,
                    "sentence": "Additionally, they train an Importance Weighted Autoencoder (IWAE) on binarized MNIST, showing that the IWAE bound significantly underestimates true log-likelihoods by at least 1 nat according to the AIS evaluation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999532699584961,
                    "sentence": "The strengths of this paper include its public evaluation framework, which is a valuable contribution to the community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999012351036072,
                    "sentence": "It offers insights into GAN behavior from a log-likelihood perspective, challenging the hypothesis that GANs memorize training data and highlighting their tendency to miss important modes in the data distribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998316764831543,
                    "sentence": "However, several aspects require clarification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999186992645264,
                    "sentence": "The inconsistent use of different example numbers (100, 1000, 10000) from various sources (train set, validation set, test set, or simulated/generated by the model) in experiments is puzzling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997290372848511,
                    "sentence": "For instance, Table 2 only reports results using a subset of examples from the testing set.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998844265937805,
                    "sentence": "Furthermore, the comparison between AIS and AIS+encoder in Figure 2c, where AIS appears slower, lacks clear explanation, particularly regarding the number of intermediate distributions used in each case.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999856173992157,
                    "sentence": "The choice of 16 independent chains for AIS seems low compared to literature standards, which often employ 100 chains.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996789693832397,
                    "sentence": "Increasing the number of chains could potentially tighten the confidence intervals reported in Table 2.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975141286849976,
                    "sentence": "The significant BDMC gap of 10 nats for GAN50, an order of magnitude larger than others, warrants further intuitive explanation from the authors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.06789163500070572,
                    "sentence": "Minor issues include the lack of reference and descriptive column explanations for Table 1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.0373227596282959,
                    "sentence": "Figure 2(a) requires clarification on whether the reported values represent the average log-likelihood of 100 individual or total training and validation examples of MNIST.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.04336501657962799,
                    "sentence": "Figure 2(c), presumably conducted on binarized MNIST, shows fewer points for AIS than for IWAE and AIS+encoder without clear justification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.055351823568344116,
                    "sentence": "The BDMC gaps mentioned in Section 5.3.1 should be explicitly confirmed as those reported in Table 2.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.05288270488381386,
                    "sentence": "Lastly, a typo in the caption of Figure 3 incorrectly labels a graph as \"(c) GMMN-10\" when it actually represents GMMN-50.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                }
            ],
            "completely_generated_prob": 0.9658502932045533,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9658502932045533,
                "mixed": 0.034149706795446697
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9658502932045533,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9658502932045533,
                    "human": 0,
                    "mixed": 0.034149706795446697
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a quantitative assessment framework for decoder-based generative models utilizing Annealed Importance Sampling (AIS) to estimate log-likelihoods, addressing the need for such evaluations as qualitative assessments of samples are still prevalent for models like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs). Although methods like Kernel Density Estimation (KDE) exist, the authors demonstrate AIS's superior accuracy and its capability for fine-grained comparisons between generative models, including GANs, GMMNs, and Variational Autoencoders (VAEs).\nThe authors provide empirical results comparing two decoder architectures trained on the continuous MNIST dataset using VAE, GAN, and GMMN objectives. Additionally, they train an Importance Weighted Autoencoder (IWAE) on binarized MNIST, showing that the IWAE bound significantly underestimates true log-likelihoods by at least 1 nat according to the AIS evaluation.\nThe strengths of this paper include its public evaluation framework, which is a valuable contribution to the community. It offers insights into GAN behavior from a log-likelihood perspective, challenging the hypothesis that GANs memorize training data and highlighting their tendency to miss important modes in the data distribution.\nHowever, several aspects require clarification. The inconsistent use of different example numbers (100, 1000, 10000) from various sources (train set, validation set, test set, or simulated/generated by the model) in experiments is puzzling. For instance, Table 2 only reports results using a subset of examples from the testing set. Furthermore, the comparison between AIS and AIS+encoder in Figure 2c, where AIS appears slower, lacks clear explanation, particularly regarding the number of intermediate distributions used in each case.\nThe choice of 16 independent chains for AIS seems low compared to literature standards, which often employ 100 chains. Increasing the number of chains could potentially tighten the confidence intervals reported in Table 2. The significant BDMC gap of 10 nats for GAN50, an order of magnitude larger than others, warrants further intuitive explanation from the authors.\nMinor issues include the lack of reference and descriptive column explanations for Table 1. Figure 2(a) requires clarification on whether the reported values represent the average log-likelihood of 100 individual or total training and validation examples of MNIST. Figure 2(c), presumably conducted on binarized MNIST, shows fewer points for AIS than for IWAE and AIS+encoder without clear justification. The BDMC gaps mentioned in Section 5.3.1 should be explicitly confirmed as those reported in Table 2. Lastly, a typo in the caption of Figure 3 incorrectly labels a graph as \"(c) GMMN-10\" when it actually represents GMMN-50."
        }
    ]
}