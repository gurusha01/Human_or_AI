{
    "version": "2025-01-09-base",
    "scanId": "d0f4caf7-99fc-47c2-8803-d9670c4d553d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998570680618286,
                    "sentence": "This paper presents a novel approach to substantially increase the number of parameters in a single layer while maintaining computational efficiency comparable to, or even surpassing, current state-of-the-art (SOTA) models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998260140419006,
                    "sentence": "The core concept revolves around leveraging a large mixture of experts (MoE), comprising small networks, where only a subset is dynamically activated through a gating network.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998522996902466,
                    "sentence": "Although the idea appears straightforward, the primary innovation lies in the design of the gating network, which is optimized to achieve two key objectives: maximizing the utilization of all available experts (importance) and ensuring a fair distribution of computational load across them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999742329120636,
                    "sentence": "Furthermore, the paper introduces two techniques aimed at increasing the batch size processed by each expert, thereby maximizing parallelization on GPUs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995912313461304,
                    "sentence": "Experimental results demonstrate the efficacy of the proposed approach when applied to recurrent neural networks (RNNs) in language modeling tasks, yielding superior performance to SOTA models with significantly reduced computation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994390606880188,
                    "sentence": "This is attributed to the selective utilization of a substantially larger number of parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990881681442261,
                    "sentence": "Additionally, results from machine translation experiments show that a model with over 30 times the number of parameters can outperform SOTA models while incurring only half of the effective computation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998887300491333,
                    "sentence": "I have several suggestions for improving the paper:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997233748435974,
                    "sentence": "- The authors could enhance the presentation by providing a more concise and intuitive explanation, particularly in Section 3.2, which is pivotal to the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999329149723053,
                    "sentence": "For instance, equation 8 warrants a more detailed description.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995516538619995,
                    "sentence": "To achieve this, the authors could relocate experimental details, such as architecture and training specifics, to the appendix, allowing for a more focused narrative.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988582134246826,
                    "sentence": "Reorganizing the experiment section to complete each experiment before proceeding to the next would also improve clarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989331364631653,
                    "sentence": "Moreover, there are minor writing errors, such as at the end of Section 3.1, that require attention.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991719126701355,
                    "sentence": "- The paper lacks crucial references related to conditional computation, which should be incorporated to provide a more comprehensive context for the research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a novel approach to substantially increase the number of parameters in a single layer while maintaining computational efficiency comparable to, or even surpassing, current state-of-the-art (SOTA) models. The core concept revolves around leveraging a large mixture of experts (MoE), comprising small networks, where only a subset is dynamically activated through a gating network. Although the idea appears straightforward, the primary innovation lies in the design of the gating network, which is optimized to achieve two key objectives: maximizing the utilization of all available experts (importance) and ensuring a fair distribution of computational load across them.\nFurthermore, the paper introduces two techniques aimed at increasing the batch size processed by each expert, thereby maximizing parallelization on GPUs. Experimental results demonstrate the efficacy of the proposed approach when applied to recurrent neural networks (RNNs) in language modeling tasks, yielding superior performance to SOTA models with significantly reduced computation. This is attributed to the selective utilization of a substantially larger number of parameters. Additionally, results from machine translation experiments show that a model with over 30 times the number of parameters can outperform SOTA models while incurring only half of the effective computation.\nI have several suggestions for improving the paper:\n- The authors could enhance the presentation by providing a more concise and intuitive explanation, particularly in Section 3.2, which is pivotal to the paper. For instance, equation 8 warrants a more detailed description. To achieve this, the authors could relocate experimental details, such as architecture and training specifics, to the appendix, allowing for a more focused narrative. Reorganizing the experiment section to complete each experiment before proceeding to the next would also improve clarity. Moreover, there are minor writing errors, such as at the end of Section 3.1, that require attention.\n- The paper lacks crucial references related to conditional computation, which should be incorporated to provide a more comprehensive context for the research."
        }
    ]
}