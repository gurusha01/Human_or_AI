{
    "version": "2025-01-09-base",
    "scanId": "85744431-eae3-4fcf-bb2c-6baa279aa7d4",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999839067459106,
                    "sentence": "This manuscript explores the application of normalizing flows (NFs) to maximum entropy constrained optimization, enabling the construction of complex densities with tractable likelihoods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999909400939941,
                    "sentence": "The presentation is clear and easy to follow, making the content accessible to readers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999822378158569,
                    "sentence": "However, the novelty of the paper is somewhat limited.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999865293502808,
                    "sentence": "The primary contributions consist of (1) extending existing NF research to the domain of MaxEnt estimation and (2) addressing optimization challenges arising from stochastic approximations of E[\"\"T\"\"] in conjunction with the annealing of Lagrange multipliers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999884963035583,
                    "sentence": "The application of NFs to MaxEnt, in itself, is not particularly innovative, as similar frameworks have been established in prior work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999868273735046,
                    "sentence": "For example, an equivalent loss to the one presented in equation (6) could be derived by minimizing the Kullback-Leibler divergence between the variational distribution p{\\phi} and the unnormalized likelihood f, where f is proportional to exp(\\sumk(-\\lambdak T - ck \"\"T_k\"\"^2)).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999993085861206,
                    "sentence": "This approach is characteristic of previous studies utilizing NFs for variational inference.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999777674674988,
                    "sentence": "To further substantiate the paper's findings, additional experiments involving more complex datasets would be beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99997878074646,
                    "sentence": "While the two provided experiments yield promising results, they are limited to toy problems, which may not adequately represent real-world scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999752044677734,
                    "sentence": "A minor suggestion is to include a brief discussion of step 8 in Algorithm 1, as this would provide additional clarity and intuition, despite the overall intuitiveness of the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This manuscript explores the application of normalizing flows (NFs) to maximum entropy constrained optimization, enabling the construction of complex densities with tractable likelihoods. The presentation is clear and easy to follow, making the content accessible to readers.\nHowever, the novelty of the paper is somewhat limited. The primary contributions consist of (1) extending existing NF research to the domain of MaxEnt estimation and (2) addressing optimization challenges arising from stochastic approximations of E[\"\"T\"\"] in conjunction with the annealing of Lagrange multipliers. The application of NFs to MaxEnt, in itself, is not particularly innovative, as similar frameworks have been established in prior work. For example, an equivalent loss to the one presented in equation (6) could be derived by minimizing the Kullback-Leibler divergence between the variational distribution p{\\phi} and the unnormalized likelihood f, where f is proportional to exp(\\sumk(-\\lambdak T - ck \"\"T_k\"\"^2)). This approach is characteristic of previous studies utilizing NFs for variational inference.\nTo further substantiate the paper's findings, additional experiments involving more complex datasets would be beneficial. While the two provided experiments yield promising results, they are limited to toy problems, which may not adequately represent real-world scenarios.\nA minor suggestion is to include a brief discussion of step 8 in Algorithm 1, as this would provide additional clarity and intuition, despite the overall intuitiveness of the approach."
        }
    ]
}