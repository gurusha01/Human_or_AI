{
    "version": "2025-01-09-base",
    "scanId": "eab363b6-d10e-4d72-a25f-e6f55d0267a2",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9993266463279724,
                    "sentence": "This study leverages the successes of deep reinforcement learning (RL) in large state spaces, where deep neural networks serve as function approximators, to introduce a novel algorithm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990571737289429,
                    "sentence": "The algorithm demonstrates superior performance in unfamiliar 3D environments using raw sensory data and exhibits enhanced generalization across various goals and environments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985306262969971,
                    "sentence": "Notably, it emerged as the winner of the Visual Doom AI competition.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992427229881287,
                    "sentence": "The core concept of the algorithm involves utilizing supplementary low-dimensional observations, such as ammo or health provided by the game engine, as a supervised target for prediction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985506534576416,
                    "sentence": "This prediction is conditioned on a predefined goal vector and the current action.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987892508506775,
                    "sentence": "Upon training, the optimal action for the current state is selected as the action that maximizes the predicted outcome according to the goal.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983842968940735,
                    "sentence": "In contrast to successor feature representations, the learning process is supervised, and there is no temporal difference (TD) relationship between the predictions of the current and next states.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998363196849823,
                    "sentence": "The authors build upon prior works that predict future states as part of RL and goal-driven function approximators, which are reviewed in section 2.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9969701170921326,
                    "sentence": "The key contributions of this study include the focus on Monte Carlo estimation, the utilization of low-dimensional measurements for prediction, parametrized goals, and a comprehensive empirical comparison with relevant prior works.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.996512770652771,
                    "sentence": "In addition to the Visual Doom AI comparison, the authors demonstrate that their algorithm can learn generalizable policies that respond to limited goal changes without requiring further training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9942835569381714,
                    "sentence": "The paper is well-written, and the empirical results are compelling, making it a significant contribution to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9946393370628357,
                    "sentence": "Some potential minor improvements include discussing the approximation in supervised training, which assumes an on-policy scenario while learning from a replay buffer.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9969084858894348,
                    "sentence": "The algorithm's reliance on additional metadata, such as information about predictable parts of the sensory input, should be more clearly acknowledged, along with the limitations of this approach, including its potential ineffectiveness in environments lacking such measurements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This study leverages the successes of deep reinforcement learning (RL) in large state spaces, where deep neural networks serve as function approximators, to introduce a novel algorithm. The algorithm demonstrates superior performance in unfamiliar 3D environments using raw sensory data and exhibits enhanced generalization across various goals and environments. Notably, it emerged as the winner of the Visual Doom AI competition.\nThe core concept of the algorithm involves utilizing supplementary low-dimensional observations, such as ammo or health provided by the game engine, as a supervised target for prediction. This prediction is conditioned on a predefined goal vector and the current action. Upon training, the optimal action for the current state is selected as the action that maximizes the predicted outcome according to the goal. In contrast to successor feature representations, the learning process is supervised, and there is no temporal difference (TD) relationship between the predictions of the current and next states.\nThe authors build upon prior works that predict future states as part of RL and goal-driven function approximators, which are reviewed in section 2. The key contributions of this study include the focus on Monte Carlo estimation, the utilization of low-dimensional measurements for prediction, parametrized goals, and a comprehensive empirical comparison with relevant prior works.\nIn addition to the Visual Doom AI comparison, the authors demonstrate that their algorithm can learn generalizable policies that respond to limited goal changes without requiring further training. The paper is well-written, and the empirical results are compelling, making it a significant contribution to the field.\nSome potential minor improvements include discussing the approximation in supervised training, which assumes an on-policy scenario while learning from a replay buffer. The algorithm's reliance on additional metadata, such as information about predictable parts of the sensory input, should be more clearly acknowledged, along with the limitations of this approach, including its potential ineffectiveness in environments lacking such measurements."
        }
    ]
}