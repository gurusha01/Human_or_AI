{
    "version": "2025-01-09-base",
    "scanId": "1ee413c4-c45c-40bb-a3ab-5f69803dcca7",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9994343519210815,
                    "sentence": "This manuscript presents a novel framework for predicting future frames in video sequences by separating motion and content into distinct encoding pathways, supplemented by multi-scale residual connections.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991428256034851,
                    "sentence": "The authors demonstrate the efficacy of their approach through qualitative and quantitative evaluations on the KTH, Weizmann, and UCF-101 datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993352890014648,
                    "sentence": "The concept of disentangling motion and content is intriguing and appears to yield favorable outcomes for this specific task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989139437675476,
                    "sentence": "Nevertheless, the originality of this contribution is somewhat tempered by the existence of prior research on multi-stream networks, and it remains unclear whether this particular disentanglement strategy offers significant benefits or has far-reaching implications beyond the realm of future frame prediction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984108209609985,
                    "sentence": "The results obtained on the KTH and Weizmann datasets are compelling, exhibiting substantial improvements over baseline performances.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985430240631104,
                    "sentence": "In contrast, the outcomes on the more diverse UCF-101 dataset are less remarkable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990225434303284,
                    "sentence": "Furthermore, the qualitative examples provided for UCF-101 are unconvincing, as previously discussed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994773864746094,
                    "sentence": "In summary, this is a well-crafted study built around an interesting, albeit not highly innovative, idea.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994900822639465,
                    "sentence": "To enhance the paper's impact, it would be beneficial to demonstrate the broader applicability of the motion-content decoupling approach to other video-related tasks, thereby mitigating concerns regarding its limited novelty and potential restricted scope.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This manuscript presents a novel framework for predicting future frames in video sequences by separating motion and content into distinct encoding pathways, supplemented by multi-scale residual connections. The authors demonstrate the efficacy of their approach through qualitative and quantitative evaluations on the KTH, Weizmann, and UCF-101 datasets.\nThe concept of disentangling motion and content is intriguing and appears to yield favorable outcomes for this specific task. Nevertheless, the originality of this contribution is somewhat tempered by the existence of prior research on multi-stream networks, and it remains unclear whether this particular disentanglement strategy offers significant benefits or has far-reaching implications beyond the realm of future frame prediction.\nThe results obtained on the KTH and Weizmann datasets are compelling, exhibiting substantial improvements over baseline performances. In contrast, the outcomes on the more diverse UCF-101 dataset are less remarkable. Furthermore, the qualitative examples provided for UCF-101 are unconvincing, as previously discussed.\nIn summary, this is a well-crafted study built around an interesting, albeit not highly innovative, idea. To enhance the paper's impact, it would be beneficial to demonstrate the broader applicability of the motion-content decoupling approach to other video-related tasks, thereby mitigating concerns regarding its limited novelty and potential restricted scope."
        }
    ]
}