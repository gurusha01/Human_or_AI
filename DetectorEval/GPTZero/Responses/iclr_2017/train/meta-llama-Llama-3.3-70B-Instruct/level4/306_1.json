{
    "version": "2025-01-09-base",
    "scanId": "c73281c6-dab8-4978-9618-c10666ee8ce1",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9993786215782166,
                    "sentence": "This manuscript proposes a meta-learning framework based on Long Short-Term Memory (LSTM) networks to learn the optimization algorithm of another learning algorithm, specifically a neural network (NN).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992453455924988,
                    "sentence": "The paper is well-structured and clearly presents its main contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992865920066833,
                    "sentence": "The core idea of establishing a connection between the Robbins-Monro update rule and the LSTM update rule to fulfill the two primary requirements of few-shot learning, namely rapid acquisition of new knowledge and slower extraction of generalizable knowledge, is particularly interesting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989264607429504,
                    "sentence": "The incorporation of various techniques, such as parameter sharing and normalization, inspired by previous work (Andrychowicz et al., 2016), as well as novel design choices, including a specific implementation of batch normalization, are well-justified.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990028142929077,
                    "sentence": "The experimental results are convincing, making this a strong paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9958785176277161,
                    "sentence": "However, I have a few concerns and questions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.996907114982605,
                    "sentence": "1. Is it necessary to use loss, gradient, and parameters as inputs to the meta-learner, or could simpler combinations be sufficient?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9965950846672058,
                    "sentence": "Were ablation studies conducted to investigate this?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9969878792762756,
                    "sentence": "2. It would be beneficial to explore the possibility of learning other architectural components of the network in a similar manner, such as the number of neurons or type of units.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986857771873474,
                    "sentence": "Do the authors have any thoughts on this?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977043867111206,
                    "sentence": "3. The related work section, which primarily focuses on meta-learning, could be more comprehensive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9976078867912292,
                    "sentence": "Meta-learning has a long history, and similar approaches have been proposed to address the same problem, even if they did not utilize LSTMs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985224008560181,
                    "sentence": "Notable examples include Samy Bengio's PhD thesis (1989) and the use of genetic programming for searching new learning rules for neural networks (S. Bengio, Y. Bengio, and J. Cloutier, 1994).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9981088638305664,
                    "sentence": "I am confident that Schmidhuber has also made relevant contributions, and I recommend updating the related work section to include these.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979601502418518,
                    "sentence": "Overall, I find the paper engaging, and I believe the discussed material is relevant to a broad audience at ICLR.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This manuscript proposes a meta-learning framework based on Long Short-Term Memory (LSTM) networks to learn the optimization algorithm of another learning algorithm, specifically a neural network (NN). The paper is well-structured and clearly presents its main contributions. The core idea of establishing a connection between the Robbins-Monro update rule and the LSTM update rule to fulfill the two primary requirements of few-shot learning, namely rapid acquisition of new knowledge and slower extraction of generalizable knowledge, is particularly interesting.\nThe incorporation of various techniques, such as parameter sharing and normalization, inspired by previous work (Andrychowicz et al., 2016), as well as novel design choices, including a specific implementation of batch normalization, are well-justified. The experimental results are convincing, making this a strong paper. However, I have a few concerns and questions:\n1. Is it necessary to use loss, gradient, and parameters as inputs to the meta-learner, or could simpler combinations be sufficient? Were ablation studies conducted to investigate this?\n2. It would be beneficial to explore the possibility of learning other architectural components of the network in a similar manner, such as the number of neurons or type of units. Do the authors have any thoughts on this?\n3. The related work section, which primarily focuses on meta-learning, could be more comprehensive. Meta-learning has a long history, and similar approaches have been proposed to address the same problem, even if they did not utilize LSTMs. Notable examples include Samy Bengio's PhD thesis (1989) and the use of genetic programming for searching new learning rules for neural networks (S. Bengio, Y. Bengio, and J. Cloutier, 1994). I am confident that Schmidhuber has also made relevant contributions, and I recommend updating the related work section to include these.\nOverall, I find the paper engaging, and I believe the discussed material is relevant to a broad audience at ICLR."
        }
    ]
}