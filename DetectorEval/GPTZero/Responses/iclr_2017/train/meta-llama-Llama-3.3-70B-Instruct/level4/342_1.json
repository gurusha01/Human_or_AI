{
    "version": "2025-01-09-base",
    "scanId": "b90754f2-57f1-4047-94e1-0094daa6d7a6",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9958327412605286,
                    "sentence": "This study integrates variational recurrent neural networks and adversarial neural networks to address domain adaptation in time series data, with comparisons to several competing algorithms on two healthcare datasets derived from MIMIC-III in domain adaptation scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9945349097251892,
                    "sentence": "The novel contribution of this research is somewhat limited, as it primarily extends variational recurrent neural networks with adversarial training to learn domain-agnostic representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9920212030410767,
                    "sentence": "Although the experimental results demonstrate the proposed method's superior performance over competing algorithms, the source of this advantage is unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.994926393032074,
                    "sentence": "The key difference between the proposed method and R-DANN lies in the use of variational RNN versus traditional RNN, yet the study provides little insight into how this difference yields significant performance disparities and substantial variations in captured temporal dependencies, as evident in Figure 4.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9904259443283081,
                    "sentence": "Detailed comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9962392449378967,
                    "sentence": "1. The description of Figure 1 requires further clarification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9953844547271729,
                    "sentence": "Specifically, it is essential to confirm whether subplot 1(b) represents the t-SNE projection of representations learned by DANN or R-DANN, as the text in section 4.4 suggests the latter.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9925850629806519,
                    "sentence": "The regularity of the VRADA plot is surprising.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9939401745796204,
                    "sentence": "Furthermore, an interpretation of the two dominant latent factors encoded in Figure 1(c) would be beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.994366466999054,
                    "sentence": "2. Table 2 reveals a notable discrepancy in performance between the two baseline methods when tested on the entire target dataset (including the validation set) versus the test set only.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9859471321105957,
                    "sentence": "In contrast, VRADA exhibits nearly identical performance in both settings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9916777610778809,
                    "sentence": "An explanation for this observation would be appreciated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.991142749786377,
                    "sentence": "3. A more detailed explanation of Figures 3 and 4 is necessary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9821170568466187,
                    "sentence": "Specifically, clarification on the interpretation of the x-axis in Figure 3 and the x and y axes in Figure 4 would be helpful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9903669357299805,
                    "sentence": "Additionally, the regularity of the rightmost plots in Figure 4, compared to those on the left, warrants further discussion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This study integrates variational recurrent neural networks and adversarial neural networks to address domain adaptation in time series data, with comparisons to several competing algorithms on two healthcare datasets derived from MIMIC-III in domain adaptation scenarios.\nThe novel contribution of this research is somewhat limited, as it primarily extends variational recurrent neural networks with adversarial training to learn domain-agnostic representations. Although the experimental results demonstrate the proposed method's superior performance over competing algorithms, the source of this advantage is unclear. The key difference between the proposed method and R-DANN lies in the use of variational RNN versus traditional RNN, yet the study provides little insight into how this difference yields significant performance disparities and substantial variations in captured temporal dependencies, as evident in Figure 4.\nDetailed comments:\n1. The description of Figure 1 requires further clarification. Specifically, it is essential to confirm whether subplot 1(b) represents the t-SNE projection of representations learned by DANN or R-DANN, as the text in section 4.4 suggests the latter. The regularity of the VRADA plot is surprising. Furthermore, an interpretation of the two dominant latent factors encoded in Figure 1(c) would be beneficial.\n2. Table 2 reveals a notable discrepancy in performance between the two baseline methods when tested on the entire target dataset (including the validation set) versus the test set only. In contrast, VRADA exhibits nearly identical performance in both settings. An explanation for this observation would be appreciated.\n3. A more detailed explanation of Figures 3 and 4 is necessary. Specifically, clarification on the interpretation of the x-axis in Figure 3 and the x and y axes in Figure 4 would be helpful. Additionally, the regularity of the rightmost plots in Figure 4, compared to those on the left, warrants further discussion."
        }
    ]
}