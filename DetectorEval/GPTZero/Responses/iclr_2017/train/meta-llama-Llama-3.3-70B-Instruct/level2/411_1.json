{
    "version": "2025-01-09-base",
    "scanId": "53656052-b4c8-4732-818b-f0a766edd79c",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999923706054688,
                    "sentence": "This paper presents a novel learning-based approach for code super-optimization, which is the task of transforming a given program into a more efficient version while preserving its input-output behavior.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999834895133972,
                    "sentence": "The authors propose to learn the proposal distribution used in stochastic search-based methods, such as Stoke, to improve the efficiency of the optimization process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999874830245972,
                    "sentence": "The learned proposal distribution is shown to outperform the state-of-the-art approaches on two datasets, including the Hacker's Delight benchmark and a dataset of automatically generated programs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999984860420227,
                    "sentence": "I decide to accept this paper, with the main reason being that it presents a well-motivated and well-executed approach to improving code super-optimization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999840259552002,
                    "sentence": "The paper clearly outlines the problem, provides a thorough review of related work, and presents a novel solution that is supported by experimental results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99998539686203,
                    "sentence": "The approach is well-motivated, as it addresses the limitation of current stochastic search-based methods, which do not learn from past behavior or leverage the semantics of the program under consideration.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999813437461853,
                    "sentence": "The authors provide a clear explanation of the learning objective, the parameterization of the proposal distribution, and the reinforcement learning framework used to estimate the parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999914169311523,
                    "sentence": "The experimental results are convincing, showing that the learned proposal distribution outperforms the state-of-the-art approaches on both datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999906420707703,
                    "sentence": "The results are also supported by a thorough analysis of the optimization process, including the evolution of the loss function and the comparison of the optimized programs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999907612800598,
                    "sentence": "To further improve the paper, I suggest that the authors provide more details on the implementation of the learning framework, including the choice of hyperparameters and the training procedure.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999900460243225,
                    "sentence": "Additionally, it would be interesting to see more examples of optimized programs and a more detailed analysis of the trade-offs between the different approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999901652336121,
                    "sentence": "Some questions I would like the authors to answer include: How do the authors plan to extend this approach to more complex programs and larger datasets?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999921917915344,
                    "sentence": "What are the potential applications of this approach beyond code super-optimization?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999869465827942,
                    "sentence": "How do the authors plan to address the issue of overfitting, which may occur when learning the proposal distribution from a limited dataset?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a novel learning-based approach for code super-optimization, which is the task of transforming a given program into a more efficient version while preserving its input-output behavior. The authors propose to learn the proposal distribution used in stochastic search-based methods, such as Stoke, to improve the efficiency of the optimization process. The learned proposal distribution is shown to outperform the state-of-the-art approaches on two datasets, including the Hacker's Delight benchmark and a dataset of automatically generated programs.\nI decide to accept this paper, with the main reason being that it presents a well-motivated and well-executed approach to improving code super-optimization. The paper clearly outlines the problem, provides a thorough review of related work, and presents a novel solution that is supported by experimental results.\nThe approach is well-motivated, as it addresses the limitation of current stochastic search-based methods, which do not learn from past behavior or leverage the semantics of the program under consideration. The authors provide a clear explanation of the learning objective, the parameterization of the proposal distribution, and the reinforcement learning framework used to estimate the parameters.\nThe experimental results are convincing, showing that the learned proposal distribution outperforms the state-of-the-art approaches on both datasets. The results are also supported by a thorough analysis of the optimization process, including the evolution of the loss function and the comparison of the optimized programs.\nTo further improve the paper, I suggest that the authors provide more details on the implementation of the learning framework, including the choice of hyperparameters and the training procedure. Additionally, it would be interesting to see more examples of optimized programs and a more detailed analysis of the trade-offs between the different approaches.\nSome questions I would like the authors to answer include: How do the authors plan to extend this approach to more complex programs and larger datasets? What are the potential applications of this approach beyond code super-optimization? How do the authors plan to address the issue of overfitting, which may occur when learning the proposal distribution from a limited dataset?"
        }
    ]
}