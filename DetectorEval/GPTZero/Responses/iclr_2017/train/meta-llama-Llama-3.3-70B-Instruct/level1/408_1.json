{
    "version": "2025-01-09-base",
    "scanId": "f3159832-b446-4f50-85e5-18841d7070fe",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.998378574848175,
                    "sentence": "Summary",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984472990036011,
                    "sentence": "The paper proposes a multi-view approach to learning acoustic word embeddings, where both acoustic sequences and their corresponding character sequences are jointly embedded into a common space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9970970153808594,
                    "sentence": "The authors use deep bidirectional LSTM embedding models and multi-view contrastive losses, including fixed-margin and cost-sensitive losses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998603880405426,
                    "sentence": "The approach is evaluated on several tasks, including acoustic word discrimination, cross-view word discrimination, and word similarity, and is shown to improve over previous approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9929555058479309,
                    "sentence": "Decision",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985274076461792,
                    "sentence": "I decide to Accept this paper, with two key reasons for this choice.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977297186851501,
                    "sentence": "Firstly, the paper tackles a specific and well-motivated problem in the field of speech processing, and the approach is well-placed in the literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987487196922302,
                    "sentence": "Secondly, the paper provides a thorough evaluation of the proposed approach, including a comparison with previous work and an analysis of the effects of different objectives and hyperparameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9965724945068359,
                    "sentence": "Supporting Arguments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986758828163147,
                    "sentence": "The paper provides a clear and well-written introduction to the problem of learning acoustic word embeddings, and motivates the need for a multi-view approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987476468086243,
                    "sentence": "The authors also provide a thorough review of related work, including previous approaches to learning acoustic word embeddings and multi-view representation learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9962747097015381,
                    "sentence": "The experimental evaluation is comprehensive, including a comparison with previous work and an analysis of the effects of different objectives and hyperparameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992290735244751,
                    "sentence": "The results show that the proposed approach improves over previous work on acoustic word discrimination, and also performs well on cross-view word discrimination and word similarity tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992551207542419,
                    "sentence": "Additional Feedback",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999717652797699,
                    "sentence": "To improve the paper, I would suggest providing more details on the implementation of the cost-sensitive loss, including the choice of hyperparameters and the effect of different margin values.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994195699691772,
                    "sentence": "Additionally, it would be interesting to see an analysis of the learned embeddings, including a visualization of the embedding space and an analysis of the relationships between words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989798665046692,
                    "sentence": "Finally, the authors could consider providing more details on the potential applications of the proposed approach, including speech recognition and query-by-example search.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9866868853569031,
                    "sentence": "Questions for the Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9591677784919739,
                    "sentence": "I would like to ask the authors to clarify the following points:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9727941155433655,
                    "sentence": "* How did the authors choose the hyperparameters for the cost-sensitive loss, and what was the effect of different margin values on the performance of the model?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9410820603370667,
                    "sentence": "* Can the authors provide more details on the implementation of the multi-view contrastive losses, including the choice of negative examples and the effect of different loss functions?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9756540060043335,
                    "sentence": "* How do the authors plan to extend the proposed approach to larger vocabularies and more complex speech tasks, such as speech recognition and query-by-example search?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984800378301695,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984800378301695,
                "mixed": 0.0015199621698304396
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984800378301695,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984800378301695,
                    "human": 0,
                    "mixed": 0.0015199621698304396
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary\nThe paper proposes a multi-view approach to learning acoustic word embeddings, where both acoustic sequences and their corresponding character sequences are jointly embedded into a common space. The authors use deep bidirectional LSTM embedding models and multi-view contrastive losses, including fixed-margin and cost-sensitive losses. The approach is evaluated on several tasks, including acoustic word discrimination, cross-view word discrimination, and word similarity, and is shown to improve over previous approaches.\nDecision\nI decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-motivated problem in the field of speech processing, and the approach is well-placed in the literature. Secondly, the paper provides a thorough evaluation of the proposed approach, including a comparison with previous work and an analysis of the effects of different objectives and hyperparameters.\nSupporting Arguments\nThe paper provides a clear and well-written introduction to the problem of learning acoustic word embeddings, and motivates the need for a multi-view approach. The authors also provide a thorough review of related work, including previous approaches to learning acoustic word embeddings and multi-view representation learning. The experimental evaluation is comprehensive, including a comparison with previous work and an analysis of the effects of different objectives and hyperparameters. The results show that the proposed approach improves over previous work on acoustic word discrimination, and also performs well on cross-view word discrimination and word similarity tasks.\nAdditional Feedback\nTo improve the paper, I would suggest providing more details on the implementation of the cost-sensitive loss, including the choice of hyperparameters and the effect of different margin values. Additionally, it would be interesting to see an analysis of the learned embeddings, including a visualization of the embedding space and an analysis of the relationships between words. Finally, the authors could consider providing more details on the potential applications of the proposed approach, including speech recognition and query-by-example search.\nQuestions for the Authors\nI would like to ask the authors to clarify the following points:\n* How did the authors choose the hyperparameters for the cost-sensitive loss, and what was the effect of different margin values on the performance of the model?\n* Can the authors provide more details on the implementation of the multi-view contrastive losses, including the choice of negative examples and the effect of different loss functions?\n* How do the authors plan to extend the proposed approach to larger vocabularies and more complex speech tasks, such as speech recognition and query-by-example search?"
        }
    ]
}