{
    "version": "2025-01-09-base",
    "scanId": "ea67f4a5-5984-449e-a521-e7000f5df1c6",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.4157833158969879,
                    "sentence": "The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.3455861508846283,
                    "sentence": "The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.25623801350593567,
                    "sentence": "The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.1296270191669464,
                    "sentence": "In some cases the method can actually improve the evaluation performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.15439678728580475,
                    "sentence": "The experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.15460404753684998,
                    "sentence": "I like that the effect of the pruning is investigated for networks of very large sizes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.10367074608802795,
                    "sentence": "The computational gains are clearly substantial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.09968096762895584,
                    "sentence": "It is a bit unfortunate that all experiments are done using a private dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.11798039078712463,
                    "sentence": "Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.05929381027817726,
                    "sentence": "It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.0644136369228363,
                    "sentence": "[2] to verify the relative merit of the proposed pruning scheme.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.11505213379859924,
                    "sentence": "While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.0999232605099678,
                    "sentence": "Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.2229737937450409,
                    "sentence": "Overall, the paper is easy to read.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.17258192598819733,
                    "sentence": "The table and figure captions could be a bit more detailed but they are still clear enough.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.15769055485725403,
                    "sentence": "The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.25646165013313293,
                    "sentence": "The paper doesn't motivate the details of the method very well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.36292415857315063,
                    "sentence": "It's not clear to me why the threshold has to ramp up after a certain period time for example.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.29490789771080017,
                    "sentence": "If this is based on preliminary findings, the paper should mention that.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.48266303539276123,
                    "sentence": "Sparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6971889734268188,
                    "sentence": "The proposed method is also very similar to the work by Han et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7313463687896729,
                    "sentence": "[2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8633344173431396,
                    "sentence": "While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7995295524597168,
                    "sentence": "The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8922441005706787,
                    "sentence": "Improving scalability is an important driving force of the progress in neural network research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9490305781364441,
                    "sentence": "While I don't think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9804935455322266,
                    "sentence": "The fact that this is possible with such a simple heuristic is a result worth sharing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9871532320976257,
                    "sentence": "Pros:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9923452138900757,
                    "sentence": "The proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9934747219085693,
                    "sentence": "The experiments are done using a state-of-the-art system for a practical application.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9860663414001465,
                    "sentence": "Cons:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9871513247489929,
                    "sentence": "The proposed method is very similar to earlier work and barely novel.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9886986017227173,
                    "sentence": "There is no comparison with other pruning methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9835432767868042,
                    "sentence": "The data is private and this prevents others from replicating the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9808489084243774,
                    "sentence": "[1] Jaeger, H. (2001).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9882215261459351,
                    "sentence": "The \"echo state\" approach to analyzing and training recurrent neural networks-with an erratum note.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9879006743431091,
                    "sentence": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.987405002117157,
                    "sentence": "[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9838634729385376,
                    "sentence": "Learning both weights and connections for efficient neural networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9934749603271484,
                    "sentence": "In Advances in Neural Information Processing Systems, 2015.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 9,
                    "completely_generated_prob": 4.1887248735431006e-08
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 6,
                    "completely_generated_prob": 1.474742012248794e-05
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.0022792978668507114
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.6927998989311738,
            "class_probabilities": {
                "human": 0.3043821056707332,
                "ai": 0.6927998989311738,
                "mixed": 0.002817995398093028
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.6927998989311738,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.6927998989311738,
                    "human": 0.3043821056707332,
                    "mixed": 0.002817995398093028
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance.\nThe experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.\nOverall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn't motivate the details of the method very well. It's not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that.\nSparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.\nImproving scalability is an important driving force of the progress in neural network research. While I don't think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing.\nPros:\nThe proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance.\nThe experiments are done using a state-of-the-art system for a practical application.\nCons:\nThe proposed method is very similar to earlier work and barely novel.\nThere is no comparison with other pruning methods.\nThe data is private and this prevents others from replicating the results.\n[1] Jaeger, H. (2001). The \"echo state\" approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.\n[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015."
        }
    ]
}