{
    "version": "2025-01-09-base",
    "scanId": "301f2631-9451-46bc-8c56-5cfeb79eb557",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.004956766031682491,
                    "sentence": "This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005270243156701326,
                    "sentence": "The paper is very clearly written, and explains the justification very well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004284852650016546,
                    "sentence": "The problem being attacked is very significant and important.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006525909062474966,
                    "sentence": "The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0059699187986552715,
                    "sentence": "The first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00408675242215395,
                    "sentence": "This is done by using L-BFGS on \"G(z) - x\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006331177428364754,
                    "sentence": "The claim is that if we're able to find such a z, then the generator can generate this particular training example.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006513544358313084,
                    "sentence": "It's demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0070372093468904495,
                    "sentence": "However, I find this experiment unreasonable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00596993463113904,
                    "sentence": "Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005365968681871891,
                    "sentence": "In fact, an identity function can potentially beat all the GAN models in the proposed metric.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007235595490783453,
                    "sentence": "And due to Cantor's proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008122469298541546,
                    "sentence": "More realistically, it should be possible to generate any image from a generator by finding a very specific z.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0092881815508008,
                    "sentence": "That a certain z exists which can generate a sample does not prove that the generator is not missing modes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011079773306846619,
                    "sentence": "It just proves that the generator is similar enough to an identity function to be able to generate any possible image.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021081140264868736,
                    "sentence": "This metric is thus measuring something potentially tangential to diversity or mode-dropping.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03894282132387161,
                    "sentence": "Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it's harder to find.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04568943381309509,
                    "sentence": "So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08840050548315048,
                    "sentence": "The second quantitative experiment considers mean pairwise distance between generated samples, and between data samples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09184928983449936,
                    "sentence": "The first number is likely to be small in the case of a mode-dropping GAN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10100187361240387,
                    "sentence": "The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08791112154722214,
                    "sentence": "Once again, this metric is not convincing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08072885125875473,
                    "sentence": "1. The distances are being measured in pixel-space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08134914189577103,
                    "sentence": "2. A GAN model could be generating garbage, and yet still perform very well in this metric.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11533790081739426,
                    "sentence": "There are no other quantitative results in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.056173983961343765,
                    "sentence": "Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0732002705335617,
                    "sentence": "Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08850881457328796,
                    "sentence": "Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04942651465535164,
                    "sentence": "This is not a perfect metric either, but arguably much better than the metrics in this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07877854257822037,
                    "sentence": "This metric is used in this ICLR submission:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 14,
                    "completely_generated_prob": 1.9423489116273474e-12
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 6,
                    "completely_generated_prob": 1.474742012248794e-05
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 6,
                    "completely_generated_prob": 1.474742012248794e-05
                }
            ],
            "completely_generated_prob": 0.026098759003974407,
            "class_probabilities": {
                "human": 0.9730010893857934,
                "ai": 0.026098759003974407,
                "mixed": 0.0009001516102321237
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9730010893857934,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.026098759003974407,
                    "human": 0.9730010893857934,
                    "mixed": 0.0009001516102321237
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph. The paper is very clearly written, and explains the justification very well. The problem being attacked is very significant and important. The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs.\nThe first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples. This is done by using L-BFGS on \"G(z) - x\". The claim is that if we're able to find such a z, then the generator can generate this particular training example. It's demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do. However, I find this experiment unreasonable. Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability. In fact, an identity function can potentially beat all the GAN models in the proposed metric. And due to Cantor's proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well. More realistically, it should be possible to generate any image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes. It just proves that the generator is similar enough to an identity function to be able to generate any possible image. This metric is thus measuring something potentially tangential to diversity or mode-dropping. Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it's harder to find. So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z.\nThe second quantitative experiment considers mean pairwise distance between generated samples, and between data samples. The first number is likely to be small in the case of a mode-dropping GAN. The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data. Once again, this metric is not convincing. 1. The distances are being measured in pixel-space. 2. A GAN model could be generating garbage, and yet still perform very well in this metric.\nThere are no other quantitative results in the paper. Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful. Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes. Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability. This is not a perfect metric either, but arguably much better than the metrics in this paper. This metric is used in this ICLR submission:"
        }
    ]
}