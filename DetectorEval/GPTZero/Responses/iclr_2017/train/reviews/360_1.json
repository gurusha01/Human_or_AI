{
    "version": "2025-01-09-base",
    "scanId": "839b9280-a4c6-4997-b004-5e276b77ee2d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9369674921035767,
                    "sentence": "In supervised learning, a significant advance occurred when the framework of semi-supervised learning was adopted, which used the weaker approach of unsupervised learning to infer some property, such as a distance measure or a smoothness regularizer, which could then be used with a small number of labeled examples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8627482056617737,
                    "sentence": "The approach rested on the assumption of smoothness on the manifold, typically.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7948048114776611,
                    "sentence": "This paper attempts to stretch this analogy to reinforcement learning, although the analogy is somewhat incoherent.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8406370282173157,
                    "sentence": "Labels are not equivalent to reward functions, and positive or negative rewards do not mean the same as positive and negative labels.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8544081449508667,
                    "sentence": "Still, the paper makes a worthwhile attempt to explore this notion of semi-supervised RL, which is clearly an important area that deserves more attention.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9032047986984253,
                    "sentence": "The authors use the term \"labeled MDP\" to mean the typical MDP framework where the reward function is unknown.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8631641864776611,
                    "sentence": "They use the confusing term \"unlabeled MDP\" to mean the situation where the reward is unknown, which is technically not an MDP (but a controlled Markov process).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8065258860588074,
                    "sentence": "In the classical RL transfer learning setup, the agent is attempting to transfer learning from a source \"labeled\" MDP to a target \"labeled\" MDP (where both reward functions are known, but the learned policy is known only in the source MDP).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7453131079673767,
                    "sentence": "In the semi-supervised RL setting, the target is an \"unlabeled\" CMP, and the source is both a \"labeled\" MDP and an \"unlabeled\" CMP.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8113448619842529,
                    "sentence": "The basic approach is to use inverse RL to infer the unknown \"labels\" and then attempt to construct transfer.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8820065259933472,
                    "sentence": "A further restriction is made to linearly solvable MDPs for technical reasons.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7705510854721069,
                    "sentence": "Experiments are reported using three relatively complex domains using the Mujoco physics simulator.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8851712346076965,
                    "sentence": "The work is interesting, but in the opinion of this reviewer, the work fails to provide a simple sufficiently general notion of semi-supervised RL that will be of sufficiently wide interest to the RL community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9367916584014893,
                    "sentence": "That remains to be done by a future paper, but in the interim, the work here is sufficiently interesting and the problem is certainly a worthwhile one to study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.6535213355143276
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.24579470214975613
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.4973606023242839,
            "class_probabilities": {
                "human": 0.5018362882844575,
                "ai": 0.4973606023242839,
                "mixed": 0.0008031093912586005
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5018362882844575,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.4973606023242839,
                    "human": 0.5018362882844575,
                    "mixed": 0.0008031093912586005
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "In supervised learning, a significant advance occurred when the framework of semi-supervised learning was adopted, which used the weaker approach of unsupervised learning to infer some property, such as a distance measure or a smoothness regularizer, which could then be used with a small number of labeled examples. The approach rested on the assumption of smoothness on the manifold, typically. \nThis paper attempts to stretch this analogy to reinforcement learning, although the analogy is somewhat incoherent. Labels are not equivalent to reward functions, and positive or negative rewards do not mean the same as positive and negative labels. Still, the paper makes a worthwhile attempt to explore this notion of semi-supervised RL, which is clearly an important area that deserves more attention. The authors use the term \"labeled MDP\" to mean the typical MDP framework where the reward function is unknown. They use the confusing term \"unlabeled MDP\" to mean the situation where the reward is unknown, which is technically not an MDP (but a controlled Markov process). \nIn the classical RL transfer learning setup, the agent is attempting to transfer learning from a source \"labeled\" MDP to a target \"labeled\" MDP (where both reward functions are known, but the learned policy is known only in the source MDP). In the semi-supervised RL setting, the target is an \"unlabeled\" CMP, and the source is both a \"labeled\" MDP and an \"unlabeled\" CMP. The basic approach is to use inverse RL to infer the unknown \"labels\" and then attempt to construct transfer. A further restriction is made to linearly solvable MDPs for technical reasons. Experiments are reported using three relatively complex domains using the Mujoco physics simulator. \nThe work is interesting, but in the opinion of this reviewer, the work fails to provide a simple sufficiently general notion of semi-supervised RL that will be of sufficiently wide interest to the RL community. That remains to be done by a future paper, but in the interim, the work here is sufficiently interesting and the problem is certainly a worthwhile one to study."
        }
    ]
}