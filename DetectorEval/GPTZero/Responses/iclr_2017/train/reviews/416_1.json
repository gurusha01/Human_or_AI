{
    "version": "2025-01-09-base",
    "scanId": "7ac4634e-8bf2-4675-a5ee-2ff2d88c7ad3",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0072806281968951225,
                    "sentence": "This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02166794054210186,
                    "sentence": "The paper is clearly written and is easy to follow.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014931176789104939,
                    "sentence": "Novelty is a weak factor in this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010661972686648369,
                    "sentence": "The main contributions come from (1) applying previous work on NFs to the problem of MaxEnt estimation and (2) addressing some of the optimization issues resulting from stochastic approximations to E[\"\"T\"\"] in combination with the annealing of Lagrange multipliers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009651962667703629,
                    "sentence": "Applying the NFs to MaxEnt is in itself not very novel as a framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014067310839891434,
                    "sentence": "For instance, one could obtain a loss equivalent to the main loss in eq.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009961445815861225,
                    "sentence": "(6) by minimizing the KLD between KL[p{\\phi};f], where f is the unormalized likelihood f \\propto exp \\sumk( - \\lambdak T - ck \"\"T_k\"\"^2 ).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00954403355717659,
                    "sentence": "This type of derivation is typical in all previous works using NFs for variational inference.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009965215809643269,
                    "sentence": "A few experiments on more complex data would strengthen the paper's results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017499420791864395,
                    "sentence": "The two experiments provided show good results but both of them are toy problems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02059900015592575,
                    "sentence": "Minor point:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0112144248560071,
                    "sentence": "Although intuitive, it would be good to have a short discussion of step 8 of algorithm 1 as well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.024461651786716186,
            "class_probabilities": {
                "human": 0.9754981327244504,
                "ai": 0.024461651786716186,
                "mixed": 4.0215488833363626e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9754981327244504,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.024461651786716186,
                    "human": 0.9754981327244504,
                    "mixed": 4.0215488833363626e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization.\nThe paper is clearly written and is easy to follow.\nNovelty is a weak factor in this paper. The main contributions come from (1) applying previous work on NFs to the problem of MaxEnt estimation and (2) addressing some of the optimization issues resulting from stochastic approximations to E[\"\"T\"\"] in combination with the annealing of Lagrange multipliers.\nApplying the NFs to MaxEnt is in itself not very novel as a framework. For instance, one could obtain a loss equivalent to the main loss in eq. (6) by minimizing the KLD between KL[p{\\phi};f], where f is the unormalized likelihood f \\propto exp \\sumk( - \\lambdak T - ck \"\"T_k\"\"^2 ). This type of derivation is typical in all previous works using NFs for variational inference.\nA few experiments on more complex data would strengthen the paper's results. The two experiments provided show good results but both of them are toy problems.\nMinor point:\nAlthough intuitive, it would be good to have a short discussion of step 8 of algorithm 1 as well."
        }
    ]
}