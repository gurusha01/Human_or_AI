{
    "version": "2025-01-09-base",
    "scanId": "3f53919b-53ad-41a8-b006-138ba106a09a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9991205334663391,
                    "sentence": "This paper primarily serves as a (well-written) exploration of a toy application.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995384812355042,
                    "sentence": "It demonstrates how SGVB can be utilized within the context of state-space models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996659159660339,
                    "sentence": "The core concept involves framing a state-space model as a deterministic temporal transformation, where innovation variables function as latent variables.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999552845954895,
                    "sentence": "Notably, the prior distribution over these innovation variables is time-invariant.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997133016586304,
                    "sentence": "The paper focuses on performing approximate inference over the innovation variables rather than directly over the states.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989128112792969,
                    "sentence": "While this approach addresses a specific problem (e.g., it does not explore scenarios where priors over the beta's are time-dependent), it remains an intriguing and valuable application.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9968395233154297,
                    "sentence": "However, the presentation of ideas could benefit from greater clarity and conciseness; the paper delves into technical specifics rather quickly, which may represent a missed opportunity for broader accessibility.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9934105277061462,
                    "sentence": "I commend the authors for the extensive detail provided in both the main text and the appendix.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9890396595001221,
                    "sentence": "The experiments, while limited to toy examples, show encouraging potential.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6372517943382263,
                    "sentence": "- Section 2.1: \"In our notation, one would typically set betat = wt, though other variants are possible\" -> It would be helpful to clarify that if Ft and Bt are excluded from beta_t, they are not treated in a Bayesian manner (e.g., they are simply optimized).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.4913986325263977,
                    "sentence": "- Section 2.2, last paragraph: \"A key contribution is [ å›  forcing the latent space to fit the transition.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7030531764030457,
                    "sentence": "This contribution appears somewhat trivial to achieve.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.4981040358543396,
                    "sentence": "- Eq 9: \"This interpretation implies the factorization of the recognition model:..\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.49070292711257935,
                    "sentence": "The factorization is not explicitly implied anywhere; in principle, one could use q(beta\"x) = q(w\"x,v)q(v) instead.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.8133971291866029,
            "class_probabilities": {
                "human": 0,
                "ai": 0.8133971291866029,
                "mixed": 0.1866028708133971
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8133971291866029,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8133971291866029,
                    "human": 0,
                    "mixed": 0.1866028708133971
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper primarily serves as a (well-written) exploration of a toy application. It demonstrates how SGVB can be utilized within the context of state-space models. The core concept involves framing a state-space model as a deterministic temporal transformation, where innovation variables function as latent variables. Notably, the prior distribution over these innovation variables is time-invariant. The paper focuses on performing approximate inference over the innovation variables rather than directly over the states. While this approach addresses a specific problem (e.g., it does not explore scenarios where priors over the beta's are time-dependent), it remains an intriguing and valuable application. However, the presentation of ideas could benefit from greater clarity and conciseness; the paper delves into technical specifics rather quickly, which may represent a missed opportunity for broader accessibility.\nI commend the authors for the extensive detail provided in both the main text and the appendix.\nThe experiments, while limited to toy examples, show encouraging potential.\n- Section 2.1: \"In our notation, one would typically set betat = wt, though other variants are possible\" -> It would be helpful to clarify that if Ft and Bt are excluded from beta_t, they are not treated in a Bayesian manner (e.g., they are simply optimized).\n- Section 2.2, last paragraph: \"A key contribution is [ å›  forcing the latent space to fit the transition.\" This contribution appears somewhat trivial to achieve.\n- Eq 9: \"This interpretation implies the factorization of the recognition model:..\" \nThe factorization is not explicitly implied anywhere; in principle, one could use q(beta\"x) = q(w\"x,v)q(v) instead."
        }
    ]
}