{
    "version": "2025-01-09-base",
    "scanId": "30754d55-3029-4d88-b648-a087851dbea8",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999322295188904,
                    "sentence": "This paper explores the application of normalizing flows (NFs), a method that enables the construction of complex probability densities with tractable likelihoods, to maximum entropy (MaxEnt) constrained optimization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999171495437622,
                    "sentence": "The manuscript is well-written and straightforward to understand.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998997449874878,
                    "sentence": "The novelty of this work is somewhat limited.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999364614486694,
                    "sentence": "The primary contributions are (1) employing existing techniques involving NFs to tackle the MaxEnt estimation problem and (2) addressing certain optimization challenges arising from stochastic approximations to E[\"\"T\"\"], combined with the annealing of Lagrange multipliers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999100565910339,
                    "sentence": "However, the use of NFs for MaxEnt estimation is not particularly innovative as a framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999011754989624,
                    "sentence": "For example, one could derive a loss equivalent to the main loss in eq.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999380707740784,
                    "sentence": "(6) by minimizing the Kullback-Leibler divergence (KLD) between KL[p{\\phi};f], where f represents the unnormalized likelihood f \\propto exp \\sumk( - \\lambdak T - ck \"\"T_k\"\"^2 ).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999185800552368,
                    "sentence": "Such derivations are standard in prior works that leverage NFs for variational inference.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999051094055176,
                    "sentence": "Including experiments on more complex datasets would enhance the strength of the paper's results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998816847801208,
                    "sentence": "While the two presented experiments yield promising outcomes, they are limited to toy problems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999287128448486,
                    "sentence": "Minor comment:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999113082885742,
                    "sentence": "Although step 8 of algorithm 1 is intuitive, a brief discussion of it would be beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper explores the application of normalizing flows (NFs), a method that enables the construction of complex probability densities with tractable likelihoods, to maximum entropy (MaxEnt) constrained optimization.\nThe manuscript is well-written and straightforward to understand.\nThe novelty of this work is somewhat limited. The primary contributions are (1) employing existing techniques involving NFs to tackle the MaxEnt estimation problem and (2) addressing certain optimization challenges arising from stochastic approximations to E[\"\"T\"\"], combined with the annealing of Lagrange multipliers. However, the use of NFs for MaxEnt estimation is not particularly innovative as a framework. For example, one could derive a loss equivalent to the main loss in eq. (6) by minimizing the Kullback-Leibler divergence (KLD) between KL[p{\\phi};f], where f represents the unnormalized likelihood f \\propto exp \\sumk( - \\lambdak T - ck \"\"T_k\"\"^2 ). Such derivations are standard in prior works that leverage NFs for variational inference. \nIncluding experiments on more complex datasets would enhance the strength of the paper's results. While the two presented experiments yield promising outcomes, they are limited to toy problems.\nMinor comment:\nAlthough step 8 of algorithm 1 is intuitive, a brief discussion of it would be beneficial."
        }
    ]
}