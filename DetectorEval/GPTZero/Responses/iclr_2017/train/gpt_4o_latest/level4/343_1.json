{
    "version": "2025-01-09-base",
    "scanId": "262dc268-2f73-4f91-b862-1a5dd9004c53",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998775720596313,
                    "sentence": "This paper introduces an approach to character language modeling (CLMs) that involves the development of a domain-specific language (DSL) to represent CLMs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997782707214355,
                    "sentence": "The experimental results indicate mixed performance compared to neural CLM methods when applied to Linux kernel data and Wikipedia text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996366500854492,
                    "sentence": "However, the proposed DSL models are marginally more compact and faster to query than neural CLMs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997256994247437,
                    "sentence": "Despite these advantages, the approach is challenging to comprehend and appears to target a niche sub-community already familiar with this methodology, lacking sufficient clarity for the broader ICLR audience.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995166063308716,
                    "sentence": "A significant concern is the paper's failure to adequately demonstrate that the proposed DSL constitutes a valid probabilistic model and to explain how the model is trained to fit the data, particularly given the absence of a gradient-based training approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994211196899414,
                    "sentence": "Furthermore, the experiments feel incomplete without presenting samples generated by the model or analyzing the learned representations to understand what the model has captured.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997134208679199,
                    "sentence": "Overall, the paper does not provide enough detail for readers to fully grasp or replicate the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998592138290405,
                    "sentence": "The majority of the model section focuses on describing the DSL but does not clarify how probabilities are computed within this framework or how training is conducted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999893844127655,
                    "sentence": "How exactly are probabilities encoded in the model?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999943196773529,
                    "sentence": "The DSL description seems to rely solely on discrete decisions, with no clear integration of probabilistic elements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999487400054932,
                    "sentence": "While training may have been addressed in prior work, this paper needs to include a discussion of the training process to ensure clarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999597072601318,
                    "sentence": "Section 2.5 falls short in explaining how training is performed or how any notion of optimality is achieved.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999494552612305,
                    "sentence": "Given that this model operates in a hypothesis space distinct from neural models or n-grams, examining samples generated by the model is crucial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999468326568604,
                    "sentence": "While the current experiments demonstrate the model's ability to score utterances, it would be particularly compelling to see whether the model can produce more structured outputs than neural approaches, such as enforcing long-range syntax constraints like balanced brackets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper introduces an approach to character language modeling (CLMs) that involves the development of a domain-specific language (DSL) to represent CLMs. The experimental results indicate mixed performance compared to neural CLM methods when applied to Linux kernel data and Wikipedia text. However, the proposed DSL models are marginally more compact and faster to query than neural CLMs. Despite these advantages, the approach is challenging to comprehend and appears to target a niche sub-community already familiar with this methodology, lacking sufficient clarity for the broader ICLR audience. A significant concern is the paper's failure to adequately demonstrate that the proposed DSL constitutes a valid probabilistic model and to explain how the model is trained to fit the data, particularly given the absence of a gradient-based training approach. Furthermore, the experiments feel incomplete without presenting samples generated by the model or analyzing the learned representations to understand what the model has captured. Overall, the paper does not provide enough detail for readers to fully grasp or replicate the proposed approach.\nThe majority of the model section focuses on describing the DSL but does not clarify how probabilities are computed within this framework or how training is conducted. How exactly are probabilities encoded in the model? The DSL description seems to rely solely on discrete decisions, with no clear integration of probabilistic elements.\nWhile training may have been addressed in prior work, this paper needs to include a discussion of the training process to ensure clarity. Section 2.5 falls short in explaining how training is performed or how any notion of optimality is achieved.\nGiven that this model operates in a hypothesis space distinct from neural models or n-grams, examining samples generated by the model is crucial. While the current experiments demonstrate the model's ability to score utterances, it would be particularly compelling to see whether the model can produce more structured outputs than neural approaches, such as enforcing long-range syntax constraints like balanced brackets."
        }
    ]
}