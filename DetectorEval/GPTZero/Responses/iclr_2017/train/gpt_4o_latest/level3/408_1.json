{
    "version": "2025-01-09-base",
    "scanId": "3c83b484-62f1-4ed8-b502-52c2e78c8804",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999984085559845,
                    "sentence": "Review of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960660934448,
                    "sentence": "Summary of Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999818205833435,
                    "sentence": "This paper proposes a novel multi-view approach for learning neural acoustic word embeddings by jointly embedding acoustic sequences and their corresponding character sequences.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999843239784241,
                    "sentence": "The authors utilize deep bidirectional LSTM models and explore several contrastive loss functions, including fixed-margin and cost-sensitive losses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999621510505676,
                    "sentence": "The proposed method outperforms prior approaches in acoustic word discrimination tasks and introduces the capability for cross-view tasks, such as comparing written and spoken words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999724626541138,
                    "sentence": "Additionally, the paper investigates the correlation between embedding distances and orthographic edit distances, providing insights into the structure of the learned embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999796152114868,
                    "sentence": "The work is a meaningful contribution to the field of speech processing and retrieval, particularly for tasks requiring whole-word reasoning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999620914459229,
                    "sentence": "Decision: Reject",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999529123306274,
                    "sentence": "While the paper introduces an interesting training criterion and demonstrates improvements over prior methods, it suffers from several critical shortcomings that limit its impact and reproducibility.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999657869338989,
                    "sentence": "The primary reasons for rejection are the lack of proper baselines based on established ASR techniques and the use of a small dataset, which raises concerns about the generalizability of the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999656081199646,
                    "sentence": "Supporting Arguments for Decision",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999991774559021,
                    "sentence": "1. Lack of ASR Baselines: The paper does not compare its approach against strong baselines from automatic speech recognition (ASR) systems, which are standard in the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999815821647644,
                    "sentence": "Without such comparisons, it is difficult to assess the practical significance of the proposed method relative to existing techniques.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999881386756897,
                    "sentence": "2. Dataset Size: The dataset used in the experiments is relatively small, with only a few thousand unique words.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999846816062927,
                    "sentence": "This limits the generalizability of the findings, especially for real-world applications where larger and more diverse datasets are common.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999915361404419,
                    "sentence": "3. Evaluation Metrics: While the paper uses average precision (AP) as the primary evaluation metric, additional metrics such as receiver operating characteristic (ROC) curves and equal error rate (EER) would provide a more comprehensive assessment of the model's performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999839663505554,
                    "sentence": "4. Result Analysis: The analysis of results is insufficient.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999848008155823,
                    "sentence": "For example, there is no detailed breakdown of performance on in-vocabulary vs. out-of-vocabulary words, which is crucial for understanding the robustness of the embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999854564666748,
                    "sentence": "5. Visualization: The paper could benefit from more insightful visualizations, such as scatter plots of embedding distances versus orthographic edit distances, to better illustrate the relationship between learned embeddings and word similarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999948263168335,
                    "sentence": "Additional Feedback for Improvement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999621510505676,
                    "sentence": "1. Baseline Comparisons: Incorporating ASR-based baselines, such as those using dynamic time warping or end-to-end ASR models, would strengthen the paper's claims and contextualize its contributions within the broader literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999675750732422,
                    "sentence": "2. Dataset Expansion: Using a larger and more diverse dataset would improve the robustness and applicability of the proposed method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999028444290161,
                    "sentence": "The authors could also explore transfer learning to address the limitations of small datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999839663505554,
                    "sentence": "3. Evaluation Metrics: Including ROC curves and EER would provide a more nuanced understanding of the model's performance across different thresholds.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999193549156189,
                    "sentence": "4. Detailed Analysis: The authors should analyze the performance on in-vocabulary vs. out-of-vocabulary words and provide insights into how the model handles unseen data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996691346168518,
                    "sentence": "5. Visualization: Adding scatter plots of embeddings versus orthographic distances, as well as t-SNE visualizations for a broader range of words, would make the results more interpretable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9792675971984863,
                    "sentence": "Questions for the Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9874653816223145,
                    "sentence": "1. How does the proposed method compare to ASR-based baselines in terms of both accuracy and computational efficiency?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9782373905181885,
                    "sentence": "2. Can the model generalize to larger vocabularies or unseen datasets?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9807614684104919,
                    "sentence": "Have you considered evaluating on a more diverse dataset?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.962957501411438,
                    "sentence": "3. Why were ROC curves and EER not included in the evaluation?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9860274195671082,
                    "sentence": "Would these metrics provide additional insights into the model's performance?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9866613149642944,
                    "sentence": "4. How does the model handle in-vocabulary versus out-of-vocabulary words?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9713714718818665,
                    "sentence": "Can you provide a detailed breakdown of these results?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9119637608528137,
                    "sentence": "In conclusion, while the paper presents an interesting approach to learning acoustic word embeddings, it requires significant improvements in experimental rigor, dataset size, and result analysis to meet the standards of the conference.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984930238596827,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984930238596827,
                "mixed": 0.001506976140317253
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984930238596827,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984930238596827,
                    "human": 0,
                    "mixed": 0.001506976140317253
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Paper\nSummary of Contributions\nThis paper proposes a novel multi-view approach for learning neural acoustic word embeddings by jointly embedding acoustic sequences and their corresponding character sequences. The authors utilize deep bidirectional LSTM models and explore several contrastive loss functions, including fixed-margin and cost-sensitive losses. The proposed method outperforms prior approaches in acoustic word discrimination tasks and introduces the capability for cross-view tasks, such as comparing written and spoken words. Additionally, the paper investigates the correlation between embedding distances and orthographic edit distances, providing insights into the structure of the learned embeddings. The work is a meaningful contribution to the field of speech processing and retrieval, particularly for tasks requiring whole-word reasoning.\nDecision: Reject\nWhile the paper introduces an interesting training criterion and demonstrates improvements over prior methods, it suffers from several critical shortcomings that limit its impact and reproducibility. The primary reasons for rejection are the lack of proper baselines based on established ASR techniques and the use of a small dataset, which raises concerns about the generalizability of the results.\nSupporting Arguments for Decision\n1. Lack of ASR Baselines: The paper does not compare its approach against strong baselines from automatic speech recognition (ASR) systems, which are standard in the field. Without such comparisons, it is difficult to assess the practical significance of the proposed method relative to existing techniques.\n \n2. Dataset Size: The dataset used in the experiments is relatively small, with only a few thousand unique words. This limits the generalizability of the findings, especially for real-world applications where larger and more diverse datasets are common.\n3. Evaluation Metrics: While the paper uses average precision (AP) as the primary evaluation metric, additional metrics such as receiver operating characteristic (ROC) curves and equal error rate (EER) would provide a more comprehensive assessment of the model's performance.\n4. Result Analysis: The analysis of results is insufficient. For example, there is no detailed breakdown of performance on in-vocabulary vs. out-of-vocabulary words, which is crucial for understanding the robustness of the embeddings.\n5. Visualization: The paper could benefit from more insightful visualizations, such as scatter plots of embedding distances versus orthographic edit distances, to better illustrate the relationship between learned embeddings and word similarity.\nAdditional Feedback for Improvement\n1. Baseline Comparisons: Incorporating ASR-based baselines, such as those using dynamic time warping or end-to-end ASR models, would strengthen the paper's claims and contextualize its contributions within the broader literature.\n2. Dataset Expansion: Using a larger and more diverse dataset would improve the robustness and applicability of the proposed method. The authors could also explore transfer learning to address the limitations of small datasets.\n3. Evaluation Metrics: Including ROC curves and EER would provide a more nuanced understanding of the model's performance across different thresholds.\n4. Detailed Analysis: The authors should analyze the performance on in-vocabulary vs. out-of-vocabulary words and provide insights into how the model handles unseen data.\n5. Visualization: Adding scatter plots of embeddings versus orthographic distances, as well as t-SNE visualizations for a broader range of words, would make the results more interpretable.\nQuestions for the Authors\n1. How does the proposed method compare to ASR-based baselines in terms of both accuracy and computational efficiency?\n2. Can the model generalize to larger vocabularies or unseen datasets? Have you considered evaluating on a more diverse dataset?\n3. Why were ROC curves and EER not included in the evaluation? Would these metrics provide additional insights into the model's performance?\n4. How does the model handle in-vocabulary versus out-of-vocabulary words? Can you provide a detailed breakdown of these results?\nIn conclusion, while the paper presents an interesting approach to learning acoustic word embeddings, it requires significant improvements in experimental rigor, dataset size, and result analysis to meet the standards of the conference."
        }
    ]
}