{
    "version": "2025-01-09-base",
    "scanId": "8df0152e-d2eb-4a60-93a4-50cc27131f62",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999357461929321,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999613761901855,
                    "sentence": "Summary of Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999632239341736,
                    "sentence": "This paper addresses the problem of learning neural acoustic word embeddings that map speech segments to fixed-dimensional vectors, enabling tasks such as speech retrieval and recognition.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999617338180542,
                    "sentence": "The authors propose a novel multi-view approach that jointly learns embeddings for acoustic sequences and their corresponding character sequences using deep bidirectional LSTMs and multi-view contrastive losses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999461770057678,
                    "sentence": "The paper introduces several loss variants, including fixed-margin and cost-sensitive losses, and evaluates their effectiveness on tasks such as word discrimination, cross-view word discrimination, and word similarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998931288719177,
                    "sentence": "The proposed approach outperforms prior methods in acoustic word discrimination and demonstrates promising results in cross-view tasks, highlighting the utility of multi-view embeddings for both spoken and written query scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998382925987244,
                    "sentence": "Decision: Accept",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995101690292358,
                    "sentence": "Key reasons for acceptance include the novelty of the multi-view approach, which is well-motivated and fills a gap in the literature, and the strong empirical results that demonstrate clear improvements over prior methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997321367263794,
                    "sentence": "The paper is methodologically rigorous, with thorough experiments and detailed analyses that support its claims.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999474823474884,
                    "sentence": "Supporting Arguments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997579455375671,
                    "sentence": "1. Problem and Motivation: The paper tackles a relevant and underexplored problem\"\"learning embeddings that capture acoustic and orthographic information jointly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998112320899963,
                    "sentence": "The motivation is well-placed in the literature, as prior work has largely focused on single-view acoustic embeddings or ignored the relationship between acoustic and orthographic representations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997962117195129,
                    "sentence": "The multi-view approach is a natural and innovative extension that addresses these gaps.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998049139976501,
                    "sentence": "2. Methodological Rigor: The use of bidirectional LSTMs and contrastive losses is appropriate for the sequential nature of the data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998642802238464,
                    "sentence": "The authors systematically explore different loss functions and architectures, providing a comprehensive evaluation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999847948551178,
                    "sentence": "The inclusion of a cost-sensitive loss to capture orthographic edit distances is a thoughtful addition that aligns well with the goal of improving word similarity tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998946785926819,
                    "sentence": "3. Results and Impact: The proposed embeddings achieve state-of-the-art performance on acoustic word discrimination and enable new cross-view tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998928904533386,
                    "sentence": "The visualization of embeddings and the analysis of word similarity further validate the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998556971549988,
                    "sentence": "The results are robust and reproducible, with the code made publicly available.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999961256980896,
                    "sentence": "Suggestions for Improvement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999536275863647,
                    "sentence": "1. Clarity of Objectives: While the paper discusses multiple objectives (e.g., obj0, obj1, etc.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999504089355469,
                    "sentence": "), the presentation could be streamlined to make the differences between these objectives clearer to readers unfamiliar with contrastive learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999613165855408,
                    "sentence": "A table summarizing the key properties of each objective would be helpful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998255372047424,
                    "sentence": "2. Evaluation on Downstream Tasks: While the paper focuses on intrinsic evaluation tasks, it would strengthen the work to include results on downstream applications such as spoken term detection or speech recognition.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997600317001343,
                    "sentence": "This would provide additional evidence of the practical utility of the embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997845888137817,
                    "sentence": "3. Phonetic Supervision: The authors mention the potential of using phonetic sequences for supervision.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999582052230835,
                    "sentence": "Including a small-scale experiment or analysis comparing orthographic and phonetic supervision would provide valuable insights.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994703531265259,
                    "sentence": "4. Negative Sampling Strategy: The paper uses random negative sampling for contrastive losses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996674060821533,
                    "sentence": "Exploring more sophisticated sampling strategies, such as hard negative mining, could further improve the embeddings and is worth discussing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9950700998306274,
                    "sentence": "Questions for the Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9961864948272705,
                    "sentence": "1. How do the embeddings generalize to unseen words in real-world speech recognition or retrieval tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9960776567459106,
                    "sentence": "Have you tested the approach on larger, more diverse datasets?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9928396344184875,
                    "sentence": "2. Could you elaborate on why the cost-sensitive loss did not yield significant improvements in word discrimination tasks, despite its success in word similarity tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9824777245521545,
                    "sentence": "3. How sensitive are the results to the choice of hyperparameters, such as the margin in the contrastive losses or the number of LSTM layers?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9774734973907471,
                    "sentence": "Overall, this paper makes a significant contribution to the field of acoustic word embeddings and is a strong candidate for acceptance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9608227014541626,
                    "sentence": "The suggestions provided are aimed at further enhancing the clarity and impact of the work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9984930238596827,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984930238596827,
                "mixed": 0.001506976140317253
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984930238596827,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984930238596827,
                    "human": 0,
                    "mixed": 0.001506976140317253
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary of Contributions\nThis paper addresses the problem of learning neural acoustic word embeddings that map speech segments to fixed-dimensional vectors, enabling tasks such as speech retrieval and recognition. The authors propose a novel multi-view approach that jointly learns embeddings for acoustic sequences and their corresponding character sequences using deep bidirectional LSTMs and multi-view contrastive losses. The paper introduces several loss variants, including fixed-margin and cost-sensitive losses, and evaluates their effectiveness on tasks such as word discrimination, cross-view word discrimination, and word similarity. The proposed approach outperforms prior methods in acoustic word discrimination and demonstrates promising results in cross-view tasks, highlighting the utility of multi-view embeddings for both spoken and written query scenarios.\nDecision: Accept\nKey reasons for acceptance include the novelty of the multi-view approach, which is well-motivated and fills a gap in the literature, and the strong empirical results that demonstrate clear improvements over prior methods. The paper is methodologically rigorous, with thorough experiments and detailed analyses that support its claims.\nSupporting Arguments\n1. Problem and Motivation: The paper tackles a relevant and underexplored problem\"\"learning embeddings that capture acoustic and orthographic information jointly. The motivation is well-placed in the literature, as prior work has largely focused on single-view acoustic embeddings or ignored the relationship between acoustic and orthographic representations. The multi-view approach is a natural and innovative extension that addresses these gaps.\n2. Methodological Rigor: The use of bidirectional LSTMs and contrastive losses is appropriate for the sequential nature of the data. The authors systematically explore different loss functions and architectures, providing a comprehensive evaluation. The inclusion of a cost-sensitive loss to capture orthographic edit distances is a thoughtful addition that aligns well with the goal of improving word similarity tasks.\n3. Results and Impact: The proposed embeddings achieve state-of-the-art performance on acoustic word discrimination and enable new cross-view tasks. The visualization of embeddings and the analysis of word similarity further validate the approach. The results are robust and reproducible, with the code made publicly available.\nSuggestions for Improvement\n1. Clarity of Objectives: While the paper discusses multiple objectives (e.g., obj0, obj1, etc.), the presentation could be streamlined to make the differences between these objectives clearer to readers unfamiliar with contrastive learning. A table summarizing the key properties of each objective would be helpful.\n2. Evaluation on Downstream Tasks: While the paper focuses on intrinsic evaluation tasks, it would strengthen the work to include results on downstream applications such as spoken term detection or speech recognition. This would provide additional evidence of the practical utility of the embeddings.\n3. Phonetic Supervision: The authors mention the potential of using phonetic sequences for supervision. Including a small-scale experiment or analysis comparing orthographic and phonetic supervision would provide valuable insights.\n4. Negative Sampling Strategy: The paper uses random negative sampling for contrastive losses. Exploring more sophisticated sampling strategies, such as hard negative mining, could further improve the embeddings and is worth discussing.\nQuestions for the Authors\n1. How do the embeddings generalize to unseen words in real-world speech recognition or retrieval tasks? Have you tested the approach on larger, more diverse datasets?\n2. Could you elaborate on why the cost-sensitive loss did not yield significant improvements in word discrimination tasks, despite its success in word similarity tasks?\n3. How sensitive are the results to the choice of hyperparameters, such as the margin in the contrastive losses or the number of LSTM layers?\nOverall, this paper makes a significant contribution to the field of acoustic word embeddings and is a strong candidate for acceptance. The suggestions provided are aimed at further enhancing the clarity and impact of the work."
        }
    ]
}