{
    "version": "2025-01-09-base",
    "scanId": "44a48dde-7bfc-43c9-92f5-dd2c5dc2d9f6",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9996962547302246,
                    "sentence": "Summary of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999578595161438,
                    "sentence": "The paper proposes a novel method for understanding what a trained Long Short Term Memory (LSTM) network has learned by extracting important phrases that contribute to its predictions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999116659164429,
                    "sentence": "The approach involves decomposing the output of an LSTM into a product of factors, where each term represents the contribution of a particular word to the predicted probability of a class.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999876081943512,
                    "sentence": "The authors demonstrate the effectiveness of their method on sentiment analysis and question answering tasks, showing that the extracted phrases can be used to construct a simple, rule-based classifier that approximates the performance of the LSTM.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990311861038208,
                    "sentence": "Decision",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9978992342948914,
                    "sentence": "I decide to reject this paper, with the main reasons being that the approach may not generalize well to other question answering tasks with free-form text answers and has unclear performance on tasks requiring continuous spans over the original document.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984358549118042,
                    "sentence": "Supporting Arguments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993621110916138,
                    "sentence": "The paper presents a promising approach for understanding LSTM models, but its limitations and potential biases need to be carefully considered.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994610548019409,
                    "sentence": "The method is evaluated on only one dataset and one model architecture, which raises concerns about its generalizability to other tasks and models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994503855705261,
                    "sentence": "Additionally, the approach may struggle with handling word types like numbers or entity names, which could require additional processing steps such as entity detection.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982742071151733,
                    "sentence": "Additional Feedback",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992372393608093,
                    "sentence": "To improve the paper, I suggest that the authors consider the following:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996026158332825,
                    "sentence": "* Evaluate the approach on a wider range of datasets and model architectures to demonstrate its generalizability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989603757858276,
                    "sentence": "* Investigate the use of attention models as a baseline for comparison, in addition to the gradient-based baseline.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998945415019989,
                    "sentence": "* Address the minor issues with undefined variables, incorrect references, and formatting errors throughout the text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994981288909912,
                    "sentence": "* Provide more detailed analysis of the extracted phrases and their relationship to the LSTM's predictions, to gain a deeper understanding of what the model has learned.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996690154075623,
                    "sentence": "Questions for the Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996271729469299,
                    "sentence": "To clarify my understanding of the paper and provide additional evidence for my assessment, I would like the authors to answer the following questions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998679757118225,
                    "sentence": "* How do the authors plan to address the potential limitations of their approach in handling word types like numbers or entity names?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998065829277039,
                    "sentence": "* Can the authors provide more detailed analysis of the extracted phrases and their relationship to the LSTM's predictions, to demonstrate the effectiveness of their method?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996371269226074,
                    "sentence": "* How do the authors plan to evaluate the generalizability of their approach to other question answering tasks and model architectures?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary of the Paper\nThe paper proposes a novel method for understanding what a trained Long Short Term Memory (LSTM) network has learned by extracting important phrases that contribute to its predictions. The approach involves decomposing the output of an LSTM into a product of factors, where each term represents the contribution of a particular word to the predicted probability of a class. The authors demonstrate the effectiveness of their method on sentiment analysis and question answering tasks, showing that the extracted phrases can be used to construct a simple, rule-based classifier that approximates the performance of the LSTM.\nDecision\nI decide to reject this paper, with the main reasons being that the approach may not generalize well to other question answering tasks with free-form text answers and has unclear performance on tasks requiring continuous spans over the original document.\nSupporting Arguments\nThe paper presents a promising approach for understanding LSTM models, but its limitations and potential biases need to be carefully considered. The method is evaluated on only one dataset and one model architecture, which raises concerns about its generalizability to other tasks and models. Additionally, the approach may struggle with handling word types like numbers or entity names, which could require additional processing steps such as entity detection.\nAdditional Feedback\nTo improve the paper, I suggest that the authors consider the following:\n* Evaluate the approach on a wider range of datasets and model architectures to demonstrate its generalizability.\n* Investigate the use of attention models as a baseline for comparison, in addition to the gradient-based baseline.\n* Address the minor issues with undefined variables, incorrect references, and formatting errors throughout the text.\n* Provide more detailed analysis of the extracted phrases and their relationship to the LSTM's predictions, to gain a deeper understanding of what the model has learned.\nQuestions for the Authors\nTo clarify my understanding of the paper and provide additional evidence for my assessment, I would like the authors to answer the following questions:\n* How do the authors plan to address the potential limitations of their approach in handling word types like numbers or entity names?\n* Can the authors provide more detailed analysis of the extracted phrases and their relationship to the LSTM's predictions, to demonstrate the effectiveness of their method?\n* How do the authors plan to evaluate the generalizability of their approach to other question answering tasks and model architectures?"
        }
    ]
}