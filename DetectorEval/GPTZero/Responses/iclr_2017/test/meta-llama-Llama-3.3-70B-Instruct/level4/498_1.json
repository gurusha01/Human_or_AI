{
    "version": "2025-01-09-base",
    "scanId": "bf0e765f-8de2-4f8a-96f7-e1a008ff3536",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999415874481201,
                    "sentence": "This paper presents a novel interpretation of dropout using a latent variable model, where the dropout variable is treated as an unobserved variable that is marginalized.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998049736022949,
                    "sentence": "The maximum likelihood estimation under this model is intractable, but it is shown that standard dropout can be viewed as a simple Monte Carlo approximation of the maximum likelihood estimate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999873161315918,
                    "sentence": "The authors then develop a theoretical framework to analyze the discrepancy, referred to as the inference gap, between the model used during training (an ensemble of models or the latent variable model) and the model used during testing (where the expectation over activations is typically approximated by a single model with averaged weights).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997571706771851,
                    "sentence": "This framework introduces several key concepts, including expectation linearity, which enables the examination of transition functions and layers that can minimize the inference gap.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997128248214722,
                    "sentence": "Theorem 3 provides a bound on the inference gap, offering valuable insights into the relationship between the training and testing models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998010993003845,
                    "sentence": "Furthermore, the paper proposes a new regularization term designed to minimize the inference gap during the learning process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992884993553162,
                    "sentence": "Experimental results on the MNIST, CIFAR-10, and CIFAR-100 datasets demonstrate the potential of this method to outperform standard dropout and achieve performance comparable to Monte Carlo Dropout, albeit at a higher computational cost due to the introduction of a new hyperparameter.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9976139068603516,
                    "sentence": "The theoretical model presented in this study offers a compelling interpretation of dropout as a latent variable model, with standard dropout being a Monte Carlo approximation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9968744516372681,
                    "sentence": "This contribution has significant potential for wide applicability in future dropout research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982264637947083,
                    "sentence": "The framework for analyzing the inference gap is also noteworthy, although its applicability may be more limited.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975183606147766,
                    "sentence": "The proposed approach is convincing, but it has some limitations, including the use of simple datasets, relatively small performance gains, and increased computational cost during training due to the new hyperparameter.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972440600395203,
                    "sentence": "Additionally, a typo is noted on page 6, line 8, where \"expecatation\" should be corrected to \"expectation\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a novel interpretation of dropout using a latent variable model, where the dropout variable is treated as an unobserved variable that is marginalized. The maximum likelihood estimation under this model is intractable, but it is shown that standard dropout can be viewed as a simple Monte Carlo approximation of the maximum likelihood estimate.\nThe authors then develop a theoretical framework to analyze the discrepancy, referred to as the inference gap, between the model used during training (an ensemble of models or the latent variable model) and the model used during testing (where the expectation over activations is typically approximated by a single model with averaged weights). This framework introduces several key concepts, including expectation linearity, which enables the examination of transition functions and layers that can minimize the inference gap. Theorem 3 provides a bound on the inference gap, offering valuable insights into the relationship between the training and testing models.\nFurthermore, the paper proposes a new regularization term designed to minimize the inference gap during the learning process. Experimental results on the MNIST, CIFAR-10, and CIFAR-100 datasets demonstrate the potential of this method to outperform standard dropout and achieve performance comparable to Monte Carlo Dropout, albeit at a higher computational cost due to the introduction of a new hyperparameter.\nThe theoretical model presented in this study offers a compelling interpretation of dropout as a latent variable model, with standard dropout being a Monte Carlo approximation. This contribution has significant potential for wide applicability in future dropout research. The framework for analyzing the inference gap is also noteworthy, although its applicability may be more limited.\nThe proposed approach is convincing, but it has some limitations, including the use of simple datasets, relatively small performance gains, and increased computational cost during training due to the new hyperparameter. Additionally, a typo is noted on page 6, line 8, where \"expecatation\" should be corrected to \"expectation\"."
        }
    ]
}