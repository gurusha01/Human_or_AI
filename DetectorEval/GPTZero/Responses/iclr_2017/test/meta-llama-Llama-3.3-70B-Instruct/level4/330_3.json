{
    "version": "2025-01-09-base",
    "scanId": "02746c1c-0c40-433a-a3c7-3afd4df02baa",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9663500785827637,
                    "sentence": "This paper presents a method for learning document embeddings by summing the embeddings of individual words, which are learned jointly and subjected to random corruption during training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9495974779129028,
                    "sentence": "Although the components of this approach are not particularly innovative, they combine to form an efficient algorithm for document representation that yields impressive empirical results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9239077568054199,
                    "sentence": "The concept of jointly training word and document embeddings is well-established, and representing documents as the sum of their word embeddings has been explored previously (e.g., in \"The Sum of Its Parts\": Joint Learning of Word and Phrase Representations with Autoencoders\" by Lebret and Collobert).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9450632929801941,
                    "sentence": "Moreover, the corruption mechanism employed in this paper is essentially a straightforward application of dropout to the input layer, paired with a word2vec-style loss function and training methodology, which does not introduce significant novelty.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9109851717948914,
                    "sentence": "However, this approach excels in terms of efficiency during generation, as it only requires averaging the word embeddings, thereby eliminating the need for a complex inference step as seen in Doc2Vec.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8722508549690247,
                    "sentence": "By design, the resulting embedding captures key global information about the document, specifically the aspects that facilitate local-context prediction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8474905490875244,
                    "sentence": "Given the simplicity of the model, its performance on sentiment analysis and document classification tasks is noteworthy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7953481674194336,
                    "sentence": "In conclusion, while this paper may not break new ground in terms of innovation, its simplicity, efficiency, and performance make it a valuable contribution worthy of broader consideration and study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7593621611595154,
                    "sentence": "As such, I recommend that it be accepted for presentation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.706120647383059,
            "class_probabilities": {
                "human": 0.29372542799595996,
                "ai": 0.706120647383059,
                "mixed": 0.0001539246209811207
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.706120647383059,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.706120647383059,
                    "human": 0.29372542799595996,
                    "mixed": 0.0001539246209811207
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a method for learning document embeddings by summing the embeddings of individual words, which are learned jointly and subjected to random corruption during training. Although the components of this approach are not particularly innovative, they combine to form an efficient algorithm for document representation that yields impressive empirical results.\nThe concept of jointly training word and document embeddings is well-established, and representing documents as the sum of their word embeddings has been explored previously (e.g., in \"The Sum of Its Parts\": Joint Learning of Word and Phrase Representations with Autoencoders\" by Lebret and Collobert). Moreover, the corruption mechanism employed in this paper is essentially a straightforward application of dropout to the input layer, paired with a word2vec-style loss function and training methodology, which does not introduce significant novelty.\nHowever, this approach excels in terms of efficiency during generation, as it only requires averaging the word embeddings, thereby eliminating the need for a complex inference step as seen in Doc2Vec. By design, the resulting embedding captures key global information about the document, specifically the aspects that facilitate local-context prediction. Given the simplicity of the model, its performance on sentiment analysis and document classification tasks is noteworthy.\nIn conclusion, while this paper may not break new ground in terms of innovation, its simplicity, efficiency, and performance make it a valuable contribution worthy of broader consideration and study. As such, I recommend that it be accepted for presentation."
        }
    ]
}