{
    "version": "2025-01-09-base",
    "scanId": "3564baea-a793-4d94-9e60-04cd71f4ba5e",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9893269538879395,
                    "sentence": "This paper presents a parallelized stochastic gradient descent (SGD) method for scenarios where gradients can be computed through linear operations, such as least squares linear regression and polynomial regression problems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.988956868648529,
                    "sentence": "The authors' goal is to replicate the effects of sequential SGD using a proposed combiner, which is optimized by employing a randomized projection matrix for dimension reduction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9868522882461548,
                    "sentence": "Experimental results indicate that the proposed method achieves better speedups compared to existing methods like Hogwild!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9843651652336121,
                    "sentence": "and Allreduce.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9937899708747864,
                    "sentence": "However, there appears to be a fundamental misconception regarding SGD.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9906877279281616,
                    "sentence": "The generation of the combiner matrix M can be costly and large.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9881219863891602,
                    "sentence": "In contrast to sequential SGD, which requires O(f) space and time to update the weight vector w, where f is the number of features, the parallel SGD approach has a space and time complexity of O(f^2) due to M being an f x f matrix.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.985953688621521,
                    "sentence": "This implies that O(f) processors would be necessary to achieve constant speedups, which is impractical, especially for datasets with thousands or millions of features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9850504398345947,
                    "sentence": "It is noteworthy that updating Mi * v, where v is an f-dimensional vector, does not necessarily require O(f^2) space and complexity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9689982533454895,
                    "sentence": "Since Mi is a low-rank matrix of the form (I - ai ai'), the complexity can be reduced to O(f) by computing it as O(v - ai (ai' v)).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9445027709007263,
                    "sentence": "If M_i is defined as the product of n rank-1 matrices, the complexity and space complexity would be O(fn), with n being significantly smaller than f in the context of this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9685908555984497,
                    "sentence": "This raises concerns about the validity of the authors' assumptions, experiments, and strategies, which seem to be based on an incorrect assumption about the space and complexity of SGD.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9767561554908752,
                    "sentence": "The source of potential speedup in this approach is unclear, as it is uncertain what computations are being parallelized and why the sequential algorithm would yield speedup if M_i*v is computed efficiently.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9878326654434204,
                    "sentence": "To enhance the clarity and theoretical solidity of this paper, the authors are advised to:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9911286234855652,
                    "sentence": "- Provide the computational complexity per step of the proposed algorithm",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9880573153495789,
                    "sentence": "- Conduct a convergence rate analysis: it is essential to understand how dimension reduction affects complexity, as mere convergence analysis is insufficient.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984800378301695,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984800378301695,
                "mixed": 0.0015199621698304396
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984800378301695,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984800378301695,
                    "human": 0,
                    "mixed": 0.0015199621698304396
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a parallelized stochastic gradient descent (SGD) method for scenarios where gradients can be computed through linear operations, such as least squares linear regression and polynomial regression problems. The authors' goal is to replicate the effects of sequential SGD using a proposed combiner, which is optimized by employing a randomized projection matrix for dimension reduction. Experimental results indicate that the proposed method achieves better speedups compared to existing methods like Hogwild! and Allreduce.\nHowever, there appears to be a fundamental misconception regarding SGD. The generation of the combiner matrix M can be costly and large. In contrast to sequential SGD, which requires O(f) space and time to update the weight vector w, where f is the number of features, the parallel SGD approach has a space and time complexity of O(f^2) due to M being an f x f matrix. This implies that O(f) processors would be necessary to achieve constant speedups, which is impractical, especially for datasets with thousands or millions of features.\nIt is noteworthy that updating Mi * v, where v is an f-dimensional vector, does not necessarily require O(f^2) space and complexity. Since Mi is a low-rank matrix of the form (I - ai ai'), the complexity can be reduced to O(f) by computing it as O(v - ai (ai' v)). If M_i is defined as the product of n rank-1 matrices, the complexity and space complexity would be O(fn), with n being significantly smaller than f in the context of this paper. This raises concerns about the validity of the authors' assumptions, experiments, and strategies, which seem to be based on an incorrect assumption about the space and complexity of SGD.\nThe source of potential speedup in this approach is unclear, as it is uncertain what computations are being parallelized and why the sequential algorithm would yield speedup if M_i*v is computed efficiently. To enhance the clarity and theoretical solidity of this paper, the authors are advised to:\n- Provide the computational complexity per step of the proposed algorithm\n- Conduct a convergence rate analysis: it is essential to understand how dimension reduction affects complexity, as mere convergence analysis is insufficient."
        }
    ]
}