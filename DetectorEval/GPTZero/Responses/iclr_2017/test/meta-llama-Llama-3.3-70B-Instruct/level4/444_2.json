{
    "version": "2025-01-09-base",
    "scanId": "750e8f19-ad35-4a89-8c15-85f7a94888b6",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9994657635688782,
                    "sentence": "This study presents a method for extracting patterns to comprehend the knowledge acquired by a trained LSTM and to develop a hand-coded algorithm that mimics the LSTM's performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987916946411133,
                    "sentence": "Although promising results are demonstrated on a single dataset with one model architecture, the generalizability of this approach remains uncertain.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988927245140076,
                    "sentence": "Nonetheless, it appears to be a valuable tool for understanding and debugging models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9973775744438171,
                    "sentence": "The WikiMovies dataset, which seems to feature template-generated questions, may be well-suited for this pattern matching approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997981071472168,
                    "sentence": "However, it is unclear whether this method can be applied to other question-answering tasks that involve free-form text answers not present as substrings in the document.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977791905403137,
                    "sentence": "Additionally, it is uncertain whether the model is required to produce a continuous span over the original document.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9967843294143677,
                    "sentence": "The approach appears to have limitations in handling specific word types, such as numbers and entity names, which can be encoded in word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983184933662415,
                    "sentence": "The algorithm's description suggests that an entity detector is required, implying that the approach may be unable to identify entities through the decomposition of the LSTM's output.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974019527435303,
                    "sentence": "The results of the manual pattern matching experiment, which utilized explicit year annotations, indicate that the automatic method struggles with word types.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977988600730896,
                    "sentence": "The inclusion of an attention model as a baseline, in addition to the gradient-based baseline, would be beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982288479804993,
                    "sentence": "Some minor issues were noted:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9945387840270996,
                    "sentence": "- The variables P and Q are not defined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9965845942497253,
                    "sentence": "- Several references appear to be incorrect, such as the citation \"in 1\" instead of \"in table 1\" in section 5.1, and similar errors above section 7 and in section 7.1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9961325526237488,
                    "sentence": "- A minor typo was found in the paragraph preceding section 6.3, where \"adam\" should be capitalized as \"Adam\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This study presents a method for extracting patterns to comprehend the knowledge acquired by a trained LSTM and to develop a hand-coded algorithm that mimics the LSTM's performance. Although promising results are demonstrated on a single dataset with one model architecture, the generalizability of this approach remains uncertain. Nonetheless, it appears to be a valuable tool for understanding and debugging models.\nThe WikiMovies dataset, which seems to feature template-generated questions, may be well-suited for this pattern matching approach. However, it is unclear whether this method can be applied to other question-answering tasks that involve free-form text answers not present as substrings in the document. Additionally, it is uncertain whether the model is required to produce a continuous span over the original document.\nThe approach appears to have limitations in handling specific word types, such as numbers and entity names, which can be encoded in word embeddings. The algorithm's description suggests that an entity detector is required, implying that the approach may be unable to identify entities through the decomposition of the LSTM's output. The results of the manual pattern matching experiment, which utilized explicit year annotations, indicate that the automatic method struggles with word types.\nThe inclusion of an attention model as a baseline, in addition to the gradient-based baseline, would be beneficial. \nSome minor issues were noted: \n- The variables P and Q are not defined.\n- Several references appear to be incorrect, such as the citation \"in 1\" instead of \"in table 1\" in section 5.1, and similar errors above section 7 and in section 7.1.\n- A minor typo was found in the paragraph preceding section 6.3, where \"adam\" should be capitalized as \"Adam\"."
        }
    ]
}