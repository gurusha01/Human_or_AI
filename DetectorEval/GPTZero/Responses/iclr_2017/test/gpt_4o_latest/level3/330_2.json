{
    "version": "2025-01-09-base",
    "scanId": "21ecda86-4a7d-41f5-acc1-a274aa6d2db9",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "Review of \"Document Vector through Corruption (Doc2VecC)\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999896883964539,
                    "sentence": "Summary of Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999950528144836,
                    "sentence": "The paper proposes Doc2VecC, a novel framework for document representation that averages word embeddings while incorporating a corruption-based, data-dependent regularization mechanism.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999918341636658,
                    "sentence": "This regularization emphasizes rare and informative words while suppressing common, non-discriminative ones.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999926090240479,
                    "sentence": "The authors claim that Doc2VecC achieves state-of-the-art performance in sentiment analysis, document classification, and semantic relatedness tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999983012676239,
                    "sentence": "The method is computationally efficient, capable of training on billions of words per hour on a single machine, and generates document representations quickly at test time.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999911785125732,
                    "sentence": "The simplicity of the model, combined with its ability to outperform more complex methods, is a key highlight.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999604225158691,
                    "sentence": "Decision: Reject",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999774694442749,
                    "sentence": "While the paper demonstrates promising results and emphasizes simplicity and efficiency, it lacks significant technical novelty and broader evaluations to justify acceptance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999832510948181,
                    "sentence": "The proposed method is more of an incremental improvement over existing techniques rather than a groundbreaking contribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999892115592957,
                    "sentence": "Supporting Arguments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999886751174927,
                    "sentence": "1. Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999849796295166,
                    "sentence": "- Simplicity and Efficiency: The model's ability to train and test efficiently is commendable, making it suitable for large-scale applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999806880950928,
                    "sentence": "- Empirical Performance: Doc2VecC achieves competitive or superior results compared to baselines like Word2Vec, Paragraph Vectors, and Skip-thought Vectors in sentiment analysis and document classification tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99998939037323,
                    "sentence": "- Data-Dependent Regularization: The corruption mechanism introduces an elegant way to favor rare/informative words, which is a meaningful addition to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999943375587463,
                    "sentence": "2. Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999961853027344,
                    "sentence": "- Limited Technical Novelty: The approach builds on well-established techniques, such as averaging word embeddings and using corruption for regularization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999949932098389,
                    "sentence": "While effective, it does not introduce fundamentally new ideas or methodologies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971389770508,
                    "sentence": "- Narrow Scope of Evaluation: The experiments focus primarily on sentiment analysis, document classification, and semantic relatedness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999975562095642,
                    "sentence": "Broader evaluations on diverse NLP tasks (e.g., machine translation, question answering) are missing, which limits the generalizability of the claims.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999977946281433,
                    "sentence": "- Comparison with Advanced Models: The paper does not compare Doc2VecC with bidirectional LSTM-based or transformer-based document representations, which are more widely used in modern NLP tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999926686286926,
                    "sentence": "This omission weakens the evaluation rigor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999954700469971,
                    "sentence": "- Unclear Details: The training objective of RNN-LM and the rationale for choosing averaged hidden states over final states for representation need clarification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999856352806091,
                    "sentence": "Suggestions for Improvement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992079734802246,
                    "sentence": "1. Broader Task Evaluation: Extend the evaluation to include a wider range of NLP tasks, such as summarization, information retrieval, or question answering, to demonstrate the generalizability of Doc2VecC.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999534547328949,
                    "sentence": "2. Comparison with Modern Architectures: Include comparisons with bidirectional LSTMs, transformers, and other state-of-the-art document representation methods to strengthen the empirical evaluation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992163181304932,
                    "sentence": "3. Clarify Methodological Choices: Provide more details on the RNN-LM training objective and justify the choice of averaged hidden states for document representation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992293119430542,
                    "sentence": "4. Ablation Studies: Conduct ablation studies to isolate the impact of the corruption mechanism and the data-dependent regularization on performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989886283874512,
                    "sentence": "5. Theoretical Insights: Offer deeper theoretical insights into why the corruption mechanism works effectively and how it compares to other regularization techniques.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9970735907554626,
                    "sentence": "Questions for the Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984862208366394,
                    "sentence": "1. How does Doc2VecC perform on tasks involving long documents or hierarchical structures, such as multi-document summarization or topic modeling?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9961599111557007,
                    "sentence": "2. Why were bidirectional LSTM-based representations excluded from the comparison?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974767565727234,
                    "sentence": "Would Doc2VecC still outperform them in terms of accuracy and efficiency?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972004890441895,
                    "sentence": "3. Can the corruption mechanism be extended or adapted for tasks requiring structured outputs, such as sequence labeling or dependency parsing?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9933505654335022,
                    "sentence": "4. How sensitive is Doc2VecC to the corruption rate (q)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.992473840713501,
                    "sentence": "Are there guidelines for selecting this hyperparameter?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.992246687412262,
                    "sentence": "In conclusion, while Doc2VecC is a promising and efficient approach, the paper's lack of technical novelty and limited evaluation scope prevent it from meeting the standards of acceptance at this conference.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9756404161453247,
                    "sentence": "Addressing the above concerns would significantly strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 36,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of \"Document Vector through Corruption (Doc2VecC)\"\nSummary of Contributions\nThe paper proposes Doc2VecC, a novel framework for document representation that averages word embeddings while incorporating a corruption-based, data-dependent regularization mechanism. This regularization emphasizes rare and informative words while suppressing common, non-discriminative ones. The authors claim that Doc2VecC achieves state-of-the-art performance in sentiment analysis, document classification, and semantic relatedness tasks. The method is computationally efficient, capable of training on billions of words per hour on a single machine, and generates document representations quickly at test time. The simplicity of the model, combined with its ability to outperform more complex methods, is a key highlight.\nDecision: Reject\nWhile the paper demonstrates promising results and emphasizes simplicity and efficiency, it lacks significant technical novelty and broader evaluations to justify acceptance. The proposed method is more of an incremental improvement over existing techniques rather than a groundbreaking contribution.\nSupporting Arguments\n1. Strengths:\n - Simplicity and Efficiency: The model's ability to train and test efficiently is commendable, making it suitable for large-scale applications.\n - Empirical Performance: Doc2VecC achieves competitive or superior results compared to baselines like Word2Vec, Paragraph Vectors, and Skip-thought Vectors in sentiment analysis and document classification tasks.\n - Data-Dependent Regularization: The corruption mechanism introduces an elegant way to favor rare/informative words, which is a meaningful addition to the field.\n2. Weaknesses:\n - Limited Technical Novelty: The approach builds on well-established techniques, such as averaging word embeddings and using corruption for regularization. While effective, it does not introduce fundamentally new ideas or methodologies.\n - Narrow Scope of Evaluation: The experiments focus primarily on sentiment analysis, document classification, and semantic relatedness. Broader evaluations on diverse NLP tasks (e.g., machine translation, question answering) are missing, which limits the generalizability of the claims.\n - Comparison with Advanced Models: The paper does not compare Doc2VecC with bidirectional LSTM-based or transformer-based document representations, which are more widely used in modern NLP tasks. This omission weakens the evaluation rigor.\n - Unclear Details: The training objective of RNN-LM and the rationale for choosing averaged hidden states over final states for representation need clarification.\nSuggestions for Improvement\n1. Broader Task Evaluation: Extend the evaluation to include a wider range of NLP tasks, such as summarization, information retrieval, or question answering, to demonstrate the generalizability of Doc2VecC.\n2. Comparison with Modern Architectures: Include comparisons with bidirectional LSTMs, transformers, and other state-of-the-art document representation methods to strengthen the empirical evaluation.\n3. Clarify Methodological Choices: Provide more details on the RNN-LM training objective and justify the choice of averaged hidden states for document representation.\n4. Ablation Studies: Conduct ablation studies to isolate the impact of the corruption mechanism and the data-dependent regularization on performance.\n5. Theoretical Insights: Offer deeper theoretical insights into why the corruption mechanism works effectively and how it compares to other regularization techniques.\nQuestions for the Authors\n1. How does Doc2VecC perform on tasks involving long documents or hierarchical structures, such as multi-document summarization or topic modeling?\n2. Why were bidirectional LSTM-based representations excluded from the comparison? Would Doc2VecC still outperform them in terms of accuracy and efficiency?\n3. Can the corruption mechanism be extended or adapted for tasks requiring structured outputs, such as sequence labeling or dependency parsing?\n4. How sensitive is Doc2VecC to the corruption rate (q)? Are there guidelines for selecting this hyperparameter?\nIn conclusion, while Doc2VecC is a promising and efficient approach, the paper's lack of technical novelty and limited evaluation scope prevent it from meeting the standards of acceptance at this conference. Addressing the above concerns would significantly strengthen the paper."
        }
    ]
}