{
    "version": "2025-01-09-base",
    "scanId": "baf224f8-fbe3-4276-b244-dcb5633d8d82",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9993787407875061,
                    "sentence": "Authors' response adequately addressed my questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998176097869873,
                    "sentence": "Thank you.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979513883590698,
                    "sentence": "Evaluation remains unchanged.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983716607093811,
                    "sentence": "This paper introduces a neural model designed to generate tree-structured outputs from scratch.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977573156356812,
                    "sentence": "The proposed model: 1) decouples the recurrence between depths and siblings, and 2) separates the processes of topology generation and label generation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997076690196991,
                    "sentence": "It demonstrates superior performance compared to prior approaches on the benchmark IFTTT dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9966518878936768,
                    "sentence": "Unlike earlier tree-decoding methods, this model eliminates the need for manually annotating subtrees with special tokens, making it a compelling alternative for such tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.996279776096344,
                    "sentence": "The paper presents robust experiments on a synthetic dataset and achieves better results than competing methods on a real-world IFTTT dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9952129125595093,
                    "sentence": "The paper includes several intriguing findings that merit further exploration.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.993651270866394,
                    "sentence": "First, on the synthetic dataset, the precision declines sharply as the number of nodes increases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9949475526809692,
                    "sentence": "Is this due to the sequential encoder's vector representation failing to capture sufficient information from long sequences, thereby hindering the tree decoder's performance?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9884984493255615,
                    "sentence": "Or is it because the tree decoder struggles to handle long sequence inputs, i.e., large tree structures?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9823681116104126,
                    "sentence": "Understanding this distinction is crucial for developing improved models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9882116317749023,
                    "sentence": "For instance, if the encoder is at fault, incorporating an attention mechanism, as seen in seq-to-seq models, might help retain more information from the input sequence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9885748624801636,
                    "sentence": "Additionally, beyond analyzing how precision varies with the number of nodes in the tree, it could be insightful to examine its relationship with 1) tree depth, 2) tree width, 3) symmetry, and other structural factors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9886956810951233,
                    "sentence": "Furthermore, since greedy search is employed during decoding, it would be interesting to investigate whether using beam search could enhance the tree decoding process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9959607720375061,
                    "sentence": "For the IFTTT dataset, providing additional statistics about the dataset would aid in understanding the task's complexity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.996985673904419,
                    "sentence": "For example, how deep are the trees?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9977489709854126,
                    "sentence": "What are the vocabulary sizes for both the language and program sides?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9978755712509155,
                    "sentence": "The paper is well-written, aside from minor typographical errors noted in my pre-review questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975154399871826,
                    "sentence": "Overall, I find this to be a solid paper with significant potential for further exploration in this area.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998790442943573,
                    "sentence": "I am inclined to recommend its acceptance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Authors' response adequately addressed my questions. Thank you. \nEvaluation remains unchanged.\nThis paper introduces a neural model designed to generate tree-structured outputs from scratch. The proposed model: 1) decouples the recurrence between depths and siblings, and 2) separates the processes of topology generation and label generation. It demonstrates superior performance compared to prior approaches on the benchmark IFTTT dataset. Unlike earlier tree-decoding methods, this model eliminates the need for manually annotating subtrees with special tokens, making it a compelling alternative for such tasks. The paper presents robust experiments on a synthetic dataset and achieves better results than competing methods on a real-world IFTTT dataset.\nThe paper includes several intriguing findings that merit further exploration. First, on the synthetic dataset, the precision declines sharply as the number of nodes increases. Is this due to the sequential encoder's vector representation failing to capture sufficient information from long sequences, thereby hindering the tree decoder's performance? Or is it because the tree decoder struggles to handle long sequence inputs, i.e., large tree structures? Understanding this distinction is crucial for developing improved models. For instance, if the encoder is at fault, incorporating an attention mechanism, as seen in seq-to-seq models, might help retain more information from the input sequence.\nAdditionally, beyond analyzing how precision varies with the number of nodes in the tree, it could be insightful to examine its relationship with 1) tree depth, 2) tree width, 3) symmetry, and other structural factors. Furthermore, since greedy search is employed during decoding, it would be interesting to investigate whether using beam search could enhance the tree decoding process.\nFor the IFTTT dataset, providing additional statistics about the dataset would aid in understanding the task's complexity. For example, how deep are the trees? What are the vocabulary sizes for both the language and program sides?\nThe paper is well-written, aside from minor typographical errors noted in my pre-review questions.\nOverall, I find this to be a solid paper with significant potential for further exploration in this area. I am inclined to recommend its acceptance."
        }
    ]
}