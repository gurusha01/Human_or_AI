{
    "version": "2025-01-09-base",
    "scanId": "4bfe2aa3-068a-42f4-9a9f-40a10323e8c4",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9931217432022095,
                    "sentence": "The paper presents dropout through the lens of a latent variable model, where the dropout variable (indicating whether units are dropped, with values of 0 or 1) is unobserved and subsequently marginalised.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9931257367134094,
                    "sentence": "While maximum likelihood estimation under this model is intractable, standard dropout is shown to correspond to a straightforward Monte Carlo approximation of maximum likelihood for this latent variable model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9907990097999573,
                    "sentence": "The authors then propose a theoretical framework to analyse the discrepancy\"\"termed the inference gap\"\"between the model used during training (a model ensemble, or in this case, the latent variable model) and the model used during testing (where an expectation over activations across multiple models is approximated by the activation of a single model with averaged weights).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9897063970565796,
                    "sentence": "This framework introduces several key concepts, such as expectation linearity, which facilitate the examination of which transition functions (or, more broadly, layers) are capable of yielding a small inference gap.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9901163578033447,
                    "sentence": "Notably, Theorem 3 provides a bound on this inference gap.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9768035411834717,
                    "sentence": "Additionally, the paper introduces a novel regularisation term designed to minimise the inference gap during the training process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9694928526878357,
                    "sentence": "Experiments conducted on MNIST, CIFAR-10, and CIFAR-100 demonstrate that the proposed method has the potential to outperform standard dropout and achieve performance comparable to Monte Carlo Dropout.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9627916216850281,
                    "sentence": "The latter is the standard approach for computing dropout outputs in a manner consistent with the training assumption of an ensemble, albeit at a significantly higher computational cost.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9835558533668518,
                    "sentence": "Overall, the study offers a compelling theoretical model for dropout as a latent variable model, framing standard dropout as a Monte Carlo approximation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9791796803474426,
                    "sentence": "This perspective is likely to have broad applicability in future research on dropout.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9890265464782715,
                    "sentence": "The framework for analysing the inference gap is intriguing, though it may have somewhat narrower applicability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.992827296257019,
                    "sentence": "The proposed model is persuasive, but it is worth noting that: 1) the experiments are limited to relatively simple datasets, 2) the performance improvements are modest, and 3) the training process incurs additional computational overhead due to the introduction of a new hyperparameter.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9953877925872803,
                    "sentence": "Typo: Page 6, line 8 \"\" \"expecatation\" should be corrected to \"expectation.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper presents dropout through the lens of a latent variable model, where the dropout variable (indicating whether units are dropped, with values of 0 or 1) is unobserved and subsequently marginalised. While maximum likelihood estimation under this model is intractable, standard dropout is shown to correspond to a straightforward Monte Carlo approximation of maximum likelihood for this latent variable model.\nThe authors then propose a theoretical framework to analyse the discrepancy\"\"termed the inference gap\"\"between the model used during training (a model ensemble, or in this case, the latent variable model) and the model used during testing (where an expectation over activations across multiple models is approximated by the activation of a single model with averaged weights). This framework introduces several key concepts, such as expectation linearity, which facilitate the examination of which transition functions (or, more broadly, layers) are capable of yielding a small inference gap. Notably, Theorem 3 provides a bound on this inference gap.\nAdditionally, the paper introduces a novel regularisation term designed to minimise the inference gap during the training process.\nExperiments conducted on MNIST, CIFAR-10, and CIFAR-100 demonstrate that the proposed method has the potential to outperform standard dropout and achieve performance comparable to Monte Carlo Dropout. The latter is the standard approach for computing dropout outputs in a manner consistent with the training assumption of an ensemble, albeit at a significantly higher computational cost.\nOverall, the study offers a compelling theoretical model for dropout as a latent variable model, framing standard dropout as a Monte Carlo approximation. This perspective is likely to have broad applicability in future research on dropout.\nThe framework for analysing the inference gap is intriguing, though it may have somewhat narrower applicability.\nThe proposed model is persuasive, but it is worth noting that: 1) the experiments are limited to relatively simple datasets, 2) the performance improvements are modest, and 3) the training process incurs additional computational overhead due to the introduction of a new hyperparameter.\nTypo: Page 6, line 8 \"\" \"expecatation\" should be corrected to \"expectation.\""
        }
    ]
}