{
    "version": "2025-01-09-base",
    "scanId": "a9c4fd21-8ce8-4673-9e79-0e0d52e6bb14",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9960451722145081,
                    "sentence": "This paper introduces a compare-aggregate model tailored for NLP tasks that involve semantic comparison of text sequences, such as question answering and textual entailment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9958389401435852,
                    "sentence": "The core structure of the model involves applying a convolutional neural network (aggregation) following an element-wise operation (comparison) performed on the attentive outputs of the LSTMs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9967091083526611,
                    "sentence": "A key contribution of this work lies in the comparison, where various methods for matching text sequences are evaluated, with element-wise subtraction and multiplication shown to consistently deliver superior performance across four distinct datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9919530153274536,
                    "sentence": "However, the main limitation is that the work is somewhat incremental and lacks significant innovation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9928665161132812,
                    "sentence": "Including a qualitative analysis of how subtraction, multiplication, and other comparison functions behave with different types of sentences would have added more depth and interest.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper introduces a compare-aggregate model tailored for NLP tasks that involve semantic comparison of text sequences, such as question answering and textual entailment. \nThe core structure of the model involves applying a convolutional neural network (aggregation) following an element-wise operation (comparison) performed on the attentive outputs of the LSTMs. \nA key contribution of this work lies in the comparison, where various methods for matching text sequences are evaluated, with element-wise subtraction and multiplication shown to consistently deliver superior performance across four distinct datasets. \nHowever, the main limitation is that the work is somewhat incremental and lacks significant innovation. Including a qualitative analysis of how subtraction, multiplication, and other comparison functions behave with different types of sentences would have added more depth and interest."
        }
    ]
}