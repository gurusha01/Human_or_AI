{
    "version": "2025-01-09-base",
    "scanId": "abfee1bf-3ec4-4380-8ba0-63c3763fdfb6",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999966025352478,
                    "sentence": "EDIT: The authors have made significant and thorough revisions to the paper, addressing many of my earlier concerns.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999370574951172,
                    "sentence": "The updated version is also much clearer and easier to follow.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999518394470215,
                    "sentence": "I now recommend this revised version for acceptance and have increased my score accordingly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999917149543762,
                    "sentence": "This paper introduces a novel method for interpreting LSTM models, which are often criticized for their lack of interpretability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999988853931427,
                    "sentence": "Specifically, the authors propose a technique to decompose the LSTM's predictions for a QA task into word-level importance scores.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999825954437256,
                    "sentence": "These scores are then used to generate patterns that can identify answers through a straightforward matching algorithm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999790787696838,
                    "sentence": "On the WikiMovies dataset, the proposed pattern-matching approach achieves accuracy levels comparable to those of a standard LSTM, demonstrating the effectiveness of the method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999756217002869,
                    "sentence": "I find the motivation behind this work compelling, as the interpretability of LSTMs remains an open challenge.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999656081199646,
                    "sentence": "Additionally, the strong performance of the pattern-matching approach was unexpected and impressive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999424815177917,
                    "sentence": "However, certain aspects of the pattern extraction process remain insufficiently detailed, and the evaluation is limited to a highly specific task where predictions are made at the word level.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998775124549866,
                    "sentence": "Therefore, while I recommend the paper as a weak accept in its current form, I encourage the authors to provide further clarification on their approach, as I believe it has significant potential to benefit the NLP research community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994707703590393,
                    "sentence": "Comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988471269607544,
                    "sentence": "- Please provide a more detailed introduction to the specific QA tasks being addressed before section 3.3, as it is unclear at that stage that the answers are entities within the document.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993563890457153,
                    "sentence": "- Section 3.3: Is the softmax output predicting a binary 0/1 value (e.g., whether a word is the answer or not)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993395209312439,
                    "sentence": "- Section 3.3: Could you clarify what the P and Q vectors represent?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987438917160034,
                    "sentence": "Are you referring to a transformation of the hidden state into a 2-dimensional vector for binary classification?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991710782051086,
                    "sentence": "- How does the performance of the pattern-matching method vary with different cutoff constant values?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990625381469727,
                    "sentence": "- Section 5.2: Are there any questions in the dataset where the answers are not entities?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992053508758545,
                    "sentence": "- How might the proposed approach be adapted for tasks where predictions are not made at the word level?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985868334770203,
                    "sentence": "For example, could it be extended to sentence-level tasks such as sentiment classification?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "EDIT: The authors have made significant and thorough revisions to the paper, addressing many of my earlier concerns. The updated version is also much clearer and easier to follow. I now recommend this revised version for acceptance and have increased my score accordingly.\nThis paper introduces a novel method for interpreting LSTM models, which are often criticized for their lack of interpretability. Specifically, the authors propose a technique to decompose the LSTM's predictions for a QA task into word-level importance scores. These scores are then used to generate patterns that can identify answers through a straightforward matching algorithm. On the WikiMovies dataset, the proposed pattern-matching approach achieves accuracy levels comparable to those of a standard LSTM, demonstrating the effectiveness of the method.\nI find the motivation behind this work compelling, as the interpretability of LSTMs remains an open challenge. Additionally, the strong performance of the pattern-matching approach was unexpected and impressive. However, certain aspects of the pattern extraction process remain insufficiently detailed, and the evaluation is limited to a highly specific task where predictions are made at the word level. Therefore, while I recommend the paper as a weak accept in its current form, I encourage the authors to provide further clarification on their approach, as I believe it has significant potential to benefit the NLP research community.\nComments:\n- Please provide a more detailed introduction to the specific QA tasks being addressed before section 3.3, as it is unclear at that stage that the answers are entities within the document.\n- Section 3.3: Is the softmax output predicting a binary 0/1 value (e.g., whether a word is the answer or not)?\n- Section 3.3: Could you clarify what the P and Q vectors represent? Are you referring to a transformation of the hidden state into a 2-dimensional vector for binary classification?\n- How does the performance of the pattern-matching method vary with different cutoff constant values?\n- Section 5.2: Are there any questions in the dataset where the answers are not entities?\n- How might the proposed approach be adapted for tasks where predictions are not made at the word level? For example, could it be extended to sentence-level tasks such as sentiment classification?"
        }
    ]
}