{
    "version": "2025-01-09-base",
    "scanId": "a827a14e-2130-4eec-a36f-a1ea33c403ac",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999608397483826,
                    "sentence": "The concept presented in this paper is sound\"\"progressively transitioning from original weights to compressed weights by compressing a subset and fine-tuning the remainder.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999716281890869,
                    "sentence": "The methodology appears solid, the results are promising, and the authors have satisfactorily addressed my questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999857544898987,
                    "sentence": "Suggestions for improving the paper:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999726414680481,
                    "sentence": "1) It would be beneficial to incorporate some of the responses into the manuscript, particularly the results combining pruning with this method, as they allow for a fair comparison to Han et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999316930770874,
                    "sentence": "and demonstrate superior performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999746680259705,
                    "sentence": "2) The encoding method (my question 4) should be explained more clearly, as its current description in the paper is somewhat ambiguous (e.g., it led to an error in my computation of n2 in question 5).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999659061431885,
                    "sentence": "Specifically, the \"5 bits\" terminology is misleading since the method actually employs variable-length encoding (averaging close to 5 bits), where:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999651908874512,
                    "sentence": "- 0 is encoded using 1 bit, e.g., 0",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999972939491272,
                    "sentence": "- Non-zero values are encoded using 5 bits, with the first bit distinguishing them from 0 and the remaining 4 bits representing the 16 possible values for powers of 2.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The concept presented in this paper is sound\"\"progressively transitioning from original weights to compressed weights by compressing a subset and fine-tuning the remainder. The methodology appears solid, the results are promising, and the authors have satisfactorily addressed my questions.\nSuggestions for improving the paper:\n1) It would be beneficial to incorporate some of the responses into the manuscript, particularly the results combining pruning with this method, as they allow for a fair comparison to Han et al. and demonstrate superior performance.\n2) The encoding method (my question 4) should be explained more clearly, as its current description in the paper is somewhat ambiguous (e.g., it led to an error in my computation of n2 in question 5). Specifically, the \"5 bits\" terminology is misleading since the method actually employs variable-length encoding (averaging close to 5 bits), where:\n- 0 is encoded using 1 bit, e.g., 0\n- Non-zero values are encoded using 5 bits, with the first bit distinguishing them from 0 and the remaining 4 bits representing the 16 possible values for powers of 2."
        }
    ]
}