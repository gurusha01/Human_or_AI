{
    "version": "2025-01-09-base",
    "scanId": "e4ee73a3-24b0-4597-9a73-ead86fc9fd09",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.21937771141529083,
                    "sentence": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.14605413377285004,
                    "sentence": "The idea is clearly motivated and well described.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11579316854476929,
                    "sentence": "In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18630655109882355,
                    "sentence": "Comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11070630699396133,
                    "sentence": "- It's not clear to me why this should be called a \"statistician\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12444575875997543,
                    "sentence": "Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1261986643075943,
                    "sentence": "One could consider a maximum likelihood approach, etc.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09761609137058258,
                    "sentence": "In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10809074342250824,
                    "sentence": "- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09820733219385147,
                    "sentence": "I still think that language needs to be clarified, specifically at the end of page 6.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21785740554332733,
                    "sentence": "My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13195626437664032,
                    "sentence": "I would like clarification on the following:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.19216229021549225,
                    "sentence": "(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.16298411786556244,
                    "sentence": "(b) I agree that the samples are of high-quality, but that is not a quantified statement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1402025818824768,
                    "sentence": "The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.31063786149024963,
                    "sentence": "To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x \" c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.32712453603744507,
                    "sentence": "I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.31258562207221985,
                    "sentence": "I still don't see a reason not to include that.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                }
            ],
            "completely_generated_prob": 0.24256741654484168,
            "class_probabilities": {
                "human": 0.7573578272058379,
                "ai": 0.24256741654484168,
                "mixed": 7.475624932039773e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.7573578272058379,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.24256741654484168,
                    "human": 0.7573578272058379,
                    "mixed": 7.475624932039773e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \nComments:\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x \" c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that."
        }
    ]
}