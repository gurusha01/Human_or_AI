{
    "version": "2025-01-09-base",
    "scanId": "9cd68a61-26b8-41be-9fad-3c01cabd0e6f",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.033316850662231445,
                    "sentence": "This paper studies the off-policy learning of actor-critic with experience replay.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022903500124812126,
                    "sentence": "This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning algorithms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04252799600362778,
                    "sentence": "The paper attacks the problem by introducing a new way to truncate importance weight, a modified trust region optimization, and by combining retrace method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03827710449695587,
                    "sentence": "The combination of the above techniques performs well on Atari and MuJoCo in terms of improving sample efficiency.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04610079526901245,
                    "sentence": "My main comment is how does each of the technique contribute to the performance gain?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08545024693012238,
                    "sentence": "If some experiments could be carried out to evaluate the separate gains from these tricks, it would be helpful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 6,
                    "completely_generated_prob": 1.474742012248794e-05
                }
            ],
            "completely_generated_prob": 0.039839419682113825,
            "class_probabilities": {
                "human": 0.9601605803178862,
                "ai": 0.039839419682113825,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9601605803178862,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.039839419682113825,
                    "human": 0.9601605803178862,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper studies the off-policy learning of actor-critic with experience replay. This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning algorithms. The paper attacks the problem by introducing a new way to truncate importance weight, a modified trust region optimization, and by combining retrace method. The combination of the above techniques performs well on Atari and MuJoCo in terms of improving sample efficiency. My main comment is how does each of the technique contribute to the performance gain? If some experiments could be carried out to evaluate the separate gains from these tricks, it would be helpful."
        }
    ]
}