{
    "version": "2025-01-09-base",
    "scanId": "d0c5bae2-5189-4a25-a4f3-237aa51ef7f2",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.10300806164741516,
                    "sentence": "The contribution of this paper can be summarized as:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0894341766834259,
                    "sentence": "1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11713475733995438,
                    "sentence": "The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13128162920475006,
                    "sentence": "2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11498290300369263,
                    "sentence": "3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08881271630525589,
                    "sentence": "Overall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06936029344797134,
                    "sentence": "However, some details and the main experimental results are not convincing enough to me.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07808055728673935,
                    "sentence": "The paper writing also needs to be improved.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12157713621854782,
                    "sentence": "More comments below:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06189041957259178,
                    "sentence": "[Major comments]",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07296913117170334,
                    "sentence": "- My main concern is that that evaluation results are NOT strong.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08900921791791916,
                    "sentence": "Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06709893047809601,
                    "sentence": "Experimenting with such a tiny WordCup2014 dataset is not convincing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08023560047149658,
                    "sentence": "Moreover, the questions are just generated by a few templates, which is far from NL questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07480098307132721,
                    "sentence": "I am not even not sure why we need to apply an LSTM in such scenario.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12515713274478912,
                    "sentence": "The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1446322351694107,
                    "sentence": "- Conjunctive queries: the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15240029990673065,
                    "sentence": "This assumption might be not always correct, so it is more necessary to justify this on real QA datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15931476652622223,
                    "sentence": "- The model is named as \"Gaussian attention\" and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08809877932071686,
                    "sentence": "[Minor comments]",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1514325886964798,
                    "sentence": "- I find Figure 2 a bit confusing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.22321026027202606,
                    "sentence": "The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.32535237073898315,
                    "sentence": "Maybe make it clearer?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.253356397151947,
                    "sentence": "- Besides \"entity recognition\", usually we still need an \"entity linker\" component which links the text mention to the KB entity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 6,
                    "completely_generated_prob": 1.474742012248794e-05
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.1519714707116977,
            "class_probabilities": {
                "human": 0.847975265147624,
                "ai": 0.1519714707116977,
                "mixed": 5.326414067820469e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.847975265147624,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.1519714707116977,
                    "human": 0.847975265147624,
                    "mixed": 5.326414067820469e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The contribution of this paper can be summarized as:\n1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution. The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015).\n2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering.\n3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries.\nOverall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering. However, some details and the main experimental results are not convincing enough to me. The paper writing also needs to be improved. More comments below:\n[Major comments]\n- My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing. Moreover, the questions are just generated by a few templates, which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. \n- Conjunctive queries: the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct, so it is more necessary to justify this on real QA datasets.\n- The model is named as \"Gaussian attention\" and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature.\n[Minor comments]\n- I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question. Maybe make it clearer?\n- Besides \"entity recognition\", usually we still need an \"entity linker\" component which links the text mention to the KB entity."
        }
    ]
}