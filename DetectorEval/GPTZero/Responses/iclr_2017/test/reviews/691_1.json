{
    "version": "2025-01-09-base",
    "scanId": "d9244256-ff0d-4642-8ce6-e952d9990e3f",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.2331629991531372,
                    "sentence": "The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.10454883426427841,
                    "sentence": "The authors focus on Super Nintendo but claim that the interface supports many others (including ALE).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.14721153676509857,
                    "sentence": "Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new \"rivalry metric\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.3053527772426605,
                    "sentence": "These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.18323089182376862,
                    "sentence": "have a long history of improving the quality of AI and machine learning research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.30785444378852844,
                    "sentence": "One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.32696208357810974,
                    "sentence": "In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.4879952669143677,
                    "sentence": "That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.45595288276672363,
                    "sentence": "The results of experiments themselves are for existing algorithms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5225708484649658,
                    "sentence": "There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5708209872245789,
                    "sentence": "And, yes, domain knowledge helps, but this is obvious.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5250027179718018,
                    "sentence": "The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.45147520303726196,
                    "sentence": "Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.3859825134277344,
                    "sentence": "In the end, I'm not very excited about this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5166917443275452,
                    "sentence": "I was hoping for a more significant scientific contribution to accompany in this new environment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7269019484519958,
                    "sentence": "It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.701879209777099,
            "class_probabilities": {
                "human": 0.29796464407233625,
                "ai": 0.701879209777099,
                "mixed": 0.00015614615056494093
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.701879209777099,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.701879209777099,
                    "human": 0.29796464407233625,
                    "mixed": 0.00015614615056494093
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new \"rivalry metric\".\nThese environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.\nThat said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! \nAlso the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.\nI was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers:"
        }
    ]
}