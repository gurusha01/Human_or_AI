{
    "version": "2025-01-09-base",
    "scanId": "c923f5c5-d0df-445d-883a-de2f5709f830",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999653697013855,
                    "sentence": "Step 1: Summarize what the paper claims to do/contribute",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999500513076782,
                    "sentence": "The paper proposes a novel method called Higher Order Recurrent Neural Networks (HORNNs) for modeling long-term dependency in sequential data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999141693115234,
                    "sentence": "HORNNs extend the standard RNN structure by using multiple memory units to keep track of more preceding RNN states, which are fed to the hidden layer through different weighted paths.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999140501022339,
                    "sentence": "The authors claim that HORNNs can capture long-term dependency more effectively than regular RNNs and LSTMs, and achieve state-of-the-art performance on language modeling tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995567202568054,
                    "sentence": "Step 2: Clearly state the decision (accept or reject) with one or two key reasons for this choice",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999751627445221,
                    "sentence": "I decide to accept this paper with the condition that the authors address some of the concerns raised in the review.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999204277992249,
                    "sentence": "The two key reasons for this decision are: (1) the paper presents a novel and interesting approach to modeling long-term dependency in sequential data, and (2) the experimental results show promising performance on language modeling tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997274279594421,
                    "sentence": "Step 3: Provide supporting arguments for the reasons for the decision",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998819828033447,
                    "sentence": "The paper provides a clear and well-motivated introduction to the problem of modeling long-term dependency in sequential data, and the proposed HORNNs approach is well-placed in the literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998127818107605,
                    "sentence": "The authors also provide a thorough analysis of the related work and a clear explanation of the proposed method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999532699584961,
                    "sentence": "The experimental results are also impressive, showing that HORNNs outperform regular RNNs and LSTMs on language modeling tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994721412658691,
                    "sentence": "Step 4: Provide additional feedback with the aim to improve the paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998018145561218,
                    "sentence": "To improve the paper, I suggest that the authors provide more details on the implementation of HORNNs, such as the patch size, and clarify the usage of probability notation in Section 4.1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997432827949524,
                    "sentence": "Additionally, the authors should provide more analysis on what spatial contrasting learns and the effects of parameter choices.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998206496238708,
                    "sentence": "It would also be beneficial to include more experiments on larger datasets, such as Imagenet, to further demonstrate the effectiveness of HORNNs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991191625595093,
                    "sentence": "Step 5: Ask questions to clarify understanding of the paper and provide additional evidence",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984070658683777,
                    "sentence": "I would like to ask the authors to clarify the following: (1) How do the authors sample from the batch statistics in Section 4.2, and (2) Have the authors experimented with other values for the fixed number of patches sampled in Algorithm 1?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987162947654724,
                    "sentence": "Additionally, I would like to see more analysis on the learned representations of HORNNs and how they compare to those of regular RNNs and LSTMs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Step 1: Summarize what the paper claims to do/contribute\nThe paper proposes a novel method called Higher Order Recurrent Neural Networks (HORNNs) for modeling long-term dependency in sequential data. HORNNs extend the standard RNN structure by using multiple memory units to keep track of more preceding RNN states, which are fed to the hidden layer through different weighted paths. The authors claim that HORNNs can capture long-term dependency more effectively than regular RNNs and LSTMs, and achieve state-of-the-art performance on language modeling tasks.\nStep 2: Clearly state the decision (accept or reject) with one or two key reasons for this choice\nI decide to accept this paper with the condition that the authors address some of the concerns raised in the review. The two key reasons for this decision are: (1) the paper presents a novel and interesting approach to modeling long-term dependency in sequential data, and (2) the experimental results show promising performance on language modeling tasks.\nStep 3: Provide supporting arguments for the reasons for the decision\nThe paper provides a clear and well-motivated introduction to the problem of modeling long-term dependency in sequential data, and the proposed HORNNs approach is well-placed in the literature. The authors also provide a thorough analysis of the related work and a clear explanation of the proposed method. The experimental results are also impressive, showing that HORNNs outperform regular RNNs and LSTMs on language modeling tasks.\nStep 4: Provide additional feedback with the aim to improve the paper\nTo improve the paper, I suggest that the authors provide more details on the implementation of HORNNs, such as the patch size, and clarify the usage of probability notation in Section 4.1. Additionally, the authors should provide more analysis on what spatial contrasting learns and the effects of parameter choices. It would also be beneficial to include more experiments on larger datasets, such as Imagenet, to further demonstrate the effectiveness of HORNNs.\nStep 5: Ask questions to clarify understanding of the paper and provide additional evidence\nI would like to ask the authors to clarify the following: (1) How do the authors sample from the batch statistics in Section 4.2, and (2) Have the authors experimented with other values for the fixed number of patches sampled in Algorithm 1? Additionally, I would like to see more analysis on the learned representations of HORNNs and how they compare to those of regular RNNs and LSTMs."
        }
    ]
}