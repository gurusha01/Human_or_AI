{
    "version": "2025-01-09-base",
    "scanId": "558b4de9-3d5b-4362-af77-d0c76e3e5d26",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999843239784241,
                    "sentence": "This paper proposes a novel end-to-end speech recognition system that combines a convolutional neural network (ConvNet) based acoustic model with a graph decoding framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999716281890869,
                    "sentence": "The system is trained to output letters directly, eliminating the need for phonetic transcription and force alignment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999485015869141,
                    "sentence": "The authors introduce an automatic segmentation criterion, referred to as AutoSegCriterion (ASG), which is simpler and faster than the popular Connectionist Temporal Classification (CTC) criterion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999555945396423,
                    "sentence": "I decide to accept this paper, with the primary reason being the innovative approach to speech recognition that breaks free from traditional HMM/GMM pre-training and force-alignment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999639391899109,
                    "sentence": "The use of a ConvNet based acoustic model and the introduction of the ASG criterion demonstrate a promising research direction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999496340751648,
                    "sentence": "The results on the Librispeech corpus, with competitive word error rates (WERs) of 7.2%, 9.4%, and 10.1% for MFCC, power spectrum, and raw waveform features, respectively, further support the acceptance of this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999532103538513,
                    "sentence": "The approach is well-motivated, and the authors provide a clear explanation of the architecture and the ASG criterion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999674558639526,
                    "sentence": "The experimental results demonstrate the effectiveness of the proposed system, and the comparison with other state-of-the-art systems, such as Deep Speech 1 & 2, shows that the proposed system is competitive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999465942382812,
                    "sentence": "However, one major drawback of the paper is the lack of context and prior work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999786615371704,
                    "sentence": "The authors do not provide a comprehensive review of existing speech recognition systems and techniques, and several relevant papers are not cited.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999616146087646,
                    "sentence": "This omission makes it difficult to fully appreciate the novelty and significance of the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999969482421875,
                    "sentence": "To improve the paper, I suggest that the authors provide a more detailed review of prior work in speech recognition, including the limitations and challenges of existing systems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999746084213257,
                    "sentence": "Additionally, the authors could provide more analysis and discussion of the results, including the strengths and weaknesses of the proposed system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999788999557495,
                    "sentence": "Some questions that I would like the authors to answer include: How does the ASG criterion compare to other sequence criteria, such as CTC, in terms of accuracy and computational efficiency?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999801516532898,
                    "sentence": "How does the proposed system perform on other speech recognition datasets and tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999580979347229,
                    "sentence": "What are the potential applications and extensions of the proposed system?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a novel end-to-end speech recognition system that combines a convolutional neural network (ConvNet) based acoustic model with a graph decoding framework. The system is trained to output letters directly, eliminating the need for phonetic transcription and force alignment. The authors introduce an automatic segmentation criterion, referred to as AutoSegCriterion (ASG), which is simpler and faster than the popular Connectionist Temporal Classification (CTC) criterion.\nI decide to accept this paper, with the primary reason being the innovative approach to speech recognition that breaks free from traditional HMM/GMM pre-training and force-alignment. The use of a ConvNet based acoustic model and the introduction of the ASG criterion demonstrate a promising research direction. The results on the Librispeech corpus, with competitive word error rates (WERs) of 7.2%, 9.4%, and 10.1% for MFCC, power spectrum, and raw waveform features, respectively, further support the acceptance of this paper.\nThe approach is well-motivated, and the authors provide a clear explanation of the architecture and the ASG criterion. The experimental results demonstrate the effectiveness of the proposed system, and the comparison with other state-of-the-art systems, such as Deep Speech 1 & 2, shows that the proposed system is competitive.\nHowever, one major drawback of the paper is the lack of context and prior work. The authors do not provide a comprehensive review of existing speech recognition systems and techniques, and several relevant papers are not cited. This omission makes it difficult to fully appreciate the novelty and significance of the proposed approach.\nTo improve the paper, I suggest that the authors provide a more detailed review of prior work in speech recognition, including the limitations and challenges of existing systems. Additionally, the authors could provide more analysis and discussion of the results, including the strengths and weaknesses of the proposed system. Some questions that I would like the authors to answer include: How does the ASG criterion compare to other sequence criteria, such as CTC, in terms of accuracy and computational efficiency? How does the proposed system perform on other speech recognition datasets and tasks? What are the potential applications and extensions of the proposed system?"
        }
    ]
}