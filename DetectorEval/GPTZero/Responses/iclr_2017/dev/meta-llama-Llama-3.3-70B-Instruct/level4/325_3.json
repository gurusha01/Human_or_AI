{
    "version": "2025-01-09-base",
    "scanId": "6311d7ab-4f48-41a4-97c2-93f92e297f4f",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999396800994873,
                    "sentence": "Review- Summary:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999234676361084,
                    "sentence": "This paper presents a heuristic method for training a deep directed generative model, where each layer samples from the same conditional distribution, analogous to the transition operator of a Markov chain.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999262094497681,
                    "sentence": "The approach approximates the gradient by replacing the posterior over latents with an alternative distribution, similar to optimizing a variational lower bound.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998881816864014,
                    "sentence": "However, the approximating distribution is heuristically constructed at each step, rather than being updated to improve the lower bound.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999053478240967,
                    "sentence": "Additionally, the conditional distributions are optimized greedily, rather than following the gradient of the joint log-likelihood, differing from variational optimization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998721480369568,
                    "sentence": "Review:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998865723609924,
                    "sentence": "The proposed method is intriguing and warrants further exploration.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998029470443726,
                    "sentence": "Nevertheless, considering the existence of alternative approaches for training the same class of models that are 1) theoretically more sound, 2) of similar computational complexity, and 3) effective in practice (e.g., Rezende & Mohamed, 2015), I remain uncertain about its potential impact.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997482299804688,
                    "sentence": "My primary concern, however, lies in the limited scope of the empirical evaluation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997735023498535,
                    "sentence": "I commend the authors for including proper estimates of the log-likelihood, facilitating future comparisons with this method on continuous MNIST.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999738335609436,
                    "sentence": "However, it is essential to note that the numbers cited from Wu et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996702671051025,
                    "sentence": "(2016) do not accurately represent the performance of a VAE, as acknowledged in the paper: \"Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995714426040649,
                    "sentence": "A more nuanced discussion of this point would be beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996195435523987,
                    "sentence": "Comparisons with inpainting results using other methods would have been valuable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995304346084595,
                    "sentence": "How does the proposed approach compare to other methods in terms of practicality?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997379779815674,
                    "sentence": "Similar to the diffusion approach by Sohl-Dickstein et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997303485870361,
                    "sentence": "(2015), the proposed method appears to be both efficient and effective for inpainting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99953293800354,
                    "sentence": "Failing to emphasize this and conduct thorough evaluations seems like a missed opportunity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995323419570923,
                    "sentence": "Minor:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994478225708008,
                    "sentence": "\"\" A citation for \"ordered visible dimension sampling\" is missing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991679191589355,
                    "sentence": "\"\" The text contains typos and frequent incorrect usage of \\citet and \\citep.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review- Summary:\nThis paper presents a heuristic method for training a deep directed generative model, where each layer samples from the same conditional distribution, analogous to the transition operator of a Markov chain. The approach approximates the gradient by replacing the posterior over latents with an alternative distribution, similar to optimizing a variational lower bound. However, the approximating distribution is heuristically constructed at each step, rather than being updated to improve the lower bound. Additionally, the conditional distributions are optimized greedily, rather than following the gradient of the joint log-likelihood, differing from variational optimization.\nReview:\nThe proposed method is intriguing and warrants further exploration. Nevertheless, considering the existence of alternative approaches for training the same class of models that are 1) theoretically more sound, 2) of similar computational complexity, and 3) effective in practice (e.g., Rezende & Mohamed, 2015), I remain uncertain about its potential impact. My primary concern, however, lies in the limited scope of the empirical evaluation.\nI commend the authors for including proper estimates of the log-likelihood, facilitating future comparisons with this method on continuous MNIST. However, it is essential to note that the numbers cited from Wu et al. (2016) do not accurately represent the performance of a VAE, as acknowledged in the paper: \"Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\" A more nuanced discussion of this point would be beneficial.\nComparisons with inpainting results using other methods would have been valuable. How does the proposed approach compare to other methods in terms of practicality? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed method appears to be both efficient and effective for inpainting. Failing to emphasize this and conduct thorough evaluations seems like a missed opportunity.\nMinor:\n\"\" A citation for \"ordered visible dimension sampling\" is missing.\n\"\" The text contains typos and frequent incorrect usage of \\citet and \\citep."
        }
    ]
}