{
    "version": "2025-01-09-base",
    "scanId": "549f5dda-241f-415e-be2e-8d998a963d46",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9941327571868896,
                    "sentence": "This paper presents a compelling application of generative models to tackle the classification problem with missing data, leveraging tensorial mixture models that account for dependent samples, a notable extension of existing mixture models that typically assume independent samples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.977286696434021,
                    "sentence": "The proposed TMM model reduces to conventional latent variable models, demonstrating its potential.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9794918894767761,
                    "sentence": "However, despite the intriguing ideas, the presentation is marred by sloppiness, including missing notations and flaws in technical derivations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8327621221542358,
                    "sentence": "At a high level, my concerns are:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9139988422393799,
                    "sentence": "(1) The joint density over all samples is modeled using a tensorial mixture generative model, but the interpretation of the CP decomposition or HT decomposition on the prior density tensor is unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8771034479141235,
                    "sentence": "The authors' interpretation of TMM as a product of mixture models when samples are independent appears to be flawed, and I will elaborate on this in the detailed technical comments below.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9306808710098267,
                    "sentence": "(2) The use of convolution operators to compute an inner product, while realizable through zero padding, may compromise the invariance structure that is a key advantage of CNNs over feed-forward neural networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9178558588027954,
                    "sentence": "However, the practical impact of this on performance is uncertain.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9253668785095215,
                    "sentence": "(3) The authors could provide more insight into the sample complexity of this method, given the model's complexity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980016946792603,
                    "sentence": "Due to my interest in the paper's ideas and the lack of clarity in the ICLR submission, I referred to the arXiv version for technical details.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9966840744018555,
                    "sentence": "Several technical typos were noted (with reference to equations in the arXiv paper):",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9966961741447449,
                    "sentence": "(1) The generative model in figure (5) is incorrect, as the product of vectors is not well-defined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972319006919861,
                    "sentence": "Instead, a Tucker decomposition should be used, representing P(X) as a sum of multi-linear operations on the tensor P(d1,..., dN), with each mode projected onto P(xi\"di; theta{di}).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979158043861389,
                    "sentence": "(2) The special case for diagonal Gaussian Mixture Models may contain typos, as I was unable to derive the third last equation on page 6.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972705245018005,
                    "sentence": "(3) The claim that TMM reduces to a product of mixture models is inaccurate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982112050056458,
                    "sentence": "The first equation on page 7 only holds when the \"sum of product\" operation equals the \"product of sum\" operation, and similarly, the second equality in equation (6) does not hold in general.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982341527938843,
                    "sentence": "Correcting this typo may improve performance on MNIST.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974283576011658,
                    "sentence": "Overall, I appreciate the ideas presented in this paper and suggest that the authors address the technical typos if the paper is accepted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.8945711917131305,
            "class_probabilities": {
                "human": 0.10523616721313742,
                "ai": 0.8945711917131305,
                "mixed": 0.00019264107373218823
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8945711917131305,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8945711917131305,
                    "human": 0.10523616721313742,
                    "mixed": 0.00019264107373218823
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a compelling application of generative models to tackle the classification problem with missing data, leveraging tensorial mixture models that account for dependent samples, a notable extension of existing mixture models that typically assume independent samples. The proposed TMM model reduces to conventional latent variable models, demonstrating its potential. However, despite the intriguing ideas, the presentation is marred by sloppiness, including missing notations and flaws in technical derivations.\nAt a high level, my concerns are:\n(1) The joint density over all samples is modeled using a tensorial mixture generative model, but the interpretation of the CP decomposition or HT decomposition on the prior density tensor is unclear. The authors' interpretation of TMM as a product of mixture models when samples are independent appears to be flawed, and I will elaborate on this in the detailed technical comments below.\n(2) The use of convolution operators to compute an inner product, while realizable through zero padding, may compromise the invariance structure that is a key advantage of CNNs over feed-forward neural networks. However, the practical impact of this on performance is uncertain.\n(3) The authors could provide more insight into the sample complexity of this method, given the model's complexity.\nDue to my interest in the paper's ideas and the lack of clarity in the ICLR submission, I referred to the arXiv version for technical details. Several technical typos were noted (with reference to equations in the arXiv paper):\n(1) The generative model in figure (5) is incorrect, as the product of vectors is not well-defined. Instead, a Tucker decomposition should be used, representing P(X) as a sum of multi-linear operations on the tensor P(d1, ..., dN), with each mode projected onto P(xi\"di; theta{di}).\n(2) The special case for diagonal Gaussian Mixture Models may contain typos, as I was unable to derive the third last equation on page 6.\n(3) The claim that TMM reduces to a product of mixture models is inaccurate. The first equation on page 7 only holds when the \"sum of product\" operation equals the \"product of sum\" operation, and similarly, the second equality in equation (6) does not hold in general. Correcting this typo may improve performance on MNIST.\nOverall, I appreciate the ideas presented in this paper and suggest that the authors address the technical typos if the paper is accepted."
        }
    ]
}