{
    "version": "2025-01-09-base",
    "scanId": "a9df27cb-0662-4b7c-8acb-e0b128898c7c",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9920228719711304,
                    "sentence": "CONTRIBUTIONS",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.959908127784729,
                    "sentence": "The training of LSTMs often yields intermediate gradients that are nearly zero, a consequence of the tanh and sigmoid nonlinearities' flat shape far from the origin.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9681685566902161,
                    "sentence": "This study demonstrates that rounding these negligible gradients to zero leads to matrices with a sparsity of up to 80% during training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9851937294006348,
                    "sentence": "Furthermore, it is shown that applying this sparsification technique to character-level LSTM language models does not substantially impact the model's final performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9823122620582581,
                    "sentence": "The authors suggest that this inherent sparsity could be leveraged by specialized hardware to enhance the energy efficiency and training speed of recurrent networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.945853590965271,
                    "sentence": "NOVELTY",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8091542720794678,
                    "sentence": "To the best of my knowledge, the concept of thresholding gradients to induce sparsity and improve efficiency in RNN training represents a novel contribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7692729830741882,
                    "sentence": "MISSING CITATIONS",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6990433931350708,
                    "sentence": "Previous research has investigated the use of low-precision arithmetic in recurrent neural network language models, including the work of Hubara et al, \"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.9961636828644501,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9961636828644501,
                "mixed": 0.003836317135549872
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9961636828644501,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9961636828644501,
                    "human": 0,
                    "mixed": 0.003836317135549872
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "CONTRIBUTIONS\nThe training of LSTMs often yields intermediate gradients that are nearly zero, a consequence of the tanh and sigmoid nonlinearities' flat shape far from the origin. This study demonstrates that rounding these negligible gradients to zero leads to matrices with a sparsity of up to 80% during training. Furthermore, it is shown that applying this sparsification technique to character-level LSTM language models does not substantially impact the model's final performance. The authors suggest that this inherent sparsity could be leveraged by specialized hardware to enhance the energy efficiency and training speed of recurrent networks.\nNOVELTY\nTo the best of my knowledge, the concept of thresholding gradients to induce sparsity and improve efficiency in RNN training represents a novel contribution.\nMISSING CITATIONS\nPrevious research has investigated the use of low-precision arithmetic in recurrent neural network language models, including the work of Hubara et al, \"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\"."
        }
    ]
}