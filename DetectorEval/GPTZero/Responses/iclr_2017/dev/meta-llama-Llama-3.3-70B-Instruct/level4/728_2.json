{
    "version": "2025-01-09-base",
    "scanId": "a20b7c50-8901-44df-a197-e42e2c41f62a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9994070529937744,
                    "sentence": "The semi-Markov decision process (SDMP) framework has been extensively utilized to model skill acquisition and temporal abstraction in reinforcement learning, and this paper introduces a modified version of this framework, referred to as the semi-aggregated MDP model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994441866874695,
                    "sentence": "However, the formal definition of the semi-aggregated MDP (SAMDP) is not sufficiently clear or robust to warrant in-depth consideration, relying heavily on heuristic approaches and illustrative examples rather than rigorous definitions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994302988052368,
                    "sentence": "Furthermore, the paper lacks the necessary theoretical underpinnings to support its claims.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991661310195923,
                    "sentence": "The experimental validation of the proposed approach is demonstrated through simplistic 2D grid world scenarios, which, although once a staple in reinforcement learning, have become overly simplistic and outdated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992343187332153,
                    "sentence": "Given the current state of the field, where more complex and realistic domains are commonly employed, it would be more appropriate for this paper to adopt similar, more challenging environments to effectively demonstrate the efficacy of the proposed SAMDP model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The semi-Markov decision process (SDMP) framework has been extensively utilized to model skill acquisition and temporal abstraction in reinforcement learning, and this paper introduces a modified version of this framework, referred to as the semi-aggregated MDP model.\nHowever, the formal definition of the semi-aggregated MDP (SAMDP) is not sufficiently clear or robust to warrant in-depth consideration, relying heavily on heuristic approaches and illustrative examples rather than rigorous definitions. Furthermore, the paper lacks the necessary theoretical underpinnings to support its claims.\nThe experimental validation of the proposed approach is demonstrated through simplistic 2D grid world scenarios, which, although once a staple in reinforcement learning, have become overly simplistic and outdated. Given the current state of the field, where more complex and realistic domains are commonly employed, it would be more appropriate for this paper to adopt similar, more challenging environments to effectively demonstrate the efficacy of the proposed SAMDP model."
        }
    ]
}