{
    "version": "2025-01-09-base",
    "scanId": "1b528efe-74b1-4772-b013-2c1742a54318",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9995850324630737,
                    "sentence": "This manuscript presents a novel exploration strategy designed to encourage the discovery of under-explored reward regions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99875807762146,
                    "sentence": "The proposed approach, which leverages importance sampling, offers a straightforward modification to the REINFORCE algorithm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9956480264663696,
                    "sentence": "Experimental results across various algorithmic toy tasks demonstrate the proposed model's superior performance compared to both REINFORCE and Q-learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9966257214546204,
                    "sentence": "The paper exhibits promising potential in the realm of automated algorithm discovery through reinforcement learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9954493641853333,
                    "sentence": "However, the primary motivation behind the paper remains somewhat ambiguous.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982044100761414,
                    "sentence": "If the central aim is to enhance exploration for policy gradient methods, it would be beneficial for the authors to have benchmarked their algorithm against standard reinforcement learning tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9969201683998108,
                    "sentence": "Given the extensive literature focused on improving REINFORCE, the authors' comparison of their method, termed UREX, to a basic version of REINFORCE on non-standard tasks raises questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997492253780365,
                    "sentence": "Specifically, asserting UREX's superiority without comparing it to more robust baselines or standard tasks may not fully demonstrate its effectiveness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9898291826248169,
                    "sentence": "Alternatively, if the main objective is to improve performance in algorithm learning tasks, the current baselines used for comparison are somewhat weak.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9870685935020447,
                    "sentence": "Clarification on the primary motivation would strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982054829597473,
                    "sentence": "Additionally, the action space considered is relatively small.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998180091381073,
                    "sentence": "Initially, the authors express concern that entropy regularization may not be scalable to larger action spaces.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9978982210159302,
                    "sentence": "Therefore, a comparative analysis of MENT and UREX in a scenario involving a large action space would provide valuable insights into whether UREX remains effective under such conditions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997711777687073,
                    "sentence": "--------------------------",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989632964134216,
                    "sentence": "Following the rebuttal:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991580843925476,
                    "sentence": "Upon reevaluation, I noted the discussion on action sequences that addresses my previous concern regarding the small action space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994595050811768,
                    "sentence": "Regarding the issue of weak baselines, it's worth noting that the literature offers several strategies to mitigate the high-variance problem associated with REINFORCE, such as the approach outlined by Mnih & Gregor in 2014.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987059831619263,
                    "sentence": "As a result, I have adjusted my rating from 6 to 7.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999051034450531,
                    "sentence": "While I continue to encourage the authors to strengthen their baselines, the rebuttal has addressed some of my initial concerns.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This manuscript presents a novel exploration strategy designed to encourage the discovery of under-explored reward regions. The proposed approach, which leverages importance sampling, offers a straightforward modification to the REINFORCE algorithm. Experimental results across various algorithmic toy tasks demonstrate the proposed model's superior performance compared to both REINFORCE and Q-learning.\nThe paper exhibits promising potential in the realm of automated algorithm discovery through reinforcement learning. However, the primary motivation behind the paper remains somewhat ambiguous. If the central aim is to enhance exploration for policy gradient methods, it would be beneficial for the authors to have benchmarked their algorithm against standard reinforcement learning tasks. Given the extensive literature focused on improving REINFORCE, the authors' comparison of their method, termed UREX, to a basic version of REINFORCE on non-standard tasks raises questions. Specifically, asserting UREX's superiority without comparing it to more robust baselines or standard tasks may not fully demonstrate its effectiveness. Alternatively, if the main objective is to improve performance in algorithm learning tasks, the current baselines used for comparison are somewhat weak. Clarification on the primary motivation would strengthen the paper.\nAdditionally, the action space considered is relatively small. Initially, the authors express concern that entropy regularization may not be scalable to larger action spaces. Therefore, a comparative analysis of MENT and UREX in a scenario involving a large action space would provide valuable insights into whether UREX remains effective under such conditions.\n--------------------------\nFollowing the rebuttal:\nUpon reevaluation, I noted the discussion on action sequences that addresses my previous concern regarding the small action space.\nRegarding the issue of weak baselines, it's worth noting that the literature offers several strategies to mitigate the high-variance problem associated with REINFORCE, such as the approach outlined by Mnih & Gregor in 2014.\nAs a result, I have adjusted my rating from 6 to 7. While I continue to encourage the authors to strengthen their baselines, the rebuttal has addressed some of my initial concerns."
        }
    ]
}