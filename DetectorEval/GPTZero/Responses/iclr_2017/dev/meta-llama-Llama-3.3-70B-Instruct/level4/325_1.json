{
    "version": "2025-01-09-base",
    "scanId": "e3be168d-89a8-40eb-bcaa-4902de68626a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9997797012329102,
                    "sentence": "This manuscript proposes a novel generative model that leverages a gradual denoising process to transform noise into model samples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997140169143677,
                    "sentence": "Similar to diffusion-based generative models, this approach employs a denoising process; however, it differs in several key aspects:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995971918106079,
                    "sentence": "- The model utilizes a significantly reduced number of denoising steps, resulting in substantially improved computational efficiency.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999449610710144,
                    "sentence": "- Unlike the diffusion approach, which involves a reverse trajectory, the conditional chain for the approximate posterior directly jumps to q(z(0) \" x) and then proceeds in the same direction as the generative model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994317293167114,
                    "sentence": "This design enables the inference chain to behave as a perturbation around the generative model, effectively pulling it towards the data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994775056838989,
                    "sentence": "This concept bears some resemblance to ladder networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995418787002563,
                    "sentence": "- Notably, this model does not provide a tractable variational bound on the log likelihood.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994546175003052,
                    "sentence": "I found the idea presented in this paper to be intriguing, and the visual sample quality produced by the short chain was impressive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987469911575317,
                    "sentence": "The inpainting results were particularly noteworthy, as one-shot inpainting is not feasible under most existing generative modeling frameworks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983447790145874,
                    "sentence": "However, a more convincing comparison of log likelihoods that does not rely on Parzen likelihoods would strengthen the argument.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9981532096862793,
                    "sentence": "The following detailed comments are provided:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982805252075195,
                    "sentence": "Sec.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974708557128906,
                    "sentence": "2:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8772013187408447,
                    "sentence": "- The phrase \"theta(0) the\" should be revised to \"theta(0) be the\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8946979641914368,
                    "sentence": "- Similarly, \"theta(t) the\" should be changed to \"theta(t) be the\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8769426941871643,
                    "sentence": "- The sentence \"what we will be using\" could be rephrased as \"which we will be doing\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.1654817909002304,
                    "sentence": "I appreciate the approach of inferring q(z^0\"x) and then running inference in the same order as the generative chain, which bears some resemblance to ladder networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9847867488861084,
                    "sentence": "- The sentence \"q.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9891184568405151,
                    "sentence": "Having learned\" would benefit from a paragraph break, resulting in \"q.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.971840500831604,
                    "sentence": "[paragraph break] Having learned\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9883489012718201,
                    "sentence": "Sec 3.3:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9908992648124695,
                    "sentence": "- The phrase \"learn to inverse\" should be revised to \"learn to reverse\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9819732308387756,
                    "sentence": "Sec.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9962171912193298,
                    "sentence": "4:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9889663457870483,
                    "sentence": "- The sentence \"For each experiments\" contains a typo and should be corrected to \"For each experiment\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9912983179092407,
                    "sentence": "- The sensitivity of the results to the infusion rate is not clear and warrants further investigation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9749117493629456,
                    "sentence": "Sec.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9899381399154663,
                    "sentence": "5:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9903069138526917,
                    "sentence": "- The statement \"appears to provide more accurate models\" is not supported by the provided evidence, as there is no direct comparison to the Sohl-Dickstein paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9544086456298828,
                    "sentence": "Fig 4 is notable and presents interesting results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9460888498041611,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9460888498041611,
                "mixed": 0.05391115019583883
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9460888498041611,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9460888498041611,
                    "human": 0,
                    "mixed": 0.05391115019583883
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This manuscript proposes a novel generative model that leverages a gradual denoising process to transform noise into model samples. Similar to diffusion-based generative models, this approach employs a denoising process; however, it differs in several key aspects:\n- The model utilizes a significantly reduced number of denoising steps, resulting in substantially improved computational efficiency.\n- Unlike the diffusion approach, which involves a reverse trajectory, the conditional chain for the approximate posterior directly jumps to q(z(0) \" x) and then proceeds in the same direction as the generative model. This design enables the inference chain to behave as a perturbation around the generative model, effectively pulling it towards the data. This concept bears some resemblance to ladder networks.\n- Notably, this model does not provide a tractable variational bound on the log likelihood.\nI found the idea presented in this paper to be intriguing, and the visual sample quality produced by the short chain was impressive. The inpainting results were particularly noteworthy, as one-shot inpainting is not feasible under most existing generative modeling frameworks. However, a more convincing comparison of log likelihoods that does not rely on Parzen likelihoods would strengthen the argument.\nThe following detailed comments are provided:\nSec. 2:\n- The phrase \"theta(0) the\" should be revised to \"theta(0) be the\".\n- Similarly, \"theta(t) the\" should be changed to \"theta(t) be the\".\n- The sentence \"what we will be using\" could be rephrased as \"which we will be doing\".\nI appreciate the approach of inferring q(z^0\"x) and then running inference in the same order as the generative chain, which bears some resemblance to ladder networks.\n- The sentence \"q. Having learned\" would benefit from a paragraph break, resulting in \"q. [paragraph break] Having learned\".\nSec 3.3:\n- The phrase \"learn to inverse\" should be revised to \"learn to reverse\".\nSec. 4:\n- The sentence \"For each experiments\" contains a typo and should be corrected to \"For each experiment\".\n- The sensitivity of the results to the infusion rate is not clear and warrants further investigation.\nSec. 5:\n- The statement \"appears to provide more accurate models\" is not supported by the provided evidence, as there is no direct comparison to the Sohl-Dickstein paper.\nFig 4 is notable and presents interesting results."
        }
    ]
}