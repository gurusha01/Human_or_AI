{
    "version": "2025-01-09-base",
    "scanId": "d9509add-53ef-4a00-997f-9efc9ae84254",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9965693354606628,
                    "sentence": "This manuscript presents a comprehensive speech recognition system leveraging a linear conditional random field framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9954713582992554,
                    "sentence": "The node potentials are estimated by a convolutional neural network (convnet), while the transition scores are determined by trained scalar values.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9893379211425781,
                    "sentence": "Notably, the convnet acoustic model generates scores for letters rather than phones, thereby minimizing the requirement for specialized knowledge during system training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9790254831314087,
                    "sentence": "At the testing phase, the scores from a word-level language model, convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are integrated to identify the optimal word hypothesis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.807368278503418,
                    "sentence": "The model can be trained from raw audio waveforms, power spectra, or Mel-Frequency Cepstral Coefficients (MFCC) features using conditional maximum likelihood estimation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7668132781982422,
                    "sentence": "Experimental results on the Librispeech corpus demonstrate that the model achieves a word error rate (WER) of 7.2% on the test-clean set using MFCC features, 9.4% using power spectral features, and 10.1% using the raw waveform.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9077162146568298,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6695314049720764,
                    "sentence": "+ The use of a convnet trained from scratch via conditional maximum likelihood to achieve reasonable performance in an English speech recognition system employing graphemic (letter-based) acoustic models is noteworthy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8177476525306702,
                    "sentence": "This approach represents a promising research direction, offering potential for further exploration.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8480488657951355,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8184096813201904,
                    "sentence": "- A significant omission in the paper is the lack of contextual information and relevant prior work that warrants citation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6541787981987,
                    "sentence": "In addition to the previously mentioned papers, the authors should be aware of another notable study, including the 2016 Interspeech paper by Zhang et al., titled \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\", which deserves consideration to enhance the manuscript's completeness and accuracy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.7145451996534505
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                }
            ],
            "completely_generated_prob": 0.8923758534658037,
            "class_probabilities": {
                "human": 0.10626570421472266,
                "ai": 0.8923758534658037,
                "mixed": 0.00135844231947367
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8923758534658037,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8923758534658037,
                    "human": 0.10626570421472266,
                    "mixed": 0.00135844231947367
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This manuscript presents a comprehensive speech recognition system leveraging a linear conditional random field framework. The node potentials are estimated by a convolutional neural network (convnet), while the transition scores are determined by trained scalar values. Notably, the convnet acoustic model generates scores for letters rather than phones, thereby minimizing the requirement for specialized knowledge during system training. At the testing phase, the scores from a word-level language model, convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are integrated to identify the optimal word hypothesis. The model can be trained from raw audio waveforms, power spectra, or Mel-Frequency Cepstral Coefficients (MFCC) features using conditional maximum likelihood estimation. Experimental results on the Librispeech corpus demonstrate that the model achieves a word error rate (WER) of 7.2% on the test-clean set using MFCC features, 9.4% using power spectral features, and 10.1% using the raw waveform.\nStrengths:\n+ The use of a convnet trained from scratch via conditional maximum likelihood to achieve reasonable performance in an English speech recognition system employing graphemic (letter-based) acoustic models is noteworthy. This approach represents a promising research direction, offering potential for further exploration.\nWeaknesses:\n- A significant omission in the paper is the lack of contextual information and relevant prior work that warrants citation. In addition to the previously mentioned papers, the authors should be aware of another notable study, including the 2016 Interspeech paper by Zhang et al., titled \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\", which deserves consideration to enhance the manuscript's completeness and accuracy."
        }
    ]
}