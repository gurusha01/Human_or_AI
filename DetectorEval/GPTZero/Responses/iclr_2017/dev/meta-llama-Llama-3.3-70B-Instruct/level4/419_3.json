{
    "version": "2025-01-09-base",
    "scanId": "6af405d2-8c33-417d-9a33-20276a86e4e3",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.02324739098548889,
                    "sentence": "This study integrates a Latent Dirichlet Allocation (LDA)-type topic model with a Recurrent Neural Network (RNN) by introducing an additive effect on the predictive distribution through the topic parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016098417341709137,
                    "sentence": "The topic distribution for a given text is inferred using a variational auto-encoder, while the RNN is trained as a Recurrent Neural Network Language Model (RNNLM).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03354957699775696,
                    "sentence": "The feature representation is then obtained by concatenating the last hidden state of the RNNLM and the topic parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.046847883611917496,
                    "sentence": "The paper is well-structured and clear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03547615557909012,
                    "sentence": "Although using the topic as an additive effect on the vocabulary facilitates inference, it is intuitive to expect that the topic would also influence the dynamics, such as the state of the RNN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.029988767579197884,
                    "sentence": "The results demonstrate strong performance when using this model as a feature extractor for the IMDB dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022579234093427658,
                    "sentence": "However, it is unclear whether the RNN is fine-tuned on the labeled IMDB data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02797294408082962,
                    "sentence": "In contrast, the results for the Penn Treebank (PTB) dataset are less impressive, with an ensemble of two Long Short-Term Memory (LSTM) networks able to match the topicRNN score, as reported in the original paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.035571880638599396,
                    "sentence": "This approach to jointly modeling topics and a language model appears to be effective and relatively straightforward to implement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8688242435455322,
                    "sentence": "Nevertheless, it is worth noting that the IMDB result is no longer state-of-the-art, as a more recent study (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification) published in May has surpassed this achievement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2999896705150604,
                    "sentence": "Several questions arise from this work: What is the significance of stop-word modeling, and how do the results change if the threshold l_t is set to 0.5 for all t?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3385445177555084,
                    "sentence": "It is surprising that the RNN outperformed the LSTM; was gradient clipping attempted in the topicLSTM case, and do Gated Recurrent Units (GRUs) also fail to work?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.41638433933258057,
                    "sentence": "The requirement for a stop-word list is unfortunate; is the link provided in footnote 4 the one used in the experiments?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15466992557048798,
                    "sentence": "Furthermore, does factoring out the topics in this manner enable the RNN to scale more efficiently with an increased number of neurons?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.19134759902954102,
                    "sentence": "How reasonable and peaked do the topic distributions appear for individual documents?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17130106687545776,
                    "sentence": "Can some examples of the inferred distributions be provided?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.22779172658920288,
                    "sentence": "The topics seem unusual for the IMDB dataset, with the top word of two topics being the same ('campbell').",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21868009865283966,
                    "sentence": "A comparison between these topics and those inferred by LDA on the same datasets would be interesting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.16822758316993713,
                    "sentence": "Minor comments include correcting \"GHz\" to \"GB\" below Figure 2 and defining the symbol \\Gamma.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 6,
                    "completely_generated_prob": 1.474742012248794e-05
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.24226556879047087,
            "class_probabilities": {
                "human": 0.7564153809257333,
                "ai": 0.24226556879047087,
                "mixed": 0.0013190502837956669
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.7564153809257333,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.24226556879047087,
                    "human": 0.7564153809257333,
                    "mixed": 0.0013190502837956669
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This study integrates a Latent Dirichlet Allocation (LDA)-type topic model with a Recurrent Neural Network (RNN) by introducing an additive effect on the predictive distribution through the topic parameters. The topic distribution for a given text is inferred using a variational auto-encoder, while the RNN is trained as a Recurrent Neural Network Language Model (RNNLM). The feature representation is then obtained by concatenating the last hidden state of the RNNLM and the topic parameters.\nThe paper is well-structured and clear. Although using the topic as an additive effect on the vocabulary facilitates inference, it is intuitive to expect that the topic would also influence the dynamics, such as the state of the RNN. The results demonstrate strong performance when using this model as a feature extractor for the IMDB dataset. However, it is unclear whether the RNN is fine-tuned on the labeled IMDB data. In contrast, the results for the Penn Treebank (PTB) dataset are less impressive, with an ensemble of two Long Short-Term Memory (LSTM) networks able to match the topicRNN score, as reported in the original paper. This approach to jointly modeling topics and a language model appears to be effective and relatively straightforward to implement.\nNevertheless, it is worth noting that the IMDB result is no longer state-of-the-art, as a more recent study (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification) published in May has surpassed this achievement.\nSeveral questions arise from this work: What is the significance of stop-word modeling, and how do the results change if the threshold l_t is set to 0.5 for all t? It is surprising that the RNN outperformed the LSTM; was gradient clipping attempted in the topicLSTM case, and do Gated Recurrent Units (GRUs) also fail to work? The requirement for a stop-word list is unfortunate; is the link provided in footnote 4 the one used in the experiments?\nFurthermore, does factoring out the topics in this manner enable the RNN to scale more efficiently with an increased number of neurons? How reasonable and peaked do the topic distributions appear for individual documents? Can some examples of the inferred distributions be provided? The topics seem unusual for the IMDB dataset, with the top word of two topics being the same ('campbell'). A comparison between these topics and those inferred by LDA on the same datasets would be interesting.\nMinor comments include correcting \"GHz\" to \"GB\" below Figure 2 and defining the symbol \\Gamma."
        }
    ]
}