{
    "version": "2025-01-09-base",
    "scanId": "472edee4-d088-492b-b413-5b39bbde6687",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999953031539917,
                    "sentence": "This paper proposes a novel decision-level fusion approach for multi-modal product classification using text and image inputs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999366998672485,
                    "sentence": "The authors train state-of-the-art deep neural networks for each input source and demonstrate the potential of combining them into a multi-modal architecture.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999615550041199,
                    "sentence": "They also propose a novel policy network that learns to choose between the text and image networks, resulting in improved top-1 accuracy on a large-scale product classification dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999628067016602,
                    "sentence": "I decide to accept this paper, with the main reason being that it presents a well-motivated and well-executed approach to multi-modal product classification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999663829803467,
                    "sentence": "The authors provide a clear and thorough analysis of the challenges and opportunities in this area, and their proposed approach shows significant promise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999510645866394,
                    "sentence": "The paper supports its claims through extensive experiments on a real-world dataset, demonstrating the effectiveness of the proposed multi-modal architecture.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999445676803589,
                    "sentence": "The authors also provide a detailed analysis of the errors made by the different networks and show the potential gain of multi-modality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999951183795929,
                    "sentence": "The use of a policy network to learn a decision rule between the text and image networks is a novel and interesting approach, and the results show that it can lead to improved performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995681047439575,
                    "sentence": "One potential limitation of the paper is that the authors only experiment with a single dataset, and it is not clear how well the approach will generalize to other datasets or domains.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989607334136963,
                    "sentence": "Additionally, the authors mention that they were only able to realize a fraction of the potential of multi-modality, and further research is needed to fully explore the possibilities of this approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999220073223114,
                    "sentence": "To improve the paper, I would suggest that the authors provide more details on the dataset used, including the distribution of classes and the quality of the images and text data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994423985481262,
                    "sentence": "Additionally, it would be interesting to see more analysis on the types of products that benefit most from the multi-modal approach, and whether there are any specific challenges or limitations that need to be addressed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995809197425842,
                    "sentence": "Some questions I would like the authors to answer include: How do the results change when using different architectures for the policy network?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996200203895569,
                    "sentence": "Can the approach be extended to other modalities, such as audio or video?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994589686393738,
                    "sentence": "How does the approach perform on datasets with a larger number of classes or more complex class relationships?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993956089019775,
                    "sentence": "Overall, this is a well-written and well-researched paper that presents a significant contribution to the field of multi-modal product classification.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990293979644775,
                    "sentence": "With some additional analysis and experimentation, it has the potential to be a highly impactful and influential work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a novel decision-level fusion approach for multi-modal product classification using text and image inputs. The authors train state-of-the-art deep neural networks for each input source and demonstrate the potential of combining them into a multi-modal architecture. They also propose a novel policy network that learns to choose between the text and image networks, resulting in improved top-1 accuracy on a large-scale product classification dataset.\nI decide to accept this paper, with the main reason being that it presents a well-motivated and well-executed approach to multi-modal product classification. The authors provide a clear and thorough analysis of the challenges and opportunities in this area, and their proposed approach shows significant promise.\nThe paper supports its claims through extensive experiments on a real-world dataset, demonstrating the effectiveness of the proposed multi-modal architecture. The authors also provide a detailed analysis of the errors made by the different networks and show the potential gain of multi-modality. The use of a policy network to learn a decision rule between the text and image networks is a novel and interesting approach, and the results show that it can lead to improved performance.\nOne potential limitation of the paper is that the authors only experiment with a single dataset, and it is not clear how well the approach will generalize to other datasets or domains. Additionally, the authors mention that they were only able to realize a fraction of the potential of multi-modality, and further research is needed to fully explore the possibilities of this approach.\nTo improve the paper, I would suggest that the authors provide more details on the dataset used, including the distribution of classes and the quality of the images and text data. Additionally, it would be interesting to see more analysis on the types of products that benefit most from the multi-modal approach, and whether there are any specific challenges or limitations that need to be addressed.\nSome questions I would like the authors to answer include: How do the results change when using different architectures for the policy network? Can the approach be extended to other modalities, such as audio or video? How does the approach perform on datasets with a larger number of classes or more complex class relationships? \nOverall, this is a well-written and well-researched paper that presents a significant contribution to the field of multi-modal product classification. With some additional analysis and experimentation, it has the potential to be a highly impactful and influential work."
        }
    ]
}