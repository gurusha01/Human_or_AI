{
    "version": "2025-01-09-base",
    "scanId": "eb32b52c-5011-47a8-a215-69c6f1ec7dc5",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999980330467224,
                    "sentence": "Review of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991059303284,
                    "sentence": "Summary of Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998152256012,
                    "sentence": "This paper introduces a novel reinforcement learning (RL) algorithm, Under-Appreciated Reward Exploration (UREX), which builds on the REINFORCE framework to address the challenge of sparse rewards in high-dimensional spaces.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973773956299,
                    "sentence": "UREX encourages exploration by prioritizing action sequences where the policy underestimates the reward, using a combination of mode-seeking and mean-seeking objectives.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997615814209,
                    "sentence": "The approach is simple to implement, requiring minimal modifications to REINFORCE, and demonstrates improved robustness to hyper-parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999989867210388,
                    "sentence": "The authors evaluate UREX on six algorithmic tasks, including multi-digit addition and binary search, and report significant performance improvements over baseline methods like entropy-regularized REINFORCE (MENT) and Q-learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983906745911,
                    "sentence": "Notably, UREX is the first RL method to solve multi-digit addition using only reward feedback, with some policies generalizing to sequences far longer than those seen during training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999985694885254,
                    "sentence": "Decision: Accept",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979138374329,
                    "sentence": "The paper makes a strong case for acceptance due to its originality, well-motivated approach, and promising empirical results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998927116394,
                    "sentence": "The introduction of UREX addresses a critical limitation in RL\"\"effective exploration in sparse reward settings\"\"and the results demonstrate its potential to advance the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "However, broader validation on standard RL benchmarks would strengthen the paper further.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999960660934448,
                    "sentence": "Supporting Arguments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999990463256836,
                    "sentence": "1. Novelty and Motivation: The paper tackles a well-defined problem\"\"exploration in sparse reward environments\"\"and proposes a principled solution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999984502792358,
                    "sentence": "The use of importance sampling to identify under-appreciated actions is innovative and bridges gaps between value-based and policy-based exploration strategies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999984502792358,
                    "sentence": "2. Empirical Results: The experimental evaluation is thorough, with UREX outperforming MENT and Q-learning on challenging algorithmic tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999988675117493,
                    "sentence": "The ability to solve multi-digit addition and generalize to sequences of up to 2000 digits is particularly noteworthy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992251396179,
                    "sentence": "3. Robustness: UREX demonstrates reduced sensitivity to hyper-parameters, a significant practical advantage over existing methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999990463256836,
                    "sentence": "4. Clarity and Simplicity: The algorithm is well-defined and easy to implement, making it accessible to the RL community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991655349731,
                    "sentence": "Suggestions for Improvement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999985098838806,
                    "sentence": "1. Broader Evaluation: While the focus on algorithmic tasks is valuable, testing UREX on standard RL benchmarks (e.g., Atari, MuJoCo) would provide a more comprehensive validation of its generality and applicability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999009966850281,
                    "sentence": "2. Analysis of Generalization: The paper mentions that UREX-trained policies generalize to longer sequences but does not delve into why some models generalize while others do not.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999542832374573,
                    "sentence": "A deeper analysis of this phenomenon could provide insights into the algorithm's strengths and limitations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999172687530518,
                    "sentence": "3. Binary Search Task: The results on the binary search task reveal that the learned policies do not fully adopt the optimal logarithmic complexity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998634457588196,
                    "sentence": "Investigating why UREX fails to learn pure binary search could guide future improvements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999552369117737,
                    "sentence": "4. Comparison with Intrinsic Motivation Methods: The paper could benefit from a comparison with intrinsic motivation or curiosity-driven exploration methods, which also aim to address sparse rewards.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994029402732849,
                    "sentence": "Questions for the Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998594522476196,
                    "sentence": "1. How does UREX perform on standard RL benchmarks with dense rewards, such as Atari or continuous control tasks in MuJoCo?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999698400497437,
                    "sentence": "Does it retain its advantages in these settings?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998098015785217,
                    "sentence": "2. Can the authors provide more details on the variance of importance weights during training?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999911367893219,
                    "sentence": "How does this affect the stability and convergence of UREX?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996321797370911,
                    "sentence": "3. For the binary search task, did the authors experiment with modifying the reward structure to explicitly penalize inefficient search strategies?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998189806938171,
                    "sentence": "If so, how did this affect the results?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998988509178162,
                    "sentence": "4. What factors contribute to the generalization of UREX-trained policies to longer sequences?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997739195823669,
                    "sentence": "Is this behavior consistent across different tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993574023246765,
                    "sentence": "Conclusion",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998810887336731,
                    "sentence": "This paper presents a significant contribution to the RL community by addressing a fundamental challenge in sparse reward environments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999780535697937,
                    "sentence": "While additional validation on broader tasks would strengthen the work, the originality, simplicity, and empirical success of UREX make it a valuable addition to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986683130264282,
                    "sentence": "I recommend acceptance with minor revisions to address the suggested improvements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9926183471516448,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9926183471516448,
                "mixed": 0.007381652848355174
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9926183471516448,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9926183471516448,
                    "human": 0,
                    "mixed": 0.007381652848355174
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Paper\nSummary of Contributions\nThis paper introduces a novel reinforcement learning (RL) algorithm, Under-Appreciated Reward Exploration (UREX), which builds on the REINFORCE framework to address the challenge of sparse rewards in high-dimensional spaces. UREX encourages exploration by prioritizing action sequences where the policy underestimates the reward, using a combination of mode-seeking and mean-seeking objectives. The approach is simple to implement, requiring minimal modifications to REINFORCE, and demonstrates improved robustness to hyper-parameters. The authors evaluate UREX on six algorithmic tasks, including multi-digit addition and binary search, and report significant performance improvements over baseline methods like entropy-regularized REINFORCE (MENT) and Q-learning. Notably, UREX is the first RL method to solve multi-digit addition using only reward feedback, with some policies generalizing to sequences far longer than those seen during training.\nDecision: Accept\nThe paper makes a strong case for acceptance due to its originality, well-motivated approach, and promising empirical results. The introduction of UREX addresses a critical limitation in RL\"\"effective exploration in sparse reward settings\"\"and the results demonstrate its potential to advance the field. However, broader validation on standard RL benchmarks would strengthen the paper further.\nSupporting Arguments\n1. Novelty and Motivation: The paper tackles a well-defined problem\"\"exploration in sparse reward environments\"\"and proposes a principled solution. The use of importance sampling to identify under-appreciated actions is innovative and bridges gaps between value-based and policy-based exploration strategies.\n2. Empirical Results: The experimental evaluation is thorough, with UREX outperforming MENT and Q-learning on challenging algorithmic tasks. The ability to solve multi-digit addition and generalize to sequences of up to 2000 digits is particularly noteworthy.\n3. Robustness: UREX demonstrates reduced sensitivity to hyper-parameters, a significant practical advantage over existing methods.\n4. Clarity and Simplicity: The algorithm is well-defined and easy to implement, making it accessible to the RL community.\nSuggestions for Improvement\n1. Broader Evaluation: While the focus on algorithmic tasks is valuable, testing UREX on standard RL benchmarks (e.g., Atari, MuJoCo) would provide a more comprehensive validation of its generality and applicability.\n2. Analysis of Generalization: The paper mentions that UREX-trained policies generalize to longer sequences but does not delve into why some models generalize while others do not. A deeper analysis of this phenomenon could provide insights into the algorithm's strengths and limitations.\n3. Binary Search Task: The results on the binary search task reveal that the learned policies do not fully adopt the optimal logarithmic complexity. Investigating why UREX fails to learn pure binary search could guide future improvements.\n4. Comparison with Intrinsic Motivation Methods: The paper could benefit from a comparison with intrinsic motivation or curiosity-driven exploration methods, which also aim to address sparse rewards.\nQuestions for the Authors\n1. How does UREX perform on standard RL benchmarks with dense rewards, such as Atari or continuous control tasks in MuJoCo? Does it retain its advantages in these settings?\n2. Can the authors provide more details on the variance of importance weights during training? How does this affect the stability and convergence of UREX?\n3. For the binary search task, did the authors experiment with modifying the reward structure to explicitly penalize inefficient search strategies? If so, how did this affect the results?\n4. What factors contribute to the generalization of UREX-trained policies to longer sequences? Is this behavior consistent across different tasks?\nConclusion\nThis paper presents a significant contribution to the RL community by addressing a fundamental challenge in sparse reward environments. While additional validation on broader tasks would strengthen the work, the originality, simplicity, and empirical success of UREX make it a valuable addition to the field. I recommend acceptance with minor revisions to address the suggested improvements."
        }
    ]
}