{
    "version": "2025-01-09-base",
    "scanId": "1ee8baa1-dd99-4dc8-91cf-32cef62fd513",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999963641166687,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999831318855286,
                    "sentence": "Summary of Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999799728393555,
                    "sentence": "This paper investigates the ability of reinforcement learning (RL) agents to actively gather information about hidden physical properties of objects through interaction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999724626541138,
                    "sentence": "The authors propose two novel environments, \"Which is Heavier\" and \"Towers,\" where agents must infer properties like mass and cohesion by manipulating objects.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999459385871887,
                    "sentence": "The study demonstrates that general-purpose deep policy gradient methods can learn effective information-seeking policies, balancing the cost of exploration against the risk of incorrect answers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999618530273438,
                    "sentence": "The authors also explore how varying the discount factor affects the learned policies, providing insights into the exploration-exploitation trade-off.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998877644538879,
                    "sentence": "The environments and tasks are well-motivated, inspired by developmental psychology, and could benefit the community if open-sourced.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999259114265442,
                    "sentence": "However, the paper lacks formal grounding in existing frameworks and does not clearly define key concepts like \"questions,\" \"answers,\" and \"cost of information,\" which limits its theoretical contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998297095298767,
                    "sentence": "Decision: Reject",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998452663421631,
                    "sentence": "The paper introduces an interesting problem and demonstrates the feasibility of solving it using deep RL.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998663067817688,
                    "sentence": "However, the lack of formalism, incomplete problem definitions, and insufficient connections to existing frameworks weaken its scientific rigor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999369382858276,
                    "sentence": "While the empirical results are promising, the paper does not provide enough clarity or theoretical grounding to justify acceptance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999452233314514,
                    "sentence": "Supporting Arguments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999641180038452,
                    "sentence": "1. Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999628663063049,
                    "sentence": "- The environments and task formulations are novel and provide a valuable testbed for studying active information gathering in RL.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999474287033081,
                    "sentence": "- The empirical results are robust and demonstrate that agents can learn meaningful experimentation strategies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999668002128601,
                    "sentence": "- The paper draws inspiration from developmental psychology, providing an interdisciplinary perspective.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999787211418152,
                    "sentence": "2. Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999981164932251,
                    "sentence": "- The paper does not formally define the Markov Decision Process (MDP) or Partially Observable MDP (POMDP) framework, including state, action spaces, and reward semantics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999754428863525,
                    "sentence": "This omission makes it difficult to evaluate the generalizability of the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999874830245972,
                    "sentence": "- Key concepts like \"questions,\" \"answers,\" \"difficulty,\" and \"cost of information\" are not rigorously defined, leading to ambiguity in the problem formulation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999855160713196,
                    "sentence": "- While the paper mentions connections to bandits, it does not provide a formal analysis or framework to ground these connections.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999802708625793,
                    "sentence": "- The notion of \"phases\" in the task structure could be better implemented using an augmented state space, which would make the transitions between phases more explicit and interpretable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999317526817322,
                    "sentence": "Additional Feedback for Improvement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999747276306152,
                    "sentence": "1. Formal Definitions: Clearly define the MDP/POMDP framework, including the state, action, and reward spaces.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998427033424377,
                    "sentence": "Formalize the terms \"questions,\" \"answers,\" and \"cost of information\" to provide a rigorous foundation for the study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999588131904602,
                    "sentence": "2. Theoretical Contributions: Strengthen the connection to existing frameworks, such as bandits or active learning, by providing formal analyses or proposing new theoretical models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999474883079529,
                    "sentence": "3. Task Design: Consider augmenting the state space to explicitly represent the \"phases\" of interaction, labeling, and reward.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996039867401123,
                    "sentence": "This would improve clarity and facilitate reproducibility.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999491572380066,
                    "sentence": "4. Open-Source Contribution: Open-sourcing the environments and tasks would significantly enhance the paper's impact and benefit the research community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999585747718811,
                    "sentence": "5. Data Efficiency: Address the issue of data efficiency, as this is a critical aspect of learning in real-world scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9284882545471191,
                    "sentence": "Questions for the Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9774088263511658,
                    "sentence": "1. How do you define the state, action, and reward spaces in your MDP/POMDP formulation?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9832643270492554,
                    "sentence": "Are the \"phases\" explicitly modeled in the state space?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9637368321418762,
                    "sentence": "2. Can you provide a formal definition of \"cost of information\" and explain how it is operationalized in your experiments?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9385811686515808,
                    "sentence": "3. How does your work relate to existing frameworks for active learning or bandit problems?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8695747256278992,
                    "sentence": "Could you provide a formal analysis to ground these connections?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.934176504611969,
                    "sentence": "4. Have you considered alternative methods for implementing the \"phases,\" such as using an augmented state space?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9448893070220947,
                    "sentence": "5. Do you plan to open-source the environments and tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7401238083839417,
                    "sentence": "If so, when?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9506608843803406,
                    "sentence": "In summary, while the paper presents an intriguing empirical study, its lack of formalism and incomplete problem definitions limit its contribution to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8883850574493408,
                    "sentence": "Addressing these issues would significantly strengthen the paper and make it more impactful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 38,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 40,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9841954571483108,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9841954571483108,
                "mixed": 0.015804542851689255
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9841954571483108,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9841954571483108,
                    "human": 0,
                    "mixed": 0.015804542851689255
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary of Contributions \nThis paper investigates the ability of reinforcement learning (RL) agents to actively gather information about hidden physical properties of objects through interaction. The authors propose two novel environments, \"Which is Heavier\" and \"Towers,\" where agents must infer properties like mass and cohesion by manipulating objects. The study demonstrates that general-purpose deep policy gradient methods can learn effective information-seeking policies, balancing the cost of exploration against the risk of incorrect answers. The authors also explore how varying the discount factor affects the learned policies, providing insights into the exploration-exploitation trade-off. The environments and tasks are well-motivated, inspired by developmental psychology, and could benefit the community if open-sourced. However, the paper lacks formal grounding in existing frameworks and does not clearly define key concepts like \"questions,\" \"answers,\" and \"cost of information,\" which limits its theoretical contributions.\nDecision: Reject \nThe paper introduces an interesting problem and demonstrates the feasibility of solving it using deep RL. However, the lack of formalism, incomplete problem definitions, and insufficient connections to existing frameworks weaken its scientific rigor. While the empirical results are promising, the paper does not provide enough clarity or theoretical grounding to justify acceptance.\nSupporting Arguments \n1. Strengths: \n - The environments and task formulations are novel and provide a valuable testbed for studying active information gathering in RL. \n - The empirical results are robust and demonstrate that agents can learn meaningful experimentation strategies. \n - The paper draws inspiration from developmental psychology, providing an interdisciplinary perspective. \n2. Weaknesses: \n - The paper does not formally define the Markov Decision Process (MDP) or Partially Observable MDP (POMDP) framework, including state, action spaces, and reward semantics. This omission makes it difficult to evaluate the generalizability of the approach. \n - Key concepts like \"questions,\" \"answers,\" \"difficulty,\" and \"cost of information\" are not rigorously defined, leading to ambiguity in the problem formulation. \n - While the paper mentions connections to bandits, it does not provide a formal analysis or framework to ground these connections. \n - The notion of \"phases\" in the task structure could be better implemented using an augmented state space, which would make the transitions between phases more explicit and interpretable. \nAdditional Feedback for Improvement \n1. Formal Definitions: Clearly define the MDP/POMDP framework, including the state, action, and reward spaces. Formalize the terms \"questions,\" \"answers,\" and \"cost of information\" to provide a rigorous foundation for the study. \n2. Theoretical Contributions: Strengthen the connection to existing frameworks, such as bandits or active learning, by providing formal analyses or proposing new theoretical models. \n3. Task Design: Consider augmenting the state space to explicitly represent the \"phases\" of interaction, labeling, and reward. This would improve clarity and facilitate reproducibility. \n4. Open-Source Contribution: Open-sourcing the environments and tasks would significantly enhance the paper's impact and benefit the research community. \n5. Data Efficiency: Address the issue of data efficiency, as this is a critical aspect of learning in real-world scenarios. \nQuestions for the Authors \n1. How do you define the state, action, and reward spaces in your MDP/POMDP formulation? Are the \"phases\" explicitly modeled in the state space? \n2. Can you provide a formal definition of \"cost of information\" and explain how it is operationalized in your experiments? \n3. How does your work relate to existing frameworks for active learning or bandit problems? Could you provide a formal analysis to ground these connections? \n4. Have you considered alternative methods for implementing the \"phases,\" such as using an augmented state space? \n5. Do you plan to open-source the environments and tasks? If so, when? \nIn summary, while the paper presents an intriguing empirical study, its lack of formalism and incomplete problem definitions limit its contribution to the field. Addressing these issues would significantly strengthen the paper and make it more impactful."
        }
    ]
}