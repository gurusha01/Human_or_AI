{
    "version": "2025-01-09-base",
    "scanId": "416d0e51-32dd-441e-b418-35073870cfd6",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999469518661499,
                    "sentence": "Review",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999875426292419,
                    "sentence": "Summary of Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999686479568481,
                    "sentence": "This paper addresses the underexplored question of how different action parameterizations in reinforcement learning (RL) impact the learning process and policy performance in locomotion tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999959409236908,
                    "sentence": "The authors compare four actuation models\"\"torques, muscle activations, target joint angles, and target joint velocities\"\"on multiple planar articulated figures and gaits.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997987747192383,
                    "sentence": "They demonstrate that higher-level action parameterizations incorporating local feedback can significantly improve learning speed, robustness, and motion quality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998860359191895,
                    "sentence": "The paper introduces a DeepRL framework for motion imitation tasks, evaluates the impact of these parameterizations, and proposes a heuristic optimization approach for muscle-tendon unit (MTU) parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998475909233093,
                    "sentence": "The experiments are well-structured, interpretable, and provide valuable insights into the embodied nature of control in biomechanical systems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999784529209137,
                    "sentence": "Decision: Reject",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997484683990479,
                    "sentence": "While the paper makes a valuable contribution by addressing an overlooked aspect of RL for locomotion, it falls short in terms of experimental scope and generalizability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999644935131073,
                    "sentence": "The use of a single neural network architecture and reward function limits the ability to draw broader conclusions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995502233505249,
                    "sentence": "Additionally, the term \"DeepRL\" is used somewhat arbitrarily, and the work is restricted to 2D planar simulations, which diminishes its applicability to real-world scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997110962867737,
                    "sentence": "Supporting Arguments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998511075973511,
                    "sentence": "1. Limited Generalizability: The experiments rely on a single neural network architecture with fixed hyperparameters and a single reward function.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997918605804443,
                    "sentence": "This narrow scope makes it difficult to assess whether the findings hold across different architectures, state representations (e.g., pixel data), or reward formulations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997666478157043,
                    "sentence": "2. Arbitrary Use of \"DeepRL\": While the paper employs neural networks for policy representation, the use of the term \"DeepRL\" feels unwarranted given the lack of exploration into the unique challenges or advantages of deep learning in this context.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999005794525146,
                    "sentence": "3. 2D Simulations: The restriction to 2D planar articulated figures significantly limits the relevance of the results to more complex, real-world 3D locomotion tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998493194580078,
                    "sentence": "Incorporating realistic physical constraints and extending to 3D would greatly enhance the paper's impact.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999583959579468,
                    "sentence": "Suggestions for Improvement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999158382415771,
                    "sentence": "1. Expand Experimental Scope: To improve generalizability, the authors should test their framework with varied neural network architectures (e.g., different numbers of neurons or layers) and alternative state representations, such as raw pixel data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998878240585327,
                    "sentence": "2. Clarify \"DeepRL\" Usage: The authors should either justify the use of \"DeepRL\" by highlighting specific deep learning challenges addressed in the paper or revise the terminology to better reflect the work's focus.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997086763381958,
                    "sentence": "3. Extend to 3D and Realistic Constraints: Extending the experiments to 3D articulated figures and incorporating realistic physical constraints, such as torque limits and ground interactions, would make the findings more applicable to real-world scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9970792531967163,
                    "sentence": "4. Reward Function Bias: The authors acknowledge that the reward function may favor certain parameterizations (e.g., PD and Vel).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9972295165061951,
                    "sentence": "Exploring alternative reward formulations or conducting sensitivity analyses would strengthen the validity of the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9962928295135498,
                    "sentence": "5. Supplementary Material: While the paper references supplemental materials and videos, including more detailed quantitative comparisons (e.g., sensitivity analyses) in the main text would improve accessibility and rigor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9894337058067322,
                    "sentence": "Questions for the Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9941495060920715,
                    "sentence": "1. How do the results change when using different neural network architectures or state representations, such as pixel data?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9941628575325012,
                    "sentence": "2. Can you provide more justification for the use of the term \"DeepRL\"?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9917230606079102,
                    "sentence": "What specific challenges of deep learning are addressed in this work?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9922248125076294,
                    "sentence": "3. Have you considered extending the framework to 3D articulated figures?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9874629974365234,
                    "sentence": "If so, what are the anticipated challenges or limitations?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9549872279167175,
                    "sentence": "4. How sensitive are the results to the choice of reward function?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9866392016410828,
                    "sentence": "Would alternative formulations lead to different conclusions about the relative performance of the actuation models?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9828620553016663,
                    "sentence": "In conclusion, while the paper addresses an important and overlooked question, its limitations in experimental scope and generalizability prevent it from making a strong contribution to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9919649958610535,
                    "sentence": "Expanding the experiments and addressing the outlined concerns would significantly enhance the paper's impact and relevance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.992329959936083,
            "class_probabilities": {
                "human": 0,
                "ai": 0.992329959936083,
                "mixed": 0.007670040063917
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.992329959936083,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.992329959936083,
                    "human": 0,
                    "mixed": 0.007670040063917
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review\nSummary of Contributions\nThis paper addresses the underexplored question of how different action parameterizations in reinforcement learning (RL) impact the learning process and policy performance in locomotion tasks. The authors compare four actuation models\"\"torques, muscle activations, target joint angles, and target joint velocities\"\"on multiple planar articulated figures and gaits. They demonstrate that higher-level action parameterizations incorporating local feedback can significantly improve learning speed, robustness, and motion quality. The paper introduces a DeepRL framework for motion imitation tasks, evaluates the impact of these parameterizations, and proposes a heuristic optimization approach for muscle-tendon unit (MTU) parameters. The experiments are well-structured, interpretable, and provide valuable insights into the embodied nature of control in biomechanical systems.\nDecision: Reject\nWhile the paper makes a valuable contribution by addressing an overlooked aspect of RL for locomotion, it falls short in terms of experimental scope and generalizability. The use of a single neural network architecture and reward function limits the ability to draw broader conclusions. Additionally, the term \"DeepRL\" is used somewhat arbitrarily, and the work is restricted to 2D planar simulations, which diminishes its applicability to real-world scenarios.\nSupporting Arguments\n1. Limited Generalizability: The experiments rely on a single neural network architecture with fixed hyperparameters and a single reward function. This narrow scope makes it difficult to assess whether the findings hold across different architectures, state representations (e.g., pixel data), or reward formulations.\n2. Arbitrary Use of \"DeepRL\": While the paper employs neural networks for policy representation, the use of the term \"DeepRL\" feels unwarranted given the lack of exploration into the unique challenges or advantages of deep learning in this context.\n3. 2D Simulations: The restriction to 2D planar articulated figures significantly limits the relevance of the results to more complex, real-world 3D locomotion tasks. Incorporating realistic physical constraints and extending to 3D would greatly enhance the paper's impact.\nSuggestions for Improvement\n1. Expand Experimental Scope: To improve generalizability, the authors should test their framework with varied neural network architectures (e.g., different numbers of neurons or layers) and alternative state representations, such as raw pixel data.\n2. Clarify \"DeepRL\" Usage: The authors should either justify the use of \"DeepRL\" by highlighting specific deep learning challenges addressed in the paper or revise the terminology to better reflect the work's focus.\n3. Extend to 3D and Realistic Constraints: Extending the experiments to 3D articulated figures and incorporating realistic physical constraints, such as torque limits and ground interactions, would make the findings more applicable to real-world scenarios.\n4. Reward Function Bias: The authors acknowledge that the reward function may favor certain parameterizations (e.g., PD and Vel). Exploring alternative reward formulations or conducting sensitivity analyses would strengthen the validity of the results.\n5. Supplementary Material: While the paper references supplemental materials and videos, including more detailed quantitative comparisons (e.g., sensitivity analyses) in the main text would improve accessibility and rigor.\nQuestions for the Authors\n1. How do the results change when using different neural network architectures or state representations, such as pixel data?\n2. Can you provide more justification for the use of the term \"DeepRL\"? What specific challenges of deep learning are addressed in this work?\n3. Have you considered extending the framework to 3D articulated figures? If so, what are the anticipated challenges or limitations?\n4. How sensitive are the results to the choice of reward function? Would alternative formulations lead to different conclusions about the relative performance of the actuation models?\nIn conclusion, while the paper addresses an important and overlooked question, its limitations in experimental scope and generalizability prevent it from making a strong contribution to the field. Expanding the experiments and addressing the outlined concerns would significantly enhance the paper's impact and relevance."
        }
    ]
}