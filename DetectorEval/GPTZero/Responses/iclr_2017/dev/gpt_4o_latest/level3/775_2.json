{
    "version": "2025-01-09-base",
    "scanId": "e212cf83-24a2-485c-8fc3-79e55429f66b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999958276748657,
                    "sentence": "Review of the Paper",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "Summary of Contributions",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "This paper investigates the impact of different action parameterizations on the performance of deep reinforcement learning (DeepRL) policies for controlling high-dimensional, planar articulated figures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999951124191284,
                    "sentence": "Specifically, it evaluates four parameterizations: joint torques, muscle activations, target joint angles, and target joint velocities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999924302101135,
                    "sentence": "The authors demonstrate that higher-level parameterizations, which incorporate local feedback, lead to more robust and higher-quality policies, particularly for complex character morphologies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999869465827942,
                    "sentence": "The paper contributes a systematic evaluation of these parameterizations, a novel optimization approach for musculotendon units (MTUs), and insights into how action spaces influence learning speed, robustness, and motion quality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999369978904724,
                    "sentence": "The work is well-written and accessible to researchers familiar with multi-body simulation and control.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999613165855408,
                    "sentence": "Decision: Reject",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999818801879883,
                    "sentence": "While the paper provides valuable insights into the role of action parameterizations in DeepRL, it falls short in several critical areas that limit its impact and relevance for the ICLR community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999801516532898,
                    "sentence": "The primary reasons for rejection are its limited generalizability and insufficient experimental rigor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999651908874512,
                    "sentence": "Supporting Arguments",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999985933303833,
                    "sentence": "1. Limited Generalizability: The findings are specific to planar articulated figures and a narrow set of tasks (gait-cycle imitation).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999890923500061,
                    "sentence": "The paper does not explore 3D scenarios or other domains where the choice of action parameterization might differ.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999862313270569,
                    "sentence": "This restricts the transferability of the results to broader DeepRL applications, which is a key expectation for ICLR contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999915957450867,
                    "sentence": "2. Relevance to ICLR: The work is more aligned with the interests of robotics and computer graphics communities, given its focus on biomechanical modeling and motion control.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999903440475464,
                    "sentence": "While DeepRL is a core component, the paper does not advance the state of the art in reinforcement learning algorithms or theory, which are central to ICLR.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940991401672,
                    "sentence": "3. Experimental Validation: The paper lacks sufficient sensitivity analysis and variance evaluation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999947547912598,
                    "sentence": "For example, the robustness of the results to changes in hyperparameters, network architectures, or reward functions is not thoroughly examined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999937415122986,
                    "sentence": "This undermines confidence in the claims of general superiority of higher-level parameterizations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999938607215881,
                    "sentence": "Suggestions for Improvement",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999929070472717,
                    "sentence": "1. Expand Generalizability: Extend the experiments to 3D articulated figures and diverse tasks to demonstrate the broader applicability of the findings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999897480010986,
                    "sentence": "Address whether the conclusions hold for different reward functions or environments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999902844429016,
                    "sentence": "2. Strengthen Experimental Rigor: Include sensitivity analyses to evaluate the robustness of the results to variations in network architectures, hyperparameters, and training conditions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999838471412659,
                    "sentence": "Report variance across multiple training runs to ensure statistical significance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974106550216675,
                    "sentence": "3. Clarify Contributions to RL: Highlight how the findings contribute to reinforcement learning beyond the specific domain of motion control.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9966387152671814,
                    "sentence": "For example, discuss the implications of action parameterization choices for other RL tasks or architectures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.996264636516571,
                    "sentence": "4. Address Bias in Reward Design: The reward function appears to favor higher-level parameterizations (e.g., PD and Vel) due to its reliance on joint positions and velocities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9879677295684814,
                    "sentence": "Provide a more balanced evaluation by testing alternative reward functions that do not inherently align with specific parameterizations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9860844016075134,
                    "sentence": "Questions for the Authors",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9967940449714661,
                    "sentence": "1. How do the results generalize to 3D articulated figures or other control tasks beyond locomotion?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979267716407776,
                    "sentence": "2. What is the sensitivity of the reported performance improvements to changes in network architecture or hyperparameters?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9964062571525574,
                    "sentence": "3. How does the choice of reward function influence the relative performance of different parameterizations?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9970329403877258,
                    "sentence": "4. Could the proposed MTU optimization approach be extended to other actuator models or tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9925392866134644,
                    "sentence": "In summary, while the paper provides a well-executed study of action parameterizations in DeepRL, its limited scope, insufficient experimental rigor, and misalignment with ICLR's core focus necessitate a rejection.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9917249083518982,
                    "sentence": "However, the work has potential for significant impact in robotics and graphics venues with additional refinements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9923625107281651,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9923625107281651,
                "mixed": 0.007637489271834829
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9923625107281651,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9923625107281651,
                    "human": 0,
                    "mixed": 0.007637489271834829
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review of the Paper\nSummary of Contributions\nThis paper investigates the impact of different action parameterizations on the performance of deep reinforcement learning (DeepRL) policies for controlling high-dimensional, planar articulated figures. Specifically, it evaluates four parameterizations: joint torques, muscle activations, target joint angles, and target joint velocities. The authors demonstrate that higher-level parameterizations, which incorporate local feedback, lead to more robust and higher-quality policies, particularly for complex character morphologies. The paper contributes a systematic evaluation of these parameterizations, a novel optimization approach for musculotendon units (MTUs), and insights into how action spaces influence learning speed, robustness, and motion quality. The work is well-written and accessible to researchers familiar with multi-body simulation and control.\nDecision: Reject \nWhile the paper provides valuable insights into the role of action parameterizations in DeepRL, it falls short in several critical areas that limit its impact and relevance for the ICLR community. The primary reasons for rejection are its limited generalizability and insufficient experimental rigor.\nSupporting Arguments\n1. Limited Generalizability: The findings are specific to planar articulated figures and a narrow set of tasks (gait-cycle imitation). The paper does not explore 3D scenarios or other domains where the choice of action parameterization might differ. This restricts the transferability of the results to broader DeepRL applications, which is a key expectation for ICLR contributions.\n2. Relevance to ICLR: The work is more aligned with the interests of robotics and computer graphics communities, given its focus on biomechanical modeling and motion control. While DeepRL is a core component, the paper does not advance the state of the art in reinforcement learning algorithms or theory, which are central to ICLR.\n3. Experimental Validation: The paper lacks sufficient sensitivity analysis and variance evaluation. For example, the robustness of the results to changes in hyperparameters, network architectures, or reward functions is not thoroughly examined. This undermines confidence in the claims of general superiority of higher-level parameterizations.\nSuggestions for Improvement\n1. Expand Generalizability: Extend the experiments to 3D articulated figures and diverse tasks to demonstrate the broader applicability of the findings. Address whether the conclusions hold for different reward functions or environments.\n2. Strengthen Experimental Rigor: Include sensitivity analyses to evaluate the robustness of the results to variations in network architectures, hyperparameters, and training conditions. Report variance across multiple training runs to ensure statistical significance.\n3. Clarify Contributions to RL: Highlight how the findings contribute to reinforcement learning beyond the specific domain of motion control. For example, discuss the implications of action parameterization choices for other RL tasks or architectures.\n4. Address Bias in Reward Design: The reward function appears to favor higher-level parameterizations (e.g., PD and Vel) due to its reliance on joint positions and velocities. Provide a more balanced evaluation by testing alternative reward functions that do not inherently align with specific parameterizations.\nQuestions for the Authors\n1. How do the results generalize to 3D articulated figures or other control tasks beyond locomotion?\n2. What is the sensitivity of the reported performance improvements to changes in network architecture or hyperparameters?\n3. How does the choice of reward function influence the relative performance of different parameterizations?\n4. Could the proposed MTU optimization approach be extended to other actuator models or tasks?\nIn summary, while the paper provides a well-executed study of action parameterizations in DeepRL, its limited scope, insufficient experimental rigor, and misalignment with ICLR's core focus necessitate a rejection. However, the work has potential for significant impact in robotics and graphics venues with additional refinements."
        }
    ]
}