{
    "version": "2025-01-09-base",
    "scanId": "7117d213-7907-41fd-94a1-2ffaa4c5813b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9914693832397461,
                    "sentence": "This paper presents an end-to-end speech recognition system built on a linear conditional random field framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.992845892906189,
                    "sentence": "A convolutional neural network (convnet) is employed to estimate node potentials, while transition scores are represented by trained scalar values.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9817689657211304,
                    "sentence": "The acoustic model within the convnet generates scores for letters rather than phonemes, thereby reducing the reliance on expert knowledge during system training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9823397397994995,
                    "sentence": "During inference, the system combines scores from a word-level language model, the convnet's node potentials, learned letter-to-letter transition scores, and a word insertion penalty to determine the highest-scoring word hypothesis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9684774279594421,
                    "sentence": "The model can be trained using raw audio waveforms, power spectra, or MFCC features through conditional maximum likelihood estimation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9204518795013428,
                    "sentence": "Experiments conducted on the Librispeech dataset demonstrate that the model achieves a word error rate (WER) of 7.2% on the test-clean set using MFCC features, 9.4% WER with power spectral features, and 10.1% WER with raw waveform inputs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8545113801956177,
                    "sentence": "Pros",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5363099575042725,
                    "sentence": "+ The use of a convnet trained from scratch with conditional maximum likelihood to build a speech recognition system for English based on graphemic (letter-based) acoustic models, rather than phonetic models, is compelling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.43482697010040283,
                    "sentence": "This represents a promising direction for future research.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6198830604553223,
                    "sentence": "Cons",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7949107885360718,
                    "sentence": "- The paper lacks sufficient context and fails to cite several relevant prior works.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.2127189338207245,
                    "sentence": "In addition to the references already mentioned in my comments, the authors should also consider citing a related 2016 Interspeech paper: Zhang et al., \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.7936028431808086,
            "class_probabilities": {
                "human": 0.1936532144943455,
                "ai": 0.7936028431808086,
                "mixed": 0.012743942324846002
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.7936028431808086,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.7936028431808086,
                    "human": 0.1936532144943455,
                    "mixed": 0.012743942324846002
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents an end-to-end speech recognition system built on a linear conditional random field framework. A convolutional neural network (convnet) is employed to estimate node potentials, while transition scores are represented by trained scalar values. The acoustic model within the convnet generates scores for letters rather than phonemes, thereby reducing the reliance on expert knowledge during system training. During inference, the system combines scores from a word-level language model, the convnet's node potentials, learned letter-to-letter transition scores, and a word insertion penalty to determine the highest-scoring word hypothesis. The model can be trained using raw audio waveforms, power spectra, or MFCC features through conditional maximum likelihood estimation. Experiments conducted on the Librispeech dataset demonstrate that the model achieves a word error rate (WER) of 7.2% on the test-clean set using MFCC features, 9.4% WER with power spectral features, and 10.1% WER with raw waveform inputs.\nPros \n+ The use of a convnet trained from scratch with conditional maximum likelihood to build a speech recognition system for English based on graphemic (letter-based) acoustic models, rather than phonetic models, is compelling. This represents a promising direction for future research.\nCons \n- The paper lacks sufficient context and fails to cite several relevant prior works. In addition to the references already mentioned in my comments, the authors should also consider citing a related 2016 Interspeech paper: Zhang et al., \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks.\""
        }
    ]
}