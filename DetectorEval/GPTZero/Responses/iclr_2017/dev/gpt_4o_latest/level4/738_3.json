{
    "version": "2025-01-09-base",
    "scanId": "925a0ca9-22bf-4103-832e-056745e4c52b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9359103441238403,
                    "sentence": "CONTRIBUTIONS",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8768526315689087,
                    "sentence": "During the training of LSTMs, many intermediate gradients approach zero due to the flat regions of the tanh and sigmoid activation functions when far from the origin.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8314352631568909,
                    "sentence": "This paper demonstrates that rounding these small gradients to zero leads to matrices with up to 80% sparsity during training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7466268539428711,
                    "sentence": "Furthermore, it shows that applying this sparsification while training character-level LSTM language models does not significantly impact the model's final performance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6898115277290344,
                    "sentence": "The authors propose that this sparsity could be leveraged by specialized hardware to enhance the energy efficiency and speed of training recurrent networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5773237943649292,
                    "sentence": "NOVELTY",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.3263426125049591,
                    "sentence": "To the best of my knowledge, the use of gradient thresholding to induce sparsity and improve efficiency during RNN training represents a novel contribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.3384464681148529,
                    "sentence": "MISSING CITATIONS",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.2536536753177643,
                    "sentence": "Previous research has investigated the use of low-precision arithmetic for training recurrent neural network language models:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.16881409287452698,
                    "sentence": "Hubara et al., \"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.15144553742985709
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.6901468531159954,
            "class_probabilities": {
                "human": 0.3045187780479302,
                "ai": 0.6901468531159954,
                "mixed": 0.005334368836074327
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.6901468531159954,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.6901468531159954,
                    "human": 0.3045187780479302,
                    "mixed": 0.005334368836074327
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "CONTRIBUTIONS \nDuring the training of LSTMs, many intermediate gradients approach zero due to the flat regions of the tanh and sigmoid activation functions when far from the origin. This paper demonstrates that rounding these small gradients to zero leads to matrices with up to 80% sparsity during training. Furthermore, it shows that applying this sparsification while training character-level LSTM language models does not significantly impact the model's final performance. The authors propose that this sparsity could be leveraged by specialized hardware to enhance the energy efficiency and speed of training recurrent networks.\nNOVELTY \nTo the best of my knowledge, the use of gradient thresholding to induce sparsity and improve efficiency during RNN training represents a novel contribution.\nMISSING CITATIONS \nPrevious research has investigated the use of low-precision arithmetic for training recurrent neural network language models: \nHubara et al., \"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\"."
        }
    ]
}