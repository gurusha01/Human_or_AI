{
    "version": "2025-01-09-base",
    "scanId": "ea29e364-744d-485c-81ed-11b0d138aaf5",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9990806579589844,
                    "sentence": "This study integrates an LDA-inspired topic model with an RNN, modeling their interaction by introducing an additive effect on the predictive distribution through the topic parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987054467201233,
                    "sentence": "A variational auto-encoder is employed to infer the topic distribution for a given text, while the RNN is trained as a standard RNNLM.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991005063056946,
                    "sentence": "The final hidden state of the RNNLM is concatenated with the topic parameters to form a feature representation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994199872016907,
                    "sentence": "The manuscript is well-written and straightforward to follow.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992031455039978,
                    "sentence": "Incorporating the topic as an additive effect on the vocabulary simplifies inference; however, it seems intuitive that the topic might also influence the RNN's dynamics, such as its internal state.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990642070770264,
                    "sentence": "The results for using this model as a feature extractor on the IMDB dataset are quite strong.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994871020317078,
                    "sentence": "Was the RNN fine-tuned on the labeled IMDB data?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989168047904968,
                    "sentence": "On the other hand, the results for PTB are less competitive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999525785446167,
                    "sentence": "According to the original paper, an ensemble of two LSTMs achieves a performance comparable to the topicRNN score.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995214939117432,
                    "sentence": "Overall, this approach of jointly modeling topics and a language model appears effective and relatively straightforward to implement.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.73826003074646,
                    "sentence": "That said, the IMDB result is no longer state-of-the-art, as a better result was reported in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7944840788841248,
                    "sentence": "Some questions for consideration:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7960615158081055,
                    "sentence": "- How critical is the stop-word modeling?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6468511819839478,
                    "sentence": "What would the results look like if \\( l_t = 0.5 \\) for all \\( t \\)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6248816251754761,
                    "sentence": "- It is surprising that the RNN outperformed the LSTM.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6462514400482178,
                    "sentence": "Was gradient clipping applied in the topicLSTM case?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7451720237731934,
                    "sentence": "Did GRUs also fail to perform effectively?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7349836230278015,
                    "sentence": "- It is unfortunate that the model relies on a stop-word list.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6611735820770264,
                    "sentence": "Is the link in footnote 4 the exact list used in the experiments?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5286585688591003,
                    "sentence": "- Does factoring out topics in this manner allow the RNN to scale more effectively with an increased number of neurons?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6646885871887207,
                    "sentence": "How interpretable is the topic distribution for individual documents?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5823503732681274,
                    "sentence": "Are the distributions typically sharp or diffuse?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5534301996231079,
                    "sentence": "Could you provide examples of the inferred distributions?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8153301477432251,
                    "sentence": "The topics for IMDB seem peculiar, with 'campbell' appearing as the top word for two different topics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7386624813079834,
                    "sentence": "It would be interesting to compare these topics with those derived from LDA on the same datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7409006953239441,
                    "sentence": "Minor comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8514583706855774,
                    "sentence": "- Below Figure 2: GHz Ã¢ ' GB",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7547836899757385,
                    "sentence": "- \\( \\Gamma \\) is not defined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.9103421900070616
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.0003567719278557345
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.9997847017652333,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997847017652333,
                "mixed": 0.00021529823476680056
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997847017652333,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997847017652333,
                    "human": 0,
                    "mixed": 0.00021529823476680056
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This study integrates an LDA-inspired topic model with an RNN, modeling their interaction by introducing an additive effect on the predictive distribution through the topic parameters. A variational auto-encoder is employed to infer the topic distribution for a given text, while the RNN is trained as a standard RNNLM. The final hidden state of the RNNLM is concatenated with the topic parameters to form a feature representation.\nThe manuscript is well-written and straightforward to follow. Incorporating the topic as an additive effect on the vocabulary simplifies inference; however, it seems intuitive that the topic might also influence the RNN's dynamics, such as its internal state. The results for using this model as a feature extractor on the IMDB dataset are quite strong. Was the RNN fine-tuned on the labeled IMDB data? On the other hand, the results for PTB are less competitive. According to the original paper, an ensemble of two LSTMs achieves a performance comparable to the topicRNN score. Overall, this approach of jointly modeling topics and a language model appears effective and relatively straightforward to implement.\nThat said, the IMDB result is no longer state-of-the-art, as a better result was reported in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).\nSome questions for consideration:\n- How critical is the stop-word modeling? What would the results look like if \\( l_t = 0.5 \\) for all \\( t \\)?\n- It is surprising that the RNN outperformed the LSTM. Was gradient clipping applied in the topicLSTM case? Did GRUs also fail to perform effectively?\n- It is unfortunate that the model relies on a stop-word list. Is the link in footnote 4 the exact list used in the experiments?\n- Does factoring out topics in this manner allow the RNN to scale more effectively with an increased number of neurons? How interpretable is the topic distribution for individual documents? Are the distributions typically sharp or diffuse? Could you provide examples of the inferred distributions? The topics for IMDB seem peculiar, with 'campbell' appearing as the top word for two different topics. It would be interesting to compare these topics with those derived from LDA on the same datasets.\nMinor comments:\n- Below Figure 2: GHz Ã¢ ' GB\n- \\( \\Gamma \\) is not defined."
        }
    ]
}