{
    "version": "2025-01-09-base",
    "scanId": "0c808ddd-f00c-425b-876a-9b24ae3de372",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.4438170790672302,
                    "sentence": "This paper uses Tensors to build generative models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3025880753993988,
                    "sentence": "The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4543730914592743,
                    "sentence": "Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5054499506950378,
                    "sentence": "This approach seems quite elegant.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5833667516708374,
                    "sentence": "It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5513434410095215,
                    "sentence": "The experiments are on simple, synthetic examples of missing data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.46905720233917236,
                    "sentence": "This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4662318229675293,
                    "sentence": "One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3882271647453308,
                    "sentence": "Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4152362048625946,
                    "sentence": "In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3914879560470581,
                    "sentence": "I was a little confused about how the input of missing data is handled experimentally.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.47569504380226135,
                    "sentence": "From the introductory discussion my impression was that the generative model was built over region patches in the image.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5390351414680481,
                    "sentence": "This led me to believe that they would marginalize over missing regions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.672537624835968,
                    "sentence": "However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6686227917671204,
                    "sentence": "Why is it appropriate to marginalize over missing pixels?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.45776084065437317,
                    "sentence": "Specifically, $x_i$ in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.466266006231308,
                    "sentence": "How is this done when only a subset of a region is missing?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2649132013320923,
                    "sentence": "It also seems like the summation in the equation following Equation 6 could be quite large.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4654388427734375,
                    "sentence": "What is the run time of this?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6201317310333252,
                    "sentence": "The paper is also a bit schizophrenic about the extent to which the results are applicable beyond images.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.536239743232727,
                    "sentence": "The motivation for the probabilistic model is mostly in terms of images.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6189960837364197,
                    "sentence": "But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.616425633430481,
                    "sentence": "This would be more convincing if there were experiments outside the image domain.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4512326419353485,
                    "sentence": "It was also not clear to me how, if at all, the proposed network makes use of translation invariance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.43889138102531433,
                    "sentence": "It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4694429934024811,
                    "sentence": "Is such invariance built into the authors' network?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.44202855229377747,
                    "sentence": "If not, why would we expect it to work well in challenging image domains?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5980320572853088,
                    "sentence": "As a minor point, the paper is not carefully proofread.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4059634208679199,
                    "sentence": "To just give a few examples from the first page or so:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.385952353477478,
                    "sentence": "\"significantly lesser\" -> \"significantly less\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.280782014131546,
                    "sentence": "\"the the\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5297887921333313,
                    "sentence": "\"provenly\" -> provably",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 9,
                    "completely_generated_prob": 4.1887248735431006e-08
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.49770307481643766,
            "class_probabilities": {
                "human": 0.5021818426438073,
                "ai": 0.49770307481643766,
                "mixed": 0.00011508253975490195
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5021818426438073,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.49770307481643766,
                    "human": 0.5021818426438073,
                    "mixed": 0.00011508253975490195
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper uses Tensors to build generative models. The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor. Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels. \nThis approach seems quite elegant. It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented.\nThe experiments are on simple, synthetic examples of missing data. This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data. One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability. Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data? In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation.\nI was a little confused about how the input of missing data is handled experimentally. From the introductory discussion my impression was that the generative model was built over region patches in the image. This led me to believe that they would marginalize over missing regions. However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information. Why is it appropriate to marginalize over missing pixels? Specifically, $x_i$ in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions. How is this done when only a subset of a region is missing? It also seems like the summation in the equation following Equation 6 could be quite large. What is the run time of this? \nThe paper is also a bit schizophrenic about the extent to which the results are applicable beyond images. The motivation for the probabilistic model is mostly in terms of images. But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images. This would be more convincing if there were experiments outside the image domain.\nIt was also not clear to me how, if at all, the proposed network makes use of translation invariance. It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing. Is such invariance built into the authors' network? If not, why would we expect it to work well in challenging image domains?\nAs a minor point, the paper is not carefully proofread. To just give a few examples from the first page or so:\n\"significantly lesser\" -> \"significantly less\"\n\"the the\"\n\"provenly\" -> provably"
        }
    ]
}