{
    "version": "2025-01-09-base",
    "scanId": "ba15cc44-285a-4e54-98a1-947c003f6404",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.3801884949207306,
                    "sentence": "This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2575814723968506,
                    "sentence": "The approach is to model successive \"extensions\" of a program tree conditioned on some embedding of the input/output pairs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.37246519327163696,
                    "sentence": "Extension probabilities are computed as a function of leaf and production rule embeddings ᅳ one of the main contributions is the so-called \"Recursive-Reverse-Recursive Neural Net\" which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3036043047904968,
                    "sentence": "There are many strong points about this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3524681031703949,
                    "sentence": "In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.30229857563972473,
                    "sentence": "The R3NN idea is a good one and the authors motivate it quite well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.35941600799560547,
                    "sentence": "Moreover, the authors have explored many variants of this model to understand what works well and what does not.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3794737458229065,
                    "sentence": "Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.43482545018196106,
                    "sentence": "Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5880947113037109,
                    "sentence": "And it's unclear why the authors did not simply train on longer programs… It also seems that the number of I/O pairs is fixed?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5930594205856323,
                    "sentence": "So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002569488948211074,
                    "sentence": "Overall however, I would certainly like to see this paper accepted at ICLR.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0027596044819802046,
                    "sentence": "Other miscellaneous comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0027389845345169306,
                    "sentence": "* Too many e's in the expansion probability expression ᅳ might be better just to write \"Softmax\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0026576544623821974,
                    "sentence": "* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002714042318984866,
                    "sentence": "* The authors claim that using hyperbolic tangent activation functions is important ᅳ I'd be interested in some more discussion on this and why something like ReLU would not be good.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00364316301420331,
                    "sentence": "* It's unclear to me how batching was done in this setting since each program has a different tree topology.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0025893256533890963,
                    "sentence": "More discussion on this would be appreciated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0026938426308333874,
                    "sentence": "Related to this, it would be good to add details on optimization algorithm (SGD?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0029174769297242165,
                    "sentence": "Adagrad?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004512477200478315,
                    "sentence": "Adam?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004148582927882671,
                    "sentence": "), learning rate schedules and how weights were initialized.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002840347122400999,
                    "sentence": "At the moment, the results are not particularly reproducible.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0022698165848851204,
                    "sentence": "* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0036355052143335342,
                    "sentence": "Or was it some other reason?)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004430073779076338,
                    "sentence": "* There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 9,
                    "completely_generated_prob": 4.1887248735431006e-08
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 7,
                    "completely_generated_prob": 2.1228438805416278e-06
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.19729206963249518,
            "class_probabilities": {
                "human": 0.8027079303675048,
                "ai": 0.19729206963249518,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.8027079303675048,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.19729206963249518,
                    "human": 0.8027079303675048,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel. The approach is to model successive \"extensions\" of a program tree conditioned on some embedding of the input/output pairs. Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called \"Recursive-Reverse-Recursive Neural Net\" which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).\nThere are many strong points about this paper. In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future. The R3NN idea is a good one and the authors motivate it quite well. Moreover, the authors have explored many variants of this model to understand what works well and what does not. Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read. Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer. And it's unclear why the authors did not simply train on longer programs… It also seems that the number of I/O pairs is fixed? So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…). Overall however, I would certainly like to see this paper accepted at ICLR.\nOther miscellaneous comments:\n* Too many e's in the expansion probability expression — might be better just to write \"Softmax\".\n* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).\n* The authors claim that using hyperbolic tangent activation functions is important — I'd be interested in some more discussion on this and why something like ReLU would not be good.\n* It's unclear to me how batching was done in this setting since each program has a different tree topology. More discussion on this would be appreciated. Related to this, it would be good to add details on optimization algorithm (SGD? Adagrad? Adam?), learning rate schedules and how weights were initialized. At the moment, the results are not particularly reproducible.\n* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs? Or was it some other reason?)\n* There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs)."
        }
    ]
}