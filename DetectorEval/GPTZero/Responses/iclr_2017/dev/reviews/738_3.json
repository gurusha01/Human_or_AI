{
    "version": "2025-01-09-base",
    "scanId": "41f07475-4be1-450e-8a3d-75381a763b53",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.764176607131958,
                    "sentence": "CONTRIBUTIONS",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8161574602127075,
                    "sentence": "When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7655966281890869,
                    "sentence": "This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7096872329711914,
                    "sentence": "The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18274173140525818,
                    "sentence": "NOVELTY",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06303270161151886,
                    "sentence": "Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04247698932886124,
                    "sentence": "MISSING CITATIONS",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07158565521240234,
                    "sentence": "Prior work has explored low-precision arithmetic for recurrent neural network language models:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03376422077417374,
                    "sentence": "Hubara et al, \"Quantized Neural Networks: Training Neural Networks with",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03712283447384834,
                    "sentence": "Low Precision Weights and Activations\",",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.07332528267997859
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.493689680772086,
            "class_probabilities": {
                "human": 0.5042687453600593,
                "ai": 0.493689680772086,
                "mixed": 0.002041573867854491
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5042687453600593,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.493689680772086,
                    "human": 0.5042687453600593,
                    "mixed": 0.002041573867854491
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "CONTRIBUTIONS\nWhen training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.\nNOVELTY\nThresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.\nMISSING CITATIONS\nPrior work has explored low-precision arithmetic for recurrent neural network language models:\nHubara et al, \"Quantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations\","
        }
    ]
}