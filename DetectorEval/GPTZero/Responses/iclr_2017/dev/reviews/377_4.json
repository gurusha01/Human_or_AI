{
    "version": "2025-01-09-base",
    "scanId": "f210e3d7-7105-4b42-a7ae-b97c58234e44",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.004060414619743824,
                    "sentence": "This paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003645175602287054,
                    "sentence": "The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0073978109285235405,
                    "sentence": "The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0028952101711183786,
                    "sentence": "We have a mixed opinion about this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0021139681339263916,
                    "sentence": "The paper is written clearly and presents interesting experimental findings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002380930120125413,
                    "sentence": "It introduces and formulates a problem potentially important for many robotics applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0019985707476735115,
                    "sentence": "Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00357670895755291,
                    "sentence": "Pros:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0022246125154197216,
                    "sentence": "+ This paper introduces a new problem of learning latent properties in the agent's environment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0021551998797804117,
                    "sentence": "+ The paper presents a framework to appropriately combine existing tools to address the formulated problem.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002630285220220685,
                    "sentence": "+ The paper tries reinforcement learning with image inputs and fist-like actuator actions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004456936381757259,
                    "sentence": "This will lead to its direct application to robots.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004799568094313145,
                    "sentence": "Cons:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003370650578290224,
                    "sentence": "- Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18924404680728912,
                    "sentence": "The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1257198452949524,
                    "sentence": "- In the Towers experiment, the results of probably the most important setting, \"Fist Pixels\", are missing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.24268192052841187,
                    "sentence": "This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.34725499153137207,
                    "sentence": "However, Figure 5 is missing the results with this setting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.42831286787986755,
                    "sentence": "Is there any reason behind this?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.27815815806388855,
                    "sentence": "- The paper lacks its comparison to any baseline methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21804821491241455,
                    "sentence": "Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18841403722763062,
                    "sentence": "For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17412540316581726,
                    "sentence": "That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                }
            ],
            "completely_generated_prob": 0.03751552122182913,
            "class_probabilities": {
                "human": 0.9622006644706045,
                "ai": 0.03751552122182913,
                "mixed": 0.00028381430756647936
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9622006644706045,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.03751552122182913,
                    "human": 0.9622006644706045,
                    "mixed": 0.00028381430756647936
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment. The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible. The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals.\nWe have a mixed opinion about this paper. The paper is written clearly and presents interesting experimental findings. It introduces and formulates a problem potentially important for many robotics applications. Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.\nPros:\n+ This paper introduces a new problem of learning latent properties in the agent's environment.\n+ The paper presents a framework to appropriately combine existing tools to address the formulated problem.\n+ The paper tries reinforcement learning with image inputs and fist-like actuator actions. This will lead to its direct application to robots.\nCons:\n- Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them. The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic.\n- In the Towers experiment, the results of probably the most important setting, \"Fist Pixels\", are missing. This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots. However, Figure 5 is missing the results with this setting. Is there any reason behind this?\n- The paper lacks its comparison to any baseline methods. Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task. For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach? That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?"
        }
    ]
}