{
    "version": "2025-01-09-base",
    "scanId": "3e198208-b9c9-4c5f-90dd-42676f91919f",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0029499398078769445,
                    "sentence": "This paper trains a generative model which transforms noise into model samples by a gradual denoising process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0027697677724063396,
                    "sentence": "It is similar to a generative model based on diffusion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0034282433334738016,
                    "sentence": "Unlike the diffusion approach:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00279109925031662,
                    "sentence": "- It uses only a small number of denoising steps, and is thus far more computationally efficient.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0017309400718659163,
                    "sentence": "- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) \" x), and then runs in the same direction as the generative model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0025634956546127796,
                    "sentence": "This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00255336775444448,
                    "sentence": "(This also seems somewhat related to ladder networks.)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0030005034059286118,
                    "sentence": "- There is no tractable variational bound on the log likelihood.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0016721165738999844,
                    "sentence": "I liked the idea, and found the visual sample quality given a short chain impressive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0019879010505974293,
                    "sentence": "The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0023887695278972387,
                    "sentence": "It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00201469287276268,
                    "sentence": "Detailed comments follow:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0024035610258579254,
                    "sentence": "Sec.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001995829865336418,
                    "sentence": "2:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0008147127227857709,
                    "sentence": "\"theta(0) the\" -> \"theta(0) be the\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0008491119369864464,
                    "sentence": "\"theta(t) the\" -> \"theta(t) be the\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00098369549959898,
                    "sentence": "\"what we will be using\" -> \"which we will be doing\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0010452165734022856,
                    "sentence": "I like that you infer q(z^0\"x), and then run inference in the same order as the generative chain.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0008884670678526163,
                    "sentence": "This reminds me slightly of ladder networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001043340191245079,
                    "sentence": "\"q.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0014271157560870051,
                    "sentence": "Having learned\" -> \"q.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0008812256273813546,
                    "sentence": "[paragraph break] Having learned\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0012317239306867123,
                    "sentence": "Sec 3.3:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0007383760530501604,
                    "sentence": "\"learn to inverse\" -> \"learn to reverse\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0008685510838404298,
                    "sentence": "Sec.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.003384294919669628,
                    "sentence": "4:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0007737059495411813,
                    "sentence": "\"For each experiments\" -> \"For each experiment\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0011698425514623523,
                    "sentence": "How sensitive are your results to infusion rate?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.000982396537438035,
                    "sentence": "Sec.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0017223820323124528,
                    "sentence": "5: \"appears to provide more accurate models\" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0032707161735743284,
                    "sentence": "Fig 4.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0018199047772213817,
                    "sentence": "-- neat!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.024462635563563526,
            "class_probabilities": {
                "human": 0.9755373644364365,
                "ai": 0.024462635563563526,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9755373644364365,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.024462635563563526,
                    "human": 0.9755373644364365,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach:\n- It uses only a small number of denoising steps, and is thus far more computationally efficient.\n- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) \" x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.)\n- There is no tractable variational bound on the log likelihood.\nI liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.\nDetailed comments follow:\nSec. 2:\n\"theta(0) the\" -> \"theta(0) be the\"\n\"theta(t) the\" -> \"theta(t) be the\"\n\"what we will be using\" -> \"which we will be doing\"\nI like that you infer q(z^0\"x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks.\n\"q. Having learned\" -> \"q. [paragraph break] Having learned\"\nSec 3.3:\n\"learn to inverse\" -> \"learn to reverse\"\nSec. 4:\n\"For each experiments\" -> \"For each experiment\"\nHow sensitive are your results to infusion rate?\nSec. 5: \"appears to provide more accurate models\" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.\nFig 4. -- neat!"
        }
    ]
}