{
    "version": "2025-01-09-base",
    "scanId": "26e34b35-fc02-489d-9989-066ae801c3f9",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0007459463668055832,
                    "sentence": "This paper explores the topic of intrinsic motivation in the context of deep RL.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0004971410962752998,
                    "sentence": "It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \"venture\", maybe).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0006024511530995369,
                    "sentence": "Novelty: none of the proposed types of intrinsic motivation are novel, and it's arguable whether the application to deep RL is novel (see e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00099998340010643,
                    "sentence": "Kompella et al 2012).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.000752375868614763,
                    "sentence": "Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0006260432419367135,
                    "sentence": "For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0006414787494577467,
                    "sentence": "So other forms of learning progress or surprise derived from the agent's competence instead might be more promising in the long run?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0010985344415530562,
                    "sentence": "See also Srivastava et al 2012 for further thoughts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0007664169534109533,
                    "sentence": "Computation time: I find the paper's claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0009020536090247333,
                    "sentence": "So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.009782813243742878,
            "class_probabilities": {
                "human": 0.9902171867562571,
                "ai": 0.009782813243742878,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9902171867562571,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.009782813243742878,
                    "human": 0.9902171867562571,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \"venture\", maybe).\nNovelty: none of the proposed types of intrinsic motivation are novel, and it's arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent's competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\nComputation time: I find the paper's claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?"
        }
    ]
}