{
    "version": "2025-01-09-base",
    "scanId": "bdf4875f-928e-4722-baa8-f39ac22e1094",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.7775489687919617,
                    "sentence": "This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8079380989074707,
                    "sentence": "A convnet estimates node potentials, while transition scores are provided by trained scalar values.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8639609217643738,
                    "sentence": "The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8748000264167786,
                    "sentence": "At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8472134470939636,
                    "sentence": "The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9292891621589661,
                    "sentence": "Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8639794588088989,
                    "sentence": "Pros",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9238882660865784,
                    "sentence": "+ It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8533665537834167,
                    "sentence": "This is a promising research direction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9485787749290466,
                    "sentence": "Cons",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8870882391929626,
                    "sentence": "- The paper is missing a lot of context / prior work that deserves to be cited.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9535713791847229,
                    "sentence": "In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\",",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.7145451996534505
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.49464236844505877,
            "class_probabilities": {
                "human": 0.5052418477688814,
                "ai": 0.49464236844505877,
                "mixed": 0.00011578378605963213
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5052418477688814,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.49464236844505877,
                    "human": 0.5052418477688814,
                    "mixed": 0.00011578378605963213
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework. A convnet estimates node potentials, while transition scores are provided by trained scalar values. The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system. At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis. The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation. Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform.\nPros\n+ It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models. This is a promising research direction.\nCons\n- The paper is missing a lot of context / prior work that deserves to be cited. In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\","
        }
    ]
}