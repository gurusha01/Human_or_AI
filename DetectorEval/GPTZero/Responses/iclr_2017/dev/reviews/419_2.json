{
    "version": "2025-01-09-base",
    "scanId": "ed058c2e-20b4-48ed-8cb2-f5099d8b1459",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.02047879435122013,
                    "sentence": "This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023433463647961617,
                    "sentence": "Experiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023144828155636787,
                    "sentence": "The authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.038003165274858475,
                    "sentence": "Some questions and comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03314879164099693,
                    "sentence": "- In Table 2, how do you use LDA features for RNN (RNN LDA features)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01899605616927147,
                    "sentence": "- I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0363144613802433,
                    "sentence": "I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02529284544289112,
                    "sentence": "- The generated text in Table 3 are not meaningful to me.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.027776340022683144,
                    "sentence": "What is this supposed to highlight?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015568168833851814,
                    "sentence": "Is this generated text for topic \"trading\"?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016616223379969597,
                    "sentence": "What about the IMDB one?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.026622451841831207,
                    "sentence": "- How scalable is the proposed method for large vocabulary size (>10K)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.029969803988933563,
                    "sentence": "- What is the accuracy on IMDB if the extracted features is used directly to perform classification?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018412502482533455,
                    "sentence": "(instead of being passed to a neural network with one hidden state).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03425568342208862,
                    "sentence": "I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                }
            ],
            "completely_generated_prob": 0.037526171686261885,
            "class_probabilities": {
                "human": 0.9624738283137382,
                "ai": 0.037526171686261885,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9624738283137382,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.037526171686261885,
                    "human": 0.9624738283137382,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. \nExperiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. \nThe authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.\nSome questions and comments:\n- In Table 2, how do you use LDA features for RNN (RNN LDA features)? \n- I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM.\n- The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic \"trading\"? What about the IMDB one?\n- How scalable is the proposed method for large vocabulary size (>10K)?\n- What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines."
        }
    ]
}