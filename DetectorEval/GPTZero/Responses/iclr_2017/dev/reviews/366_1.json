{
    "version": "2025-01-09-base",
    "scanId": "1c8644b5-7586-4697-8fd3-b49beb5e5344",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.030923038721084595,
                    "sentence": "The authors propose NVI for LDA variants.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018094297498464584,
                    "sentence": "The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.020988840609788895,
                    "sentence": "The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021715573966503143,
                    "sentence": "In general, I like the direction of this paper and NVI looks promising for LDA.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0315496064722538,
                    "sentence": "The experimental results however confound model vs inference which makes it hard to understand the significance of the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.026593316346406937,
                    "sentence": "Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016875097528100014,
                    "sentence": "This makes it hard to understand when the proposed method can be expected to work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01922532357275486,
                    "sentence": "Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016236966475844383,
                    "sentence": "Figure 1: Is this prior or posterior?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013431212864816189,
                    "sentence": "The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008479517884552479,
                    "sentence": "Section 3.2: it is not clear what you mean by unimodal in softmax basis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013808528892695904,
                    "sentence": "Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012435496784746647,
                    "sentence": "Isn't the softmax basis still multimodal?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009913809597492218,
                    "sentence": "None of the numbers include error bars.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014120582491159439,
                    "sentence": "Are the results statistically significant?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010237038135528564,
                    "sentence": "Minor comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01872524619102478,
                    "sentence": "Last term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.028536172583699226,
                    "sentence": "The idea of using an inference network is much older, cf.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04142223671078682,
                    "sentence": "Helmholtz machine.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.039837804045504216,
            "class_probabilities": {
                "human": 0.9601216422360306,
                "ai": 0.039837804045504216,
                "mixed": 4.055371846526579e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9601216422360306,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.039837804045504216,
                    "human": 0.9601216422360306,
                    "mixed": 4.055371846526579e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)\nIn general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. \nCan you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?\nFigure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing.\nSection 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?\nNone of the numbers include error bars. Are the results statistically significant?\nMinor comments:\nLast term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps?\nThe idea of using an inference network is much older, cf. Helmholtz machine."
        }
    ]
}