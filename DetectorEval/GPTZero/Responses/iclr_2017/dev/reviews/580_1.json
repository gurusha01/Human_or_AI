{
    "version": "2025-01-09-base",
    "scanId": "03a71dc3-fa10-48d4-b675-1f36d3eac459",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.992948055267334,
                    "sentence": "Summary: The authors present a simple RNN with linear dynamics for language modeling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9933199286460876,
                    "sentence": "The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9857648611068726,
                    "sentence": "Overall, the quantitative comparison on a benchmark task is underwhelming.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9914319515228271,
                    "sentence": "It's unclear why the authors didn't consider a more common dataset, and they only considered a single dataset.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9929171800613403,
                    "sentence": "On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9899712800979614,
                    "sentence": "Overall, I recommend that the paper is accepted, despite the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9934907555580139,
                    "sentence": "It provides an interesting read and an important contribution to the research dialogue.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9882070422172546,
                    "sentence": "Feedback",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9960881471633911,
                    "sentence": "The paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9027640223503113,
                    "sentence": "Some of the experiments were very compelling, whereas some of them (eg.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9204889535903931,
                    "sentence": "4.6) sort of feels like you're just showing the reader that the model fits the data well, not that the model has any particularly important property.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7972549796104431,
                    "sentence": "We trust that the model fits the data well, since you get reasonable perplexity results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8765255212783813,
                    "sentence": "LSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5468559861183167,
                    "sentence": "It would have been nice if you compared your model to non-linear methods on this sort of data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5811054706573486,
                    "sentence": "Don't be scared of negative results!",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0885455310344696,
                    "sentence": "It would be interesting if the non-linear methods were substantially better on these tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11568720638751984,
                    "sentence": "You should definitely add a discussion of Belanger and Kakade 2015 to the related work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11407887190580368,
                    "sentence": "They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.089051254093647,
                    "sentence": "On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07771759480237961,
                    "sentence": "More broadly, it would be useful for readers if you discussed LDS more directly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06892683357000351,
                    "sentence": "A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06811965256929398,
                    "sentence": "For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07645093649625778,
                    "sentence": "One last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05919754505157471,
                    "sentence": "Could you do something similar in your model?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0765395313501358,
                    "sentence": "What if you said that each matrix is a sparse/convex combination of a set of dictionary matrices?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09215816855430603,
                    "sentence": "This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10225885361433029,
                    "sentence": "This could also provide more scalability to word-level problems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.5710657228372709
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.013701276613118245
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                }
            ],
            "completely_generated_prob": 0.39352607818653346,
            "class_probabilities": {
                "human": 0.6018634136970511,
                "ai": 0.39352607818653346,
                "mixed": 0.004610508116415329
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.6018634136970511,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.39352607818653346,
                    "human": 0.6018634136970511,
                    "mixed": 0.004610508116415329
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary: The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It's unclear why the authors didn't consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. \nOverall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. \nFeedback\nThe paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you're just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. \nLSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don't be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. \nYou should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. \nMore broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. \nOne last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model?\nWhat if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems."
        }
    ]
}