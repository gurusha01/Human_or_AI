{
    "version": "2025-01-09-base",
    "scanId": "c7975f03-b7b3-4e1b-b288-1582e22b671c",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.04692646488547325,
                    "sentence": "This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.034625131636857986,
                    "sentence": "Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04152604937553406,
                    "sentence": "This paper shows promising results in automated algorithm discovery using reinforcement learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012630059383809566,
                    "sentence": "However it is not very clear what is the main motivation of the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.026242010295391083,
                    "sentence": "Is the main motivation better exploration for policy gradient methods?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015707606449723244,
                    "sentence": "If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018697373569011688,
                    "sentence": "While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01081420574337244,
                    "sentence": "If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01518066693097353,
                    "sentence": "Authors should make it clear which is the main motivation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013119575567543507,
                    "sentence": "Also the action space is too small.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008829215541481972,
                    "sentence": "In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012785756029188633,
                    "sentence": "So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04647546634078026,
                    "sentence": "--------------------------",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015124691650271416,
                    "sentence": "After rebuttal:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013213668018579483,
                    "sentence": "I missed the action sequences argument when I pointed about small action space issue.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.020145997405052185,
                    "sentence": "For question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02650599740445614,
                    "sentence": "For example, see Mnih & Gregor, 2014.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.020238900557160378,
                    "sentence": "I have increased my rating from 6 to 7.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022771650925278664,
                    "sentence": "I still encourage the authors to improve their baseline.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 7,
                    "completely_generated_prob": 2.1228438805416278e-06
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.0405660129718972,
            "class_probabilities": {
                "human": 0.9593934640665033,
                "ai": 0.0405660129718972,
                "mixed": 4.052296159948909e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9593934640665033,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.0405660129718972,
                    "human": 0.9593934640665033,
                    "mixed": 4.052296159948909e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning.\nThis paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation.\nAlso the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space.\n--------------------------\nAfter rebuttal:\nI missed the action sequences argument when I pointed about small action space issue.\nFor question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014.\nI have increased my rating from 6 to 7. I still encourage the authors to improve their baseline."
        }
    ]
}