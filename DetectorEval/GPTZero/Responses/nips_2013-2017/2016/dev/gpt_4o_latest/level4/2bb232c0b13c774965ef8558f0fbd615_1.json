{
    "version": "2025-01-09-base",
    "scanId": "8aeef93f-0d4f-4b60-a79b-22e6b5559e1d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9969743490219116,
                    "sentence": "The authors present an end-to-end system for parsing handwritten text from paragraph images containing multiple horizontal lines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9931089282035828,
                    "sentence": "This is achieved by incorporating an attention mechanism into the single-line parser proposed by Graves et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9914413094520569,
                    "sentence": "[18] and replacing the original softmax decoder with Bidirectional-LSTMs for text decoding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9846991896629333,
                    "sentence": "The system is evaluated on the Rimes and IAM datasets, where it achieves performance comparable to state-of-the-art methods on Rimes but falls short on IAM.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9881464838981628,
                    "sentence": "The work is a commendable application of guided attention over images to extract sequences of handwritten text lines, building on prior approaches such as those by Xu et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9926856160163879,
                    "sentence": "[38] and others who employed region-based attention on images.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9914745092391968,
                    "sentence": "However, the paper's exposition could be improved to make it more self-contained.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9973759055137634,
                    "sentence": "As it stands, understanding the proposed model requires prior familiarity with Graves et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9943926334381104,
                    "sentence": "[18].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998276948928833,
                    "sentence": "Enhancing section 3 with a more detailed explanation of the MD-LSTM model (as described in [18]) would significantly aid comprehension.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990155696868896,
                    "sentence": "There are also some unusual results and claims that require further clarification from the authors:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998828649520874,
                    "sentence": "- Why does the proposed method outperform approaches that utilize ground truth line labeling (as shown in Table 2 and discussed in section 5.3)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8866819739341736,
                    "sentence": "Could this be due to the re-initialization of the decoder RNN when ground truth line breaks are used, which does not occur in the character sequence of the attention mechanism?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.778006374835968,
                    "sentence": "Additional details on the authors' implementation of Graves et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.891409695148468,
                    "sentence": "'s method would help address this question.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8659847974777222,
                    "sentence": "- In line 142, the authors claim their method is 20-30x faster than the character decoder approach by Bluche et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8107669353485107,
                    "sentence": "[6], but no actual runtime comparisons are provided.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6204582452774048,
                    "sentence": "Including these would strengthen the claim.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8738936185836792,
                    "sentence": "- Regarding results, the system achieves superior CER% performance in one configuration on the Rimes dataset but performs significantly worse on the IAM dataset, likely due to limited training data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5590439438819885,
                    "sentence": "A straightforward experiment to verify this hypothesis would be to pretrain the model on the combined test and training data of the other dataset before fine-tuning it on the IAM training set.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6514939665794373,
                    "sentence": "The authors are encouraged to include this experiment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7542253732681274,
                    "sentence": "- In line 213, the authors should explain why the attention mechanism fails to capture punctuation marks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8216601610183716,
                    "sentence": "Overall, while the system demonstrates potential, addressing these issues would enhance the clarity, rigor, and impact of the work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.5710657228372709
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.45887534985363754
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.07332528267997859
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.8119739381064363,
            "class_probabilities": {
                "human": 0.18679001347792185,
                "ai": 0.8119739381064363,
                "mixed": 0.0012360484156418805
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8119739381064363,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8119739381064363,
                    "human": 0.18679001347792185,
                    "mixed": 0.0012360484156418805
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The authors present an end-to-end system for parsing handwritten text from paragraph images containing multiple horizontal lines. This is achieved by incorporating an attention mechanism into the single-line parser proposed by Graves et al. [18] and replacing the original softmax decoder with Bidirectional-LSTMs for text decoding. The system is evaluated on the Rimes and IAM datasets, where it achieves performance comparable to state-of-the-art methods on Rimes but falls short on IAM. The work is a commendable application of guided attention over images to extract sequences of handwritten text lines, building on prior approaches such as those by Xu et al. [38] and others who employed region-based attention on images. \nHowever, the paper's exposition could be improved to make it more self-contained. As it stands, understanding the proposed model requires prior familiarity with Graves et al. [18]. Enhancing section 3 with a more detailed explanation of the MD-LSTM model (as described in [18]) would significantly aid comprehension. \nThere are also some unusual results and claims that require further clarification from the authors: \n- Why does the proposed method outperform approaches that utilize ground truth line labeling (as shown in Table 2 and discussed in section 5.3)? Could this be due to the re-initialization of the decoder RNN when ground truth line breaks are used, which does not occur in the character sequence of the attention mechanism? Additional details on the authors' implementation of Graves et al.'s method would help address this question. \n- In line 142, the authors claim their method is 20-30x faster than the character decoder approach by Bluche et al. [6], but no actual runtime comparisons are provided. Including these would strengthen the claim. \n- Regarding results, the system achieves superior CER% performance in one configuration on the Rimes dataset but performs significantly worse on the IAM dataset, likely due to limited training data. A straightforward experiment to verify this hypothesis would be to pretrain the model on the combined test and training data of the other dataset before fine-tuning it on the IAM training set. The authors are encouraged to include this experiment. \n- In line 213, the authors should explain why the attention mechanism fails to capture punctuation marks. \nOverall, while the system demonstrates potential, addressing these issues would enhance the clarity, rigor, and impact of the work."
        }
    ]
}