{
    "version": "2025-01-09-base",
    "scanId": "c6c3b919-2b14-4db5-93e1-fe07fd31ed95",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0014485404826700687,
                    "sentence": "This paper considers the question of adapting the step-size in Stochastic Gradient descent (SGD) and some of its variants.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0015639554476365447,
                    "sentence": "It proposes to use the Barzilai Borwein (BB) method to automatically compute step-sizes in SGD and stochastic variance reduced gradient (SVRG) instead of relying on predefined fixed (decreasing) schemes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001429596683010459,
                    "sentence": "For SGD a smoothing technique is additionally used.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0008714563446119428,
                    "sentence": "The paper addresses an important question for SGD type of algorithms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001137235900387168,
                    "sentence": "The BB method is first implemented within the SVGR.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0007332501118071377,
                    "sentence": "The simulation are convincing is that the optimal step-size is learned after an adaptation phase.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0008197264978662133,
                    "sentence": "I am however wondering why in Figure 1 there is this strong overshoot towards much too small step-sizes in the first iterations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0006704127881675959,
                    "sentence": "It looks suboptimal.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0007649462786503136,
                    "sentence": "For the implementation within SGD, the author(s) need to introduce a smoothing technique.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0005226160283200443,
                    "sentence": "My concern is that within the smoothing formula they reintroduce a deterministic non-adaptive decrease.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0006064939661882818,
                    "sentence": "They explicitly reintroduce a decrease in 1/k+1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001047783880494535,
                    "sentence": "Hence the proposed adaptation scheme seems to present the same drawbacks as a predefined scheme.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0013435962609946728,
                    "sentence": "Other comments: In lemma 1, the expectation should be a conditional expectation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 13,
                    "completely_generated_prob": 1.4476848035102901e-11
                }
            ],
            "completely_generated_prob": 0.00982192507144982,
            "class_probabilities": {
                "human": 0.9901780749285501,
                "ai": 0.00982192507144982,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9901780749285501,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.00982192507144982,
                    "human": 0.9901780749285501,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper considers the question of adapting the step-size in Stochastic Gradient descent (SGD) and some of its variants. It proposes to use the Barzilai Borwein (BB) method to automatically compute step-sizes in SGD and stochastic variance reduced gradient (SVRG) instead of relying on predefined fixed (decreasing) schemes. For SGD a smoothing technique is additionally used. The paper addresses an important question for SGD type of algorithms. The BB method is first implemented within the SVGR. The simulation are convincing is that the optimal step-size is learned after an adaptation phase. I am however wondering why in Figure 1 there is this strong overshoot towards much too small step-sizes in the first iterations. It looks suboptimal. For the implementation within SGD, the author(s) need to introduce a smoothing technique. My concern is that within the smoothing formula they reintroduce a deterministic non-adaptive decrease. They explicitly reintroduce a decrease in 1/k+1. Hence the proposed adaptation scheme seems to present the same drawbacks as a predefined scheme. Other comments: In lemma 1, the expectation should be a conditional expectation."
        }
    ]
}