{
    "version": "2025-01-09-base",
    "scanId": "4119c14b-0083-4f53-a743-751040c65cf4",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0755489245057106,
                    "sentence": "This paper combines the residual like network with multimodal fusion of language and visual features for Visual QA task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1373680830001831,
                    "sentence": "Impressive results on standard benchmarks show progress.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.10675054043531418,
                    "sentence": "While the novelty is limited, accepting the paper will help others build on the state-of-the-art results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11598634719848633,
                    "sentence": "This paper proposes to combine residual like architecture with multimodal fusion for visual question answering.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.112481489777565,
                    "sentence": "A particular novelty is the usage of multiplicative interactions to combine visual features with word embeddings.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13256725668907166,
                    "sentence": "The results are impressive, betting the state-of-the-art by a margin.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.20623177289962769,
                    "sentence": "However, this paper used many pretrained models and embeddings, so it would make the paper better if all these effects are better analyzed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21746981143951416,
                    "sentence": "It is better not not refer to equations in other papers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1642564982175827,
                    "sentence": "For example in section 3.1, it is better if the equations are reproduced in this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.22962212562561035,
                    "sentence": "Question: In section 5.2, what happens if you use sigmoid(W_q*q) as the attentional mask for visualization?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 10,
                    "completely_generated_prob": 5.7802509961791254e-09
                }
            ],
            "completely_generated_prob": 0.06633396618935229,
            "class_probabilities": {
                "human": 0.9336248632755252,
                "ai": 0.06633396618935229,
                "mixed": 4.117053512248776e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9336248632755252,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.06633396618935229,
                    "human": 0.9336248632755252,
                    "mixed": 4.117053512248776e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper combines the residual like network with multimodal fusion of language and visual features for Visual QA task. Impressive results on standard benchmarks show progress. While the novelty is limited, accepting the paper will help others build on the state-of-the-art results. This paper proposes to combine residual like architecture with multimodal fusion for visual question answering. A particular novelty is the usage of multiplicative interactions to combine visual features with word embeddings. The results are impressive, betting the state-of-the-art by a margin. However, this paper used many pretrained models and embeddings, so it would make the paper better if all these effects are better analyzed. It is better not not refer to equations in other papers. For example in section 3.1, it is better if the equations are reproduced in this paper. Question: In section 5.2, what happens if you use sigmoid(W_q*q) as the attentional mask for visualization?"
        }
    ]
}