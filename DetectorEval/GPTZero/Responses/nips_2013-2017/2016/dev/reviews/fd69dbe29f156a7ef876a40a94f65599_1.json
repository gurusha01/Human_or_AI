{
    "version": "2025-01-09-base",
    "scanId": "ce14cd27-1a00-4c8c-939c-1a97a6108d56",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.1000608280301094,
                    "sentence": "The paper proposes multiple reasoning layers through which question representations are iteratively updated based on the image.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1299993395805359,
                    "sentence": "A reasoning layer consists of two parts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13951319456100464,
                    "sentence": "The first part is called Question-Image interaction which is a multilayer perceptron that takes in the previous layer's question representation and the image representation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18017913401126862,
                    "sentence": "The second part is weighted pooling where the updated representations from the Question-Image interaction are summed based on the attention weights learned through backpropagation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17553474009037018,
                    "sentence": "The model utilizes object proposals to generate candidate image regions and these image regions are encoded using a convolutional neural network.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1955167055130005,
                    "sentence": "The encoded image features interact with the question representations and a soft attention mechanism is applied to generate attention distribution over image regions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2687641680240631,
                    "sentence": "Strength: The technical contributions are a clever and simple extension/combination of existing ideas such as \"Neural Reasoner\" [B. Peng, Z. Lu, H. Li, and K.-F. Wong.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3560066819190979,
                    "sentence": "Towards neural network-based reasoning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.35257434844970703,
                    "sentence": "arXiv preprint 287 arXiv:1508.05508, 2015], spatial coordinates [R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.30282506346702576,
                    "sentence": "Natural language object retrieval.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.28787851333618164,
                    "sentence": "arXiv 265 preprint arXiv:1511.04164, 2015], and soft attention mechanism [K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.24578432738780975,
                    "sentence": "Show, attend and tell: 307 Neural image caption generation with visual attention.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2150053083896637,
                    "sentence": "arXiv preprint arXiv:1502.03044, 2015].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.14765664935112,
                    "sentence": "The paper is well-written and easy to follow, especially the architecture of the model and the explanations for it are modular and simple (image understanding layer, question encoding layer, reasoning layer, and answering layer).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12257865071296692,
                    "sentence": "Haven't yet encountered a VQA system that changes the question representation based on image.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.16668641567230225,
                    "sentence": "This novelty adds strength to this paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11115456372499466,
                    "sentence": "Weaknesses: In section 3.1, the paper states that adding an 8D representation remedies the problem of lacking spatial information for object location.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1616193950176239,
                    "sentence": "Is there an ablation done to compare the performances of the models with these spatial coordinates and without?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13195322453975677,
                    "sentence": "Without any experiment results, it is hard to tell if these extra spatial coordinates are actually helping the model understand the spatial relationship between object proposals.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12214591354131699,
                    "sentence": "If these spatial coordinates turn out not to be much helpful, then the model might still suffer from the same issue, which seems like a major drawback.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11502501368522644,
                    "sentence": "In section 4.3, it is analyzed that (starting line 209), compared to SAN, the proposed model deals only with selected object proposal regions that have high probability to be an object, and thus gives better results when answering questions involving objects.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.14023776352405548,
                    "sentence": "Also in line 216, it is stated that a similar pattern can be observed in the VQA dataset, having a prominent improvement in the Other type.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08745774626731873,
                    "sentence": "However, the results in Table 3 do not seem to be supporting this claim.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.19169385731220245,
                    "sentence": "Compared with SAN, the proposed model only performs better in Yes/No question and actually performs worse in Other type than SAN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1744173914194107,
                    "sentence": "The results and analysis seem to be inconsistent and it is difficult to see a concrete / quantitative evidence that supports the strength of the model, namely the capability of answering Other types.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15263082087039948,
                    "sentence": "Overall, the proposed model seems to be an extension / application of \"Neural Reasoner\" [B. Peng, Z. Lu, H. Li, and K.-F. Wong.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1827394664287567,
                    "sentence": "Towards neural network-based reasoning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3069995045661926,
                    "sentence": "arXiv preprint 287 arXiv:1508.05508, 2015] using images and attention mechanism, thereby weakening the novelty of the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2182430922985077,
                    "sentence": "=== post rebuttal === The improved performance mentioned in the rebuttal is good, but it does not show any additional strength of the model, as other models should improve similarly with fine-tuning the visual representation (I assume that is what \"better results by fine-tuning the initial learning.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3078485131263733,
                    "sentence": "means).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1293175369501114,
                    "sentence": "I think the two contributions stated in lines 43-46 are limited: 1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1228264570236206,
                    "sentence": "Iteratively updating the question representation is similar to [21], but applied to images.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08014929294586182,
                    "sentence": "Any ablations showing benefit of this idea e.g.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1499621868133545,
                    "sentence": "against a SAN [33] architecture / a single or double attention on the image features is not shown, so it remains unclear how benefitial that is.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21003173291683197,
                    "sentence": "A comparison to related work is not sufficient as it is unclear if the difference is due to the experimental setup or actually the different architecture, especially as the difference in performance is small.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.18994945287704468,
                    "sentence": "2. The use of object proposals is not novel as done already: [23].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3594244420528412,
                    "sentence": "Thus the limited novelty combined with limited improvements over related work, and no ablation experiments lead me to the recommendation to reject the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 37,
                    "completely_generated_prob": 8.44453234384748e-33
                }
            ],
            "completely_generated_prob": 0.07286165202115429,
            "class_probabilities": {
                "human": 0.9268225467824988,
                "ai": 0.07286165202115429,
                "mixed": 0.0003158011963468892
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9268225467824988,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.07286165202115429,
                    "human": 0.9268225467824988,
                    "mixed": 0.0003158011963468892
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper proposes multiple reasoning layers through which question representations are iteratively updated based on the image. A reasoning layer consists of two parts. The first part is called Question-Image interaction which is a multilayer perceptron that takes in the previous layer's question representation and the image representation. The second part is weighted pooling where the updated representations from the Question-Image interaction are summed based on the attention weights learned through backpropagation. The model utilizes object proposals to generate candidate image regions and these image regions are encoded using a convolutional neural network. The encoded image features interact with the question representations and a soft attention mechanism is applied to generate attention distribution over image regions. Strength: The technical contributions are a clever and simple extension/combination of existing ideas such as \"Neural Reasoner\" [B. Peng, Z. Lu, H. Li, and K.-F. Wong. Towards neural network-based reasoning. arXiv preprint 287 arXiv:1508.05508, 2015], spatial coordinates [R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell. Natural language object retrieval. arXiv 265 preprint arXiv:1511.04164, 2015], and soft attention mechanism [K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: 307 Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015]. The paper is well-written and easy to follow, especially the architecture of the model and the explanations for it are modular and simple (image understanding layer, question encoding layer, reasoning layer, and answering layer). Haven't yet encountered a VQA system that changes the question representation based on image. This novelty adds strength to this paper. Weaknesses: In section 3.1, the paper states that adding an 8D representation remedies the problem of lacking spatial information for object location. Is there an ablation done to compare the performances of the models with these spatial coordinates and without? Without any experiment results, it is hard to tell if these extra spatial coordinates are actually helping the model understand the spatial relationship between object proposals. If these spatial coordinates turn out not to be much helpful, then the model might still suffer from the same issue, which seems like a major drawback. In section 4.3, it is analyzed that (starting line 209), compared to SAN, the proposed model deals only with selected object proposal regions that have high probability to be an object, and thus gives better results when answering questions involving objects. Also in line 216, it is stated that a similar pattern can be observed in the VQA dataset, having a prominent improvement in the Other type. However, the results in Table 3 do not seem to be supporting this claim. Compared with SAN, the proposed model only performs better in Yes/No question and actually performs worse in Other type than SAN. The results and analysis seem to be inconsistent and it is difficult to see a concrete / quantitative evidence that supports the strength of the model, namely the capability of answering Other types. Overall, the proposed model seems to be an extension / application of \"Neural Reasoner\" [B. Peng, Z. Lu, H. Li, and K.-F. Wong. Towards neural network-based reasoning. arXiv preprint 287 arXiv:1508.05508, 2015] using images and attention mechanism, thereby weakening the novelty of the approach. === post rebuttal === The improved performance mentioned in the rebuttal is good, but it does not show any additional strength of the model, as other models should improve similarly with fine-tuning the visual representation (I assume that is what \"better results by fine-tuning the initial learning.\" means). I think the two contributions stated in lines 43-46 are limited: 1. Iteratively updating the question representation is similar to [21], but applied to images. Any ablations showing benefit of this idea e.g. against a SAN [33] architecture / a single or double attention on the image features is not shown, so it remains unclear how benefitial that is. A comparison to related work is not sufficient as it is unclear if the difference is due to the experimental setup or actually the different architecture, especially as the difference in performance is small. 2. The use of object proposals is not novel as done already: [23]. Thus the limited novelty combined with limited improvements over related work, and no ablation experiments lead me to the recommendation to reject the paper."
        }
    ]
}