{
    "version": "2025-01-09-base",
    "scanId": "1c8dc761-af26-4409-a05c-8591b257379d",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.4945733845233917,
                    "sentence": "The authors have developed an end-to-end system for parsing hand written text from paragraph (multiple horizontal lines) images.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.48077815771102905,
                    "sentence": "They achieve this by adding an attention mechanism to the single-line parser of Graves et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5331260561943054,
                    "sentence": "[18] and then decoding the text using Bidirectional-LSTMs (instead of softmax as used originally by Graves et al).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5198042988777161,
                    "sentence": "They evaluate recognition performance on the Rimes and IAM datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5486501455307007,
                    "sentence": "The performance is comparable to the state-of-the-art methods on Rimes, but lags behind on IAM.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.466324120759964,
                    "sentence": "This is a very nice application of guided attention over an image for identifying a sequence of handwritten text lines (as the authors correctly point out, this builds on the approach of Xu et al [38] and others, who have used region attention over an image).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5087620615959167,
                    "sentence": "On the exposition, the paper could be written to be more self-contained.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.579405665397644,
                    "sentence": "It is difficult to understand the model as presented in the paper without first referring to Graves et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04510151222348213,
                    "sentence": "[18].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4116304814815521,
                    "sentence": "It would help to improve section 3 if the MD-LSTM model is explained in more detail (as in [18]).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.20411840081214905,
                    "sentence": "There are a few peculiar results and statements that should be explained by the authors: - why does the proposed approach give better performance than the methods that use ground truth line labelling (in table 2 & section 5.3)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.12501247227191925,
                    "sentence": "Is it because the line breaks in the ground truth are used to re-initialize the decoder RNN, but this re-initialization does not happen in the character sequence of the attention mechanism?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11603645980358124,
                    "sentence": "More details on the authors' implementation of Graves et al should be provided to answer this question.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1027183085680008,
                    "sentence": "- line 142, the authors say their method is 20-30x faster than the character decoder method of Bluche et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06345103681087494,
                    "sentence": "[6], but do not provide the actual time for comparison.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06905752420425415,
                    "sentence": "- In terms of results, the performance is superior to the state of the art (for one setting) for CER% on the Rimes dataset, but substantially worse on the IAM dataset, probably due to lack of training data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0972137451171875,
                    "sentence": "A simple experiment to test this would be to first train the model on the test+training data of the other dataset and then finetune on the IAM training data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07043387740850449,
                    "sentence": "I suggest the authors add this experiment.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0746571496129036,
                    "sentence": "- line 213, why does the attention mechanism miss punctuation?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 19,
                    "completely_generated_prob": 7.952272508076387e-17
                }
            ],
            "completely_generated_prob": 0.09735685931658823,
            "class_probabilities": {
                "human": 0.9025971025412087,
                "ai": 0.09735685931658823,
                "mixed": 4.603814220295465e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9025971025412087,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.09735685931658823,
                    "human": 0.9025971025412087,
                    "mixed": 4.603814220295465e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The authors have developed an end-to-end system for parsing hand written text from paragraph (multiple horizontal lines) images. They achieve this by adding an attention mechanism to the single-line parser of Graves et al. [18] and then decoding the text using Bidirectional-LSTMs (instead of softmax as used originally by Graves et al). They evaluate recognition performance on the Rimes and IAM datasets. The performance is comparable to the state-of-the-art methods on Rimes, but lags behind on IAM. This is a very nice application of guided attention over an image for identifying a sequence of handwritten text lines (as the authors correctly point out, this builds on the approach of Xu et al [38] and others, who have used region attention over an image). On the exposition, the paper could be written to be more self-contained. It is difficult to understand the model as presented in the paper without first referring to Graves et al. [18]. It would help to improve section 3 if the MD-LSTM model is explained in more detail (as in [18]). There are a few peculiar results and statements that should be explained by the authors: - why does the proposed approach give better performance than the methods that use ground truth line labelling (in table 2 & section 5.3)? Is it because the line breaks in the ground truth are used to re-initialize the decoder RNN, but this re-initialization does not happen in the character sequence of the attention mechanism? More details on the authors' implementation of Graves et al should be provided to answer this question. - line 142, the authors say their method is 20-30x faster than the character decoder method of Bluche et al. [6], but do not provide the actual time for comparison. - In terms of results, the performance is superior to the state of the art (for one setting) for CER% on the Rimes dataset, but substantially worse on the IAM dataset, probably due to lack of training data. A simple experiment to test this would be to first train the model on the test+training data of the other dataset and then finetune on the IAM training data. I suggest the authors add this experiment. - line 213, why does the attention mechanism miss punctuation?"
        }
    ]
}