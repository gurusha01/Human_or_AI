{
    "version": "2025-01-09-base",
    "scanId": "620b1231-a9fa-457a-abfc-abb9f45cf511",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.25854453444480896,
                    "sentence": "Arguing that graphical models and neural networks have complementary strengths, the authors introduce a family of probabilistic models that combine structured prior distributions formulated as graphical models with highly nonlinear observation models implemented using neural networks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.21362191438674927,
                    "sentence": "The goal is to combine the interpretability and efficient inference algorithms of graphical models with the representation learning power of neural nets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2511278986930847,
                    "sentence": "The main contribution of the paper is an efficient stochastic variational inference algorithm for training such hybrid models that uses a recognition model, implemented a neural network, to deal with the non-conjugate observation model to enable efficient mean field updates for inferring the local latent variables.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.30859607458114624,
                    "sentence": "The experimental section is minimal, showing qualitative results on a synthetic dataset and a small dataset of low-resolution video.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.28277429938316345,
                    "sentence": "The idea of using a neural recognition model to implement conjugacy in a surrogate objective used to infer the local latent variables seems like a powerful and important one.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2017132043838501,
                    "sentence": "At the same time it is less direct than taking the variational autoencoder (VAE) route and using the recognition model to produce the parameters of the variational posterior directly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.22195221483707428,
                    "sentence": "It would have been good to contrast the two approaches and discuss their relative strengths and weaknesses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1364404857158661,
                    "sentence": "I found the paper to be well-written overall, though it was too dense and not detailed enough in parts.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1675477921962738,
                    "sentence": "The supplementary material was essential for me to understand some details unclear from the main text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1658327877521515,
                    "sentence": "Though the experimental section is very bare-bones and lacks quantitative results, I do not think that is a serious weakness for a paper with a substantial conceptual contribution like this one.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2655353248119354,
                    "sentence": "The related work section missed several recent papers on sequence modeling in the VAE framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.27931836247444153,
                    "sentence": "Another notable omission was the work of Titsias and Lazaro-Gredilla [1].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.39490917325019836,
                    "sentence": "References [1] M. K. Titsias and M. Lazaro-Gredilla.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.749663770198822,
                    "sentence": "Doubly Stochastic Variational Bayes for non-Conjugate Inference, ICML, 2014",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 14,
                    "completely_generated_prob": 1.9423489116273474e-12
                }
            ],
            "completely_generated_prob": 0.22760053783156092,
            "class_probabilities": {
                "human": 0.7710548832661043,
                "ai": 0.22760053783156092,
                "mixed": 0.0013445789023346778
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.7710548832661043,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.22760053783156092,
                    "human": 0.7710548832661043,
                    "mixed": 0.0013445789023346778
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Arguing that graphical models and neural networks have complementary strengths, the authors introduce a family of probabilistic models that combine structured prior distributions formulated as graphical models with highly nonlinear observation models implemented using neural networks. The goal is to combine the interpretability and efficient inference algorithms of graphical models with the representation learning power of neural nets. The main contribution of the paper is an efficient stochastic variational inference algorithm for training such hybrid models that uses a recognition model, implemented a neural network, to deal with the non-conjugate observation model to enable efficient mean field updates for inferring the local latent variables. The experimental section is minimal, showing qualitative results on a synthetic dataset and a small dataset of low-resolution video. The idea of using a neural recognition model to implement conjugacy in a surrogate objective used to infer the local latent variables seems like a powerful and important one. At the same time it is less direct than taking the variational autoencoder (VAE) route and using the recognition model to produce the parameters of the variational posterior directly. It would have been good to contrast the two approaches and discuss their relative strengths and weaknesses. I found the paper to be well-written overall, though it was too dense and not detailed enough in parts. The supplementary material was essential for me to understand some details unclear from the main text. Though the experimental section is very bare-bones and lacks quantitative results, I do not think that is a serious weakness for a paper with a substantial conceptual contribution like this one. The related work section missed several recent papers on sequence modeling in the VAE framework. Another notable omission was the work of Titsias and Lazaro-Gredilla [1]. References [1] M. K. Titsias and M. Lazaro-Gredilla. Doubly Stochastic Variational Bayes for non-Conjugate Inference, ICML, 2014"
        }
    ]
}