{
    "version": "2025-01-09-base",
    "scanId": "0aaf5fba-c999-4e91-8eb0-109aa893cf6b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.026416078209877014,
                    "sentence": "This paper applies Richardson-Romberg extrapolation (RRE) to recent \"big-data MCMC\" methods, with a focus on Stochastic Gradient Langevin Dynamics (SGLD).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021399030461907387,
                    "sentence": "RRE is a standard numerical integration tool that has recently seen some nice applications to numerical stochastic differential equations (SDEs); one of the nice features of this paper is that it provides a link between the MCMC literature and the SDE literature, which may be beneficial for both communities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02857835218310356,
                    "sentence": "The authors introduce a new SGLD-RRE method, develop some theoretical results on the convergence of the new algorithm (along with arguments for when SGLD-RRE improves on vanilla SGLD), and show some numerical results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03569284826517105,
                    "sentence": "They obtain impressive improvements on toy numerical examples, and modest but consistent improvements in large-scale examples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03145022317767143,
                    "sentence": "Overall it's a solid paper, clearly written.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.025722790509462357,
                    "sentence": "As I said above, this is a solid paper, with interesting links between two somewhat disconnected fields; interesting theoretical results; and good empirical performance of the novel proposed algorithm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03438587486743927,
                    "sentence": "The paper is very clear and easy to read and seems technically sound.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03742273896932602,
                    "sentence": "The method ends up being a fairly straightforward application of RRE to SGLD, which is computationally convenient.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.028785765171051025,
                    "sentence": "I believe practitioners will use these methods, though the methods will not revolutionize current practice.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02857835777103901,
                    "sentence": "Minor comments: I found Fig 1a slightly misleading - this is not the empirical estimated posterior distribution here, but rather a gaussian with the empirical posterior mean and variance plugged in.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04390684515237808,
                    "sentence": "This should be clarified / emphasized a bit more for the unwary reader.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.044342875480651855,
                    "sentence": "Fig 2a \"\" it would be helpful to add a bit of discussion about why the bias is not necessarily decreasing as the stepsize decreases; this is a bit counter-intuitive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 12,
                    "completely_generated_prob": 1.0731758790825431e-10
                }
            ],
            "completely_generated_prob": 0.039837804045504216,
            "class_probabilities": {
                "human": 0.9601216422360306,
                "ai": 0.039837804045504216,
                "mixed": 4.055371846526579e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9601216422360306,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.039837804045504216,
                    "human": 0.9601216422360306,
                    "mixed": 4.055371846526579e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper applies Richardson-Romberg extrapolation (RRE) to recent \"big-data MCMC\" methods, with a focus on Stochastic Gradient Langevin Dynamics (SGLD). RRE is a standard numerical integration tool that has recently seen some nice applications to numerical stochastic differential equations (SDEs); one of the nice features of this paper is that it provides a link between the MCMC literature and the SDE literature, which may be beneficial for both communities. The authors introduce a new SGLD-RRE method, develop some theoretical results on the convergence of the new algorithm (along with arguments for when SGLD-RRE improves on vanilla SGLD), and show some numerical results. They obtain impressive improvements on toy numerical examples, and modest but consistent improvements in large-scale examples. Overall it's a solid paper, clearly written. As I said above, this is a solid paper, with interesting links between two somewhat disconnected fields; interesting theoretical results; and good empirical performance of the novel proposed algorithm. The paper is very clear and easy to read and seems technically sound. The method ends up being a fairly straightforward application of RRE to SGLD, which is computationally convenient. I believe practitioners will use these methods, though the methods will not revolutionize current practice. Minor comments: I found Fig 1a slightly misleading - this is not the empirical estimated posterior distribution here, but rather a gaussian with the empirical posterior mean and variance plugged in. This should be clarified / emphasized a bit more for the unwary reader. Fig 2a \"\" it would be helpful to add a bit of discussion about why the bias is not necessarily decreasing as the stepsize decreases; this is a bit counter-intuitive."
        }
    ]
}