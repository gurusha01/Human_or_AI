{
    "version": "2025-01-09-base",
    "scanId": "5486aef7-9077-44be-9bf3-29df8978845c",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.921531617641449,
                    "sentence": "This paper is about batch-sequential Bayesian Optimization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8957646489143372,
                    "sentence": "A batch-sequential version of the Knowldege Gradient criterion is introduced.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.915787935256958,
                    "sentence": "After some background on related work and Gaussian processes, the parallel (q-KG) criterion is defined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9392566084861755,
                    "sentence": "Its computation is detailed, following the route of the standard KG criterion, and a number of numerical experiments are presented where q-KG appears to outperform some state-of-the-art batch-sequential Bayesian Optimization algorithms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8928705453872681,
                    "sentence": "This is a very good paper and I think that it possesses important qualities that would justify its publication in NIPS.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9118053317070007,
                    "sentence": "The potential impact on society of parallelizing Bayesian optimization algorithms is paramount.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9262154698371887,
                    "sentence": "A minor criticism is that speed-ups (from sequential to batch-sequential) are not studied, and also a few statements are slightly imprecise in the literature review; for instance the idea of integrating EI with respect to posterior distributions was already present in \"Kriging is well-suited to parallelize optimization\" (along with Constant Liar), while Chevalier et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9663011431694031,
                    "sentence": "presented notably CL-mix in [2].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9553432464599609,
                    "sentence": "Also q-EI maximization using natural gradient is studied in \"Differentiating the multipoint Expected Improvement for optimal batch design\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9163175225257874,
                    "sentence": "Finally I have the following remarks and questions: Is A compact, f continuous?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9807987809181213,
                    "sentence": "Why restrict A to be an LHS at the initial stage?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9808920621871948,
                    "sentence": "In Algorithm 1: what about hyperparameter re-estimation (or Bayesian updating)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9844966530799866,
                    "sentence": "In 5.2: is it a max or a min in the definition of g?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9661686420440674,
                    "sentence": "About 5.2 again: the smoothness of g doesn't seem to go without saying.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9787440896034241,
                    "sentence": "Similarly, the derivative of the Cholesky factor might be non obvious.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9747467637062073,
                    "sentence": "MatÃ©rn is 5/2 and not 2/5 I guess About Figure 1 and the corresponding experiments: was it possible to fit the same GPs using different softwares?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 16,
                    "completely_generated_prob": 0.9538361662901652
                }
            ],
            "completely_generated_prob": 0.8944041024838034,
            "class_probabilities": {
                "human": 0.10559589751619658,
                "ai": 0.8944041024838034,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8944041024838034,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8944041024838034,
                    "human": 0.10559589751619658,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "ai_paraphrased",
                    "result_message": "We are highly confident that this text has been rewritten by AI, an AI paraphraser or AI bypasser",
                    "confidence_score": 0.9949650940794934,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.005034905920506572,
                        "ai_paraphrased": 0.9949650940794934
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.0050349058205065725,
                            "ai_paraphrased": 0.9949650940794934
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper is about batch-sequential Bayesian Optimization. A batch-sequential version of the Knowldege Gradient criterion is introduced. After some background on related work and Gaussian processes, the parallel (q-KG) criterion is defined. Its computation is detailed, following the route of the standard KG criterion, and a number of numerical experiments are presented where q-KG appears to outperform some state-of-the-art batch-sequential Bayesian Optimization algorithms. This is a very good paper and I think that it possesses important qualities that would justify its publication in NIPS. The potential impact on society of parallelizing Bayesian optimization algorithms is paramount. A minor criticism is that speed-ups (from sequential to batch-sequential) are not studied, and also a few statements are slightly imprecise in the literature review; for instance the idea of integrating EI with respect to posterior distributions was already present in \"Kriging is well-suited to parallelize optimization\" (along with Constant Liar), while Chevalier et al. presented notably CL-mix in [2]. Also q-EI maximization using natural gradient is studied in \"Differentiating the multipoint Expected Improvement for optimal batch design\". Finally I have the following remarks and questions: Is A compact, f continuous? Why restrict A to be an LHS at the initial stage ? In Algorithm 1: what about hyperparameter re-estimation (or Bayesian updating)? In 5.2: is it a max or a min in the definition of g? About 5.2 again: the smoothness of g doesn't seem to go without saying. Similarly, the derivative of the Cholesky factor might be non obvious. MatÃ©rn is 5/2 and not 2/5 I guess About Figure 1 and the corresponding experiments: was it possible to fit the same GPs using different softwares?"
        }
    ]
}