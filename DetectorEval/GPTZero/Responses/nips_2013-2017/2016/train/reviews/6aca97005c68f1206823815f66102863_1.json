{
    "version": "2025-01-09-base",
    "scanId": "bd3eae53-d04a-4a34-89d0-e849aaf116b3",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.005782450549304485,
                    "sentence": "The paper proposes to replace standard Monte Carlo methods for ABC with a method based on Bayesian density estimation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0071642124094069,
                    "sentence": "Although density estimation has been used before within the context of ABC, this approach allows for the direct replacement of sample-based approximations of the posterior with an analytic approximation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005588246043771505,
                    "sentence": "The novelty is similar in scope that of ABC with variational inference [1], but the approach discussed here is quite different.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.029435623437166214,
                    "sentence": "[1] Tran, Nott and Kohn, 2015, \"Variational Bayes with Intractable Likelihood\" The paper is actually very well written and easy to understand.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00821264460682869,
                    "sentence": "The level of technical writing is sufficient for the expert, while eliding unnecessary details.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00838402844965458,
                    "sentence": "While paper heavily builds upon previous work, the key idea of proposition 1 is used elegantly throughout, both to choose the proposal prior and to estimate the posterior approximation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009282749146223068,
                    "sentence": "In addition, there are other moderate novel but useful contributions sprinkled throughout the paper, such as the extension of MDN to SVI.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00729144923388958,
                    "sentence": "However, the authors must also discuss other work in the use of SVI with ABC, such as for example [1].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009150346741080284,
                    "sentence": "The paper lacks a firm theoretical underpinning, apart from the asymptotic motivation that Proposition 1 provides to the proposed algorithm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01055042166262865,
                    "sentence": "However, I believe that this is more than sufficient for this type of paper, and I do not count that as a negative, especially given the NIPS format [I doubt that an explanation of the model, experiments as well heavy theory could fit in the eight pages provided].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008892131969332695,
                    "sentence": "The experimental results are a good mix of simple examples and larger datasets, and are clearly presented.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013924984261393547,
                    "sentence": "I also like how the authors disentangle the effect of selecting the proposal distribution from the posterior estimation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008601725101470947,
                    "sentence": "The plots are trying to take the taking effective sample size into account, but I am not sure that this is the best metric.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011863036081194878,
                    "sentence": "After all, samples are purely computational beasts in this setting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013536953367292881,
                    "sentence": "Wouldn't it make more sense to measure actual CPU time?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 15,
                    "completely_generated_prob": 2.5938383721896624e-13
                }
            ],
            "completely_generated_prob": 0.021601982568463345,
            "class_probabilities": {
                "human": 0.9783576840560043,
                "ai": 0.021601982568463345,
                "mixed": 4.0333375532255976e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9783576840560043,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.021601982568463345,
                    "human": 0.9783576840560043,
                    "mixed": 4.0333375532255976e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper proposes to replace standard Monte Carlo methods for ABC with a method based on Bayesian density estimation. Although density estimation has been used before within the context of ABC, this approach allows for the direct replacement of sample-based approximations of the posterior with an analytic approximation. The novelty is similar in scope that of ABC with variational inference [1], but the approach discussed here is quite different. [1] Tran, Nott and Kohn, 2015, \"Variational Bayes with Intractable Likelihood\" The paper is actually very well written and easy to understand. The level of technical writing is sufficient for the expert, while eliding unnecessary details. While paper heavily builds upon previous work, the key idea of proposition 1 is used elegantly throughout, both to choose the proposal prior and to estimate the posterior approximation. In addition, there are other moderate novel but useful contributions sprinkled throughout the paper, such as the extension of MDN to SVI. However, the authors must also discuss other work in the use of SVI with ABC, such as for example [1]. The paper lacks a firm theoretical underpinning, apart from the asymptotic motivation that Proposition 1 provides to the proposed algorithm. However, I believe that this is more than sufficient for this type of paper, and I do not count that as a negative, especially given the NIPS format [I doubt that an explanation of the model, experiments as well heavy theory could fit in the eight pages provided]. The experimental results are a good mix of simple examples and larger datasets, and are clearly presented. I also like how the authors disentangle the effect of selecting the proposal distribution from the posterior estimation. The plots are trying to take the taking effective sample size into account, but I am not sure that this is the best metric. After all, samples are purely computational beasts in this setting. Wouldn't it make more sense to measure actual CPU time?"
        }
    ]
}