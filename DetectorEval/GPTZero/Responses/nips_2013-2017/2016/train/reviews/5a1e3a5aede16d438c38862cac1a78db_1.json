{
    "version": "2025-01-09-base",
    "scanId": "044595bc-9327-4573-a331-a1ff516a8f48",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0004356290155556053,
                    "sentence": "This paper analyzes solutions to the optimization problem min.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0004040276980958879,
                    "sentence": "\"\"x\"\"_1 s.t.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0004323378670960665,
                    "sentence": "\"\"\\Phi x - y\"\" \\le \\tau where the second norm is either the L1 or L-infinity norm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0006544084171764553,
                    "sentence": "The authors give some dual characterization of the solution sets, proving that recovery of the optimal x's structure is sometimes feasible.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0001539499353384599,
                    "sentence": "Given the crowded space in which this paper falls---there have been 100s (1000s?)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00031773955561220646,
                    "sentence": "of papers on sparse recovery in linear systems---it is imperative to carefully situate the paper and justify it in the context of NIPS.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0003816950193140656,
                    "sentence": "Somehow, I do not feel that this paper has really done that.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00034881659667007625,
                    "sentence": "The proof techniques are claimed to be novel--that's fine, but at some level they amount to constructing a dual vector that optimizes the problem under some noise conditions on w (where y = \\Phi x + w), arguing that sign recovery/structure recovery is then possible.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.000264764268649742,
                    "sentence": "Of course, the dual witness technique has a long history in analysis of convex relaxations of sparsity problems, dating to Donoho, Candes, Tao, and Wainwright, so it is not clear that this really is particularly new.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00021774233027826995,
                    "sentence": "The experimental evaluation is a bit lacking as well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00020070793107151985,
                    "sentence": "What is the experiment actually being run?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00032652763184159994,
                    "sentence": "Is there noise in the observation of y?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00035763124469667673,
                    "sentence": "What noise distribution on w is being used?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0002797311171889305,
                    "sentence": "My understanding of the justification for this paper was that variant noise distributions for w would change the analysis slightly (fine).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0003214501484762877,
                    "sentence": "But the experiments do not seem to study this...",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0003848056367132813,
                    "sentence": "Moreover, Figure 2 seems to suggest that the classical L2 loss techniques are the best.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0003097473527304828,
                    "sentence": "Is this a consequence of the noise distribution used in the experiments?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0005666740471497178,
                    "sentence": "Is it because using L1 or L-infinity is actually not a good idea?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0006265682750381529,
                    "sentence": "I am not sure what to take home from the experiments; they seem to provide little explanatory evidence either way.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 19,
                    "completely_generated_prob": 7.952272508076387e-17
                }
            ],
            "completely_generated_prob": 0.006608679833334143,
            "class_probabilities": {
                "human": 0.9933913201666659,
                "ai": 0.006608679833334143,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9933913201666659,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.006608679833334143,
                    "human": 0.9933913201666659,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper analyzes solutions to the optimization problem min. \"\"x\"\"_1 s.t. \"\"\\Phi x - y\"\" \\le \\tau where the second norm is either the L1 or L-infinity norm. The authors give some dual characterization of the solution sets, proving that recovery of the optimal x's structure is sometimes feasible. Given the crowded space in which this paper falls---there have been 100s (1000s?) of papers on sparse recovery in linear systems---it is imperative to carefully situate the paper and justify it in the context of NIPS. Somehow, I do not feel that this paper has really done that. The proof techniques are claimed to be novel--that's fine, but at some level they amount to constructing a dual vector that optimizes the problem under some noise conditions on w (where y = \\Phi x + w), arguing that sign recovery/structure recovery is then possible. Of course, the dual witness technique has a long history in analysis of convex relaxations of sparsity problems, dating to Donoho, Candes, Tao, and Wainwright, so it is not clear that this really is particularly new. The experimental evaluation is a bit lacking as well. What is the experiment actually being run? Is there noise in the observation of y? What noise distribution on w is being used? My understanding of the justification for this paper was that variant noise distributions for w would change the analysis slightly (fine). But the experiments do not seem to study this... Moreover, Figure 2 seems to suggest that the classical L2 loss techniques are the best. Is this a consequence of the noise distribution used in the experiments? Is it because using L1 or L-infinity is actually not a good idea? I am not sure what to take home from the experiments; they seem to provide little explanatory evidence either way."
        }
    ]
}