{
    "version": "2025-01-09-base",
    "scanId": "914497ee-db31-489f-bf59-1a3bb2bce2a5",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999861121177673,
                    "sentence": "The paper introduces Conditional Generative Moment-Matching Networks (CGMMN), a novel extension of Generative Moment-Matching Networks (GMMN) that models conditional distributions using a Conditional Maximum Mean Discrepancy (CMMD) criterion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999992311000824,
                    "sentence": "The authors propose a deep generative architecture that combines input variables with auxiliary stochastic variables, enabling the generation of samples conditioned on input data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999958276748657,
                    "sentence": "The paper evaluates CGMMN across tasks such as predictive modeling, contextual generation, and Bayesian dark knowledge distillation, demonstrating competitive performance compared to state-of-the-art methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999962449073792,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999814033508301,
                    "sentence": "1. Novelty and Originality: The paper extends the GMMN framework to conditional distributions, filling a gap in the literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999602437019348,
                    "sentence": "While conditional generative models like Conditional GANs and Conditional VAEs exist, CGMMN offers a simpler training process using CMMD, avoiding the complexities of adversarial training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999205470085144,
                    "sentence": "2. Technical Soundness: The theoretical foundations of CMMD are well-explained, with clear derivations and connections to kernel mean embeddings and MMD.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999071359634399,
                    "sentence": "The method is rigorously justified, and the empirical estimator for CMMD is practical and computationally feasible.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995975494384766,
                    "sentence": "3. Broad Applicability: The experiments cover a diverse range of tasks, including predictive modeling (MNIST, SVHN), generative modeling (MNIST, Yale Face dataset), and Bayesian dark knowledge distillation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998530149459839,
                    "sentence": "This breadth demonstrates the versatility of CGMMN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975736141204834,
                    "sentence": "4. Competitive Performance: CGMMN achieves results comparable to or better than strong baselines, such as Conditional GANs, Conditional VAEs, and max-margin DGMs, particularly in predictive and generative tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980091452598572,
                    "sentence": "5. Clarity and Organization: The paper is well-structured, with detailed explanations of the methodology, theoretical background, and experimental setup.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9968907237052917,
                    "sentence": "Figures and tables effectively illustrate the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979575872421265,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9736934304237366,
                    "sentence": "1. Limited Comparison: While CGMMN is compared to a variety of baselines, the paper does not include comparisons with Conditional GANs or Conditional VAEs in the generative tasks, which are direct competitors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9550856351852417,
                    "sentence": "2. Scalability Concerns: The use of kernel methods (e.g., CMMD) may face scalability issues for large datasets due to the cubic complexity of kernel matrix computations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8084170818328857,
                    "sentence": "Although a mini-batch version is proposed, its effectiveness on very large datasets is not thoroughly evaluated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999203622341156,
                    "sentence": "3. Generative Quality: While the generated samples are diverse, some visual artifacts and noise are present, especially in the MNIST and Yale Face experiments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991552233695984,
                    "sentence": "The paper partially addresses this by combining CGMMN with autoencoders, but further improvements in sample quality are needed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995519518852234,
                    "sentence": "4. Ablation Studies: The paper lacks detailed ablation studies to isolate the contributions of different components, such as the choice of kernel, network architecture, or the impact of auxiliary variables.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999369978904724,
                    "sentence": "Arguments for Acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999419450759888,
                    "sentence": "- The paper presents a novel and theoretically grounded approach to conditional generative modeling.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999385476112366,
                    "sentence": "- It demonstrates competitive results across diverse tasks, showcasing the broad applicability of CGMMN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998886585235596,
                    "sentence": "- The method is simpler to train compared to adversarial approaches, making it accessible for practical use.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998362064361572,
                    "sentence": "Arguments Against Acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998303055763245,
                    "sentence": "- The scalability of the method for very large datasets is unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998578429222107,
                    "sentence": "- Comparisons with other conditional generative models, such as Conditional GANs and Conditional VAEs, are missing in some experiments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997780919075012,
                    "sentence": "Recommendation:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992122650146484,
                    "sentence": "Overall, the paper makes a significant contribution to the field of deep generative modeling by extending moment-matching networks to conditional distributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988518357276917,
                    "sentence": "While there are some limitations, the strengths outweigh the weaknesses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985913038253784,
                    "sentence": "I recommend acceptance, with minor revisions to address scalability concerns and include additional comparisons with Conditional GANs and VAEs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9926183471516448,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9926183471516448,
                "mixed": 0.007381652848355174
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9926183471516448,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9926183471516448,
                    "human": 0,
                    "mixed": 0.007381652848355174
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper introduces Conditional Generative Moment-Matching Networks (CGMMN), a novel extension of Generative Moment-Matching Networks (GMMN) that models conditional distributions using a Conditional Maximum Mean Discrepancy (CMMD) criterion. The authors propose a deep generative architecture that combines input variables with auxiliary stochastic variables, enabling the generation of samples conditioned on input data. The paper evaluates CGMMN across tasks such as predictive modeling, contextual generation, and Bayesian dark knowledge distillation, demonstrating competitive performance compared to state-of-the-art methods.\nStrengths:\n1. Novelty and Originality: The paper extends the GMMN framework to conditional distributions, filling a gap in the literature. While conditional generative models like Conditional GANs and Conditional VAEs exist, CGMMN offers a simpler training process using CMMD, avoiding the complexities of adversarial training.\n2. Technical Soundness: The theoretical foundations of CMMD are well-explained, with clear derivations and connections to kernel mean embeddings and MMD. The method is rigorously justified, and the empirical estimator for CMMD is practical and computationally feasible.\n3. Broad Applicability: The experiments cover a diverse range of tasks, including predictive modeling (MNIST, SVHN), generative modeling (MNIST, Yale Face dataset), and Bayesian dark knowledge distillation. This breadth demonstrates the versatility of CGMMN.\n4. Competitive Performance: CGMMN achieves results comparable to or better than strong baselines, such as Conditional GANs, Conditional VAEs, and max-margin DGMs, particularly in predictive and generative tasks.\n5. Clarity and Organization: The paper is well-structured, with detailed explanations of the methodology, theoretical background, and experimental setup. Figures and tables effectively illustrate the results.\nWeaknesses:\n1. Limited Comparison: While CGMMN is compared to a variety of baselines, the paper does not include comparisons with Conditional GANs or Conditional VAEs in the generative tasks, which are direct competitors.\n2. Scalability Concerns: The use of kernel methods (e.g., CMMD) may face scalability issues for large datasets due to the cubic complexity of kernel matrix computations. Although a mini-batch version is proposed, its effectiveness on very large datasets is not thoroughly evaluated.\n3. Generative Quality: While the generated samples are diverse, some visual artifacts and noise are present, especially in the MNIST and Yale Face experiments. The paper partially addresses this by combining CGMMN with autoencoders, but further improvements in sample quality are needed.\n4. Ablation Studies: The paper lacks detailed ablation studies to isolate the contributions of different components, such as the choice of kernel, network architecture, or the impact of auxiliary variables.\nArguments for Acceptance:\n- The paper presents a novel and theoretically grounded approach to conditional generative modeling.\n- It demonstrates competitive results across diverse tasks, showcasing the broad applicability of CGMMN.\n- The method is simpler to train compared to adversarial approaches, making it accessible for practical use.\nArguments Against Acceptance:\n- The scalability of the method for very large datasets is unclear.\n- Comparisons with other conditional generative models, such as Conditional GANs and Conditional VAEs, are missing in some experiments.\nRecommendation:\nOverall, the paper makes a significant contribution to the field of deep generative modeling by extending moment-matching networks to conditional distributions. While there are some limitations, the strengths outweigh the weaknesses. I recommend acceptance, with minor revisions to address scalability concerns and include additional comparisons with Conditional GANs and VAEs."
        }
    ]
}