{
    "version": "2025-01-09-base",
    "scanId": "739ea380-9dbe-4525-9296-79ed3b4b111f",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9951719045639038,
                    "sentence": "The authors employ fast approximate-nearest-neighbors structures and arg-top-K operations to compute attention weights within a Neural Turing Machine.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9866366386413574,
                    "sentence": "They present experiments demonstrating that this approach does not compromise training performance and, in fact, enables training on larger-scale problems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9938902854919434,
                    "sentence": "While the novelty of this work is limited\"\"see, for instance, section 3.3 of the memory networks paper (http://arxiv.org/pdf/1410.3916.pdf)\"\"there are now multiple memory network studies that utilize memories with millions of items and hashing techniques to achieve scalable lookups.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9904188513755798,
                    "sentence": "However, I have not encountered such methods being applied to writeable memories before.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9831871390342712,
                    "sentence": "Although this paper may not represent a significant conceptual breakthrough, the practical implementation and success of these techniques are certainly noteworthy and merit publication.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9803572297096252,
                    "sentence": "Therefore, I recommend accepting the paper, albeit with some revisions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9911836385726929,
                    "sentence": "1. The paper lacks sufficient detail to allow for replication of its results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9905553460121155,
                    "sentence": "In particular, the descriptions of the tasks are insufficient.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.990553081035614,
                    "sentence": "Given that the original NTM paper's task code has not been released and there is no \"standard\" version of these tasks, it is essential for the authors to provide precise and thorough descriptions of how the tasks were constructed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9844977855682373,
                    "sentence": "Additionally, I urge the authors to commit to releasing the code for their experiments to facilitate reproducibility.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9797582626342773,
                    "sentence": "2. The paper does not adequately address the limitations of the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9859864115715027,
                    "sentence": "Under what conditions does the method fail, and does it always perform as expected?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9897540211677551,
                    "sentence": "While the authors describe their model as being smooth, it is important to note that the argmax (or K-argmax) operation is inherently not smooth.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.97977215051651,
                    "sentence": "For example, with $K=1$ in the read-only setting, their model closely resembles the MemNN-WSH model described in http://arxiv.org/pdf/1503.08895.pdf (albeit without hashing), which was reported to perform less effectively than fully smooth models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.97563236951828,
                    "sentence": "Is this discrepancy due to the $K=1$ setting or the nature of the tasks?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9753014445304871,
                    "sentence": "A deeper analysis of failure cases and a discussion of the types of tasks or setups that enable successful training would significantly enhance the paper's value.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9183469414710999,
                    "sentence": "The authors' model involves making discrete decisions during training without explicitly accounting for this (unlike the approach in http://arxiv.org/abs/1511.07275).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9084368944168091,
                    "sentence": "While the fact that they achieve results with this method is a meaningful contribution, it is important to better understand the model's limitations and the conditions under which it succeeds or fails.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9406570792198181,
                    "sentence": "Finally, I recommend the authors remove section 3.6 from the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9648374319076538,
                    "sentence": "Additionally, I suggest they refrain from labeling the omniglot dataset as \"non-synthetic\" or \"real-world.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 8,
                    "completely_generated_prob": 0.9187750751329665
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9984800378301695,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984800378301695,
                "mixed": 0.0015199621698304396
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984800378301695,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984800378301695,
                    "human": 0,
                    "mixed": 0.0015199621698304396
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The authors employ fast approximate-nearest-neighbors structures and arg-top-K operations to compute attention weights within a Neural Turing Machine. They present experiments demonstrating that this approach does not compromise training performance and, in fact, enables training on larger-scale problems. While the novelty of this work is limited\"\"see, for instance, section 3.3 of the memory networks paper (http://arxiv.org/pdf/1410.3916.pdf)\"\"there are now multiple memory network studies that utilize memories with millions of items and hashing techniques to achieve scalable lookups. However, I have not encountered such methods being applied to writeable memories before. Although this paper may not represent a significant conceptual breakthrough, the practical implementation and success of these techniques are certainly noteworthy and merit publication. Therefore, I recommend accepting the paper, albeit with some revisions.\n1. The paper lacks sufficient detail to allow for replication of its results. In particular, the descriptions of the tasks are insufficient. Given that the original NTM paper's task code has not been released and there is no \"standard\" version of these tasks, it is essential for the authors to provide precise and thorough descriptions of how the tasks were constructed. Additionally, I urge the authors to commit to releasing the code for their experiments to facilitate reproducibility.\n2. The paper does not adequately address the limitations of the proposed approach. Under what conditions does the method fail, and does it always perform as expected? While the authors describe their model as being smooth, it is important to note that the argmax (or K-argmax) operation is inherently not smooth. For example, with $K=1$ in the read-only setting, their model closely resembles the MemNN-WSH model described in http://arxiv.org/pdf/1503.08895.pdf (albeit without hashing), which was reported to perform less effectively than fully smooth models. Is this discrepancy due to the $K=1$ setting or the nature of the tasks? A deeper analysis of failure cases and a discussion of the types of tasks or setups that enable successful training would significantly enhance the paper's value. The authors' model involves making discrete decisions during training without explicitly accounting for this (unlike the approach in http://arxiv.org/abs/1511.07275). While the fact that they achieve results with this method is a meaningful contribution, it is important to better understand the model's limitations and the conditions under which it succeeds or fails.\nFinally, I recommend the authors remove section 3.6 from the paper. Additionally, I suggest they refrain from labeling the omniglot dataset as \"non-synthetic\" or \"real-world.\""
        }
    ]
}