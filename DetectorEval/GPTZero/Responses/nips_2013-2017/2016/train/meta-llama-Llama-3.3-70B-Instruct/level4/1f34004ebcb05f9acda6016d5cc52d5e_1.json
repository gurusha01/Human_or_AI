{
    "version": "2025-01-09-base",
    "scanId": "faf9bcec-65d5-40ea-b8ec-799807c4968e",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9997768402099609,
                    "sentence": "This manuscript examines the convergence properties of a non-convex loss minimization problem for a general graph-based ranking model, which is characterized by a random walk governed by node and edge weights derived from random walks based on node and edge features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998512268066406,
                    "sentence": "Since existing optimization techniques, requiring exact objective function values, are inapplicable, the proposed approach operates on two tiers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998527765274048,
                    "sentence": "Initially, a linearly convergent method is employed to approximate the stationary distribution of the Markov random walk, validated among other approaches, demonstrating that the loss function value can be approximated to any specified precision.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996451139450073,
                    "sentence": "Furthermore, the authors develop a gradient-based method for constrained non-convex optimization problems utilizing an inexact oracle and establish its convergence to a stationary point.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997841119766235,
                    "sentence": "The primary contribution lies in adapting this approach to constrained optimization problems where function values can be computed with known precision, proving its convergence, and leveraging it in the second tier of the proposed algorithm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998527765274048,
                    "sentence": "The paper is well-structured, and the proofs appear correct, although a thorough examination of all proofs was not conducted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998412132263184,
                    "sentence": "However, a concern arises as the key optimization concepts seem to build upon previous work, notably \"Yurii Nesterov and Vladimir Spokoiny, Random Gradient-Free Minimization of Convex Functions, Foundations of Computational Mathematics, 2015, pp.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997971653938293,
                    "sentence": "1\"\"40\", and the supervised PageRank algorithm has been previously proposed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999801754951477,
                    "sentence": "A minor suggestion is the inclusion of a conclusion, which would be feasible given the available space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994519352912903,
                    "sentence": "It is recommended to contextualize the optimization problem within a broader framework, positioning the studied algorithm as a specific instance, to better situate the contribution relative to the current state-of-the-art.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 10,
                    "completely_generated_prob": 0.9316904254198173
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This manuscript examines the convergence properties of a non-convex loss minimization problem for a general graph-based ranking model, which is characterized by a random walk governed by node and edge weights derived from random walks based on node and edge features. Since existing optimization techniques, requiring exact objective function values, are inapplicable, the proposed approach operates on two tiers. Initially, a linearly convergent method is employed to approximate the stationary distribution of the Markov random walk, validated among other approaches, demonstrating that the loss function value can be approximated to any specified precision. Furthermore, the authors develop a gradient-based method for constrained non-convex optimization problems utilizing an inexact oracle and establish its convergence to a stationary point. The primary contribution lies in adapting this approach to constrained optimization problems where function values can be computed with known precision, proving its convergence, and leveraging it in the second tier of the proposed algorithm. The paper is well-structured, and the proofs appear correct, although a thorough examination of all proofs was not conducted. However, a concern arises as the key optimization concepts seem to build upon previous work, notably \"Yurii Nesterov and Vladimir Spokoiny, Random Gradient-Free Minimization of Convex Functions, Foundations of Computational Mathematics, 2015, pp. 1\"\"40\", and the supervised PageRank algorithm has been previously proposed. A minor suggestion is the inclusion of a conclusion, which would be feasible given the available space. It is recommended to contextualize the optimization problem within a broader framework, positioning the studied algorithm as a specific instance, to better situate the contribution relative to the current state-of-the-art."
        }
    ]
}