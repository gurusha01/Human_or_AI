{
    "version": "2025-01-09-base",
    "scanId": "4ea0b4af-ee93-4656-afbb-278f33709285",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9959604144096375,
                    "sentence": "The manuscript proposes a novel approach to training Restricted Boltzmann Machines (RBMs) using the gamma-smoothed Wasserstein distance, denoted as $W{\\gamma}(\\hat{p}, p{\\theta})$ (equations 3 and 4), where $\\hat{p}$ represents the data distribution and $p{\\theta}$ represents the model distribution.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9947324395179749,
                    "sentence": "The authors derive the derivatives of $W{\\gamma}(\\hat{p}, p_{\\theta})$ and employ gradient descent for training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9926015734672546,
                    "sentence": "However, the observed shrinkage in the Gaussian example presented in section 4.3, as well as the results in section 4.2, raises significant concerns regarding the consistency of the Wasserstein training method, which must be thoroughly investigated before the manuscript can be considered suitable for publication.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9965685606002808,
                    "sentence": "The authors cite reference [2], which allegedly proves the statistical consistency of the method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974880814552307,
                    "sentence": "Nevertheless, the results in section 4.3, specifically the non-zero shrinkage obtained for the simple case of modeling a standard normal distribution with a normal distribution having a covariance matrix $\\sigma^2 I$, are perplexing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985529184341431,
                    "sentence": "This inconsistency is alarming, as a failure of consistency would be a severe flaw in the formulation of a statistical learning criterion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980892539024353,
                    "sentence": "In section 3, the authors discuss the stability and KL regularization of the smoothed Wasserstein objective function, stating that some regularization with respect to the KL divergence is required for learning based on samples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9938847422599792,
                    "sentence": "This regularization, however, compromises the \"purity\" of the smoothed Wasserstein objective function.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9052807092666626,
                    "sentence": "The experiments conducted in section 4, which include the MNIST-small dataset, a subset of the UCI PLANTS dataset, and 28-dimensional binary codes, yield results (reported in lines 168-172 and figures 3 and 4) that produce \"compact and contiguous regions that are prototypical of real spread, but are less diverse than the data.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9132936596870422,
                    "sentence": "These findings reinforce the concern that minimizing the smoothed Wasserstein distance may not lead to a consistent density estimator.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9181371927261353,
                    "sentence": "In section 4.4, the authors attempt to reinterpret these weaknesses as strengths in the context of data completion or denoising.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9040791988372803,
                    "sentence": "Overall, while the authors are commended for exploring an alternative to standard KL-based training of RBMs, the shrinkage observed in the Gaussian example in section 4.3 raises serious concerns regarding the consistency of the Wasserstein training method, which must be carefully addressed before the manuscript can be considered for publication.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9764161705970764,
                    "sentence": "Additional points of concern include the title, which overclaims the scope of the manuscript by only discussing training of restricted Boltzmann machines.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9855644702911377,
                    "sentence": "The introduction (line 7) incorrectly states that RBMs are restricted to binary visible units, as evidenced by the existence of Exponential Family Harmoniums.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9804953336715698,
                    "sentence": "Figure 5 would benefit from the use of different colors to represent different values of $\\gamma$, and the last entry should be $\\gamma = 0$ instead of OpenCV.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9846112132072449,
                    "sentence": "The text can be modified to explain that OpenCV was used for computations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9622511267662048,
                    "sentence": "Following the rebuttal, the charge of a \"fatal flaw\" has been reevaluated, and it is acknowledged that the small sample size in the experiments in section 4.3 may have contributed to the observed inconsistencies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994733929634094,
                    "sentence": "However, a comparison with the KL version of the experiment, where a Gaussian with a covariance matrix $\\theta^2 I$ is fitted to $n = 100$ samples drawn from a 10-dimensional Gaussian, reveals that the maximum likelihood estimator yields values of $\\theta$ within the range 0.95-1.05, whereas the Wasserstein estimator (with $\\gamma = 0$) yields a value of approximately $\\theta = 0.65$.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980802536010742,
                    "sentence": "The convergence rate of $O(n^{-1/(10+1)})$ for this simple problem is concerning and suggests that the method may not be practical.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9953487515449524,
                    "sentence": "A study investigating how the $\\theta$ that minimizes $W_0$ varies with $n$ would be beneficial in understanding the shrinkage bias.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9246899485588074,
                    "sentence": "The response to reviewer 5 regarding lines 177-178 suggests that the entropy regularizer contributes to the clustering effect, which is also observed in figure 5 for the true Wasserstein distance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9079524278640747,
                    "sentence": "This raises the question of whether a KL matching criterion with an entropy penalty (without Wasserstein) could be used to achieve similar clustering effects, as the virtues of clumping are discussed in section 4.4 in the context of data completion and denoising.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8447126150131226,
                    "sentence": "The use of supplementary materials to improve the clarity of technical explanations is recommended, as the current presentation is found to be poor.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7566421031951904,
                    "sentence": "Ultimately, while the manuscript presents interesting work, the issue of whether the clumping effect arises from the Wasserstein distance or the entropic prior needs to be more clearly articulated and dissected, and the claims regarding Wasserstein training should be carefully evaluated against a KL + entropic prior construction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.5710657228372709
                }
            ],
            "completely_generated_prob": 0.9304437370893981,
            "class_probabilities": {
                "human": 0.05461488332272685,
                "ai": 0.9304437370893981,
                "mixed": 0.014941379587875211
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9304437370893981,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9304437370893981,
                    "human": 0.05461488332272685,
                    "mixed": 0.014941379587875211
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The manuscript proposes a novel approach to training Restricted Boltzmann Machines (RBMs) using the gamma-smoothed Wasserstein distance, denoted as $W{\\gamma}(\\hat{p}, p{\\theta})$ (equations 3 and 4), where $\\hat{p}$ represents the data distribution and $p{\\theta}$ represents the model distribution. The authors derive the derivatives of $W{\\gamma}(\\hat{p}, p_{\\theta})$ and employ gradient descent for training. However, the observed shrinkage in the Gaussian example presented in section 4.3, as well as the results in section 4.2, raises significant concerns regarding the consistency of the Wasserstein training method, which must be thoroughly investigated before the manuscript can be considered suitable for publication.\nThe authors cite reference [2], which allegedly proves the statistical consistency of the method. Nevertheless, the results in section 4.3, specifically the non-zero shrinkage obtained for the simple case of modeling a standard normal distribution with a normal distribution having a covariance matrix $\\sigma^2 I$, are perplexing. This inconsistency is alarming, as a failure of consistency would be a severe flaw in the formulation of a statistical learning criterion.\nIn section 3, the authors discuss the stability and KL regularization of the smoothed Wasserstein objective function, stating that some regularization with respect to the KL divergence is required for learning based on samples. This regularization, however, compromises the \"purity\" of the smoothed Wasserstein objective function. The experiments conducted in section 4, which include the MNIST-small dataset, a subset of the UCI PLANTS dataset, and 28-dimensional binary codes, yield results (reported in lines 168-172 and figures 3 and 4) that produce \"compact and contiguous regions that are prototypical of real spread, but are less diverse than the data.\" These findings reinforce the concern that minimizing the smoothed Wasserstein distance may not lead to a consistent density estimator.\nIn section 4.4, the authors attempt to reinterpret these weaknesses as strengths in the context of data completion or denoising. Overall, while the authors are commended for exploring an alternative to standard KL-based training of RBMs, the shrinkage observed in the Gaussian example in section 4.3 raises serious concerns regarding the consistency of the Wasserstein training method, which must be carefully addressed before the manuscript can be considered for publication.\nAdditional points of concern include the title, which overclaims the scope of the manuscript by only discussing training of restricted Boltzmann machines. The introduction (line 7) incorrectly states that RBMs are restricted to binary visible units, as evidenced by the existence of Exponential Family Harmoniums. Figure 5 would benefit from the use of different colors to represent different values of $\\gamma$, and the last entry should be $\\gamma = 0$ instead of OpenCV. The text can be modified to explain that OpenCV was used for computations.\nFollowing the rebuttal, the charge of a \"fatal flaw\" has been reevaluated, and it is acknowledged that the small sample size in the experiments in section 4.3 may have contributed to the observed inconsistencies. However, a comparison with the KL version of the experiment, where a Gaussian with a covariance matrix $\\theta^2 I$ is fitted to $n = 100$ samples drawn from a 10-dimensional Gaussian, reveals that the maximum likelihood estimator yields values of $\\theta$ within the range 0.95-1.05, whereas the Wasserstein estimator (with $\\gamma = 0$) yields a value of approximately $\\theta = 0.65$. The convergence rate of $O(n^{-1/(10+1)})$ for this simple problem is concerning and suggests that the method may not be practical. A study investigating how the $\\theta$ that minimizes $W_0$ varies with $n$ would be beneficial in understanding the shrinkage bias.\nThe response to reviewer 5 regarding lines 177-178 suggests that the entropy regularizer contributes to the clustering effect, which is also observed in figure 5 for the true Wasserstein distance. This raises the question of whether a KL matching criterion with an entropy penalty (without Wasserstein) could be used to achieve similar clustering effects, as the virtues of clumping are discussed in section 4.4 in the context of data completion and denoising. The use of supplementary materials to improve the clarity of technical explanations is recommended, as the current presentation is found to be poor. Ultimately, while the manuscript presents interesting work, the issue of whether the clumping effect arises from the Wasserstein distance or the entropic prior needs to be more clearly articulated and dissected, and the claims regarding Wasserstein training should be carefully evaluated against a KL + entropic prior construction."
        }
    ]
}