{
    "version": "2025-01-09-base",
    "scanId": "0802813b-6260-4b5c-89b5-efb393ac5a3b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9994205236434937,
                    "sentence": "This manuscript tackles the issue of learning deep feature embeddings, where a major obstacle is the non-uniform density of the feature space, hindering effective hard negative mining.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994428753852844,
                    "sentence": "The authors propose a novel unit that incorporates the mean position of a pair of features, drawing inspiration from [35], alongside their difference, to compute the similarity between the feature pair.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993748664855957,
                    "sentence": "By leveraging positional information, this unit effectively accounts for the non-uniform density in the feature space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985352754592896,
                    "sentence": "Notably, the unit is differentiable and can be seamlessly integrated into existing convolutional neural networks (CNNs).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987397193908691,
                    "sentence": "A double-header hinge loss is formulated based on pairwise differences and similarities before and after the unit.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9985322952270508,
                    "sentence": "During each minibatch of stochastic gradient descent (SGD), hard quadruplets are mined by identifying pairs of positive examples with low similarity and corresponding negative pairs with high similarity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997416615486145,
                    "sentence": "The efficacy of this approach is demonstrated through evaluations on image retrieval tasks using the CUB-200-2011 and CARS196 datasets, as well as on transfer and zero-shot learning tasks using the ImageNet-10K and ImageNet 2010 datasets, yielding improvements across all tasks, particularly in terms of training time.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974477887153625,
                    "sentence": "The paper addresses a well-known, significant problem in computer vision, and its formulation appears to be novel.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9930047392845154,
                    "sentence": "The writing is clear, and the references are appropriate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9934980869293213,
                    "sentence": "The results are intriguing, suggesting that the combination of the proposed unit and quadruple hard mining offers advantages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9965416193008423,
                    "sentence": "However, clarification is needed regarding the distinction between the \"PDDM score\" and the \"Quadruplet+PDDM\" rows in Table 1; specifically, whether they represent retrieval outcomes using the output of the PDDM module versus the learned embedding before the PDDM module using Euclidean distance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982001781463623,
                    "sentence": "Furthermore, the importance of the embedding loss $E_e$ in Equation (4) is questionable, given that gradients can be backpropagated through the PDDM modules.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998049259185791,
                    "sentence": "Additionally, it is unclear whether the features for hard negative mining are computed anew every minibatch or cached and periodically refreshed for efficiency purposes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 13,
                    "completely_generated_prob": 0.9448919484304374
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This manuscript tackles the issue of learning deep feature embeddings, where a major obstacle is the non-uniform density of the feature space, hindering effective hard negative mining. The authors propose a novel unit that incorporates the mean position of a pair of features, drawing inspiration from [35], alongside their difference, to compute the similarity between the feature pair. By leveraging positional information, this unit effectively accounts for the non-uniform density in the feature space. Notably, the unit is differentiable and can be seamlessly integrated into existing convolutional neural networks (CNNs). A double-header hinge loss is formulated based on pairwise differences and similarities before and after the unit. During each minibatch of stochastic gradient descent (SGD), hard quadruplets are mined by identifying pairs of positive examples with low similarity and corresponding negative pairs with high similarity. The efficacy of this approach is demonstrated through evaluations on image retrieval tasks using the CUB-200-2011 and CARS196 datasets, as well as on transfer and zero-shot learning tasks using the ImageNet-10K and ImageNet 2010 datasets, yielding improvements across all tasks, particularly in terms of training time. The paper addresses a well-known, significant problem in computer vision, and its formulation appears to be novel. The writing is clear, and the references are appropriate. The results are intriguing, suggesting that the combination of the proposed unit and quadruple hard mining offers advantages. However, clarification is needed regarding the distinction between the \"PDDM score\" and the \"Quadruplet+PDDM\" rows in Table 1; specifically, whether they represent retrieval outcomes using the output of the PDDM module versus the learned embedding before the PDDM module using Euclidean distance. Furthermore, the importance of the embedding loss $E_e$ in Equation (4) is questionable, given that gradients can be backpropagated through the PDDM modules. Additionally, it is unclear whether the features for hard negative mining are computed anew every minibatch or cached and periodically refreshed for efficiency purposes."
        }
    ]
}