{
    "version": "2025-01-09-base",
    "scanId": "ad4d73cd-3eeb-40a0-9d46-869fb406e37e",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999999463558197,
                    "sentence": "This paper proposes an unsupervised approach for learning visual similarities between large numbers of exemplars using Convolutional Neural Networks (CNNs).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999994039535522,
                    "sentence": "The authors address the limitations of traditional CNN training methods, which require large amounts of labeled data, by formulating an optimization problem that selects batches of compact, mutually dissimilar cliques of samples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999993443489075,
                    "sentence": "This approach enables the learning of exemplar similarities without the need for labels.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991655349731,
                    "sentence": "The paper's main claims are: (1) the proposed approach can learn visual similarities between exemplars in an unsupervised manner, and (2) it outperforms state-of-the-art methods on posture analysis and object classification tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999994039535522,
                    "sentence": "The authors support these claims through extensive experimental evaluations on three datasets: Olympic Sports, Leeds Sports, and PASCAL VOC 2007.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991655349731,
                    "sentence": "The paper is well-written, and the authors provide a clear and concise overview of the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986886978149,
                    "sentence": "The experimental evaluations are thorough, and the results demonstrate the effectiveness of the proposed method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999990463256836,
                    "sentence": "The authors also provide a detailed analysis of the learned similarity structures, which shows that the proposed approach can capture fine-grained similarities between exemplars.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998927116394,
                    "sentence": "The paper's strengths include: (1) the proposal of a novel approach for unsupervised learning of visual similarities, (2) the thorough experimental evaluations, and (3) the detailed analysis of the learned similarity structures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9066488146781921,
                    "sentence": "The paper's weaknesses include: (1) the complexity of the proposed approach, which may make it difficult to implement and reproduce, and (2) the limited analysis of the computational cost of the proposed method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9350482225418091,
                    "sentence": "Overall, I believe that this paper makes a significant contribution to the field of computer vision and machine learning.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9306600093841553,
                    "sentence": "The proposed approach has the potential to be applied to a wide range of tasks, including posture analysis, object classification, and image retrieval.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.44397038221359253,
                    "sentence": "I recommend accepting this paper for publication.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9232330322265625,
                    "sentence": "Arguments pro acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9971055388450623,
                    "sentence": "* The paper proposes a novel approach for unsupervised learning of visual similarities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9927073121070862,
                    "sentence": "* The experimental evaluations are thorough and demonstrate the effectiveness of the proposed method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9950609803199768,
                    "sentence": "* The paper provides a detailed analysis of the learned similarity structures.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9227727651596069,
                    "sentence": "Arguments con acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9893731474876404,
                    "sentence": "* The proposed approach may be complex and difficult to implement and reproduce.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9910140633583069,
                    "sentence": "* The paper does not provide a detailed analysis of the computational cost of the proposed method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.947571873664856,
                    "sentence": "Rating: 8/10",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9178137183189392,
                    "sentence": "Recommendation: Accept with minor revisions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7865360379219055,
                    "sentence": "The authors should provide a more detailed analysis of the computational cost of the proposed method and consider simplifying the approach to make it more accessible to a wider audience.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.45887534985363754
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                }
            ],
            "completely_generated_prob": 0.992329959936083,
            "class_probabilities": {
                "human": 0,
                "ai": 0.992329959936083,
                "mixed": 0.007670040063917
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.992329959936083,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.992329959936083,
                    "human": 0,
                    "mixed": 0.007670040063917
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes an unsupervised approach for learning visual similarities between large numbers of exemplars using Convolutional Neural Networks (CNNs). The authors address the limitations of traditional CNN training methods, which require large amounts of labeled data, by formulating an optimization problem that selects batches of compact, mutually dissimilar cliques of samples. This approach enables the learning of exemplar similarities without the need for labels.\nThe paper's main claims are: (1) the proposed approach can learn visual similarities between exemplars in an unsupervised manner, and (2) it outperforms state-of-the-art methods on posture analysis and object classification tasks. The authors support these claims through extensive experimental evaluations on three datasets: Olympic Sports, Leeds Sports, and PASCAL VOC 2007.\nThe paper is well-written, and the authors provide a clear and concise overview of the proposed approach. The experimental evaluations are thorough, and the results demonstrate the effectiveness of the proposed method. The authors also provide a detailed analysis of the learned similarity structures, which shows that the proposed approach can capture fine-grained similarities between exemplars.\nThe paper's strengths include: (1) the proposal of a novel approach for unsupervised learning of visual similarities, (2) the thorough experimental evaluations, and (3) the detailed analysis of the learned similarity structures. The paper's weaknesses include: (1) the complexity of the proposed approach, which may make it difficult to implement and reproduce, and (2) the limited analysis of the computational cost of the proposed method.\nOverall, I believe that this paper makes a significant contribution to the field of computer vision and machine learning. The proposed approach has the potential to be applied to a wide range of tasks, including posture analysis, object classification, and image retrieval. I recommend accepting this paper for publication.\nArguments pro acceptance:\n* The paper proposes a novel approach for unsupervised learning of visual similarities.\n* The experimental evaluations are thorough and demonstrate the effectiveness of the proposed method.\n* The paper provides a detailed analysis of the learned similarity structures.\nArguments con acceptance:\n* The proposed approach may be complex and difficult to implement and reproduce.\n* The paper does not provide a detailed analysis of the computational cost of the proposed method.\nRating: 8/10\nRecommendation: Accept with minor revisions. The authors should provide a more detailed analysis of the computational cost of the proposed method and consider simplifying the approach to make it more accessible to a wider audience."
        }
    ]
}