{
    "version": "2025-01-09-base",
    "scanId": "27319d7b-2d24-4ed9-815e-56af6cc095c8",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.41672882437705994,
                    "sentence": "The paper presents a methodology for estimating class priors from labeled samples and noisy positive samples, building upon an existing approach that estimates priors from clean positive samples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4855833649635315,
                    "sentence": "Experimental results demonstrate good performance compared to existing methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.176321342587471,
                    "sentence": "UPDATE AFTER REBUTTAL: The main result highlighted in the rebuttal, relating the observed P(y\"x) to the true one, is indeed noteworthy, but it has been previously established in [1] using similar ideas without noise in the positives.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.32124292850494385,
                    "sentence": "Consequently, the result did not surprise me.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.25847333669662476,
                    "sentence": "While it is valuable to have this written down, I believe it falls below the novelty threshold.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06715773046016693,
                    "sentence": "Regarding related work on asymmetric noise, my primary concern is that from (Scott et al., 2013) Lemma 1 + Proposition 3 (part 2), one can relate the output of a \"mixture proportion estimator\" (MPE) to the mixing weights for generic asymmetric positive and negative label noise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.052661605179309845,
                    "sentence": "This implies that the estimator of Sanderson & Scott could be used in the asymmetric noise scenario.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.031732507050037384,
                    "sentence": "Although Sanderson & Scott focused on an application to multiclass positive + unlabelled learning without noise in the positives, their estimator is conceivably applicable in the scenario of noisy positives as well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04769609495997429,
                    "sentence": "I believe this is similarly true of Ramaswamy et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.046106450259685516,
                    "sentence": "'s estimator.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05411549657583237,
                    "sentence": "Learning from positive and unlabelled data is a well-studied problem, and the paper makes a reasonable case for attention to be made in the case where the positives are subject to noise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.034412600100040436,
                    "sentence": "Existing methods in the PU literature for estimating the class prior are not directly applicable here, making it of interest to design methods for this scenario.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07377730309963226,
                    "sentence": "The paper is well-motivated, and its technical analysis of identifiability issues is appreciated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1124488040804863,
                    "sentence": "The basic idea is to build upon the AlphaMax framework of (Jain et al., 2016) and cast the estimation problem as determining the mixing weight for a certain mixture model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.09211430698633194,
                    "sentence": "This estimation is performed on the outputs of any probabilistically-consistent scorer on the inputs, avoiding the curse of dimensionality.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0542580746114254,
                    "sentence": "However, I have two concerns about the novelty of the work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.14535167813301086,
                    "sentence": "Firstly, the analysis in section 3 appears to closely follow that of (Jain et al., 2016), with natural extensions to account for the noisy positive setting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.17511136829853058,
                    "sentence": "Unless I missed something, there don't seem to be any fundamentally new insights or techniques used in the proofs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.15519876778125763,
                    "sentence": "The idea of using univariate transformations in section 4 appears particularly similar to what is done in (Jain et al., 2016).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0025293161161243916,
                    "sentence": "Secondly, the distinction of the scenario considered here to the general asymmetric label noise setting seems worthy of more discussion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002398650161921978,
                    "sentence": "This is to clarify why one does not conduct all analysis in the general case, rather than assuming that one of the observed distributions is the underlying marginal.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0026835829485207796,
                    "sentence": "There are methods for the latter, such as the practically viable estimators of Sanderson and Scott, Liu and Tao, and Ramaswamy et al., which could be applied to the scenario considered here.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008726206608116627,
                    "sentence": "Overall, I think the paper's contribution is reasonable, being an extension of (Jain et al., 2016) to the noisy PU case, but I think the novelty is a weak point.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.994137704372406,
                    "sentence": "Other comments include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9907005429267883,
                    "sentence": "- Section 3 should explicitly identify the missing results needed for the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9981311559677124,
                    "sentence": "- The use of a subscript and superscript in a^lambda_mu is confusing; consider using a(lambda, mu) instead.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9970566034317017,
                    "sentence": "- The introduction of \\Pi^res could be deferred till after Lemma 1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9996469020843506,
                    "sentence": "- Consider swapping statements 4 and 5 for Lemma 1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.99948650598526,
                    "sentence": "- In the proof of Lemma 1, consider not having all the equations inline to improve readability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9989463686943054,
                    "sentence": "- There is a typo in \"Theorem 1 (identifiablity)\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9987815618515015,
                    "sentence": "- Section 4 could be more explicit about the suggested algorithm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.998550534248352,
                    "sentence": "- It might be worth explaining in more detail what the probabilistic model in Figure 1 corresponds to.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.998490035533905,
                    "sentence": "- Equation (12) may be related to the result about the label probability function in Ward et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9986914992332458,
                    "sentence": "(2009).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.3063829682933457
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.16856730524349867,
            "class_probabilities": {
                "human": 0.7487902883499815,
                "ai": 0.16856730524349867,
                "mixed": 0.0826424064065197
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.7487902883499815,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.16856730524349867,
                    "human": 0.7487902883499815,
                    "mixed": 0.0826424064065197
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper presents a methodology for estimating class priors from labeled samples and noisy positive samples, building upon an existing approach that estimates priors from clean positive samples. Experimental results demonstrate good performance compared to existing methods. \nUPDATE AFTER REBUTTAL: The main result highlighted in the rebuttal, relating the observed P(y\"x) to the true one, is indeed noteworthy, but it has been previously established in [1] using similar ideas without noise in the positives. Consequently, the result did not surprise me. While it is valuable to have this written down, I believe it falls below the novelty threshold. \nRegarding related work on asymmetric noise, my primary concern is that from (Scott et al., 2013) Lemma 1 + Proposition 3 (part 2), one can relate the output of a \"mixture proportion estimator\" (MPE) to the mixing weights for generic asymmetric positive and negative label noise. This implies that the estimator of Sanderson & Scott could be used in the asymmetric noise scenario. Although Sanderson & Scott focused on an application to multiclass positive + unlabelled learning without noise in the positives, their estimator is conceivably applicable in the scenario of noisy positives as well. I believe this is similarly true of Ramaswamy et al.'s estimator.\nLearning from positive and unlabelled data is a well-studied problem, and the paper makes a reasonable case for attention to be made in the case where the positives are subject to noise. Existing methods in the PU literature for estimating the class prior are not directly applicable here, making it of interest to design methods for this scenario. The paper is well-motivated, and its technical analysis of identifiability issues is appreciated. \nThe basic idea is to build upon the AlphaMax framework of (Jain et al., 2016) and cast the estimation problem as determining the mixing weight for a certain mixture model. This estimation is performed on the outputs of any probabilistically-consistent scorer on the inputs, avoiding the curse of dimensionality. However, I have two concerns about the novelty of the work. \nFirstly, the analysis in section 3 appears to closely follow that of (Jain et al., 2016), with natural extensions to account for the noisy positive setting. Unless I missed something, there don't seem to be any fundamentally new insights or techniques used in the proofs. The idea of using univariate transformations in section 4 appears particularly similar to what is done in (Jain et al., 2016). \nSecondly, the distinction of the scenario considered here to the general asymmetric label noise setting seems worthy of more discussion. This is to clarify why one does not conduct all analysis in the general case, rather than assuming that one of the observed distributions is the underlying marginal. There are methods for the latter, such as the practically viable estimators of Sanderson and Scott, Liu and Tao, and Ramaswamy et al., which could be applied to the scenario considered here. \nOverall, I think the paper's contribution is reasonable, being an extension of (Jain et al., 2016) to the noisy PU case, but I think the novelty is a weak point. Other comments include: \n- Section 3 should explicitly identify the missing results needed for the approach.\n- The use of a subscript and superscript in a^lambda_mu is confusing; consider using a(lambda, mu) instead.\n- The introduction of \\Pi^res could be deferred till after Lemma 1.\n- Consider swapping statements 4 and 5 for Lemma 1.\n- In the proof of Lemma 1, consider not having all the equations inline to improve readability.\n- There is a typo in \"Theorem 1 (identifiablity)\".\n- Section 4 could be more explicit about the suggested algorithm.\n- It might be worth explaining in more detail what the probabilistic model in Figure 1 corresponds to.\n- Equation (12) may be related to the result about the label probability function in Ward et al. (2009)."
        }
    ]
}