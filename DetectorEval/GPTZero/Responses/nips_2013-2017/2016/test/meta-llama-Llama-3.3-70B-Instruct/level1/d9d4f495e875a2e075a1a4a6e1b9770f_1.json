{
    "version": "2025-01-09-base",
    "scanId": "03f82256-d9ed-457b-921e-453c40d7570a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999992251396179,
                    "sentence": "This paper presents a novel approach to video prediction, specifically designed for interactive agents such as robots.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999993443489075,
                    "sentence": "The authors propose an action-conditioned video prediction model that explicitly models pixel motion, allowing it to generalize to previously unseen objects.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992251396179,
                    "sentence": "The model is trained on a large dataset of robot interactions, consisting of 59,000 sequences with 1.5 million video frames.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998927116394,
                    "sentence": "The experiments demonstrate that the proposed method outperforms prior state-of-the-art methods in terms of video prediction quality, both quantitatively and qualitatively.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999985098838806,
                    "sentence": "The paper is well-written, and the authors provide a clear and concise overview of the related work, highlighting the limitations of existing methods and the contributions of their approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999944567680359,
                    "sentence": "The technical details of the model are well-explained, and the experiments are thoroughly described.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940395355225,
                    "sentence": "The results are impressive, showing that the model can predict plausible video sequences more than 10 time steps into the future.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999940991401672,
                    "sentence": "The strengths of the paper include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969005584717,
                    "sentence": "* The proposal of a novel approach to video prediction that explicitly models pixel motion, allowing for generalization to unseen objects.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999978542327881,
                    "sentence": "* The collection of a large dataset of robot interactions, which is a significant contribution to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999957084655762,
                    "sentence": "* The thorough evaluation of the proposed method, including comparisons to prior state-of-the-art methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999954104423523,
                    "sentence": "* The clear and concise writing style, making the paper easy to follow.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999986469745636,
                    "sentence": "The weaknesses of the paper include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989727139472961,
                    "sentence": "* The model's performance degrades over time, as uncertainty increases further into the future.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997091889381409,
                    "sentence": "* The model does not explicitly extract an internal object-centric representation, which could be a promising future direction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990878701210022,
                    "sentence": "* The paper could benefit from more discussion on the potential applications of the proposed method, beyond the specific task of video prediction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9973248839378357,
                    "sentence": "Arguments pro acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998466968536377,
                    "sentence": "* The paper presents a novel and significant contribution to the field of video prediction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99983811378479,
                    "sentence": "* The experiments demonstrate the effectiveness of the proposed method, outperforming prior state-of-the-art methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992808699607849,
                    "sentence": "* The paper is well-written, and the authors provide a clear and concise overview of the related work and technical details.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7610806226730347,
                    "sentence": "Arguments con acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9816789627075195,
                    "sentence": "* The model's performance degrades over time, which could limit its applicability in certain scenarios.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9151428937911987,
                    "sentence": "* The paper could benefit from more discussion on the potential applications of the proposed method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9603981375694275,
                    "sentence": "* The model does not explicitly extract an internal object-centric representation, which could be a limitation for certain tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7455664277076721,
                    "sentence": "Overall, I believe that the paper is a strong contribution to the field of video prediction and interactive agents, and I recommend acceptance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.10581302642822266,
                    "sentence": "The proposed method has the potential to be widely applicable, and the collection of the large dataset of robot interactions is a significant contribution to the field.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.0836644247174263,
                    "sentence": "With some minor revisions to address the weaknesses mentioned above, the paper could be even stronger.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                }
            ],
            "completely_generated_prob": 0.9417040358744394,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9417040358744394,
                "mixed": 0.05829596412556053
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9417040358744394,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9417040358744394,
                    "human": 0,
                    "mixed": 0.05829596412556053
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a novel approach to video prediction, specifically designed for interactive agents such as robots. The authors propose an action-conditioned video prediction model that explicitly models pixel motion, allowing it to generalize to previously unseen objects. The model is trained on a large dataset of robot interactions, consisting of 59,000 sequences with 1.5 million video frames. The experiments demonstrate that the proposed method outperforms prior state-of-the-art methods in terms of video prediction quality, both quantitatively and qualitatively.\nThe paper is well-written, and the authors provide a clear and concise overview of the related work, highlighting the limitations of existing methods and the contributions of their approach. The technical details of the model are well-explained, and the experiments are thoroughly described. The results are impressive, showing that the model can predict plausible video sequences more than 10 time steps into the future.\nThe strengths of the paper include:\n* The proposal of a novel approach to video prediction that explicitly models pixel motion, allowing for generalization to unseen objects.\n* The collection of a large dataset of robot interactions, which is a significant contribution to the field.\n* The thorough evaluation of the proposed method, including comparisons to prior state-of-the-art methods.\n* The clear and concise writing style, making the paper easy to follow.\nThe weaknesses of the paper include:\n* The model's performance degrades over time, as uncertainty increases further into the future.\n* The model does not explicitly extract an internal object-centric representation, which could be a promising future direction.\n* The paper could benefit from more discussion on the potential applications of the proposed method, beyond the specific task of video prediction.\nArguments pro acceptance:\n* The paper presents a novel and significant contribution to the field of video prediction.\n* The experiments demonstrate the effectiveness of the proposed method, outperforming prior state-of-the-art methods.\n* The paper is well-written, and the authors provide a clear and concise overview of the related work and technical details.\nArguments con acceptance:\n* The model's performance degrades over time, which could limit its applicability in certain scenarios.\n* The paper could benefit from more discussion on the potential applications of the proposed method.\n* The model does not explicitly extract an internal object-centric representation, which could be a limitation for certain tasks.\nOverall, I believe that the paper is a strong contribution to the field of video prediction and interactive agents, and I recommend acceptance. The proposed method has the potential to be widely applicable, and the collection of the large dataset of robot interactions is a significant contribution to the field. With some minor revisions to address the weaknesses mentioned above, the paper could be even stronger."
        }
    ]
}