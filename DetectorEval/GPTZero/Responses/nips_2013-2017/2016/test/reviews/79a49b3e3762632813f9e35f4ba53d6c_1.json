{
    "version": "2025-01-09-base",
    "scanId": "ebd62e9f-18ab-468d-a69c-c34a779c84ea",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.1904241442680359,
                    "sentence": "The paper proposes a method for estimating the class prior given on labelled sample, as well as noisy positive samples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.13042773306369781,
                    "sentence": "This builds upon an existing approach to estimate the prior when the positive samples are clean.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.20492403209209442,
                    "sentence": "Experiments show good performance compared to some existing methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0721854642033577,
                    "sentence": "UPDATE AFTER REBUTTAL: Regarding the main result highlighted in the rebuttal: it's indeed nice to relate the observed P(y\"x) to the true one, but again this was already done in [1] using similar ideas, just without noise in the positives.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.031971968710422516,
                    "sentence": "The result here thus didn't really surprise me.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03180413693189621,
                    "sentence": "As mentioned in my review, it's certainly valuable to have all this written down, but to me falls below the novelty threshold.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.025665663182735443,
                    "sentence": "Regarding the related work on asymmetric noise: my basic point is that from (Scott et al., 2013) Lemma 1 + Proposition 3 (part 2), one has a means of relating the output of a \"mixture proportion estimator\" (MPE) to the mixing weights for generic asymmetric positive and negative label noise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006476973183453083,
                    "sentence": "As I understand, that means that the estimator of Sanderson & Scott that I mentioned could be used in the asymmetric noise scenario.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011074475012719631,
                    "sentence": "The authors are correct that Sanderson & Scott focussed on an application to (multiclass) positive + unlabelled learning, without noise in the positives; but the point remains that their estimator is conceivably applicable in the scenario of noisy positives as well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021064821630716324,
                    "sentence": "I believe this is similarly true of Ramaswamy et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0484950914978981,
                    "sentence": "'s estimator.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013264295645058155,
                    "sentence": "============== Learning from positive and unlabelled data is a fairly well studied problem, and the paper makes a reasonable case for attention to be made in the case where the positives are subject to noise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.013008187524974346,
                    "sentence": "Existing methods in the PU literature for estimating the class prior are not directly applicable here, and so it is of interest to design methods for the scenario.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010974779725074768,
                    "sentence": "The paper thus is well motivated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009377427399158478,
                    "sentence": "On the technical side, the paper has a fairly careful theoretical analysis of the identifiably issues in this problem, which is appreciated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017876699566841125,
                    "sentence": "This is used to determine a canonical form in which estimation is performed, leading to a practical algorithm for the estimation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.032860610634088516,
                    "sentence": "The basic idea is to build upon the AlphaMax framework of (Jain et al., 2016), and cast the estimation problem as determining the mixing weight for a certain mixture model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02640531212091446,
                    "sentence": "This estimation is performed on the outputs of any probabilistically-consistent scorer on the inputs (i.e.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.050755687057971954,
                    "sentence": "it does not suffer from the curse of dimensionality.)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.052896905690431595,
                    "sentence": "Thus, the basic technical content seems sound.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004671723581850529,
                    "sentence": "I do however have two concerns about the novelty of the work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006605449132621288,
                    "sentence": "(1) The analysis in section 3 appears to closely follow that of (Jain et al., 2016), with some natural extensions to account for the fact that one is in the noisy positive setting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006095662713050842,
                    "sentence": "Unless I missed something, there don't seem to be any fundamentally new insights or techniques used in the proofs of the section: the two basic messages of (Jain et al., 2016) are translated, namely that working with all possible distributions leads to non-identifiability, while disallowing distributions that are themselves mixtures alleviates the problem.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007627394050359726,
                    "sentence": "One has two calls to AlphaMax for the estimation, but the basic idea of mixture modelling is as per the original paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007038864307105541,
                    "sentence": "The idea of then using univariate transformations in section 4 appears particularly similar to what is done in (Jain et al., 2016).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008975675329566002,
                    "sentence": "The paper claims that the latter was restricted to the case where the transformation results in calibrated probability, but doesn't Thm 9 of the latter also allow for arbitrary transformations?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009470461867749691,
                    "sentence": "Certainly it is valuable for all this to be written down in the noisy PU setting, but this leads to my next concern.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01389484666287899,
                    "sentence": "(2) The precise distinction of the scenario considered here to the general asymmetric label noise setting, seems worthy of a little more discussion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008205326274037361,
                    "sentence": "This is first to make clear why one does not just conduct all analysis in the general case, rather than assuming that one of the observed distributions is in fact the underlying marginal (i.e.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007453196682035923,
                    "sentence": "why one does not work in the general mutually contaminated framework of Scott et al., 2013).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006464648060500622,
                    "sentence": "Is there some reason analogues of Theorem 1 would not exist here?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.007414567284286022,
                    "sentence": "(As noted, at least some of the results have precedent in Scott et al., 2013, albeit in a different language.)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.009932443499565125,
                    "sentence": "The second reason is there are methods for the latter which I think could be applied for the scenario considered here, in which case the paper should compare against them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.041875988245010376,
                    "sentence": "In particular, I refer to the practically viable estimators of Sanderson and Scott.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.40116316080093384,
                    "sentence": "Class proportion estimation with application to multiclass anomaly rejection.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2854808568954468,
                    "sentence": "AISTATS 2014.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.305824875831604,
                    "sentence": "Liu and Tao.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5407578945159912,
                    "sentence": "Classification with Noisy Labels by Importance Reweighting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5196041464805603,
                    "sentence": "PAMI 2016.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.4299336373806,
                    "sentence": "(arXiV draft since 2014) Ramaswamy et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5853528380393982,
                    "sentence": "Mixture Proportion Estimation via Kernel Embedding of Distributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.459067702293396,
                    "sentence": "ICML 2016.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.972440242767334,
                    "sentence": "(arXiV draft since early 2016, so reasonable for authors to be unaware of) All three of the above seem like they would be relevant at the end of the third paragraph in the introduction, as a contrast to the cited work of Scott et al., 2013 that relies on density estimation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9703813791275024,
                    "sentence": "I note that the first method in particular also does not require the use of calibrated probabilities, but rather a probabilistically consistent ranker (as the estimate is based on a derivative of the RHS of the ROC curve).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9728931188583374,
                    "sentence": "Overall, I think the paper's contribution is reasonable, being an extension of (Jain et al., 2016) to the noisy PU case, but I think the novelty is a weak point.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9793168902397156,
                    "sentence": "Other comments: - section 3 is said to include \"a few missing results needed for our approach\" -- explicitly identifying which these are seems prudent.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9712408781051636,
                    "sentence": "- I found the use of a subscript and superscript in a^lambda_mu a bit confusing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9502478837966919,
                    "sentence": "Consider just a(lambda, mu)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9454060792922974,
                    "sentence": "- the introduction of \\Pi^res could perhaps be deferred till after Lemma 1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9302765130996704,
                    "sentence": "- consider swapping statements 4 and 5 for lemma 1, since the proof seems to do the same.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.900820255279541,
                    "sentence": "- in the proof of lemma 1, consider not having all the equations inline to improve readability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9651047587394714,
                    "sentence": "- typo: \"Theorem 1 (identifiablity)\" - I felt that section 4 could be more explicit about the suggested algorithm, i.e.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9579371213912964,
                    "sentence": "that one simply perform the mixture modelling on the classifier outputs on the positive and unlabelled samples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9553003907203674,
                    "sentence": "- it might be worth explaining in more detail what exactly the probabilistic model summarised in Figure 1 corresponds to.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9697774648666382,
                    "sentence": "That is, is it meant to represent an assumed generative view of the data, which must be satisfied for the data generation process?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9720117449760437,
                    "sentence": "Or a means by which one processes the available data one has?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9782165288925171,
                    "sentence": "Put another way, suppose one is able to draw from the unlabelled distribution, and then separately from the noisy positive distribution; what relevance does Fig 1 have?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9902822375297546,
                    "sentence": "- is (12) related to the result about the label probability function (Proposition 1) in Ward et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9861078262329102,
                    "sentence": "Presence-only data and the EM algorithm.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9695073366165161,
                    "sentence": "Biometrics, 2009.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 60,
                    "completely_generated_prob": 6.0457689380187346e-27
                }
            ],
            "completely_generated_prob": 0.22778574767484971,
            "class_probabilities": {
                "human": 0.7716823288576541,
                "ai": 0.22778574767484971,
                "mixed": 0.0005319234674962524
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.7716823288576541,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.22778574767484971,
                    "human": 0.7716823288576541,
                    "mixed": 0.0005319234674962524
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper proposes a method for estimating the class prior given on labelled sample, as well as noisy positive samples. This builds upon an existing approach to estimate the prior when the positive samples are clean. Experiments show good performance compared to some existing methods. UPDATE AFTER REBUTTAL: Regarding the main result highlighted in the rebuttal: it's indeed nice to relate the observed P(y\"x) to the true one, but again this was already done in [1] using similar ideas, just without noise in the positives. The result here thus didn't really surprise me. As mentioned in my review, it's certainly valuable to have all this written down, but to me falls below the novelty threshold. Regarding the related work on asymmetric noise: my basic point is that from (Scott et al., 2013) Lemma 1 + Proposition 3 (part 2), one has a means of relating the output of a \"mixture proportion estimator\" (MPE) to the mixing weights for generic asymmetric positive and negative label noise. As I understand, that means that the estimator of Sanderson & Scott that I mentioned could be used in the asymmetric noise scenario. The authors are correct that Sanderson & Scott focussed on an application to (multiclass) positive + unlabelled learning, without noise in the positives; but the point remains that their estimator is conceivably applicable in the scenario of noisy positives as well. I believe this is similarly true of Ramaswamy et al.'s estimator. ============== Learning from positive and unlabelled data is a fairly well studied problem, and the paper makes a reasonable case for attention to be made in the case where the positives are subject to noise. Existing methods in the PU literature for estimating the class prior are not directly applicable here, and so it is of interest to design methods for the scenario. The paper thus is well motivated. On the technical side, the paper has a fairly careful theoretical analysis of the identifiably issues in this problem, which is appreciated. This is used to determine a canonical form in which estimation is performed, leading to a practical algorithm for the estimation. The basic idea is to build upon the AlphaMax framework of (Jain et al., 2016), and cast the estimation problem as determining the mixing weight for a certain mixture model. This estimation is performed on the outputs of any probabilistically-consistent scorer on the inputs (i.e. it does not suffer from the curse of dimensionality.) Thus, the basic technical content seems sound. I do however have two concerns about the novelty of the work. (1) The analysis in section 3 appears to closely follow that of (Jain et al., 2016), with some natural extensions to account for the fact that one is in the noisy positive setting. Unless I missed something, there don't seem to be any fundamentally new insights or techniques used in the proofs of the section: the two basic messages of (Jain et al., 2016) are translated, namely that working with all possible distributions leads to non-identifiability, while disallowing distributions that are themselves mixtures alleviates the problem. One has two calls to AlphaMax for the estimation, but the basic idea of mixture modelling is as per the original paper. The idea of then using univariate transformations in section 4 appears particularly similar to what is done in (Jain et al., 2016). The paper claims that the latter was restricted to the case where the transformation results in calibrated probability, but doesn't Thm 9 of the latter also allow for arbitrary transformations? Certainly it is valuable for all this to be written down in the noisy PU setting, but this leads to my next concern. (2) The precise distinction of the scenario considered here to the general asymmetric label noise setting, seems worthy of a little more discussion. This is first to make clear why one does not just conduct all analysis in the general case, rather than assuming that one of the observed distributions is in fact the underlying marginal (i.e. why one does not work in the general mutually contaminated framework of Scott et al., 2013). Is there some reason analogues of Theorem 1 would not exist here? (As noted, at least some of the results have precedent in Scott et al., 2013, albeit in a different language.) The second reason is there are methods for the latter which I think could be applied for the scenario considered here, in which case the paper should compare against them. In particular, I refer to the practically viable estimators of Sanderson and Scott. Class proportion estimation with application to multiclass anomaly rejection. AISTATS 2014. Liu and Tao. Classification with Noisy Labels by Importance Reweighting. PAMI 2016. (arXiV draft since 2014) Ramaswamy et al. Mixture Proportion Estimation via Kernel Embedding of Distributions. ICML 2016. (arXiV draft since early 2016, so reasonable for authors to be unaware of) All three of the above seem like they would be relevant at the end of the third paragraph in the introduction, as a contrast to the cited work of Scott et al., 2013 that relies on density estimation. I note that the first method in particular also does not require the use of calibrated probabilities, but rather a probabilistically consistent ranker (as the estimate is based on a derivative of the RHS of the ROC curve). Overall, I think the paper's contribution is reasonable, being an extension of (Jain et al., 2016) to the noisy PU case, but I think the novelty is a weak point. Other comments: - section 3 is said to include \"a few missing results needed for our approach\" -- explicitly identifying which these are seems prudent. - I found the use of a subscript and superscript in a^lambda_mu a bit confusing. Consider just a(lambda, mu)? - the introduction of \\Pi^res could perhaps be deferred till after Lemma 1. - consider swapping statements 4 and 5 for lemma 1, since the proof seems to do the same. - in the proof of lemma 1, consider not having all the equations inline to improve readability. - typo: \"Theorem 1 (identifiablity)\" - I felt that section 4 could be more explicit about the suggested algorithm, i.e. that one simply perform the mixture modelling on the classifier outputs on the positive and unlabelled samples. - it might be worth explaining in more detail what exactly the probabilistic model summarised in Figure 1 corresponds to. That is, is it meant to represent an assumed generative view of the data, which must be satisfied for the data generation process? Or a means by which one processes the available data one has? Put another way, suppose one is able to draw from the unlabelled distribution, and then separately from the noisy positive distribution; what relevance does Fig 1 have? - is (12) related to the result about the label probability function (Proposition 1) in Ward et al. Presence-only data and the EM algorithm. Biometrics, 2009."
        }
    ]
}