{
    "version": "2025-01-09-base",
    "scanId": "1a57ae02-7741-4b2c-aca8-9cfd7fae93a4",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.980560839176178,
                    "sentence": "The paper considers online learning in a non-stationary stochastic environment, and considers the achievable regret as a function of the change in the mean loss vector (measured by the number of switches or the amount of shift) and the cumulative variance of the random losses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9682577848434448,
                    "sentence": "Algorithms are given with regret upper bounds, as well as matching lower bounds.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.960619330406189,
                    "sentence": "ADDED AFTER THE REBUTTAL: - Indeed, the proof of Theorem 4.1 is correct, and the constructed environment is oblivious; it was my mistake, sorry about it.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.95878005027771,
                    "sentence": "- The generalization in the analysis of UCB-V seems quite straightforward, as--if I recall correctly--the applied concentration inequalities do not require that the random variables have the same distribution (neither in your nor in their case).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9458747506141663,
                    "sentence": "- Comparison to Jadbabaie et al: If in their work one chooses M to be the previous gradient, then their D_T is essentially the same as your Lambda (up to some constants), and the difference is that you take expectation inside the norms in V. While I might be mistaken here, but it seems to me that with a slight modification of their proof you can move the expectation inside.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8961405158042908,
                    "sentence": "Nevertheless, you have to check this, and describe the connection to related work with care (what follows easily and what not).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9380127787590027,
                    "sentence": "- You should still argue why the proposed complexity parameters are interesting, e.g., present an example when Lambda is not zero or linear in T. ----------------------------------------- While the paper might have some interesting contributions, it does not consider the existing literature and does not cite the references adequately.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.9476717710494995,
                    "sentence": "This, in fact, makes it hard to asses the novelty of the results, as several of them seem to easily follow from existing work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0145569471642375,
                    "sentence": "In fact, the paper of Jadbabaie, Rakhlin, Shahrampour, and Sridharan, whose arxiv version is \"sort of\" cited in the paper (included in the references but never discussed properly), which was published in AISTATS 2015, provides an algorithm quite similar to Algorithm 2, which achieves similar bounds, but in the stronger adversarial case.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.014966793358325958,
                    "sentence": "The main addition of the paper to admitted existing work (Besbes et al., 2014--please correct the order of the authors in the reference) is to consider the variance of the random losses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.006690676789730787,
                    "sentence": "As far as I can see, this is achieved by combining the techniques to derive second order bounds and some sort of shifting bounds (even blocking, in the simplest case).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.012017891742289066,
                    "sentence": "This would be interesting if it had not been done before by Jadbabaie et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004658700432628393,
                    "sentence": "(2015).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.025658050552010536,
                    "sentence": "However, given the previous work, a thorough discussion of novelty would be required.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015341045334935188,
                    "sentence": "Also, the writing of the paper has to be improved dramatically: e.g., not even the UCB-V algorithm of Audibert et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01946706511080265,
                    "sentence": "(2009) is referenced (although used almost identically).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022135449573397636,
                    "sentence": "Concerning motivation, other than saying that the complexity measures used in the paper appear in the literature, why are they interesting?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.015073954127728939,
                    "sentence": "For example, if there is a clearly suboptimal arm, why does it matter if its mean changes or it has a relatively high variance (assume that even the smallest loss from this arm is bigger than the losses of any other arm)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021329855546355247,
                    "sentence": "What are the interesting cases when \\Lambda is not constant or linear in time?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02590651623904705,
                    "sentence": "Also, what is the connection to tracking/shifting bounds (Herbster and Warmuth 1998, 2001)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.011633864603936672,
                    "sentence": "Note that in the standard online learning literature, tracking and shifting bounds usually correspond to properties of the comparator sequence, not of the loss functions, so this should be clarified to avoid confusion.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.005707247648388147,
                    "sentence": "The switching results should also relate to the large body of work done in information theory on coding piecewise stationary sources (although there usually the log-loss is considered, but the techniques are essentially the same, although instead of the variance the the entropy/entropy-rate of the source is considered).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.010070227086544037,
                    "sentence": "The sleeping expert-type reductions are also available in generic forms, see e.g., Gyorgy et al.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.01085127517580986,
                    "sentence": "(2012) and the references therein.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02484768256545067,
                    "sentence": "All in all, I see potential in the paper, but substantial improvements are needed before it can be published.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023121781647205353,
                    "sentence": "References: J-Y.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.033420514315366745,
                    "sentence": "Audibert, R. Munos, and Cs Szepesvari.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.027508344501256943,
                    "sentence": "Exploration-exploitation trade-off using variance estimates in multi-armed bandits.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0729973316192627,
                    "sentence": "Theoretical Computer Science, 410:1876-1902, 2009.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.025792744010686874,
                    "sentence": "A. Gyorgy, T. Linder, and G. Lugosi, \"Efficient Tracking of Large Classes of Experts,\" IEEE Transactions on Information Theory, vol.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05991111323237419,
                    "sentence": "58, pp.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03466704860329628,
                    "sentence": "6709-6725, November 2012.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.021568819880485535,
                    "sentence": "Herbster, M. and Warmuth, M.K.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03248189762234688,
                    "sentence": "(1998) \"Tracking the Best Expert\", in the special issue on context sensitivity and concept drift of the Journal of Machine Learning, Vol.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.057997774332761765,
                    "sentence": "32(2), pp.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.11640136688947678,
                    "sentence": "151-178, August 1998.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0295152235776186,
                    "sentence": "Herbster, M. and Warmuth, M.K.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.053491126745939255,
                    "sentence": "(2001) \"Tracking the Best Linear Predictor\", Journal of Machine Learning Research, Vol.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08493976294994354,
                    "sentence": "1, pp.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07225712388753891,
                    "sentence": "281-309, September 2001.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 40,
                    "completely_generated_prob": 1.5376683706824398e-23
                }
            ],
            "completely_generated_prob": 0.20373528541556546,
            "class_probabilities": {
                "human": 0.795766055976091,
                "ai": 0.20373528541556546,
                "mixed": 0.0004986586083435559
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.795766055976091,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.20373528541556546,
                    "human": 0.795766055976091,
                    "mixed": 0.0004986586083435559
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper considers online learning in a non-stationary stochastic environment, and considers the achievable regret as a function of the change in the mean loss vector (measured by the number of switches or the amount of shift) and the cumulative variance of the random losses. Algorithms are given with regret upper bounds, as well as matching lower bounds. ADDED AFTER THE REBUTTAL: - Indeed, the proof of Theorem 4.1 is correct, and the constructed environment is oblivious; it was my mistake, sorry about it. - The generalization in the analysis of UCB-V seems quite straightforward, as--if I recall correctly--the applied concentration inequalities do not require that the random variables have the same distribution (neither in your nor in their case). - Comparison to Jadbabaie et al: If in their work one chooses M to be the previous gradient, then their D_T is essentially the same as your Lambda (up to some constants), and the difference is that you take expectation inside the norms in V. While I might be mistaken here, but it seems to me that with a slight modification of their proof you can move the expectation inside. Nevertheless, you have to check this, and describe the connection to related work with care (what follows easily and what not). - You should still argue why the proposed complexity parameters are interesting, e.g., present an example when Lambda is not zero or linear in T. ----------------------------------------- While the paper might have some interesting contributions, it does not consider the existing literature and does not cite the references adequately. This, in fact, makes it hard to asses the novelty of the results, as several of them seem to easily follow from existing work. In fact, the paper of Jadbabaie, Rakhlin, Shahrampour, and Sridharan, whose arxiv version is \"sort of\" cited in the paper (included in the references but never discussed properly), which was published in AISTATS 2015, provides an algorithm quite similar to Algorithm 2, which achieves similar bounds, but in the stronger adversarial case. The main addition of the paper to admitted existing work (Besbes et al., 2014--please correct the order of the authors in the reference) is to consider the variance of the random losses. As far as I can see, this is achieved by combining the techniques to derive second order bounds and some sort of shifting bounds (even blocking, in the simplest case). This would be interesting if it had not been done before by Jadbabaie et al. (2015). However, given the previous work, a thorough discussion of novelty would be required. Also, the writing of the paper has to be improved dramatically: e.g., not even the UCB-V algorithm of Audibert et al. (2009) is referenced (although used almost identically). Concerning motivation, other than saying that the complexity measures used in the paper appear in the literature, why are they interesting? For example, if there is a clearly suboptimal arm, why does it matter if its mean changes or it has a relatively high variance (assume that even the smallest loss from this arm is bigger than the losses of any other arm)? What are the interesting cases when \\Lambda is not constant or linear in time? Also, what is the connection to tracking/shifting bounds (Herbster and Warmuth 1998, 2001)? Note that in the standard online learning literature, tracking and shifting bounds usually correspond to properties of the comparator sequence, not of the loss functions, so this should be clarified to avoid confusion. The switching results should also relate to the large body of work done in information theory on coding piecewise stationary sources (although there usually the log-loss is considered, but the techniques are essentially the same, although instead of the variance the the entropy/entropy-rate of the source is considered). The sleeping expert-type reductions are also available in generic forms, see e.g., Gyorgy et al. (2012) and the references therein. All in all, I see potential in the paper, but substantial improvements are needed before it can be published. References: J-Y. Audibert, R. Munos, and Cs Szepesvari. Exploration-exploitation trade-off using variance estimates in multi-armed bandits. Theoretical Computer Science, 410:1876-1902, 2009. A. Gyorgy, T. Linder, and G. Lugosi, \"Efficient Tracking of Large Classes of Experts,\" IEEE Transactions on Information Theory, vol. 58, pp. 6709-6725, November 2012. Herbster, M. and Warmuth, M.K. (1998) \"Tracking the Best Expert\", in the special issue on context sensitivity and concept drift of the Journal of Machine Learning, Vol. 32(2), pp. 151-178, August 1998. Herbster, M. and Warmuth, M.K. (2001) \"Tracking the Best Linear Predictor\", Journal of Machine Learning Research, Vol. 1, pp. 281-309, September 2001."
        }
    ]
}