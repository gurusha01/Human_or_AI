{
    "version": "2025-01-09-base",
    "scanId": "e0d0bb7e-a216-4dd0-ada7-bf8adb14cd59",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9536607265472412,
                    "sentence": "This paper presents a structured regression model for inferring neural connectivity from viral tracing data.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.951789140701294,
                    "sentence": "Combining a nonnegativity constraint, a spatial smoothness regularizer, a low-rank assumption, and a mask that handles ambiguity within the injection site, the authors derive a model that can leverage observed tracer data to infer the underlying connectivity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8106939792633057,
                    "sentence": "While the individual components of this model have been explored in other structured regression problems, this paper presents a thorough and well-executed application of these ideas to an interesting problem.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08673324435949326,
                    "sentence": "The underlying model is a non-negative linear regression, y = Wx + \\eta, where \\eta is drawn from a spherical Gaussian model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03140318766236305,
                    "sentence": "The weight matrix, W, is assumed to be nonnegative and, in probabilistic terms, drawn from a spatially smooth prior.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.023810552433133125,
                    "sentence": "Optionally, a low-rank assumption may be incorporated into the weight model, which can dramatically improve memory efficiency for large-scale problems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0042190346866846085,
                    "sentence": "While the individual components of this model (nonnegative regression, Laplacian regularized least squares, low-rank constraints) are well-studied, I think this is a nice combination and application of these techniques to a real-world, scientific problem.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0021786470897495747,
                    "sentence": "The presentation of the model, the synthetic examples, and the real world applications (and supplementary movies) are particularly clear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.051586367189884186,
                    "sentence": "While it is certainly valid to directly construct a objective function that captures both the reconstruction error and the domain-specific constraints and inductive biases, I think a probabilistic perspective could elucidate a number of potential extensions and connections to existing work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.027897587046027184,
                    "sentence": "For example, Jonas and Kording [1] tackle a similar connectomics problem by formulating a probabilistic generative model in terms of latent variables of each neuron.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0321173258125782,
                    "sentence": "While the setup is somewhat different (theirs is an unsupervised modeling problem whereas yours is framed as a regression problem), one could imagine a similar set of latent variables serving as a prior for your weight matrix.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03584100306034088,
                    "sentence": "Similarly, from a probabilistic perspective, the Gaussian noise model is not particularly realistic given that the fluorescence observations are nonnegative.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03513919562101364,
                    "sentence": "Likewise, the spherical covariance seems likely to be a strong assumption.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03650636598467827,
                    "sentence": "While these may justifiable and computationally advantageous simplifications, it is worth discussing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05690303444862366,
                    "sentence": "Finally, the orthogonal projector, P_{\\Omega}, seems like a rather fancy way of saying that your loss function is only summing over observable voxels.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03857880085706711,
                    "sentence": "Minor comments: - The labels in Figure 3 are not very legible.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02994425781071186,
                    "sentence": "[1] Jonas, Eric, and Konrad Kording.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05831727385520935,
                    "sentence": "\"Automatic discovery of cell types and microcircuitry from neural connectomics.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.046257536858320236,
                    "sentence": "eLife 4 (2015): e04250.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 19,
                    "completely_generated_prob": 2.0006107514718907e-12
                }
            ],
            "completely_generated_prob": 0.07823590649547306,
            "class_probabilities": {
                "human": 0.8958597161695051,
                "ai": 0.07823590649547306,
                "mixed": 0.025904377335021836
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.8958597161695051,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.07823590649547306,
                    "human": 0.8958597161695051,
                    "mixed": 0.025904377335021836
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a structured regression model for inferring neural connectivity from viral tracing data. Combining a nonnegativity constraint, a spatial smoothness regularizer, a low-rank assumption, and a mask that handles ambiguity within the injection site, the authors derive a model that can leverage observed tracer data to infer the underlying connectivity. While the individual components of this model have been explored in other structured regression problems, this paper presents a thorough and well-executed application of these ideas to an interesting problem. The underlying model is a non-negative linear regression, y = Wx + \\eta, where \\eta is drawn from a spherical Gaussian model. The weight matrix, W, is assumed to be nonnegative and, in probabilistic terms, drawn from a spatially smooth prior. Optionally, a low-rank assumption may be incorporated into the weight model, which can dramatically improve memory efficiency for large-scale problems. While the individual components of this model (nonnegative regression, Laplacian regularized least squares, low-rank constraints) are well-studied, I think this is a nice combination and application of these techniques to a real-world, scientific problem. The presentation of the model, the synthetic examples, and the real world applications (and supplementary movies) are particularly clear. While it is certainly valid to directly construct a objective function that captures both the reconstruction error and the domain-specific constraints and inductive biases, I think a probabilistic perspective could elucidate a number of potential extensions and connections to existing work. For example, Jonas and Kording [1] tackle a similar connectomics problem by formulating a probabilistic generative model in terms of latent variables of each neuron. While the setup is somewhat different (theirs is an unsupervised modeling problem whereas yours is framed as a regression problem), one could imagine a similar set of latent variables serving as a prior for your weight matrix. Similarly, from a probabilistic perspective, the Gaussian noise model is not particularly realistic given that the fluorescence observations are nonnegative. Likewise, the spherical covariance seems likely to be a strong assumption. While these may justifiable and computationally advantageous simplifications, it is worth discussing. Finally, the orthogonal projector, P_{\\Omega}, seems like a rather fancy way of saying that your loss function is only summing over observable voxels. Minor comments: - The labels in Figure 3 are not very legible. [1] Jonas, Eric, and Konrad Kording. \"Automatic discovery of cell types and microcircuitry from neural connectomics.\" eLife 4 (2015): e04250."
        }
    ]
}