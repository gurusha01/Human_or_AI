{
    "version": "2025-01-09-base",
    "scanId": "060d05cc-ca43-446a-a778-28c595077607",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.7013601064682007,
                    "sentence": "This computational neuroscience paper introduces a bottom-up visual feature saliency model, modified through dynamical systems modeling, to capture the dynamics of scanpaths.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5022003054618835,
                    "sentence": "I found the paper engaging and appreciated its innovative ideas, particularly in defining \"affordances\" (such as curiosity and brightness invariance) that influence scanpaths, as well as the application of the Least Action Principle.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5702651739120483,
                    "sentence": "However, these \"affordances\" appear somewhat ad hoc and lack further justification or deeper motivation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5899350643157959,
                    "sentence": "In the context of the Least Action Principle and its derivation, it would have been valuable to connect this work to existing research in eye movement studies, such as earlier and more recent contributions from the Wolpert lab.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5736016035079956,
                    "sentence": "While the limited biological grounding is a relatively minor concern\"\"especially in an era dominated by deep learning models excelling at (static) visual salience\"\"the primary issue lies in the model's mixed and underwhelming performance compared to these benchmark models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.6564634442329407,
                    "sentence": "Considering the model's claim to replicate biological eye movement dynamics, it would have been helpful to include figures in the main paper that explicitly characterize the generated scanpaths, not just in terms of their overall statistics but also in their alignment with actual eye movement patterns.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                }
            ],
            "completely_generated_prob": 0.7012240715000121,
            "class_probabilities": {
                "human": 0.2976865220809293,
                "ai": 0.7012240715000121,
                "mixed": 0.0010894064190585953
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.7012240715000121,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.7012240715000121,
                    "human": 0.2976865220809293,
                    "mixed": 0.0010894064190585953
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This computational neuroscience paper introduces a bottom-up visual feature saliency model, modified through dynamical systems modeling, to capture the dynamics of scanpaths. I found the paper engaging and appreciated its innovative ideas, particularly in defining \"affordances\" (such as curiosity and brightness invariance) that influence scanpaths, as well as the application of the Least Action Principle. However, these \"affordances\" appear somewhat ad hoc and lack further justification or deeper motivation.\nIn the context of the Least Action Principle and its derivation, it would have been valuable to connect this work to existing research in eye movement studies, such as earlier and more recent contributions from the Wolpert lab. While the limited biological grounding is a relatively minor concern\"\"especially in an era dominated by deep learning models excelling at (static) visual salience\"\"the primary issue lies in the model's mixed and underwhelming performance compared to these benchmark models. Considering the model's claim to replicate biological eye movement dynamics, it would have been helpful to include figures in the main paper that explicitly characterize the generated scanpaths, not just in terms of their overall statistics but also in their alignment with actual eye movement patterns."
        }
    ]
}