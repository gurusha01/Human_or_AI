{
    "version": "2025-01-09-base",
    "scanId": "b9cb0089-62ec-4f78-90bc-7a4135ffba5c",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.8105753064155579,
                    "sentence": "The paper introduces a novel approach to studying eye movements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6681789755821228,
                    "sentence": "The authors provide an accurate summary of the current state of the art (to the best of my understanding, though I acknowledge that I am not an expert in perception or eye movements).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8722008466720581,
                    "sentence": "Broadly speaking, existing approaches rely on a saliency map defined over an image, with fixation points determined through an optimization or maximization process.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7222830057144165,
                    "sentence": "In contrast, the proposed model (EYMOL) is based on a different premise, being directly defined on gaze trajectories and grounded in a \"Least Action Principle.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7180303931236267,
                    "sentence": "While some of the mathematical details, particularly those in the extensive appendix, were beyond my expertise due to my limited physics background, I was still able to grasp the core elements of the approach and how it diverges from previous models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.517118513584137,
                    "sentence": "The model is effectively applied to a real dataset of eye movements and is benchmarked against several alternative models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5449585914611816,
                    "sentence": "Although it does not outperform on every metric, the results are promising and suggest that the proposed approach has validity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5130111575126648,
                    "sentence": "I believe the paper could be of interest to certain members of the vision science community (e.g., attendees of VSS).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3141089379787445,
                    "sentence": "However, I am less convinced of its suitability as a significant contribution to NIPS, though I think this judgment should be informed by the opinions of other reviewers with greater expertise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.24216261506080627,
                    "sentence": "The paper likely advances a novel and intriguing theory that has the potential to inspire further empirical research, making it both innovative and valuable for the field of perception sciences.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 7,
                    "completely_generated_prob": 0.0010573188656143365
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                }
            ],
            "completely_generated_prob": 0.49464236844505877,
            "class_probabilities": {
                "human": 0.5052418477688814,
                "ai": 0.49464236844505877,
                "mixed": 0.00011578378605963213
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5052418477688814,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.49464236844505877,
                    "human": 0.5052418477688814,
                    "mixed": 0.00011578378605963213
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper introduces a novel approach to studying eye movements. The authors provide an accurate summary of the current state of the art (to the best of my understanding, though I acknowledge that I am not an expert in perception or eye movements). Broadly speaking, existing approaches rely on a saliency map defined over an image, with fixation points determined through an optimization or maximization process. In contrast, the proposed model (EYMOL) is based on a different premise, being directly defined on gaze trajectories and grounded in a \"Least Action Principle.\" While some of the mathematical details, particularly those in the extensive appendix, were beyond my expertise due to my limited physics background, I was still able to grasp the core elements of the approach and how it diverges from previous models. The model is effectively applied to a real dataset of eye movements and is benchmarked against several alternative models. Although it does not outperform on every metric, the results are promising and suggest that the proposed approach has validity.\nI believe the paper could be of interest to certain members of the vision science community (e.g., attendees of VSS). However, I am less convinced of its suitability as a significant contribution to NIPS, though I think this judgment should be informed by the opinions of other reviewers with greater expertise. The paper likely advances a novel and intriguing theory that has the potential to inspire further empirical research, making it both innovative and valuable for the field of perception sciences."
        }
    ]
}