{
    "version": "2025-01-09-base",
    "scanId": "9f14b95f-2b91-4c0a-ad50-fe8ccc8baee2",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999993443489075,
                    "sentence": "This paper proposes a novel model of visual attention based on the principle of least action, which is a fundamental concept in physics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999988675117493,
                    "sentence": "The model, called EYe MOvement Laws (EYMOL), is derived from a generalized Lagrangian that incorporates three key principles: boundedness of the trajectory, curiosity-driven movement, and brightness invariance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979734420776,
                    "sentence": "The resulting Euler-Lagrange equations describe the dynamics of visual attention and can be used to predict human fixations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999979138374329,
                    "sentence": "The paper is well-written and provides a clear explanation of the theoretical framework and the mathematical derivations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983310699463,
                    "sentence": "The authors also provide experimental results on saliency detection tasks, which demonstrate the effectiveness of the proposed model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999989867210388,
                    "sentence": "The model is shown to perform well on both image and video datasets, and its simplicity and real-time capabilities make it an attractive solution for various applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999972581863403,
                    "sentence": "The strengths of the paper include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999983310699463,
                    "sentence": "* The proposal of a novel and theoretically grounded model of visual attention",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998152256012,
                    "sentence": "* The use of a variational approach to derive the model, which provides a clear and intuitive understanding of the underlying principles",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999986886978149,
                    "sentence": "* The experimental results, which demonstrate the effectiveness of the model on various datasets",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999989867210388,
                    "sentence": "* The simplicity and real-time capabilities of the model, which make it suitable for practical applications",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999973177909851,
                    "sentence": "The weaknesses of the paper include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999985694885254,
                    "sentence": "* The lack of a detailed comparison with other state-of-the-art models of visual attention",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998518824577332,
                    "sentence": "* The limited analysis of the model's parameters and their impact on the results",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998183846473694,
                    "sentence": "* The need for further validation of the model on more diverse datasets and tasks",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993361234664917,
                    "sentence": "Overall, the paper presents a significant contribution to the field of visual attention and provides a promising solution for saliency detection tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995369911193848,
                    "sentence": "However, further research is needed to fully explore the capabilities and limitations of the proposed model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9795553684234619,
                    "sentence": "Arguments pro acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9912742972373962,
                    "sentence": "* The paper proposes a novel and theoretically grounded model of visual attention",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980992674827576,
                    "sentence": "* The model is shown to be effective on various datasets and tasks",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983387589454651,
                    "sentence": "* The simplicity and real-time capabilities of the model make it suitable for practical applications",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9891870021820068,
                    "sentence": "Arguments con acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983243346214294,
                    "sentence": "* The lack of a detailed comparison with other state-of-the-art models of visual attention",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984220862388611,
                    "sentence": "* The limited analysis of the model's parameters and their impact on the results",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984437227249146,
                    "sentence": "* The need for further validation of the model on more diverse datasets and tasks",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9178872108459473,
                    "sentence": "Recommendation: Accept with minor revisions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9690671563148499,
                    "sentence": "The authors should provide a more detailed comparison with other state-of-the-art models of visual attention and analyze the impact of the model's parameters on the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9722339510917664,
                    "sentence": "Additionally, further validation of the model on more diverse datasets and tasks would strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 24,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9954476479514417,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9954476479514417,
                "mixed": 0.004552352048558421
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9954476479514417,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9954476479514417,
                    "human": 0,
                    "mixed": 0.004552352048558421
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a novel model of visual attention based on the principle of least action, which is a fundamental concept in physics. The model, called EYe MOvement Laws (EYMOL), is derived from a generalized Lagrangian that incorporates three key principles: boundedness of the trajectory, curiosity-driven movement, and brightness invariance. The resulting Euler-Lagrange equations describe the dynamics of visual attention and can be used to predict human fixations.\nThe paper is well-written and provides a clear explanation of the theoretical framework and the mathematical derivations. The authors also provide experimental results on saliency detection tasks, which demonstrate the effectiveness of the proposed model. The model is shown to perform well on both image and video datasets, and its simplicity and real-time capabilities make it an attractive solution for various applications.\nThe strengths of the paper include:\n* The proposal of a novel and theoretically grounded model of visual attention\n* The use of a variational approach to derive the model, which provides a clear and intuitive understanding of the underlying principles\n* The experimental results, which demonstrate the effectiveness of the model on various datasets\n* The simplicity and real-time capabilities of the model, which make it suitable for practical applications\nThe weaknesses of the paper include:\n* The lack of a detailed comparison with other state-of-the-art models of visual attention\n* The limited analysis of the model's parameters and their impact on the results\n* The need for further validation of the model on more diverse datasets and tasks\nOverall, the paper presents a significant contribution to the field of visual attention and provides a promising solution for saliency detection tasks. However, further research is needed to fully explore the capabilities and limitations of the proposed model.\nArguments pro acceptance:\n* The paper proposes a novel and theoretically grounded model of visual attention\n* The model is shown to be effective on various datasets and tasks\n* The simplicity and real-time capabilities of the model make it suitable for practical applications\nArguments con acceptance:\n* The lack of a detailed comparison with other state-of-the-art models of visual attention\n* The limited analysis of the model's parameters and their impact on the results\n* The need for further validation of the model on more diverse datasets and tasks\nRecommendation: Accept with minor revisions. The authors should provide a more detailed comparison with other state-of-the-art models of visual attention and analyze the impact of the model's parameters on the results. Additionally, further validation of the model on more diverse datasets and tasks would strengthen the paper."
        }
    ]
}