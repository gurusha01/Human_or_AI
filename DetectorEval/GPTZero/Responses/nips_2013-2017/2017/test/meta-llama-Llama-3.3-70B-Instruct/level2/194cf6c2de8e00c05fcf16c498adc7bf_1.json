{
    "version": "2025-01-09-base",
    "scanId": "fc800279-a545-4d4d-b2fa-83b451398091",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999998152256012,
                    "sentence": "This paper proposes a novel model of visual attention based on the principle of least action, which is a fundamental concept in physics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999997615814209,
                    "sentence": "The model, called EYe MOvement Laws (EYMOL), is derived from a generalized Lagrangian that incorporates three key principles: boundedness of the trajectory, curiosity-driven movement, and brightness invariance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999966025352478,
                    "sentence": "The resulting Euler-Lagrange equations describe the dynamics of visual attention and can be used to predict human fixations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "The paper is well-written, and the authors provide a clear and concise explanation of the mathematical framework and the experimental setup.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999974966049194,
                    "sentence": "The model is evaluated on several datasets, including images and videos, and the results show that it competes well with state-of-the-art models in saliency detection.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999966025352478,
                    "sentence": "The strengths of the paper include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999971389770508,
                    "sentence": "* The proposal of a novel and theoretically grounded model of visual attention that integrates multiple principles.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999967813491821,
                    "sentence": "* The use of a variational approach to derive the model, which provides a clear and consistent mathematical framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999998152256012,
                    "sentence": "* The evaluation of the model on multiple datasets, which demonstrates its robustness and generalizability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999953508377075,
                    "sentence": "However, there are also some weaknesses and limitations:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999969005584717,
                    "sentence": "* The model is based on a simplification of the brightness invariance term, which may not be accurate in all cases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999995768070221,
                    "sentence": "* The parameters of the model are estimated using a simulated annealing algorithm, which may not be the most efficient or effective method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9283314943313599,
                    "sentence": "* The model is not directly compared to other models of visual attention that are based on different principles, such as information maximization or surprise.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9024509191513062,
                    "sentence": "Overall, the paper presents a significant contribution to the field of visual attention and provides a new perspective on the underlying mechanisms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.902985155582428,
                    "sentence": "The model has the potential to be useful in a variety of applications, including image and video analysis, robotics, and human-computer interaction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.4338720142841339,
                    "sentence": "Arguments pro acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.830994725227356,
                    "sentence": "* The paper proposes a novel and theoretically grounded model of visual attention.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8831611275672913,
                    "sentence": "* The model is evaluated on multiple datasets and shows promising results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8951109051704407,
                    "sentence": "* The paper provides a clear and concise explanation of the mathematical framework and the experimental setup.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5906960368156433,
                    "sentence": "Arguments con acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7542226314544678,
                    "sentence": "* The model is based on a simplification of the brightness invariance term, which may not be accurate in all cases.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7796846628189087,
                    "sentence": "* The parameters of the model are estimated using a simulated annealing algorithm, which may not be the most efficient or effective method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.79741370677948,
                    "sentence": "* The model is not directly compared to other models of visual attention that are based on different principles.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.29460635781288147,
                    "sentence": "Recommendation: Accept with minor revisions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9268601536750793,
                    "sentence": "The authors should consider addressing the limitations and weaknesses mentioned above, such as providing a more detailed comparison with other models of visual attention and exploring alternative methods for parameter estimation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9313236474990845,
                    "sentence": "Additionally, the authors should consider providing more insights into the potential applications and implications of the model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.45887534985363754
                }
            ],
            "completely_generated_prob": 0.8596764774936939,
            "class_probabilities": {
                "human": 0.1099278184806362,
                "ai": 0.8596764774936939,
                "mixed": 0.030395704025669885
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8596764774936939,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8596764774936939,
                    "human": 0.1099278184806362,
                    "mixed": 0.030395704025669885
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper proposes a novel model of visual attention based on the principle of least action, which is a fundamental concept in physics. The model, called EYe MOvement Laws (EYMOL), is derived from a generalized Lagrangian that incorporates three key principles: boundedness of the trajectory, curiosity-driven movement, and brightness invariance. The resulting Euler-Lagrange equations describe the dynamics of visual attention and can be used to predict human fixations.\nThe paper is well-written, and the authors provide a clear and concise explanation of the mathematical framework and the experimental setup. The model is evaluated on several datasets, including images and videos, and the results show that it competes well with state-of-the-art models in saliency detection.\nThe strengths of the paper include:\n* The proposal of a novel and theoretically grounded model of visual attention that integrates multiple principles.\n* The use of a variational approach to derive the model, which provides a clear and consistent mathematical framework.\n* The evaluation of the model on multiple datasets, which demonstrates its robustness and generalizability.\nHowever, there are also some weaknesses and limitations:\n* The model is based on a simplification of the brightness invariance term, which may not be accurate in all cases.\n* The parameters of the model are estimated using a simulated annealing algorithm, which may not be the most efficient or effective method.\n* The model is not directly compared to other models of visual attention that are based on different principles, such as information maximization or surprise.\nOverall, the paper presents a significant contribution to the field of visual attention and provides a new perspective on the underlying mechanisms. The model has the potential to be useful in a variety of applications, including image and video analysis, robotics, and human-computer interaction.\nArguments pro acceptance:\n* The paper proposes a novel and theoretically grounded model of visual attention.\n* The model is evaluated on multiple datasets and shows promising results.\n* The paper provides a clear and concise explanation of the mathematical framework and the experimental setup.\nArguments con acceptance:\n* The model is based on a simplification of the brightness invariance term, which may not be accurate in all cases.\n* The parameters of the model are estimated using a simulated annealing algorithm, which may not be the most efficient or effective method.\n* The model is not directly compared to other models of visual attention that are based on different principles.\nRecommendation: Accept with minor revisions. The authors should consider addressing the limitations and weaknesses mentioned above, such as providing a more detailed comparison with other models of visual attention and exploring alternative methods for parameter estimation. Additionally, the authors should consider providing more insights into the potential applications and implications of the model."
        }
    ]
}