{
    "version": "2025-01-09-base",
    "scanId": "fcbb2489-7540-47da-b0db-a1fa5cad2c42",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999087929725647,
                    "sentence": "This paper explores theoretical limitations of variance-reduced stochastic algorithms, including SAG, SDCA, SAGA, and SVRG, which have significantly impacted machine learning and convex optimization by achieving linear convergence for strongly convex problems with Lipschitz gradients.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988292455673218,
                    "sentence": "Notably, these algorithms converge at a rate dependent on $n + \\kappa$ rather than $n\\kappa$.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989081025123596,
                    "sentence": "One key finding is that sequential iterative algorithms require knowledge of the individual function returned by the oracle to attain linear convergence rates.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989038109779358,
                    "sentence": "This is intuitively consistent with the need for algorithms like SAG and SAGA to update their gradient memory buffers based on the function index.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988990426063538,
                    "sentence": "The paper also discusses the limitations of applying variance-reduced methods in conjunction with data augmentation during learning, although this limitation only applies when the data augmentation is stochastic and not based on a finite set of augmented samples.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989351034164429,
                    "sentence": "Another significant result concerns the acceleration of variance-reduced algorithms in the sense of Nesterov, aiming to improve the convergence rate from $\\kappa$ to $\\sqrt{\\kappa}$.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999062180519104,
                    "sentence": "It is demonstrated that \"oblivious\" algorithms, characterized by update rules that are fixed on average for any iteration, cannot be accelerated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987446069717407,
                    "sentence": "A practical implication of one of the results is that knowing the strong convexity parameter is necessary for achieving accelerated convergence when the algorithm is stationary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989134073257446,
                    "sentence": "The analysis of oblivious algorithms is further extended using restart techniques, which are known to mitigate the non-adaptive nature of accelerated algorithms when local conditioning is unknown.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9978725910186768,
                    "sentence": "Overall, the paper is well-written and pedagogically sound.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983952045440674,
                    "sentence": "However, there are minor issues, such as the definition of CLI being used before it is formally defined in Definition 2, and a typo in Line 224, where \"let us we describe\" should be corrected to \"let us describe\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper explores theoretical limitations of variance-reduced stochastic algorithms, including SAG, SDCA, SAGA, and SVRG, which have significantly impacted machine learning and convex optimization by achieving linear convergence for strongly convex problems with Lipschitz gradients. Notably, these algorithms converge at a rate dependent on $n + \\kappa$ rather than $n\\kappa$.\nOne key finding is that sequential iterative algorithms require knowledge of the individual function returned by the oracle to attain linear convergence rates. This is intuitively consistent with the need for algorithms like SAG and SAGA to update their gradient memory buffers based on the function index.\nThe paper also discusses the limitations of applying variance-reduced methods in conjunction with data augmentation during learning, although this limitation only applies when the data augmentation is stochastic and not based on a finite set of augmented samples.\nAnother significant result concerns the acceleration of variance-reduced algorithms in the sense of Nesterov, aiming to improve the convergence rate from $\\kappa$ to $\\sqrt{\\kappa}$. It is demonstrated that \"oblivious\" algorithms, characterized by update rules that are fixed on average for any iteration, cannot be accelerated.\nA practical implication of one of the results is that knowing the strong convexity parameter is necessary for achieving accelerated convergence when the algorithm is stationary.\nThe analysis of oblivious algorithms is further extended using restart techniques, which are known to mitigate the non-adaptive nature of accelerated algorithms when local conditioning is unknown.\nOverall, the paper is well-written and pedagogically sound. However, there are minor issues, such as the definition of CLI being used before it is formally defined in Definition 2, and a typo in Line 224, where \"let us we describe\" should be corrected to \"let us describe\"."
        }
    ]
}