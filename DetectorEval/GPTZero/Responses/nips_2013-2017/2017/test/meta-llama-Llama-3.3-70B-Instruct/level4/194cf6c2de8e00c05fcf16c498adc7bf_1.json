{
    "version": "2025-01-09-base",
    "scanId": "3d832a5e-c879-434d-b701-d4a9716ba608",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9997209310531616,
                    "sentence": "This computational neuroscience paper presents a novel bottom-up visual feature saliency model, incorporating dynamical systems modelling to capture scanpath dynamics.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997184872627258,
                    "sentence": "The paper offers an engaging read, introducing a range of intriguing concepts, including the definition of \"affordances\" (such as curiosity and brightness invariance) that influence scanpath generation, as well as the application of the Least Action Principle.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997605085372925,
                    "sentence": "Although these \"affordances\" are somewhat intuitively introduced and lack further justification, they contribute to the paper's overall theoretical framework.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999792754650116,
                    "sentence": "In the context of the Least Action Principle, a more detailed discussion of its relevance to existing research in eye movement studies, such as the work conducted by the Wolpert lab, would have been beneficial.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999798595905304,
                    "sentence": "While the absence of biological motivation may not be a significant concern in the current era of deep learning models for static visual salience, the model's inconsistent performance in comparison to these benchmarks is a notable limitation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996209740638733,
                    "sentence": "Given the model's claim to capture biological eye movement dynamics, it would have been valuable to include detailed characterizations of the generated scanpaths in the main paper, extending beyond static metrics to provide a more comprehensive understanding of the model's capabilities.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This computational neuroscience paper presents a novel bottom-up visual feature saliency model, incorporating dynamical systems modelling to capture scanpath dynamics. The paper offers an engaging read, introducing a range of intriguing concepts, including the definition of \"affordances\" (such as curiosity and brightness invariance) that influence scanpath generation, as well as the application of the Least Action Principle. Although these \"affordances\" are somewhat intuitively introduced and lack further justification, they contribute to the paper's overall theoretical framework. \nIn the context of the Least Action Principle, a more detailed discussion of its relevance to existing research in eye movement studies, such as the work conducted by the Wolpert lab, would have been beneficial. While the absence of biological motivation may not be a significant concern in the current era of deep learning models for static visual salience, the model's inconsistent performance in comparison to these benchmarks is a notable limitation. Given the model's claim to capture biological eye movement dynamics, it would have been valuable to include detailed characterizations of the generated scanpaths in the main paper, extending beyond static metrics to provide a more comprehensive understanding of the model's capabilities."
        }
    ]
}