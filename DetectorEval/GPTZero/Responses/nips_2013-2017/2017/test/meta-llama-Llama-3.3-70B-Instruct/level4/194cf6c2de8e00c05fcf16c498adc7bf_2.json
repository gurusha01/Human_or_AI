{
    "version": "2025-01-09-base",
    "scanId": "80750883-b780-4971-b2fd-0429c832c8ba",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999995231628418,
                    "sentence": "Review: Variational Laws of Visual Attention for Dynamic Scenes",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999994039535522,
                    "sentence": "This paper explores the locations in static and dynamic images that capture human attention.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992251396179,
                    "sentence": "The authors propose a model based on three fundamental principles of visual attention, formulated as an energy function that guides eye movements: (1) a harmonic oscillator constraint at the image borders, considering the limited retina size; (2) a \"curiosity-driven principle\" that emphasizes regions with significant brightness changes in both fine and blurred image versions; and (3) brightness invariance, which increases with changes in brightness.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999992847442627,
                    "sentence": "By deriving a cost function from these principles, the authors obtain differential equations that predict eye movements across static or dynamic images, depending on the initial location and velocity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999989867210388,
                    "sentence": "The technique is evaluated quantitatively on datasets of static and dynamic scenes with accompanying human eye movements, demonstrating comparable performance to state-of-the-art methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991059303284,
                    "sentence": "The formal definition of saliency and modeling of eye movements are crucial issues in computational vision and cognitive science.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999991059303284,
                    "sentence": "Psychologists have long struggled with vague definitions of saliency, and the authors' novel model offers a potential solution to understanding what makes something salient and provides a formal framework for eye movements within the bottom-up tradition.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999993443489075,
                    "sentence": "Although the method may not excel in every metric across all datasets, it performs well and presents a unique perspective on the problem.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8657625913619995,
                    "sentence": "However, some of my understanding is based on inference, as the paper is poorly written and difficult to follow.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8335679769515991,
                    "sentence": "I suggest that the authors have their work proofread by others and expand on abbreviations used in equations and Section 3.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.7809094786643982,
                    "sentence": "Additionally, I recommend moving the two-page Appendix to supplementary material and utilizing the extra pages to define each variable and function, even if they are conventional within the field, to facilitate understanding for the diverse NIPS audience.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.5666801333427429,
                    "sentence": "As a psychologist, I would have appreciated a connection between the authors' work and the existing psychological literature on eye movements as optimal information-gathering strategies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.39310595393180847,
                    "sentence": "For example, Najemnik and Geisler (2008) demonstrate that human eye movement statistics are consistent with an optimal search strategy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.3600829541683197,
                    "sentence": "I would be interested in seeing a discussion on how the authors' approach compares to these findings, whether it provides independent information that could be integrated to produce a better model, or if it offers a restatement of similar models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.45887534985363754
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                }
            ],
            "completely_generated_prob": 0.5602423080170303,
            "class_probabilities": {
                "human": 0.4374690933651747,
                "ai": 0.5602423080170303,
                "mixed": 0.002288598617794985
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.5602423080170303,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.5602423080170303,
                    "human": 0.4374690933651747,
                    "mixed": 0.002288598617794985
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Review: Variational Laws of Visual Attention for Dynamic Scenes\nThis paper explores the locations in static and dynamic images that capture human attention. The authors propose a model based on three fundamental principles of visual attention, formulated as an energy function that guides eye movements: (1) a harmonic oscillator constraint at the image borders, considering the limited retina size; (2) a \"curiosity-driven principle\" that emphasizes regions with significant brightness changes in both fine and blurred image versions; and (3) brightness invariance, which increases with changes in brightness. By deriving a cost function from these principles, the authors obtain differential equations that predict eye movements across static or dynamic images, depending on the initial location and velocity. The technique is evaluated quantitatively on datasets of static and dynamic scenes with accompanying human eye movements, demonstrating comparable performance to state-of-the-art methods.\nThe formal definition of saliency and modeling of eye movements are crucial issues in computational vision and cognitive science. Psychologists have long struggled with vague definitions of saliency, and the authors' novel model offers a potential solution to understanding what makes something salient and provides a formal framework for eye movements within the bottom-up tradition. Although the method may not excel in every metric across all datasets, it performs well and presents a unique perspective on the problem.\nHowever, some of my understanding is based on inference, as the paper is poorly written and difficult to follow. I suggest that the authors have their work proofread by others and expand on abbreviations used in equations and Section 3. Additionally, I recommend moving the two-page Appendix to supplementary material and utilizing the extra pages to define each variable and function, even if they are conventional within the field, to facilitate understanding for the diverse NIPS audience.\nAs a psychologist, I would have appreciated a connection between the authors' work and the existing psychological literature on eye movements as optimal information-gathering strategies. For example, Najemnik and Geisler (2008) demonstrate that human eye movement statistics are consistent with an optimal search strategy. I would be interested in seeing a discussion on how the authors' approach compares to these findings, whether it provides independent information that could be integrated to produce a better model, or if it offers a restatement of similar models."
        }
    ]
}