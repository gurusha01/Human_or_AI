{
    "version": "2025-01-09-base",
    "scanId": "58c696ef-670e-4831-8d9a-528fcf56693c",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999667406082153,
                    "sentence": "This paper introduces an asynchronous variant of SAGA, a stochastic gradient method originally proposed for finite-sum convex optimization (Defazio et al., 2014).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999619722366333,
                    "sentence": "The paper, as I understand it, tackles two key challenges: asynchronicity and proximity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999772310256958,
                    "sentence": "The authors present three notable advancements over the original SAGA.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999489784240723,
                    "sentence": "First, they introduce a sparse update mechanism based on block coordinate updates, leveraging extended support for gradients when the regularization is decomposable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999223947525024,
                    "sentence": "Second, they develop an asynchronous version of SAGA that tolerates delays of up to 鈭⻭/10.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999222755432129,
                    "sentence": "Lastly, they extend the method to handle nonsmooth regularizers through the use of proximal operators.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998733997344971,
                    "sentence": "Regarding convergence, the proposed method retains a linear convergence rate under the strong convexity of the overall objective function f and the individual Lipschitz continuity of the gradients of fi.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997842311859131,
                    "sentence": "Although the step size is slightly reduced compared to the original SAGA, the convergence factor remains comparable, likely due to differing assumptions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997912645339966,
                    "sentence": "The paper effectively integrates several advanced techniques from prior works, including SAGA with variance reduction, sparse updates, Hogwild, and asynchronous methods like AROCK, to achieve improvements over these approaches.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997321367263794,
                    "sentence": "The proofs are well-structured and clearly presented.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995969533920288,
                    "sentence": "In my view, these contributions are both significant and practically relevant for obvious reasons.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997761845588684,
                    "sentence": "The numerical experiments strongly corroborate the theoretical findings, demonstrating a linear speedup of the proposed algorithm on three large-scale datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998379945755005,
                    "sentence": "Overall, this paper makes substantial contributions in both theoretical and experimental aspects and deserves acceptance at NIPS.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998385310173035,
                    "sentence": "Minor Comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997838735580444,
                    "sentence": "- Some concepts and notations should be explicitly defined, such as support (supp) and 惟(路).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996022582054138,
                    "sentence": "Additionally, there are instances of inconsistent phrasing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998351335525513,
                    "sentence": "- Ensure consistent usage of the term \"step size\" throughout the text (e.g., lines 46 and 142).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998863935470581,
                    "sentence": "- Line 152: Remove either \"a\" or \"the.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999755322933197,
                    "sentence": "- Line 361: The variable \"y\" should be in bold.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998831748962402,
                    "sentence": "- Line 127: Remove one instance of \"a.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper introduces an asynchronous variant of SAGA, a stochastic gradient method originally proposed for finite-sum convex optimization (Defazio et al., 2014). The paper, as I understand it, tackles two key challenges: asynchronicity and proximity. The authors present three notable advancements over the original SAGA. First, they introduce a sparse update mechanism based on block coordinate updates, leveraging extended support for gradients when the regularization is decomposable. Second, they develop an asynchronous version of SAGA that tolerates delays of up to 鈭歯/10. Lastly, they extend the method to handle nonsmooth regularizers through the use of proximal operators. \nRegarding convergence, the proposed method retains a linear convergence rate under the strong convexity of the overall objective function f and the individual Lipschitz continuity of the gradients of fi. Although the step size is slightly reduced compared to the original SAGA, the convergence factor remains comparable, likely due to differing assumptions. \nThe paper effectively integrates several advanced techniques from prior works, including SAGA with variance reduction, sparse updates, Hogwild, and asynchronous methods like AROCK, to achieve improvements over these approaches. The proofs are well-structured and clearly presented. \nIn my view, these contributions are both significant and practically relevant for obvious reasons. The numerical experiments strongly corroborate the theoretical findings, demonstrating a linear speedup of the proposed algorithm on three large-scale datasets. \nOverall, this paper makes substantial contributions in both theoretical and experimental aspects and deserves acceptance at NIPS.\nMinor Comments: \n- Some concepts and notations should be explicitly defined, such as support (supp) and 惟(路). Additionally, there are instances of inconsistent phrasing. \n- Ensure consistent usage of the term \"step size\" throughout the text (e.g., lines 46 and 142). \n- Line 152: Remove either \"a\" or \"the.\" \n- Line 361: The variable \"y\" should be in bold. \n- Line 127: Remove one instance of \"a.\""
        }
    ]
}