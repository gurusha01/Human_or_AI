{
    "version": "2025-01-09-base",
    "scanId": "d9887d26-69de-4137-b528-d0d39d8d053b",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998037815093994,
                    "sentence": "This paper introduces an attention-based model designed for action recognition and human-object interaction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997904300689697,
                    "sentence": "The model can operate with or without additional supervision and achieves accuracy improvements with minimal increases in network size and computational cost.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997949600219727,
                    "sentence": "The authors provide a comprehensive empirical and analytical evaluation of the attention module and propose a derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods, which are commonly used in fine-grained classification tasks.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998140931129456,
                    "sentence": "Experimental results on three benchmark datasets validate the effectiveness of the proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992195963859558,
                    "sentence": "A notable strength of this work is its ability to learn attention maps in an unsupervised manner.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988452196121216,
                    "sentence": "These attention maps offer interpretability by highlighting where the network focuses, leveraging both bottom-up saliency and top-down attention mechanisms.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9983382225036621,
                    "sentence": "This eliminates the need for bounding box detection, which is typically required in hard attention methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9945827722549438,
                    "sentence": "Overall, this is an interesting and well-motivated study.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.997231125831604,
                    "sentence": "The paper is well-written and presents its ideas clearly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9976652264595032,
                    "sentence": "However, one major concern remains:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979735612869263,
                    "sentence": "In second-order pooling, the fully-connected weight matrix is approximated as the product of two rank-1 vectors.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986947774887085,
                    "sentence": "How does this approximation impact information retention compared to the original matrix?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990570545196533,
                    "sentence": "Does this loss of information affect the quality of the attention maps?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990274906158447,
                    "sentence": "Additional theoretical analysis and further discussion on this aspect would strengthen the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990403652191162,
                    "sentence": "Minor issues:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993056654930115,
                    "sentence": "- Page 3: \"network architecture that incorporate(s) this attention module and explore(s) a pose...\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988263845443726,
                    "sentence": "- Page 5: \"It consist(s) of a stack of modules\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986347556114197,
                    "sentence": "- Page 8: \"... image resized to 450px at(as) input time did\"?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 1,
            "class_probabilities": {
                "human": 0,
                "ai": 1,
                "mixed": 0
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 1,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 1,
                    "human": 0,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper introduces an attention-based model designed for action recognition and human-object interaction. The model can operate with or without additional supervision and achieves accuracy improvements with minimal increases in network size and computational cost. The authors provide a comprehensive empirical and analytical evaluation of the attention module and propose a derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods, which are commonly used in fine-grained classification tasks. Experimental results on three benchmark datasets validate the effectiveness of the proposed approach.\nA notable strength of this work is its ability to learn attention maps in an unsupervised manner. These attention maps offer interpretability by highlighting where the network focuses, leveraging both bottom-up saliency and top-down attention mechanisms. This eliminates the need for bounding box detection, which is typically required in hard attention methods.\nOverall, this is an interesting and well-motivated study. The paper is well-written and presents its ideas clearly. However, one major concern remains:\nIn second-order pooling, the fully-connected weight matrix is approximated as the product of two rank-1 vectors. How does this approximation impact information retention compared to the original matrix? Does this loss of information affect the quality of the attention maps? Additional theoretical analysis and further discussion on this aspect would strengthen the paper.\nMinor issues: \n- Page 3: \"network architecture that incorporate(s) this attention module and explore(s) a pose ...\" \n- Page 5: \"It consist(s) of a stack of modules\" \n- Page 8: \"... image resized to 450px at(as) input time did\"?"
        }
    ]
}