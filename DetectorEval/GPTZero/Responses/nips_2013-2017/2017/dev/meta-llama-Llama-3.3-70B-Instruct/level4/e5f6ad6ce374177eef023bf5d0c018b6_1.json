{
    "version": "2025-01-09-base",
    "scanId": "db9f181a-a5c7-46ea-a167-d17abf19df33",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999383687973022,
                    "sentence": "This paper presents a novel architecture for conditional video generation, building upon convolutional LSTMs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998077154159546,
                    "sentence": "The key insight is that the traditional hierarchical representation, where each layer captures increasingly abstract scene properties, may not be optimal for video generation, as generative models require preserving precise object position information even at the output layer.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999760091304779,
                    "sentence": "The proposed PredRNN model extends convolutional LSTMs by incorporating two memory cells: one that flows through time at the same layer and another that flows up through the layers.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998549818992615,
                    "sentence": "The authors evaluate their model on the moving MNIST digits and KTH action recognition datasets, demonstrating superior mean squared error (MSE) performance compared to prior work.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997053742408752,
                    "sentence": "The paper is well-written and provides a thorough review of relevant literature.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993423819541931,
                    "sentence": "The introduced architecture is both novel and intriguing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994634985923767,
                    "sentence": "However, the comparison to prior work could be further strengthened to better understand the empirical significance of the results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997095465660095,
                    "sentence": "For instance, the work by Kalchbrenner (2016) claims to achieve near-optimal performance on the MNIST digit task but is not included as a baseline.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999599039554596,
                    "sentence": "Previous studies on MNIST, such as Kalchbrenner (2016) and Srivastava (2015), report cross-entropy loss, whereas the authors focus on maximum likelihood output.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984409809112549,
                    "sentence": "To facilitate comparison with prior work, it would be beneficial to also report the likelihood.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9990239143371582,
                    "sentence": "Moreover, while the computational complexity is mentioned as an advantage, a detailed analysis or comparison with prior work is lacking, making it challenging to assess the model's computational efficiency.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993283152580261,
                    "sentence": "A minor notational suggestion is to use \"M\" instead of \"C\" for the cell state in equation 3 to clarify the connection with equation 4.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996872544288635,
                    "sentence": "After reviewing the author's response, I believe the paper is stronger due to the planned comparison with prior work, which I assume will be included in the final version.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995986223220825,
                    "sentence": "However, I still have concerns regarding the training loss used in this model (MSE) and whether it provides an unfair advantage when evaluating performance using MSE, as other approaches may employ different losses.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 6,
                    "completely_generated_prob": 0.9000234362273952
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9997862822885396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997862822885396,
                "mixed": 0.00021371771146045916
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997862822885396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997862822885396,
                    "human": 0,
                    "mixed": 0.00021371771146045916
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a novel architecture for conditional video generation, building upon convolutional LSTMs. The key insight is that the traditional hierarchical representation, where each layer captures increasingly abstract scene properties, may not be optimal for video generation, as generative models require preserving precise object position information even at the output layer. The proposed PredRNN model extends convolutional LSTMs by incorporating two memory cells: one that flows through time at the same layer and another that flows up through the layers. The authors evaluate their model on the moving MNIST digits and KTH action recognition datasets, demonstrating superior mean squared error (MSE) performance compared to prior work. The paper is well-written and provides a thorough review of relevant literature.\nThe introduced architecture is both novel and intriguing. However, the comparison to prior work could be further strengthened to better understand the empirical significance of the results. For instance, the work by Kalchbrenner (2016) claims to achieve near-optimal performance on the MNIST digit task but is not included as a baseline. Previous studies on MNIST, such as Kalchbrenner (2016) and Srivastava (2015), report cross-entropy loss, whereas the authors focus on maximum likelihood output. To facilitate comparison with prior work, it would be beneficial to also report the likelihood. Moreover, while the computational complexity is mentioned as an advantage, a detailed analysis or comparison with prior work is lacking, making it challenging to assess the model's computational efficiency.\nA minor notational suggestion is to use \"M\" instead of \"C\" for the cell state in equation 3 to clarify the connection with equation 4.\nAfter reviewing the author's response, I believe the paper is stronger due to the planned comparison with prior work, which I assume will be included in the final version. However, I still have concerns regarding the training loss used in this model (MSE) and whether it provides an unfair advantage when evaluating performance using MSE, as other approaches may employ different losses."
        }
    ]
}