{
    "version": "2025-01-09-base",
    "scanId": "9430fcb9-1af6-4599-8bca-7e8a48533256",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0010799337178468704,
                    "sentence": "Summary:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0021026977337896824,
                    "sentence": "Rademacher complexity is a powerful tool for producing generalization guarantees.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002174209337681532,
                    "sentence": "The Rademacher complexity of a class H on a sample S is roughly the expected maximum gap between training and test performance if S were randomly partitioned into training and test sets, and we took the maximum over all h in H (eqn 11).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0026868004351854324,
                    "sentence": "This paper makes the observation that the core steps in the standard generalization bound proof will still go through if instead of taking the max over all h in H, you only look at the h's that your procedure can possibly output when given half of a double sample (Lemma 1).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0027299518696963787,
                    "sentence": "While unfortunately the usual final (or initial) high-probability step in this argument does not seem to go through directly, the paper shows (Theorem 2) that one can nonetheless get a useful generation bound from this using other means.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002690179506316781,
                    "sentence": "The paper then shows how this generalization bound yields good sample complexity guarantees for a number of natural auction classes.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0023320065811276436,
                    "sentence": "In several cases, this improves over the best prior guarantees known, and also simplifies their analysis.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0022423367481678724,
                    "sentence": "Evaluation:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002223302610218525,
                    "sentence": "While the core idea is to some extent an observation, I like the paper because it is a nice, cute idea that one could actually teach in a class.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0018159272149205208,
                    "sentence": "And it allows for a simpler sample complexity analysis for auctions, which is great.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0025059243198484182,
                    "sentence": "It may also have other uses too.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0018882722361013293,
                    "sentence": "On the negative side, I wish the paper were able to answer whether the linear dependence on 1/delta is really necessary (see below).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0020644082687795162,
                    "sentence": "That would make the paper feel more complete to me.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001485452870838344,
                    "sentence": "Still, I am overall positive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0023428204003721476,
                    "sentence": "Questions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0041165840812027454,
                    "sentence": "Do you know if the linear dependence on delta in Theorem 2 is required?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0029504636768251657,
                    "sentence": "I see why you need it in your proof (due to using Markov) and you explain nicely why the usual use of McDiarmid to get an O(log(1/delta)) bound doesn't go through.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0014186184853315353,
                    "sentence": "But this leaves open whether (a) there is a different way to get an O(log(1/delta)) bound, or on the other hand (b) there exist D,H for which you indeed have a more heavy-tailed chance of failure.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0018627269892022014,
                    "sentence": "If this were resolved, the paper would feel more complete.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002462813165038824,
                    "sentence": "On a related note, do you know if you can get a bound along the lines of Theorem 2 in terms of the maximum of hypotheses that can be produced from half of the actual sample S?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004769529215991497,
                    "sentence": "(That is, replacing the sup over S of size m in the definition of \\hat{\\tau} with the actual training sample S?)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00020314916037023067,
                    "sentence": "Suggestions:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00022748134506400675,
                    "sentence": "- If I am not mistaken, The proof of Lemma 1 basically goes through the textbook Rademacher bound proof, changing it where needed to replace H with \\hat{H}_{S \\union S'}.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0002433765766909346,
                    "sentence": "That's fine, but you should explicitly tell the reader that that is what you are doing up front, so they know what's new vs what's old.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0003299275995232165,
                    "sentence": "E.g., you have a nice discussion of this form for Theorem 2, where you talk about where it differs from the usual argument, that I think is quite useful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0005668438388966024,
                    "sentence": "- In Section 4 you examine several interesting scenarios (single bidder, multiple iid regular,...), and for each one you say \"In this case, the space of hypotheses H is...\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.000704405247233808,
                    "sentence": "I think what you mean to say is more that \"In this case, it is known that an optimal auction belongs to the hypothesis class H of...\".",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0008568066405132413,
                    "sentence": "In other words, the generalization guarantees apply to any H you want, but now you are saying what the meaningful H is in each scenario, right?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00098902836907655,
                    "sentence": "I think this is worth saying explicitly, since otherwise the reader might get confused why H is changing each time.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0012286985293030739,
                    "sentence": "Also I'll point out that the idea of counting the number of possible outputs of an auction mechanism in order to get a sample complexity bound has been used in the auction literature before.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0022641525138169527,
                    "sentence": "The first I know of is the FOCS 2005 paper \"Mechanism design via machine learning\" by Balcan, Blum, Hartline, and Mansour, though that paper operates in a transductive setting where an argument of this type is much easier to make.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 6,
                    "completely_generated_prob": 1.474742012248794e-05
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 6,
                    "completely_generated_prob": 1.474742012248794e-05
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 29,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.009180599824664711,
            "class_probabilities": {
                "human": 0.9907801111847393,
                "ai": 0.009180599824664711,
                "mixed": 3.928899059593116e-05
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9907801111847393,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.009180599824664711,
                    "human": 0.9907801111847393,
                    "mixed": 3.928899059593116e-05
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Summary:\nRademacher complexity is a powerful tool for producing generalization guarantees. The Rademacher complexity of a class H on a sample S is roughly the expected maximum gap between training and test performance if S were randomly partitioned into training and test sets, and we took the maximum over all h in H (eqn 11). This paper makes the observation that the core steps in the standard generalization bound proof will still go through if instead of taking the max over all h in H, you only look at the h's that your procedure can possibly output when given half of a double sample (Lemma 1). While unfortunately the usual final (or initial) high-probability step in this argument does not seem to go through directly, the paper shows (Theorem 2) that one can nonetheless get a useful generation bound from this using other means. The paper then shows how this generalization bound yields good sample complexity guarantees for a number of natural auction classes. In several cases, this improves over the best prior guarantees known, and also simplifies their analysis.\nEvaluation:\nWhile the core idea is to some extent an observation, I like the paper because it is a nice, cute idea that one could actually teach in a class. And it allows for a simpler sample complexity analysis for auctions, which is great. It may also have other uses too. On the negative side, I wish the paper were able to answer whether the linear dependence on 1/delta is really necessary (see below). That would make the paper feel more complete to me. Still, I am overall positive.\nQuestions:\nDo you know if the linear dependence on delta in Theorem 2 is required? I see why you need it in your proof (due to using Markov) and you explain nicely why the usual use of McDiarmid to get an O(log(1/delta)) bound doesn't go through. But this leaves open whether (a) there is a different way to get an O(log(1/delta)) bound, or on the other hand (b) there exist D,H for which you indeed have a more heavy-tailed chance of failure. If this were resolved, the paper would feel more complete.\nOn a related note, do you know if you can get a bound along the lines of Theorem 2 in terms of the maximum of hypotheses that can be produced from half of the actual sample S? (That is, replacing the sup over S of size m in the definition of \\hat{\\tau} with the actual training sample S?)\nSuggestions: \n- If I am not mistaken, The proof of Lemma 1 basically goes through the textbook Rademacher bound proof, changing it where needed to replace H with \\hat{H}_{S \\union S'}. That's fine, but you should explicitly tell the reader that that is what you are doing up front, so they know what's new vs what's old. E.g., you have a nice discussion of this form for Theorem 2, where you talk about where it differs from the usual argument, that I think is quite useful.\n- In Section 4 you examine several interesting scenarios (single bidder, multiple iid regular,...) , and for each one you say \"In this case, the space of hypotheses H is ...\". I think what you mean to say is more that \"In this case, it is known that an optimal auction belongs to the hypothesis class H of ...\". In other words, the generalization guarantees apply to any H you want, but now you are saying what the meaningful H is in each scenario, right? I think this is worth saying explicitly, since otherwise the reader might get confused why H is changing each time.\nAlso I'll point out that the idea of counting the number of possible outputs of an auction mechanism in order to get a sample complexity bound has been used in the auction literature before. The first I know of is the FOCS 2005 paper \"Mechanism design via machine learning\" by Balcan, Blum, Hartline, and Mansour, though that paper operates in a transductive setting where an argument of this type is much easier to make."
        }
    ]
}