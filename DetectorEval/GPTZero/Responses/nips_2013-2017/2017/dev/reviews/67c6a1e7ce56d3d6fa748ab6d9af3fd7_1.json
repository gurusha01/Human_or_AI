{
    "version": "2025-01-09-base",
    "scanId": "f5f1ffbe-dffa-4aef-957a-71b60392b5cd",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.5271103382110596,
                    "sentence": "This paper presents an attention-based model for action recognition and human object interaction.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.31383827328681946,
                    "sentence": "The model can be guided by extra supervision or not.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3432811200618744,
                    "sentence": "It achieves accuracy improvements without increasing the network size and computational cost a lot.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3746708035469055,
                    "sentence": "Authors provide an extensively empirical and analytical analysis of the attention module, and they further introduce a derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods for fine-grained classification typically.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.46655088663101196,
                    "sentence": "Experiments on three benchmarks have demonstrated the efficacy of the proposed method.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.2473846971988678,
                    "sentence": "One key advantage is to learn the attention map in an unsupervised manner.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3281516432762146,
                    "sentence": "To make a prediction, the attention maps can provide insight into where the network should look in terms of both bottom-up saliency and top-down attention.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.29617103934288025,
                    "sentence": "This allows it to get rid of detecting the bounding box usually required in hard attention.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.31026986241340637,
                    "sentence": "This is an interesting work, and the idea sounds well motivated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.1804323047399521,
                    "sentence": "The paper reads well too.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.3045370578765869,
                    "sentence": "One major concern is:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6254462003707886,
                    "sentence": "The fully-connected weight matrix in second-order pooling is approximated by the product of two vectors with rank-1.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5790886878967285,
                    "sentence": "Then, how about the information loss compared the original one?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.567833423614502,
                    "sentence": "Does such loss affect the attention map?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.46251922845840454,
                    "sentence": "Some theoretical analysis and more discussions are expected.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.49170324206352234,
                    "sentence": "Minor issues:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7611430287361145,
                    "sentence": "Page 3, \"network architecture that incorporate(s) this attention module and explore(s) a pose...\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7172786593437195,
                    "sentence": "Page 5, \"It consist(s) of a stack of modules\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8055569529533386,
                    "sentence": "Page 8, \"... image resized to 450px at(as) input time did\"?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.4946996466431095,
            "class_probabilities": {
                "human": 0.5053003533568904,
                "ai": 0.4946996466431095,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5053003533568904,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.4946996466431095,
                    "human": 0.5053003533568904,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents an attention-based model for action recognition and human object interaction. The model can be guided by extra supervision or not. It achieves accuracy improvements without increasing the network size and computational cost a lot. Authors provide an extensively empirical and analytical analysis of the attention module, and they further introduce a derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods for fine-grained classification typically. Experiments on three benchmarks have demonstrated the efficacy of the proposed method. \nOne key advantage is to learn the attention map in an unsupervised manner. To make a prediction, the attention maps can provide insight into where the network should look in terms of both bottom-up saliency and top-down attention. This allows it to get rid of detecting the bounding box usually required in hard attention.\nThis is an interesting work, and the idea sounds well motivated. The paper reads well too. One major concern is: \nThe fully-connected weight matrix in second-order pooling is approximated by the product of two vectors with rank-1. Then, how about the information loss compared the original one? Does such loss affect the attention map? Some theoretical analysis and more discussions are expected.\nMinor issues: \nPage 3, \"network architecture that incorporate(s) this attention module and explore(s) a pose ...\"\nPage 5, \"It consist(s) of a stack of modules\"\nPage 8, \"... image resized to 450px at(as) input time did\"?"
        }
    ]
}