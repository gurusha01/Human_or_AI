{
    "version": "2025-01-09-base",
    "scanId": "207d270e-a9ed-4290-80b8-98ac27b094fd",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.802727997303009,
                    "sentence": "The paper proposes an approach to train predictive models performance of which is not based on classic likelihood objectives, but is instead based on performance on some task external to the model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8120871782302856,
                    "sentence": "In order to achieve this, the model parameters are optimized so as to minimize the loss on external task, which in turn may involve a sub-optimization problem that depends on model parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8461463451385498,
                    "sentence": "A synthetic and real-data experiments are presented that clearly illustrate the usefulness of proposed approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6500309705734253,
                    "sentence": "The introduction is very well-motivated and the exposition is generally clear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7181277871131897,
                    "sentence": "The paper is technically sound and builds on sound foundations - I see no obvious flaws.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7723358869552612,
                    "sentence": "The paper contains good motivation about why the proposed approach is necessary.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7311749458312988,
                    "sentence": "I see this work as a worthwhile and well-motivated application of existing technical contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.6382827162742615,
                    "sentence": "The important technical piece that make this approach possible, which is differentiation though an argmax are already presented in [Amos 2016].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5642234086990356,
                    "sentence": "While building on existing results, this work applies them in a very relevant and well-motivated formulation of task-based learning and I believe would be of interest to the machine learning community.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.5827398896217346,
                    "sentence": "The proposed benefit, but also a potential issue of the proposed approach is that the model is not independent of the task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.7116457223892212,
                    "sentence": "Is is possible to characterize how much the model may be overfitting to the specific task and whether it generalizes well to a different (potentially only slightly different) task?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.8044750094413757,
                    "sentence": "It would be good to reference related work on meta-learning \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\" by Finn et al, as that's another example of differentiation through an optimization procedure.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.49770307481643766,
            "class_probabilities": {
                "human": 0.5021818426438073,
                "ai": 0.49770307481643766,
                "mixed": 0.00011508253975490195
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.5021818426438073,
            "confidence_category": "low",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.49770307481643766,
                    "human": 0.5021818426438073,
                    "mixed": 0.00011508253975490195
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly uncertain about this document. The writing style and content are not particularly AI-like.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper proposes an approach to train predictive models performance of which is not based on classic likelihood objectives, but is instead based on performance on some task external to the model. In order to achieve this, the model parameters are optimized so as to minimize the loss on external task, which in turn may involve a sub-optimization problem that depends on model parameters. A synthetic and real-data experiments are presented that clearly illustrate the usefulness of proposed approach. \nThe introduction is very well-motivated and the exposition is generally clear. The paper is technically sound and builds on sound foundations - I see no obvious flaws. \nThe paper contains good motivation about why the proposed approach is necessary. I see this work as a worthwhile and well-motivated application of existing technical contributions. The important technical piece that make this approach possible, which is differentiation though an argmax are already presented in [Amos 2016]. While building on existing results, this work applies them in a very relevant and well-motivated formulation of task-based learning and I believe would be of interest to the machine learning community.\nThe proposed benefit, but also a potential issue of the proposed approach is that the model is not independent of the task. Is is possible to characterize how much the model may be overfitting to the specific task and whether it generalizes well to a different (potentially only slightly different) task? \nIt would be good to reference related work on meta-learning \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\" by Finn et al, as that's another example of differentiation through an optimization procedure."
        }
    ]
}