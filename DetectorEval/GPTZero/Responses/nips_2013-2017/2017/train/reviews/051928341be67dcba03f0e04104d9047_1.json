{
    "version": "2025-01-09-base",
    "scanId": "956fa5f3-b2c8-4ee4-a837-7cdc937fe4df",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.002181245246902108,
                    "sentence": "General Impression:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.00183915626257658,
                    "sentence": "Overall I think the proposed method is interesting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002003750065341592,
                    "sentence": "The results are quite good and the attention maps seem to illustrate these gains might be due to improved attention mechanisms and not simply increased model capacity.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0024128668010234833,
                    "sentence": "I found the writing to be a bit weak and sometimes a bit confusing, though I imagine given more time the authors could improve the submission appropriately.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002381945727393031,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0015556806465610862,
                    "sentence": "- Interesting model with good results.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0009385335724800825,
                    "sentence": "- Thorough qualitative and quantitative experiments.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0011877771466970444,
                    "sentence": "- I'm fairly impressed by the shift between unary/pairwise attentions and the final attention.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0011738748289644718,
                    "sentence": "Though I would have liked to see marginalized trinary attention maps somewhere as well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001428075134754181,
                    "sentence": "Did I miss these in the text or supplement?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0022581855300813913,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0013098744675517082,
                    "sentence": "- As I said above, I found the writing / presentation a bit jumbled at times.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001000328455120325,
                    "sentence": "- The novelty here feels a bit limited.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0009823029395192862,
                    "sentence": "Undoubtedly the architecture is more complex than and outperforms the MCB for VQA model [7], but much of this added complexity is simply repeating the intuition of [7] at higher (trinary) and lower (unary) orders.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0011842339299619198,
                    "sentence": "I don't think this is a huge problem, but I would suggest the authors clarify these contributions (and any I may have missed).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0016703191213309765,
                    "sentence": "- I don't think the probabilistic connection is drawn very well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0232678409665823,
                    "sentence": "It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.018542662262916565,
                    "sentence": "- Figure 2 is at an odd level of abstraction where it is not detailed enough to understand the network's functionality but also not abstract enough to easily capture the outline of the approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.016589179635047913,
                    "sentence": "I would suggest trying to simplify this figure to emphasize the unary/pairwise/trinary potential generation more clearly.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.029609760269522667,
                    "sentence": "- Figure 3 is never referenced unless I missed it.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.022957902401685715,
                    "sentence": "Some things I'm curious about:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.035043615847826004,
                    "sentence": "- What values were learned for the linear coefficients for combining the marginalized potentials in equations (1)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0202470850199461,
                    "sentence": "It would be interesting if different modalities took advantage of different potential orders.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02734145149588585,
                    "sentence": "- I find it interesting that the 2-Modalities Unary+Pairwise model under-performs MCB [7] despite such a similar architecture.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02747240662574768,
                    "sentence": "I was disappointed that there was not much discussion about this in the text.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.028966791927814484,
                    "sentence": "Any intuition into this result?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03334469348192215,
                    "sentence": "Is it related to swap to the MCB / MCT decision computation modules?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.024104280397295952,
                    "sentence": "- The discussion of using sequential MCB vs a single MCT layers for the decision head was quite interesting, but no results were shown.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.017725538462400436,
                    "sentence": "Could the authors speak a bit about what was observed?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 4,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 21,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 23,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.025041194076269924,
            "class_probabilities": {
                "human": 0.974671313226954,
                "ai": 0.025041194076269924,
                "mixed": 0.00028749269677611
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.974671313226954,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.025041194076269924,
                    "human": 0.974671313226954,
                    "mixed": 0.00028749269677611
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "General Impression:\nOverall I think the proposed method is interesting. The results are quite good and the attention maps seem to illustrate these gains might be due to improved attention mechanisms and not simply increased model capacity. I found the writing to be a bit weak and sometimes a bit confusing, though I imagine given more time the authors could improve the submission appropriately. \nStrengths:\n- Interesting model with good results.\n- Thorough qualitative and quantitative experiments.\n- I'm fairly impressed by the shift between unary/pairwise attentions and the final attention. Though I would have liked to see marginalized trinary attention maps somewhere as well. Did I miss these in the text or supplement? \nWeaknesses:\n- As I said above, I found the writing / presentation a bit jumbled at times.\n- The novelty here feels a bit limited. Undoubtedly the architecture is more complex than and outperforms the MCB for VQA model [7], but much of this added complexity is simply repeating the intuition of [7] at higher (trinary) and lower (unary) orders. I don't think this is a huge problem, but I would suggest the authors clarify these contributions (and any I may have missed).\n- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\n- Figure 2 is at an odd level of abstraction where it is not detailed enough to understand the network's functionality but also not abstract enough to easily capture the outline of the approach. I would suggest trying to simplify this figure to emphasize the unary/pairwise/trinary potential generation more clearly. \n- Figure 3 is never referenced unless I missed it.\nSome things I'm curious about:\n- What values were learned for the linear coefficients for combining the marginalized potentials in equations (1)? It would be interesting if different modalities took advantage of different potential orders.\n- I find it interesting that the 2-Modalities Unary+Pairwise model under-performs MCB [7] despite such a similar architecture. I was disappointed that there was not much discussion about this in the text. Any intuition into this result? Is it related to swap to the MCB / MCT decision computation modules?\n- The discussion of using sequential MCB vs a single MCT layers for the decision head was quite interesting, but no results were shown. Could the authors speak a bit about what was observed?"
        }
    ]
}