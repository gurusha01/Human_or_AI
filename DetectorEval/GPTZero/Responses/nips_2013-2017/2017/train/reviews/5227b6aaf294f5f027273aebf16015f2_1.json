{
    "version": "2025-01-09-base",
    "scanId": "dd33eb7c-8945-4c5c-9bb2-8655e6036481",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.0014782440848648548,
                    "sentence": "This paper presents a very neat idea of using a unique operator to replace the original conv filter operation, which is matrix inner product.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0017681490862742066,
                    "sentence": "Instead of computing similarity between filter weights and pixel values (or activation values if intermediate layers), they compute the angle between them in the hypersphere space.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.001889119972474873,
                    "sentence": "They also defined three types of operators all as functions of the angle: linear, cosine, and sigmoid.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0009837403194978833,
                    "sentence": "To better suit such an operation they also redefined the regularization term and loss function.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0024419091641902924,
                    "sentence": "Ablation study shows the effectiveness of each addition introduced, and better results are generated for image classification task in CIFAR 10 and CIFAR 100 and image feature embedding task.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.002833401318639517,
                    "sentence": "Improvements are seen in terms of both higher accuracy and easier and faster convergence.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0028917985036969185,
                    "sentence": "The paper is theoretically sound, clearly structured, experiments well organized, and the writing and presentation is elegant.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.004523548297584057,
                    "sentence": "The drawback, though, is that the author didn't extend experiments to larger and more realistic image classification tasks such as ImageNet, and gave no explanation or hint of future work on it.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.008617923595011234,
                    "sentence": "It can also be worthwhile to test with more diverse tasks such as certain small scale reinforcement learning problems.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 6,
                    "completely_generated_prob": 1.474742012248794e-05
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.009998674697021016,
            "class_probabilities": {
                "human": 0.990001325302979,
                "ai": 0.009998674697021016,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.990001325302979,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.009998674697021016,
                    "human": 0.990001325302979,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents a very neat idea of using a unique operator to replace the original conv filter operation, which is matrix inner product. Instead of computing similarity between filter weights and pixel values (or activation values if intermediate layers), they compute the angle between them in the hypersphere space. They also defined three types of operators all as functions of the angle: linear, cosine, and sigmoid. To better suit such an operation they also redefined the regularization term and loss function. Ablation study shows the effectiveness of each addition introduced, and better results are generated for image classification task in CIFAR 10 and CIFAR 100 and image feature embedding task. Improvements are seen in terms of both higher accuracy and easier and faster convergence.\nThe paper is theoretically sound, clearly structured, experiments well organized, and the writing and presentation is elegant. \nThe drawback, though, is that the author didn't extend experiments to larger and more realistic image classification tasks such as ImageNet, and gave no explanation or hint of future work on it. It can also be worthwhile to test with more diverse tasks such as certain small scale reinforcement learning problems."
        }
    ]
}