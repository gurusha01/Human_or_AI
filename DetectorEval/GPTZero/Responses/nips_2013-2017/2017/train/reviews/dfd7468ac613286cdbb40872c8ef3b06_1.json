{
    "version": "2025-01-09-base",
    "scanId": "6d8a10fc-23ee-4185-9035-866e269cebc9",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.030962161719799042,
                    "sentence": "This paper presents the MMD-GAN, a generative architecture which combines the benefits of generative adversarial nets (GANs) and generative moment matching nets (GMMNs).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03400172293186188,
                    "sentence": "The architecture is based on MMD, but with a kernel function learned by an adversary trying to maximize MMD.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03599631041288376,
                    "sentence": "There's a bit of theory showing non-degeneracy, and experiments demonstrate that the generated samples are competitive with W-GANs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06211189180612564,
                    "sentence": "Overall, I think this is a strong paper which presents a simple idea and executes it well.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.056630972772836685,
                    "sentence": "The proposed method is intuitively appealing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08892207592725754,
                    "sentence": "The modeling choices are well motivated, the writing is clear, the theoretical justification is nice, and relationships to other methods are clearly highlighted.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.0538821667432785,
                    "sentence": "The experiments compare against strong baselines on challenging datasets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06435699760913849,
                    "sentence": "Being able to make GMMNs competitive with WGANs is impressive, since to the best of my knowledge there was still a large performance gap between them.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.040735285729169846,
                    "sentence": "While the results seem significantly better than the baselines, the paper could use more analysis of where this improvement comes from.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.037284187972545624,
                    "sentence": "How does the MMD-GAN eliminate the need for large mini-batches?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03555776923894882,
                    "sentence": "(Don't the samples in each mini-batch still need to essentially tile the space in order to achieve low MMD?)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04101274162530899,
                    "sentence": "Is there an explanation for why it outperforms the WGAN?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.08963780105113983,
                    "sentence": "Minor comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03284633159637451,
                    "sentence": "- Section 3 describes the method in relation to the GAN, but isn't the WGAN a more direct match?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.07320500910282135,
                    "sentence": "If I understand right, the MMD-GAN is essentially a kernelized W-GAN.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.06535203754901886,
                    "sentence": "- \"Also, the gradient has to be bounded, which can be done by clipping phi.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.04994985833764076,
                    "sentence": "Is this the same as the Lipschitz constraint from WGANs?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.05068027600646019,
                    "sentence": "If so, why not use the regularization method from the \"improved WGAN\" paper, rather than the original version (which the authors found was less effective)?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.033524129539728165,
                    "sentence": "- If the claim is that the proposed method prevents mode dropping, estimating log-likelihoods is probably a stronger quantitative way to test it.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.03266652673482895,
                    "sentence": "E.g., see Wu et al., 2017.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.02728148177266121,
                    "sentence": "- The experiment of Section 5.3 doesn't seem to be measuring stability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                },
                {
                    "generated_prob": 0.043391771614551544,
                    "sentence": "(I assume this refers to avoiding the degenerate solutions that regular GANs fall into?)",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": false
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.00010005932717626924
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.0006564766595293492
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.00408719312638748
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                },
                {
                    "start_sentence_index": 20,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.058471088130131146,
            "class_probabilities": {
                "human": 0.9415289118698688,
                "ai": 0.058471088130131146,
                "mixed": 0
            },
            "average_generated_prob": 0,
            "predicted_class": "human",
            "confidence_score": 0.9415289118698688,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.058471088130131146,
                    "human": 0.9415289118698688,
                    "mixed": 0
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {},
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written entirely by a human.",
            "document_classification": "HUMAN_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This paper presents the MMD-GAN, a generative architecture which combines the benefits of generative adversarial nets (GANs) and generative moment matching nets (GMMNs). The architecture is based on MMD, but with a kernel function learned by an adversary trying to maximize MMD. There's a bit of theory showing non-degeneracy, and experiments demonstrate that the generated samples are competitive with W-GANs.\nOverall, I think this is a strong paper which presents a simple idea and executes it well. The proposed method is intuitively appealing. The modeling choices are well motivated, the writing is clear, the theoretical justification is nice, and relationships to other methods are clearly highlighted. The experiments compare against strong baselines on challenging datasets. Being able to make GMMNs competitive with WGANs is impressive, since to the best of my knowledge there was still a large performance gap between them. \nWhile the results seem significantly better than the baselines, the paper could use more analysis of where this improvement comes from. How does the MMD-GAN eliminate the need for large mini-batches? (Don't the samples in each mini-batch still need to essentially tile the space in order to achieve low MMD?) Is there an explanation for why it outperforms the WGAN?\nMinor comments:\n- Section 3 describes the method in relation to the GAN, but isn't the WGAN a more direct match? If I understand right, the MMD-GAN is essentially a kernelized W-GAN. \n- \"Also, the gradient has to be bounded, which can be done by clipping phi.\" Is this the same as the Lipschitz constraint from WGANs? If so, why not use the regularization method from the \"improved WGAN\" paper, rather than the original version (which the authors found was less effective)?\n- If the claim is that the proposed method prevents mode dropping, estimating log-likelihoods is probably a stronger quantitative way to test it. E.g., see Wu et al., 2017.\n- The experiment of Section 5.3 doesn't seem to be measuring stability. (I assume this refers to avoiding the degenerate solutions that regular GANs fall into?)"
        }
    ]
}