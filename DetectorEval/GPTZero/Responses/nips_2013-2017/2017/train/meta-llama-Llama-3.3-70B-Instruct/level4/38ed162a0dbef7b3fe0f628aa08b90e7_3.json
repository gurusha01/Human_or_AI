{
    "version": "2025-01-09-base",
    "scanId": "63585165-75c3-41d4-b03e-fa4ccc039288",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9998106956481934,
                    "sentence": "The manuscript is well-structured and rigorous, effectively demonstrating how certain functional spaces realized by Deep Convolutional Neural Networks (DCNNs) can be expressed in terms of multilayer kernels.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998264908790588,
                    "sentence": "Additionally, it highlights the architecture's desirable properties, including invariance to group transformations and stability under diffeomorphic transformations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994605779647827,
                    "sentence": "Based on the following comments, I recommend acceptance:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997525811195374,
                    "sentence": "* The primary contribution of the paper appears to be the reinterpretation of CNN architectures within the framework of Reproducing Kernel Hilbert Spaces (RKHS).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996716976165771,
                    "sentence": "It would be beneficial to include discussions in the text on why this reformulation is useful, such as whether the kernel formulation could lead to interesting results on generalization bounds or if it offers computational efficiency advantages through standard kernel techniques and approximations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995468854904175,
                    "sentence": "Furthermore, it is essential to clarify whether the novelty of this approach extends beyond merely reformulating DCNNs in the context of RKHS.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993066191673279,
                    "sentence": "Specifically, the authors should address how their work differs from \"Convolutional Kernel Networks\" by Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995967745780945,
                    "sentence": "* The authors emphasize the importance of preserving signal information.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995059370994568,
                    "sentence": "However, it is crucial to justify this emphasis, especially since DCNNs are commonly applied to tasks like image classification, where only task-relevant information needs to be retained.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991284012794495,
                    "sentence": "Unless the specific task at hand is signal reconstruction, it would be helpful to comment on the broader implications of signal preservation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991403222084045,
                    "sentence": "* Regarding the aspect of group invariance, it would be interesting to explore potential connections with the invariant kernel approach developed by Hans Burkhardt.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994972944259644,
                    "sentence": "Investigating this relationship could provide additional insights into the manuscript's contributions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9982419610023499,
                    "sentence": "* As a point of curiosity, the definition of patch, pooling, and kernel maps that commute with the group action raises questions about the generality of operators that commute with all group elements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9971436858177185,
                    "sentence": "Would any other such operator ensure the equivariance property of DCNNs with respect to group transformations, or are there specific reasons why the chosen operators are uniquely suited for this purpose?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 3,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                }
            ],
            "completely_generated_prob": 0.9984984300152882,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984984300152882,
                "mixed": 0.0015015699847118259
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984984300152882,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984984300152882,
                    "human": 0,
                    "mixed": 0.0015015699847118259
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The manuscript is well-structured and rigorous, effectively demonstrating how certain functional spaces realized by Deep Convolutional Neural Networks (DCNNs) can be expressed in terms of multilayer kernels. Additionally, it highlights the architecture's desirable properties, including invariance to group transformations and stability under diffeomorphic transformations. Based on the following comments, I recommend acceptance:\n* The primary contribution of the paper appears to be the reinterpretation of CNN architectures within the framework of Reproducing Kernel Hilbert Spaces (RKHS). It would be beneficial to include discussions in the text on why this reformulation is useful, such as whether the kernel formulation could lead to interesting results on generalization bounds or if it offers computational efficiency advantages through standard kernel techniques and approximations. Furthermore, it is essential to clarify whether the novelty of this approach extends beyond merely reformulating DCNNs in the context of RKHS. Specifically, the authors should address how their work differs from \"Convolutional Kernel Networks\" by Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid.\n* The authors emphasize the importance of preserving signal information. However, it is crucial to justify this emphasis, especially since DCNNs are commonly applied to tasks like image classification, where only task-relevant information needs to be retained. Unless the specific task at hand is signal reconstruction, it would be helpful to comment on the broader implications of signal preservation.\n* Regarding the aspect of group invariance, it would be interesting to explore potential connections with the invariant kernel approach developed by Hans Burkhardt. Investigating this relationship could provide additional insights into the manuscript's contributions.\n* As a point of curiosity, the definition of patch, pooling, and kernel maps that commute with the group action raises questions about the generality of operators that commute with all group elements. Would any other such operator ensure the equivariance property of DCNNs with respect to group transformations, or are there specific reasons why the chosen operators are uniquely suited for this purpose?"
        }
    ]
}