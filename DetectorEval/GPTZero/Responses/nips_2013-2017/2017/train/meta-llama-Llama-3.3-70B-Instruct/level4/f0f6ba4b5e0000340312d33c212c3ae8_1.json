{
    "version": "2025-01-09-base",
    "scanId": "5c5a65d7-068a-480c-9147-3a6a5541f175",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9996229410171509,
                    "sentence": "This study presents an enhanced version of the conventional word hashing trick for embedding representation, utilizing a weighted combination of multiple vectors indexed by different hash functions to represent each word, which can be achieved through a predefined dictionary or during online training.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9994403719902039,
                    "sentence": "This approach is notable for its simplicity and ease of implementation, significantly reducing the number of embedding parameters.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996216297149658,
                    "sentence": "The results obtained are generally satisfactory, and the methodology employed is quite refined.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9971787333488464,
                    "sentence": "Moreover, the hash embeddings appear to function as an effective regularizer.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99849933385849,
                    "sentence": "However, to further enhance the clarity of the presentation, it would be beneficial to include the final selected vocabulary sizes and the parameter reduction achieved by the hash embedding in Table 2.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9852607250213623,
                    "sentence": "Additionally, Table 3 lacks a comparison with a state-of-the-art joint model for the DBPedia dataset, as presented in [1].",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9623522162437439,
                    "sentence": "The claim made on Line 255, stating that the ensemble would require the same training time as a single large model, seems inaccurate.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9344425797462463,
                    "sentence": "This equivalence in training time would only hold if each model in the ensemble had an architecture with fewer non-embedding weights.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8620214462280273,
                    "sentence": "Given that the number of non-embedding weights in each network of the ensemble is reportedly the same as in the large model, the training time for the ensemble would likely be substantially longer.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8879802823066711,
                    "sentence": "Table 3 highlights the top three performing models, but a more insightful comparison could be achieved by categorizing the approaches into embedding-only methods and RNN/CNN methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9025776386260986,
                    "sentence": "It would also be intriguing to explore the integration of these embeddings into more context-sensitive RNN/CNN models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9463741183280945,
                    "sentence": "Minor suggestions for improvement include:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9420035481452942,
                    "sentence": "- L9: Correcting the typo \"million(s)\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8793789744377136,
                    "sentence": "- L39: Amending \"to(o) much\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.924420177936554,
                    "sentence": "- L148: Revising the sentence for better clarity",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9674975872039795,
                    "sentence": "- L207: Providing a definition or explanation for \"patience\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9700647592544556,
                    "sentence": "- L235: Correcting the typo \"table table\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9639976620674133,
                    "sentence": "- Table 4: Clarifying whether the importance weights were summed to obtain the results, and specifying the order of the highest and lowest weights.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9353034496307373,
                    "sentence": "[1] Miyato, Takeru, Andrew M. Dai, and Ian Goodfellow.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8023641705513,
                    "sentence": "\"Virtual adversarial training for semi-supervised text classification.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9022064208984375,
                    "sentence": "ICLR 2017.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 16,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 18,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                }
            ],
            "completely_generated_prob": 0.9984800378301695,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9984800378301695,
                "mixed": 0.0015199621698304396
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9984800378301695,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9984800378301695,
                    "human": 0,
                    "mixed": 0.0015199621698304396
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "This study presents an enhanced version of the conventional word hashing trick for embedding representation, utilizing a weighted combination of multiple vectors indexed by different hash functions to represent each word, which can be achieved through a predefined dictionary or during online training. This approach is notable for its simplicity and ease of implementation, significantly reducing the number of embedding parameters.\nThe results obtained are generally satisfactory, and the methodology employed is quite refined. Moreover, the hash embeddings appear to function as an effective regularizer. However, to further enhance the clarity of the presentation, it would be beneficial to include the final selected vocabulary sizes and the parameter reduction achieved by the hash embedding in Table 2. Additionally, Table 3 lacks a comparison with a state-of-the-art joint model for the DBPedia dataset, as presented in [1].\nThe claim made on Line 255, stating that the ensemble would require the same training time as a single large model, seems inaccurate. This equivalence in training time would only hold if each model in the ensemble had an architecture with fewer non-embedding weights. Given that the number of non-embedding weights in each network of the ensemble is reportedly the same as in the large model, the training time for the ensemble would likely be substantially longer.\nTable 3 highlights the top three performing models, but a more insightful comparison could be achieved by categorizing the approaches into embedding-only methods and RNN/CNN methods. It would also be intriguing to explore the integration of these embeddings into more context-sensitive RNN/CNN models.\nMinor suggestions for improvement include:\n- L9: Correcting the typo \"million(s)\"\n- L39: Amending \"to(o) much\"\n- L148: Revising the sentence for better clarity\n- L207: Providing a definition or explanation for \"patience\"\n- L235: Correcting the typo \"table table\"\n- Table 4: Clarifying whether the importance weights were summed to obtain the results, and specifying the order of the highest and lowest weights.\n[1] Miyato, Takeru, Andrew M. Dai, and Ian Goodfellow. \"Virtual adversarial training for semi-supervised text classification.\" ICLR 2017."
        }
    ]
}