{
    "version": "2025-01-09-base",
    "scanId": "813b6567-02dd-46d8-b8ae-15ad10da6b76",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.999629020690918,
                    "sentence": "The authors investigate human questioning behavior in a K-ary answer setting, where they manually define a probabilistic context-free grammar (PCFG) for a \"battleship\" domain.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992913007736206,
                    "sentence": "This domain features a partially observable grid with hidden colored shapes, and the agent's task is to pose a single-word question that maximizes information gain about the board's state.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9968057870864868,
                    "sentence": "The PCFG serves as a prior over questions, represented as lambda calculus statements.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9989920258522034,
                    "sentence": "The authors evaluate question quality using a linear function that considers informativeness, complexity (in terms of length or negative log probability under the PCFG), and answer type.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.996209979057312,
                    "sentence": "Human data from a previous study is hand-coded into the grammar, and the authors perform leave-one-out cross-validation to compare lesioned model versions, finding that all factors contribute significantly, with complexity being the most important.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9974668025970459,
                    "sentence": "This work offers a unique perspective on question-asking, integrating modern linguistic and cognitive theories, which could lead to novel research in active learning with more sophisticated question-asking models.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9987332820892334,
                    "sentence": "The paper is well-written and clearly presented, with a thorough evaluation from both quantitative and qualitative perspectives.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9975146055221558,
                    "sentence": "As a computational cognitive scientist, I appreciate how this paper advances the field of self-directed learning, addressing previous findings that question informativeness is a poor predictor of human question-asking behavior.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9991085529327393,
                    "sentence": "The authors' approach resolves part of this issue, highlighting the importance of complexity and the size of possible questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9992080926895142,
                    "sentence": "However, the approach has significant limitations that may restrict its appeal to the broader NIPS audience.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998477041721344,
                    "sentence": "While it may be possible to automate the conversion of human question text to semantic logic statements, it is unclear how the method can be scaled or applied to more complex domains.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9883701205253601,
                    "sentence": "The battleship domain is relatively simple, and the current implementation is already computationally demanding.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9953681826591492,
                    "sentence": "Nevertheless, the ideas presented are intriguing, particularly in the context of active learning with richer questions.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9864139556884766,
                    "sentence": "To increase the paper's appeal, the authors may need to provide a clear scenario where their technique can be applied to a domain of interest to the broader community, offering novel insights.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.0017499015666544437,
                    "sentence": "Minor comments include the possibility of performing a nested model comparison using the log-likelihoods of the lesioned models to obtain tests of statistical significance.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.0015795716317370534,
                    "sentence": "Additionally, for sequential question-asking, the authors may want to explore models from the linguistics literature on formal semantics and pragmatics, such as the Questions under Discussion literature, which includes works like Ginzburg (1995) and Rojas-Esponda (2013).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 9,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.02318840472169716
                }
            ],
            "completely_generated_prob": 0.9064748201438849,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9064748201438849,
                "mixed": 0.0935251798561151
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9064748201438849,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9064748201438849,
                    "human": 0,
                    "mixed": 0.0935251798561151
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The authors investigate human questioning behavior in a K-ary answer setting, where they manually define a probabilistic context-free grammar (PCFG) for a \"battleship\" domain. This domain features a partially observable grid with hidden colored shapes, and the agent's task is to pose a single-word question that maximizes information gain about the board's state. The PCFG serves as a prior over questions, represented as lambda calculus statements. The authors evaluate question quality using a linear function that considers informativeness, complexity (in terms of length or negative log probability under the PCFG), and answer type. Human data from a previous study is hand-coded into the grammar, and the authors perform leave-one-out cross-validation to compare lesioned model versions, finding that all factors contribute significantly, with complexity being the most important.\nThis work offers a unique perspective on question-asking, integrating modern linguistic and cognitive theories, which could lead to novel research in active learning with more sophisticated question-asking models. The paper is well-written and clearly presented, with a thorough evaluation from both quantitative and qualitative perspectives. As a computational cognitive scientist, I appreciate how this paper advances the field of self-directed learning, addressing previous findings that question informativeness is a poor predictor of human question-asking behavior. The authors' approach resolves part of this issue, highlighting the importance of complexity and the size of possible questions.\nHowever, the approach has significant limitations that may restrict its appeal to the broader NIPS audience. While it may be possible to automate the conversion of human question text to semantic logic statements, it is unclear how the method can be scaled or applied to more complex domains. The battleship domain is relatively simple, and the current implementation is already computationally demanding. Nevertheless, the ideas presented are intriguing, particularly in the context of active learning with richer questions. To increase the paper's appeal, the authors may need to provide a clear scenario where their technique can be applied to a domain of interest to the broader community, offering novel insights.\nMinor comments include the possibility of performing a nested model comparison using the log-likelihoods of the lesioned models to obtain tests of statistical significance. Additionally, for sequential question-asking, the authors may want to explore models from the linguistics literature on formal semantics and pragmatics, such as the Questions under Discussion literature, which includes works like Ginzburg (1995) and Rojas-Esponda (2013)."
        }
    ]
}