{
    "version": "2025-01-09-base",
    "scanId": "033ba9a6-cbbe-4f50-9253-f1b9c27c8a1a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9995683431625366,
                    "sentence": "The paper investigates the impact of eigenvalue decay on function approximation when using Gradient Descent (GD).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999305784702301,
                    "sentence": "It highlights that employing GD with infinitely differentiable kernels to approximate non-smooth functions can result in a large number of iterations.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996081590652466,
                    "sentence": "To address this issue, the paper suggests two potential remedies: using second-order methods or introducing explicit regularization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9980708360671997,
                    "sentence": "However, explicit regularization introduces bias, while second-order methods are computationally expensive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996882677078247,
                    "sentence": "To overcome these limitations, the paper proposes a novel approach called Eigen-Pro.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9910070300102234,
                    "sentence": "The finding that GD struggles to approximate non-smooth functions with infinitely differentiable kernels is both novel and insightful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9677558541297913,
                    "sentence": "The connection established between this limitation and the eigenvalue decay of kernel functions is particularly noteworthy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9612542390823364,
                    "sentence": "The introduction of Eigen-Pro, which leverages implicit regularization through randomization, offers an innovative and computationally efficient alternative with potential practical applications.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.361713707447052,
                    "sentence": "While the aforementioned strengths of the paper are commendable, there are two key weaknesses that need to be addressed.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.11640491336584091,
                    "sentence": "First, the implications of the derived results are not entirely clear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.17978794872760773,
                    "sentence": "If the primary concern is computational speed-up, the comparisons with PEGASOS reveal only marginal improvements, and in some cases, Eigen-Pro is slower.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.16918379068374634,
                    "sentence": "Although it is impressive that the proposed method matches the performance of state-of-the-art kernel-based algorithms, a more explicit comparison is necessary to better understand the trade-offs between time and accuracy.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.1678784340620041,
                    "sentence": "For instance, it might be beneficial to demonstrate results by training a deep neural network (commonly used as a state-of-the-art benchmark) and comparing its performance with the kernel approximation in a synthetic setting.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.23480971157550812,
                    "sentence": "This would provide a clearer perspective on the time and accuracy trade-offs.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.21161332726478577,
                    "sentence": "Second, while the longer version of the paper is accessible and easy to follow, the submitted version is less reader-friendly and could benefit from improved clarity and organization.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.2900754511356354,
                    "sentence": "In summary, this is a strong paper with valuable contributions, but it falls short in effectively explaining the experimental results and ensuring readability.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 5,
                    "completely_generated_prob": 0.8871651474786718
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 6,
                    "completely_generated_prob": 1.474742012248794e-05
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.11111110864197542
                }
            ],
            "completely_generated_prob": 0.7854914307804987,
            "class_probabilities": {
                "human": 0.1940350855797065,
                "ai": 0.7854914307804987,
                "mixed": 0.020473483639794968
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.7854914307804987,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.7854914307804987,
                    "human": 0.1940350855797065,
                    "mixed": 0.020473483639794968
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "The paper investigates the impact of eigenvalue decay on function approximation when using Gradient Descent (GD). It highlights that employing GD with infinitely differentiable kernels to approximate non-smooth functions can result in a large number of iterations. To address this issue, the paper suggests two potential remedies: using second-order methods or introducing explicit regularization. However, explicit regularization introduces bias, while second-order methods are computationally expensive. To overcome these limitations, the paper proposes a novel approach called Eigen-Pro.\nThe finding that GD struggles to approximate non-smooth functions with infinitely differentiable kernels is both novel and insightful. The connection established between this limitation and the eigenvalue decay of kernel functions is particularly noteworthy. The introduction of Eigen-Pro, which leverages implicit regularization through randomization, offers an innovative and computationally efficient alternative with potential practical applications.\nWhile the aforementioned strengths of the paper are commendable, there are two key weaknesses that need to be addressed. First, the implications of the derived results are not entirely clear. If the primary concern is computational speed-up, the comparisons with PEGASOS reveal only marginal improvements, and in some cases, Eigen-Pro is slower. Although it is impressive that the proposed method matches the performance of state-of-the-art kernel-based algorithms, a more explicit comparison is necessary to better understand the trade-offs between time and accuracy. For instance, it might be beneficial to demonstrate results by training a deep neural network (commonly used as a state-of-the-art benchmark) and comparing its performance with the kernel approximation in a synthetic setting. This would provide a clearer perspective on the time and accuracy trade-offs.\nSecond, while the longer version of the paper is accessible and easy to follow, the submitted version is less reader-friendly and could benefit from improved clarity and organization.\nIn summary, this is a strong paper with valuable contributions, but it falls short in effectively explaining the experimental results and ensuring readability."
        }
    ]
}