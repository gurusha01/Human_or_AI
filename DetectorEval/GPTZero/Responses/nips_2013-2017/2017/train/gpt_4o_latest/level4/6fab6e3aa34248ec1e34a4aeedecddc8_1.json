{
    "version": "2025-01-09-base",
    "scanId": "19d8189e-80c9-4193-80f0-bc3a27591b98",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9999480247497559,
                    "sentence": "Overall Impression:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999743700027466,
                    "sentence": "This paper presents a novel and intriguing idea that has the potential to inspire future research into multi-modal early-fusion methods.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999574422836304,
                    "sentence": "However, the quality of the writing and presentation could be improved.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999756217002869,
                    "sentence": "The experiments effectively demonstrate the approach's utility across multiple tasks, but their scope is somewhat limited, making it difficult to generalize the method beyond the vision + language domain.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999947190284729,
                    "sentence": "Refining the text and expanding the experiments to include other model architectures or diverse types of multi-modal data would enhance the submission.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999947547912598,
                    "sentence": "---",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999988853931427,
                    "sentence": "Strengths:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999827742576599,
                    "sentence": "+ The neurological motivations behind the CBN approach are compelling, and the method itself is refreshingly simple.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999504089355469,
                    "sentence": "+ The ablation study comparing fine-tuning batch norm parameters (Ft BN) with question-conditioned batch norm predictions is insightful.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998822212219238,
                    "sentence": "It shows that adapting to new image statistics (Ft BN) yields notable improvements (~1% VQA, 2% Crop GuessWhich), which are further amplified (~2% VQA, 4% Crop GuessWhich) by conditioning on the question.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999297857284546,
                    "sentence": "+ The commitment to releasing public code for reproducibility is commendable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999337792396545,
                    "sentence": "+ The tSNE visualizations are particularly interesting, demonstrating that language-conditioned modulation has a significant impact on visual features.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998182058334351,
                    "sentence": "---",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9999766945838928,
                    "sentence": "Weaknesses:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998865127563477,
                    "sentence": "- The inclusion of Section 2.1 is unclear.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998239278793335,
                    "sentence": "While Batch Normalization and the proposed Conditional Batch Normalization (CBN) are general techniques, the methodology appears independent of the specific choice of model.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9998033046722412,
                    "sentence": "The space dedicated to describing the ResNet architecture could be better utilized to provide deeper motivation and intuition for the CBN approach.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999750018119812,
                    "sentence": "- While the neurological motivation for language modulation of early vision is appreciated, the rationale for implementing this through normalization parameters is less convincing, particularly in Section 3.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8697971701622009,
                    "sentence": "The introduction mentions that the proposed approach reduces overfitting compared to fine-tuning, but it does not adequately situate CBN within the broader context of alternative early-fusion strategies.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8067511320114136,
                    "sentence": "- Since CBN is a generalizable method, demonstrating its effectiveness across a broader range of model architectures for vision + language tasks would strengthen the argument.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8211713433265686,
                    "sentence": "For example, CBN could be applied to the MCB architecture.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8116984963417053,
                    "sentence": "That said, I acknowledge that memory constraints due to backpropagation through the CNN might pose challenges.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8737363219261169,
                    "sentence": "- Given the emphasis on early modulation of vision, it is surprising that the majority of the performance gains in both the VQA and GuessWhat tasks come from applying CBN to Stage 4 (the highest-level stage).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8399088978767395,
                    "sentence": "Additional discussion on this observation would be valuable.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8492157459259033,
                    "sentence": "The supplementary figures are also intriguing, as they suggest that question-conditioned separations in image space only emerge at later stages.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8765432834625244,
                    "sentence": "- Figures 2 and 3 appear somewhat redundant.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984534978866577,
                    "sentence": "---",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8312819004058838,
                    "sentence": "Minor Comments:",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8176727890968323,
                    "sentence": "- It would be interesting to see how different questions alter the feature representation of a single image.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8256299495697021,
                    "sentence": "For example, applying a gradient visualization method to the visual features while varying the question could provide additional insights.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9145698547363281,
                    "sentence": "- Consider adding a space before citation brackets.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.8666067719459534,
                    "sentence": "- The bolding of baseline models is inconsistent.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9693586826324463,
                    "sentence": "- Equation 2 contains a typo: \"gammaj\" should be \"gammac.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9723724722862244,
                    "sentence": "- Line 34: \"to let the question to attend\" â ' \"to let the question attend.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9837208390235901,
                    "sentence": "- Line 42: Missing citation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9820871353149414,
                    "sentence": "- Line 53: The first discussion of batch norm lacks a citation.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9778234958648682,
                    "sentence": "- Line 58: \"to which we refer as\" â ' \"which we refer to as.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9839004874229431,
                    "sentence": "- Line 89: \"is achieved a\" â ' \"is achieved through a.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 1,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 5,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 7,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 8,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 12,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 13,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 17,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 19,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 22,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 25,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 26,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 27,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 28,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 30,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 31,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 32,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 33,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 34,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 35,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 36,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 37,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.8945711917131305,
            "class_probabilities": {
                "human": 0.10523616721313742,
                "ai": 0.8945711917131305,
                "mixed": 0.00019264107373218823
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.8945711917131305,
            "confidence_category": "medium",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.8945711917131305,
                    "human": 0.10523616721313742,
                    "mixed": 0.00019264107373218823
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is moderately confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Overall Impression:\nThis paper presents a novel and intriguing idea that has the potential to inspire future research into multi-modal early-fusion methods. However, the quality of the writing and presentation could be improved. The experiments effectively demonstrate the approach's utility across multiple tasks, but their scope is somewhat limited, making it difficult to generalize the method beyond the vision + language domain. Refining the text and expanding the experiments to include other model architectures or diverse types of multi-modal data would enhance the submission.\n---\nStrengths:\n+ The neurological motivations behind the CBN approach are compelling, and the method itself is refreshingly simple.\n+ The ablation study comparing fine-tuning batch norm parameters (Ft BN) with question-conditioned batch norm predictions is insightful. It shows that adapting to new image statistics (Ft BN) yields notable improvements (~1% VQA, 2% Crop GuessWhich), which are further amplified (~2% VQA, 4% Crop GuessWhich) by conditioning on the question.\n+ The commitment to releasing public code for reproducibility is commendable.\n+ The tSNE visualizations are particularly interesting, demonstrating that language-conditioned modulation has a significant impact on visual features.\n---\nWeaknesses:\n- The inclusion of Section 2.1 is unclear. While Batch Normalization and the proposed Conditional Batch Normalization (CBN) are general techniques, the methodology appears independent of the specific choice of model. The space dedicated to describing the ResNet architecture could be better utilized to provide deeper motivation and intuition for the CBN approach.\n- While the neurological motivation for language modulation of early vision is appreciated, the rationale for implementing this through normalization parameters is less convincing, particularly in Section 3. The introduction mentions that the proposed approach reduces overfitting compared to fine-tuning, but it does not adequately situate CBN within the broader context of alternative early-fusion strategies.\n- Since CBN is a generalizable method, demonstrating its effectiveness across a broader range of model architectures for vision + language tasks would strengthen the argument. For example, CBN could be applied to the MCB architecture. That said, I acknowledge that memory constraints due to backpropagation through the CNN might pose challenges.\n- Given the emphasis on early modulation of vision, it is surprising that the majority of the performance gains in both the VQA and GuessWhat tasks come from applying CBN to Stage 4 (the highest-level stage). Additional discussion on this observation would be valuable. The supplementary figures are also intriguing, as they suggest that question-conditioned separations in image space only emerge at later stages.\n- Figures 2 and 3 appear somewhat redundant.\n---\nMinor Comments:\n- It would be interesting to see how different questions alter the feature representation of a single image. For example, applying a gradient visualization method to the visual features while varying the question could provide additional insights.\n- Consider adding a space before citation brackets.\n- The bolding of baseline models is inconsistent.\n- Equation 2 contains a typo: \"gammaj\" should be \"gammac.\"\n- Line 34: \"to let the question to attend\" â ' \"to let the question attend.\"\n- Line 42: Missing citation.\n- Line 53: The first discussion of batch norm lacks a citation.\n- Line 58: \"to which we refer as\" â ' \"which we refer to as.\"\n- Line 89: \"is achieved a\" â ' \"is achieved through a.\""
        }
    ]
}