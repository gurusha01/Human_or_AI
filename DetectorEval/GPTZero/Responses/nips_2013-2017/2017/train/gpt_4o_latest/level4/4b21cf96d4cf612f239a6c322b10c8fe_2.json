{
    "version": "2025-01-09-base",
    "scanId": "fd03a03f-46fe-496c-803b-ca4956978a9a",
    "documents": [
        {
            "sentences": [
                {
                    "generated_prob": 0.9995930790901184,
                    "sentence": "Overall, I find the paper to be well-written, with the proposed idea being both intuitive and well-motivated.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993704557418823,
                    "sentence": "However, I have a few questions regarding certain technical decisions made in the paper.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996345043182373,
                    "sentence": "* Regarding the encoder (Fig.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997429847717285,
                    "sentence": "4), is it truly necessary to explicitly generate K posterior mean/variances separately before combining them into one via a weighted average?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9996439814567566,
                    "sentence": "Alternatively, how does this approach compare to having the encoder network directly output the posterior mean/variances?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.999636709690094,
                    "sentence": "What performance gain does the current architecture provide?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9995638728141785,
                    "sentence": "* Considering that the decoder's primary task is conditional caption generation (i.e., posterior inference is not strictly required), it seems that employing a VAE might be excessive.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9997196793556213,
                    "sentence": "Why not directly train the decoder using maximum likelihood?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9993858933448792,
                    "sentence": "By sampling z from p(z\"c) and marginalizing it, one could approximately maximize p(x\"c) = \\sum{zi} p(zi\"c)p(x\"zi, c).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.99941086769104,
                    "sentence": "This approach could yield a stronger LSTM baseline while still incorporating z.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9988253116607666,
                    "sentence": "* For the GMM-CVAE model, how was the KL-divergence computed?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9979703426361084,
                    "sentence": "* During training, the standard deviations \\sigmak in the prior effectively govern the trade-off between KL-divergence (as a regularization term) and reconstruction error (as the loss).",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9981000423431396,
                    "sentence": "Have the authors experimented with different \\sigmak values during training, as they did during testing?",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9986096024513245,
                    "sentence": "This question arises because it seems more consistent to use the same \\sigma_k values for both training and testing.",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.9984007477760315,
                    "sentence": "Typographical Error",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                },
                {
                    "generated_prob": 0.998599648475647,
                    "sentence": "* L.36: The phrase \"maximize (an upper bound on) the likelihood\" should likely be corrected to \"lower bound.\"",
                    "perplexity": 0,
                    "highlight_sentence_for_ai": true
                }
            ],
            "paragraphs": [
                {
                    "start_sentence_index": 0,
                    "num_sentences": 2,
                    "completely_generated_prob": 0.8254776901813464
                },
                {
                    "start_sentence_index": 2,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 6,
                    "num_sentences": 4,
                    "completely_generated_prob": 0.8708627247549962
                },
                {
                    "start_sentence_index": 10,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 11,
                    "num_sentences": 3,
                    "completely_generated_prob": 0.850090677245877
                },
                {
                    "start_sentence_index": 14,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                },
                {
                    "start_sentence_index": 15,
                    "num_sentences": 1,
                    "completely_generated_prob": 0.8181818033057853
                }
            ],
            "completely_generated_prob": 0.9997862822885396,
            "class_probabilities": {
                "human": 0,
                "ai": 0.9997862822885396,
                "mixed": 0.00021371771146045916
            },
            "average_generated_prob": 1,
            "predicted_class": "ai",
            "confidence_score": 0.9997862822885396,
            "confidence_category": "high",
            "confidence_scores_raw": {
                "identity": {
                    "ai": 0.9997862822885396,
                    "human": 0,
                    "mixed": 0.00021371771146045916
                }
            },
            "confidence_thresholds_raw": {
                "identity": {
                    "ai": {
                        "reject": 0.65,
                        "low": 0.75,
                        "medium": 0.92
                    },
                    "human": {
                        "reject": 0.7,
                        "low": 0.82,
                        "medium": 0.92
                    },
                    "mixed": {
                        "reject": 0.7,
                        "low": 0.8,
                        "medium": 0.88
                    }
                }
            },
            "overall_burstiness": 0,
            "writing_stats": {},
            "subclass": {
                "ai": {
                    "predicted_class": "pure_ai",
                    "result_message": "",
                    "confidence_score": 0.9999999998,
                    "confidence_category": "high",
                    "class_probabilities": {
                        "pure_ai": 0.9999999999,
                        "ai_paraphrased": 9.999999997e-11
                    },
                    "confidence_scores_raw": {
                        "identity": {
                            "pure_ai": 0.9999999998,
                            "ai_paraphrased": 9.999999997e-11
                        }
                    },
                    "confidence_thresholds_raw": {
                        "identity": {
                            "pure_ai": {
                                "reject": 0.65,
                                "low": 0.75,
                                "medium": 0.92
                            },
                            "ai_paraphrased": {
                                "reject": 0.85,
                                "low": 0.9,
                                "medium": 0.95
                            }
                        }
                    }
                },
                "human": {},
                "mixed": {}
            },
            "result_message": "Our detector is highly confident that the text is written by AI.",
            "document_classification": "AI_ONLY",
            "version": "2025-01-09-base",
            "language": "en",
            "inputText": "Overall, I find the paper to be well-written, with the proposed idea being both intuitive and well-motivated. However, I have a few questions regarding certain technical decisions made in the paper.\n* Regarding the encoder (Fig. 4), is it truly necessary to explicitly generate K posterior mean/variances separately before combining them into one via a weighted average? Alternatively, how does this approach compare to having the encoder network directly output the posterior mean/variances? What performance gain does the current architecture provide?\n* Considering that the decoder's primary task is conditional caption generation (i.e., posterior inference is not strictly required), it seems that employing a VAE might be excessive. Why not directly train the decoder using maximum likelihood? By sampling z from p(z\"c) and marginalizing it, one could approximately maximize p(x\"c) = \\sum{zi} p(zi\"c)p(x\"zi, c). This approach could yield a stronger LSTM baseline while still incorporating z.\n* For the GMM-CVAE model, how was the KL-divergence computed?\n* During training, the standard deviations \\sigmak in the prior effectively govern the trade-off between KL-divergence (as a regularization term) and reconstruction error (as the loss). Have the authors experimented with different \\sigmak values during training, as they did during testing? This question arises because it seems more consistent to use the same \\sigma_k values for both training and testing.\nTypographical Error\n* L.36: The phrase \"maximize (an upper bound on) the likelihood\" should likely be corrected to \"lower bound.\""
        }
    ]
}