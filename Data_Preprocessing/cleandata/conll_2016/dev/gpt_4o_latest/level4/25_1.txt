This paper introduces a method to annotate word senses with temporal information (past, present, future, or atemporal). The authors frame the problem as a graph-based semi-supervised classification task, leveraging both item-specific features—such as temporal indicators in glosses—and the structural relationships within WordNet, such as semantic links between synsets, while also incorporating unlabeled data. The authors conduct a complete annotation of WordNet, using a previously labeled dataset for training and treating the remaining WordNet data as unlabeled. They decompose the task into two stages: first, a binary classification (temporal vs. atemporal), and second, a finer-grained classification (past, present, or future) for the temporal subset. To evaluate their approach intrinsically, they annotate a subset of WordNet synsets via crowdsourcing. Their system is compared against a state-of-the-art temporal tagger (Stanford's SUTime) with a heuristic backup strategy, as well as prior work. The proposed method achieves an 11% improvement in accuracy and demonstrates superior performance over previous systems using only 400 labeled examples. Additionally, the authors evaluate their resource on the TempEval-3 task, achieving a 10% improvement in F1 score across four labels.
The paper is well-organized and generally clear, with a sound and well-motivated approach. The resulting resource, which provides fine-grained temporal annotations at the word sense level, has significant potential to enhance various NLP tasks. However, I have several remarks, particularly regarding the experimental setup.
More details are needed about the task described in the extrinsic evaluation section. For example, providing an example would clarify what the system is predicting (the features mention "entity pairs," but it is not clear what these pairs represent) and what the features entail (e.g., what are the entity attributes? Is the POS for a pair treated as one dimension or two? Are the lemmas obtained automatically?). The description of the labels is also unclear; phrases like "event to document creation time" and "event to same sentence event" need further explanation—are these the types of pairs being considered, or are they relations (as suggested on p.8)? The footnote about the 14 relations is ambiguous: why are the other relations excluded, and what makes a mapping "too complex"? Additionally, are the reported scores macro- or micro-averaged? Lastly, the ablation study suggests potential redundancy between Lexica and Entity features due to their similar scores—could the authors provide insights into this behavior?
I also have questions regarding the use of SVMs. For the extrinsic evaluation, the authors mention optimizing the algorithm's parameters—what are these parameters? Since an SVM is also employed within the MinCut framework, was it optimized as well, and if so, how? If the LibSVM library (via the Weka wrapper) was used, a citation for LibSVM should be included.
Other remarks:
- It would be helpful to provide the number of examples per label in the gold dataset. While figures are given for coarse-grained labels (127 temporal vs. 271 atemporal), the finer-grained distribution is missing.
- An estimate of the number of words that are temporally ambiguous (e.g., "present") would be informative.
- Table 3's caption claims the results are "significantly better," but no significance test or p-value is reported.
Minor remarks:
- Related work: What specific task was performed in (Filannino and Nenadic, 2014)?
- Related work: The phrase "requires a post-calibration procedure" needs a reference, and the explanation of calibration in footnote 3.3 on p.4 could be clarified.
- Related work: The statement "their model differs from ours" should specify how.
- Table 3 is too small—consider removing the parentheses, placing "(p, r, f1)" in the caption, and reporting only two metrics (e.g., precision and F1). The caption could also be shortened.
- The information in Table 4 would be more effectively conveyed as a graph.
- On p.7, the number "1064" should be corrected to "1264."
- TempEval-3: A reference is needed.
- Table 6: Ordering the scores in one column would improve clarity.
- On p.5, paragraph 3: "atemporal)" should be corrected to "atemporal."