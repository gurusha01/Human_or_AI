This paper proposes a methodology for annotating word senses with temporal information, categorizing them as past, present, future, or atemporal. The approach utilizes a graph-based semi-supervised classification algorithm, which integrates item-specific information, such as temporal indicators in glosses, and the structural relationships between synsets in Wordnet. This framework also leverages unlabeled data, enabling a comprehensive annotation of Wordnet based on a previously labeled training dataset and the remainder of Wordnet as unlabeled data. Specifically, the task is divided into a binary classification (temporal vs. atemporal) and then further refined into past, present, or future for temporal instances. To evaluate the approach intrinsically, a subset of Wordnet synsets is annotated using crowd-sourcing. The system's performance is compared to a state-of-the-art time tagger, Stanford's SUTime, and previous works, yielding an approximately 11% improvement in accuracy. Notably, the approach achieves superior performance to previous systems using only 400 labeled data points. Furthermore, an evaluation on the TempEval-3 task demonstrates a 10% improvement in F1 score across four labels.
The paper is well-structured and clear, with a sound and justified approach. The development of a resource with fine-grained temporal information at the word sense level is a significant contribution, with potential applications in enhancing various NLP tasks. However, several aspects of the experimental settings warrant further clarification. 
In the extrinsic evaluation section, additional details would be beneficial, such as an illustrative example to elucidate the prediction target (e.g., entity pairs and their features, including entity attributes, POS dimensions, and lemma acquisition methods). The description of labels, including "event to document creation time" and "event to same sentence event," is unclear, and it is uncertain whether these represent the considered pairs or relations. The footnote regarding the 14 relations lacks justification for ignoring other relations and the criteria for determining a "complex" mapping. Moreover, it is essential to specify whether the scores are macro or micro-averaged. The ablation study suggests potential redundancy between Lexica and Entity, given their similar scores, which deserves further investigation.
Regarding the use of SVM, the parameters optimized for the extrinsic evaluation should be specified, and it should be clarified whether the SVM within the MinCut framework is also optimized and how. If the LibSVM library is utilized, a reference should be included.
Additional suggestions include providing the number of examples per label in the gold data for finer-grained labels and an estimate of the number of temporally ambiguous words. The claim of "significantly better" results in Table 3 should be supported by a significance test and p-value.
Minor remarks include clarifying the task performed in (Filannino and Nenadic, 2014), adding a reference for the post-calibration procedure, and explaining how the model differs from others. Table 3 could be improved by removing parentheses, placing "(p,r,f1)" in the caption, and presenting only two scores. The caption should also be condensed. The information in Table 4 would be better represented graphically. Several typographical corrections are also suggested, including the reference for TempEval-3 and the ordering of scores in Table 6.