This paper describes a new deterministic dependency parsing algorithm and
analyses its behaviour across a range of languages.
The core of the algorithm is a set of rules defining permitted dependencies
based on POS tags.
The algorithm starts by ranking words using a slightly biased PageRank over a
graph with edges defined by the permitted dependencies.
Stepping through the ranking, each word is linked to the closest word that will
maintain a tree and is permitted by the head rules and a directionality
constraint.
Overall, the paper is interesting and clearly presented, though seems to differ
only slightly from Sogaard (2012), "Unsupervised Dependency Parsing without
Training".
I have a few questions and suggestions:
Head Rules (Table 1) - It would be good to have some analysis of these rules in
relation to the corpus.
For example, in section 3.1 the fact that they do not always lead to a
connected graph is mentioned, but not how frequently it occurs, or how large
the components typically are.
I was surprised that head direction was chosen using the test data rather than
training or development data.
Given how fast the decision converges (10-15 sentences), this is not a major
issue, but a surprising choice.
How does tie-breaking for words with the same PageRank score work?
Does it impact performance significantly, or are ties rare enough that it
doesn't have an impact?
The various types of constraints (head rules, directionality, distance) will
lead to upper bounds on possible performance of the system.
It would be informative to include oracle results for each constraint, to show
how much they hurt the maximum possible score.
That would be particularly helpful for guiding future work in terms of where to
try to modify this system.
Minor:
- 4.1, "we obtain [the] rank"
- Table 5 and Table 7 have columns in different orders. I found the Table 7
arrangement clearer.
- 6.1, "isolate the [contribution] of both"