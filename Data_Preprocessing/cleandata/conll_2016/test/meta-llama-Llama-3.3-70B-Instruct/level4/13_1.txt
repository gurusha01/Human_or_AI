This paper proposes a convolutional neural network (CNN) based approach for event linking, where vector representations are generated from word embeddings and then combined with linguistic features to produce a coreference score. The model utilizes a single-layer neural network to compute similarities between event mentions, and its performance is evaluated on the ACE dataset and an expanded version, yielding results comparable to previous feature-rich systems.
The primary contribution of this work, in my view, lies in its development of a neural approach to entity linking that integrates word embeddings with linguistic features, demonstrating that relying solely on word embeddings is insufficient for optimal performance. Notably, the linguistic features employed are limited and do not require external resources, making this approach more efficient.
Experimental Setting:
- The use of gold trigger words instead of predicted ones is noted, although the authors provide a justification for this choice. However, this decision may impact the fairness of comparisons with other systems that utilize predicted triggers.
- The inconsistency in train/test splits across different papers is a concern, and it is recommended that the authors adhere to established splits whenever possible.
Unclear Points:
- While the numbers indicating the need for cross-sentential information are convincing, the statement in the second paragraph (lines 65-70) is unclear.
- The method for generating position embeddings, described as similar to word embeddings, lacks specificity. It is unclear whether these embeddings are randomly initialized or lexicalized, and why relative positions next to different words should share the same embedding.
- The process of using left and right neighbors to create representations (lines 307-311) is not well-explained, and it is uncertain whether this affects only the max-pooling operation.
- The decision to append word embeddings of one word before and after the trigger words seems arbitrary, and the rationale behind this choice is not provided.
- The usage of the event-mention representation ve (line 330) is unclear, as subsequent sections only reference v{sent+lex}.
- The incorporation of pairwise features in section 3.2 is not well-documented, particularly regarding the encoding of binary features and the distance feature, and whether these are kept fixed during training.
Other Issues and Suggestions:
- The potential applicability of this approach to entity coreference resolution is worth exploring, as it would enable comparisons with more existing work and popular datasets like OntoNotes.
- The use of a square function as nonlinearity is intriguing, and its novelty and potential applicability to other tasks are worth investigating.
- The release of the ACE++ dataset would facilitate comparisons with new methods, and a comparison with feature-rich systems on this dataset would be beneficial.
- Significance testing would help substantiate the comparisons between results.
- The related work section could be enhanced by mentioning the neural network approach by Wiseman et al. (2015) for (entity) coreference resolution.
Minor Issues:
- The word "that" in line 143 is redundant.
- There is an inconsistency in referring to a baseline as "same type" in table 6 and "same event" in the text (line 670).
References:
- Wiseman, S., Rush, A. M., Weston, J., & Shieber, S. M. (2015). Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution. ACL 2015.