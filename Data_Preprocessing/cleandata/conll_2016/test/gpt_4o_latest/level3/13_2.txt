Review of the Paper
Summary and Contributions
This paper proposes a convolutional neural network (CNN)-based model for event coreference resolution, which eliminates the reliance on external knowledge sources. The model consists of two parts: (1) generating event mention representations using sentential features derived from CNNs, and (2) scoring coreference decisions based on these representations and pairwise features. The authors also introduce the ACE++ dataset, an extended version of the ACE 2005 corpus, to address data scarcity issues. The model achieves state-of-the-art performance on the ACE dataset and provides an error analysis to highlight areas for future work. The key contributions of the paper are:
1. A CNN-based approach for event coreference resolution that avoids external semantic resources, making it more domain-agnostic.
2. A detailed analysis of argument sharing and dataset size effects, motivating the creation of the ACE++ dataset.
3. Empirical evidence that sentential features can effectively replace external knowledge sources for event linking.
Strengths
1. Well-Written and Clear Motivation: The paper is well-structured, with a clear explanation of the problem, motivation, and proposed solution. The analysis of argument sharing in the ACE corpus and the impact of dataset size effectively justifies the creation of the ACE++ dataset.
2. State-of-the-Art Performance: The proposed model achieves competitive results on the ACE dataset without relying on external semantic features, demonstrating its robustness and domain independence.
3. Error Analysis: The paper includes a thoughtful error analysis, identifying challenges such as pronoun resolution and annotation inconsistencies, which provides valuable insights for future research.
4. Empirical Evaluation: The experiments are thorough, with ablation studies and comparisons to baselines and prior work, highlighting the importance of different feature classes.
Weaknesses
1. Lack of Motivation for CNNs: The choice of CNNs for event mention representation is not well-justified. The authors do not compare CNNs to alternative architectures like recurrent neural networks (RNNs) or transformers, which could provide more context-aware representations.
2. Novelty Concerns: The use of CNNs for event linking is not novel, and the paper does not sufficiently compare its approach to existing continuous-space methods, limiting its contribution to the methodological landscape.
3. Insufficient ACE++ Dataset Details: The paper lacks critical details about the ACE++ dataset, such as its event type coverage, annotation process, and availability for future research. This omission limits the reproducibility and utility of the dataset for the community.
4. Limited Evaluation Scope: The model is only evaluated on the ACE dataset, and its generalizability to other resources like the EventCorefBank remains untested, raising concerns about its robustness.
5. Missing Performance Without Gold References: The results in Table 3 rely on gold references, but the paper does not report performance without them, which would provide a more realistic assessment of the model's utility in practical settings.
Questions to Authors
1. Why were CNNs chosen for event mention representation instead of RNNs or transformers? Have you compared their performance?
2. Can you provide more details about the ACE++ dataset, including its event type coverage, annotation process, and plans for public release?
3. How does the model perform without relying on gold references for event mentions and arguments?
4. Have you considered testing the model on other datasets, such as the EventCorefBank, to evaluate its robustness across domains?
Conclusion
While the paper presents a well-executed study with state-of-the-art results, its lack of novelty in using CNNs, insufficient dataset details, and limited evaluation scope weaken its overall impact. Addressing these issues, particularly by providing a stronger justification for the choice of CNNs and expanding the evaluation, would significantly enhance the paper's contributions.