Summary of the Paper
The paper explores the application of weight pruning to compress Neural Machine Translation (NMT) models. The authors investigate three magnitude-based pruning schemes: class-blind, class-uniform, and class-distribution. They demonstrate that class-blind pruning outperforms the other two schemes and can prune 40% of the weights of a state-of-the-art NMT system with negligible performance loss. With retraining, they can recover and even surpass the original performance with an 80%-pruned model.
Main Contributions
1. The paper shows that weight pruning is an effective method for compressing NMT models, achieving a compression ratio of 20% with no loss of performance.
2. The authors propose a simple class-blind pruning scheme that outperforms other pruning schemes.
3. The paper provides insights into the distribution of redundancy in NMT architectures, highlighting the importance of higher layers, attention, and softmax weights.
Strengths
1. The paper presents a thorough evaluation of the pruning schemes, including a comparison of their performance on a state-of-the-art NMT system.
2. The authors provide a detailed analysis of the distribution of redundancy in NMT architectures, which can inform future research on model compression.
3. The paper demonstrates the effectiveness of weight pruning as a regularization technique, which can lead to improved performance on the validation set.
Weaknesses
1. The paper focuses on a single NMT architecture and does not explore the applicability of the pruning schemes to other architectures.
2. The authors do not provide a comprehensive comparison with other model compression techniques, such as knowledge distillation or low-rank approximations.
3. The paper does not investigate the potential speedup of the pruned models during inference, which is an important consideration for practical applications.
Questions to Authors
1. Can you provide more details on the computational resources required for retraining the pruned models?
2. How do you plan to exploit the sparsity of the pruned models to speed up training and runtime?
3. Can you comment on the potential applicability of the pruning schemes to other sequence-to-sequence models, such as speech recognition or text summarization?