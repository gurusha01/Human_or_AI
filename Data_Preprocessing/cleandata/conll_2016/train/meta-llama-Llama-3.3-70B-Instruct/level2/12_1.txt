Summary of the Paper
This paper introduces a novel approach to sequence tagging that learns to correct errors from cross-lingual projection using an explicit noise layer. The approach is framed as joint learning over two corpora, one tagged with gold standard and the other with projected tags. The model uses a bidirectional Long Short-Term Memory (BiLSTM) network with a noise transformation layer to encode the mapping between projected high-resource tags and low-resource tags. The approach is evaluated on eight simulated low-resource settings and two real low-resource languages, Malagasy and Kinyarwanda, and achieves state-of-the-art results.
Main Contributions
1. Novel Approach to Sequence Tagging: The paper proposes a new approach to sequence tagging that learns to correct errors from cross-lingual projection using an explicit noise layer.
2. Joint Learning over Two Corpora: The approach is framed as joint learning over two corpora, one tagged with gold standard and the other with projected tags.
3. Noise Transformation Layer: The model uses a noise transformation layer to encode the mapping between projected high-resource tags and low-resource tags.
Strengths
1. State-of-the-Art Results: The approach achieves state-of-the-art results on eight simulated low-resource settings and two real low-resource languages, Malagasy and Kinyarwanda.
2. Effective Use of Noisy Projected Data: The approach effectively uses noisy projected data to improve POS tagging accuracy.
3. Generalizability: The technique is general and can be applied to other NLP tasks and modeling approaches.
Weaknesses
1. Limited Analysis of Noise Transformation Layer: The paper could provide more analysis of the noise transformation layer and its impact on the results.
2. Dependence on Quality of Parallel Data: The approach relies on the quality of parallel data, which may not always be available or of high quality.
3. Computational Complexity: The approach may be computationally expensive due to the use of a BiLSTM network and joint learning over two corpora.
Questions to Authors
1. Can you provide more analysis of the noise transformation layer and its impact on the results?
2. How does the approach handle cases where the parallel data is of poor quality or limited in size?
3. Can you provide more details on the computational complexity of the approach and its potential applications to other NLP tasks?