Summary of the Paper
The paper presents a unified framework for sentence pair scoring tasks, which includes Answer Sentence Selection, Next Utterance Ranking, Recognizing Textual Entailment, and Semantic Textual Similarity. The authors propose a task-independent model that can be fine-tuned for specific tasks and demonstrate its effectiveness on several datasets. They also introduce two new datasets, yodaqa/large2470 and wqmprop, for Answer Sentence Selection and Property Selection tasks, respectively.
Main Contributions
1. Unification of sentence pair scoring tasks: The paper provides a single framework for multiple tasks, allowing for easier comparison and evaluation of models.
2. Introduction of new datasets: The authors introduce two new datasets, yodaqa/large2470 and wqmprop, which can be used for future research in sentence pair scoring tasks.
3. Demonstration of transfer learning: The paper shows that a model trained on one dataset can be fine-tuned and applied to another dataset, achieving competitive results.
Strengths
1. Comprehensive framework: The paper provides a thorough overview of sentence pair scoring tasks and proposes a unified framework for evaluation and comparison of models.
2. Introduction of new datasets: The new datasets introduced in the paper can be used to evaluate and improve models for sentence pair scoring tasks.
3. Demonstration of transfer learning: The paper shows the potential of transfer learning in sentence pair scoring tasks, which can be useful for applying models to new tasks and datasets.
Weaknesses
1. Limited evaluation: The paper only evaluates a limited number of models and datasets, which may not be representative of the entire range of sentence pair scoring tasks.
2. Lack of comparison to state-of-the-art models: The paper does not compare its results to state-of-the-art models for each task, which makes it difficult to evaluate the effectiveness of the proposed framework.
3. Limited analysis of results: The paper provides limited analysis of the results, which makes it difficult to understand the strengths and weaknesses of the proposed framework.
Questions to Authors
1. How do the authors plan to extend the framework to include other sentence pair scoring tasks, such as question answering and paraphrasing?
2. Can the authors provide more details on the datasets used in the paper, including the size and characteristics of each dataset?
3. How do the authors plan to evaluate the effectiveness of the proposed framework in comparison to state-of-the-art models for each task?