Summary of the Paper
This paper presents a transition-based parser that jointly produces syntactic and semantic dependencies. The parser uses a stack long short-term memory (LSTM) to learn a representation of the entire algorithm state, and a greedy inference algorithm to select the next transition. The parser is evaluated on the CoNLL 2008 and 2009 English shared tasks, and achieves the best parsing performance among models that jointly learn syntax and semantics.
Main Contributions
1. Joint syntactic and semantic dependency parsing: The paper presents a novel approach to joint parsing, which learns a representation of the entire algorithm state using a stack LSTM.
2. Greedy inference algorithm: The paper proposes a greedy inference algorithm that selects the next transition based on the current state, which achieves linear time complexity.
3. State-of-the-art results: The paper reports state-of-the-art results on the CoNLL 2008 and 2009 English shared tasks, outperforming previous joint models.
Strengths
1. Effective use of stack LSTM: The paper demonstrates the effectiveness of using a stack LSTM to learn a representation of the entire algorithm state, which allows the parser to capture long-range dependencies.
2. Efficient greedy inference algorithm: The paper proposes a greedy inference algorithm that achieves linear time complexity, making it suitable for large-scale parsing tasks.
3. State-of-the-art results: The paper reports state-of-the-art results on the CoNLL 2008 and 2009 English shared tasks, demonstrating the effectiveness of the proposed approach.
Weaknesses
1. Limited comparison to other models: The paper only compares the proposed model to a limited set of previous joint models, and does not provide a comprehensive comparison to other state-of-the-art models.
2. Lack of analysis on error types: The paper does not provide a detailed analysis of the error types made by the parser, which could provide insights into areas for improvement.
3. Dependence on pretrained embeddings: The paper relies on pretrained embeddings, which may not be available for all languages or domains, and may limit the applicability of the proposed approach.
Questions to Authors
1. How does the proposed approach handle out-of-vocabulary words, and what strategies can be used to improve its performance on unseen words?
2. Can the proposed approach be extended to handle other types of dependencies, such as semantic role labeling or coreference resolution?
3. How does the proposed approach compare to other state-of-the-art models in terms of computational efficiency and scalability?