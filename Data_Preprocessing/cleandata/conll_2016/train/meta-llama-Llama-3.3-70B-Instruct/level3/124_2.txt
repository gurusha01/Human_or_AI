This paper introduces a novel approach, TBuckets, for measuring the quality of Latent Dirichlet Allocation (LDA) based topics. The approach groups topic words into thematic groups, or buckets, and uses the properties of these buckets to compute a coherence score for a topic. The authors propose three techniques for creating buckets: TBuckets-Clustering, TBuckets-SVD, and TBuckets-SVD-Reorg. The evaluation of these techniques on three publicly available datasets shows that TBuckets-SVD-Reorg outperforms the state-of-the-art on two out of three datasets.
The main contributions of this work are:
1. The introduction of the TBuckets approach, which provides a new way of measuring topic coherence.
2. The proposal of three techniques for creating buckets, including TBuckets-Clustering, TBuckets-SVD, and TBuckets-SVD-Reorg.
3. The evaluation of the techniques on three publicly available datasets, demonstrating the effectiveness of TBuckets-SVD-Reorg.
The strengths of this paper are:
1. The novelty of the TBuckets approach, which provides a fresh perspective on measuring topic coherence.
2. The thorough evaluation of the techniques on three datasets, providing a comprehensive understanding of their performance.
3. The comparison with state-of-the-art results, demonstrating the competitiveness of TBuckets-SVD-Reorg.
4. The application of TBuckets to weakly supervised text classification, highlighting its potential utility in real-world tasks.
The weaknesses of this paper are:
1. The lack of clarity on whether there are overlapping snippets in the training and testing data of datasets 1 and 2, which could impact the evaluation results.
2. The use of bold text in some areas, which can be distracting and could be minimized for better readability.
3. The evaluation could be improved by using datasets from a single domain, rather than combining different domains, to improve prediction and interpretability of results.
Questions to the authors:
1. Can you provide more details on the datasets used in the evaluation, including the number of documents and topics in each dataset?
2. How did you determine the optimal number of buckets for each topic, and what is the impact of this parameter on the performance of the techniques?
3. Can you provide more insights into the application of TBuckets to weakly supervised text classification, including the potential benefits and limitations of this approach?