This paper proposes a modification to the output layer of recurrent neural network models to learn from both gold and projected annotations in low-resource languages. The method achieves good results on POS tagging for simulated and truly low-resource languages, and its modularity makes it likely to be used by researchers working on different NLP problems.
The main contributions of this work are: 
1. The development of a language-independent method for NER using cross-lingual wikification, which generates language-independent features for NER.
2. The proposal of a cross-lingual NER model that can be applied to all languages in Wikipedia.
3. The demonstration of the effectiveness of the proposed model on a wide range of languages, including low-resource languages.
The strengths of this paper include:
1. The proposed method is strong and elegant, and achieves good results on POS tagging for simulated and truly low-resource languages.
2. The experimental setup is thorough, and the results are compared to strong baselines.
3. The paper provides a detailed analysis of the quality of wikifier features and their dependence on Wikipedia size.
However, there are some weaknesses:
1. The experimental setup is unusual and could be improved by plotting tagging accuracy while varying the size of gold annotations to better understand the method's benefits and limitations.
2. The paper's focus on cross-lingual projections with the same tag set limits its potential impact, and exploring other sources of noisy labels and different tag sets could be beneficial.
3. The equal weighting of gold and noisy labels in the training objective is surprising, and tuning the contribution of these terms could be informative and potentially beneficial.
4. The paper lacks clarity on the vector representation of projected data and how it is constructed, particularly for words that are never aligned, which raises questions about the approach's effectiveness in these cases.
Some questions to the authors include:
1. How do the authors plan to address the limitation of the paper's focus on cross-lingual projections with the same tag set?
2. Can the authors provide more details on the vector representation of projected data and how it is constructed?
3. How do the authors plan to explore other sources of noisy labels and different tag sets in future work? 
Overall, this is a well-written paper that proposes a novel and effective method for cross-lingual NER. With some revisions to address the weaknesses and questions raised, this paper has the potential to make a significant contribution to the field of NLP.