This paper presents a novel approach to cross-lingual Named Entity Recognition (NER) using a cross-lingual wikifier to disambiguate words and phrases to the English Wikipedia. The proposed model is language-independent and can be applied to all languages in Wikipedia, with the only requirement being a multilingual Wikipedia dump.
The main contributions of this paper are:
1. The introduction of a language-independent method for NER using cross-lingual wikification, which achieves state-of-the-art results on several languages, including low-resource languages.
2. The demonstration that wikifier features are strong signals for NER, even in the absence of lexical features, and that they can be used to improve monolingual NER models.
3. The analysis of the quality of wikifier features and their dependence on Wikipedia size, which shows that smaller Wikipedia sizes result in worse features.
The strengths of this paper include:
1. The straightforward and effective technique used to achieve state-of-the-art results on several languages.
2. The clear writing and coverage of previous work, which provides a comprehensive overview of the field.
3. The significant improvements over strong baselines, which demonstrate the effectiveness of the proposed approach.
However, there are also some weaknesses:
1. The lack of novelty in the approach, as it applies a known technique to a new area with unsurprising results.
2. The unclear practical significance of the results, as they require sparse matrix representations which can be tricky to implement on GPUs.
3. The need for improvement in several areas, including clarifying terminology, citing relevant work, and providing more detailed results and statistics.
To further improve this work, I suggest:
1. Using the pruning results to inform architecture changes, such as reducing the number of hidden layers or embedding dimensions.
2. Exploring the link between pruning and retraining with dropout, a related technique in neural networks.
3. Providing more detailed analysis of the results, including error analysis and comparison to other state-of-the-art models.
Overall, this paper presents a significant contribution to the field of cross-lingual NER, and with some improvements, it has the potential to be even more effective and widely applicable. 
Questions to Authors:
1. Can you provide more details on how the cross-lingual wikifier is trained and how it handles out-of-vocabulary words?
2. How do you plan to address the issue of Wikipedia size and its impact on the quality of wikifier features?
3. Can you provide more analysis on the results, including error analysis and comparison to other state-of-the-art models?