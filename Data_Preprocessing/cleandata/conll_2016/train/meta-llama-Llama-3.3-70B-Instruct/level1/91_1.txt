This paper proposes a simple yet effective method for compressing Neural Machine Translation (NMT) models using magnitude-based weight pruning. The authors investigate three pruning schemes: class-blind, class-uniform, and class-distribution, and find that class-blind pruning outperforms the other two. They demonstrate that an NMT model with over 200 million parameters can be pruned by 40% with negligible performance loss, and by adding a retraining phase, they can prune 80% of the parameters with no performance loss.
The main contributions of this work are:
1. The authors show that magnitude-based weight pruning is an effective method for compressing NMT models, achieving a high compression ratio with minimal performance loss.
2. They propose and compare three pruning schemes, finding that class-blind pruning is the most effective.
3. The authors provide insights into the distribution of redundancy in NMT architectures, highlighting the importance of higher layers, attention, and softmax weights.
The strengths of this paper are:
1. The authors provide a thorough evaluation of their method, including a comparison of different pruning schemes and an analysis of the distribution of redundancy in NMT architectures.
2. The results demonstrate a significant compression ratio with minimal performance loss, making this method a promising approach for deploying NMT models on resource-constrained devices.
3. The authors provide a clear and well-structured presentation of their work, making it easy to follow and understand.
The weaknesses of this paper are:
1. The authors do not provide a detailed comparison with other compression methods, such as low-rank approximations or knowledge distillation.
2. The authors do not investigate the effect of pruning on the interpretability of the NMT model, which could be an important consideration in certain applications.
3. The authors do not provide a clear explanation of why class-blind pruning outperforms the other two schemes, which could be an interesting area for further research.
Questions to authors:
1. Can you provide more details on the computational resources required for pruning and retraining the NMT model?
2. How do you plan to exploit the sparsity of the pruned models to speed up training and runtime?
3. Can you provide a more detailed comparison with other compression methods, such as low-rank approximations or knowledge distillation?