This paper proposes a novel approach to domain adaptation in statistical machine translation (SMT) using semi-supervised convolutional neural networks (CNNs) for data selection. The main contributions of this work are:
1. Effective data selection with limited in-domain data: The proposed method can select a small amount of in-domain data that outperforms baseline systems trained with large amounts of general-domain data, even when the in-domain data is as small as 100 sentences.
2. Improved performance over state-of-the-art methods: The semi-supervised CNN approach significantly outperforms state-of-the-art language model-based data selection methods, achieving up to 3.1 BLEU improvement over the baseline system.
3. Stability and robustness: The method is shown to be stable and robust, even when the in-domain data is very small, with a standard deviation of 0.12 in BLEU score over three repeated experiments.
The strengths of this paper include:
1. Novel application of CNNs to SMT: The use of CNNs for data selection in SMT is a new and innovative approach that leverages the strengths of deep learning models in text classification tasks.
2. Effective use of word embeddings: The proposed method utilizes word embeddings learned from large amounts of unlabeled data to improve the accuracy of domain classification, even when the in-domain data is limited.
3. Extensive experimentation: The paper presents thorough experimentation on four different language directions and three test domains, demonstrating the effectiveness and robustness of the proposed approach.
The weaknesses of this paper include:
1. Limited analysis of CNN architecture: The paper does not provide a detailed analysis of the CNN architecture and its hyperparameters, which could be important for understanding the performance of the model.
2. Comparison to other deep learning models: The paper only compares the proposed method to language model-based data selection methods and does not evaluate its performance against other deep learning models, such as recurrent neural networks (RNNs) or long short-term memory (LSTM) networks.
3. Lack of theoretical analysis: The paper does not provide a theoretical analysis of the proposed method, which could help to understand the underlying mechanisms and limitations of the approach.
Questions to authors:
1. How did you select the hyperparameters for the CNN model, and what is the sensitivity of the model to these hyperparameters?
2. Can you provide more details on the word embedding learning process and how it affects the performance of the proposed method?
3. Have you considered applying the proposed method to other NLP tasks, such as language modeling or text classification, and what are the potential benefits and challenges of doing so?