This paper presents the results of an experimental evaluation of state-of-the-art tools for sentiment analysis, with a focus on app reviews and tweets in both Italian and English languages. The main contributions of this work are:
1. Development of App2Check and Tweet2Check tools: The authors have developed two predictive models for sentiment quantification, one specialized on apps reviews and one on tweets, using supervised learning techniques.
2. Experimental evaluation on large datasets: The authors have evaluated their tools on large datasets of app reviews and tweets, including 11 thousand apps reviews in Italian and English, and 3899 tweets in Italian and 1000 tweets in English.
3. Comparison with state-of-the-art research tools: The authors have compared their tools with 19 state-of-the-art research tools, including iFeel platform, SentiStrength, and Sentiment140, and shown that their tools outperform these competitors in most cases.
The strengths of this paper are:
1. Large-scale experimental evaluation: The authors have conducted a large-scale experimental evaluation on multiple datasets, which provides a comprehensive comparison of their tools with state-of-the-art research tools.
2. Strong performance of App2Check and Tweet2Check: The results show that App2Check and Tweet2Check outperform most of the research tools in terms of macro F1 score, accuracy, and F1 score on positive, negative, and neutral classes.
3. Use of supervised learning techniques: The authors have used supervised learning techniques to develop their predictive models, which allows for high-performance sentiment analysis.
The weaknesses of this paper are:
1. Lack of transparency in the development of App2Check and Tweet2Check: The authors do not provide detailed information on the development of their tools, including the features used, the training data, and the algorithms employed.
2. Limited analysis of results: The authors do not provide a detailed analysis of the results, including the reasons why their tools outperform the research tools, and the limitations of their approach.
3. No comparison with commercial tools: The authors only compare their tools with research tools, and do not provide a comparison with commercial tools, which may be more widely used in practice.
Questions to authors:
1. Can you provide more details on the development of App2Check and Tweet2Check, including the features used, the training data, and the algorithms employed?
2. How do you plan to address the limitations of your approach, including the lack of transparency in the development of your tools, and the limited analysis of results?
3. Can you provide a comparison of your tools with commercial tools, and discuss the potential advantages and disadvantages of using your tools in practice?