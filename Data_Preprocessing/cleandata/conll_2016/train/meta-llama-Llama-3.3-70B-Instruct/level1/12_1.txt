This paper proposes a novel approach to sequence tagging that learns to correct errors from cross-lingual projection using an explicit noise layer. The main contributions of this work are: 
1. The proposal of a deep learning model that jointly learns from both annotated and projected data, 
2. The introduction of a noise transformation layer that models the mapping between clean and noisy tags, and 
3. The evaluation of the approach on both simulated and real low-resource languages, achieving state-of-the-art results.
The strengths of this paper are: 
1. The authors' approach to modeling noise in cross-lingual projection is novel and effective, 
2. The use of a bidirectional LSTM with a noise layer allows for the joint learning of clean and noisy tags, 
3. The evaluation on both simulated and real low-resource languages demonstrates the robustness of the approach, and 
4. The results show that the proposed method outperforms other state-of-the-art algorithms.
The weaknesses of this paper are: 
1. The assumption that the noise in the projected data can be modeled using a simple linear transformation may not always hold, 
2. The use of a small amount of annotated data may not be sufficient to train a robust model, 
3. The evaluation on only two real low-resource languages may not be representative of all low-resource languages, and 
4. The comparison to other state-of-the-art algorithms may not be comprehensive.
Questions to authors: 
1. How did the authors determine the optimal size of the annotated dataset, and 
2. Can the authors provide more details on the noise transformation layer, such as how it is initialized and updated during training? 
3. How does the proposed approach handle cases where the projected tags are highly noisy or inconsistent? 
4. Can the authors provide more analysis on the learned noise transformation matrices, such as what types of errors are being corrected?