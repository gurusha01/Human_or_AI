This paper presents a comprehensive review of the task of Sentence Pair Scoring, which involves classifying a pair of sentences based on their semantic similarity. The authors unify various tasks, including Answer Sentence Selection, Next Utterance Ranking, Recognizing Textual Entailment, and Semantic Textual Similarity, under a single framework. They also introduce two new datasets, yodaqa/large2470 and wqmprop, to alleviate the problems of existing datasets.
The main contributions of this work are:
1. Unification of various tasks under a single framework, allowing for a more comprehensive understanding of the sentence pair scoring problem.
2. Introduction of new datasets, yodaqa/large2470 and wqmprop, which provide a more realistic and challenging evaluation setting for sentence pair scoring models.
3. Demonstration of the feasibility of pursuing universal, task-independent models for sentence pair scoring, using cross-task transfer learning.
The strengths of this paper are:
1. The authors provide a thorough review of the existing literature on sentence pair scoring, highlighting the fragmentation of research in this area and the need for a unified framework.
2. The introduction of new datasets and the demonstration of their effectiveness in evaluating sentence pair scoring models.
3. The authors' use of cross-task transfer learning to demonstrate the feasibility of universal, task-independent models for sentence pair scoring.
The weaknesses of this paper are:
1. The authors' models, although showing promising results, still lag behind the state-of-the-art on some tasks, such as Recognizing Textual Entailment and Semantic Textual Similarity.
2. The paper could benefit from a more detailed analysis of the results, including a discussion of the limitations of the models and the datasets used.
3. Some of the experimental settings, such as the use of a single dataset for hyperparameter tuning, may not be optimal and could be improved.
Questions to authors:
1. How do the authors plan to address the limitations of their models, particularly on tasks where they lag behind the state-of-the-art?
2. Can the authors provide more details on the datasets used, including the size and characteristics of the datasets, and how they were collected and preprocessed?
3. How do the authors plan to extend their work to other tasks and datasets, and what are the potential applications of their research?