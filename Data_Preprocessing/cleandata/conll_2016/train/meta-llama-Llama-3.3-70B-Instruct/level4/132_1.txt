A potential integration of word2vec and LDA is an intriguing concept. However, the primary issue with this paper lies in its opaque technical explanations. A thorough revision of Section 2 is necessary to provide a clear, high-level overview of the model combination, enabling readers familiar with word2vec and LDA to grasp the approach with relative ease. Unfortunately, the current presentation falls short of achieving this clarity.
More detailed comments:
The third paragraph of the introduction is unclear. Specifically, the phrase "requires deriving a new approximation" lacks context - what exactly is being approximated, and why is prototype development time-consuming? Furthermore, the reasoning behind the ease of feature evaluation is not provided.
The decision to utilize the same word vectors for both pivot and target languages (deviating from the standard word2vec approach) requires justification. What motivated this choice?
The concept of separating words from a marginal distribution is not well-explained. Additionally, the term "co-adaptation" is unclear. The phrase "If we only included structure up to this point" is also ambiguous - what type of structure is being referred to?
A minor error is present: "it's similarity" should be "its similarity." Moreover, Footnote 1 compromises anonymity.
A significant concern is the lack of evaluation. The field of NLP has moved beyond relying on example clusters as evidence. Figure 2 appears to present a quantitative evaluation, but its description is buried in an overly lengthy caption.
The conclusion overstates the model's capabilities regarding word analogies, as only a limited selection of examples (e.g., king + queen) is provided, rather than a comprehensive demonstration.
Lastly, the reference to Chang's work lacks specificity, with the conference/journal name abbreviated as "Advances in ..." - more detailed information would be helpful to identify the venue accurately.