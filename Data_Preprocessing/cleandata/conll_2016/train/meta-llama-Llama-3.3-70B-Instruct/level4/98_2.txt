This paper presents an assessment of a minimally supervised dependency parser, which is a variant of the DMV model that utilizes manually defined prior probabilities, on a substantial portion of the treebanks from Universal Dependencies, version 1.2. The reported results indicate that, on average, the parser's performance is slightly inferior to that of a few delexicalized transfer parsers, although it exhibits substantially better performance on several non-Indo-European languages.
The concept of incorporating basic "universal" rules into an otherwise unsupervised parser has been explored previously in the literature, and the primary contribution of this paper lies in its empirical evaluation of this approach using the new UD treebanks. However, the methodology and evaluation raise several unanswered questions.
One notable omission is the exclusive focus on unlabeled parsing, which may have been justified in the past due to the lack of standardized dependency labels, but seems misguided given the unified analysis provided by Universal Dependencies in terms of typed dependencies. Furthermore, the manual definition of universal rules could have been easily extended to accommodate labeled dependencies.
Additionally, the process of setting prior probabilities remains unclear, including the underlying universal grammar they are intended to encode and the criteria used to determine them. It is uncertain whether alternative approaches were explored and, if so, how they were evaluated. The paper merely presents a set of numbers without providing a rationale or explanation beyond their basis in UD annotation style.
The paper's claim that the unsupervised system performs better on non-Indo-European languages appears to be supported by the results, but a more in-depth analysis is necessary to understand the underlying factors. Specifically, it would be beneficial to examine the types of dependencies that are handled more effectively by the unsupervised system. Although a comprehensive error analysis may be beyond the scope of this paper, an examination of a small sample could yield valuable insights.
Lastly, the comparison with delexicalized transfer parsers seems to be influenced by several biases. The restriction to unlabeled dependencies is one such factor, as the delexicalized parser could have potentially produced labeled dependencies. Another factor is the arbitrary limitation of training data to 10,000 tokens per treebank. Moreover, it appears that the delexicalized parsers were not adequately optimized, as simply replacing word forms and lemmas with underscores without revising the feature models is unlikely to produce optimal results.