I find some of the motivations behind the proposed method compelling, particularly its significantly faster training time compared to neural networks, as well as its ability to preserve certain distribution properties when reducing dimensionality.
However, I remain skeptical about the importance of vectors being transformable with PPMI, as the rationale provided is not entirely convincing.
A major concern is the lack of a direct comparison with related work, which is a critical oversight.
Detailed comments include:
- On page 3, the definition of Kendall's tau appears unconventional and does not align with the original formula. Its origin and basis are unclear.
- Also on page 3, it is puzzling why Spearman correlation, a standard metric in semantic tasks (and used by the authors during evaluation), was not employed.
- The choice of datasets for evaluation deviates from the standard sets commonly used in the NLP community for assessing semantic relatedness. While exploring alternative datasets is commendable, including results from the standard datasets would be beneficial.
- Figure 1 seems to be incomplete, as only two lines are visible, with no indication of the third line.
- The absence of a direct comparison to related work is notable, with only a cursory mention.
Additionally, there are minor errors, such as the typo "large extend" which should be "large extent".