This manuscript presents a novel approach to evaluating topic models by grouping the top n words of each topic into clusters, referred to as "buckets", based on the cosine similarity of their corresponding word embeddings. The simplest configuration involves considering each word individually and assigning it to an existing bucket if its cosine similarity to the other words in the bucket falls below a specified threshold, or creating a new bucket for the word. Additionally, two more complex methods utilizing eigenvectors and reorganization are proposed. The methodology is assessed on three standard datasets and in a weakly supervised text classification setting, demonstrating performance that is either superior to or on par with the current state of the art (RÃ¶der et al., 2015).
The fundamental concept underlying this paper is straightforward, yet somewhat ad hoc, and the authors do not provide novel insights into why topic quality should be measurable in terms of word-word similarity. Given the distinct notions of context (document vs. sequential context) used to define topics and word embeddings, it is not immediately clear why this approach should be effective. Nevertheless, the proposed method appears to yield satisfactory results, although significance tests for Table 1 would be beneficial.
The paper is generally well-written, despite some minor language issues. However, the description of techniques in Section 3 is somewhat challenging to follow, likely due to the authors' use of passive voice in instances where design choices are being made. To improve clarity, the authors should consider explaining each method in a separate subsection. There is sufficient space for this, as the 8-page content limit has not been fully utilized, and the relatively uninformative "trace" of the method on page 3 could be condensed.
One aspect that warrants further investigation is the sensitivity of the proposed technique to different word embeddings. For instance, how would the results be affected if the authors had employed word2vec instead of GloVe?