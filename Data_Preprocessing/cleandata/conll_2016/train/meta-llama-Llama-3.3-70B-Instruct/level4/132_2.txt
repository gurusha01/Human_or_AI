This paper introduces a neural-styled topic model that builds upon the word2vec objective to learn document embeddings, which are then refined through sparsification to mimic the output of traditional topic models.
I found the proposed model to be intriguing, and the examples presented by the authors showed promise. However, a significant limitation of the paper is the lack of empirical evaluation of the model. The evaluation relies solely on tables of examples without any indication of their representativeness or comparison to standard or neural topic models. To address this, the authors could have employed the topic representation of each document in a supervised document categorization setup to compare with a topic model of the same topic cardinality, or directly evaluated the model on a dataset with document similarity annotations based on pairwise comparisons of topic vectors. Without empirical evaluation, it is challenging to assess the true value of the model, making it difficult to accept the paper.
Although it is commendable that the authors are releasing their code, they have compromised anonymity by including the GitHub link in the submitted paper version, which is a serious issue.
Several other concerns need to be addressed:
- The selection process for the examples in Figures 3-6 is unclear, and presenting a subset of actual topics may be perceived as cherry-picking.
- In Section 2.2.1, the authors discuss calculating word representations for topics based on pairwise comparison with each word in the vocabulary, but this approach may be computationally expensive for reasonable vocabulary sizes and topic numbers.
- The authors mention using SpaCy to identify "tokens" in Section 3.1, but the specifics of this process are unclear, particularly in relation to the use of word2vec pre-trained embeddings.
- The model's handling of out-of-vocabulary (OOV) terms is not explicitly stated, which may become an issue when applying the trained model to novel documents.
- The finding that 20 topics worked best on the 20 Newsgroups corpus is not surprising, and a simple evaluation based on information-theoretic comparison with true document labels could have provided a direct comparison with LDA.
- The authors should compare their model with other neural topic models, such as those presented in Cao et al. (2015), Nguyen et al. (2015), and Shamanta et al. (2015).
Additionally, there are several minor issues that need attention:
- Line 315: "it's similarity" should be corrected to "its similarity".
- Line 361: The phrase "topic basis" is unclear, and the sentence structure is awkward.
- The caption of Figure 5 should refer to "terms" instead of "words".
- The reference formatting is inconsistent and requires standardization.