This manuscript explores the application of translation model pruning to neural machine translation, presenting three straightforward threshold and histogram pruning schemes. Two of these schemes are applied individually to each weight class, while the third is applied globally to the entire model. The authors demonstrate that retraining the pruned models yields performance comparable to the full model, even when a significant 90% of the weights are removed. A comprehensive analysis is provided to explain the superiority of the class-blind pruning approach and the performance gains achieved through retraining.
Although the core concept of the paper is relatively simple, it appears to be highly useful for neural machine translation applications where memory is limited. The analysis section is particularly noteworthy, as it offers valuable insights into the components of the model that are often treated as opaque. While these findings are intriguing in their own right, the primary motivation of the paper is model compression. To strengthen this argument, it would be beneficial to include data on the actual memory usage of the compressed model in comparison to the uncompressed model.
Some minor suggestions for improvement are as follows:
- The related work section could be enhanced by referencing existing research on pruning translation models in phrase-based statistical machine translation, such as the studies by Johnson et al. (EMNLP 07) and Zens et al. (EMNLP 12).
- Figure 5 was somewhat difficult to interpret. Adding an additional bar plot beneath Figure 4 to display the highest discarded weight magnitude by class would be more informative and facilitate comparison across all pruning methods.