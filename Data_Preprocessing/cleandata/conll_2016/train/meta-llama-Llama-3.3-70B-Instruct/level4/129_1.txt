The paper presents a novel approach to selecting training data for machine translation (MT) by utilizing a convolutional neural network (CNN) classifier to score and rank general-domain sentences. The comparison to prior work employing continuous or n-gram based language models is thorough, although it is unclear whether the paper also evaluates against bilingual data selection methods, such as the sum of difference of cross-entropies.
Initially, the motivation for using a CNN over a recurrent neural network (RNN) or long short-term memory (LSTM) network was not immediately apparent, but the paper effectively argues that certain sections of a text or sentence are more important than others, which can be effectively captured by a CNN. However, the paper does not provide experimental evidence to determine whether a bag-of-words (BOW) or sequential (SEQ) representation, or a combination of both, is more important and why.
The textual description of the CNN, including the use of one-hot or semi-supervised pre-trained embeddings, is clear and detailed, highlighting the key aspects of the model. Nevertheless, a visual representation of the layers, illustrating how inputs are combined, would be highly beneficial.
The paper is well-written overall, but some citations in parentheses are unnecessary, and the use of \citet versus \citep is inconsistent (e.g., line 385).
The experiments and evaluation support the claims made in the paper, but some concerns arise regarding the method used to determine the number of selected in-domain sentences (line 443) based on a separate validation set. Specifically, it is unclear what validation data is used, and how the hyperparameters of the CNN models are chosen. The sensitivity of the models to these hyperparameters is also not discussed.
Table 2 should compare the scores of different approaches with the same number of sentences selected, as the approach presented in the paper still appears to outperform the baselines in this scenario, as shown in Figure 1.
Additional comments include:
* An experiment comparing the technique presented in the paper to baselines with more in-domain data available, beyond just the development set, would be of interest.
* The results or discussion section could benefit from including example sentences selected by the different methods to support the claims made in section 5.4.
* Regarding the argument in section 5.4 about abstracting away from surface forms, a comparison to the work of Axelrod (2015), which replaces some words with part-of-speech (POS) tags to reduce language model data sparsity, could provide insight into whether the word2vec embeddings offer an additional advantage.
* The use of the sum of source and target classification scores is similar to the source and target Lewis-Moore language model data selection method, which uses the sum of difference of cross-entropies. A reference to this work around line 435 would be appropriate.
Finally, it is worth exploring whether weights for the sum of both source and target classification scores could be learned by extending the CNN model to the bilingual or parallel setting.