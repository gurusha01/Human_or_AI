The paper presents a novel modification to the output layer of recurrent neural network models, enabling the learning of model parameters from both gold and projected annotations in low-resource languages. By introducing an additional fully connected layer to model the noise generation process, the traditional softmax output layer is extended to define a distribution over noisy labels. 
This submission is overall strong, with a proposed method that is well-suited, straightforward, and elegant. The paper demonstrates promising results on POS tagging for eight simulated low-resource languages and two actual low-resource languages, leveraging a small set of gold annotations and a large set of cross-lingually projected annotations for training. The method's modularity makes it likely to be adopted by researchers addressing various NLP problems in low-resource scenarios.
However, the experimental setup is somewhat unconventional. While there may be scenarios where building a POS tagger with limited annotations (e.g., 1000 tokens) is necessary, such cases are relatively rare. A more comprehensive empirical validation of the proposed method could be achieved by plotting the tagging accuracy of the proposed method and baselines against varying sizes of gold annotations. This would provide insights into questions such as the potential performance impact on the target language when using this method with ample gold annotations, the approximate amount of gold annotations below which this method is beneficial, and whether this threshold depends on the target language.
The potential applications of noisy labels extend beyond cross-lingual projections, as they could be obtained from other sources (e.g., crowd sourcing) and used with different tag sets than gold annotations. Although this additional potential impact is exciting, the paper only presents results using cross-lingual projections with the same tag set.
It is noteworthy that the proposed training objective assigns equal weights to gold and noisy labels. Given the assumption of a small gold-annotated corpus, it would be informative to investigate whether tuning the contribution of the two terms in the objective function is beneficial.
In the description of the projected data (line 357), the paper mentions pairs of word tokens (x_t) and their vector representations \tilde{y}, but does not explicitly specify the form of these vector representations (e.g., a distribution over cross-lingually projected POS tags for this word type). A relevant question arises regarding whether the approach remains effective if \tilde{y} is constructed using projected POS tags at the token level (rather than aggregating predictions for the same word type). Furthermore, since only one-to-one word alignments are preserved, it is unclear how to construct \tilde{y} for words that are never aligned.
Finally, in line 267, one of the two closing brackets should be replaced with an opening bracket to correct the notation.