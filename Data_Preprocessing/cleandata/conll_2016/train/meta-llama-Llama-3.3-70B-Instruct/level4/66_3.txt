This paper revisits the crucial problem of joint semantic and syntactic dependency parsing, leveraging recent advancements in neural network models. The joint parsing approach is a key factor in the success of transition-based neural network parsers, making it a promising area of research.
The authors make two significant contributions. Firstly, they propose a novel transition system that improves upon the existing Hendersen (2008) system. Secondly, they demonstrate the effectiveness of neural networks in tackling this problem, which had previously posed challenges for linear models. The authors attribute this success to the neural network's ability to automatically learn relevant features. However, another potential advantage of neural networks in this context is their parameter efficiency, as they do not require learning a weight for each feature/class pair, unlike linear models.
The proposed transition system could potentially be integrated with various other neural network models, such as the global beam-search model introduced by Andor (2016). Furthermore, there are numerous orthogonal improvements that could be explored to enhance the authors' method, which could lead to state-of-the-art results.
Deriving a dynamic oracle for this transition system would be a valuable addition, even if it is presented in an appendix or future work. Initially, the transition system appears to share similarities with the arc-eager oracle. Specifically, the M-S action restricts semantic arcs between the word at the buffer's start and those on the semantic stack, while the M-D action restricts semantic arcs between the word at the stack's top and those in the buffer. The L and R actions seem to exclude the reverse arc and no other arcs.