I had the opportunity to review this paper previously in its draft form as an ACL 2016 short paper, at which time I identified a flaw in the experimental setup that has since been addressed. Given my earlier suggestion that I would be willing to accept the paper for another ACL event pending correction of this flaw, I now have no reservations about doing so.
A fellow reviewer noted that the paper's setup may be somewhat artificial, particularly in the context of real low-resource languages, where the costs associated with finding versus paying annotators can be significant. I agree that this limitation should be acknowledged in the paper to ensure that the method is not overstated.
The authors may also want to consider discussing relevant prior work on annotation projection for extremely low-resource languages, such as the research presented by Johannsen et al. (2016, ACL) and Agic et al. (2015, ACL), in order to provide a more comprehensive overview of related research.
In my opinion, this paper makes a valuable contribution, and I am voting to accept it. However, it would be beneficial to clarify whether the data used in the study will be made available to the public. In evaluating this aspect, I have proceeded under the assumption that the data will be publicly available, as is standard practice in research.