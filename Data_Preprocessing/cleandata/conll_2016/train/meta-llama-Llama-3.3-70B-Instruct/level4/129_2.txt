This paper presents a methodology for in-domain data selection in Statistical Machine Translation (SMT) utilizing a convolutional neural network (CNN) classifier, mirroring the framework established by Johnson and Zhang in 2015. The proposed method yields an improvement of approximately 0.5 BLEU points over language model-based data selection and demonstrates robustness even with minimal in-domain data availability.
The authors claim a 3.1 BLEU point improvement, but an examination of the results reveals that such significant gains are only achievable when in-domain data is included in the training set. Training solely on in-domain data already results in a 2.8 BLEU point increase. A comparison with a system that interpolates separate in-domain and out-domain models could provide additional insights.
The more notable finding, in this reviewer's opinion, emerges from the second experiment, which shows the effectiveness of the CNN classifier with very limited in-domain data. However, this experiment is limited to the zh2en task, where actual in-domain data is present in the training set, potentially simplifying the selection process. It remains to be seen whether this result would hold for other tasks lacking in-domain data in the training set. Although the en2es and en2zh tasks' development sets contain only a few hundred sentence pairs, suggesting a positive trend, reporting results for all tasks with only 100 sentence pairs used for training would strengthen the claim.
Translating social media text poses unique challenges, including a high out-of-vocabulary (OOV) rate due to non-standard spellings and the presence of special character sequences like usernames, hashtags, and emoticons. It would be beneficial to know if any special preprocessing or filtering steps were applied to the data. Since data selection does not address the OOV issue, providing examples of improvements made through adaptation via data selection could offer valuable insights into the kinds of enhancements achieved.
Regarding specific sections:
- In Section 3.2, clarifying how different vectors (word embeddings, segment vectors, and one-hot vectors) are combined in the model would be helpful, potentially aided by an architectural illustration. Additionally, specifying the "designated loss function" used would be beneficial.
- Section 5.2 would benefit from mentioning how system weights were tuned for completeness.