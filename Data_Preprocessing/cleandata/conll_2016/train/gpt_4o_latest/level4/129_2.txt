The paper presents a method for in-domain data selection for SMT using a convolutional neural network classifier, building on the framework proposed by Johnson and Zhang (2015). The proposed approach achieves approximately 0.5 BLEU points improvement over language model-based data selection and demonstrates robustness even when only a very small in-domain dataset is available.
The authors report improvements of 3.1 BLEU points. However, the results indicate that gains of this magnitude are only observed when in-domain data are included in the training setâ€”training exclusively on the in-domain data already yields a +2.8 BLEU improvement. It would be valuable to compare this to a system that interpolates separate in-domain and out-domain models.
The second experiment, in my view, provides the more compelling result, showing that the CNN classifier remains effective even with minimal in-domain data. However, this experiment is conducted only on the zh2en task, where the training set includes actual in-domain data, potentially simplifying the selection process. Would similar results hold for other tasks where no in-domain data are present in the training set? The results for the en2es and en2zh tasks suggest this might be the case, as their development sets contain only a few hundred sentence pairs. To strengthen the claim, it would be helpful to report results for all tasks when training is limited to just 100 sentence pairs.
When translating social media text, one often encounters challenges distinct from other domains, such as a high OOV rate caused by non-standard spelling (particularly in Latin scripts). Additionally, such texts may include unique character sequences like usernames, hashtags, or emoticons. Was any specific preprocessing or filtering applied to address these issues? Since data selection alone cannot resolve the OOV problem, it would be insightful to understand the nature of the improvements achieved through adaptation via data selection, perhaps by providing concrete examples.
The following comments pertain to specific sections:
Section 3.2:
- Greater clarity is needed regarding how the various vectors (word embeddings, segment vectors, and one-hot vectors) are integrated within the model. An illustration of the architecture would be particularly beneficial.
- What exactly was the "designated loss function"?
Section 5.2:
- For completeness, it would be useful to mention how the system weights were tuned.