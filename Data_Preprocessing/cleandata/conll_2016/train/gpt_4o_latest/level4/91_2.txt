This paper investigates the application of translation model pruning techniques to neural machine translation (NMT). The authors examine three straightforward pruning strategies: two threshold and histogram-based methods applied individually to each weight class, and a third approach applied globally to the entire model. They demonstrate that retraining the pruned models can recover performance equivalent to the original full model, even when up to 90% of the weights are removed. Additionally, the authors provide a detailed analysis that highlights the advantages of the class-blind pruning approach and explains the performance improvements achieved through retraining.
Although the core idea of the paper is relatively simple, it appears highly relevant for memory-constrained NMT applications. I particularly appreciated the analysis section, which sheds light on the internal workings of model components that are often treated as opaque systems. While these insights are valuable in their own right, the paper's primary focus is on model compression. This argument could be made more compelling by including quantitative comparisons of the actual memory usage of the compressed and uncompressed models.
Minor comments:
- There is a significant body of prior work on pruning translation models in phrase-based statistical machine translation (SMT) that could be cited in the related work section. For example:
  - Johnson, J., Martin, J., Foster, G., and Kuhn, R.: Improving Translation Quality by Discarding Most of the Phrasetable. EMNLP 2007.
  - Zens, R., Stanton, D., and Peng, X.: A Systematic Comparison of Phrase Table Pruning Techniques. EMNLP 2012.
- Figure 5 was somewhat difficult to interpret. It might be more informative to include an additional bar plot beneath Figure 4 that illustrates the highest discarded weight magnitudes by class. This would also facilitate a clearer comparison across all pruning methods.