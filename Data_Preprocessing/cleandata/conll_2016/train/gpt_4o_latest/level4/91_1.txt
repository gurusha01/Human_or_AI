This paper explores three straightforward weight-pruning techniques for neural machine translation (NMT) and demonstrates that magnitude-based pruning outperforms the others. It also shows that retraining after pruning can restore the original performance, even under significant pruning levels.
The primary strength of this paper lies in the simplicity of the technique and the strong results achieved. The writing is clear, and the paper does a commendable job of situating the work within the context of prior research.
However, a notable weakness is the limited novelty of the work, as it primarily applies an established technique to a new type of neural network and task (NMT), yielding results that are not particularly unexpected. 
Additionally, the practical implications of the findings are unclear. Leveraging these results would require sparse matrix representations, which are challenging to optimize for GPU performance. Since speed, rather than memory, is the primary bottleneck in NMT, the relevance of pruning is questionable. If there is recent work addressing this issue, it should be discussed, and the paper should better articulate why pruning is important in this context.
One way to address this limitation could be to use the pruning results to guide architectural modifications. For example, Figure 3 suggests the possibility of reducing the number of hidden layers to two and potentially decreasing the dimensionality of source and target embeddings.
Another suggestion is to explore connections between pruning with retraining and dropout. For instance, the paper "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks" (Gal, arXiv 2016) might provide a useful theoretical link.
Detailed Comments:
- Line 111: Consider replacing "softmax weights" with "output embeddings," which may be a more precise term.
  
- Section 3.2: Referring to n as the "dimension" of the network is misleading. Presenting parameter sizes as integer multiples of n implies a logical constraint that may not exist.
- Line 319: Cite Bahdanau et al. for the attention mechanism rather than Luong, as the former introduced the concept.
- Section 3.3: The distinction between class-uniform and class-distribution pruning is minimal, as evidenced by their similar results. Consider removing one of these approaches.
- Figure 3: Suggest exploring hybrid pruning strategies, such as applying class-blind pruning for most classes and class-uniform pruning for embeddings.
- Figure 4: Include perplexity as an additional metric.
- Section 4.2 & Figure 6: Clarify which pruning method is being used.
- Figure 7: Specify whether the loss pertains to the training or test datasets.
- Figure 8: The diagram appears to omit softmax weights and is somewhat difficult to interpret. Consider providing relevant statistics, such as the proportion of each class removed by class-blind pruning at different levels.
- Line 762: Cite Le et al., "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units" (arXiv 2015), as a relevant reference.