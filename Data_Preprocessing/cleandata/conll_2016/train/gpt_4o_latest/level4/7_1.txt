I find some of the motivation convincing: the proposed method offers significantly faster training compared to training a neural network. Additionally, it preserves certain properties of the distribution when reducing dimensionality.
That said, I remain unconvinced about the importance of ensuring that vectors are transformable with PPMI.
Most critically, the work lacks a direct comparison to related approaches.
Detailed comments:
- p.3: The definition of Kendall's tau provided by the authors is unusual. It does not correspond to the original formula, and its origin or justification is unclear.
- p.3: Why not use Spearman correlation, which is the standard metric for semantic tasks (and is also employed by the authors during evaluation)?
- The datasets selected for evaluation are not the standard benchmarks typically used by the NLP community for measuring semantic relatedness. While it is commendable to explore alternative datasets, I strongly recommend including results on the standard benchmarks as well.
- Figure 1 appears to show only two lines. Where is the third line?
- There is no explicit comparison to related work, aside from a general statement.
Some typos:
- "large extend" should be "large extent."