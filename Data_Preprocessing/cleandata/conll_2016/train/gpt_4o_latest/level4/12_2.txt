The paper introduces a modification to the output layer of recurrent neural network models, enabling the learning of model parameters from both gold and projected annotations in low-resource languages. Specifically, the traditional softmax output layer, which defines a distribution over possible labels, is augmented with a fully connected layer that models the noise generation process. This results in an additional output layer representing the distribution over noisy labels.
Overall, this is a strong submission. The proposed method is well-suited, simple, and elegant. The paper demonstrates promising results on POS tagging tasks for eight simulated low-resource languages and two genuinely low-resource languages, leveraging a small set of gold annotations and a larger set of cross-lingually projected annotations for training. The method's modularity makes it appealing for researchers tackling various NLP problems in low-resource settings.
From a practical perspective, the experimental setup is somewhat unconventional. While there are scenarios, such as evaluations in certain DARPA-sponsored research projects, where building a POS tagger with as few as 1,000 token annotations might be necessary, such situations are relatively rare. A more comprehensive empirical validation would involve plotting the tagging accuracy of the proposed method (and baselines) against varying sizes of gold annotations. Such an analysis would address questions like: Does the proposed method negatively impact performance when ample gold annotations are available? What is the approximate threshold of gold annotations below which the method becomes advantageous? Does this threshold vary across target languages?
Beyond cross-lingual projections, noisy labels could also be derived from other sources (e.g., crowdsourcing) and potentially involve tag sets different from those of the gold annotations. While this opens up exciting possibilities for broader applicability, the paper only evaluates the method using cross-lingual projections with the same tag set.
It is noteworthy that the proposed training objective assigns equal weights to gold and noisy labels. Given the availability of a small gold-annotated corpus in the setup, it would have been insightful to explore whether tuning the relative contributions of the two terms in the objective function could yield better results.
In line 357, the paper describes the projected data as pairs of word tokens (x_t) and their vector representations (\tilde{y}) but does not explicitly clarify the nature of the vector representation (e.g., whether it is a distribution over cross-lingually projected POS tags for the word type). A pertinent question is whether the method remains effective if \tilde{y} is constructed using projected POS tags at the token level rather than aggregating predictions for the same word type. Additionally, since only one-to-one word alignments are retained, it is unclear how \tilde{y} is constructed for words that are never aligned.
Finally, in line 267, one of the two closing brackets should be replaced with an opening bracket.