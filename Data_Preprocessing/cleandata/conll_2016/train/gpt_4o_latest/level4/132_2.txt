This paper introduces a neural-inspired topic model that extends the word2vec objective to simultaneously learn document embeddings. These embeddings are then constrained via sparsification to emulate the output of a traditional topic model.
I found the proposed model to be innovative, and the examples provided by the authors were compelling and showed promise. However, the paper lacks any form of empirical evaluation, relying solely on illustrative examples without evidence of their representativeness or any direct comparison with standard or neural topic models. The absence of empirical evaluation makes it difficult to assess the actual value of the model, which significantly hinders the paper's acceptability. To address this, the authors could consider: (1) using the topic representation of each document in a supervised document classification task to benchmark against a topic model with the same number of topics (as an indirect measure of representation quality); or (2) performing direct evaluation on a dataset with document similarity annotations (e.g., pairwise comparisons of topic vectors).
While it is commendable that the authors plan to release their code, including the GitHub link in the submitted version compromises anonymity. This could be grounds for outright rejection, though I leave this decision to the program chairs.
Additional concerns:
- How were the examples in Figures 3-6 selected? Presenting only a subset of the topics raises concerns about potential cherry-picking.
- In Section 2.2.1, you discuss calculating word representations for topics based on pairwise comparisons with each word in the vocabulary. For a reasonably large vocabulary and number of topics, this approach seems computationally prohibitive. Is it truly feasible?
- In Section 3.1, you mention identifying "tokens" using SpaCy. Could you clarify how this was done? Did you extract only noun chunks (as in Section 3.2), or were other chunk types included? Additionally, since you use pre-trained word2vec embeddings (which include limited multiword terms), this process was not entirely clear.
- How does your model handle out-of-vocabulary (OOV) terms? While this may not be an issue in the experiments reported (since the model is trained on the entire document collection), applying the trained model to new documents could pose challenges. Specifically, the word2vec embeddings for OOV terms would not be aligned with the tuned embeddings, potentially affecting performance.
- The finding that 20 topics performed best on the 20 Newsgroups dataset is unsurprising given the dataset's structure. A simple evaluation could have been conducted using an information-theoretic comparison with the true document labels, allowing for direct comparison with models like LDA.
- You should compare your model with other neural topic models, such as:
  - Cao, Ziqiang, Sujian Li, Yang Liu, Wenjie Li, and Heng Ji. "A Novel Neural Topic Model and Its Supervised Extension." In AAAI, pp. 2210-2216. 2015.
  - Nguyen, Dat Quoc, Richard Billingsley, Lan Du, and Mark Johnson. "Improving Topic Models with Latent Feature Word Representations." Transactions of the Association for Computational Linguistics 3 (2015): 299-313.
  - Shamanta, Debakar, Sheikh Motahar Naim, Parang Saraf, Naren Ramakrishnan, and M. Shahriar Hossain. "Concurrent Inference of Topic Models and Distributed Vector Representations." In Machine Learning and Knowledge Discovery in Databases, pp. 441-457. Springer International Publishing, 2015.
Minor issues:
- Line 315: "it's similarity" should be corrected to "its similarity."
- Line 361: The phrase "topic basis" being "affected" is unclear, and the use of "are" here is awkward.
- Figure 5 caption: The term "words" might be better replaced with "terms."
- Reference formatting is inconsistent. For example, "Advances in ...", "Advances in Neural ...", and Roder et al. lack the conference name. Ensure uniformity in formatting.