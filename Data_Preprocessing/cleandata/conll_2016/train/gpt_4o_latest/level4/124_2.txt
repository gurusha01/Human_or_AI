This paper explores the use of eye-tracking features for sentiment analysis as a form of cognitive feature. I find the idea of leveraging eye-tracking features as a proxy for cognitive load in sentiment analysis to be quite intriguing.
The discussion on the features and the comparison of feature sets is both clear and informative. Additionally, I appreciate that the feasibility of the proposed approach is thoroughly addressed in Section 7.
I wonder if the evaluation could benefit from avoiding the conflation of different domains, such as the movie review corpus and the tweet corpus. For instance, the prediction accuracy for movie reviews (or tweets) might improve if tweets (or movie reviews) were excluded from the training data. This separation could also make the results easier to interpret. The results in Table 2 appear relatively low compared to state-of-the-art results for the Pang and Lee dataset but are much more competitive when compared to results for Twitter data.
In Section 3.3, I assume there are no overlapping snippets between the training and testing data for datasets 1 and 2, correct? This holds even if they originate from the same sources, such as Pang & Lee and Sentiment 140.
Minor: some of the additional use of bold text is somewhat distracting (or perhaps that's just my personal preference).