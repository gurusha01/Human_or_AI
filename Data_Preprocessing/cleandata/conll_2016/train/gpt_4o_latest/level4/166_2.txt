This paper addresses the cross-lingual direct transfer of NER models by leveraging a recently proposed cross-lingual wikification model. Overall, the central idea is not particularly groundbreaking or novel, as it does not introduce any fundamentally new technology. The contribution is primarily incremental, combining two existing research directions: (1) direct transfer methods for downstream NLP tasks (e.g., NER, parsing, or POS tagging), and (2) the latest advancements in cross-lingual wikification techniques. Nevertheless, I found the paper to be well-constructed, presenting a coherent and clear narrative supported by sufficient experiments and empirical evidence, with results that are convincing. However, I do have several comments regarding the presentation of the work.
Related Work: The related work section would benefit from a more detailed discussion on how this paper connects to the work of Kazama and Torisawa (2007). Additionally, the authors should explicitly highlight the differences between their approach and other NER systems that utilize encyclopedic knowledge from Wikipedia. While these distinctions are mentioned in the text, they need to be emphasized further to improve readability and help situate the work within the broader research context.
Although the authors justify their decision to exclude POS tags as features, it would still be valuable to include experiments incorporating POS tag features, similar to Tackstrom et al. This would provide readers with empirical evidence on the utility (or lack thereof) of such features across different languages, at least for those with universal POS tag annotations available.
Section 3.3: This section would benefit from a running example, as the description of the modified model from Tsai and Roth is not entirely clear. A concrete example could help clarify how the model operates in its current form.
The authors frequently mention that the approaches by Tackstrom et al. (2012) and Nothman et al. (2012) are orthogonal to their method and could be combined with it. It would strengthen the paper if preliminary results were provided for a subset of languages using these combined approaches. This would enrich the discussion and provide additional insights. Similarly, while the authors note that their approach is orthogonal to projection-based methods, it would be useful to compare their results with a strong projection baseline. This would further contextualize the performance of wikification-based methods and highlight their strengths or limitations.
The observation that Dutch is the best training language for Spanish and Spanish is the best for Yoruba raises an intriguing question. Is this merely a statistical coincidence, or is there a deeper linguistic or structural explanation? A more in-depth discussion of these findings would add significant value to the paper.
While the proposed idea is sound, the results in Table 4 are not particularly compelling, as the improvements are minor and not consistent across all scenarios. Including a statistical significance test for the results in Table 4 would help substantiate the claims.
Minor Comments:
- Section 2.1: Projection can also be achieved through methods that do not rely on parallel data, making such models more broadly applicable, even for languages lacking parallel resources. For example, see the work of Peirsman and Pado (NAACL 2009) or Vulic and Moens (EMNLP 2013), which utilize bilingual semantic spaces instead of direct alignment links for transfer.
- Several typos were identified in the text, and the paper would benefit from more thorough proofreading. Examples include: the first sentence of Section 3 ("as a the base model"), a non-parsable sentence on Page 3 ("They avoid the traditional pipeline of NER then EL by..."), and "to disambiguate every n-grams" on Page 8.