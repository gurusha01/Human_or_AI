A combination of word2vec and LDA has the potential to be interesting. However, the primary issue with this paper lies in the lack of clarity in its technical details. Section 2 requires a complete rewrite to enable readers familiar with word2vec and LDA to grasp a high-level understanding of how the models are integrated. The current explanation fails to achieve this goal.
Detailed comments:
The third paragraph of the introduction is unclear. For instance, what does "requires deriving a new approximation" refer to? An approximation of what? Additionally, why is developing prototypes described as time-consuming? And why is feature evaluation considered easier?
Why are the same word vectors used for both pivot and target, unlike in word2vec? What is the rationale behind this choice?
What is meant by separating words from a marginal distribution?
What is the definition of co-adaptation in this context?
The phrase "If we only included structure up to this point" is vagueâ€”what type of structure is being referred to?
The phrase "it's similarity" should be corrected to "its similarity."
Footnote 1 compromises anonymity.
The paper appears to lack any formal evaluation. Simply providing example clusters is no longer sufficient in NLP research. Although Figure 2 seems to suggest a quantitative evaluation, it is inadequately explained and buried within an overly lengthy caption.
The claim in the conclusion that the model solves word analogies is an overstatement. The paper only presents a few cherry-picked examples, such as king + queen, which do not substantiate the broader claim.
Finally, the reference to Chang lists the venue as "Advances in ...," which is incomplete. It is unreasonable to expect the reader to guess the full conference or journal name.