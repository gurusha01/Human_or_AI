General Comments:
This paper investigates the relationship between part-of-speech (PoS) tags and word embeddings. The authors leverage word embeddings to derive some intriguing, albeit somewhat predictable, insights into the consistency of PoS tags and the evident link between word vector representations and PoS categories. A notable strength of the paper lies in its detailed error analysis, particularly the examination of classification outliers.
That said, the paper overlooks a critical foundational point: the original motivation for PoS tagging in corpora like the BNC. These corpora were not designed solely for linguistic exploration of morphosyntactic categories (which are often grounded in semantic prototype theory, e.g., Croft, 1991) but were primarily tagged to support downstream NLP tasks, such as parsing. Consequently, the discussion could be reframed to evaluate whether the distinctions captured by distributional vectors offer greater utility for parsing compared to the original tags (or even UPOS).
Additionally, the paper lacks sufficient engagement with prior work on distributional PoS induction. A good starting point would be the review by Christodoulopoulos et al. (2010), along with more recent non-DNN approaches such as Blunsom and Cohn (2011), Yatbaz et al. (2012), and others. When viewed in the context of this body of work, the findings in Section 5 appear to offer limited novelty, as there are systems with stricter constraints on external knowledge that achieve comparable results.
Specific Issues:
In the abstract, one of the claimed contributions is that "distributional vectors do contain information about PoS affiliation." If this interpretation is correct, it is hardly a novel finding, particularly for English. Nearly every distributionally-based PoS induction system over the past 15 years that reports "many-to-one" or "cluster purity" metrics demonstrates this same result.
The claim in lines 79-80 ("relations between... vectors... are mostly semantic") is inaccurate. Papers such as <MIKOLOV or COLOBERT> and subsequent work clearly show that these vectors encode substantial syntactic information. This is also evident from prior comments on cluster purity scores. Interestingly, this statement is contradicted in the opening of Section 2 (lines 107-108).
Why transition to UPOS? The fine-grained distinctions in the original tagset seem to provide richer insights and are arguably more compelling.
Footnote 3 is unclear. Were the failed attempts your own experiments or those of prior work? What criteria were used to determine failure? Additionally, what about Brown cluster vectors? These align almost perfectly with UPOS tags and warrant discussion.
The observation that "proper nouns are not much similar to common nouns" (lines 331-332) seems unremarkable. Isn't this difference largely explained by the presence of "the" (the most frequent function word), which predominantly co-occurs with common nouns?
While the focus on frequent word/tag pairs is understandable for practical reasons, it would be valuable to analyze the behavior in the tail of the distribution. Specifically, examining low-frequency words could shed light on both the vector representations and the types of errors made by the classifier. This could inspire alternative approaches that incorporate features beyond pure distributional and morphological (e.g., lemmatized) signals to better generalize PoS tags for infrequent words.
Minor Issues:
Update sentential references to use \newcite{} formatting, e.g., "Mikolov et al. (2013b) showed."