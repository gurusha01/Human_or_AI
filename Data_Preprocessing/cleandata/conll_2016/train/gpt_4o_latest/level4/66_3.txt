This paper revisits the long-standing problem of joint semantic and syntactic dependency parsing, leveraging recent advancements in neural network models. Joint models remain one of the most promising aspects of the success of transition-based neural network parsers.
The paper makes two main contributions. First, it introduces a novel transition system that appears to improve upon the Hendersen (2008) system on which it is based. Second, it demonstrates that neural networks perform well on this problem, where linear models have historically struggled. The authors attribute this success to the neural network's ability to automatically learn relevant feature representations. However, I believe there is an additional advantage worth highlighting. In linear models, a weight must be learned for each feature/class pair, which significantly increases the number of parameters when jointly learning two tasks. Neural networks, by contrast, are far more parameter-efficient in this regard.
I suspect that the proposed transition system would perform equally well with other neural network architectures, such as the global beam-search model of Andor (2016). Additionally, there are numerous orthogonal improvements that could be explored. I anticipate that extensions to the authors' approach could achieve state-of-the-art results.
It would be valuable to see an effort to derive a dynamic oracle for this transition system, even if only as an appendix or in future work. At first glance, the system appears somewhat analogous to the arc-eager oracle. Specifically, the M-S action excludes all semantic arcs between the word at the start of the buffer and the words on the semantic stack, while the M-D action excludes all semantic arcs between the word at the top of the stack and the words in the buffer. The L and R actions seem to exclude the reverse arc and no others.