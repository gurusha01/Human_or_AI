I previously reviewed this paper when it was submitted as a short paper draft for ACL 2016. At that time, the experimental setup had a flaw, which has now been addressed and corrected.
Back then, I mentioned that I would be open to accepting the draft for another *ACL event, provided the flaw was resolved. Since that issue has been rectified, I see no reason to withhold acceptance now.
Another reviewer noted that the paper's setup might appear somewhat artificial when considering real low-resource languages, particularly regarding the trade-offs between finding and paying annotators. I believe this point should be explicitly addressed in the writeup to avoid overstating the method's practicality.
There are also relevant works in annotation projection for extremely low-resource languages, such as Johannsen et al. (2016, ACL) and Agic et al. (2015, ACL). Including a discussion of these in the related work section would enhance the paper's completeness.
In conclusion, I find this to be a valuable contribution and recommend acceptance.
It should be clarified whether the data used in the study will be made publicly available. For now, I evaluate this aspect in good faith, assuming that the data will be accessible for research purposes.