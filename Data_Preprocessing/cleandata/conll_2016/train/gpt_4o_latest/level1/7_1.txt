Review of the Paper
Summary and Contributions
The paper introduces Positive-Only Projection (PoP), a novel technique for constructing semantic spaces and word embeddings using random projections. Unlike traditional random projection methods, PoP employs a random projection matrix \( R \) with \( E(R) > 0 \), enabling the application of weighting techniques like Positive Pointwise Mutual Information (PPMI) to the resulting embeddings. The authors demonstrate that PoP, combined with PPMI, achieves competitive results in semantic similarity tasks (e.g., MEN relatedness test) while being computationally efficient and scalable. The primary contributions of the paper are:
1. Introduction of PoP as a Randomized Dimensionality Reduction Method: PoP constructs semantic spaces incrementally and efficiently, addressing limitations of existing random projection-based methods (e.g., RI) by enabling post-projection transformations like PPMI.
2. Competitive Performance with Reduced Computational Costs: The PoP+PPMI+Pearson combination achieves a Spearman correlation of 0.75 on the MEN test, comparable to state-of-the-art neural embeddings, but without the need for computationally expensive training.
3. Theoretical and Empirical Exploration of PoP Parameters: The paper provides a detailed analysis of the impact of dimensionality, sparsity, and weighting transformations on PoP's performance, offering insights into its behavior and optimization.
Strengths
1. Computational Efficiency: The PoP method is highly scalable and efficient, requiring significantly less computational power compared to neural embeddings. This makes it a practical alternative for resource-constrained settings.
2. Compatibility with Weighting Techniques: The ability to apply PPMI to PoP-constructed spaces is a major strength, as it bridges the gap between random projection methods and count-based models, leveraging the benefits of both.
3. Empirical Validation: The paper provides extensive experimental results, including comparisons with RI and count-based models, as well as detailed parameter studies. The results are robust and demonstrate the method's effectiveness across different configurations.
4. Clarity of Methodology: The paper clearly explains the PoP method, its mathematical foundations, and its implementation, making it accessible to readers familiar with random projection techniques and distributional semantics.
Weaknesses
1. Lack of Theoretical Justification: While the empirical results are promising, the paper lacks a rigorous theoretical analysis of PoP's properties, such as bounds on the error \( \delta \) introduced by the randomization process. This limits the method's interpretability and generalizability.
2. Limited Evaluation Scope: The evaluation focuses primarily on the MEN relatedness test. Additional benchmarks, such as SimLex-999 or downstream tasks (e.g., text classification), would strengthen the claims of general applicability.
3. Sensitivity to Parameters: The paper acknowledges that PoP's performance is influenced by parameters like dimensionality and sparsity. However, the lack of clear guidelines for selecting these parameters in practice may hinder adoption.
4. Comparison with Neural Embeddings: While the paper claims competitive performance with neural embeddings, it does not provide direct comparisons with specific models like Word2Vec or GloVe on the same dataset, making it difficult to contextualize the results.
Questions to Authors
1. Can you provide a theoretical analysis or bounds for the error \( \delta \) introduced by the PoP method? How does this compare to existing random projection techniques like RI?
2. Have you evaluated PoP on additional benchmarks or downstream NLP tasks? If not, do you plan to extend the evaluation in future work?
3. How does PoP perform when integrated with neural embedding techniques as an initialization or preprocessing step, as suggested in the discussion?
4. Could you elaborate on the choice of Kendall's \( \tau_b \) as the similarity measure? Have you tested alternative measures for non-Gaussian spaces?
Recommendation
The paper presents a novel and computationally efficient method for constructing semantic spaces, with promising results in semantic similarity tasks. However, the lack of theoretical justification and limited evaluation scope are notable weaknesses. I recommend acceptance with minor revisions, contingent on addressing the theoretical and evaluation concerns during the author response period.