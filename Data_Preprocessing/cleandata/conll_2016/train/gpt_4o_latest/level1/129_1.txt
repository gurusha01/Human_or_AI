Review of the Paper
Summary
The paper introduces a novel method for domain adaptation in Statistical Machine Translation (SMT) using semi-supervised Convolutional Neural Networks (CNNs) for data selection. The proposed approach addresses the challenge of adapting SMT systems when only a small amount of in-domain data is available. By training a semi-supervised CNN classifier on in-domain and general-domain data, the method assigns domain relevance scores to general-domain sentences, selecting the most relevant ones for SMT training. Experimental results across four language pairs and three test domains demonstrate significant improvements in BLEU scores compared to both baseline systems and state-of-the-art language model-based data selection methods. Notably, the method achieves robust performance even with as few as 100 in-domain sentences.
Contributions
1. Semi-Supervised CNN for Data Selection: The primary contribution is the introduction of a semi-supervised CNN framework for domain classification and data selection. This method effectively leverages word embeddings trained on large unlabeled corpora, enabling robust classification even with minimal in-domain data. The use of convolution and max-pooling layers highlights domain-specific features while ignoring general-domain noise, which is a key innovation.
   
2. Practical Application to Low-Resource Scenarios: The paper demonstrates that the proposed method can achieve significant BLEU improvements (up to 3.1 points) even when the in-domain data is limited to 100 sentences. This makes the approach particularly valuable for fine-grained or emerging domains, such as social media content, where large-scale in-domain data is often unavailable.
3. Empirical Validation Across Multiple Tasks: The extensive experiments on four language pairs and diverse domains (e.g., SMS, tweets, Facebook posts) validate the generalizability of the method. The results consistently outperform both baseline systems and state-of-the-art LM-based methods, showcasing the robustness of the approach.
Strengths
1. Significant Performance Gains: The method achieves substantial BLEU score improvements over strong baselines, particularly in low-resource settings. The results are statistically significant and demonstrate the practical utility of the approach.
   
2. Robustness with Minimal In-Domain Data: The ability to achieve meaningful improvements with as few as 100 in-domain sentences is a major strength. This addresses a critical gap in domain adaptation for SMT, where large in-domain datasets are often unavailable.
3. Innovative Use of CNNs: The paper effectively adapts CNNs, traditionally used for image and text classification, to the domain adaptation problem in SMT. The combination of convolutional layers, max-pooling, and word embeddings is well-motivated and empirically validated.
4. Comprehensive Evaluation: The experiments are thorough, comparing the proposed method against multiple baselines and analyzing its performance under varying amounts of in-domain data. The inclusion of statistical significance testing further strengthens the results.
Weaknesses
1. Limited Discussion of Computational Efficiency: While the method is empirically effective, the paper does not provide sufficient details on its computational efficiency compared to LM-based methods. Given the large-scale nature of SMT datasets, this is an important consideration for practical deployment.
2. Dependence on Pre-Trained Word Embeddings: The reliance on high-quality word embeddings trained on large general-domain corpora may limit the applicability of the method in truly low-resource languages or domains where such embeddings are unavailable.
3. Lack of Ablation Studies: The paper does not include ablation studies to isolate the contributions of different components of the semi-supervised CNN (e.g., word embeddings vs. convolution layers). Such analysis would provide deeper insights into the method's effectiveness.
Questions to Authors
1. How does the computational cost of training the semi-supervised CNN compare to LM-based data selection methods, particularly for large datasets?
2. Could the method be extended to languages or domains where pre-trained word embeddings are unavailable? If so, how would the performance be affected?
3. Did you explore alternative architectures (e.g., transformers) for the classification task, and if so, how do they compare to CNNs?
Additional Comments
Overall, this paper presents a compelling and well-validated approach to domain adaptation for SMT. Addressing the weaknesses and providing further analysis on computational efficiency and architectural choices could strengthen the work further.