Review of the Paper
Summary  
This paper introduces a transition-based parser for joint syntactic and semantic dependency parsing that employs stack LSTMs to represent the entire algorithmic state. The proposed parser operates with a greedy inference algorithm that achieves linear runtime complexity, making it computationally efficient. The model is evaluated on the CoNLL 2008 and 2009 shared tasks, where it achieves state-of-the-art performance among joint models, demonstrating competitive results compared to pipeline-based systems. The authors emphasize the model's ability to bypass expensive, expert-crafted features by leveraging learned representations, which is particularly beneficial for practical applications.
Main Contributions  
1. Representation Learning for Entire Algorithmic State: The paper's primary contribution is the use of stack LSTMs to encode the complete state of the parsing algorithm. This approach eliminates the need for handcrafted features, which are typically expensive to design and compute, and allows the model to learn representations directly from data. This is a significant advancement over prior joint models, which relied on limited, locally scoped features.
2. Greedy Parsing with Linear Runtime: The authors propose a highly efficient greedy inference algorithm that performs joint parsing in linear time. This makes the system practical for real-world applications, as evidenced by its fast runtime on the CoNLL datasets.
3. Empirical Results on Joint Parsing Tasks: The model achieves state-of-the-art performance among joint syntactic-semantic parsers on the CoNLL 2008 and 2009 tasks. It also demonstrates competitive results in multilingual settings, showcasing its adaptability across languages.
Strengths  
1. Efficiency: The linear runtime complexity and fast end-to-end runtime make the proposed parser highly practical for large-scale applications. The reported runtime is significantly faster than many pipeline-based systems, which often require complex feature extraction and global optimization.
2. Elimination of Handcrafted Features: By leveraging stack LSTMs, the model avoids the need for expensive, expert-designed features, which is a notable step forward in reducing the reliance on linguistic intuition in parser design.
3. Strong Empirical Performance: The parser outperforms previous joint models on the CoNLL tasks and achieves results comparable to pipeline systems, demonstrating the effectiveness of the proposed approach.
4. Open-Source Implementation: The authors' commitment to releasing an open-source implementation enhances the reproducibility and practical utility of their work.
Weaknesses  
1. Overfitting in Joint Parsing: The joint model shows signs of overfitting on in-domain data (WSJ) compared to the hybrid model, which performs better on out-of-domain data (Brown corpus). This suggests that the joint approach may lack robustness in cross-domain scenarios.
2. Limited Comparison to State-of-the-Art SRL Systems: While the model performs well among joint parsers, it falls short of state-of-the-art SRL systems that use pipeline approaches with global optimization and additional features. The paper could have explored ways to integrate such advances into the proposed framework.
3. Dependence on Large Training Data: The model's performance in low-resource settings (e.g., Japanese) is relatively weak, highlighting its reliance on large training datasets. This limitation could have been addressed by incorporating techniques for low-resource learning, such as transfer learning or character-based embeddings.
Questions to Authors  
1. How does the proposed model handle low-resource languages or domains with limited training data? Have you considered transfer learning or other techniques to improve performance in such cases?  
2. Could the joint model's overfitting to in-domain data be mitigated by incorporating regularization techniques or domain adaptation strategies?  
3. How does the model's performance compare when additional linguistic features (e.g., morphological features) are incorporated, especially for morphologically rich languages?  
Additional Comments  
The paper is well-written and provides a thorough description of the proposed model and its evaluation. However, the discussion of limitations and potential future directions could be expanded to provide a more balanced perspective. Overall, this work represents a significant contribution to the field of joint syntactic and semantic parsing.