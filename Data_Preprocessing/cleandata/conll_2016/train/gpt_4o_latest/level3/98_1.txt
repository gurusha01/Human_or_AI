Review
Summary and Contributions
This paper investigates the performance of delexicalized transfer parsers and a minimally supervised dependency parser enriched with external probabilities on Universal Dependencies (UD) treebanks. The authors compare multiple parser configurations, including fully unsupervised, minimally supervised with handcrafted rules, and minimally supervised with probabilities learned from other treebanks. The primary contribution is the demonstration that minimally supervised parsers can outperform delexicalized transfer parsers for less-resourced languages, particularly those outside the Indo-European family. Additionally, the paper highlights the utility of UD treebanks in harmonizing annotation styles, which improves cross-lingual parsing performance.
Strengths
1. Relevance to Low-Resource Languages: The paper addresses an important problem in NLPâ€”parsing for less-resourced languages. The finding that minimally supervised parsers perform better for non-Indo-European languages is a valuable insight for the community.
2. Use of Universal Dependencies: By leveraging UD treebanks, the paper benefits from a standardized annotation style, which is a significant step forward compared to previous work.
3. Comprehensive Experiments: The authors evaluate their methods on 32 languages, providing a broad perspective on the performance of different parsers across diverse linguistic families.
Weaknesses
1. Suboptimal Baseline Choices: The use of MaltParser with default settings is a significant limitation. Tools like MaltOptimizer or optimized configurations of MSTParser should have been employed to ensure a fair comparison.
2. Lack of Benchmarking with Recent Parsers: The paper does not explore or compare its results with more recent cross-lingual parsers, such as those based on neural architectures, which could provide stronger baselines.
3. Arbitrary Probabilities in Tables 3-4: The inclusion of manually assigned probabilities appears arbitrary and lacks sufficient justification or experimental validation. A deeper exploration of their impact is necessary.
4. Insufficient Context on McDonald et al. (2011): The authors should clarify that McDonald et al. (2011) could not use UD treebanks due to their unavailability at the time, to avoid misinterpretation of the comparison.
5. Limited Discussion of Prior Work: The paper omits several relevant references, such as Tiedemann (2015), Vilares et al., Guo et al. (2015), and Ammar et al., which could provide a more comprehensive context for the contributions.
Questions to Authors
1. Why were more recent neural cross-lingual parsers not included in the benchmarking? Would these parsers outperform the minimally supervised approach for less-resourced languages?
2. Can you provide additional experiments or analysis to justify the manually assigned probabilities in Tables 3-4? How sensitive are the results to these values?
3. Have you considered using optimization tools like MaltOptimizer for MaltParser? If not, how do you justify the use of default settings?
Additional Comments
- Table 1 could be condensed into a footnote, as it does not add significant value to the main discussion. Table 2, however, is relevant and should be retained.
- The claim that minimally supervised parsers are better for less-resourced languages is intriguing but needs further validation with newer cross-lingual parsers to strengthen the argument.
Recommendation
While the paper provides interesting insights into parsing for less-resourced languages, its methodological limitations and lack of benchmarking with state-of-the-art parsers weaken its impact. I recommend a major revision to address these issues.