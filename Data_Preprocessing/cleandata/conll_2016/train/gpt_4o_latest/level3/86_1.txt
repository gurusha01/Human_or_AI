Review of the Paper
Summary and Contributions  
The paper presents two sentiment analysis tools, App2Check and Tweet2Check, designed for app reviews and tweets in both Italian and English. These tools utilize supervised learning techniques to quantify sentiment polarity on a 1â€“5 scale. The authors claim that their tools outperform 19 state-of-the-art research tools and commercial solutions in terms of accuracy and macro-F1 scores. The paper also introduces new benchmark datasets for app reviews and tweets, which are made available for the research community. While the authors highlight the tools' superior performance, the lack of methodological transparency due to "non-disclosure restrictions" significantly limits the paper's scientific contribution.
Strengths  
1. Empirical Performance: The experimental results suggest that App2Check and Tweet2Check achieve higher accuracy and macro-F1 scores compared to existing tools, particularly in app reviews. The tools also demonstrate robustness across multiple datasets and languages.  
2. Benchmark Datasets: The inclusion of new benchmark datasets for app reviews and tweets is a valuable contribution to the sentiment analysis community, as it provides resources for further research and evaluation.  
3. Practical Relevance: The tools address specific use cases, such as app reviews and tweets, which are highly relevant for real-world applications like brand monitoring and customer feedback analysis.
Weaknesses  
1. Lack of Methodological Transparency: The paper does not disclose critical details about the learning algorithms, training data, or resources used to develop the models, citing "non-disclosure restrictions." This omission makes it impossible to evaluate the novelty, reproducibility, or scientific rigor of the work.  
2. Limited Research Contribution: Without methodological details, the paper does not qualify as research. It reads more like a product demonstration, which is unsuitable for a research conference.  
3. Overemphasis on Performance Metrics: While the results are impressive, the paper focuses solely on empirical performance without providing insights into the underlying techniques or innovations. This limits its utility for advancing the field.  
4. Unclear Novelty: The paper does not clarify how the proposed tools differ from or improve upon existing supervised learning approaches for sentiment analysis. The lack of comparative discussion on methodology makes it difficult to assess the novelty of the work.
Questions to Authors  
1. Can you provide more details about the supervised learning techniques used in App2Check and Tweet2Check? Specifically, what algorithms and feature engineering methods were employed?  
2. What is the size and composition of the training data used to develop the predictive models? Were any domain-specific preprocessing or augmentation techniques applied?  
3. How do the tools handle challenges such as sarcasm, multilingual text, or domain adaptation?  
Recommendation  
The paper is unsuitable for acceptance in its current form due to the lack of methodological transparency and limited research contribution. A demo session submission might be more appropriate, but even that would require the authors to disclose more details about the tools' underlying methods and architecture.