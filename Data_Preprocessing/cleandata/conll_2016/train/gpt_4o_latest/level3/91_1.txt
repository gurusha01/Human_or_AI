Review of the Paper
Summary and Contributions:  
This paper investigates weight pruning as a compression technique for Neural Machine Translation (NMT) models, focusing on three magnitude-based pruning schemes: class-blind, class-uniform, and class-distribution. The primary contribution is the finding that class-blind pruning, combined with retraining, is the most effective approach, achieving up to 80% pruning with no performance loss on a state-of-the-art NMT system. The authors also provide insights into the distribution of redundancy in NMT architectures, revealing that higher layers, attention, and softmax weights are more critical than lower layers and embedding weights. Additionally, the paper demonstrates that pruning can act as a form of regularization, improving performance beyond the baseline in some cases.
Strengths:  
1. Simplicity of Approach: The use of magnitude-based pruning is straightforward and computationally efficient, making it accessible for practical applications.  
2. Strong Results: The paper demonstrates significant compression (up to 80%) without performance degradation, which is a notable achievement for large-scale NMT models.  
3. Well-Written and Comprehensive: The paper is clearly written, with a thorough review of prior work and detailed experimental analysis. The visualization of redundancy patterns (e.g., Figure 8) provides valuable insights into NMT architectures.  
4. Empirical Rigor: The experiments are well-designed, with comparisons across different pruning schemes and detailed evaluations of performance metrics such as BLEU and perplexity.  
Weaknesses:  
1. Lack of Novelty: The work primarily applies a known technique (magnitude-based pruning) to NMT, yielding results that are unsurprising given prior work in other domains like CNNs.  
2. Limited Practical Significance: While the compression results are impressive, the practical utility is unclear due to challenges in leveraging sparse matrix representations on GPUs, where speed is often a greater bottleneck than memory in NMT systems.  
3. Underexplored Connections: The paper does not explore connections between pruning and related techniques like dropout, which could provide a deeper theoretical understanding.  
Suggestions for Improvement:  
1. Architecture Optimization: Use the pruning insights to guide architectural changes, such as reducing hidden layers or embedding dimensions, to achieve compression without relying on sparsity.  
2. Dropout Connections: Investigate the relationship between pruning and dropout, referencing relevant works like Gal (2016), to provide a theoretical foundation for the observed regularization effects.  
3. Pruning Scheme Simplification: Consider dropping either class-uniform or class-distribution pruning, as their similarity adds little value to the analysis.  
4. Clarifications and Revisions:  
   - Line 111: Replace "softmax weights" with "output embeddings" for clarity.  
   - Section 3.2: Avoid referring to "n" as the "dimension" of the network; parameter scaling by "n" is not a logical constraint.  
   - Line 319: Cite Bahdanau et al. for the attention mechanism instead of Luong.  
Questions to Authors:  
1. Can the sparsity achieved through pruning be leveraged to reduce training or inference time on GPUs? If not, how do you envision the practical deployment of these compressed models?  
2. Have you considered iterative pruning and retraining, as suggested in Han et al. (2015b), to achieve even higher compression rates?  
Additional Comments:  
- Figure 3: A hybrid pruning approach (e.g., class-blind for most classes, class-uniform for embeddings) could be explored for better trade-offs.  
- Figure 4: Include perplexity results to complement BLEU scores.  
- Figure 7: Specify whether the loss pertains to the training or test corpora.  
- Figure 8: Add statistics on class removal proportions for better interpretability.  
- Line 762: Cite Le et al. (2015) for initializing recurrent networks with ReLUs.  
Conclusion:  
While the paper provides strong empirical results and valuable insights into NMT redundancy, the lack of novelty and unclear practical significance limit its impact. Addressing the suggested improvements and exploring connections to related techniques could significantly enhance the contribution of this work.