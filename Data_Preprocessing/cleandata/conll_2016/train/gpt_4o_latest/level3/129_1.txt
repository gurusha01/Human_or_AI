Review of the Paper
Summary and Contributions
This paper introduces a semi-supervised convolutional neural network (CNN)-based method for selecting in-domain training data for statistical machine translation (SMT). The proposed approach is particularly effective when only a small amount of in-domain data is available, addressing a critical challenge in domain adaptation. The authors demonstrate that their method outperforms state-of-the-art language model (LM)-based data selection techniques, achieving up to a 3.1 BLEU improvement in certain tasks. The key contributions of the paper, as I see them, are:
1. Semi-Supervised CNN for Data Selection: The novel use of semi-supervised CNNs for domain classification and data selection, leveraging word embeddings learned from unlabeled data, is a significant methodological advancement.
2. Performance with Limited In-Domain Data: The ability of the proposed method to achieve substantial improvements with as few as 100 in-domain sentences is a notable contribution, highlighting its practicality for low-resource scenarios.
3. Empirical Validation: The paper provides extensive experimental results across multiple language pairs and domains, demonstrating the robustness and generalizability of the proposed method.
Strengths
1. Novelty and Practicality: The use of CNNs for domain adaptation in SMT is novel and well-motivated. The method's ability to perform well with minimal in-domain data makes it highly practical for real-world applications, especially in emerging domains like social media.
2. Strong Empirical Results: The proposed method consistently outperforms baseline LM-based approaches, including n-gram and RNN-based methods, across multiple tasks and datasets. The results are statistically significant and well-supported.
3. Clear Explanation of CNN Architecture: The textual description of the CNN architecture is detailed and easy to follow, aiding reproducibility.
4. Robustness to Data Scarcity: The experiments convincingly show that the method remains effective even with extremely limited in-domain data, outperforming LM-based approaches in such scenarios.
Weaknesses
1. Comparison Gap: The paper does not compare its method against bilingual data selection techniques, such as the sum of cross-entropy differences (Moore and Lewis, 2010). This omission makes it difficult to fully assess the relative performance of the proposed approach.
2. Lack of Visual Representation: While the CNN architecture is well-described, a visual representation of the layers would greatly enhance understanding, especially for readers less familiar with CNNs.
3. Experimental Clarity: The process for determining the number of selected in-domain sentences and the hyperparameter tuning of the CNN models is not clearly explained, leaving some ambiguity in the experimental setup.
4. Baseline Comparison: The paper does not compare its method to Axelrod (2015), which uses POS tags to reduce LM data sparsity. This comparison could help assess the added value of word embeddings in the proposed approach.
5. Limited Exploration of Representations: While the authors justify the use of CNNs over RNNs/LSTMs, they do not experimentally explore the impact of bag-of-words (BOW) versus sequential (SEQ) representations, leaving a gap in understanding the significance of these choices.
Questions to Authors
1. How does the proposed method compare to bilingual data selection techniques, such as the sum of cross-entropy differences? Could this comparison be added to strengthen the evaluation?
2. Could you clarify the process for determining the number of selected in-domain sentences and the hyperparameter tuning of the CNN models?
3. Have you considered extending the CNN model to a bilingual/parallel setting to combine source and target classification scores? If so, what challenges do you foresee?
Additional Comments
- Addressing minor citation formatting issues (e.g., \citet vs. \citep) would improve the overall presentation quality.
- Including example sentences selected by different methods in the results or discussion section would provide stronger qualitative support for the claims in Section 5.4.
- Adding a reference to Lewis-Moore LM data selection (sum of cross-entropy differences) around line 435 would make the related work section more comprehensive.
Recommendation
Overall, this paper makes a strong contribution to the field of domain adaptation for SMT, particularly in low-resource settings. While there are some gaps in comparison and experimental clarity, the novelty, practicality, and empirical strength of the proposed method make it a valuable addition to the literature. With minor revisions to address the identified weaknesses, this paper would be a strong candidate for acceptance.