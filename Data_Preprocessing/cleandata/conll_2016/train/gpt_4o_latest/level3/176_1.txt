Review of the Submission
Summary and Contributions
This paper proposes a unified framework for addressing multiple sentence pair scoring tasks by treating them as instances of understanding semantic relations between sentences. The authors aim to develop task-independent neural network models that can be fine-tuned for specific tasks. The primary contributions of the paper are as follows:
1. Unified Framework for Sentence Pair Scoring: The paper consolidates various f2-type tasks (e.g., Answer Sentence Selection, Textual Entailment, Semantic Textual Similarity) under a single framework, enabling cross-task benchmarking and evaluation.
2. Transfer Learning for Task Generalization: The authors demonstrate the potential of transfer learning by pretraining on one task (Ubuntu Dialogue Dataset) and fine-tuning on others, showing promising results.
3. Open-Source Framework: The release of the dataset-sts software package and associated tools for modular implementation of models and datasets is a valuable contribution to the research community.
Strengths
1. Novel Perspective: Treating sentence pair scoring tasks as instances of a broader semantic relation problem is an exciting and promising approach. It has the potential to unify fragmented research efforts in this domain.
2. Transfer Learning Results: The demonstration of cross-task transfer learning (e.g., Ubu. RNN model) highlights the feasibility of developing general-purpose models for semantic comprehension, which is a significant step forward.
3. Open-Source Contribution: The availability of the dataset-sts framework and tools for reproducibility and further research is commendable and aligns with the community's push for open science.
Weaknesses
1. Incomplete Results Table: The first results table is missing, which hinders the ability to fully evaluate the claims and contributions of the paper. This omission must be addressed before publication.
2. Lack of Dataset Delivery: The abstract promises a new dataset, but the paper fails to deliver it. This discrepancy undermines the credibility of the work and should be resolved.
3. Insufficient Discussion of Results: The analysis of results is too brief, lacking detailed interpretation and insights into the reasons for success or failure across tasks. For example, the variability in performance across tasks is not adequately explained.
4. Missing Examples: The paper does not include example instances from the datasets, particularly challenging cases, which would help contextualize the results and demonstrate the model's capabilities.
5. Unclear Recommendation of Ubu. RNN: The recommendation of the Ubu. RNN model for new tasks lacks clarity. It is unclear whether its effectiveness stems from task generality or the size of the pretraining dataset.
Questions to Authors
1. Could you clarify why the new dataset mentioned in the abstract is not included in the paper? Are there plans to release it in the future?
2. What are the specific reasons for the variability in performance across tasks? Are there task-specific characteristics that influence the model's success or failure?
3. Could you provide examples of challenging instances from the datasets and discuss how the proposed models handle them?
4. Regarding the Ubu. RNN model, could you elaborate on whether its performance gains are due to the generality of the task or the size of the pretraining dataset?
Recommendation
While the paper has several shortcomings, including incomplete results, insufficient analysis, and failure to deliver the promised dataset, its core idea and contributions are significant. The unified framework and transfer learning results are exciting and worthy of further exploration. I recommend accepting the paper for publication, provided the authors address the identified weaknesses and polish the submission.