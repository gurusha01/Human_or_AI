Review
Summary and Contributions
This paper presents a transition-based parser that jointly produces syntactic and semantic dependencies using stack LSTMs, combining the transition system of Henderson et al. (2008) with the neural architecture of Dyer et al. (2015). The proposed model avoids the need for expert-crafted features, instead learning representations of the entire parser state. The authors demonstrate the effectiveness of their approach on the CoNLL 2008 and 2009 shared tasks, achieving competitive results with state-of-the-art systems while maintaining linear time complexity. The paper also introduces several innovations, such as a new semantic transition (M-SELF) to handle self-dependencies in semantic graphs and a mechanism for predicate sense disambiguation. The authors provide an open-source implementation, emphasizing the practical utility of their system.
The primary contributions of the paper are:
1. Integration of Henderson et al.'s transition system with Dyer et al.'s stack LSTM architecture: This combination enables efficient joint syntactic and semantic parsing, filling a gap in parser design by avoiding feature engineering while maintaining competitive performance.
2. Empirical evaluation on CoNLL 2008 and 2009 tasks: The model achieves state-of-the-art results among joint parsers and demonstrates robustness across multiple languages.
3. Practical utility: The system is computationally efficient, with a runtime significantly faster than many pipeline-based approaches, making it suitable for real-world applications.
Strengths
1. Clear and Well-Written Presentation: The paper is well-organized, with clear explanations of the model, training procedure, and experimental setup. The justification for hyperparameter choices and the inclusion of detailed discussions enhance its readability.
2. Empirical Rigor: The results on CoNLL 2008 and 2009 tasks are promising, with the model outperforming prior joint parsers and achieving comparable performance to pipeline-based systems. The multilingual evaluation further demonstrates the model's adaptability.
3. Efficiency: The linear runtime of the greedy parsing algorithm and its competitive performance make it a strong candidate for practical use, especially in resource-constrained settings.
4. Open-Source Implementation: The commitment to releasing the code increases the reproducibility and impact of the work.
Weaknesses
1. Limited Novelty: While the integration of Henderson et al.'s transition system and Dyer et al.'s stack LSTM is effective, the method lacks significant originality. The contributions primarily lie in combining existing techniques rather than introducing fundamentally new ideas.
2. Reliance on Pretrained Embeddings: The model's performance heavily depends on high-quality pretrained embeddings, which may limit its applicability in low-resource languages or domains without sufficient training data.
3. Overfitting in Joint Parsing: The joint model shows signs of overfitting to in-domain data (e.g., WSJ) compared to the hybrid approach, particularly in out-of-domain evaluations. This raises concerns about its generalizability.
Questions to Authors
1. How does the model perform when pretrained embeddings are unavailable or of lower quality? Have you considered alternative methods for handling low-resource languages?
2. Could the overfitting observed in the joint model be mitigated by incorporating additional regularization techniques or domain adaptation methods?
3. How does the model compare to recent neural approaches for SRL that leverage contextual embeddings (e.g., BERT)?
Recommendation
This paper makes a meaningful contribution by successfully integrating prior work to address joint syntactic and semantic parsing. While the method lacks significant novelty, its empirical results, efficiency, and practical utility justify its acceptance. I recommend acceptance with minor revisions to address concerns about generalizability and reliance on pretrained embeddings.