Review of the Paper
Summary and Contributions
This paper introduces a novel coreference resolution task tailored to Wikipedia articles, focusing on identifying coreference chains for the "main concept" (MC) of an article. The authors annotate a dataset of 30 Wikipedia documents and propose a binary classifier that significantly outperforms baselines derived from state-of-the-art coreference systems. The classifier leverages features extracted from Wikipedia markup and Freebase, achieving an F1 score of 89% for MC resolution. Additionally, the integration of the classifier into the Stanford deterministic coreference system (Dcoref) improves overall coreference resolution performance on Wikipedia texts by 4% in CoNLL F1.
The primary contributions of this work are:
1. Task Definition and Dataset: The introduction of the MC resolution task and the creation of the WCR dataset, which addresses the gap in adapting coreference resolution to Wikipedia.
2. Classifier Design: A robust binary classifier leveraging Wikipedia-specific features, which achieves significant performance gains over baselines.
3. Impact on Coreference Research: The task bridges coreference resolution and entity linking, providing a practical, domain-relevant setting that revitalizes coreference research.
Strengths
1. Novel Task and Dataset: The paper addresses a practical and underexplored problem in coreference resolution, with a clear focus on Wikipedia as a domain. The WCR dataset is a valuable resource for future research.
2. Performance Gains: The proposed classifier demonstrates substantial improvements over baselines, with a 13-point F1 increase for MC resolution and a 4-point CoNLL F1 improvement when integrated into Dcoref.
3. Practical Relevance: The focus on resolving mentions of the main concept aligns with real-world applications like information extraction, where identifying key entities is crucial.
4. Integration with Existing Systems: The seamless integration of the classifier into Dcoref highlights its practical utility and potential for broader adoption.
Weaknesses
1. Limited Dataset Size: The WCR dataset consists of only 30 documents, which raises concerns about the generalizability of the results. A larger dataset would provide stronger evidence of the approach's effectiveness.
2. Feature Engineering Over Generalization: While the classifier achieves strong results, its reliance on handcrafted features specific to Wikipedia and Freebase may limit its applicability to other domains or datasets.
3. Ambiguity in Methodology: Certain sections, such as the description of "candidate lists" and the integration of features from Freebase, lack clarity. This could hinder reproducibility.
4. Lack of Novel Techniques: The techniques employed, including feature engineering and SVM classifiers, are effective but not groundbreaking. The novelty lies primarily in the task and dataset rather than the methodology.
Questions to Authors
1. How does the classifier perform on other datasets or domains beyond Wikipedia? Could it generalize to other structured or semi-structured text genres?
2. Can you provide more details on the "candidate list" and how it is constructed during preprocessing? This section is unclear.
3. How were the redirects and aliases from Wikipedia and Freebase filtered to avoid introducing noise into the features?
Additional Comments
The paper is well-motivated and addresses a relevant gap in coreference resolution research. However, better organization and clearer terminology would enhance readability. Expanding the dataset and exploring more generalizable techniques could strengthen the impact of this work. Overall, the paper makes a meaningful contribution to the field and has potential for practical applications.