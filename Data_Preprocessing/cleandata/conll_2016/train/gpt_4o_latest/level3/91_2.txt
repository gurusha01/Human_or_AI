Review of the Paper
Summary and Contributions
This paper explores the application of magnitude-based weight pruning to Neural Machine Translation (NMT) models, focusing on three pruning schemes: class-blind, class-uniform, and class-distribution. The authors demonstrate that class-blind pruning, combined with retraining, is the most effective, enabling up to 90% weight pruning with minimal performance degradation. The study provides valuable insights into the distribution of redundancy in NMT architectures, highlighting the importance of higher layers, attention, and softmax weights. The primary contributions of this work are:
1. Effective Compression via Pruning and Retraining: The authors achieve significant model compression (up to 80% pruning) with no loss in performance, and even slight performance gains in some cases.
2. Analysis of Pruning Schemes: The paper rigorously compares three pruning schemes and establishes the superiority of class-blind pruning for NMT.
3. Insights into NMT Redundancy: The work uncovers patterns of redundancy in NMT architectures, such as the higher importance of certain weight classes and the role of pruning as a regularizer.
Strengths
1. Significant Compression with Minimal Performance Loss: The ability to prune up to 80% of weights without performance degradation is a strong result, particularly for memory-constrained applications like mobile NMT.
2. Comprehensive Analysis of Pruning Schemes: The comparison of three pruning schemes is thorough, with clear evidence supporting the superiority of class-blind pruning. The analysis of performance loss by weight class is particularly insightful.
3. Retraining as a Regularizer: The finding that retraining not only recovers but can improve performance is compelling and provides a practical benefit for model optimization.
4. Practical Relevance: The approach addresses a critical challenge in deploying NMT models on resource-constrained devices, making the work highly relevant to real-world applications.
5. Clarity of Experimental Setup: The experiments are well-documented, with clear descriptions of datasets, hyperparameters, and evaluation metrics.
Weaknesses
1. Lack of Memory Consumption Comparisons: While the paper focuses on weight pruning, it does not provide a detailed comparison of memory usage between compressed and uncompressed models. This omission limits the practical applicability of the results.
2. Limited Contextualization with Prior Work: The paper could benefit from a more thorough discussion of related work, particularly pruning in phrase-based Statistical Machine Translation (SMT), to better contextualize its contributions.
3. Unclear Visualization in Figure 5: Figure 5 is difficult to interpret, and the paper would benefit from an additional bar plot under Figure 4 to show discarded weight magnitudes by class, as suggested in the guidelines.
4. No Exploration of Runtime Efficiency: Although the paper achieves significant compression, it does not explore how pruning impacts runtime efficiency during inference, which is critical for deployment.
Questions to Authors
1. Can you provide a detailed comparison of memory consumption between compressed and uncompressed models, including any potential gains in runtime efficiency?
2. Have you considered iterative pruning and retraining, as suggested in prior work, to achieve even higher compression rates or performance improvements?
3. Could you elaborate on why class-blind pruning outperforms class-uniform and class-distribution pruning in terms of theoretical underpinnings?
Additional Comments
The paper presents a strong case for the use of magnitude-based pruning in NMT, with practical implications for model deployment in constrained environments. However, addressing the weaknesses identified above, particularly the lack of memory and runtime comparisons, would significantly enhance the impact of this work.