Review of the Paper
Summary and Contributions  
This paper introduces Positive-only Projection (PoP), a novel method for constructing semantic spaces and word embeddings using random projections. The key claimed contributions are:  
1. Efficiency and Scalability: The PoP method is computationally efficient, enabling faster model construction compared to neural networks, with models built in seconds or minutes.  
2. Preservation of Distribution Properties: Unlike traditional random projection methods, PoP preserves non-Gaussian distributions, allowing for the application of weighting techniques such as Positive Pointwise Mutual Information (PPMI).  
3. Competitive Performance: PoP-constructed models, when combined with PPMI, achieve performance comparable to neural embeddings in semantic similarity tasks (e.g., MEN relatedness test), without requiring computationally expensive training.  
Strengths  
1. Efficiency: The PoP method significantly reduces computational overhead compared to neural network-based embeddings, making it a practical choice for resource-constrained settings.  
2. PPMI Compatibility: The ability to apply PPMI to PoP-constructed spaces is a notable advantage over traditional random projection methods, which cannot leverage such transformations.  
3. Empirical Validation: The paper demonstrates competitive results on the MEN relatedness test, showing that PoP+PPMI+Pearson achieves a Spearman correlation of 0.75, comparable to state-of-the-art methods.  
Weaknesses  
1. Lack of Theoretical Justification: The mathematical foundations of the PoP method are underdeveloped. The absence of a theoretical analysis for the error bounds (δ) and variance (σ²δ) limits the rigor of the proposed method.  
2. Unclear Importance of PPMI Transformability: While the paper emphasizes the compatibility of PoP with PPMI, it does not clearly articulate why this compatibility is critical or how it compares to other weighting techniques.  
3. Missing Comparisons to Related Work: The paper lacks direct comparisons to other dimensionality reduction methods or neural embedding techniques, making it difficult to contextualize the contributions.  
4. Inconsistent Use of Evaluation Metrics: The unconventional use of Kendall's τb for similarity measurement is not well-justified, and the standard Spearman correlation is not consistently applied across experiments.  
5. Non-Standard Datasets: The evaluation primarily relies on the MEN dataset, which, while useful, is insufficient to validate the generalizability of the method. Inclusion of standard benchmarks like WordSim-353 or SimLex-999 is recommended.  
6. Presentation Issues: Figure 1 is incomplete (missing the third line mentioned), and there is a typo ("large extend" should be "large extent").  
Questions to Authors  
1. Can you provide a clearer explanation of why PPMI transformability is a critical feature of PoP, especially in comparison to other weighting techniques?  
2. Why was Kendall's τb chosen as the primary similarity measure, and how does it compare to Spearman correlation in terms of performance?  
3. Could you include a comparison of PoP with other dimensionality reduction methods (e.g., SVD, PCA) and neural embeddings in future revisions?  
Conclusion  
While the paper presents an interesting and efficient method for constructing semantic spaces, the lack of theoretical rigor, missing comparisons to related work, and inconsistent evaluation metrics limit its overall impact. Addressing these weaknesses could significantly strengthen the contribution. I recommend a major revision before acceptance.