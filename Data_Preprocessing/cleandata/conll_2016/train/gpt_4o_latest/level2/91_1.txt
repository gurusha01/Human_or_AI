Review of the Paper
Summary and Contributions
This paper investigates the use of magnitude-based weight pruning as a compression technique for Neural Machine Translation (NMT) models, specifically focusing on three pruning schemes: class-blind, class-uniform, and class-distribution. The authors demonstrate that class-blind pruning, combined with retraining, can reduce the size of a state-of-the-art NMT model by up to 80% without any performance loss, as measured by BLEU scores on the WMT'14 English-German task. The paper also provides insights into the distribution of redundancy in NMT architectures, highlighting the relative importance of different weight classes and layers. The authors argue that pruning not only compresses the model but also acts as a regularizer, potentially improving performance. The main contributions of this paper are:
1. A systematic comparison of three magnitude-based pruning schemes, with evidence that class-blind pruning outperforms the others.
2. A demonstration that retraining after pruning can recover and even improve baseline performance, achieving 80% compression with no BLEU score degradation.
3. Insights into the redundancy structure of NMT models, including the importance of higher layers, attention, and softmax weights.
Strengths
1. Practical Impact: The paper addresses a critical challenge in deploying NMT models on resource-constrained devices by significantly reducing model size while maintaining performance. This has clear practical utility for real-world applications.
2. Comprehensive Evaluation: The authors provide a thorough experimental comparison of pruning schemes and retraining strategies, supported by quantitative results (e.g., BLEU scores, perplexity changes) and qualitative insights (e.g., weight importance visualization).
3. Novelty in Context: While pruning has been extensively studied for CNNs, its application to NMT models, particularly with LSTM architectures, is novel and fills a gap in the literature.
4. Clarity of Results: The paper is well-structured, with clear visualizations (e.g., weight importance heatmaps, loss curves) that effectively support the claims.
Weaknesses
1. Limited Scope of Pruning Techniques: The paper focuses solely on magnitude-based pruning, despite acknowledging that other techniques like Optimal Brain Damage (OBD) and Optimal Brain Surgery (OBS) may be more principled. A comparative study would have strengthened the paper's contributions.
2. Sparse Matrix Utilization: While the paper achieves impressive compression, it does not explore how the sparsity of pruned models could be exploited to improve training or inference efficiency. This limits the practical utility of the proposed approach.
3. Generality Across Tasks: The experiments are limited to a single NMT task (English-German translation). It is unclear whether the findings generalize to other language pairs or architectures, such as Transformer-based models.
4. Insufficient Discussion of Limitations: The paper does not adequately discuss potential drawbacks of aggressive pruning, such as the risk of catastrophic forgetting during retraining or the impact on rare word translations.
Questions to Authors
1. How does the proposed pruning approach compare to more sophisticated techniques like OBD or OBS in terms of performance and computational cost?
2. Can the sparsity introduced by pruning be leveraged to reduce training or inference time, and if so, how?
3. Have you tested the generalizability of your findings on other NMT tasks or architectures, such as Transformer models?
Recommendation
Overall, this paper presents a significant and practically relevant contribution to the field of NMT model compression. While there are some limitations in the scope of techniques explored and the generalizability of results, the proposed approach is simple, effective, and well-supported by experimental evidence. I recommend acceptance, provided the authors address the questions and limitations raised.