Review of the Paper
Summary and Contributions
This paper addresses the task of identifying all mentions of the "main concept" (MC) in a Wikipedia article, a seldom-studied problem in coreference resolution (CR). The authors propose a binary classification approach that leverages Wikipedia markup and external knowledge bases like Freebase to extract features for mention classification. The main contributions of the paper are as follows:
1. Novel Task Definition and Dataset: The paper revisits the task of identifying mentions of the MC in Wikipedia articles and introduces a dedicated dataset (WCR) annotated with OntoNotes guidelines. This dataset highlights the limitations of existing CR systems when applied to Wikipedia texts.
   
2. Classifier Design: The authors develop a classifier that outperforms state-of-the-art CR systems and baselines on the task of identifying MC mentions. The classifier uses features derived from Wikipedia markup, Freebase attributes, and linguistic cues.
3. Integration into a Full CR Pipeline: The authors demonstrate that incorporating their classifier into the Stanford deterministic rule-based CR system (Dcoref) improves performance on a full CR task for Wikipedia articles.
Strengths
1. Clear Motivation and Novelty: The paper addresses a gap in the CR literature by focusing on Wikipedia, a resource widely used in NLP but underexplored for CR. The task of identifying MC mentions is well-motivated, and the proposed approach is novel in leveraging Wikipedia-specific features.
2. Strong Empirical Results: The classifier achieves significant improvements over baselines, with a 13-point F1 gain for MC mention identification and a 4-point CoNLL F1 gain when integrated into Dcoref. The results are robust across pronominal and non-pronominal mentions.
3. Feature Engineering: The paper provides a thorough exploration of features, including linguistic, positional, and semantic attributes derived from Wikipedia and Freebase. The feature ablation study is insightful and demonstrates the utility of each feature family.
4. Reproducibility and Resource Sharing: The authors commit to sharing their dataset (WCR) and classifier outputs, which will facilitate future research on this task.
Weaknesses
1. Limited Dataset Size: While the WCR dataset is diverse, its small size (30 articles, ~60k tokens) raises concerns about generalizability. The authors acknowledge this limitation but do not explore strategies for scaling the dataset.
2. Error Analysis: Although the paper provides examples of classification errors, the analysis is limited. A deeper discussion of failure cases (e.g., ambiguous mentions, noisy redirects) could help identify areas for improvement.
3. Baseline Comparisons: The baselines are derived from existing CR systems but are not optimized for the MC task. While this highlights the novelty of the proposed approach, additional baselines (e.g., rule-based systems tailored to Wikipedia) could provide a more comprehensive evaluation.
4. Integration with Other CR Systems: The integration of the classifier into Dcoref is promising, but it would be valuable to test its impact on other CR systems (e.g., Scoref) to assess general applicability.
Questions to Authors
1. How does the classifier perform on other text genres (e.g., newswire, blogs)? Could the approach generalize beyond Wikipedia?
2. Have you considered semi-supervised or unsupervised methods to expand the WCR dataset?
3. Could noisy redirects in Wikipedia be filtered or weighted to reduce their negative impact on classification?
Conclusion
This paper makes a significant contribution to the field of CR by addressing a novel and practically useful task. The proposed classifier demonstrates strong performance, and its integration into a full CR pipeline is a valuable extension. While the dataset size and baseline comparisons could be improved, the work lays a solid foundation for future research. I recommend acceptance, with minor revisions to address the identified weaknesses.