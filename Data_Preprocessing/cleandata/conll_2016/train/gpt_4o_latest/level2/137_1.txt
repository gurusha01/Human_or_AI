Review
Summary and Contributions
This paper investigates compositionality detection for compound nouns using a novel integrated compositional distributional approach based on Anchored Packed Trees (APTs). The authors compare this method with state-of-the-art neural word embeddings and traditional compositional methods, demonstrating that APTs, which encode syntactic structure in contextual features, outperform neural embeddings and untyped co-occurrence-based methods. The key contributions of the paper are:  
1. Introduction of APT-based Composition: The paper proposes a method that aligns vector spaces using dependency paths before composition, addressing challenges in compositionality detection for compound nouns.  
2. Hybrid Composition Model: The authors introduce a hybrid approach combining aligned and unaligned APTs, achieving state-of-the-art performance on the Reddy et al. (2011) dataset.  
3. Empirical Validation: The paper provides extensive experiments comparing APTs with neural embeddings, demonstrating the importance of syntactic structure in compositionality detection.  
Strengths
1. Novelty and Innovation: The use of APTs to encode syntactic structure for compositionality detection is a significant advancement over traditional and neural methods, addressing a critical gap in compositional distributional semantics.  
2. Comprehensive Evaluation: The authors conduct rigorous experiments on the Reddy et al. (2011) dataset, comparing multiple models and parameter settings. The hybrid APT model achieves a Spearman's rank correlation of 0.79, surpassing previous state-of-the-art methods.  
3. Theoretical Rigor: The paper provides a detailed explanation of the APT framework, including feature alignment and reduction, making the methodology transparent and reproducible.  
4. Practical Implications: The results highlight the importance of incorporating syntactic structure into compositional methods, which could influence future work in natural language understanding and representation learning.  
Weaknesses
1. Dataset Limitations: The experiments are limited to the Reddy et al. (2011) dataset, which contains only 90 compound nouns. While the results are promising, the generalizability of the approach to other datasets and phrase types remains unclear.  
2. Sparse Representations: The paper acknowledges that sparse elementary representations may limit the performance of intersective composition methods. While smoothing techniques are briefly mentioned, their integration into the proposed approach is not explored in detail.  
3. Comparison with Neural Models: While the paper compares APTs with neural embeddings, the parameter optimization for neural models appears limited. For example, the authors use standard configurations for word2vec but do not explore more recent embedding techniques such as contextualized embeddings (e.g., BERT).  
4. Computational Complexity: The APT approach involves constructing and aligning high-dimensional dependency-based representations, which may be computationally expensive compared to neural embeddings. This aspect is not discussed in the paper.  
Questions to Authors
1. How does the APT approach perform on datasets with larger and more diverse sets of compound nouns or other phrase types (e.g., verb-noun phrases)?  
2. Could the hybrid APT model be extended to incorporate contextualized embeddings, such as those from transformer-based models?  
3. What are the computational trade-offs of using APTs compared to neural embeddings, particularly for large-scale datasets?  
Overall Assessment
This paper presents a novel and well-executed approach to compositionality detection, with clear improvements over existing methods. While the dataset and scope of evaluation are somewhat narrow, the results are compelling and suggest significant potential for future research. Addressing the limitations and expanding the evaluation to broader datasets and tasks could further strengthen the impact of this work.  
Recommendation: Accept with minor revisions.