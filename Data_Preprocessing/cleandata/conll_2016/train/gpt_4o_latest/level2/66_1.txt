Review
Summary and Contributions
This paper introduces a transition-based parser that jointly predicts syntactic and semantic dependencies using stack LSTMs to represent the entire algorithmic state. The key contributions of the paper are:  
1. Joint Syntactic-Semantic Parsing with Stack LSTMs: The parser learns representations for the entire state of the algorithm, eliminating the need for expert-crafted features. This approach is applied to a greedy inference algorithm that operates in linear time.  
2. Empirical Performance: The model achieves state-of-the-art performance among joint parsers on the CoNLL 2008 and 2009 English shared tasks and demonstrates competitive results across multiple languages.  
3. Efficiency: The proposed system is computationally efficient, with an end-to-end runtime significantly faster than comparable systems.  
Strengths
1. Novel Representation Learning: The use of stack LSTMs to encode the entire parser state is a significant innovation. This approach obviates the need for handcrafted features, which is a notable step forward in parser design.  
2. Strong Empirical Results: The parser achieves competitive performance on both syntactic and semantic tasks, outperforming previous joint models and rivaling pipeline-based systems that rely on carefully designed features.  
3. Efficiency: The system's runtime efficiency (177.6 seconds for the CoNLL 2009 English test set) makes it practical for real-world applications, especially compared to slower pipeline-based systems.  
4. Comprehensive Evaluation: The paper evaluates the model on multiple datasets (CoNLL 2008, CoNLL 2009 English, and multilingual datasets) and provides detailed comparisons with both joint and pipeline-based systems.  
Weaknesses
1. Limited Novelty in Transition-Based Parsing: While the use of stack LSTMs is innovative, the transition-based parsing framework largely builds on prior work (e.g., Henderson et al., 2008). The novelty lies more in the representation learning than in the parsing algorithm itself.  
2. Overfitting in Joint Parsing: The joint model shows signs of overfitting to the in-domain (WSJ) data, as evidenced by its weaker performance on out-of-domain (Brown corpus) data compared to the hybrid model. This suggests that the joint approach may not generalize as well as pipeline-based alternatives in some scenarios.  
3. Dependence on Pretrained Embeddings: The model's strong performance in some languages (e.g., Chinese) relies heavily on pretrained embeddings. The paper does not explore scenarios where such embeddings are unavailable or less effective.  
4. Lack of Morphological Features: The absence of morphological features limits the parser's performance on morphologically rich languages like Czech. While the authors suggest future work on character-based embeddings, this omission weakens the current system's applicability to such languages.  
Questions to Authors
1. How does the model perform when pretrained embeddings are unavailable or of lower quality, particularly for low-resource languages?  
2. Could you elaborate on the scalability of the parser to longer sentences or larger datasets? Does the linear runtime hold in practice for such cases?  
3. Have you considered incorporating morphological features or character-based embeddings to improve performance on morphologically rich languages?  
Additional Comments
The paper is well-written and provides a thorough analysis of the proposed method. The open-source implementation is a valuable contribution to the community. However, addressing the identified weaknesses, particularly overfitting and reliance on pretrained embeddings, would further strengthen the work. Encouragingly, the parser's efficiency and performance make it a promising candidate for practical applications.  
Overall, this paper presents a meaningful contribution to joint syntactic and semantic parsing, with strong empirical results and practical utility. While there are areas for improvement, the work is a significant step forward in the field.