The paper presents a positive-only projection (PoP) word embedding method. This
is a random projection method with a random projection matrix whose expected
value is positive. The authors argue that this enables the application of PPMI
which is not possible with an expected value of 0 and that being a random
projection method, their computation is efficient.
My main reservation about this paper has to do with its clarity. Particularly:
1. I could not understand the core difference between the method proposed in
the paper and previous random projection methods. Hence, I could not understand
how (and whether) the advantages the authors argue to achieve hold.
2. It was hard to follow the arguments of the paper starting from the
introduction. 
3. Some of the arguments of the paper are not supported: 
- Line 114: Sentence starts with "in addition"
- Line 137: Sentence starts with "Since"
- Line 154: Sentence starts with "thus"
4. While I have worked on vector space modeling (who hasn't ?), I am not an
expert to random projections and have not used them in my research. It was hard
for me to understand the logic behind this research avenue from the paper. I
believe that a paper should be self contained and possible to follow by people
with some experience in the field.
5. The paper has lots of English mistakes (86: "To a large extend", 142: "such
PPMI").
In addition, I cannot see why the paper is evaluating only on MEN. There are a
couple of standard benchmarks (MEN, WordSeim, SimLex and a couple of others) -
if you present a new method, I feel that it is insufficient to evaluate only on
one dataset unless you provide a good justification.
I recommend that the authors will substantially improve the presentation in the
paper and will resubmit to another conference.