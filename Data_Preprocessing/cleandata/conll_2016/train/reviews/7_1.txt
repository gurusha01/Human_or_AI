I am buying some of the motivation: the proposed method is much faster to train
than it is to train a neural network. Also, it keeps some properties of the
distribution when going to lower dimensionality. 
However, I am not convinced why it is so important for vectors to be
transformable with PPMI.
Most importantly, there is no direct comparison to related work.
Detailed comments:
- p.3: The definition of Kendall's tau that the authors use is strange. This is
NOT the original formula; I am not sure what it is and where it comes from.
- p.3: Why not use Spearman correlation as is standard in semantic tasks (and
as teh authors do at evaluation time)?
- The datasets chosen for evaluation are not the standard ones for measuring
semantic relatedness that the NLP community prefers. It is nice to try other
sets, but I would recommend to also include results on the standard ones.
- I can only see two lines on Figure 1. Where is the third line?
- There is no direct comparison to related work, just a statement that 
Some typos:
- large extend -- extent