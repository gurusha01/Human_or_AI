This paper is about introducing eye-tracking features for sentiment analysis as
a type of cognitive feature.  I think that the idea of introducing eye-tracking
features as a proxy for cognitive load for sentiment analysis is an interesting
one.  
I think the discussion on the features and comparison of feature sets is clear
and very helpful.  I also like that the feasibility of the approach is
addressed in section 7.
I wonder if it would help the evaluation if the datasets didn't conflate
different domains, e.g., the movie review corpus and the tweet corpus.             
For one
it might improve the prediction of movie review (resp. tweets) if the tweets
(resp. movie reviews) weren't in the training.              It would also make the
results
easier to interpret.  The results in Table 2 would seem rather low compared to
state-of-the art results for the Pang and Lee data, but look much better if
compared to results for Twitter data.
In Section 3.3, there are no overlapping snippets in the training data and
testing data of datasets 1 and 2, right?  Even if they come from the same
sources (e.g., Pang & Lee and Sentiment 140).
Minor: some of the extra use of bold is distracting (or maybe it's just me);