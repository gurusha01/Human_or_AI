A combination of word2vec and LDA could be potentially interesting. The main
problem with the current paper is that the technical details are
incomprehensible. Section 2 needs a complete rewrite so that a reader familiar
with word2vec and LDA could relatively easily get a high-level picture of how
the models are being combined. The current presentation doesn't achieve that.
More detailed comments:
The third paragraph of the introduction makes no sense to me. "requires
deriving a new approximation" - approximation of what? why is it time consuming
to develop prototypes? Why is it easier to evaluate features?
Why use the same word vectors for pivot and target (unlike in word2vec)? What's
the motivation for that decision?
what does it mean to separate words from a marginal distribution?
what's co-adaptation?
"If we only included structure up to this point" - what kind of structure?
"it's similarity" -> its
Footnote 1 breaks anonymity.
There doesn't appear to be any evaluation. The days when it was ok to just give
some example clusters are long gone in NLP. Figure 2 looks like it might be a
quantitative evaluation, but it's only described in the overly long caption.
The statement in the conclusion that the model solves word analogies is
overstating what was shown, which was just a few cherry-picked examples of king
+ queen etc. sort.
The Chang ref has the conference/journal name as "Advances in ..." You'd like
me to guess the venue?