This paper applies the idea of translation model pruning to neural MT. The
authors explore three simple threshold and histogram pruning schemes, two of
which are applied separately to each weight class, while the third is applied
to the entire model. The authors also show that retraining the models produces
performance equal to the full model, even when 90% of the weights are pruned.
An extensive analysis explains the superiority of the class-blind pruning
scheme, as well as the performance boost through retraining. 
While the main idea of the paper is simple, it seems quite useful for
memory-restricted applications of NMT. I particularly liked the analysis
section which gives further insight into the model components that are usually
treated like black boxes. While these insights are interesting by themselves,
the paper's main motivation is model compression. This argument would be
stronger if the paper included some numbers on actual memory consumption of the
compressed model in comparison to the uncompressed model.     
Some minor remarks:
- There is a substantial amount of work on pruning translation models in
phrase-based SMT, which could be referenced in related work, e.g. 
Johnson, J., Martin, J., Foster, G. and Kuhn, R.: Improving Translation Quality
by Discarding Most of the Phrasetable. EMNLP 07 or
Zens, R., Stanton, D. and Peng X.: A Systematic Comparison of Phrase Table
Pruning Techniques. EMNLP 12
- It took me a while to understand Figure 5. I would find it more informative
to add an additional barplot under figure 4 showing highest discarded weight
magnitude by class. This would also allow a comparison across all pruning
methods.