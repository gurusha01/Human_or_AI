This paper describes four methods of obtaining multilingual word embeddings and
a modified QVEC metric for evaluating the efficacy of these embeddings. The
embedding methods are: 
(1) multiCluster : Uses a dictionary to map words to multilingual clusters.
Cluster embeddings are then obtained which serve as embeddings for the words
that reside in each cluster. 
(2) multiCCA : Extends the approach presented by Faruqui and Dyer (2014) for
embedding bilingual words, to multilingual words by using English embeddings as
the anchor space. Bilingual dictionaries (other_language -> English) are then
used to obtain projections from other monolingual embeddings for words in other
languages to the anchor space. 
(3) multiSkip : Extends the approach presented by Luong et al. (2015b) for
embedding using source and target context (via alignment), to the multilingual
case by extending the objective function to include components for all
available parallel corpora. 
(4) Translation invariance : Uses a low rank decomposition of the word PMI
matrix with an objective with includes bilingual alignment frequency
components. May only work for  bilingual embeddings. 
The evaluation method uses CCA to maximize the correlation between the word
embeddings and possibly hand crafted linguistic data. Basis vectors are
obtained for the aligned dimensions which produce a score which is invariant to
rotation and linear transformations. The proposed method also extends this to
multilingual evaluations. 
In general, the paper is well written and describes the work clearly. A few
major issues:
(1) What is the new contribution with respect to the translation invariance
embedding approach of Gardner et al.? If it is the extension to multilingual
embeddings, a few lines explaining the novelty would help. 
(2) The use of super-sense annotations across multiple languages is a problem.
The number of features in the intersection of multiple languages may become
really small. How do the authors propose to address this problem (beyond
footnote 9)?
(3) How much does coverage affect the score in table 2? For example, for
dependency parsing, multi cluster and multiCCA have significantly different
coverage numbers with scores that are close. 
(4) In general, the results in table 3 do not tell a consistent story. Mainly,
for most of the intrinsic metrics, the multilingual embedding techniques do not
seem to perform the best.  Given that one of the primary goals of this paper
was to create embeddings that perform well under the word translation metric
(intra-language), it is disappointing that the method that performs best (by
far) is the invariance approach. It is also strange that the multi-cluster
approach, which discards inter-cluster (word and language) semantic information
performs the best with respect to the extrinsic metrics.
Other questions for the authors:
(1) What is the loss in performance by fixing the word embeddings in the
dependency parsing task? What was the gain by simply using these embeddings as
alternatives to the random embeddings in the LSTM stack parser? 
(2) Is table 1 an average over the 17 embeddings described in section 5.1? 
(3) Are there any advantages of using the multi-Skip approach instead of
learning bilingual embeddings and performing multi-CCA to learning projections
across the distinct spaces?
(4) The dictionary extraction approach (from parallel corpora via alignments or
from google translate) may not reflect the challenges of using real lexicons.
Did you explore the use of any real multi-lingual dictionaries?