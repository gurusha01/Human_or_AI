This paper performs an overdue circling-back to the problem of joint semantic
and syntactic dependency parsing, applying the recent insights from neural
network models. Joint models are one of the most promising things about the
success of transition-based neural network parsers.
There are two contributions here. First, the authors present a new transition
system, that seems better than the Hendersen (2008) system it is based on. The
other contribution is to show that the neural network succeeds on this problem,
where linear models had previously struggled. The authors attribute this
success to the ability of the neural network to automatically learn which
features to extract. However, I think there's another advantage to the neural
network here, that might be worth mentioning. In a linear model, you need to
learn a weight for each feature/class pair. This means that if you jointly
learn two problems, you have to learn many more parameters. The neural network
is much more economical in this respect.
I suspect the transition-system would work just as well with a variety of other
neural network models, e.g. the global beam-search model of Andor (2016). There
are many other orthogonal improvements that could be made. I expect extensions
to the authors' method to produce state-of-the-art results.
It would be nice to see an attempt to derive a dynamic
oracle for this transition system, even if it's only in an appendix or in
follow-up work. At first glance, it seems similar to the
arc-eager oracle. The M-S action excludes all semantic arcs between the word at
the start of the buffer and the words on the semantic stack, and the M-D action
excludes all semantic arcs between the word at the top of the stack and the
words in the buffer. The L and R actions seem to each exclude the reverse arc,
and no other.