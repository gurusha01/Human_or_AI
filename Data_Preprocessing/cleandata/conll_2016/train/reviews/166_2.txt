This paper is concerned with cross-lingual direct transfer of NER models using
a very recent cross-lingual wikification model. In general, the key idea is not
highly innovative and creative, as it does not really propose any core new
technology. The contribution is mostly incremental, and marries the two
research paths: (1) direct transfer for downstream NLP tasks (such as NER,
parsing, or POS tagging), and (2) very recent developments in the cross-lingual
wikification technology. However, I pretty much liked the paper, as it is built
on a coherent and clear story with enough experiments and empirical evidence to
support its claims, with convincing results. I still have several comments
concerning the presentation of the work.
Related work: a more detailed description in related work on how this paper
relates to work of Kazama and Torisawa (2007) is needed. It is also required to
state a clear difference with other related NER system that in one way or
another relied on the encyclopaedic Wikipedia knowledge. The differences are
indeed given in the text, but they have to be further stressed to facilitate
reading and placing the work in context. 
Although the authors argue why they decided to leave out POS tags as features,
it would still be interesting to report experiments with POS tags features
similar to Tackstrom et al.: the reader might get an overview supported by
empirical evidence regarding the usefulness (or its lack) of such features for
different languages (i.e., for the languages for which universal POS are
available at least). 
Section 3.3 could contribute from a running example, as I am still not exactly
sure how the edited model from Tsai and Roth works now (i.e., the given
description is not entirely clear).
Since the authors mention several times that the approaches from Tackstrom et
al. (2012) and Nothman et al. (2012) are orthogonal to theirs and that they can
be combined with the proposed approach, it would be beneficial if they simply
reported some preliminary results on a selection of languages using the
combination of the models. It will add more flavour to the discussion. Along
the same line, although I do acknowledge that this is also orthogonal approach,
why not comparing with a strong projection baseline, again to put the results
into more even more context, and show the usefulness (or limitations) of
wikification-based approaches.
Why is Dutch the best training language for Spanish, and Spanish the best
language for Yoruba? Only a statistical coincidence or something more
interesting is going on there? A paragraph or two discussing these results in
more depth would be quite interesting.
Although the idea is sound, the results from Table 4 are not that convincing
with only small improvements detected (and not in all scenarios). A statistical
significance test reported for the results from Table 4 could help support the
claims.
Minor comments:
- Sect. 2.1: Projection can also be performed via methods that do not require
parallel data, which makes such models more widely applicable (even for
languages that do not have any parallel resources): e.g., see the work of
Peirsman and Pado (NAACL 2009) or Vulic and Moens (EMNLP 2013) which exploit
bilingual semantic spaces instead of direct alignment links to perform the
transfer.
- Several typos detected in the text, so the paper should gain quite a bit from
a more careful proofreading (e.g., first sentence of Section 3: "as a the base
model"; This sentence is not 'parsable', Page 3: "They avoid the traditional
pipeline of NER then EL by...", "to disambiguate every n-grams" on Page 8)