This paper proposes a new method for the evaluation of topic models that
partitions the top n words of each topic into clusters or "buckets" based on
cosine similarity of their associated word embeddings. In the simplest setup,
the words are considered one by one, and each is either put into an existing
"bucket" – if its cosine similarity to the other words in the bucket is below
a certain threshold – or a new bucket is created for the word. Two more
complicated methods based on eigenvectors and reorganisation are also
suggested. The method is evaluated on three standard data sets and in a  weakly
supervised text classification setting. It outperforms or is en par with the
state of the art (Röder et al., 2015).
The basic idea behind the paper is rather simple and has a certain ad
hoc-flavour. The authors do not offer any new explanations for why topic
quality should be measurable in terms of word–word similarity. It is not
obvious to me why this should be so, given that topics and word embeddings are
defined with respect to two rather different notions of context (document vs.
sequential context). At the same time, the proposed method seems to work quite
well. (I would like to see some significance tests for Table 1 though.)
Overall the paper is clearly written, even though there are some language
issues. Also, I found the description of the techniques in Section 3 a bit hard
to follow; I believe that this is mostly due to the authors using passive voice
("the threshold is computed as") in places were they were actually making a
design choice. I find that the authors should try to explain the different
methods more clearly, with one subsection per method. There seems to be some
space for that: The authors did not completely fill the 8 pages of content, and
they could easily downsize the rather uninformative "trace" of the method on
page 3.
One question that I had was how sensitive the proposed technique was to
different word embeddings. For example, how would the scores be if the authors
had used word2vec instead of GloVe?