The aim of this paper is to show that distributional information stored in word
vector models contain information about POS labels. They use a version of the
BNC annotated with UD POS and in which words have been replaced by lemmas. They
train word embeddings on this corpus, then use the resulting vectors to train a
logistic classifier to predict the word POS. Evaluations are performed on the
same corpus (using cross-validation) as well as on other corpora. Results are
clearly presented and discussed and analyzed at length.
The paper is clear and well-written. The main issue with this paper is that it
does not contain anything new in terms of NLP or ML. It describe a set of
straightforward experiments without any new NLP or ML ideas or methods. Results
are interesting indeed, in so far that they provide an empirical grounding to
the notion of POS. In that regard, it is certainly worth being published in a
(quantitative/emprirical) linguistic venue.
On another note, the literature on POS tagging and POS induction using word
embeddings should be cited more extensively (cf. for instance Lin, Ammar, Duer
and Levin 2015; Ling et al. 2015 [EMNLP]; Plank, SÃ¸gaard and Goldberg
2016...).