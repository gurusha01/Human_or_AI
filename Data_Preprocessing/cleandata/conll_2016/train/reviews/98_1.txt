This paper presents results on the UD treebanks to test delexicalized transfer
parsers and an unsupervised parser which is enriched with external
probabilities.
The paper is interesting, but I think it could be improved further.
(5.2) "McDonald et al. (2011) presented 61.7% of averaged accuracy over 8
languages. On the same languages, our transfer parser on UD reached 70.1%."
Mcdonald et al could not use the UD treebanks since they were not available,
you should definitely state that this is the case here.
In footnote 9 you say: "We used the Malt parser with its default feature set.
Tuning in this specific delexicalized task would probably bring a
bit better results." You are using MaltParser with default settings, why don't
you use MaltOptimizer? Optimizing one model would be very easy. 
In the same way MSTParser could be optimized further.
In the same line, why don't you use more recent parsers that produce better
results? These parsers have been already applied to universal dependencies with
the leave one out setup (see references below). For instance, the authors say
that  the unsupervised parser "performs better for languages from less
resourced language families (non-Indo-European)", it would be interesting to
see whether this holds with more recent (and cross lingual) parsers.
Probabilities: Why do you use this probabilities? it seems like a random
decision (Tables 3-4) (esp 3), at least we need more details or a set of
experiments to see whether they make sense or not.
There are some papers that the authors should take into account.
1. Cross-Lingual Dependency Parsing with Universal Dependencies and Predicted
PoS Labels
J Tiedemann
2. One model, two languages: training bilingual parsers with harmonized
treebanks
D Vilares, MA Alonso, C Gómez-Rodríguez  (it presents results with
MaltParser)
And for results with more recent parsers (and also delexicalized parsers):
1. Crosslingual dependency parsing based on distributed representations. 
Jiang Guo, Wanxiang Che, David
Yarowsky, Haifeng Wang, and Ting Liu. 2015.  In Proc. of ACL
2. Many languages, one parser
W Ammar, G Mulcaire, M Ballesteros, C Dyer, NA Smith
-Minor points:
 I don't think we need Table 1 and Table 2, this could be solved with a
footnote to the UD website. Perhaps Table 2 should be included due to the
probabilities, but Table 1 definitely not.