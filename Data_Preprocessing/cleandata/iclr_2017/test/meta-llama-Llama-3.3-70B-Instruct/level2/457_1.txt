This paper presents a novel method called Incremental Network Quantization (INQ) for efficiently converting any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version. The main claim of the paper is that INQ can achieve lossless low-precision quantization for any pre-trained full-precision CNN model with no assumption on its architecture. The authors support this claim by conducting extensive experiments on the ImageNet large-scale classification task using various deep CNN architectures, including AlexNet, VGG-16, GoogleNet, and ResNets.
The approach is well-motivated, and the authors provide a clear explanation of the limitations of existing network quantization methods. The paper is well-placed in the literature, and the authors provide a comprehensive review of related works. The experimental results demonstrate the efficacy of the proposed method, showing that the quantized CNN models with 5-bit, 4-bit, 3-bit, and even 2-bit ternary weights have improved or comparable accuracy against their full-precision baselines.
Based on the provided information, I decide to accept this paper. The two key reasons for this choice are:
1. The paper presents a novel and well-motivated approach to network quantization, which addresses the limitations of existing methods.
2. The experimental results demonstrate the efficacy of the proposed method, showing improved or comparable accuracy against full-precision baselines.
To further improve the paper, I provide the following feedback:
* The authors could provide more details on the computational and power efficiency of the proposed method, as well as its implementation on hardware platforms.
* The authors could also explore the application of the proposed method to other computer vision tasks, such as object detection and semantic segmentation.
* Additionally, the authors could provide more analysis on the distribution of the quantized weights, as shown in Appendix 1, to gain more insights into the behavior of the proposed method.
I would like the authors to answer the following questions to clarify my understanding of the paper:
* Can the authors provide more details on the weight partition strategy used in the experiments, and how it affects the performance of the proposed method?
* How does the proposed method handle the case where the pre-trained full-precision CNN model has a large number of parameters, and how does it affect the computational and power efficiency of the method?
* Can the authors provide more information on the future works mentioned in the paper, such as extending the incremental idea behind INQ to low-precision activations and low-precision gradients?