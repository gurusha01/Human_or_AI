This paper presents a batch policy gradient method for training chatbots in a reinforcement learning setting, where the rewards are noisy and expensive to obtain. The authors demonstrate the efficacy of their method through a series of synthetic experiments and an Amazon Mechanical Turk experiment on a restaurant recommendations dataset.
The main claims of the paper are that the proposed batch policy gradient method can efficiently use minimal labelled data to improve chatbots previously trained through maximum likelihood on unlabelled data, and that the method outperforms baselines on synthetic and real experiments.
I decide to accept this paper with the following key reasons: 
1. The paper tackles a specific and well-defined problem in the field of natural language processing, namely the training of chatbots in a reinforcement learning setting with noisy and expensive rewards.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the differences between the proposed method and existing approaches.
The supporting arguments for these reasons are as follows:
* The paper provides a clear and concise introduction to the problem of training chatbots in a reinforcement learning setting, and motivates the need for a batch policy gradient method.
* The authors provide a thorough review of the related work in the field, and clearly explain the differences between their approach and existing methods.
* The experimental results demonstrate the efficacy of the proposed method, and provide a clear comparison with baseline methods.
Additional feedback to improve the paper includes:
* Providing more details on the implementation of the GTD(λ) algorithm, and explaining why this algorithm was chosen for estimating the value function.
* Including more qualitative results from the Amazon Mechanical Turk experiment, to provide a better understanding of the types of improvements that the proposed method can achieve.
* Discussing potential limitations of the proposed method, and providing suggestions for future work to address these limitations.
Some questions that I would like the authors to answer to clarify my understanding of the paper include:
* Can you provide more details on how the behaviour policy was learned in the experiments, and how this policy was used to estimate the importance sampling coefficients?
* How did you choose the hyperparameters for the proposed method, such as the step size and the return coefficient λ?
* Can you provide more information on the types of errors that the proposed method was able to correct in the Amazon Mechanical Turk experiment, and how these errors were evaluated?