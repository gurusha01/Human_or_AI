This paper explores the use of eligibility traces in combination with recurrent networks in the Atari domain, with a focus on deep reinforcement learning. The authors investigate the benefits of both recurrent nets and eligibility traces in some Atari games and highlight the importance of optimization used in training. The main claim of the paper is that eligibility traces can improve and stabilize learning, and using Adam can strongly accelerate learning.
I decide to accept this paper, with the key reason being that the approach is well-motivated and the results are scientifically rigorous. The authors provide a clear and concise background on eligibility traces, deep Q-networks, and recurrent Q-networks, and their experimental setup is well-designed. The results show that eligibility traces can improve performance in certain Atari games, and the use of Adam as an optimizer can accelerate learning.
The supporting arguments for this decision include the fact that the paper provides a thorough analysis of the effects of using eligibility traces and different optimization functions. The authors also acknowledge the limitations of their approach, such as the potential issue with the frozen network, and suggest future experiments to address these limitations. Additionally, the paper is well-written and easy to follow, with clear explanations of the algorithms and experimental setup.
To improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process, as the choice of hyperparameters can significantly affect the results. Additionally, it would be interesting to see more experiments on different Atari games to further demonstrate the effectiveness of the approach. I would also like the authors to clarify how they chose the value of Î» = 0.8 for the eligibility traces, and whether this value was tuned or chosen based on prior knowledge.
Some questions I would like the authors to answer include: How did you choose the architecture of the recurrent Q-network, and whether you tried other architectures? How did you handle the issue of exploration-exploitation trade-off in the Atari games? Did you try other optimization algorithms besides Adam and RMSprop? 
Overall, the paper is well-written and provides a significant contribution to the field of deep reinforcement learning. With some minor revisions to address the suggested improvements, the paper is ready for publication.