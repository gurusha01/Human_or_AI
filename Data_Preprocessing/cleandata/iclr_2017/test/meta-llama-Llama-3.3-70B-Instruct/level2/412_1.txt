This paper presents a novel layerwise optimization algorithm for Piecewise-Linear Convolutional Neural Networks (PL-CNNs), which is a large class of convolutional neural networks. The key observation of the approach is that the parameter estimation of one layer of a PL-CNN can be formulated as a difference-of-convex (DC) program, which can be viewed as a latent structured SVM problem. The authors use the concave-convex procedure (CCCP) to solve the DC program, which requires iteratively solving a convex structured SVM problem. They extend the block-coordinate Frank-Wolfe (BCFW) algorithm to improve its initialization, memory requirements, and time complexity, making it suitable for training PL-CNNs.
The paper claims to provide several advantages over the state-of-the-art backpropagation algorithms, including a monotonic decrease in the learning objective at each iteration, no requirement for tuning the learning rate, and improved performance on several benchmark datasets. The authors demonstrate the effectiveness of their approach on the MNIST, CIFAR-10, CIFAR-100, and ImageNet datasets, showing that it consistently improves over the state-of-the-art variants of backpropagation.
Based on the provided information, I decide to Accept this paper. The main reasons for this decision are:
1. The paper tackles a specific question/problem, namely, the optimization of PL-CNNs, and provides a well-motivated approach.
2. The approach is well-placed in the literature, building upon existing work on DC programs, CCCP, and BCFW algorithms.
3. The paper provides a clear and detailed description of the algorithm, including the extensions to the BCFW algorithm, and demonstrates its effectiveness on several benchmark datasets.
To further improve the paper, I suggest the following:
* Provide more detailed analysis of the time and memory complexity of the proposed algorithm, especially in comparison to the state-of-the-art backpropagation algorithms.
* Discuss the potential limitations of the approach, such as the requirement for a good initialization of the parameters and the potential for getting stuck in local minima.
* Consider providing more experimental results, such as comparisons with other optimization algorithms or evaluations on different architectures.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* Can you provide more details on how the latent space H is constructed and how its size affects the computational complexity of the algorithm?
* How do you handle the case where the DC decomposition of the objective function is not unique, and how does this affect the convergence of the algorithm?
* Can you provide more insights into the relationship between the proposed approach and other optimization algorithms, such as second-order methods or natural gradient descent?