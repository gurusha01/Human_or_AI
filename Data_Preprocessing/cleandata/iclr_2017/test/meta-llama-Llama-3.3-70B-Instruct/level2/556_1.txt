Summary of the Paper's Claims and Contributions
The paper empirically investigates the geometry of loss functions for state-of-the-art neural networks using multiple stochastic optimization methods. The authors aim to answer two questions: (1) What types of changes to the optimization procedure result in different local minima? and (2) Do different optimization algorithms find qualitatively different types of local minima? The paper claims to provide evidence that different optimization algorithms find different local minima, and that the shape of the loss function around these local minima is characteristic of the optimization algorithm.
Decision and Key Reasons
I decide to accept this paper, with two key reasons: (1) The paper provides a thorough and well-motivated empirical analysis of the geometry of loss functions for deep neural networks, which is a crucial aspect of understanding the behavior of optimization algorithms in deep learning. (2) The paper's findings, such as the observation that different optimization algorithms find different local minima, and that the shape of the loss function around these local minima is characteristic of the optimization algorithm, are novel and provide valuable insights into the optimization process.
Supporting Arguments
The paper's experimental setup and analysis are well-designed and thorough, with a clear description of the methods used and the results obtained. The authors provide a detailed comparison of the performance of different optimization algorithms, including stochastic gradient descent (SGD), SGD with momentum, RMSprop, Adadelta, and ADAM, as well as a new gradient descent method based on the Runge-Kutta integrator. The paper also provides a comprehensive discussion of the related work, including the theoretical analysis of loss surfaces and the performance of optimization algorithms.
Additional Feedback and Suggestions
To further improve the paper, I suggest that the authors provide more discussion on the implications of their findings for the design of optimization algorithms and the training of deep neural networks. Additionally, it would be interesting to see more analysis on the effects of batch normalization and extreme initializations on the loss surface, as well as more experiments on different neural network architectures and datasets.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend their analysis to more complex neural network architectures and larger datasets? (2) Can the authors provide more insights into the relationship between the shape of the loss function and the generalization error of the neural network? (3) How do the authors think their findings can be used to improve the design of optimization algorithms and the training of deep neural networks?