This paper presents a novel actor-critic deep reinforcement learning agent with experience replay, called ACER, which achieves state-of-the-art performance on both discrete and continuous control tasks. The authors introduce several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.
The main claims of the paper are that ACER is a stable, sample-efficient, and high-performing algorithm that can match the performance of the best known methods on Atari and outperform popular techniques on several continuous control problems. The authors support these claims with extensive experimental results on 57 Atari games and 6 continuous control tasks, demonstrating the effectiveness of ACER in both discrete and continuous action spaces.
The approach is well-motivated, building on recent advances in reinforcement learning, including the use of experience replay, off-policy learning, and trust region optimization. The authors provide a clear and detailed explanation of the algorithm, including the mathematical derivations and implementation details.
The results are impressive, with ACER achieving state-of-the-art performance on many tasks and outperforming other methods, including DQN and A3C. The authors also provide an ablation analysis, which helps to understand the contribution of each component of the algorithm to its overall performance.
One potential limitation of the paper is that the authors do not provide a detailed comparison with other state-of-the-art methods, such as PPO or TD3. Additionally, the authors could have provided more insight into the hyperparameter tuning process and the sensitivity of the algorithm to different hyperparameters.
Overall, I would accept this paper, as it presents a significant contribution to the field of reinforcement learning, with a well-motivated and well-executed approach, and impressive experimental results.
Decision: Accept
Reasons:
1. The paper presents a novel and well-motivated approach to actor-critic deep reinforcement learning with experience replay.
2. The experimental results are impressive, with ACER achieving state-of-the-art performance on many tasks.
3. The authors provide a clear and detailed explanation of the algorithm, including the mathematical derivations and implementation details.
Additional feedback:
* The authors could have provided more insight into the hyperparameter tuning process and the sensitivity of the algorithm to different hyperparameters.
* A more detailed comparison with other state-of-the-art methods, such as PPO or TD3, would be helpful to understand the strengths and weaknesses of ACER.
* The authors could have discussed potential applications of ACER to real-world problems and the potential challenges and limitations of deploying the algorithm in practice.
Questions for the authors:
* Can you provide more details on the hyperparameter tuning process and the sensitivity of the algorithm to different hyperparameters?
* How does ACER compare to other state-of-the-art methods, such as PPO or TD3, in terms of performance and sample efficiency?
* What are the potential applications of ACER to real-world problems, and what are the potential challenges and limitations of deploying the algorithm in practice?