This paper proposes SYMSGD, a parallel stochastic gradient descent (SGD) algorithm that retains the sequential semantics of SGD in expectation. The main claim of the paper is that SYMSGD achieves a significant speedup over a heavily optimized sequential baseline on a shared-memory machine, while maintaining the same accuracy as sequential SGD.
I decide to accept this paper, with the main reason being that the approach is well-motivated and the results are impressive. The paper provides a clear and concise explanation of the problem, and the proposed solution is novel and effective. The evaluation section provides a thorough comparison with other parallel SGD algorithms, and the results show that SYMSGD outperforms them in terms of speedup and accuracy.
The supporting arguments for this decision are as follows. Firstly, the paper provides a clear and concise explanation of the problem, and the proposed solution is novel and effective. The use of probabilistically sound combiners to parallelize SGD is a key innovation, and the paper provides a thorough analysis of the trade-offs involved in using this approach. Secondly, the evaluation section provides a thorough comparison with other parallel SGD algorithms, and the results show that SYMSGD outperforms them in terms of speedup and accuracy. The paper also provides a detailed analysis of the factors that affect the performance of SYMSGD, such as the frequency of model combinations and the learning rate.
To improve the paper, I would suggest the following. Firstly, the paper could benefit from a more detailed analysis of the theoretical properties of SYMSGD, such as its convergence rate and stability. Secondly, the paper could provide more insight into the practical implications of using SYMSGD, such as its potential applications and limitations. Finally, the paper could benefit from a more detailed comparison with other parallel SGD algorithms, such as those using distributed computing frameworks.
Some questions I would like the authors to answer are: (1) How does SYMSGD handle non-linear dependencies between model parameters? (2) Can SYMSGD be extended to handle other types of machine learning algorithms, such as neural networks? (3) How does SYMSGD perform on larger-scale datasets and more complex models? (4) Are there any potential limitations or drawbacks to using SYMSGD, such as increased communication overhead or decreased accuracy in certain scenarios? 
Overall, the paper presents a significant contribution to the field of parallel machine learning, and the results are impressive. With some additional analysis and comparison, the paper could be even stronger.