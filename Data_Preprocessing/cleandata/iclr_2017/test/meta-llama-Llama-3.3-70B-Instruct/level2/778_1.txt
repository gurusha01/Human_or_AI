This paper proposes a novel approach to reduce the test-time computational load of deep neural networks by factorizing both weights and activations into integer and non-integer components. The authors claim that their method achieves significant acceleration and memory compression with minimal loss of accuracy. 
I decide to accept this paper with the following key reasons: 
1. The approach is well-motivated and placed in the literature, building upon previous studies on matrix/tensor factorization and integer decomposition.
2. The paper supports its claims with extensive experiments on three different convolutional neural networks, demonstrating the effectiveness of the proposed method in reducing computational load and memory usage.
The supporting arguments for the decision include:
* The paper provides a clear and concise explanation of the proposed method, including the ternary weight decomposition and binary activation encoding.
* The experiments are well-designed and comprehensive, covering various network architectures and tasks, including image classification and face recognition.
* The results show significant acceleration and memory compression rates, with minimal loss of accuracy, demonstrating the practical usefulness of the proposed method.
Additional feedback to improve the paper includes:
* Providing more detailed analysis of the time and space complexity of the proposed method, including the computational cost of the ternary weight decomposition and binary activation encoding.
* Investigating the applicability of the proposed method to other deep learning architectures, such as recurrent neural networks and transformers.
* Exploring the potential of the proposed method for real-time applications, such as object detection and tracking.
Questions to be answered by the authors include:
* How does the proposed method handle the case where the input data is not well-represented by the ternary basis, and what are the implications for the accuracy of the compressed network?
* Can the authors provide more insights into the optimization process of the ternary weight decomposition, including the choice of hyperparameters and the convergence of the algorithm?
* How does the proposed method compare to other network compression techniques, such as pruning and quantization, in terms of acceleration and memory compression rates?