The paper presents a general "compare-aggregate" framework for sequence matching problems, which is a fundamental task in natural language processing. The authors propose a model that follows this framework and evaluate it on four different datasets, including MovieQA, InsuranceQA, WikiQA, and SNLI. The model consists of four layers: preprocessing, attention, comparison, and aggregation. The authors focus on the comparison function, which is used to match two vectors representing words from two sequences. They evaluate six different comparison functions, including neural networks, neural tensor networks, Euclidean distance, cosine similarity, subtraction, and multiplication.
I decide to accept this paper with minor revisions. The main reason for this decision is that the paper presents a well-motivated and well-executed study on the effectiveness of the "compare-aggregate" framework for sequence matching problems. The authors provide a clear and concise overview of the framework and the different comparison functions they evaluate. The experimental results are thorough and well-analyzed, and the authors provide a detailed discussion of the results and their implications.
The paper supports its claims through a comprehensive evaluation of the model on four different datasets, using a range of metrics and baselines. The results show that the proposed model achieves state-of-the-art performance on several datasets and is competitive with other models on the remaining datasets. The authors also provide a detailed analysis of the comparison functions and their impact on the model's performance.
To improve the paper, I suggest that the authors provide more details on the preprocessing layer and its impact on the model's performance. Additionally, the authors could provide more analysis on the attention mechanism and its role in the model. It would also be helpful to include more related work on sequence matching problems and the "compare-aggregate" framework.
Some questions I would like the authors to answer include: How do the authors plan to extend the model to multi-task learning, and what are the potential challenges and benefits of doing so? How do the authors think the model could be improved, and what are the potential limitations of the current approach? What are the implications of the results for other sequence matching problems, and how could the model be applied to other tasks?
Overall, the paper is well-written, and the authors provide a clear and concise overview of the framework and the experimental results. With some minor revisions, the paper has the potential to make a significant contribution to the field of natural language processing.