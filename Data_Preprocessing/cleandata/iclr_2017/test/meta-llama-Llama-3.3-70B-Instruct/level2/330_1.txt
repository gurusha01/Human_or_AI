The paper "Document Vector through Corruption (Doc2VecC)" presents a novel approach to document representation learning, which efficiently learns vector representations of documents by averaging word embeddings. The authors claim that their model, Doc2VecC, outperforms several state-of-the-art document representation learning algorithms, including Word2Vec, Paragraph Vectors, and Skip-thought Vectors, on various tasks such as sentiment analysis, document classification, and semantic relatedness.
I decide to accept this paper, with two key reasons for this choice. Firstly, the approach is well-motivated and grounded in the literature, with a clear explanation of how the corruption model introduces a data-dependent regularization that favors informative or rare words. Secondly, the experimental results demonstrate the effectiveness of Doc2VecC in comparison to other state-of-the-art methods, with significant improvements in performance on several tasks.
The paper provides a thorough evaluation of Doc2VecC on multiple tasks, including sentiment analysis, document classification, and semantic relatedness. The results show that Doc2VecC outperforms other methods, including Word2Vec, Paragraph Vectors, and Skip-thought Vectors, on these tasks. The authors also provide a detailed analysis of the corruption model and its effect on the word embeddings, demonstrating that it helps to dampen the embeddings of common and non-discriminative words.
To further improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process, as well as a more detailed comparison with other methods, such as LSTM-based methods, on the semantic relatedness task. Additionally, it would be interesting to see an analysis of the learned word embeddings and document representations, to gain a better understanding of what the model has learned.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the corruption rate q affect the performance of Doc2VecC, and what is the optimal value of q for different tasks? How does the model handle out-of-vocabulary words, and what is the effect of vocabulary size on the performance of Doc2VecC? What are the computational resources required to train and test Doc2VecC, and how does it compare to other methods in terms of efficiency?