This paper proposes a novel approach to train stochastic neural networks for probabilistic inference, specifically for drawing samples from given target distributions. The method, called Amortized Stein Variational Gradient Descent (ASVGD), iteratively adjusts the neural network parameters to minimize the KL divergence with the target distribution. The authors also introduce SteinGAN, an application of ASVGD for training deep energy models, which obtains realistic-looking images competitive with state-of-the-art results.
I decide to accept this paper, with two key reasons: (1) the paper tackles a specific and important problem in probabilistic inference, and (2) the approach is well-motivated and supported by theoretical analysis and empirical results.
The paper provides a clear and well-structured introduction to the problem, and the proposed method is easy to follow. The authors also provide a thorough review of related work, highlighting the differences and advantages of their approach. The empirical results demonstrate the effectiveness of SteinGAN in generating realistic-looking images, and the comparison with other methods (e.g., DCGAN) is informative.
To further improve the paper, I suggest the authors provide more details on the implementation of SteinGAN, such as the architecture of the neural network and the hyperparameter settings. Additionally, it would be helpful to include more analysis on the computational efficiency of ASVGD compared to other methods, as well as more discussion on the potential applications of SteinGAN beyond image generation.
Some questions I would like the authors to answer to clarify my understanding of the paper include: (1) How does the choice of kernel in SVGD affect the performance of SteinGAN? (2) Can the authors provide more insight into the relationship between the repulsive term in SVGD and the diversity of generated samples? (3) How does SteinGAN handle mode collapse, a common issue in generative adversarial training? 
Overall, this paper presents a significant contribution to the field of probabilistic inference and generative modeling, and I believe it deserves to be accepted.