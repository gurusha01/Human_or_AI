The paper proposes a novel approach to training Skip-Gram Negative Sampling (SGNS) word embedding models using Riemannian optimization. The authors reformulate the SGNS optimization problem as a two-step procedure, where the first step involves searching for a low-rank matrix that maximizes the SGNS objective, and the second step involves computing word embeddings from the low-rank matrix. The authors propose using the Riemannian optimization framework to solve the first step, which involves optimizing the SGNS objective over the manifold of low-rank matrices.
My decision is to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-defined problem in the field of natural language processing, namely the optimization of SGNS word embedding models. Secondly, the approach proposed by the authors is well-motivated and supported by a thorough analysis of the problem, including a clear explanation of the limitations of existing approaches and a detailed description of the proposed algorithm.
The paper provides a clear and concise summary of the main claims and contributions, including the reformulation of the SGNS optimization problem as a two-step procedure and the proposal of a Riemannian optimization algorithm to solve the first step. The authors also provide a thorough evaluation of the proposed approach, including a comparison with existing state-of-the-art methods and an analysis of the results.
To further improve the paper, I would suggest that the authors provide more details on the implementation of the proposed algorithm, including the choice of hyperparameters and the computational complexity of the method. Additionally, it would be interesting to see a more detailed analysis of the results, including a discussion of the strengths and limitations of the proposed approach and a comparison with other related work in the field.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the choice of hyperparameters, such as the dimensionality of the word embeddings and the number of iterations, affect the performance of the proposed algorithm? How does the proposed approach compare to other optimization methods, such as stochastic gradient descent, in terms of computational complexity and convergence rate? What are the potential applications of the proposed approach beyond word embedding models, and how could it be extended to other areas of natural language processing?