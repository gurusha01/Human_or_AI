This paper proposes a novel algorithm for pruning whole neurons from a trained neural network using a second-order Taylor series approximation of the change in error resulting from the removal of a given neuron as a pruning criterion. The authors compare this method to a first-order method and a brute-force serial removal method, which exhaustively finds the next best single neuron to remove at each stage.
The main claim of the paper is that the proposed algorithm can effectively prune neurons from a trained network without significant loss in performance, and that this can lead to a reduction in the computational memory footprint of the network. The authors also investigate the nature of learning representations in neural networks and provide evidence for the hypothesis that learning representations are divided between units that either participate in the output approximation or learn to cancel each other's influence.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific question of how much pruning algorithms can teach us about the fundamentals of learning representations in neural networks. 
2. The approach is well-motivated and placed in the literature, with a clear explanation of the historical context and related work.
3. The paper supports its claims with experimental results, including a comparison of the proposed algorithm to other methods and an investigation of the nature of learning representations in neural networks.
However, there are some limitations and potential areas for improvement. For example, the authors note that the proposed algorithm is computationally expensive and may not be suitable for large-scale networks. Additionally, the authors acknowledge that the algorithm does not use re-training to recover from errors made in pruning decisions, which may limit its effectiveness in certain scenarios.
To improve the paper, I would suggest the following:
* Provide more detailed analysis of the computational complexity of the proposed algorithm and its potential scalability to larger networks.
* Investigate the use of re-training or other methods to recover from errors made in pruning decisions, and evaluate the effectiveness of these approaches.
* Consider applying the proposed algorithm to other datasets and network architectures to further evaluate its generalizability and effectiveness.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* Can you provide more details on the derivation of the second-order Taylor series approximation used in the proposed algorithm?
* How do you select the threshold value used in the mean and median thresholding approach?
* Can you provide more information on the experimental setup and hyperparameters used in the experiments, such as the learning rate and number of epochs? 
Overall, the paper provides a valuable contribution to the understanding of neural network pruning and learning representations, and with some revisions and additional analysis, it has the potential to be a strong and impactful paper.