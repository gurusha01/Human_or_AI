This paper proposes a novel model called Classless Association, which trains two parallel Multilayer Perceptrons (MLPs) to learn the association between two input samples that represent the same unknown class. The model is motivated by the Symbol Grounding Problem and association learning in infants. The authors claim to contribute a novel training rule based on matching the output vectors of the model to a statistical distribution, a novel architecture for learning the association in the classless scenario, and an evaluation of the model against two cases: totally supervised and totally unsupervised.
I decide to accept this paper with some minor revisions. The main reason for this decision is that the paper presents a well-motivated approach to a complex problem, and the results show promising performance compared to clustering algorithms and a supervised method. The authors provide a clear explanation of the model and its components, and the experiments are well-designed to evaluate the model's performance.
One of the strengths of the paper is its ability to learn the association between input samples without labeled data. The authors demonstrate that their model can learn to discriminate between different digits and assign them to the same pseudo-class, even when the input samples are rotated or inverted. The results also show that the model can reach better performance than clustering algorithms and a supervised method in some cases.
However, there are some limitations to the paper. One of the main limitations is that the model requires a uniform distribution of the input samples, which may not always be the case in real-world scenarios. The authors acknowledge this limitation and propose to explore the performance of the model when the number of classes and the statistical distribution are unknown.
To improve the paper, I suggest that the authors provide more analysis of the learning behavior of the model, particularly when the number of classes and the statistical distribution are unknown. Additionally, the authors could explore the performance of the model in more complex scenarios, such as multimodal datasets.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How do the authors plan to extend the model to deeper architectures, and what are the potential benefits and challenges of doing so?
* How does the model perform when the number of classes is unknown, and what methods can be used to determine the optimal number of pseudo-classes?
* Can the authors provide more insight into the role of the weighting vector in the EM-training rule, and how it affects the performance of the model?
Overall, the paper presents a novel and well-motivated approach to the classless association problem, and the results show promising performance. With some minor revisions and additional analysis, the paper has the potential to make a significant contribution to the field.