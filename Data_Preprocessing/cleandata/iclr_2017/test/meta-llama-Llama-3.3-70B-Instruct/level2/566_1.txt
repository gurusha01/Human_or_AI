This paper proposes a novel batch active learning framework for deep neural networks, specifically Convolutional Neural Networks (CNNs), using a variational approximation to perform Bayesian inference. The authors introduce a scalable and efficient active learning method that can be applied to most neural networks, which is a significant contribution to the field.
The paper clearly outlines its main claims, including the introduction of a batch active learning framework, the use of variational inference for Bayesian inference, and the derivation of a formulation of the posterior and prior distributions of the weights using statistical knowledge on the Maximum Likelihood Estimator. The authors also provide a thorough review of related works, discussing the limitations of traditional active learning techniques and the potential of batch mode active learning for deep networks.
The approach is well-motivated, and the authors provide a solid theoretical foundation for their method. The use of Fisher information and the Kronecker factored approximation of the Fisher information matrix are particularly noteworthy, as they enable the authors to develop a computationally efficient active learning criterion.
The experimental results on the MNIST and USPS datasets demonstrate the effectiveness of the proposed method, achieving better test accuracy than random sampling and uncertainty sampling. The authors also show that their method is scalable with increasing size of queries, which is an essential aspect of active learning.
However, there are some limitations to the paper. The authors acknowledge that their approximations are subject to improvements, and the use of asymptotic distributions may be unstable on small subsets of observed data. Additionally, the paper could benefit from a more detailed analysis of the computational complexity of the proposed method and a comparison with other state-of-the-art active learning techniques.
Overall, I would recommend accepting this paper, as it presents a significant contribution to the field of active learning for deep neural networks. The authors provide a well-motivated and theoretically sound approach, and the experimental results demonstrate the effectiveness of their method.
To improve the paper, I would suggest the following:
* Provide a more detailed analysis of the computational complexity of the proposed method and compare it with other state-of-the-art active learning techniques.
* Investigate the use of regularization techniques to stabilize the asymptotic distributions on small subsets of observed data.
* Consider exploring other approximations of the Fisher information matrix, such as submodular functions, to further improve the efficiency and effectiveness of the proposed method.
Questions for the authors:
* Can you provide more details on the computational complexity of the proposed method and how it compares to other active learning techniques?
* How do you plan to address the potential instability of the asymptotic distributions on small subsets of observed data?
* Have you considered exploring other applications of the proposed method, such as curriculum learning or transfer learning?