This manuscript explores the application of neural conversational models to batch reinforcement learning, leveraging human scoring data for select responses from a dialogue model, which is costly to obtain. Consequently, the approach adopts off-policy learning, wherein a base policy is trained on unsupervised data, deployed to gather human scores, and subsequently refined offline using these scores.
Although the overall contribution may be considered incremental, building upon off-policy actor-critic methods for dialogue generation, the methodology is well-justified and the paper is articulate and accessible. 
However, a primary concern arises from the limited size of the primary dataset utilized, comprising merely 6000 conversations related to restaurant recommendations. Notably, this dataset is substantially smaller than those commonly employed in the literature for dialogue generation, such as Twitter or the Ubuntu Dialogue Corpus. It is somewhat surprising that basic RNN chatbots can produce coherent utterances with such a restricted dataset. The work by Wen et al. (2016) achieved similar outcomes on a comparable small-scale restaurant dataset, albeit through directly mapping dialogue states to surface forms rather than relying on contextual embedding representations. Therefore, it remains uncertain whether the proposed approaches will yield improvements when significantly larger amounts of unsupervised data are available.
References:
Wen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. "A Network-based End-to-End Trainable Task-oriented Dialogue System." arXiv preprint arXiv:1604.04562 (2016).