This paper proposes a novel approach to training neural networks without explicit labels, instead utilizing link/not-link information between example pairs. The methodology employs a pair of networks, where each network supervises the other.
However, the paper's presentation lacks clarity, and the writing could be enhanced for better comprehension. Certain design decisions are not adequately justified, such as the use of the power function in the E-step for distribution approximation (section 2.1) and the exclusive consideration of a uniform distribution. Although using a different prior may violate the assumption of no prior class knowledge, it is unclear what practical scenarios would benefit from the proposed setup.
Furthermore, the concept bears resemblance to existing co-training methods in semi-supervised learning, which have been extensively explored. To make this work suitable for the conference, it is essential to address these concerns, provide clearer explanations, and demonstrate the practical applicability of the proposed approach.