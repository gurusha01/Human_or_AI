This paper presents a novel reinforcement learning environment, "The Retro Learning Environment", which leverages the open-source LibRetro API to provide access to a range of emulators and associated games, with the initial focus on the SNES platform and five accompanying games. The authors contend that SNES games offer greater challenges compared to those on the Atari 2600 due to their more complex graphics, AI, and game mechanics. The paper evaluates several variants of Deep Q-Networks (DQN) in experiments and proposes a comparison of learning algorithms through competitive multiplayer games.
The concept of progressing to more complex games beyond the Atari 2600 is commendable, and the potential for easy addition of new consoles and games to the environment is promising. However, with the recent release of OpenAI Universe and DeepMind Lab, it is questionable whether another environment is necessary at this time. Furthermore, the use of emulated game ROMs without ownership raises legal concerns, which may become more pronounced as the community transitions to more advanced and recent games, particularly those still generating revenue for companies like Nintendo.
Beyond the introduction of the environment, the provision of DQN benchmarks for five games is a positive aspect, although it does not significantly contribute to the field. The authors' claim of introducing a new benchmarking technique, where algorithms compete against each other rather than the in-game AI, seems overstated, as this concept has been a cornerstone of AI competitions for decades. The finding that reinforcement learning algorithms tend to specialize in their opponents is also not particularly surprising.
In summary, while this is an acceptable paper, it does not bring sufficient novelty or impact to warrant presentation at a major conference. Nevertheless, the new environment may still find its place within the increasingly crowded landscape of game-playing frameworks.
Additional minor comments include:
- The presence of numerous typos throughout the paper.
- The mention of Infinite Mario as a benchmark platform is inaccurate, as it was discontinued due to Nintendo's objections.
- The statement that the Retro Learning Environment requires both an emulator and a computer version of the console game (ROM file) upon initialization, whereas the emulator is provided with the environment, does not clearly differentiate it from the Atari 2600 Arcade Learning Environment, which also requires the Stella emulator provided with it.
- The absence of DQN and Double Deep Q-Network (DDDQN) results for Super Mario is notable.
- It is unclear whether Figure 2 displays the F-Zero results with or without reward shaping.
- The reference to Du et al appears to be incomplete.