This paper presents a reinterpretation of dropout regularization that, while not entirely novel, remains imperfectly understood. The authors establish valuable theorems that provide estimates or bounds for crucial quantities relevant to the analysis of dropout-regularized networks from their proposed viewpoint. Additionally, they introduce an explicit regularization term with a well-understood influence on these key quantities. The experimental section effectively demonstrates that the proposed regularization has the anticipated effect, validating the usefulness and significance of their perspective on dropout.
The introduced regularization appears to positively influence model performance, although this is only demonstrated using relatively small-scale benchmark problems. Consequently, it is questionable whether this approach will substantially impact the training practices of practitioners. Nevertheless, the authors' general perspective aligns well with the recently introduced concept of "Dropout as a Bayesian approximation," and the insights and theorems presented in this paper may facilitate future research in this direction.