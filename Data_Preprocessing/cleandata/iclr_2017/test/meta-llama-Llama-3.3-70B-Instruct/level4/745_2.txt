Overall, this paper presents an intriguing concept and is well-structured, with clear motivation. However, in its current form, it falls short of the standards required for publication in ICLR due to several key concerns:
- The paper's focus does not align with representation learning, making it a better fit for a broader machine learning or data mining conference.
- The proposed method has limited applicability, being restricted to a narrow class of models. It cannot be applied to widely used formulations such as Support Vector Machines (SVM), logistic regression, or neural networks. The rationale behind choosing Stochastic Gradient Descent (SGD) for these specific formulations is unclear. For instance, in the case of linear regression, a comparison with linear programming approaches is warranted. Moreover, the development of a parallel algorithm for linear regression problems seems unnecessary unless dealing with large datasets, which leads to the next point.
- The datasets utilized in the paper are small-scale and primarily serve to demonstrate the concept rather than provide comprehensive validation. Most of the considered datasets can be solved efficiently using a single-core CPU in a matter of seconds. While Hogwild! is advantageous for sparse datasets due to its asynchronous nature, the proposed approach offers minimal or no improvement over Hogwild! on very sparse data. For dense datasets, the necessity of using SYMSGD over simple parallelization of gradient computation via GPUs is not justified. Collectively, these experimental results lack convincing evidence to support the proposed method's efficacy.