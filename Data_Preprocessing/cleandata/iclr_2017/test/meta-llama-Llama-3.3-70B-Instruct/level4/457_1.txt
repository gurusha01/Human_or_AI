This paper addresses the ongoing quest for compressing neural network models, with a particular focus on utilizing low-precision weight representations. Although previous approaches, which have explored weights as low as 1 or 2 bits, have been hindered by significant accuracy losses, the proposed iterative quantization scheme offers a promising alternative. By quantizing network weights in a staged manner, where the largest weights are quantized and fixed while unquantized weights adapt to mitigate errors, the authors demonstrate exceptional effectiveness. The experimental results reveal that models with 4-bit or 3-bit weights can be achieved with negligible accuracy reduction, and even at 2 bits, the accuracy decrease is minimal compared to other quantization methods.
The paper is well-structured, and the technique appears to be novel to the best of my knowledge. The experiments are comprehensive, and the results are highly convincing, leading me to recommend acceptance. However, a secondary review of the writing style and grammar would be beneficial to enhance clarity. Additionally, the explanation of the pruning-inspired partitioning strategy could be improved, such as providing a clearer justification for the chosen 50% splitting ratio, which is currently only mentioned in a figure caption and not explicitly discussed in the main text.