This manuscript introduces a novel and valuable compilation of video game benchmarks within a flexible framework, providing initial performance baselines for a subset of these benchmarks.
Regarding reward structures, it is unclear how many of the potential games have been equipped with mechanisms for score extraction and incremental reward structures. An examination of the GitHub repository suggests that this has been implemented for approximately 10 games. Could the authors clarify plans for expanding this to additional games and provide a timeline for such expansions?
The concept of "rivalry" training is a less convincing aspect of the paper and may benefit from reduced emphasis. This topic is well-established within the multi-agent literature, which remains uncited in the current work. To avoid potential controversy, it is recommended that the authors refrain from claiming novel contributions in this area, as the methods described do not appear to introduce new techniques for training agents against multiple opponents or for benchmarking through competitive evaluation. Instead, the focus should be on the establishment of single-agent and multi-agent baselines for the new benchmark suite.
A technical correction is necessary regarding the definition of the Q-function, which is currently described as predicting the score at the end of the game given the current state and action. A more accurate definition would be that it estimates the cumulative discounted reward obtainable from a state, starting with a specific action and following a defined policy.
Several minor adjustments are also suggested:
* In Equation (1), it should be clarified that the Q-network within the max() function refers to the target network, characterized by distinct parameters theta'.
* The reference to Du et al. is missing the publication year.
* For some references, it would be more appropriate to cite the published papers rather than their arXiv versions.