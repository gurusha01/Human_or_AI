I would like to start by apologizing for the delay in submitting my review.
Summary: This study investigates the transfer learning of a specific reading comprehension model (AS Reader) through various experiments, utilizing a large artificial dataset to adapt to a target dataset.
From my understanding, the authors conduct several transfer learning experiments, although I am not entirely certain about the details. The experiments appear to involve:
1. Training the model on the large artificial dataset and evaluating its performance on smaller target datasets (section 4.1).
2. Pre-training the model on the large artificial dataset, followed by fine-tuning on a subset of examples from the target dataset, and then testing on the remaining target examples. Multiple models are trained using different fine-tuning subsets, and their performance is compared to that of randomly initialized and fine-tuned models (section 4.2).
3. Pre-training the model on the large artificial dataset, and then selectively resetting either the embedding or encoder component to random initialization to assess the importance of pre-training in each component. The model is then fine-tuned on a subset of target examples and evaluated on the remaining target examples (section 4.3).
I find it challenging to follow the results due to the composite nature of the test set, which comprises multiple subtasks. The reported performance sometimes represents the mean across tasks, while other times it reflects performance on specific tasks or the mean performance of multiple models. I suggest including standard deviations to provide a more comprehensive understanding. Additionally, clarification on the term "best validation" would be beneficial.
The work is interesting and unassuming, but the presentation could be improved by simplifying the experimental setup. A notable conclusion is presented at the end of section 4.1, where the authors highlight nuanced differences between the datasets.
Minor issues include unexplained acronyms such as GRU, BT, and CBT, as well as typographical errors ("benfits" on page 2 and "subsubset" on page 6).