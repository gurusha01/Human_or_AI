Review- Clarity: The innovative contribution presented in Section 2.2 of the paper was challenging to comprehend due to inconsistent notation, particularly with regards to the variables l, p, and m, which has left me uncertain about the underlying model.
Originality: The paper's novelty stems from the application of the RFN model, incorporating the ReLU non-linearity and dropout training, to the biclustering problem, which appears to be a promising approach.
Significance: The proposed algorithm seems to be a valuable tool for unsupervised data modeling, with the authors providing a compelling argument for its significance, including outperforming and addressing practical limitations of the state-of-the-art FABIA method.
Quality: The experimental design and results are of high quality.
Comments:
1) The introduction suggests that the method achieves significant speed improvements over FABIA due to GPU compatibility facilitated by rectified units. However, the underlying mechanism and the method's scalability, particularly in terms of the number of supported biclusters, are unclear. The experiments only demonstrate the method's capability with 3-5 biclusters.
2) The introduction posits that dropout training enhances sparsity in bicluster assignments, a hypothesis that, while reasonable, requires more substantial theoretical justification or empirical evidence to be fully convincing.
3) The description of the model as "deep" solely based on the use of ReLU and dropout is misleading, as depth in models typically implies a more complex architecture, which is not evident in this case.