This paper presents a method for predicting future frames in a video sequence based on a set of past frames, utilizing a convolutional neural network (CNN) that operates in the space of affine transformations. Unlike most related works, which focus on pixels or flow, this approach takes a set of affine transforms describing patch motion in past frames as input and outputs a set of affine transforms predicting future patch motion.
The authors make several simplifying assumptions, including modeling a sequence of frames using their patch-affine framework, which is not unreasonable given the success of similar hypotheses in the optical flow community, such as those employed in "Locally affine sparse-to-dense matching for motion and occlusion estimation" by Leordeanu et al., "EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow" by Revaud et al., and "Optical Flow With Semantic Segmentation and Localized Layers" by Sevilla-Lara et al. Reformulating the prediction task as predicting motion rather than raw pixels also seems reasonable, as the motion space is significantly smaller and more tractable, particularly for high-resolution videos.
However, despite these valid points, the paper has several significant flaws. Notably, the authors' decision not to compare their approach with previous methods in terms of pixel prediction error appears convenient and dismissive of quantitative comparisons. The network's output on datasets like moving digits (Figure 4) could be compared to other papers, but the authors chose not to, raising suspicions.
The newly proposed metric has several issues. Firstly, using C3D for action classification is not state-of-the-art. Secondly, this metric does not evaluate the network's claimed capability of next frame prediction but instead assesses whether another network can accurately classify actions from predicted frames. This proxy metric is only weakly related to the intended measurement and does not make sense for the final task. 
Furthermore, the estimation of affine motion of patches is not clearly explained, with only a vague mention of a global solution. Estimating motion for all patches is akin to solving optical flow, an active research area. The paper's approach depends heavily on this input, and errors in motion estimation, as seen in the provided videos, can significantly impact the network. The patch-affine hypothesis also fails when patches cover multiple objects with contradictory motions, as observed in UCF101 videos.
Additionally, the network is not trained end-to-end, minimizing the difference between noisy ground-truth and output affine transforms instead of minimizing a loss in the actual output space (frame pixels) where exact ground-truth is available. While the authors note that minimizing a loss in the transformation space introduces artifacts, they do not explore alternative losses, such as the gradient loss introduced by Mathieu et al., which has been shown to address the issue of blurry results.
The approach is also not sufficiently compared to previous work, particularly the closely related "SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY" by Taraucean et al., which also outputs predictions in the motion space. Experimental results should include comparisons with such relevant studies.
The comparison with optical flow is unfair, using an approach over 10 years old by Brox et al. and assuming constant flow for all frames without considering basic extrapolation to account for the flow of all input frame pairs. The approach is not compared to challenging baselines, further undermining the validity of the results.
Lastly, the authors' response to a reviewer's question regarding ground-truth frames {X0, X1 ...} and predicted frames {Y1, Y2, ...} is disagreed with, highlighting a need for clearer clarification on the method's capabilities and limitations.