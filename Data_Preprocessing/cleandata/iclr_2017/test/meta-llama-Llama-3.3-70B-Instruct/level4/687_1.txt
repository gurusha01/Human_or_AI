The authors have made a genuine attempt to explore the "fundamental nature of learning representations in neural networks", a subject of significant interest and relevance to our field. Their approach involves utilizing simple pruning algorithms to observe performance degradation as a function of unit removal. Although this concept is intriguing and potentially informative, I believe it falls short of achieving its intended goal.
Initially, I found the introduction to pruning to be lengthy and lacking in novelty or surprise. For instance, Figure 1 and a significant portion of Section 3.3.0 are unnecessary. The proposed pruning algorithms, while sensible, are overly simplistic and would only be justified if they effectively addressed the research question. However, in my opinion, this paper does not offer a unique, concise, or innovative perspective on pruning.
Furthermore, and most pertinent to my overall assessment, Section 4 fails to provide a thorough analysis, merely scratching the surface of the topic. The figures primarily illustrate the anticipated decline in performance as neurons are removed or gain values are adjusted. The experiments, which only cover a toy problem and MNIST, are not comprehensive enough to convince me that the findings can be generalized to the broader context of neural networks.
Ultimately, the paper lacks essential insights into algorithms, architecture, or mathematics, which I consider a crucial aspect of all but the most experimentally focused papers.