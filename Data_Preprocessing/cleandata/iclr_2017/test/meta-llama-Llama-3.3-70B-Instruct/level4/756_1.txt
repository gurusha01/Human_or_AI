This manuscript proposes a technique for mapping data instances to a lower-dimensional space, aiming to conserve a certain degree of similarity between them.
However, the concept presented is not as innovative as claimed, since numerous pre-trained embeddings, including auto-encoders and word2vec, have long employed similar methods to represent items in a low-dimensional space where their similarities are inherently encoded. Furthermore, upon examining the specific application to word and context embeddings, it becomes apparent that the approach bears a strong resemblance to a similarity function outlined in "A Simple Word Embedding Model for Lexical Substitution" by Melamud et al. in 2015. Therefore, a more precise assertion of novelty is required, one that contextualizes the work within the existing body of research.
Moreover, the evaluation methodology could be enhanced. There exists a plethora of benchmarks for assessing word embeddings in context, such as: