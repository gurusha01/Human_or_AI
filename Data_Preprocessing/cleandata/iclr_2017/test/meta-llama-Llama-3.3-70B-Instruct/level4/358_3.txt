The authors propose a variant of the variational autoencoder (VAE) that incorporates dataset-level latent variables, presenting a well-motivated and clearly described idea. The primary contribution of this paper lies in its extension beyond the traditional VAE's simple graphical model structure, introducing more complex and intriguing structures to the deep learning community.
Comments:
- The terminology "statistician" is unclear, as learning an approximate posterior over summary statistics is not the sole method for summarizing a dataset using a neural network. Alternative approaches, such as maximum likelihood, could also be considered. The paper would benefit from avoiding the introduction of new terms like "statistic network" and instead using more accurate descriptions, such as "approximate posterior".
- The experiments are well-conducted, and the response to the question regarding "one shot generation" is appreciated. However, the language at the end of page 6 requires clarification. The interpretation of Figure 5 is as follows: given an input set, compute the approximate posterior over the context vector and then generate samples from the forward model using samples from the approximate posterior. Further clarification is needed on the following points:
(a) Are the data point dependent vectors z generated from the forward model or sampled from the approximate posterior?
(b) While the samples appear to be of high quality, this statement lacks quantification. A key advantage of VAEs over GANs is the ability to compute log-probabilities naturally. To evaluate the "one shot generation" performance, it would be more informative to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. The log probability performance of these networks, compared to a standard VAE without the context latent variable, is likely to be impressive, and its inclusion would be beneficial.