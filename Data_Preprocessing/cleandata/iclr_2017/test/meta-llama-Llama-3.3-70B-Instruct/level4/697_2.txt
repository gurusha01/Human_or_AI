Dear authors,
The authors' response has addressed some of my initial concerns, but I still have several outstanding questions:
-- The response highlights a key contribution as a novel formulation, wherein word embedding learning is decomposed into two stages: the first stage utilizes Riemannian optimization to obtain a low-rank matrix X, and the second stage factorizes X into two matrices, W and C. The authors claim that this approach outperforms existing methods that directly optimize W and C. However, given that the ultimate outcome (the factors) remains the same, I would appreciate it if the authors could provide further insight and justification for why their proposed method yields superior results.
Upon closer examination, it appears that the initial stage of the proposed method and existing approaches (such as SGD) both involve optimizing over low-rank matrices, albeit with differing parameterizations. While Riemannian optimization does eliminate the rotational degree of freedom (as mentioned in section 2.3), I am not entirely convinced that this is the primary source of the observed improvement. To clarify this, it would be helpful to include learning curves for the objectives, which would enable a more direct comparison of the effectiveness of Riemannian optimization.
-- Another point that I found unclear is the following. The authors mention that a limitation of other approaches is that their factors W and C do not directly reflect similarity. I was wondering if the authors attempted to multiply the factors W and C obtained from other optimizers, and then re-factorize the resulting product using the method outlined in section 2.3, ultimately using the new W for downstream tasks. It is unclear to me whether this would have a significant impact on performance.
In general, I believe that applying advanced optimization techniques to machine learning problems is a valuable pursuit. To strengthen the paper from a machine learning perspective, I recommend providing more comprehensive comparisons and discussions, as outlined above. However, as my expertise lies outside the realm of NLP, I leave it to other reviewers to assess the significance of the experimental results.