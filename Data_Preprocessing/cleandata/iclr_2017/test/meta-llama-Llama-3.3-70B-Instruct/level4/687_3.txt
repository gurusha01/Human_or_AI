This paper proposes a novel pruning method for neural networks utilizing the second-order Taylor expansion, which is compared to a first-order method and brute-force pruning. The experiments, conducted on several toy examples including a two-layer network on MNIST, reveal that the second-order method performs significantly worse than the brute-force baseline. Additionally, the success of brute-force pruning leads the authors to suggest that the hypothesis of Mozer et al., stating that neurons either contribute to performance or cancel out the effect of other neurons, is likely correct.
The authors have made a considerable effort to clearly explain the details of the paper, making the content accessible to those unfamiliar with pruning methods. They have also been responsive and thorough in addressing questions raised during the pre-review process.
However, a major criticism of the paper is its lack of focus, absence of a concrete conclusion, and failure to clarify its contribution to the existing literature. The conclusion section lacks coherence, with each paragraph discussing a different aspect of the research without a clear overarching theme. The paragraphs touch on the ineffectiveness of pruning methods against the brute-force baseline, the introduction of the second-order Taylor method, the potential benefits of re-training, and the varying effectiveness of brute-force pruning in shallow and deep networks.
The paper's title and the authors' responses to pre-review questions initially suggested that the research focuses on learned representations, but this aspect is barely mentioned in the conclusion and not at all in the abstract's results section. The authors' statement that brute-force search reveals neurons can be pruned without re-training and with minimal performance impact is not a novel conclusion, as similar results can be obtained from existing second-order methods.
Furthermore, some conclusions about representations, such as the cancellation effect, may be influenced by the greedy serial pruning method used, which is a limitation that the authors acknowledge but do not fully address. The use of brute-force pruning, which is also a serial process, raises similar concerns.
Overall, the paper's contribution is unclear, with insufficient conclusions about learned representations and inadequate benchmarking against state-of-the-art pruning methods. To improve the paper, it is suggested that the authors focus on using a state-of-the-art pruning method or solely brute-force pruning, and concentrate on a limited number of examples, such as 2-layer MNIST and a deeper CIFAR10 network. Adding an itemized list of exact contributions and streamlining the paper accordingly could significantly enhance its impact, but would require a major revision.
The confusion surrounding the paper's purpose appears to stem from the abstract, which mentions testing hypotheses about neural network learning representations and numerical approaches to pruning as orthogonal aspects that are mixed up throughout the paper.