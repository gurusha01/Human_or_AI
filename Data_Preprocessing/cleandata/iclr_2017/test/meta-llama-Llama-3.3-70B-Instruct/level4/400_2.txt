The paper presents a novel neural decoder, DRNN, designed for tree structures, which exhibits two significant enhancements over conventional methods: (1) bidirectional information flow, originating from both parent and sibling nodes, a desirable property in tree structures, and (2) the utilization of a probability distribution to delineate tree boundaries, specifically the last sibling or leaf node, thereby eliminating the need for special ending symbols and reducing the parameter learning burden.
The authors evaluate the DRNN model through two tasks: recovering synthetic trees and functional programs, demonstrating superior performance compared to traditional seq2seq models. However, the synthetic tree recovery task has limitations, primarily due to two factors: (1) the surface form inherently containing topological information, which simplifies the task, and (2) the model's performance degrades substantially as the number of nodes increases, even at moderate scales, as illustrated in Figure 3. This raises questions about whether a basic baseline focused solely on capturing surface string topological information would yield significantly worse results, and whether the DRNN model is fully utilized given the relatively short information flow lengths.
While the experiments are intriguing, more challenging tasks that heavily rely on tree structure information could better demonstrate the DRNN's capabilities. For instance, incorporating the proposed DRNN into the decoder of seq2seq parsing models, such as Vinyals et al. (2014), could provide a more convincing showcase of the model's potential and its advantages over traditional alternatives. Such tasks would offer a more comprehensive understanding of the DRNN's performance and its suitability for complex tree structure decoding.