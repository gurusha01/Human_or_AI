The key contributions of this paper can be outlined as follows:
1. The introduction of a TransGaussian model, analogous to TransE, which parameterizes subject and object embeddings using a Gaussian distribution, allowing for natural adaptation to path queries as formulated by Guu et al. (2015).
2. The development of an LSTM + attention model, leveraging entity and relation representations learned by TransGaussian, to learn a relation distribution for question answering from natural language questions.
3. Experimental evaluation on the WorldCup2014 dataset, with a focus on path queries and conjunctive queries.
In general, the Gaussian parameterization exhibits promising properties, making it potentially suitable for knowledge base completion and question answering. However, certain aspects of the paper, including key experimental results and details, are not entirely convincing, and the writing could be improved. Further comments are provided below:
[Major comments]
- A primary concern is the lack of robustness in the evaluation results. Given the existence of competitive benchmarks such as FB15k and WebQuestions for knowledge base completion and KB-based question answering, the reliance on the small WorldCup2014 dataset is unconvincing. Furthermore, the use of template-generated questions, which are far removed from natural language questions, raises questions about the necessity of applying an LSTM in this context. Demonstrating the model's effectiveness on established benchmarks would significantly strengthen the paper.
- The handling of conjunctive queries assumes that detected entities in questions can be aligned with one or more relations, allowing for conjunctions. This assumption may not always hold true, making it essential to validate this approach using real question answering datasets.
- The model's naming as "Gaussian attention" may be misleading, as it seems more closely related to knowledge base embedding literature than the traditional attention mechanism.
[Minor comments]
- Figure 2 is somewhat confusing, with the first row of orange blocks representing KB relations and the second row denoting individual words of the natural language question. Clarifying this representation would be beneficial.
- In addition to "entity recognition," the importance of an "entity linker" component, which connects text mentions to knowledge base entities, should be acknowledged.