This paper proposes a novel approach to learning low-dimensional state representations from raw observations in a multi-task setting, deviating from traditional methods that learn a joint representation by sharing information across tasks. Instead, the authors aim to identify and solve individual tasks separately, extending the learning with robotic priors approach by incorporating an additional term for task coherence into the loss function. This ensures that a task's representation remains consistent between training episodes. The method's efficacy is demonstrated through experiments on multi-task slot-car racing and mobile navigation.
However, several unclear issues remain:
1. The first concern is whether the method's appeal is limited to specific scenarios like slot-car racing, and if not, why it was not benchmarked against traditional multi-task learning approaches. Although the authors argue that their method is orthogonal to multi-task learning, they acknowledge that both methods explore shared knowledge between tasks. A clearer explanation of the advantages and disadvantages of the proposed method in general multi-task settings, particularly in comparison to multi-task learning, is needed. The authors' response did not adequately address this concern, and the lack of comparison to multi-task joint learning undermines the method's practical advantages.
2. Further clarification is required on the results of the mobile navigation scenario. The plot provided seems insufficient to support the claim that the method identifies all tasks, and the results appear weaker compared to the multi-slot car racing experiment. Notably, there is a lack of comparison to alternative methods, including baseline approaches, which raises questions about the soundness of the argument.
3. The proposed gated neural network architecture appears to employ a soft gating structure. A potential baseline for comparison could be a hard-gated unit, which might affect the conclusion. This is particularly relevant given the authors' emphasis on the constraint that the representation should remain consistent during training. Although the authors reiterate their modeling approach, they do not address the comparison to hard gating, which is a lesser concern compared to the first question.
In summary, while there are lingering concerns regarding the lack of comparisons, the review leans slightly towards accepting the submission.