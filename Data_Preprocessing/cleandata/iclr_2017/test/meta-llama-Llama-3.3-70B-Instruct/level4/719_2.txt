This paper presents a novel approach to classless association, a variant of unsupervised learning where class labels are unknown but a prior association between examples is available. The authors propose a dual-stream architecture, comprising two neural networks that process examples from the same class in tandem. Each stream relies on the target information (pseudo-classes or cluster indices) from the other, generating an intermediate representation z that is constrained to match a statistical distribution, specifically a uniform distribution. The model is trained using an Expectation-Maximization (EM) framework, where the E-step estimates the current statistical distribution given the output vectors z, and the M-step updates the architecture's weights based on z and the pseudo-classes. Experimental results on a reorganized MNIST dataset demonstrate improved performance compared to traditional clustering algorithms, both in terms of association accuracy and purity. Additionally, the authors provide a comparison with a supervised method, where their proposed architecture, as expected, performs worse but shows promising results.
However, the underlying motivation for the proposed architecture is not immediately clear, and the paper's technical details obscure the rationale behind matching distributions and utilizing pseudo-targets from each stream. It appears that the approach assumes a uniform statistical distribution of classes, but it is unclear how this would generalize to other priors or scenarios where the prior is unknown. Further justification for the current setup is necessary.
An intriguing extension would be to apply this approach to examples from different datasets, such as MNIST and Rotated-MNIST or Background-MNIST, to assess the architecture's ability to handle diverse inputs. It is challenging to intuitively understand the differences between examples in the two streams.
Ultimately, the authors have identified an interesting approach to classless association, which could be applied to various many-to-one problems. This is a valuable contribution, and it would be exciting to see this idea developed further with extensive experiments on large-scale datasets and tasks. However, the current version of the paper lacks clear theoretical motivations and convincing experimental results. Despite this, I would recommend this paper for presentation at the ICLR workshop.
Several minor points require attention: 
- A typo in Figure 1's caption should be corrected ("that" -> "than").
- The necessity of Equation 2 is unclear and requires explanation.
- The batch size M is significantly larger than those used in classical models, but no justification is provided.
- The choice of a uniform distribution should be clarified, even if it is the simplest prior to assume.
- A typo on page 6, in the second paragraph, line 3, should be corrected ("that" -> "than").