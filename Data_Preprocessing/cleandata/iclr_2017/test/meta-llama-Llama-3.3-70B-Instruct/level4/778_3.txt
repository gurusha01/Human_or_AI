This manuscript presents a novel quantization approach for both weights and activations that eliminates the need for re-training, achieving a 20x compression ratio and 15x speed-up in the VGG-16 model. The paper is well-structured and effectively conveys the methodology and results.
However, I have several major concerns: firstly, the paper lacks a comparison with existing pruning methods outlined in section 1.1, making it challenging to assess the method's performance. Secondly, other compression schemes, including pruning, re-training, and vector-quantization [e.g., 1, 2, 3], appear to yield higher accuracies, compression ratios, and speed-ups, potentially making them more suitable for low-power, low-memory devices. The advantages of the proposed method, beyond potentially reducing compression time, are unclear. Notably, using a pre-trained network as a starting point for a quantized model with subsequent fine-tuning may not significantly differ in processing time from the proposed method. Lastly, the significant speed-up and memory reduction in the VGG model seem to stem primarily from the fully-connected layers, particularly the last one, with relatively modest improvements in the convolutional layers, raising questions about the method's efficacy in all-convolutional networks like the Inception architecture.
Reference: Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.