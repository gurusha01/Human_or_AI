This manuscript proposes a framework for active learning using convolutional neural networks (CNNs), although the term "deep" may be somewhat misleading given the relatively simple architecture consisting of only two hidden layers with 20 filters each. The active learning strategy employed is based on a greedy selection method that utilizes variational free energy and involves a series of approximations.
The readability of the paper is occasionally compromised due to (a) numerous grammatical errors and (b) inconsistent notation, as exemplified on page 5, line 1, where the variable f is introduced without prior definition. Despite these issues, I am inclined to recommend acceptance, albeit with a low score, contingent upon the correction of grammatical errors, ideally with the assistance of a native English speaker, in the final version.
The topic addressed in the paper is intriguing, and the authors appear to have successfully demonstrated the feasibility of active learning in CNNs, albeit solely on simplified datasets. The inclusion of novel results on uncertainty sampling and curriculum learning is noteworthy, but it is puzzling that these methods exhibit poor performance on the USPS dataset. Specifically, uncertainty sampling yielded superior results on MNIST but failed to do so on USPS, warranting further explanation.
I have an additional inquiry regarding the necessity of initially sampling a larger subset D from the universe U, from which active learning is then applied. Is this step motivated by computational efficiency or can it potentially enhance outcomes? If the latter, it would be beneficial to provide a comparison with the results obtained without this step.
Some of the approximations presented are complex, and it would be helpful to know if the corresponding code is available. Furthermore, the term "groundtruth" used in Figures 1 and 2 requires clarification.
In Section 5.2, the time complexity of the proposed approach is analyzed, revealing a selection time of up to 30 seconds per minibatch. A comparison with the time required for updating the model using backpropagation would provide valuable context.