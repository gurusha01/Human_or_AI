This manuscript investigates off-policy learning in actor-critic methods utilizing experience replay, a crucial and complex issue that affects the sample efficiency of reinforcement learning algorithms. The authors address this challenge by proposing a novel importance weight truncation method, a modified trust region optimization approach, and integrating the retrace method. The synergistic combination of these techniques yields impressive results on Atari and MuJoCo benchmarks, demonstrating enhanced sample efficiency. A key question arises regarding the individual contribution of each technique to the overall performance improvement. Conducting additional experiments to quantify the separate benefits of these methods would provide valuable insights, helping to elucidate their respective roles in achieving the observed gains.