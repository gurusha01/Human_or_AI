This paper introduces a theoretically grounded optimization approach for Skip-Gram Negative Sampling (SGNS), a variant of the word2vec algorithm. 
Although the proposed methodology exhibits theoretical elegance, its practical advantages are unclear. Specifically, it is uncertain whether the incorporation of Riemannian optimization yields significant benefits, such as accelerated model convergence, over existing alternatives. The evaluation results do not demonstrate a substantial improvement of RO-SGNS, with only a 1% difference observed on word similarity benchmarks, which falls within the range of variability attributable to hyperparameter tuning, as noted in previous research (Levy et al., 2015). Nonetheless, the established connection to Riemannian optimization is a valuable contribution, potentially providing insights into related methods in future studies.