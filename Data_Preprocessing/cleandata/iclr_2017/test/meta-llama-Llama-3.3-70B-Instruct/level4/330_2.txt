This manuscript introduces a framework for generating document representations by averaging word embeddings, incorporating a data-dependent regularization technique that prioritizes informative words while minimizing the impact of common words. 
The core concept involves representing documents as a mean of their word embeddings, with a regularization mechanism that favors rare or informative words and pushes common words towards zero. 
Experimental results on sentiment analysis and document classification tasks demonstrate that the proposed approach yields the lowest error rates when compared to existing baseline methods for document embedding.
Although the motivation behind exploring optimal document encoding methods is commendable, the paper lacks substantial technical novelty. 
Most techniques employed are not innovative, with the primary advantage of the proposed method being its simplicity and efficiency. 
To be convinced of the method's superiority in learning document representations, I would need to see robust performance across a broader range of tasks beyond the two presented. 
Regarding the RNN-LM, clarification is needed on whether the language model is trained to optimize classification error or as a traditional language model, and whether the final hidden state or the average of all hidden states is utilized as the document representation. 
A commonly used approach for document representation involves employing a bidirectional LSTM and concatenating the final hidden states. 
A comparison between the proposed method and this bidirectional LSTM approach, particularly for tasks like document classification and sentiment analysis, would provide valuable insights into its effectiveness.