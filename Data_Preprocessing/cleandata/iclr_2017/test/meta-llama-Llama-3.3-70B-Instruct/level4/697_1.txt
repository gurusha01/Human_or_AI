This paper explores the application of Grassmannian SGD for optimizing the skip gram negative sampling (SGNS) objective, aiming to improve word embeddings. However, it remains unclear what specific benefits the proposed optimization method offers over the traditional SGD-based approach, as neither method provides theoretical guarantees, and empirical comparisons only demonstrate slight improvements. Notably, the core concept of projector splitting algorithms has been previously applied to various machine learning problems, as seen in the works of Vandereycken on matrix completion and Sepulchre on matrix factorization.
A detailed analysis of the computational costs associated with both approaches is lacking. For example, the expense of performing SVD in equation (7) is not discussed. It is worth noting that an efficient low-rank update to the SVD can be achieved, with a rank-one update requiring O(nd) operations. Therefore, it would be beneficial to clarify the computational cost per iteration of the proposed method to provide a comprehensive understanding of its practical implications.