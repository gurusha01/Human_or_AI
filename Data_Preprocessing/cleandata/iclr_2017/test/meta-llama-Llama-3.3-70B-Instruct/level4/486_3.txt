This paper introduces graph convolutional networks, which are derived from an approximation of graph convolutions. The model's propagation step can be distilled into three key operations: a linear transformation of node representations, multiplication of the transformed representations with a normalized affinity matrix that includes self-connections, and application of a nonlinearity function.
The proposed model is applied to semi-supervised learning on graphs, yielding impressive results that surpass other baseline methods by a substantial margin. The evaluation of the propagation model is also noteworthy, as it assesses various model variants and design choices. The fact that such a straightforward model outperforms all baselines is surprising, particularly given that most experiments employ a two-layer model, which is inherently local and only considers interactions within a 2-hop neighborhood. The computational efficiency of the model (as discussed in section 6.3) raises the question of whether adding more layers would provide any benefits.
Although the model is motivated by graph convolutions, its simplified form, as described in the paper, involves relatively basic operations. In comparison to the methods proposed by Duvenaud et al. (2015) and Li et al. (2016), the presented GCN approach is simpler and performs fewer operations. It would be interesting to see a comparison between the proposed GCN and these existing methods.
In summary, the model's simplicity, interesting connection to graph convolutions, and strong experimental results are notable strengths. While some questions remain unanswered, I believe this paper is worthy of acceptance.