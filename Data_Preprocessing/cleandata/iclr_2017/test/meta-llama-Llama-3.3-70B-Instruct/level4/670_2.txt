This manuscript extends the work of Jonschkowski & Brock by proposing a method to learn state representations for multiple tasks simultaneously, rather than focusing on a single task. The concept of learning representations for multiple tasks is intriguing and relatively unexplored, making this research direction noteworthy. The approach outlined in the paper involves learning a distinct representation for each task and a corresponding policy, with the task being automatically detected and integrated into the neural network architecture.
The authors position their proposed method as orthogonal to multi-task learning, despite sharing the same ultimate goal of learning to solve multiple tasks. However, a more in-depth discussion on this point, as touched upon during the pre-review question phase, would be beneficial. Incorporating references to other relevant multi-task learning studies, such as policy distillation and actor-mimic (both presented at ICLR '16), could also enhance the paper.
The method suggests jointly learning a task classifier alongside a state representation learner, utilizing a differentiable gating mechanism to regulate the flow of information. A task coherence prior is introduced for this gating mechanism to ensure the learned task classifier exhibits temporal coherence. This structural addition enables the method to outperform the standard, non-multi-task approach.
The evaluation consists of two experimental scenarios. The first scenario involves controlling one of two cars navigating a track, where task detection is straightforward, and the learned state representation is linear in the observation. The paper presents sufficient comparisons to demonstrate the usefulness of the proposed approach over a standard non-multi-task setup.
In contrast, the second navigation scenario only qualitatively presents the state representation, without evaluating the resulting control policy or comparing it to other learned state representations. Since the effectiveness of the multi-task state representation learning approach hinges on its ability to also learn control better, it is essential to evaluate control performance with the same comparisons as in the first experiment. Without this evaluation, the experiment remains incomplete.
To align with the standards of publications at venues like ICLR, the method should undergo more thorough evaluation across a broader range of setups. This would demonstrate the approach's generality and its applicability to more complex tasks. Although the method is theoretically scalable, the experiments do not demonstrate its capability to handle more realistic scenarios, such as scaling beyond MNIST-level images to 3D or real images, or higher-dimensional control tasks. Evaluating the method in these more complex scenarios is crucial, as unforeseen issues may arise when attempting to scale. If scaling up is straightforward, then conducting and including these experiments in the paper should also be feasible.
In summary, the pros and cons of this paper are as follows:
Cons:
- The approach does not inherently share information across tasks for improved learning and necessitates learning a distinct policy for each task.
- Only one experimental setup evaluates the learned policy with multi-task state representation.
- There are no experiments on more realistic scenarios, such as 3D environments or high-dimensional control problems.
Pros:
- This approach allows the same network to be used for multiple tasks, a feature not commonly found in transfer and multi-task learning approaches.
- It introduces a novel method to learn a single policy for multiple tasks, including a task coherence prior that ensures meaningful task classification.
- The approach has been experimentally validated on two toy tasks, with one task showing improvement over baseline approaches.
My rating would be higher if the paper included an evaluation of the control policy for navigation and incorporated another more challenging and compelling scenario.
Additionally, some minor comments and questions for potential improvement, though less critical than the above points, include:
Approach:
Could this approach be integrated with other state representation learning methods, such as those utilizing autoencoders?
Experiments:
An additional useful comparison would be to evaluate performance in a single-task setting (e.g., controlling only the red car) as an upper bound on policy performance. Does the learned multi-task policy achieve the same level of performance? This upper bound would be tighter than the "known car position" baseline.
Would the "observations" baseline eventually reach the performance of the LRP approach? It would be useful to determine if this approach merely accelerates learning or enables superior performance.
If aliasing issues with images are present, consider using higher-resolution images to mitigate this problem.