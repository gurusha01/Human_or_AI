This paper presents two key contributions: 
(1) a novel model for predicting the next step, where both inputs and outputs are represented in the space of affine transforms between consecutive frames, and 
(2) an innovative evaluation methodology that assesses the quality of generated data by measuring the degradation in performance of a separate model, such as a classifier, when tested on this generated data.
The authors demonstrate that, according to this evaluation metric, their proposed model outperforms other baseline models, including the recent work by Mathieu et al. that utilizes adversarial training.
The paper's strengths include:
- Addressing a significant challenge in unsupervised video learning, namely evaluation.
- The results indicate that using Mean Squared Error (MSE) in transform space effectively mitigates the blurring issue, a primary objective of this paper.
- The generated data results in less performance degradation of the C3D model on the UCF-101 dataset compared to other baselines.
- The paper validates the assumption that videos can be approximated by a sequence of affine transforms starting from an initial frame over a considerable number of time steps.
However, the paper also has several weaknesses:
- The proposed evaluation metric is limited in its applicability, as it only considers the performance of a specific classifier on a given task. This narrow focus does not account for the broader goal of unsupervised learning, which aims to develop widely applicable representations across various tasks.
- The interaction between the generative models being compared and the idiosyncrasies of the chosen classifier could lead to unintended consequences, making it challenging to draw definitive conclusions about the relative merits of these models. Conducting experiments with multiple classifiers (e.g., C3D, dual-stream network, and other state-of-the-art methods) and demonstrating consistent rankings of generative models across different classifiers could help alleviate this issue.
- The use of only 4 or 8 input frames sampled at 25fps seems insufficient for capturing the context required to extrapolate complex motion patterns seen in UCF-101. The model's ability to generate non-trivial motion patterns would be more convincing if it could be shown to effectively predict and generate more complex motions.
- The motion prediction model lacks access to content information, relying solely on previous motion data. This limitation might hinder the model's ability to utilize cues like object boundaries or handle scenarios where motion fields collide, potentially leading to disadvantages in predicting realistic motions.
In terms of quality and clarity, the paper is well-written and easy to follow, with clearly specified and validated assumptions. The experimental details appear adequate.
The originality of the paper lies in its unique implementation of predicting motion for video generation and the proposed evaluation protocol, although the concept of generating videos by predicting motion has been explored in previous works.
The significance of this paper is notable, particularly if the evaluation method is extended to include multiple classifiers representative of different state-of-the-art approaches. Given the challenges in evaluating generative video models, this work could contribute to establishing a benchmark set for such evaluations.
Minor comments and suggestions include:
(1) Clarifying the distinction between inputs to the classifier (output of the next step prediction model) and inputs to the next step prediction model itself in the caption for Table 1.
(2) Including a histogram of affine transform parameters to better understand their typical range and potential outliers.
(3) Considering an alternative metric for comparing transforms, such as the closeness of A^{-1}B to the identity matrix.
(4) Refining the statement regarding the performance of the classifier on ground truth data as an upper bound for generative models, acknowledging the possibility, though unlikely, that a generative model could potentially enhance the data in ways that improve classifier performance.
Overall, this paper proposes a future prediction model in affine transform space, which reduces blurriness and generates relatively realistic videos, at least from the perspective of the C3D classifier. However, the paper could be improved by demonstrating the model's capability to predict more complex motion flows and by strengthening the experiments through the inclusion of additional classifiers beyond C3D.