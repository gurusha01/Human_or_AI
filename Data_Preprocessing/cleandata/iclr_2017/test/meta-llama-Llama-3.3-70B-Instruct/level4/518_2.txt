The authors introduce amortized SVGD, a novel extension of the existing SVGD method, which is a particle-based variational approach that minimizes KL divergence at each update step. This amortized version involves training a neural network to learn the underlying dynamics. The proposed method is then applied to train energy-based models with tractable unnormalized densities.
A key aspect of SVGD is the incorporation of a "repulsive force" that prevents degeneracy by promoting the distribution of probability mass beyond the mode, thereby introducing an entropy-like term. However, it is intriguing to see how this term remains effective in high-dimensional spaces. The authors' previous work, which focused on toy and UCI datasets, did not seem to encounter this issue.
The experimental results presented in this paper involve applying the kernel to the hidden representation of an autoencoder, a crucial step that resembles the approach taken by Li et al. (2015) for MMD. Nevertheless, unlike Li et al. (2015), the autoencoder is an integral part of the model and is not fixed, which undermines the authors' motivation and criticisms of prior work. By requiring the autoencoder to project onto a low-dimensional space, the majority of the effort is devoted to training the autoencoder, which changes with each iteration, before applying the proposed method.
In contrast to previous studies that utilized inference networks, the amortized SVGD approach appears to be slower than its non-amortized counterpart. This is due to the need to update xi before regressing to update eta, a process that incurs additional computational costs during training.
I suggest that the paper be rejected and that the authors provide more comprehensive experimental results, particularly with regards to the influence of the autoencoder, the comparison between incremental and full updates, and the training time of amortized versus non-amortized approaches. Although the current results show promise, they are unclear due to the numerous hyperparameters being adjusted by the authors.