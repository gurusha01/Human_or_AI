This innovative study introduces a novel approach to enhance existing reinforcement learning (RL) models by incorporating self-supervised tasks that foster improved internal representations. 
The authors propose two specific tasks, namely depth prediction and loop closure detection, which rely on a 3D environment and some positional information. These assumptions are well-suited for a wide range of navigation and robotics tasks.
Comprehensive experiments demonstrate that the inclusion of these auxiliary tasks leads to significant improvements in performance and learning speed. Furthermore, an in-depth analysis of value functions and internal representations reveals that the model discovers certain structural properties that would not be apparent without the auxiliary tasks.
Although this work is specifically tailored to 3D environment tasks, it provides additional evidence that leveraging input data beyond sparse external reward signals can substantially enhance learning speed and internal representation. The study is original, well-presented, and strongly supported by empirical evidence.
One minor limitation of the experimental methodology is that selecting the top-5 runs makes it challenging to determine whether the model's superiority is due to the chosen hyperparameter range or its robustness to these settings. An analysis of performance as a function of hyperparameters could help confirm the approach's superiority over the baselines. It is likely that incorporating auxiliary tasks enhances the model's robustness to suboptimal hyperparameters.
Another limitation is the authors' dismissal of navigation literature as unrelated to RL, which may be an oversimplification. While acknowledging the constraints of paper length, a comparative analysis with this literature could have provided valuable insights into the quality of the learned representations, and would have strengthened the study's conclusions.