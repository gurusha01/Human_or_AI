I appreciate the authors' demonstration that incorporating auxiliary tasks into the learning process not only avoids interfering with the primary RL tasks but actually enhances their performance. This outcome is somewhat expected given the capabilities of deep networks, which can learn a robust representation of the environment that serves as a foundation for achieving specific objectives. Although the initial representations are inevitably influenced by the task's performance, it is evident that certain fundamental stages, such as edge detection in sensory representations, are shared across tasks. Consequently, supplementing the training with additional tasks effectively increases the training dataset's size. However, it remains challenging to account for this effect to ensure a fair comparison, and the paper could benefit from further analysis, such as exploring how the representation changes with and without auxiliary training.
Nevertheless, I strongly disagree with the authors' implicit definition of supervised and self-supervised learning. The essence of unsupervised learning lies in the absence of external labels, regardless of whether these labels are provided by humans or generated through other means, such as expensive machinery used to train a network for later autonomous task execution. In my view, Expectation-Maximization (EM) is a self-supervised approach, where the model itself predicts labels that are then used to iteratively refine parameter learning. In contrast, the use of externally provided labels in this work constitutes a supervised learning task, which diverges from the authors' implied categorization.