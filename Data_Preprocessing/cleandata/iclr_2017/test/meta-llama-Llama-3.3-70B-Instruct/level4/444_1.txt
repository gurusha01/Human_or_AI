EDIT: The revisions made to this manuscript are comprehensive and effectively address many of my initial concerns, resulting in a more readable paper. Consequently, I strongly recommend the revised version for acceptance and have accordingly increased my evaluation score.
This manuscript introduces a novel approach to interpreting Long Short-Term Memory (LSTM) models, which are notoriously difficult to interpret. Specifically, the authors propose a method to decompose the predictions made by an LSTM for a question-answering (QA) task into importance scores assigned to individual words. These scores are then utilized to generate patterns, which, through a simple matching algorithm, can identify answers. Notably, when applied to the WikiMovies dataset, the pattern matching method extracted via this approach achieves accuracy levels comparable to those of a standard LSTM, thereby demonstrating the efficacy of the proposed methodology.
The motivation underlying this work is compelling, as the interpretation of LSTMs remains an ongoing challenge. The high performance achieved by the pattern matching approach is particularly noteworthy and somewhat surprising. However, certain aspects of the pattern extraction process lack clarity, and the evaluation is limited to a specific task where predictions are generated at the level of individual words. Therefore, in its current form, I recommend this manuscript as a weak accept, with the hope that the authors will provide further clarification on their methodology, as I believe it holds potential utility for researchers in the field of Natural Language Processing (NLP).
Comments:
- It would be beneficial to provide a more detailed introduction to the specific QA tasks to which the models are applied before Section 3.3, as the nature of the task (i.e., identifying an entity within the document as the answer) is not clearly established at that point.
- In Section 3.3, could you clarify whether the softmax function is predicting a binary value (0/1), indicating whether a given word is the answer or not?
- Also in Section 3.3, could you elaborate on what the P and Q vectors represent? Are they simply transformations of the hidden state into a two-dimensional vector space for the purpose of binary prediction?
- How does the performance of the pattern matching method vary with different values of the cutoff constant?
- In Section 5.2, are there any questions for which the answers are not entities?
- Could you discuss potential extensions of the proposed approach to scenarios where predictions are not made at every word, such as sentence-level sentiment classification?