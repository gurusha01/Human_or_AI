This paper presents an examination of various innovations in deep reinforcement learning (RL) and their impact on game solving in the Atari domain. While the paper is well-structured and clearly written, it comes across as a collection of the researcher's recent techniques, lacking a cohesive narrative. The work is likely to resonate with researchers currently implementing deep RL methods, but may not garner significant attention from the broader scientific community.
The introductory claims regarding the approach's stability and sample efficiency prompted expectations of theoretical analysis supporting these properties. However, it becomes apparent that these are empirical claims, and clarifying this in the abstract would be beneficial.
The proposed innovations are grounded in established methods, and it is noteworthy that the same approach demonstrates effectiveness in both discrete and continuous domains. The empirical results presented are reasonably comprehensive, although the inclusion of confidence intervals on additional plots would enhance the analysis. Furthermore, the results do not provide a clear distinction between the effects of each innovation, making it challenging to understand the individual contributions and intuition behind the performance differences, such as why ACER outperforms A3C. Additionally, the discrepancy between matching results on discrete tasks and state-of-the-art performance on continuous tasks is not adequately explained.
The paper provides a thorough review of related literature, and it is commendable that this work highlights Retrace, including its theoretical characterization in Section 7, thereby drawing attention to this significant contribution.