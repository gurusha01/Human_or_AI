This paper presents a methodologically intriguing approach, and its methodological contributions alone would warrant acceptance. However, the authors' claims of surpassing existing baselines for the Traveling Salesman Problem (TSP) are not substantiated, as the local search method LK-H can solve all the authors' instances to optimality in a matter of seconds on a CPU, whereas the authors' method yields suboptimal results in 25 hours on a GPU.
The dominance of LK-H raises concerns about the authors' decision to retain Figure 1 in its original form, which compares their method to a poorly implemented local search by Google, rather than the more effective LK-H method. This omission may mislead readers and perpetuate misconceptions, as evidenced by its use in a talk at NIPS to claim that RNNs outperform local search. The authors should have revised the figure to reflect the true strength of local search, acknowledging that their method is inferior.
Despite the paper's interesting methodology, I initially voted for rejection due to the misleading claims and figures. However, after the rebuttal and revisions, I am torn. The paper is well-written, and the method shows promise, but the authors' reliance on weak baselines and reluctance to acknowledge the superiority of state-of-the-art methods, such as LK-H, are concerning. The statement that greedy approaches are "just a few percent worse than optimality" is misleading, as achieving near-optimality is relatively trivial, and the true challenge lies in pushing those last few percent.
The revised paper has toned down some of the claims, but the section on Knapsack solving raises new concerns. The authors have removed discussion of strong baseline methods, such as ExpKnap and MinKnap, which yielded optimal solutions, and instead introduced weaker baselines, such as random search and greedy. This revision creates a misleading impression that the authors' method is superior to all baselines. I find it irresponsible to hide the better baselines and suggest that the authors should have used more robust methods, such as the OR-tools solver from Google.
I appreciate the authors' commitment to making their model and training code available, but I would like to know when this will happen. Additionally, I have questions about the missing values in Table 3 and the performance of RL pretraining with 10,000 batches for Sampling T=1 and T=T. The difference in performance between RL pretraining Sampling T=1 and T=T is not clear, and I would like to see the numbers for 10,000 batches to better understand the results.