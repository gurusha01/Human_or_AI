This manuscript proposes a similarity encoder leveraging a conventional feed-forward neural network to produce similarity-preserving embeddings, which is then applied to extend the CBOW word2vec model by transforming the learned embeddings using their average context vectors. The approach is evaluated through experiments on analogy tasks and named entity recognition.
Although the paper presents some plausible intuitive reasoning for the capability of a feed-forward neural network to generate effective similarity-preserving embeddings, the underlying architecture and methodology lack novelty. Upon examination, the model appears to be a basic neural network trained using stochastic gradient descent on similarity signals, which is a well-established approach.
The concept of utilizing context embeddings to enhance the expressive capacity of learned word representations is somewhat more innovative. While incorporating explicit contextual information is not a new idea, particularly for tasks such as word sense disambiguation (as seen in works like 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al., which deserves citation), the specific implementation presented here is original to my knowledge.
However, the evaluation of the proposed method is unconvincing. The training corpora for the embeddings are significantly smaller than those typically used in practice for unsupervised or semi-supervised embedding learning. The results on the analogy task provide limited insight into the method's benefits for larger corpora. As the authors acknowledge, the expected gain may be less substantial when using larger datasets, since the global context statistics introduced by the ConEc can also be captured by word2vec with extended training.
It can be argued that extrinsic evaluations are more crucial for real-world applications, making the experiments on NER a positive aspect. Nevertheless, the embeddings were trained on a very small corpus, leaving me unconvinced that the observed benefits will persist when trained on larger datasets.
In conclusion, I believe this paper contributes limited novelty and provides weak experimental evidence to support its claims. Therefore, I cannot recommend it for acceptance.