This manuscript explores the application of eligibility traces to recurrent DQN agents, leveraging the forward view introduced by Sutton and Barto to facilitate their use with neural networks. Experimental results on the Atari games Pong and Tennis demonstrate the superiority of traces over standard Q-learning.
The paper is well-structured and the investigation of traces in deep RL is a notable contribution, given the limited existing research in this area. However, the experimental scope is restricted and fails to address the most compelling questions. 
As highlighted in previous inquiries, n-step returns have been consistently shown to outperform 1-step returns in both classical RL literature and recent deep network studies. A prior study [1] demonstrated that incorporating n-step returns into the forward view with neural networks yields significant improvements in both Atari and TORCS environments. Notably, their n-step Q-learning approach combines returns of varying lengths in expectation, whereas traces achieve this explicitly. This paper does not provide a comparative analysis between traces and n-step returns, merely illustrating the benefits of traces in the forward view on two Atari games. This finding, although positive, is not particularly substantial. A more intriguing investigation would be to determine whether traces can enhance existing methods known to perform well with neural networks.
The paper also claims to demonstrate the profound impact of optimization. However, based on experiments conducted on two games with fixed hyperparameter settings, it is challenging to draw conclusive insights. Similar experiments have been conducted in other studies with more comprehensive analyses. It can be argued that these experiments underscore the importance of hyperparameter values rather than the optimization algorithm itself. Without optimizing the hyperparameters, it is difficult to make assertions about the relative merits of the methods.
[1] "Asynchronous Methods for Deep Reinforcement Learning", ICML 2016.