This paper presents a novel approach to training learning rate controllers using an actor-critic reinforcement learning (RL) algorithm for supervised learning tasks. The results demonstrate that the proposed method surpasses traditional optimizers, including SGD, ADAM, and RMSprop, in experiments on the MNIST and CIFAR 10 datasets.
However, two primary issues warrant attention. Firstly, the absence of comparisons with recently proposed similar methods, such as "Learning Step Size Controllers for Robust Neural Network Training" by Daniel et al. and "Learning to learn by gradient descent by gradient descent" by Andrychowicz et al., is notable. The work by Daniel et al. bears significant resemblance to the proposed method, as it also employs a policy search RL approach (REPS), and it is unclear what the drawbacks of their method are. Although their approach utilizes more prior knowledge, as acknowledged by the authors, it is not evident why this is a disadvantage.
Secondly, the experimental results raise concerns. Some of the reported numbers for competing methods are unexpectedly low, such as the poor performance of RMSprop in Table 2 and Table 3. These findings suggest that the methods may not have been properly tuned, highlighting the need for comparisons on standard architectures with previously reported results. For instance, using established architectures like ResNet or Network in Network as baselines could provide a more comprehensive evaluation.