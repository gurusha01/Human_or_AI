This manuscript proposes a deep reinforcement learning approach incorporating eligibility traces, wherein the authors integrate DRQN with eligibility traces to enhance training efficacy. The novel algorithm is assessed on two distinct problems utilizing a unified set of hyper-parameters and benchmarked against DQN.
The subject matter is highly intriguing, as although the concept of eligibility traces in RL updates is not new, its application to deep RL remains relatively unexplored. The paper is well-structured and clearly written, with a thorough review of relevant literature. However, to significantly strengthen this promising work, additional experiments are necessary. Given the paper's experimental nature, it is essential to include a broader range of problems, varied hyper-parameter settings, and comparative analyses with vanilla DRQN, Deepmind's DQN implementation, and other state-of-the-art methodologies to provide a more comprehensive evaluation.