This paper presents a novel approach to extracting rule-based classifiers from trained LSTM models, which is then applied to a factoid question-answering task. The results show that the extracted rules achieve comparable performance to the original LSTM, providing insight into the features that the LSTM model captures. 
Understanding the computations performed by RNNs to comprehend their functionality is a crucial research direction, as it can help identify their limitations and areas for improvement. Although the proposed method has a relatively inflexible approach, defining each rule as an ordered sequence of words, the authors' experimentation with three different scores for selecting salient words (state-difference, cell-difference, and gradient) yields comparable performance. This suggests that the extracted rules closely mimic the RNN. Notably, most of the extracted rules consist of only two or three words, which is a somewhat surprising outcome.
Extending this approach to other natural language processing tasks, such as machine translation, would be an interesting direction for future research, as the rules learned in this context are likely to be distinct.
Other comments:
- Equation (12) can be simplified by dividing both the numerator and denominator by $e^{P h_t}$, which reveals that it is over-parametrized with two vectors $P$ and $Q$, and can be computed using a single vector.
- In Section 4.1, it would be helpful to clarify whether the focus is on the forward LSTM.
- Equation (13) should define $c_0 = 0$ for clarity.
- Equations (13) and (15) appear to be identical, suggesting a potential mistake.
- In Table 1, the word "film" should be highlighted in the third column.
- The text should be revised to read "are shown in Table 2" instead of "are shown in 2".
- To address issues with number representation, replacing each digit with the hashtag symbol may be a useful solution.