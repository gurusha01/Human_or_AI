This manuscript presents a novel perspective on dropout as a latent variable model, enabling the authors to investigate the "inference gap" that arises between the network's output during training, where dropout is applied to each sample, and testing, where expected dropout values are used to scale node outputs. The concept of expectation linearity is introduced, and under certain mild assumptions, bounds on the inference gap are derived. Additionally, the authors propose utilizing a per-sample-based inference gap as a regularizer and provide an analysis of the accuracy of models with expectation-linearization constraints compared to those without.
One potential limitation of the latent variable model view of dropout is its apparent restriction to probabilistic models, whereas dropout can be more broadly applied to deep networks. However, it is anticipated that the regularizer formulation of dropout would remain effective even in non-probabilistic models.
On page 8, the term "MC dropout" is not explicitly defined and should be clarified. Furthermore, on page 9, it is stated that the proposed regularizer enables standard dropout networks to outperform those using Monte Carlo dropout, but this appears to be specific to the MNIST dataset and not observed on the CIFAR dataset.
The results presented in Tables 1 and 2 suggest that Monte Carlo dropout achieves the best performance across tasks and methods, albeit at a higher computational cost. Providing commentary on the computational efficiency of various dropout procedures, in conjunction with the accuracy results, would be highly valuable.
There are a few minor typographical errors: on page 2, "x is he input" should be corrected to "x is the input," and on page 5, the reference to equation (1) is incorrect in two places within the paragraph.
Overall, this is a well-written paper that contributes meaningfully to the field, and it is recommended that it be accepted and discussed at the conference.