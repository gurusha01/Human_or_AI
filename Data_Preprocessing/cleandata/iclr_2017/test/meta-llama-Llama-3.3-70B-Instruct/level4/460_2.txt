This paper presents a novel actor-critic deep reinforcement learning (RL) approach that incorporates experience replay, combining truncated importance sampling and trust region policy optimization. Additionally, it introduces stochastic duelling networks, a new method for estimating the critic in continuous action spaces. The proposed approach is evaluated on Atari games and continuous control problems, demonstrating performance comparable to state-of-the-art methods.
The primary contributions of this work, as outlined in the introduction, include the integration of 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks. These enhancements yield positive results and may have a beneficial impact on future RL research.
However, it appears that each of these improvements is relatively incremental. Furthermore, the ACER framework seems more complex and fragile to implement compared to standard deep Q-learning with prioritized replay, which achieves similar performance on Atari games. For the Atari domain, prioritized replay may be preferred due to its simplicity. Moreover, while improving sample efficiency in deep RL is a worthwhile goal, it would be more impactful to pursue this objective in problem settings where sample efficiency is crucial. Unfortunately, the paper only assesses sample efficiency in the Atari and continuous control tasks domains, where sample efficiency is not a primary concern. As a result, it is unclear whether the proposed ACER method will generalize to problems where sample efficiency is essential.
Several technical aspects require clarification:
- Regarding Retrace, it is assumed that $Q^{ret}$ is computed recursively from the end of each trajectory. A comment on this would be appreciated.
- The derivation of equation (7) is unclear, and it is uncertain whether an approximation sign is missing.
- In Section 3.1, the paper argues that $Q^{ret}$ provides a lower-variance estimate of the action-value function. It is unclear why this is not used in equation (8) for the bias correction term.
- The paper states that a replay memory of 50,000 frames is used, which is smaller than the experience replay transitions used in previous work, such as the paper "Prioritized Experience Replay" by Schaul et al. This discrepancy may significantly impact the performance of both ACER and competing models. To properly evaluate the improvements of ACER, the authors should also conduct experiments with larger experience replay memories.
Additional comments:
- It is suggested that Section 7 be moved to the appendix.
- The sentence "Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability" appears to contain an error, as it is likely that large values of lambda are intended.
- Above equation (6), it should be mentioned that the squared error is used.
- Equation (9) appears to be missing a "t" subscript.
- The explanation of stochastic duelling networks is unclear and could be rephrased for better understanding.
- The sentence "To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games" requires clarification.
- Figure 2 (Bottom) should include labels on the vertical axes.