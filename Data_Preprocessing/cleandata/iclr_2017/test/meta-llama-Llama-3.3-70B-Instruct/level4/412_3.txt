This paper presents a novel approach to optimizing the objective function of Convolutional Neural Networks (CNNs) by employing a layer-wise optimization strategy. Specifically, the method optimizes the parameters of one layer at a time while keeping the parameters of other layers fixed. A key contribution of this work is the formulation of the optimization problem at each layer as the optimization of a piecewise linear (PL) function, which is analogous to the optimization problem encountered in latent structural Support Vector Machines (SVMs). This connection enables the paper to leverage ideas from the latent structural SVM literature, particularly the concave-convex procedure, to learn CNN parameters.
The paper is well-written and explores an interesting connection between CNNs and latent structural SVMs, which could potentially bridge the gap between these two research communities. However, the proposed method has some limitations. Firstly, it relies on layer-wise optimization, which is essentially a coordinate descent algorithm that may not be competitive with other optimization strategies for learning CNNs. While this approach guarantees improvement at each step, it is unclear how the potential loss in optimization quality balances with the gain in guaranteed improvement. Secondly, the paper focuses on optimizing the CNN objective, but it is well-known that a better objective function does not necessarily translate to a good model, particularly in the presence of overfitting. The standard stochastic gradient descent (SGD) with backpropagation may not always improve the objective, but this could be beneficial in preventing overfitting, as the ultimate goal of learning is to achieve good generalization performance, not solely to optimize the objective function.
The experimental evaluation is somewhat limited. The paper only uses the CIFAR10 dataset, which is relatively small by current standards, and it is unclear whether the results would generalize to larger datasets like ImageNet. Furthermore, the comparison to SGD is limited to a crippled variant without batch normalization, dropout, or other regularization techniques. While the paper aims to focus on optimization, the comparison to SGD in terms of optimization alone may not be meaningful, as SGD is designed to balance optimization with regularization to prevent overfitting.