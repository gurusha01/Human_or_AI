The authors' responses to the pre-review questions are underwhelming, particularly regarding dataset density and the rationale behind subsampling. Their justification, which relies on the common practice of subsampling in recommender systems research, falls short of addressing the crucial question of how the results would be affected without subsampling. This omission is notable, given the paper's emphasis on tackling the cold-start issue, which seems at odds with the decision to reduce data sparsity through subsampling.
Apart from this, the authors have adequately addressed the pre-review questions. The paper's primary contribution lies in proposing novel user and item embedding methods to capture complex, non-linear interactions between users and items. Although this approach bears similarities to recent deep recommender systems research, the network formulation exhibits distinct differences.
Overall, the paper is reasonably well-structured and contributes to an important area of research, albeit with some notable shortcomings. These include:
1) An unconventional evaluation methodology, as evidenced by the sole reporting of Recall@M, which is not typically used in recommender systems research. To ensure completeness, additional performance metrics, such as RMSE or AUC, should be reported, even if the results are not particularly strong.
2) Given the relative simplicity of the contribution, which involves a standard recommender systems task with a new model, it is disappointing that non-standard data samples are used. Ideally, the results should be compared to those of competing methods using identical data, error measures, and evaluation protocols to facilitate a fair and direct comparison.
Without addressing these concerns, it is challenging to determine whether the reported performance improvements are genuinely attributable to the proposed method or are instead influenced by the choice of datasets and loss functions.