This manuscript presents a novel hierarchical generative model, wherein the lower level focuses on individual data points within datasets, while the higher level captures unordered sets of datasets. The core concept revolves around utilizing a "double" variational bound, where a higher-level latent variable characterizes datasets, and a lower-level latent variable describes specific examples within those datasets.
The application of hierarchical modeling is a significant and impactful area of research, yet it remains relatively underinvestigated in the deep learning community.
The strengths of this paper include:
- The few-shot learning outcomes appear promising, although a deeper expertise in this specific area would be necessary for a thorough evaluation.
- The introduction of a "double" variational bound in the context of a hierarchical generative model is clearly articulated and demonstrates broad applicability.
Several questions arise from the presentation:
- During the training of the statistic network, are minibatches (subsets of examples) employed?
- If not, would utilizing minibatches provide an unbiased estimation of the full gradient if all examples were considered? For instance, if the statistic network aims to identify a feature present in any example within the dataset and use this as a characterization, this seems aligned with the graphical model depicted on the right side of figure 1. However, training the statistic network on minibatches might limit its ability to learn such characterizations, as any given minibatch may not include all examples from the dataset. This approach could potentially restrict the model's expressive power.
Potential avenues for further exploration include:
- Applying this type of model to hierarchical forecasting, such as predicting electricity demand or sales, which could offer a practical and intriguing use case.