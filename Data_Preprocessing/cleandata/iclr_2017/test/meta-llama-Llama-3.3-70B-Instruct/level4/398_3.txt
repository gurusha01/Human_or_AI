This paper presents a intriguing concept: mitigating chaotic behavior in Recurrent Neural Networks (RNNs). Unlike many other papers on novel RNN architectures that focus primarily on performance enhancements while neglecting in-depth analysis, this paper provides a thorough explanation of why its proposed method may be effective.
The paper offers numerous comparisons between chaotic systems (GRUs and LSTMs) and the stable system (proposed CFN model), yet the reviewer remains unconvinced by the paper's central claim. This claim suggests that chaotic behavior enables dynamic systems to possess rich representation power but renders them unstable. Although the LSTM exhibits sensitive behavior even with minimal noise added to the input, it still performs remarkably well despite this chaotic behavior.
Evaluating model complexity is a challenging task, and many papers address this by using the same number of hidden units or similar model sizes. This paper's experiments employed the same number of parameters for both the LSTM and CFN, but it is possible that the CFN has a simpler computational graph. Building on this idea, can a stable dynamic system be developed that has multiple attractors, rather than just one?
The paper's finding that CFN layers are updated at different timescales, with decaying speed decreasing as the layer gets higher, is noteworthy. Additional statistics on this phenomenon would be valuable, such as the average relaxation time of the entire hidden units at each layer.
Techniques like batch normalization and layer normalization can improve the stability of RNN training. It would be interesting to see how the behavior of batch-normalized or layer-normalized LSTMs compares to the proposed CFN. Moreover, integrating batch normalization or layer normalization into new architectures can be non-trivial, making a comparison between normalized versions of LSTMs and CFNs potentially useful.
The overall quality of the work is good, with clear explanations, nice analyses, and proofs. Although the performance is not superior to LSTMs, the simplicity of the model makes it an interesting contribution. However, there are concerns about the model's potential performance on more challenging tasks, such as translation. Figure 4, which shows that the hidden units at the second layer tend to retain information longer than those at the first layer, is particularly interesting and warrants further exploration.