This paper presents a "batch" approach for reinforcement learning (RL) setups aimed at enhancing chat-bots, providing a comprehensive overview of the utilized RL setup and introducing an algorithm that bears resemblance to previously published online setups for the same problem. The authors conduct a comparison with the online version and delve into various modeling choices. 
The writing is found to be clear, with the algorithm representing a logical extension of the online version. 
Some constructive suggestions for improvement are as follows:
- The comparison between constant and per-state value functions yields differing results in artificial and real-life tasks, with no difference observed in the former and a notable difference in the latter. Elucidating the reason behind this discrepancy and incorporating it into the discussion would be beneficial. One potential explanation could be:
- In the artificial task, the constant value function may have been given an unfair advantage, as it is capable of updating all model weights, whereas the per-state value function is limited to updating only the top layer.
- In Section 2.2, 
   the sentence preceding the last one lacks a definition for 's'.
   the final sentence requires the addition of "... in the stochastic case" at its conclusion.
- In Section 4.1's last paragraph, the statement "While Bot-1 is not significant ..." would be more accurately phrased as "While Bot-1 is not significantly different from ML ...".