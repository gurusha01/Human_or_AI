This paper proposes a novel approach to training word embeddings using the Skip-Gram Negative Sampling (SGNS) model, which is a popular method for learning word representations. The authors reformulate the SGNS optimization problem as a two-step procedure, where the first step involves searching for a low-rank matrix that maximizes the SGNS objective, and the second step involves recovering the word embeddings from this low-rank matrix.
The paper claims to contribute to the field of natural language processing by introducing a new algorithm based on Riemannian optimization framework that optimizes the SGNS objective over low-rank matrices directly. The authors demonstrate the superiority of their approach by comparing it to state-of-the-art competitors, such as the original SGNS optimization method and the SVD-over-SPPMI method.
2. I decide to accept this paper, with two key reasons for this choice: 
Firstly, the paper tackles a specific and well-defined problem in the field of natural language processing, namely the optimization of the SGNS model for word embeddings. 
Secondly, the approach proposed by the authors is well-motivated and grounded in the literature, and the experimental results demonstrate the effectiveness of the proposed method.
3. The paper provides a clear and well-structured presentation of the proposed approach, including a detailed description of the Riemannian optimization framework and the projector-splitting algorithm used to optimize the SGNS objective. The authors also provide a thorough analysis of the experimental results, including a comparison to state-of-the-art competitors and an evaluation of the proposed method on several benchmark datasets.
The paper supports its claims through a combination of theoretical analysis and empirical evaluation. The authors provide a detailed derivation of the SGNS optimization problem and its reformulation as a two-step procedure, and they demonstrate the effectiveness of the proposed approach through a series of experiments on benchmark datasets.
4. To further improve the paper, I would suggest that the authors provide more details on the implementation of the Riemannian optimization framework and the projector-splitting algorithm, including any hyperparameter tuning or optimization techniques used to improve the performance of the proposed method. Additionally, the authors could provide more analysis on the computational complexity of the proposed approach and its scalability to large-scale datasets.
Some questions I would like the authors to answer to clarify my understanding of the paper include: 
* Can the authors provide more details on the choice of hyperparameters, such as the dimensionality of the word embeddings and the number of iterations used in the optimization procedure?
* How does the proposed approach handle out-of-vocabulary words or words that are not seen during training?
* Can the authors provide more analysis on the interpretability of the word embeddings learned using the proposed approach, including any visualizations or qualitative evaluations of the learned representations?