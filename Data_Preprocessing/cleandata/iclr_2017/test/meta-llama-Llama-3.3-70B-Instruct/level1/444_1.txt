Summary
This paper presents a novel approach for visualizing the importance of specific inputs in determining the output of a Long Short Term Memory (LSTM) network. The authors propose a method for decomposing the output of an LSTM into a product of factors, where each term can be interpreted as the contribution of a particular word. They then use these importance scores to extract phrases from a trained LSTM, which are validated by using them as input to a simple, rules-based classifier. The approach is demonstrated on both sentiment analysis and question answering tasks, with promising results.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and important problem in the field of natural language processing, namely the interpretability of LSTM models. Secondly, the approach presented in the paper is well-motivated and supported by a thorough analysis of the related work.
Supporting Arguments
The paper provides a clear and concise overview of the related work, highlighting the limitations of existing approaches for visualizing LSTM models. The authors then present a novel approach for decomposing the output of an LSTM, which is supported by a thorough mathematical analysis. The experimental results demonstrate the effectiveness of the approach, with the extracted phrases providing a good approximation of the LSTM's output. The paper also provides a detailed comparison with other approaches, highlighting the advantages of the proposed method.
Additional Feedback
To further improve the paper, I would suggest providing more examples of the extracted phrases, as well as a more detailed analysis of the errors made by the rules-based classifier. Additionally, it would be interesting to see a comparison with other interpretability methods, such as attention mechanisms or saliency maps. Finally, the authors could consider providing more details on the computational cost of the approach, as well as its potential applications in real-world scenarios.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
* Can you provide more details on the computational cost of the approach, and how it compares to other interpretability methods?
* How do you plan to extend the approach to more complex models, such as transformers or graph neural networks?
* Can you provide more examples of the extracted phrases, and how they relate to the specific task or dataset?
* How do you think the approach could be used in real-world scenarios, such as text classification or question answering?