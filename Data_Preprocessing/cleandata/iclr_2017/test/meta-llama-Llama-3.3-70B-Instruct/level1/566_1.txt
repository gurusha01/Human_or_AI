Summary
The paper proposes a scalable batch active learning framework for deep neural networks, particularly Convolutional Neural Networks (CNNs). The approach relies on a variational approximation to perform Bayesian inference, leveraging statistical knowledge on the Maximum Likelihood Estimator (MLE) to deduce posterior and prior distributions of the weights. The authors introduce a greedy active selection scheme based on a criterion derived from the variational free energy, which is approximated using a Kronecker-factored approximation of the Fisher information. The method is evaluated on MNIST and USPS datasets, demonstrating improved test accuracy compared to random sampling and scalability with increasing query size.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a significant problem in deep learning, namely, scaling up active learning to deep networks, and (2) the proposed approach is well-motivated, leveraging established concepts in Bayesian inference and variational methods.
Supporting Arguments
The paper provides a clear and well-structured presentation of the proposed approach, including a thorough review of related work and a detailed explanation of the theoretical foundations. The experimental evaluation demonstrates the effectiveness of the method, with notable improvements in test accuracy compared to baseline approaches. The use of a greedy selection scheme and approximations to the Fisher information matrix enables scalability, making the approach suitable for large datasets.
Additional Feedback
To further improve the paper, I suggest the authors consider the following points:
* Provide more detailed analysis of the approximations used, particularly the Kronecker-factored approximation of the Fisher information, and discuss potential limitations.
* Investigate the robustness of the approach to different types of noise and outliers in the data.
* Consider extensions to other deep learning architectures, such as recurrent neural networks or transformers.
* Provide more insight into the computational efficiency of the approach, including a detailed analysis of the time complexity.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to address the following questions:
* Can you provide more intuition on why the variational free energy is a suitable criterion for active learning in deep networks?
* How do you plan to address the potential instability of the asymptotic distribution used to approximate the posterior, particularly in the context of active learning with small subsets of observed data?
* Can you discuss potential connections between your approach and other MLE-based active learning criteria, such as the one proposed in Zhang & Oles (2000)?