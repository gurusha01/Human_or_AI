Summary
The paper proposes a novel approach to next frame prediction in video sequences by predicting transformations between frames instead of directly predicting pixel values. This approach leads to sharper results and uses a smaller prediction model. The authors also introduce a new evaluation protocol that feeds generated frames to a classifier trained on ground truth sequences to measure the preservation of discriminative features. The proposed method outperforms more sophisticated models on the UCF-101 dataset while being more efficient in terms of parameters and computational cost.
Decision
I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper supports its claims with correct and scientifically rigorous results.
Supporting Arguments
The paper tackles a specific question/problem in next frame prediction by introducing a transformation-based model that operates in the space of affine transforms. The approach is well-motivated, as it addresses the limitations of existing methods that directly predict pixel values. The authors provide a clear and concise explanation of their model, including the affine transform extractor, predictor, and multi-step prediction. The experimental results demonstrate the effectiveness of the proposed method, including qualitative and quantitative comparisons to state-of-the-art models.
Additional Feedback
To further improve the paper, I suggest the authors provide more analysis on the limitations of their approach, such as the underestimation of transformations due to the use of MSE as a criterion. Additionally, the authors could explore the use of multi-scale architectures and recurrent units to improve the prediction accuracy of their model. It would also be interesting to see an extension of the proposed approach to factor out the "what" from the "where", appearance from motion.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the implementation of the affine transform extractor, including the choice of patch size and stride?
2. How do you plan to address the limitation of underestimation of transformations due to the use of MSE as a criterion?
3. Have you considered exploring other evaluation protocols, such as using multiple classifiers or evaluating the generated frames on other tasks, such as object detection or segmentation?