Summary
The paper presents a novel approach for learning state representations in multi-task reinforcement learning, called Multi-Task Learning with Robotic Priors (MT-LRP). MT-LRP learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed or the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. The authors demonstrate the effectiveness of MT-LRP in simulated experiments, showing that it can learn better state representations for reinforcement learning and analyze why and when it manages to do so.
Decision
I decide to Accept this paper, with two key reasons for this choice: (1) the approach is well-motivated and placed in the literature, and (2) the paper supports its claims with thorough experiments and analysis.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of multi-task state representation learning in reinforcement learning, and motivates the need for a new approach. The authors provide a thorough review of related work, highlighting the limitations of existing methods and the contributions of their approach. The experimental evaluation is thorough and well-designed, demonstrating the effectiveness of MT-LRP in various scenarios. The analysis of the results provides valuable insights into why and when MT-LRP works well, and highlights potential avenues for future work.
Additional Feedback
To further improve the paper, I suggest that the authors consider providing more details on the implementation of the gated neural network architecture, such as the specific activation functions used and the initialization of the weights. Additionally, it would be interesting to see more experiments on the robustness of MT-LRP to different types of distractions and noise in the observations. Finally, the authors may want to consider discussing potential applications of MT-LRP to real-world problems, such as robotics or autonomous driving.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions: (1) Can you provide more details on how the task detector Ï‡ is trained and how it is used to switch between tasks? (2) How do you handle the case where the number of tasks is unknown or changing over time? (3) Can you provide more insights into the role of the task-separation prior and how it affects the learning performance of MT-LRP?