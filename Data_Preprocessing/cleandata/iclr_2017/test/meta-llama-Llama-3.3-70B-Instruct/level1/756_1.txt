Summary
The paper introduces a novel neural network architecture called similarity encoders (SimEcs) that learn similarity-preserving representations by mapping data into an embedding space where original similarities can be approximated linearly. The authors also propose context encoders (ConEcs), a variation of SimEcs, to enhance word embeddings with local context information and global word statistics. The paper demonstrates the effectiveness of SimEcs and ConEcs in various tasks, including dimensionality reduction, word embedding, and named entity recognition.
Decision
I decide to Accept this paper with minor revisions. The paper tackles a specific and important problem in the field of machine learning, and the approach is well-motivated and well-placed in the literature. The authors provide a clear and concise explanation of the proposed method, and the experimental results demonstrate the effectiveness of SimEcs and ConEcs.
Supporting Arguments
The paper addresses the limitations of traditional dimensionality reduction methods, such as kernel PCA, and provides a novel solution that can handle large datasets and unknown similarity functions. The authors also provide a clear connection between SimEcs and word2vec, and demonstrate the advantages of ConEcs in creating word embeddings for out-of-vocabulary words and distinguishing between multiple meanings of words. The experimental results are thorough and well-presented, and the authors provide a clear discussion of the results and their implications.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process for SimEcs and ConEcs. Additionally, it would be helpful to include more comparisons with other state-of-the-art methods in the field, such as t-SNE and autoencoders. Finally, the authors may want to consider providing more theoretical analysis of the proposed method, such as the convergence properties of SimEcs and the relationship between SimEcs and other dimensionality reduction methods.
Questions for the Authors
1. Can you provide more details on the hyperparameter tuning process for SimEcs and ConEcs, and how the authors selected the optimal hyperparameters for each experiment?
2. How do the authors plan to address the issue of determining the optimal architecture for SimEcs and ConEcs, and what are the potential limitations of using deeper neural networks or more elaborate architectures?
3. Can you provide more comparisons with other state-of-the-art methods in the field, such as t-SNE and autoencoders, and how do SimEcs and ConEcs perform in comparison to these methods?