This paper presents an actor-critic deep reinforcement learning agent with experience replay, called ACER, which is stable, sample efficient, and performs well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. The authors introduce several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.
The specific question tackled by the paper is how to design a stable and sample-efficient actor-critic method that applies to both continuous and discrete action spaces. The approach is well-motivated, building on recent advances in deep neural networks, variance reduction techniques, and parallel training of RL agents.
The paper supports its claims with extensive experiments on Atari and continuous control tasks, demonstrating that ACER matches the performance of the best known methods on Atari and outperforms popular techniques on several continuous control problems. The results are scientifically rigorous, with careful evaluation of different components of the algorithm and sensitivity analysis of hyper-parameters.
To improve the paper, I would suggest providing more details on the implementation of the trust region policy optimization method and the stochastic dueling network architectures. Additionally, it would be helpful to include more comparisons with other state-of-the-art methods and to discuss potential limitations and future directions of the research.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* Can you provide more intuition on why the truncated importance sampling with bias correction trick is effective in reducing variance and improving stability?
* How do you choose the hyper-parameters, such as the learning rate and the trust region constraint, and how sensitive is the algorithm to these choices?
* Can you discuss potential applications of the ACER algorithm beyond the domains considered in the paper, such as robotics or real-world control problems?
Overall, I would accept this paper, as it presents a significant contribution to the field of reinforcement learning, with a well-motivated approach, rigorous experiments, and promising results. The additional feedback and questions are intended to help improve the paper and provide more clarity on the methodology and results.