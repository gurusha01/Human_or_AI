Summary of the Paper
The paper proposes a batch policy gradient method (BPG) for reinforcement learning of chatbots with recurrent neural network architectures, where the rewards are noisy and expensive to obtain. The authors demonstrate the efficacy of their method through synthetic experiments and an Amazon Mechanical Turk experiment on a restaurant recommendations dataset. The paper tackles the problem of training chatbots using reinforcement learning, where the goal is to improve the chatbot's responses based on stochastic rewards assigned by human labellers.
Decision
I decide to Accept this paper, with the main reasons being that the approach is well-motivated and the paper supports its claims with empirical results.
Supporting Arguments
The paper clearly identifies the problem of noisy and expensive rewards in reinforcement learning for chatbots and proposes a batch policy gradient method to address this issue. The authors provide a thorough review of related work and demonstrate the advantages of their approach over existing methods. The empirical results on synthetic and real-world datasets show the effectiveness of the proposed method in improving the chatbot's responses.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of the GTD(λ) algorithm and the choice of hyperparameters. Additionally, it would be helpful to include more qualitative examples of the chatbot's responses before and after applying the BPG method to illustrate the improvements. The authors may also consider discussing the potential limitations of their approach and future directions for research.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on how the behaviour policy is learned from the unlabelled data, and how this policy is used in the BPG algorithm?
2. How do you choose the hyperparameters, such as the step size and the return coefficient λ, in the BPG algorithm?
3. Can you provide more insights into the trade-offs between using a constant estimator and the GTD(λ) estimator for the value function, and how this choice affects the performance of the BPG method?