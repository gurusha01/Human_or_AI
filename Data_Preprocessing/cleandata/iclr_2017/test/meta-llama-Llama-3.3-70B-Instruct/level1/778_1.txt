Summary of the Paper
The paper proposes a novel approach to reduce the test-time computational load of deep neural networks by factorizing both weights and activations into integer and non-integer components. The method approximates the real-valued weight matrix using a ternary matrix and a real-valued coefficient matrix, and transforms the activation vector into a weighted sum of binary vectors. This enables fast feed-forward propagation using simple logical operations, making it suitable for deployment on low-power CPUs or specialized hardware. The authors demonstrate the effectiveness of their approach on three different networks, achieving significant acceleration and memory compression with minimal increase in error.
Decision
I decide to Accept this paper, with the main reasons being:
1. The approach is well-motivated and placed in the literature, building upon existing work on matrix/tensor factorization and integer decomposition.
2. The paper provides thorough experimental evaluations on various networks, demonstrating the effectiveness of the proposed method in reducing computational load and memory usage.
Supporting Arguments
The paper provides a clear and concise explanation of the proposed approach, including the mathematical formulation and algorithmic details. The experimental results are comprehensive and well-presented, covering various aspects of the method's performance, such as acceleration rate, memory compression rate, and error increase. The authors also provide a thorough discussion of the related work, highlighting the advantages and limitations of their approach.
Additional Feedback
To further improve the paper, I suggest the authors consider the following:
* Providing more insights into the choice of hyperparameters, such as the number of basis vectors (kw and kx), and their impact on the performance of the method.
* Investigating the applicability of the proposed approach to other types of neural networks, such as recurrent neural networks (RNNs) or long short-term memory (LSTM) networks.
* Exploring the potential of the proposed method for other applications, such as edge computing or real-time processing.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the computational complexity of the proposed method, particularly in terms of the number of logical operations required for feed-forward propagation?
* How do you plan to address the potential issue of error propagation when compressing convolutional layers, as mentioned in the paper?
* Are there any plans to release the implementation of the proposed method as open-source code, to facilitate further research and experimentation by the community?