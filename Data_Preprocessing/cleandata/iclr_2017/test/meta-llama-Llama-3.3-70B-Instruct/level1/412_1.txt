Summary
The paper presents a novel layerwise optimization algorithm for Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a class of convolutional neural networks that employ piecewise linear non-linearities such as ReLU and max-pool, and an SVM classifier as the final layer. The algorithm formulates the parameter estimation of one layer as a difference-of-convex (DC) program, which can be solved using the concave-convex procedure (CCCP). The CCCP algorithm provides a monotonic decrease in the learning objective at each iteration, and the optimal step-size can be computed analytically. The paper demonstrates the effectiveness of the algorithm on several benchmark datasets, including MNIST, CIFAR-10, and CIFAR-100, and shows that it outperforms state-of-the-art variants of backpropagation.
Decision
I decide to Accept this paper.
Reasons
1. The paper tackles a specific and well-defined problem in the field of deep learning, namely the optimization of PL-CNNs.
2. The approach is well-motivated and placed in the literature, with a clear connection to existing work on DC programs and latent structured SVMs.
3. The paper provides a thorough and rigorous analysis of the algorithm, including a proof of convergence and a discussion of the computational complexity.
Supporting Arguments
* The paper presents a novel and innovative approach to optimizing PL-CNNs, which has the potential to improve the state-of-the-art in deep learning.
* The experimental results demonstrate the effectiveness of the algorithm on several benchmark datasets, and show that it outperforms state-of-the-art variants of backpropagation.
* The paper provides a clear and well-written presentation of the algorithm and its analysis, making it easy to follow and understand.
Additional Feedback
* One potential area for improvement is to provide more discussion on the relationship between the proposed algorithm and other optimization methods, such as stochastic gradient descent and its variants.
* It would be interesting to see more experimental results on larger datasets, such as ImageNet, to further demonstrate the scalability of the algorithm.
* The paper could benefit from more visualizations and illustrations of the algorithm and its components, to help readers understand the intuition behind the approach.
Questions for the Authors
* Can you provide more insight into the choice of the proximal term and its effect on the convergence of the algorithm?
* How does the algorithm handle non-convexities in the loss function, and what are the implications for the convergence of the algorithm?
* Are there any plans to extend the algorithm to other types of neural networks, such as recurrent neural networks or generative adversarial networks?