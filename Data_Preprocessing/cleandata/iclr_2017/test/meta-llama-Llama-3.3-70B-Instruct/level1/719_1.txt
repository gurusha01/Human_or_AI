Summary of the Paper's Contributions
The paper proposes a novel model, Classless Association, which trains two parallel Multilayer Perceptrons (MLPs) to learn the association between two input samples representing the same unknown class. The model is motivated by the Symbol Grounding Problem and association learning in infants. The authors introduce an Expectation-Maximization (EM) training rule that matches the network output against a statistical distribution, allowing the model to learn without labeled data.
Decision and Key Reasons
I decide to Accept this paper, with two key reasons:
1. The paper tackles a specific and interesting problem, namely, learning the association between two input samples without labeled data.
2. The approach is well-motivated, and the authors provide a clear explanation of the EM training rule and its application to the classless association task.
Supporting Arguments
The paper provides a thorough explanation of the model's architecture and training procedure. The authors also evaluate the model on four classless datasets generated from MNIST, demonstrating its effectiveness in learning the association between input samples. The comparison with supervised and unsupervised methods shows that the model reaches good performance in terms of association accuracy and purity.
Additional Feedback and Suggestions
To further improve the paper, I suggest:
* Providing more details on the hyperparameter tuning process, as the choice of hyperparameters seems to have a significant impact on the model's performance.
* Exploring the application of the model to more complex scenarios, such as multimodal datasets, as mentioned in the conclusion.
* Investigating the effect of using different statistical distributions, rather than just the uniform distribution, on the model's performance.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more insight into the choice of the power function in the E-step of the EM training rule?
* How do you plan to extend the model to handle cases where the number of classes is unknown or the statistical distribution is not uniform?
* Have you considered using other architectures, such as convolutional neural networks, instead of MLPs, and how might this affect the model's performance?