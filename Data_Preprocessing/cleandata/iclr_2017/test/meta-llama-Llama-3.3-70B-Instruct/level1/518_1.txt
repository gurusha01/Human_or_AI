Summary: The paper proposes a novel approach to train stochastic neural networks for probabilistic inference, specifically for drawing samples from given target distributions. The method, called Amortized Stein Variational Gradient Descent (ASVGD), iteratively adjusts the neural network parameters to minimize the KL divergence with the target distribution. The authors also apply this method to train deep energy models, resulting in a generative adversarial network called SteinGAN, which produces realistic-looking images competitive with state-of-the-art results.
Decision: Accept
Reasons: The paper tackles a specific and important problem in probabilistic inference, and the approach is well-motivated and grounded in the literature. The authors provide a clear and concise explanation of the method, and the empirical results demonstrate the effectiveness of SteinGAN in generating high-quality images.
Supporting arguments: The paper provides a thorough review of the related work, highlighting the limitations of traditional variational inference methods and the potential benefits of using neural networks for probabilistic inference. The authors also provide a detailed derivation of the ASVGD algorithm and its connection to Stein variational gradient descent. The empirical results are impressive, with SteinGAN producing images that are comparable to or better than those generated by other state-of-the-art methods.
Additional feedback: To further improve the paper, the authors could provide more details on the implementation of SteinGAN, such as the architecture of the neural network and the hyperparameter settings. Additionally, it would be interesting to see more comparisons with other generative models, such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). The authors could also explore the application of ASVGD to other problems in probabilistic inference, such as Bayesian neural networks and uncertainty quantification.
Questions for the authors: 
1. Can you provide more details on the choice of kernel in the Stein variational gradient descent algorithm, and how it affects the performance of SteinGAN?
2. How do you handle the issue of mode collapse in SteinGAN, which is a common problem in generative adversarial networks?
3. Can you provide more insights into the connection between ASVGD and other variational inference methods, such as VAEs and GANs?
4. How do you plan to extend the application of ASVGD to other problems in probabilistic inference, and what are the potential challenges and opportunities in doing so?