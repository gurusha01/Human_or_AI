Summary of the Paper's Contributions
The paper proposes a novel algorithm for pruning whole neurons from a trained neural network using a second-order Taylor series approximation of the change in error resulting from the removal of a given neuron. The authors evaluate this method against a first-order method and a brute-force serial removal method, which exhaustively finds the next best single neuron to remove at each stage. The paper also investigates the fundamental nature of learning representations in neural networks, providing evidence for the "dualist" nature of hidden units, where some units participate in the output approximation and others learn to cancel each other's influence.
Decision: Accept
I decide to accept this paper because it tackles a specific and important question in the field of neural networks, namely, how to prune neurons from a trained network without significant loss in performance. The approach is well-motivated, and the authors provide a thorough evaluation of their method against other pruning techniques. The paper also provides valuable insights into the nature of learning representations in neural networks, which is a fundamental question in the field.
Supporting Arguments
The paper's contributions are significant because they provide a new perspective on neural network pruning, which is an essential technique for reducing the computational complexity and memory requirements of neural networks. The authors' evaluation of their method against other pruning techniques is thorough and well-designed, and the results provide strong evidence for the effectiveness of their approach. Additionally, the paper's investigation into the fundamental nature of learning representations in neural networks provides valuable insights that can inform the design of future neural network architectures and pruning techniques.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the computational complexity of their method and how it compares to other pruning techniques. Additionally, it would be helpful to see more experiments on deeper networks and other popular datasets to further validate the effectiveness of their approach. Finally, the authors may want to consider discussing the potential applications of their method in real-world scenarios, such as embedded devices or mobile devices, where computational resources are limited.
Questions for the Authors
1. Can you provide more details on how you plan to parallelize the brute-force method or approximate it using a subset of the training data?
2. How do you think your method will perform on even deeper networks, and what are the potential limitations of your approach in such cases?
3. Can you discuss the potential applications of your method in real-world scenarios, such as embedded devices or mobile devices, and how it can be used to improve the efficiency of neural networks in such contexts?