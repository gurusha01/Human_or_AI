Summary
The paper proposes a novel neural network architecture, called Doubly Recurrent Neural Networks (DRNNs), for generating tree-structured objects from encoded representations. The architecture consists of two separate recurrent modules, one for ancestral information and one for fraternal information, which are combined to predict the label and topology of each node in the tree. The authors evaluate the effectiveness of DRNNs in various tasks, including synthetic tree recovery, mapping sentences to functional programs, and machine translation.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper proposes a novel and well-motivated architecture for tree-structured decoding, and (2) the experimental results demonstrate the effectiveness of DRNNs in various tasks.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem of tree-structured decoding and the limitations of existing approaches. The proposed DRNN architecture is well-motivated and grounded in the literature, and the authors provide a thorough explanation of the model and its components. The experimental results are impressive, demonstrating the ability of DRNNs to recover latent tree structure in sequences and map sentences to simple functional programs. The results also show that DRNNs exhibit desirable properties, such as invariance to structural changes and coarse-to-fine generation, in machine translation tasks.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the training procedure and hyperparameter tuning for the DRNN models. Additionally, it would be helpful to include more visualizations of the generated trees and a more detailed analysis of the errors made by the model. Finally, the authors may want to consider evaluating the DRNN architecture on more complex tasks, such as parsing or code generation, to further demonstrate its effectiveness.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the authors plan to scale the DRNN architecture to larger and more complex tasks, such as parsing or code generation?
* Can the authors provide more details on the computational efficiency of the DRNN architecture compared to existing approaches?
* How do the authors plan to address the exposure bias issue in the DRNN architecture, where the model is trained on gold-standard labels but generates labels during testing?