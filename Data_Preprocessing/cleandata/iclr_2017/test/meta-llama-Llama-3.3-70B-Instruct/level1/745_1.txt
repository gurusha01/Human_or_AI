Summary: This paper proposes SYMSGD, a parallel stochastic gradient descent (SGD) algorithm that retains the sequential semantics of SGD in expectation. SYMSGD uses a probabilistic model combiner to combine local models learned by each thread, allowing for parallelization of SGD without changing its convergence properties. The algorithm is applicable to linear learners with linear update rules and is evaluated on 9 datasets, showing up to 13x speedup over a heavily optimized sequential baseline on 16 cores.
Decision: Accept
Reasons: The paper tackles a specific problem in parallelizing SGD, which is a well-known method for regression and classification tasks. The approach is well-motivated, and the authors provide a clear explanation of the limitations of current parallelization techniques. The paper also provides a thorough evaluation of the proposed algorithm, demonstrating its effectiveness in terms of accuracy and performance.
Supporting arguments: The paper provides a detailed analysis of the convergence properties of SYMSGD, showing that it retains the same convergence rates as sequential SGD. The authors also provide an empirical evaluation of the algorithm on a range of datasets, demonstrating its scalability and accuracy. The paper also discusses the limitations of current parallelization techniques, such as HOGWILD! and ALLREDUCE, and shows how SYMSGD addresses these limitations.
Additional feedback: To further improve the paper, the authors could provide more details on the implementation of SYMSGD, including the specific optimizations used to reduce the overhead of computing the combiner matrix. Additionally, the authors could provide more analysis on the trade-offs between the dimensionality of the projected space and the accuracy of the algorithm. Finally, the authors could consider evaluating SYMSGD on more datasets, including those with non-linear update rules, to demonstrate its applicability to a wider range of machine learning tasks.
Questions for the authors: Can you provide more details on the computational overhead of computing the combiner matrix, and how this overhead is reduced in practice? How do the authors plan to extend SYMSGD to non-linear update rules, such as those used in logistic regression? Can you provide more analysis on the trade-offs between the dimensionality of the projected space and the accuracy of the algorithm, and how these trade-offs are affected by the specific dataset and learning task?