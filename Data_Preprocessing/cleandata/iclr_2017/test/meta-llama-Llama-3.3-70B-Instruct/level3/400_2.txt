This paper proposes a novel neural network architecture, called Doubly Recurrent Neural Networks (DRNNs), for generating tree-structured objects from encoded representations. The DRNN model has two key improvements: it allows information to flow in two directions (from parent and siblings) and uses a probability distribution to model tree boundaries, eliminating the need for special ending symbols.
The paper claims to contribute to the field of neural network architectures by introducing a new model that can effectively generate tree-structured objects, which is a challenging task in natural language processing and other areas of artificial intelligence. The authors demonstrate the effectiveness of the DRNN model in several tasks, including recovering synthetic trees, mapping sentences to functional programs, and machine translation.
Based on the provided information, I decide to accept this paper with two key reasons: 
1. The DRNN model shows promising results in various tasks, outperforming traditional seq2seq models in some cases.
2. The paper provides a clear and well-motivated approach to modeling tree-structured data, which is a significant contribution to the field.
The supporting arguments for these reasons include the experimental results, which demonstrate the effectiveness of the DRNN model in recovering synthetic trees and mapping sentences to functional programs. Additionally, the paper provides a clear and detailed explanation of the DRNN architecture and its components, making it easy to understand and implement.
However, I also have some additional feedback to improve the paper. One potential limitation of the paper is the simplicity of the tasks used to evaluate the DRNN model. While the results are promising, it would be beneficial to see the model applied to more complex and challenging tasks, such as seq2seq parsing. Furthermore, the paper could benefit from a more detailed analysis of the DRNN model's performance, including an examination of its strengths and weaknesses compared to other models.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How does the DRNN model handle cases where the tree structure is highly imbalanced or has a large number of nodes?
* Can the DRNN model be applied to other types of structured data, such as graphs or tables?
* How does the choice of hyperparameters, such as the number of recurrent modules and the size of the hidden states, affect the performance of the DRNN model? 
Overall, the paper presents a significant contribution to the field of neural network architectures, and with some additional experimentation and analysis, it has the potential to be a highly impactful work.