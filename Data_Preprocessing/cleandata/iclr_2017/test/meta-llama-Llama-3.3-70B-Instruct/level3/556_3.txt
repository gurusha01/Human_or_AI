Summary of the Paper's Contributions
The paper presents a novel approach for learning state representations in multi-task reinforcement learning, called Multi-Task Learning with Robotic Priors (MT-LRP). The method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The approach is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. The paper provides an extensive analysis of the method, including experimental results in a multi-task slot-car racing scenario, demonstrating the effectiveness of MT-LRP in identifying tasks and learning task-specific state representations.
Decision and Key Reasons
Based on the review, I decide to Accept the paper. The key reasons for this decision are:
1. The paper provides a well-motivated and well-placed approach in the literature, addressing a significant problem in reinforcement learning.
2. The experimental results demonstrate the effectiveness of MT-LRP in learning state representations and identifying tasks, outperforming baseline methods in several scenarios.
Supporting Arguments
The paper's approach is well-motivated by the need for efficient and scalable reinforcement learning methods that can handle multiple tasks and high-dimensional observations. The use of robotic priors and a gated neural network architecture is a novel and effective way to address this challenge. The experimental results provide strong evidence for the effectiveness of MT-LRP, demonstrating its ability to learn good state representations and identify tasks in a multi-task scenario.
Additional Feedback and Suggestions
To further improve the paper, I suggest the following:
* Provide more detailed analysis of the task-coherence prior and its impact on the learning process.
* Consider adding more experimental results to demonstrate the scalability and robustness of MT-LRP in more complex scenarios.
* Provide more discussion on the potential applications and implications of MT-LRP in real-world reinforcement learning problems.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence for my assessment, I would like the authors to answer the following questions:
* Can you provide more details on the implementation of the gated neural network architecture and the training process?
* How do you plan to address the potential issue of overfitting in the MT-LRP approach, especially in scenarios with limited data?
* Can you provide more insights into the role of the task-separation prior and its impact on the learning process?