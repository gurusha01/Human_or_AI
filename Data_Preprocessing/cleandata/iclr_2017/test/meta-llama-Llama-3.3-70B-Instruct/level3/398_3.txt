Summary
The paper proposes a novel method for visualizing the importance of specific inputs in determining the output of a Long Short Term Memory (LSTM) network. By decomposing the output of an LSTM into a product of factors, the authors are able to assign importance scores to words according to their contribution to the LSTM's prediction. The paper demonstrates the effectiveness of this approach in extracting representative phrases from trained LSTMs, which can be used to construct simple, rules-based classifiers that approximate the output of the original LSTM.
Decision
I decide to accept this paper, with the main reason being that it presents a well-motivated and well-executed approach to visualizing the importance of inputs in LSTMs. The paper provides a clear and concise explanation of the methodology, and the experiments demonstrate the effectiveness of the approach in extracting meaningful phrases from LSTMs.
Supporting Arguments
The paper is well-placed in the literature, building on existing work on visualizing LSTMs and providing a novel approach to decomposing the output of an LSTM. The experiments are thorough and well-designed, demonstrating the effectiveness of the approach in both sentiment analysis and question answering tasks. The paper also provides a clear and concise explanation of the methodology, making it easy to follow and understand.
Additional Feedback
One potential area for improvement is in providing more analysis of the limitations of the approach. For example, the paper notes that the approach may not work well for more complex tasks, but it would be helpful to provide more discussion of why this is the case and how the approach could be improved. Additionally, it would be helpful to provide more comparison to other methods for visualizing LSTMs, such as the work of Hendrik et al. (2016) and Karpathy et al. (2016).
Questions for the Authors
I would like to ask the authors to provide more information on the following points:
* Can you provide more analysis of the limitations of the approach, and how it could be improved for more complex tasks?
* How does the approach compare to other methods for visualizing LSTMs, such as the work of Hendrik et al. (2016) and Karpathy et al. (2016)?
* Can you provide more examples of the extracted phrases, and how they are used in the rules-based classifier?
* How does the approach handle out-of-vocabulary words, and can it be extended to handle more complex linguistic structures?