Summary of the Paper's Contributions
The paper proposes a novel approach to reduce the test-time computational load of deep neural networks by factorizing both weights and activations into integer and non-integer components. The authors introduce a ternary matrix decomposition and binary activation encoding, which enables fast feed-forward propagation using simple logical operations. The approach is demonstrated to achieve significant acceleration and memory compression on various neural networks, including CNN, VGG-16, and VGG-Face, with minimal loss in accuracy.
Decision: Accept
I decide to accept this paper due to its innovative approach to network compression and its potential to enable efficient deployment of deep neural networks on low-power devices. The paper provides a thorough analysis of the proposed method, including its time and space complexity, and demonstrates its effectiveness on various benchmarks.
Supporting Arguments
The paper tackles a specific and important problem in the field of deep learning, namely reducing the computational load of neural networks. The approach is well-motivated, building on existing work on matrix factorization and integer decomposition. The authors provide a clear and detailed explanation of their method, including the optimization procedures for the generator and discriminator. The experimental results are convincing, demonstrating significant acceleration and memory compression on various benchmarks.
Additional Feedback
To further improve the paper, I suggest the authors provide more insights into the optimization procedure for the generator and discriminator, including the choice of hyperparameters and the convergence criteria. Additionally, it would be helpful to include more detailed comparisons with other network compression methods, such as pruning and quantization. Finally, the authors may consider exploring the application of their method to other domains, such as natural language processing and computer vision.
Questions for the Authors
1. Can you provide more details on the optimization procedure for the generator and discriminator, including the choice of hyperparameters and the convergence criteria?
2. How do you plan to improve the approximation error further, as mentioned in the conclusion?
3. Have you considered exploring the application of your method to other domains, such as natural language processing and computer vision?