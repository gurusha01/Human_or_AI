Summary
The paper presents a novel approach for learning state representations in multi-task reinforcement learning, called Multi-Task Learning with Robotic Priors (MT-LRP). MT-LRP learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. The authors demonstrate the effectiveness of MT-LRP in a multi-task slot-car racing scenario, where it outperforms several state representation learning baselines.
Decision
I decide to Accept this paper, with the main reason being that it presents a well-motivated and novel approach to multi-task state representation learning, which is a crucial problem in reinforcement learning. The paper is well-written, and the experiments demonstrate the effectiveness of the proposed method.
Supporting Arguments
The paper tackles a specific and important question in reinforcement learning, namely learning state representations in multi-task scenarios. The approach is well-motivated, and the authors provide a clear explanation of the robotic priors and the gated neural network architecture. The experiments are thorough, and the results demonstrate the effectiveness of MT-LRP in comparison to several baselines. The paper also provides a good discussion of the results and identifies potential avenues for future work.
Additional Feedback
To further improve the paper, I would like to see more analysis on the role of the task-separation loss and its influence on the learning performance. Additionally, it would be interesting to see more experiments on the transferability of the learned state representations to new tasks. The authors may also want to consider comparing MT-LRP to other multi-task learning approaches, such as policy distillation.
Questions for the Authors
I would like the authors to clarify the following points:
* Can you provide more insight into why MT-LRP outperforms the baselines, particularly in the static-visual-cue scenario?
* How do you plan to address the issue of task-separation loss in scenarios where the number of expected tasks exceeds the number of actual tasks?
* Have you considered applying MT-LRP to more complex reinforcement learning tasks, such as those involving high-dimensional state spaces or partial observability?