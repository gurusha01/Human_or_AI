Summary
The paper proposes a novel approach for multi-task state representation learning in reinforcement learning, called MT-LRP. This method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The approach is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. The authors demonstrate the effectiveness of MT-LRP in a multi-task slot-car racing scenario, where the agent must learn to control different cars, each corresponding to a separate task.
Decision
I decide to accept this paper, with the main reason being that it presents a well-motivated and theoretically sound approach to multi-task state representation learning in reinforcement learning. The paper provides a clear and concise overview of the related work, and the proposed method is well-placed in the literature.
Supporting Arguments
The paper tackles a specific and relevant problem in reinforcement learning, and the proposed approach is elegant and theoretically well-founded. The use of a gated neural network architecture and the extension of the learning with robotic priors objective are well-motivated and contribute to the novelty of the paper. The experimental evaluation is thorough and demonstrates the effectiveness of MT-LRP in a realistic scenario.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the hyperparameter selection process, particularly for the weight balancing the influence of the additional loss term (ωτ). Additionally, it would be interesting to see a more detailed analysis of the learned state representations and how they relate to the tasks. Furthermore, the authors may want to consider exploring the application of MT-LRP to more complex and realistic scenarios, such as those involving multiple tasks with overlapping or conflicting objectives.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on how the hyperparameters were selected, particularly for the weight ωτ?
2. How do the learned state representations relate to the tasks, and can you provide more insight into the structure of the representations?
3. Have you considered exploring the application of MT-LRP to more complex and realistic scenarios, and if so, what are the potential challenges and opportunities in such scenarios?