Summary of the Paper's Claims and Contributions
The paper proposes an innovative approach to automatically learn learning rates for stochastic gradient descent (SGD) based machine learning algorithms using an actor-critic framework from reinforcement learning (RL). The authors claim that their method can achieve comparable convergence speed to expert-designed optimizers while obtaining better test accuracy. They also demonstrate the effectiveness of their approach in preventing overfitting to some extent. The paper's contributions include proposing an actor-critic algorithm for learning rate control, exploiting long-term rewards to choose better learning rates, and improving generalization performance by feeding different training examples to the actor and critic networks.
Decision and Key Reasons
Based on the evaluation of the paper, I decide to Reject the paper. The two key reasons for this decision are:
1. Limited experiments and comparisons: The paper's experiments are limited to two image classification datasets (MNIST and CIFAR-10) and do not provide a comprehensive comparison with existing methods, such as n-step Q-learning. This lack of comparison makes it difficult to assess the significance of the results on eligibility traces.
2. Insufficient robustness and generalizability: The paper's results are not sufficiently robust or generalizable due to the small number of games tested and the lack of hyperparameter tuning. This raises concerns about the applicability of the proposed method to other domains and tasks.
Supporting Arguments
The paper's approach is well-motivated, and the use of an actor-critic framework is a good choice for learning rate control. However, the experiments are not convincing, and the results are not robust enough to support the claims made in the paper. The comparison with other adaptive learning rate methods, such as vSGD, is limited, and the paper does not provide a clear understanding of how the proposed method performs in different scenarios.
Additional Feedback and Questions
To improve the paper, I suggest the following:
* Conduct more comprehensive experiments on various datasets and tasks to demonstrate the robustness and generalizability of the proposed method.
* Provide a detailed comparison with existing methods, including n-step Q-learning, to assess the significance of the results on eligibility traces.
* Investigate the effect of hyperparameter tuning on the performance of the proposed method.
* Clarify the relationship between the proposed method and other adaptive learning rate methods, such as vSGD.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How do the authors plan to extend the proposed method to other variants of SGD methods?
* Can the authors provide more insights into the choice of the state function χ(·) and its impact on the performance of the proposed method?
* How do the authors plan to address the issue of overfitting in the proposed method, especially when dealing with large datasets?