Summary
The paper explores the concept of transfer learning in text comprehension, where a model is pre-trained on a large-scale dataset and then fine-tuned on a smaller target dataset. The authors investigate the effectiveness of pre-training on two large-scale datasets, BookTest and CNN/Daily Mail, and evaluate the transfer of learned skills to two target tasks: bAbI and SQuAD. The results show that pre-training can be beneficial when combined with target-domain adjustment, but the skills learned from the pre-training datasets do not generalize well to the target tasks without additional training.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks a thorough comparison with other state-of-the-art models and techniques, making it difficult to assess the significance of the results. Secondly, the paper's focus on a specific model architecture (AS Reader) and pre-training datasets limits the generalizability of the findings.
Supporting Arguments
The paper's results are not competitive with state-of-the-art models on the SQuAD dataset, and the authors acknowledge that their model is not designed specifically for this task. Furthermore, the paper's evaluation methodology is novel, but it is not clear how it compares to other evaluation methods. The authors also do not provide a clear explanation for why the skills learned from the pre-training datasets do not generalize well to the target tasks without additional training.
Additional Feedback
To improve the paper, I suggest that the authors provide a more thorough comparison with other state-of-the-art models and techniques, and explore the use of different model architectures and pre-training datasets. Additionally, the authors could investigate the reasons behind the limited generalizability of the skills learned from the pre-training datasets and provide more insights into the transfer learning process.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the results of this paper compare to other state-of-the-art models and techniques in transfer learning for text comprehension?
* What are the limitations of the AS Reader model architecture, and how do they impact the results of the paper?
* Can the authors provide more insights into the reasons behind the limited generalizability of the skills learned from the pre-training datasets?
* How do the authors plan to address the issue of limited generalizability in future work?