Summary of the Paper's Claims and Contributions
The paper proposes a novel neural network architecture, called Doubly Recurrent Neural Networks (DRNNs), specifically designed for generating tree-structured objects from encoded representations. The authors claim that their approach outperforms state-of-the-art methods in recovering latent tree structures from sequences and mapping sentences to simple functional programs. The paper also presents experimental results demonstrating the effectiveness of DRNNs in various tasks, including synthetic tree recovery, language-to-program mapping, and machine translation.
Decision and Key Reasons
Based on the provided guidelines, I decide to Reject this paper. The two key reasons for this decision are:
1. Lack of novelty: The proposed method, although performing well, does not introduce significant new ideas or substantial improvements over existing approaches. The use of recurrent neural networks for tree-structured data is not novel, and the authors' contributions, while useful, do not seem to revolutionize the field.
2. Unclear descriptions and lack of rigor: The paper's descriptions of certain aspects, such as the definition of $w$ and the estimation of $h$ in Figure 1, are unclear. Additionally, the construction of the bicluster matrix and the training process could be more rigorously explained.
Supporting Arguments
While the paper presents promising experimental results, the lack of novelty and unclear descriptions raise concerns about the paper's overall quality and impact. The authors' replies to concerns about novelty are unconvincing, and the paper's suitability for publication is uncertain. Furthermore, the unclear descriptions and lack of rigor in certain sections may hinder the reader's understanding and reproduction of the results.
Additional Feedback and Questions
To improve the paper, I suggest that the authors:
* Provide more detailed and clear explanations of the proposed method, including the construction of the bicluster matrix and the training process.
* Discuss the novelty of their approach more explicitly, highlighting what sets it apart from existing methods.
* Consider adding more theoretical analysis or proofs to support the claims made in the paper.
I would like the authors to answer the following questions to clarify my understanding of the paper:
* Can you provide more insight into the design choices behind the DRNN architecture, and how they relate to existing recurrent neural network models?
* How do you plan to address the issue of unclear descriptions and lack of rigor in the paper, and what specific changes would you make to improve the paper's clarity and readability?