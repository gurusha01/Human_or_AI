Summary of the Paper's Contributions
The paper proposes a novel approach to automatically learn the learning rate for stochastic gradient descent (SGD) using an actor-critic algorithm from reinforcement learning (RL). The authors argue that the performance of SGD is highly sensitive to the choice of learning rate, and manual tuning is often tedious and inefficient. The proposed algorithm learns a policy to control the learning rate, which is updated based on the long-term reward predicted by a critic network. The authors demonstrate the effectiveness of their approach on two image classification datasets, MNIST and CIFAR-10, and show that it achieves comparable convergence speed to expert-designed optimizers while achieving better test accuracy.
Decision and Reasons
Based on the review, I decide to Reject the paper. The main reasons for this decision are:
1. Lack of computational overhead analysis: The paper does not provide a clear analysis of the computational overhead of the actor-critic algorithm, which is a crucial aspect of any optimization method.
2. Unclear comparison to other methods: The paper compares the proposed method to other adaptive first-order methods, but the comparison is not thorough, and it is unclear how the proposed method performs in comparison to early stopping.
Supporting Arguments
The paper proposes an interesting approach to learning the learning rate, but it has some limitations. The actor-critic algorithm is not well-motivated, and the choice of the state function and the critic network is not clearly justified. Additionally, the paper does not provide a clear analysis of the computational overhead of the algorithm, which is a crucial aspect of any optimization method. The comparison to other methods is also limited, and it is unclear how the proposed method performs in comparison to early stopping.
Additional Feedback
To improve the paper, the authors should provide a clear analysis of the computational overhead of the actor-critic algorithm and compare it to other optimization methods. They should also provide a more thorough comparison to other adaptive first-order methods and early stopping. Additionally, the authors should consider providing more details on the choice of the state function and the critic network, and justify their design choices.
Questions for the Authors
1. Can you provide a clear analysis of the computational overhead of the actor-critic algorithm and compare it to other optimization methods?
2. How does the proposed method perform in comparison to early stopping, and what are the advantages and disadvantages of each approach?
3. Can you provide more details on the choice of the state function and the critic network, and justify their design choices?