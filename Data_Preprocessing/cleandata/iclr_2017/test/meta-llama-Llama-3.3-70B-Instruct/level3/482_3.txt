Summary
This paper proposes a novel approach to reduce the test-time computational load of deep neural networks by factorizing both weights and activations into integer and non-integer components. The authors demonstrate the effectiveness of their method on three different convolutional neural networks, achieving significant acceleration and memory compression with minimal loss in accuracy.
Decision
I decide to reject this paper, with the primary reason being that the study's relevance to the ICLR conference is questionable, as it may be more suited to a data science or medical venue. Additionally, while the approach is well-motivated and the results are impressive, the paper's focus on network compression and acceleration may not align with the conference's primary themes.
Supporting Arguments
The paper provides a thorough review of related work and clearly explains the proposed method, including the ternary weight decomposition and binary activation encoding. The experiments demonstrate the effectiveness of the approach on various networks, including CNN, VGG-16, and VGG-Face. However, the paper's relevance to the ICLR conference is not explicitly stated, and the focus on network compression and acceleration may not be of primary interest to the conference attendees.
Additional Feedback
To improve the paper, the authors could provide more context on how their approach relates to the broader themes of the ICLR conference. Additionally, they could consider providing more detailed analysis of the trade-offs between acceleration, memory compression, and accuracy, as well as exploring the applicability of their method to other types of neural networks.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors envision their approach being applied in practice, and what are the potential benefits and challenges of deploying compressed networks in real-world scenarios?
2. Can the authors provide more insight into the choice of ternary weights and binary activations, and how these choices affect the overall performance of the compressed network?
3. How do the authors plan to address the potential limitations of their approach, such as the increased complexity of the compressed network and the potential for overfitting?