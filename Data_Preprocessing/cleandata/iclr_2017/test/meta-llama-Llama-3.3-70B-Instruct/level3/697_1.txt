Summary
The paper proposes a novel approach for learning state representations in multi-task reinforcement learning, called Multi-Task Learning with Robotic Priors (MT-LRP). MT-LRP learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. The authors demonstrate the effectiveness of MT-LRP in a multi-task slot-car racing scenario, where the agent must learn to control each car, with different cars corresponding to separate tasks.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-defined problem in multi-task reinforcement learning, and proposes a novel and well-motivated approach to address it. Secondly, the authors provide thorough and convincing experimental results, demonstrating the effectiveness of MT-LRP in a realistic scenario.
Supporting Arguments
The paper is well-written, and the authors provide a clear and concise introduction to the problem and their approach. The related work section is thorough, and the authors demonstrate a good understanding of the existing literature in multi-task reinforcement learning and state representation learning. The experimental results are convincing, and the authors provide a detailed analysis of the performance of MT-LRP, as well as a comparison with several baseline methods.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the computational cost of the proposed approach, including the expense of SVD in equation (7) and the cost of each iteration. Additionally, it would be interesting to see more experiments on the scalability of MT-LRP to more complex scenarios, and a more detailed analysis of the role of the task-separation loss in the learning process.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the authors plan to extend MT-LRP to more complex scenarios, with a larger number of tasks and higher-dimensional state spaces?
* Can the authors provide more insights on the role of the task-separation loss in the learning process, and how it affects the performance of MT-LRP?
* How does the authors' approach relate to other multi-task learning methods, such as policy distillation and multi-task reinforcement learning with shared neural network layers?