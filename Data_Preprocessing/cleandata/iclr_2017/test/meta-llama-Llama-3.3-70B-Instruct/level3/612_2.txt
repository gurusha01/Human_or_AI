Summary
The paper proposes a novel approach for multi-task state representation learning in reinforcement learning, called MT-LRP. This method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The approach is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. The authors demonstrate the effectiveness of MT-LRP in a multi-task slot-car racing scenario, where the agent must learn to control each car, with different cars corresponding to separate tasks.
Decision
I decide to accept this paper, with two key reasons for this choice. Firstly, the approach is well-motivated and placed in the literature, addressing a significant problem in reinforcement learning. Secondly, the paper provides a clear and thorough evaluation of the proposed method, demonstrating its effectiveness in a realistic scenario.
Supporting Arguments
The paper tackles a specific question/problem in reinforcement learning, namely, learning state representations for multiple tasks. The approach is well-motivated, building on existing work in robotic priors and gated neural networks. The authors provide a clear and detailed explanation of the proposed method, including the gated neural network architecture and the learning objective. The experimental evaluation is thorough, demonstrating the effectiveness of MT-LRP in a multi-task slot-car racing scenario.
Additional Feedback
To improve the paper, I suggest adding more classifiers to the experimental evaluation, to demonstrate the robustness of the proposed method. Additionally, the authors could provide more insight into the learned state representations, to help understand how the method works. The paper could also benefit from a more detailed discussion of the limitations of the approach and potential avenues for future work.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to address the issue of task separation, particularly in scenarios where the number of expected tasks exceeds the number of actual tasks?
2. Can the authors provide more insight into the learned state representations, to help understand how the method works?
3. How do the authors plan to extend the proposed method to more complex scenarios, such as those involving multiple agents or partial observability?