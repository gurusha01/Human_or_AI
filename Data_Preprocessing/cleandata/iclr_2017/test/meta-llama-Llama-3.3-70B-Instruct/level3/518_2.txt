Summary of the Paper
The paper proposes a novel approach to reduce the test-time computational load of deep neural networks by factorizing both weights and activations into integer and non-integer components. The authors introduce a ternary matrix decomposition and binary activation encoding, which enables fast feed-forward propagation using simple logical operations. The approach is evaluated on three different convolutional neural networks, demonstrating significant acceleration and memory compression with minimal loss of accuracy.
Decision
I decide to Reject this paper, with two key reasons for this choice. Firstly, the paper lacks comprehensive experimental results, particularly on the influence of the autoencoder, incremental updates, and training time of amortized vs non-amortized approaches. Secondly, the addition of a "repulsive force" in SVGD, which is not clearly explained in the paper, raises concerns about its effectiveness in high dimensions.
Supporting Arguments
The paper proposes an interesting approach to reducing the computational load of deep neural networks. However, the experimental results are limited, and the authors do not provide a thorough analysis of the trade-offs between acceleration, memory compression, and accuracy. Furthermore, the use of a ternary basis matrix and binary activation encoding, while innovative, requires more rigorous evaluation to demonstrate its effectiveness in various scenarios.
Additional Feedback
To improve the paper, I suggest the authors provide more comprehensive experimental results, including comparisons with other state-of-the-art methods. Additionally, the authors should clarify the role of the "repulsive force" in SVGD and provide more details on its implementation. The paper would also benefit from a more detailed analysis of the time and space complexity of the proposed approach.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the implementation of the "repulsive force" in SVGD and its effectiveness in high dimensions?
2. How do you plan to address the lack of comprehensive experimental results, particularly on the influence of the autoencoder and incremental updates?
3. Can you provide more insights into the trade-offs between acceleration, memory compression, and accuracy in the proposed approach?