Summary
The paper proposes a novel approach for multi-task state representation learning in reinforcement learning, called MT-LRP. This method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The approach is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. The authors demonstrate the effectiveness of MT-LRP in a multi-task slot-car racing scenario, where the agent must learn to control each car, with different cars corresponding to separate tasks.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's evaluation methodology is not comprehensive, as it does not compare the proposed approach with previous work in a thorough manner. Secondly, the paper's claims are not fully supported by the results, as the experiments are limited to a specific scenario and do not demonstrate the generalizability of the approach.
Supporting Arguments
The paper's approach is well-motivated, and the use of a gated neural network architecture is a good idea. However, the evaluation methodology is not rigorous, as it only compares MT-LRP with a few baselines and does not provide a thorough analysis of the results. Additionally, the paper's claims about the effectiveness of MT-LRP are not fully supported by the results, as the experiments are limited to a specific scenario and do not demonstrate the generalizability of the approach. Furthermore, the paper does not provide a clear explanation of how the task-coherence prior works, and the results do not demonstrate the necessity of this prior.
Additional Feedback
To improve the paper, the authors should provide a more comprehensive evaluation of the proposed approach, including comparisons with previous work and a thorough analysis of the results. Additionally, the authors should provide more details about the task-coherence prior and demonstrate its necessity. The authors should also consider experimenting with different scenarios to demonstrate the generalizability of the approach.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How does the task-coherence prior work, and why is it necessary?
* Why did the authors choose to use a gated neural network architecture, and how does it compare to other architectures?
* How do the authors plan to extend the approach to more complex scenarios, and what are the potential limitations of the approach?
* Can the authors provide more details about the experimental setup, including the hyperparameters used and the computational resources required?
* How do the authors plan to address the issue of overfitting, and what regularization techniques did they use?