Summary
The paper presents a novel approach for learning state representations in multi-task reinforcement learning, called Multi-Task Learning with Robotic Priors (MT-LRP). MT-LRP learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. The authors demonstrate the effectiveness of MT-LRP in a multi-task slot-car racing scenario, where the agent must learn to control each car, with different cars corresponding to separate tasks.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, I find it hard to understand the contributions of the paper, as the authors do not clearly explain how their findings can contribute to future works on optimization in deep learning, making its practical applications unclear. Secondly, the paper lacks a clear explanation of how the task-coherence prior variants influence the learning performance, which is a crucial aspect of the proposed method.
Supporting Arguments
The paper builds on the approach of Goodfellow et al. (2015) and explores the optimization landscape in deep learning using different algorithms. However, the authors do not provide a clear explanation of how their method differs from existing approaches, such as multi-task learning (Caruana, 1997) and policy distillation (Rusu et al., 2016). Additionally, the experimental results are limited to a single scenario, and it is unclear how the method would perform in more complex environments.
Additional Feedback
To improve the paper, I suggest that the authors provide a clearer explanation of the contributions of their method and its practical applications. Additionally, they should provide more detailed analysis of the task-coherence prior variants and their influence on the learning performance. The authors may also consider plotting eigenspectra to provide further insight into the optimization landscape. Furthermore, the paper would benefit from a more detailed comparison with existing approaches and a more extensive evaluation of the method in different scenarios.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How does the proposed method differ from existing approaches, such as multi-task learning and policy distillation?
2. Can you provide more detailed analysis of the task-coherence prior variants and their influence on the learning performance?
3. How do you plan to extend the method to more complex environments and scenarios?
4. Can you provide more insight into the optimization landscape of the proposed method, for example, by plotting eigenspectra?