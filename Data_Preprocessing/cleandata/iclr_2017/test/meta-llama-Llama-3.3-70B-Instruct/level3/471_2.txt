The paper proposes a novel approach to train stochastic neural networks for probabilistic inference, leveraging the Stein variational gradient descent (SVGD) algorithm. The authors introduce an amortized version of SVGD, which enables the method to adaptively improve its efficiency by leveraging fast experience, especially in cases where it needs to perform fast inference repeatedly on a large number of similar tasks.
I decide to accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper provides instructive artificial experiments and thorough real-world experiments, although the latter shows only modest improvement.
The approach is well-motivated, as it addresses the key computational challenge of developing efficient inference techniques to approximate complex distributions. The authors provide a clear and concise overview of the related work, highlighting the limitations of traditional variational inference methods and the potential benefits of using SVGD. The paper is well-written, and the authors provide a detailed explanation of the proposed method, including the mathematical derivations and the algorithmic implementation.
The artificial experiments are instructive, as they provide valuable insights into the behavior of the proposed method. The real-world experiments are thorough, although the results show only modest improvement compared to state-of-the-art methods. However, the authors provide a detailed discussion of the results, highlighting the strengths and limitations of their approach.
To improve the paper, I suggest that the authors provide more intuition on why the batch version of the algorithm outperforms the online version. Additionally, the authors could provide more details on the hyperparameter tuning process and the computational resources required to run the experiments.
I would like to ask the authors to clarify the following points: (1) How do the authors choose the kernel function and the bandwidth parameter in the SVGD algorithm? (2) Can the authors provide more details on the architecture of the neural network used in the experiments? (3) How do the authors plan to extend the proposed method to more complex distributions and tasks? 
Overall, the paper provides a significant contribution to the field of probabilistic inference, and the proposed method has the potential to be widely applicable in various domains. With some minor revisions to address the above points, the paper is ready for publication.