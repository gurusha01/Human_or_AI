This paper proposes a novel neural network architecture for generating tree-structured objects from encoded representations. The architecture, called Doubly Recurrent Neural Networks (DRNNs), models the information flow in a tree with two separate recurrent modules: one carrying ancestral information and the other carrying fraternal information. The topology of the tree is modeled explicitly and separately from the label prediction.
The paper claims to contribute to the field of natural language processing and machine learning by introducing a new approach to tree-structured decoding. The authors demonstrate the effectiveness of their approach through various experiments, including synthetic tree recovery, mapping sentences to functional programs, and machine translation.
The specific question or problem tackled by the paper is how to generate tree-structured objects from encoded representations. The approach is well-motivated, as it addresses the limitations of existing methods that rely on sequential decoding or require a fixed tree structure. The paper is also well-placed in the literature, as it builds upon existing work on recurrent neural networks and tree-structured decoding.
However, I have some concerns regarding the clarity and quality of the paper. Specifically, I found the notation and model explanation in Section 2.2 to be unclear and inconsistent. Additionally, some of the claims made in the paper, such as the claim that dropout increases sparsity in bicluster assignments, need better argumentation or experimental support.
Based on these concerns, I decide to reject the paper. The main reasons for this decision are the lack of clarity in the model explanation and the need for more rigorous experimental support for some of the claims made in the paper.
To improve the paper, I suggest that the authors provide a clearer and more consistent explanation of the model, including the notation and the architecture of the DRNNs. Additionally, the authors should provide more experimental evidence to support their claims, particularly with regards to the effectiveness of dropout in increasing sparsity in bicluster assignments.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* Can you provide a more detailed explanation of the notation used in Section 2.2, particularly with regards to the ancestral and fraternal hidden states?
* How do you ensure that the DRNNs are able to capture the hierarchical structure of the trees, particularly in cases where the trees are deep or have a large number of nodes?
* Can you provide more experimental evidence to support the claim that dropout increases sparsity in bicluster assignments, particularly in the context of tree-structured decoding?