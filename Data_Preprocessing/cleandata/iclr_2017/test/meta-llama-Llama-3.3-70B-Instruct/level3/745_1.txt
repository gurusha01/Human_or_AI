The paper proposes a novel approach to train stochastic neural networks for probabilistic inference, leveraging Stein variational gradient descent (SVGD) to iteratively adjust the network parameters and minimize the KL divergence with the target distribution. The authors also introduce an amortized MLE algorithm, SteinGAN, which adaptively trains a neural sampler to approximate the likelihood function and generates realistic-looking images competitive with state-of-the-art results.
I decide to accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, addressing a significant problem in probabilistic inference, and (2) the paper provides a clear and concise explanation of the proposed method, along with empirical results demonstrating its effectiveness.
The paper supports its claims through a combination of theoretical analysis and empirical evaluations. The authors provide a detailed derivation of the SVGD algorithm and its connection to the Stein operator, as well as an intuitive explanation of the amortized MLE algorithm, SteinGAN. The empirical results on several datasets, including MNIST, CIFAR-10, CelebA, and LSUN, demonstrate the effectiveness of SteinGAN in generating realistic-looking images and its competitiveness with state-of-the-art methods.
To further improve the paper, I suggest providing additional analysis on the computational complexity and convergence rate of the proposed algorithm. Specifically, it would be helpful to provide a detailed analysis of the time and space complexity of the parallel SGD algorithm, as well as a convergence rate analysis to establish the theoretical foundations of the method. Additionally, the authors may consider providing more insights into the choice of the kernel function and its impact on the performance of the algorithm.
Some questions I would like the authors to answer to clarify my understanding of the paper include: (1) How does the choice of the kernel function affect the performance of the SVGD algorithm, and are there any guidelines for selecting an appropriate kernel? (2) Can the authors provide more details on the implementation of the SteinGAN algorithm, including the specific architecture of the neural network and the hyperparameter settings used in the experiments? (3) How does the proposed method handle cases where the target distribution is multimodal or has a complex structure, and are there any plans to extend the method to handle such cases?