Summary of the Paper's Contributions
The paper proposes a novel factorization approach to reduce the test-time computational load of Deep Neural Networks (DNNs). The method factorizes both weights and activations into integer and non-integer components, allowing for efficient computation using logical operations. The authors demonstrate the effectiveness of their approach on various networks, including CNNs for handwritten digits, VGG-16 for ImageNet classification, and VGG-Face for face recognition, achieving significant acceleration and memory compression with minimal loss in accuracy.
Decision and Key Reasons
I decide to Accept this paper, with the key reasons being:
1. The paper tackles a specific and relevant problem in the field of deep learning, namely reducing the computational load of DNNs at test-time.
2. The proposed approach is well-motivated and grounded in existing literature, with a clear explanation of the factorization method and its advantages.
Supporting Arguments
The paper provides a thorough explanation of the factorization approach, including the ternary matrix decomposition and binary activation encoding. The authors also present comprehensive experiments on various networks, demonstrating the effectiveness of their method in achieving acceleration and memory compression. The results show that the proposed approach can achieve significant speedup and compression rates with minimal loss in accuracy, making it a promising technique for deploying DNNs on resource-constrained devices.
Additional Feedback and Suggestions
To further improve the paper, I suggest:
* Providing a more comprehensive comparison with existing methods, including a detailed analysis of the trade-offs between accuracy, speed, and memory compression.
* Investigating the applicability of the proposed approach to other types of neural networks, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks.
* Exploring the use of more advanced discrete optimization algorithms to further improve the approximation error.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the computational complexity of the factorization approach, including the time and memory requirements for the ternary matrix decomposition and binary activation encoding?
* How do you plan to address the potential issue of error propagation when compressing convolutional layers, and what strategies can be employed to mitigate this effect?
* Are there any plans to release the implementation of the proposed approach as open-source code, and if so, what programming language and framework will be used?