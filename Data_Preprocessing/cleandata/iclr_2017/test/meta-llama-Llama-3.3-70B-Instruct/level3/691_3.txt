Summary
The paper presents a novel approach to reducing the test-time computational load of deep neural networks by factorizing both weights and activations into integer and non-integer components. The authors propose a unified framework that combines matrix/tensor factorization and integer decomposition, allowing for fast feed-forward propagation using simple logical operations. The method is evaluated on three different convolutional neural networks, demonstrating significant acceleration and memory compression with minimal loss of accuracy.
Decision
I decide to Accept this paper, with the primary reason being the novelty and effectiveness of the proposed approach. The authors provide a well-motivated and well-placed contribution in the literature, and the results demonstrate the potential of the method for various machine learning tasks.
Supporting Arguments
The paper tackles a specific and relevant problem in the field of deep learning, and the approach is well-motivated by the need for efficient deployment of deep neural networks on low-power CPUs or specialized hardware. The authors provide a clear and concise explanation of the method, and the experimental results demonstrate the effectiveness of the approach on various networks and tasks. The paper also provides a thorough analysis of the time and space complexity of the method, which is essential for understanding its practical implications.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed analysis of the sensitivity of the method to the choice of hyperparameters, such as the number of basis vectors (kw and kx). Additionally, it would be beneficial to include more comparisons with other state-of-the-art methods for network compression and acceleration. The authors may also consider providing more insights into the potential applications of the method beyond the evaluated tasks, such as semantic segmentation, image description, and stereo matching.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to improve the approximation error further, as mentioned in the conclusion?
2. Can the authors provide more details on the discrete optimization algorithm used for ternary matrix decomposition?
3. How do the authors envision the method being applied to other types of neural networks, such as recurrent neural networks or graph neural networks?