The paper proposes a novel approach to automatically learn learning rates for stochastic gradient descent (SGD) based machine learning algorithms using an actor-critic framework from reinforcement learning. The authors claim that their method can achieve comparable convergence speed to expert-designed optimizers while achieving better test accuracy.
I decide to reject this paper with two key reasons. Firstly, the presentation of the paper has issues, including a misleading claim of being an attention model and an unconvincing parallel with memory networks/neural Turing machines. Secondly, the experimental section is flawed due to the lack of testing on standard benchmarks and comparison with state-of-the-art models, as well as a lack of discussion on the results and insights of the proposed model.
To support my decision, I argue that the paper's contribution, although novel, is not well-motivated in the literature. The authors fail to provide a clear explanation of how their method improves upon existing adaptive learning rate methods. Furthermore, the calculation of μ_context in section 2.2 may lose the order of relations, which could affect the validity of the approach.
To improve the paper, I suggest that the authors provide more context on the related work, particularly on adaptive learning rate methods, and clarify the differences between their approach and existing methods. Additionally, the authors should provide more detailed analysis of the results, including discussions on the convergence speed, test accuracy, and the effect of the learning rate controller on the model's performance.
I would like to ask the authors to clarify the following points: (1) How does the proposed method handle non-stationary environments, where the optimal learning rate may change over time? (2) Can the authors provide more insights on the choice of the state function χ(·) and its impact on the performance of the algorithm? (3) How does the proposed method compare to other adaptive learning rate methods, such as AdaGrad and Adam, in terms of computational complexity and hyperparameter tuning?