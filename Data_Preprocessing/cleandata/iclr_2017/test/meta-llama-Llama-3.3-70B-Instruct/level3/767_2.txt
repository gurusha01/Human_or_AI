This paper proposes a novel approach to improve the performance of dropout neural networks by reducing the inference gap between the training and inference phases. The authors formulate dropout as a latent variable model and introduce the concept of expectation-linearity to quantify the inference gap. They also propose a regularization scheme to control the inference gap and provide theoretical guarantees on the trade-off between model accuracy and expectation-linearity.
The paper tackles the specific question of how to reduce the inference gap in dropout neural networks, which is a well-motivated problem given the widespread use of dropout in deep learning. The approach is well-placed in the literature, building on existing work on dropout and latent variable models.
The paper supports its claims with a combination of theoretical analysis and empirical experiments. The theoretical results provide a rigorous understanding of the inference gap and its relationship to model accuracy, while the experiments demonstrate the effectiveness of the proposed regularization scheme on several benchmark datasets.
I decide to accept this paper because it makes a significant contribution to the understanding and improvement of dropout neural networks. The paper is well-written, and the authors provide a clear and concise explanation of their approach and results.
To improve the paper, I would suggest the following:
* Provide more detailed comparisons with other recently proposed methods, such as those by Daniel et al. and Andrychowicz et al., which also use policy search RL methods.
* Consider providing more experimental results on standard architectures, such as ResNets and DenseNets, to demonstrate the broader applicability of the proposed approach.
* Clarify the relationship between the proposed regularization scheme and other regularization techniques, such as dropout and weight decay.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* Can you provide more intuition on why the proposed regularization scheme is effective in reducing the inference gap?
* How does the choice of hyperparameters, such as the regularization constant Î», affect the performance of the proposed approach?
* Are there any plans to extend the proposed approach to other types of neural networks, such as recurrent neural networks or generative adversarial networks?