Summary
The paper proposes a semi-supervised learning approach based on in-painting with an adversarial loss, called Context-Conditional Generative Adversarial Networks (CC-GANs). The authors demonstrate the effectiveness of their approach on two classification benchmarks, STL-10 and PASCAL VOC 2007, and show that it outperforms existing semi-supervised methods. The CC-GAN model consists of a generator that fills in missing patches in an image and a discriminator that distinguishes between real and generated images.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the authors' response to pre-review questions about dataset density and subsampling was weak and unconvincing, failing to address the potential impact on results. Secondly, the paper's emphasis on addressing the cold-start issue is at odds with the decision to subsample the data, which reduces sparsity and may not accurately represent real-world scenarios.
Supporting Arguments
The paper proposes new user and item embedding methods to learn complex interactions between users and items, building on recent work in deep recommender systems. However, the evaluation methodology is unusual, relying solely on Recall@M, and additional performance measures like RMSE or AUC would be necessary for completeness. Furthermore, the comparison to competing methods is limited by the use of unusual data samples, making it difficult to determine whether performance improvements are due to the method or dataset and loss function choices.
Additional Feedback
To improve the paper, I suggest that the authors provide more convincing responses to pre-review questions, particularly regarding dataset density and subsampling. Additionally, they should consider reporting additional performance measures and using more standard evaluation methodologies. The authors should also provide more detailed comparisons to competing methods and explore the use of different data samples to demonstrate the robustness of their approach.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more detailed explanations for the choice of subsampling method and its potential impact on results?
2. How do you plan to address the cold-start issue in scenarios where data is highly sparse or scarce?
3. Can you provide additional experimental results using different evaluation metrics and data samples to demonstrate the robustness of your approach?