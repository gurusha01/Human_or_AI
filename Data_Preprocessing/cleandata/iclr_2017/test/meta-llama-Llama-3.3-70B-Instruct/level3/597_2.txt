I will provide a review of the research paper based on the provided guidelines.
Summary of the paper's claims and contributions
The paper proposes a new method for improving the performance of deep neural networks (DNNs) by reducing the inference gap between the training and inference phases. The authors formulate dropout as a tractable approximation of a latent variable model and introduce the notion of expectation-linear dropout neural networks. They also propose a regularization scheme to control the inference gap and provide theoretical analysis and experimental results to demonstrate the effectiveness of their approach.
Decision and key reasons
I decide to reject this paper. The key reasons for this decision are:
1. The paper's claims of beating existing baselines for the Traveling Salesman Problem (TSP) are not supported, as the local search method LK-H outperforms the proposed method.
2. The authors' decision to leave Figure 1 unchanged, comparing their method to a poor implementation of local search, is seen as irresponsible and misleading.
3. The paper's use of poor baselines makes the authors' method appear stronger than it is, and the authors' reluctance to acknowledge this is a concern.
Supporting arguments
The paper's methodological contribution is interesting, but the lack of strong baselines and the misleading comparison in Figure 1 undermine the validity of the results. Additionally, the authors' statement about their method being only a few percent worse than optimality is misleading, as getting to that point is typically trivial, and the hard part is pushing those last few percent.
Additional feedback and questions
To improve the paper, I suggest that the authors:
1. Use stronger baselines, such as the OR-tools solver from Google, to evaluate their method.
2. Provide more information on the performance of their method with more training data.
3. Clarify the difference between RL pretraining Sampling T=1 and T=T* and provide more data to support their conclusions.
I would like the authors to answer the following questions:
1. How do the authors plan to address the issue of poor baselines and misleading comparisons in their experiments?
2. Can the authors provide more details on the theoretical analysis of their method and its relationship to existing work on dropout and neural networks?
3. How do the authors plan to extend their method to more complex problems and larger datasets?