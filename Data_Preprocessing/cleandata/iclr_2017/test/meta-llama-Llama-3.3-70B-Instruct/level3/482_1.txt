Summary of the Paper's Contributions
The paper proposes a novel approach to reduce the test-time computational load of deep neural networks by factorizing both weights and activations into integer and non-integer components. The authors introduce a ternary matrix decomposition and binary activation encoding, which enables fast feed-forward propagation using simple logical operations. The method is evaluated on three different convolutional neural networks, demonstrating significant acceleration and memory compression with minimal loss of accuracy.
Decision and Key Reasons
Based on the evaluation, I decide to Accept this paper. The key reasons for this decision are:
1. The paper tackles an important problem in deep learning, and the proposed approach shows promising results in reducing computational load and memory usage.
2. The authors provide a thorough evaluation of their method on various networks and tasks, demonstrating its effectiveness and versatility.
Supporting Arguments
The paper is well-written, organized, and presented, making it an enjoyable read. The authors provide a clear motivation for their approach, placing it in the context of existing research on network compression and acceleration. The experimental results are convincing, showing significant improvements in computational efficiency and memory usage while maintaining acceptable levels of accuracy.
Additional Feedback and Suggestions
To further improve the paper, I suggest the authors provide more detailed analysis of the results, particularly in terms of the trade-offs between acceleration, compression, and accuracy. Additionally, it would be helpful to include more visualizations and plots to illustrate the effects of the proposed method on different networks and tasks. Furthermore, the authors may consider exploring the application of their method to other domains, such as natural language processing or speech recognition.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence for my assessment, I would like the authors to answer the following questions:
1. Can you provide more details on the optimization algorithm used for ternary matrix decomposition, and how it compares to other existing methods?
2. How do you plan to address the potential limitations of the proposed method, such as the need for pre-computing canonical coefficients and the reliance on lookup tables?
3. Have you considered exploring other applications of the proposed method, such as model pruning or knowledge distillation, and if so, what are the potential benefits and challenges?