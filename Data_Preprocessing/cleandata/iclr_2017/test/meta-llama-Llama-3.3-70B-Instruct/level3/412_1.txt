Summary of the Paper's Contributions
The paper proposes a novel approach to biclustering using Rectified Factor Networks (RFNs), which overcomes the limitations of existing methods, particularly FABIA. RFNs efficiently construct sparse, non-linear, high-dimensional representations of the input data, allowing for the identification of rare and small events. The authors demonstrate the effectiveness of RFNs on various synthetic and real-world datasets, including gene expression data and the 1000 Genomes Project data.
Decision and Key Reasons
I decide to Reject this paper, with two key reasons:
1. Lack of convincing experimental results: Although the paper presents extensive experiments on synthetic and real-world datasets, the results are not entirely convincing. The performance of RFNs is compared to 13 other biclustering methods, but the evaluation metrics and experimental settings are not thoroughly described.
2. Questionable accuracy on CIFAR-10: The paper reports a best model accuracy of 70.2% on CIFAR-10, which is significantly lower than existing work. This raises concerns about the effectiveness of RFNs on more complex datasets.
Supporting Arguments
The paper's theoretical analysis and motivation for using RFNs are well-presented, and the authors provide a clear explanation of the RFN model and its advantages over FABIA. However, the experimental results are not sufficient to support the claims made in the paper. The authors should provide more detailed information about the experimental settings, evaluation metrics, and hyperparameter tuning to increase the credibility of the results.
Additional Feedback and Questions
To improve the paper, I suggest the authors:
* Provide more detailed information about the experimental settings, including the specific datasets used, evaluation metrics, and hyperparameter tuning procedures.
* Compare the performance of RFNs to state-of-the-art methods on more complex datasets, such as ImageNet or CIFAR-100.
* Investigate the effect of different hyperparameters, such as the number of hidden units, dropout rate, and learning rate, on the performance of RFNs.
* Consider using more robust evaluation metrics, such as the area under the receiver operating characteristic curve (AUC-ROC) or the area under the precision-recall curve (AUC-PR), to assess the performance of RFNs.
I would like the authors to answer the following questions to clarify my understanding of the paper:
* Can you provide more information about the specific datasets used in the experiments, including the number of samples, features, and classes?
* How did you tune the hyperparameters of RFNs, and what was the effect of different hyperparameters on the performance of the model?
* Can you provide more details about the computational resources used to train RFNs, including the type of hardware and the training time?