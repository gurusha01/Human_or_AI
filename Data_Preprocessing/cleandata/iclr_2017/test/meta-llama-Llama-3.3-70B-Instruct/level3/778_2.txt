Summary of the Paper
This paper proposes a novel approach to reduce the test-time computational load of deep neural networks by factorizing both weights and activations into integer and non-integer components. The authors introduce a ternary matrix decomposition and binary activation encoding, which enables fast feed-forward propagation using simple logical operations. The method is evaluated on three different convolutional neural networks, including CNN for handwritten digits, VGG-16 for ImageNet classification, and VGG-Face for large-scale face recognition.
Decision and Reasons
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks a clear comparison with state-of-the-art networks, particularly those achieving high accuracy on benchmark datasets such as MNIST and ImageNet. Secondly, the authors do not provide a comprehensive evaluation of the proposed method on various network architectures and tasks, which makes it difficult to assess the generalizability and effectiveness of the approach.
Supporting Arguments
The paper presents some promising results on reducing computational load and memory usage, but the evaluation is limited to a few specific networks and tasks. The authors claim that their method can achieve significant acceleration and compression rates, but the results are not consistently impressive across all experiments. For example, the error rate increase is relatively high for some settings, which may not be acceptable for many applications. Furthermore, the paper does not provide a detailed analysis of the trade-offs between accuracy, computational load, and memory usage, which is essential for understanding the practical implications of the proposed method.
Additional Feedback and Questions
To improve the paper, I suggest that the authors provide a more comprehensive evaluation of their method on various network architectures and tasks, including a clear comparison with state-of-the-art networks. Additionally, the authors should consider providing more detailed analysis of the trade-offs between accuracy, computational load, and memory usage. Some specific questions I would like the authors to answer include: (1) How does the proposed method perform on other benchmark datasets, such as CIFAR-10 and CIFAR-100? (2) Can the authors provide a more detailed comparison with other network compression methods, such as pruning and quantization? (3) How does the choice of ternary basis affect the reconstruction error and the overall performance of the network? (4) Can the authors provide more insights into the optimization algorithm used for ternary matrix decomposition and binary activation encoding? 
I would also like to see a clear table comparing the original results with the results after compression is applied for all tasks, as well as results when compression is applied to state-of-the-art networks where float representation is crucial. This would help to better understand the effectiveness and limitations of the proposed method.