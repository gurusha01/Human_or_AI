Summary of the Paper's Contributions
The paper proposes a novel approach to automatically learn learning rates for stochastic gradient descent (SGD) based machine learning algorithms using an actor-critic framework from reinforcement learning (RL). The authors design an algorithm that learns to predict a learning rate at each time step by utilizing the long-term reward predicted by a critic network. The approach is simple, yet effective, and has been shown to achieve comparable convergence speed with expert-designed optimizers while achieving better test accuracy on two image classification datasets, MNIST and CIFAR-10.
Decision and Reasons
I decide to Accept this paper with two key reasons: (1) the proposed approach is well-motivated and grounded in the literature, and (2) the experimental results demonstrate the effectiveness of the approach in achieving better test accuracy and preventing overfitting to some extent.
Supporting Arguments
The paper tackles a specific question of automatically learning learning rates for SGD-based machine learning algorithms, which is a crucial problem in the field. The approach is well-motivated, as it leverages the recent success of RL in sequential decision problems. The authors provide a clear and concise explanation of the algorithm, and the experimental results are thorough and well-presented. The comparison with other adaptive learning rate methods, such as vSGD, demonstrates the superiority of the proposed approach.
Additional Feedback
To further improve the paper, I suggest the authors provide more insights into the learned learning rate controllers, such as visualizing the learned learning rates over time or analyzing the effect of different hyperparameters on the performance of the algorithm. Additionally, it would be interesting to see the application of the proposed approach to other variants of SGD methods or other machine learning algorithms.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on the design of the state function χ(·) and how it affects the performance of the algorithm?
2. How do you handle the case where the actor network outputs a learning rate that is too large or too small, and how do you prevent the algorithm from diverging in such cases?
3. Can you provide more insights into the effect of the discount factor γ on the performance of the algorithm, and how you chose the value of γ in the experiments?