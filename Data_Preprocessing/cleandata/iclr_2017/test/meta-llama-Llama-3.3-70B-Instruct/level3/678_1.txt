This paper explores the concept of transfer learning in reading comprehension, specifically examining the effectiveness of pre-training a model on large artificial datasets and fine-tuning it on smaller target datasets. The authors investigate the performance of the Attention Sum Reader (AS Reader) model on two target datasets, bAbI and SQuAD, after pre-training on the BookTest and CNN/Daily Mail datasets.
The paper claims to contribute to the understanding of transfer learning in reading comprehension by demonstrating the potential benefits of pre-training on large datasets and fine-tuning on smaller target datasets. The authors also examine the importance of pre-training in different model components, such as word embeddings and encoder parameters.
I decide to reject this paper, primarily due to the unclear presentation and lack of transparency in the experimental setup. The paper's complexity and the reporting of mean performance across tasks and models without standard deviations make it challenging to understand the results. Additionally, the authors fail to define unexplained acronyms, such as GRU, BT, and CBT, which may confuse readers unfamiliar with these terms.
To support this decision, I argue that the paper's experimental setup is overly complex, making it difficult to replicate and understand the results. The authors should simplify the setup and provide clearer explanations of their methods and results. Furthermore, the paper's lack of transparency in reporting standard deviations and other relevant statistics undermines the validity of the conclusions.
To improve the paper, I suggest that the authors provide more detailed explanations of their experimental setup, including the hyperparameters used and the specific models employed. They should also report standard deviations and other relevant statistics to increase the transparency of their results. Additionally, the authors should define all technical terms and acronyms to ensure that the paper is accessible to a broader audience.
I would like the authors to answer the following questions to clarify my understanding of the paper:
1. Can you provide more details on the hyperparameters used for the AS Reader model and the pre-training datasets?
2. How did you select the specific tasks from the bAbI dataset, and what was the criteria for choosing these tasks?
3. Can you provide more information on the entity anonymization process used in the CNN/Daily Mail dataset, and how it affects the results?
By addressing these concerns and providing more clarity and transparency in their experimental setup and results, the authors can strengthen their paper and increase its potential for acceptance.