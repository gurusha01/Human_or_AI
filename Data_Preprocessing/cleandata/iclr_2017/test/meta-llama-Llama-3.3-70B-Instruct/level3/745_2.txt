Summary of the Paper
The paper proposes a novel approach to train stochastic neural networks for probabilistic inference, specifically for drawing samples from given target distributions. The method, called Amortized Stein Variational Gradient Descent (ASVGD), iteratively adjusts the neural network parameters to minimize the KL divergence between the target distribution and the output distribution. The authors also propose an application of ASVGD, called SteinGAN, for training deep energy models, which achieves competitive results with state-of-the-art generative adversarial networks (GANs).
Decision
I decide to reject this paper, with two key reasons: (1) the paper's topic is not directly relevant to representation learning, and (2) the experimental results are unconvincing due to the use of small datasets.
Supporting Arguments
Firstly, the paper's focus on probabilistic inference and generative modeling, while interesting, does not align closely with the conference's focus on representation learning. The authors could have done more to explicitly connect their work to representation learning, such as exploring the use of ASVGD for learning representations in downstream tasks.
Secondly, the experimental results are limited to small datasets (e.g., MNIST, CIFAR-10), which can be easily solved with a single-core CPU. The authors do not demonstrate the scalability of their approach to larger datasets, which is a critical aspect of representation learning. Furthermore, the comparison to existing methods, such as Hogwild, is not thorough, and the authors do not provide sufficient evidence to support their claims of superiority.
Additional Feedback
To improve the paper, the authors could consider the following suggestions:
* Provide a clearer connection between ASVGD and representation learning, such as exploring its use in learning representations for downstream tasks.
* Conduct more extensive experiments on larger datasets to demonstrate the scalability of their approach.
* Compare their method to a wider range of existing methods, including Hogwild, and provide more detailed analysis of the results.
* Consider providing more theoretical analysis of the ASVGD algorithm, such as its convergence properties and computational complexity.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on how ASVGD can be used for representation learning, and what benefits it offers over existing methods?
* How do you plan to scale up your approach to larger datasets, and what computational resources would be required?
* Can you provide more insight into the choice of kernel and bandwidth in the SVGD algorithm, and how these hyperparameters affect the performance of ASVGD?