This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.  The basic idea is to use a "double" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.  
Hierarchical modeling is an important and high impact problem, and I think that it's under-explored in the Deep Learning literature.  
Pros:
  -The few-shot learning results look good, but I'm not an expert in this area.  
  -The idea of using a "double" variational bound in a hierarchical generative model is well presented and seems widely applicable.  
Questions: 
  -When training the statistic network, are minibatches (i.e. subsets of the examples) used?  
  -If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?  For example, what if the statistic network wants to pull out if any example from the dataset has a certain feature and treat that as the characterization.  This seems to fit the graphical model on the right side of figure 1.  If your statistic network is trained on minibatches, it won't be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset.  Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.  
Suggestions: 
  -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.