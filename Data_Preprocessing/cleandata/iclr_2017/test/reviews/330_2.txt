This paper presents a framework for creating document representations. 
The main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. 
Experiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. 
While I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.
Most of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method. 
For this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  
For RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states?
One of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. 
I think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.