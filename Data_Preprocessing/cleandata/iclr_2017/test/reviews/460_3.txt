The paper looks at several innovations for deep RL, and evaluates their effect on solving games in the Atari domain.  The paper reads a bit like a laundry list of the researcher's latest tricks.  It is written clearly enough, but lacks a compelling message.  I expect the work will be interesting to people already implementing deep RL methods, but will probably not get much attention from the broader community.
The claims on p.1 suggest the approach is stable and sample efficience, and so I expected to see some theoretical analysis with respect to these properties. But this is an empirical claim; it would help to clarify that in the abstract.
The proposed innovations are based on sound methods.  It is particularly nice to see the same approach working for both discrete and continuous domains.
The paper has reasonably complete empirical results. It would be nice to see confidence intervals on more of the plots. Also, the results don't really tease apart the effect of each of the various innovations, so it's harder to understand the impact of each piece and to really get intuition, for example about why ACER outperforms A3C.  Also, it wasn't clear to me why you only get matching results on discrete tasks, but get state-of-the-art on continuous tasks.
The paper has good coverage of the related literature. It is nice to see this work draw more attention to Retrace, including the theoretical characterization in Sec.7.