The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.
I'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. 1, 3, 7, 11). The rest of it might be more suited for an "introduction to training CNNs" course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).
The paper states that "it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer", which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given ("the nature of design patterns is that they only apply some of the time") does not excuse making such sweeping claims. This should probably be removed.
"We feel that normalization puts all the layer's input samples on more equal footing, which allows backprop to train more effectively" (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with "we feel", as this doesn't seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper.
The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don't think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)2 + h(x)3 + ... is not a particularly interesting object, it is just a nonlinear function of x.
Overall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.