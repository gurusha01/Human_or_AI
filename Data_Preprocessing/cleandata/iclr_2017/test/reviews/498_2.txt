This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout "inference gap" which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.
One relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I'd expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.
MC dropout on page 8 is not defined, please define.
On page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?
From Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.
Couple of typos:
- Pg. 2 " … x is he input …" -> " … x is the input …"
- Pg. 5 " … as defined in (1), is …" -> ref. to (1) is not right at two places in this paragraph
Overall it is a good paper, I think should be accepted and discussed at the conference.