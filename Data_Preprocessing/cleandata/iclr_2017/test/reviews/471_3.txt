The paper discuss a "batch" method for RL setup to improve chat-bots.
The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. 
I find the writing clear, and the algorithm a natural extension of the online version.
Below are some constructive remarks:
- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:
- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.
- section 2.2:
   sentence before last: s' is not defined. 
   last sentence: missing "... in the stochastic case." at the end.
- Section 4.1 last paragraph: "While Bot-1 is not significant ..." => "While Bot-1 is not significantly different from ML ..."