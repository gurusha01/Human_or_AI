The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.
I have two main concerns. One is the lack of comparisons to similar recently proposed methods - "Learning Step Size Controllers for Robust Neural Network Training" by Daniel et al. and "Learning to learn by gradient descent by gradient descent" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?
My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list: