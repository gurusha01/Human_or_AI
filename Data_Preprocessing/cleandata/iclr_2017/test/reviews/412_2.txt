A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.
Summary:
———
I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.
Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.
Clarity: Some of the derivations and intuitions could be explained in more detail.
Originality: The suggested idea is reasonable albeit heuristics are required.
Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.
Details:
————
1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.
2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (