This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.
Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in "A Simple Word Embedding Model for Lexical Substitution" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.
In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: 
*