this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. 
First, considering the related work [1,2] the proposed approach brings marginal novelty. Especially
Context Encoders is just a small improvement over word2vec. 
Experimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].
[1]