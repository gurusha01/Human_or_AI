Clarity: The novel contribution of the paper --- Section 2.2 --- was very difficult to understand. The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used.
Originality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering. It sounds like a good idea. 
Significance: The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. (I.E. The previous state-of-the-art, FABIA, is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)
Quality: The experiments are high-quality. 
Comments:
1) The introduction claims that this method is much faster than FABIA because the use of rectified units allow it to be run on GPUs. It is not clear to me how this works. How many biclusters can be supported with this method? It looks like the number of biclusters used for this method in the experiments is only 3-5?
2) The introduction claims that using dropout during training increases sparsity in the bicluster assignments. This seems like a reasonable hypothesis, but this claim should be supported with a better argument or experiments.
3) How is the model deep? The model isn't deep just because it uses a relu and dropout.