This paper presents extensions to previous work using embeddings for modeling Knowledge Bases and performing Q&A on them, centered around the use of multivariate gaussian likelihood instead of inner products to score attention. This is supposed to allow more control on the attention by dealing with its spread.
This is a dense paper centered around a quite complicated model. With the supplementary material, this makes a 16p paper. It might be clearer to make 2 separate papers: one on KB completion and another one on Q&A.
I like the idea of controlling the spread of the attention. This makes sense. However, I do not feel that this paper is convincing enough to justify its use compared to usual inner products.
For several reasons:
- These should be more ablation experiments to separate the different pieces of the model and study their influence separately. The only interesting point in that sense is Table 8 in Appendix B. We need more of this. 
- In particular, a canonical experiments comparing Gaussian interaction vs inner product would be very useful. 
- Experiments on existing benchmarks (for KB completion, or QA) would help. I agree with the authors that it is difficult to find the perfect benchmark, so it is a good idea to propose a new one (WorldCup2014). But this should come in addition to experiments on existing data.
- Table 11 of Appendix C (page 16) that compares TransE and TransGaussian for the task of link prediction on WordNet can be seen as fixing the two points above (simple setting on existing benchmark). Unfortunately, TransGaussian does not perform well compared to simpler TransE. This, along with the poor results of TransGaussian (SINGLE) of Table 2, indicate that training TransGaussian seems pretty complex, and hence question the actual validity of this architecture.