This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.
The authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR '16), may be appropriate as well.
The method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.
The evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the "task" is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.
In the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.
Lastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.
In summary, here are the pros and cons of this paper:
Cons
- The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task
- Only one experimental set-up that evaluates learned policy with multi-task state representation
- No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems
Pros: 
- This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches
- Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful
- Experimentally validated on two toy tasks. One task shows improvement over baseline approaches
Thus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.
Lastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:
Approach:
Could this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.
Experiments:
One additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the "known car position" baseline (which is also useful in its own right).
Does the "observations" baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.
If there are aliasing issues with the images, why not just use higher resolution images?