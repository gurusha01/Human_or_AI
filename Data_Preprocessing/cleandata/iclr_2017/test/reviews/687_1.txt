The authors have put forward a sincere effort to investigate the "fundamental nature of learning representations in neural networks", a topic of great interest and importance to our field.  They propose to do this via a few simplistic pruning algorithms, to essentially monitor performance decay as a function of unit pruning.  This is an interesting idea and one that could potentially be instructive, though in total I don't think that has been achieved here.  
First, I find the introduction of pruning lengthy and not particularly novel or surprising.  For example, Fig 1 is not necessary, nor is most of the preamble section 3.3.0.  The pruning algorithms themselves are sensible (though overly simplistic) approaches, which of course would not matter if they were effective in addressing the question.  However, in looking for contributions this paper makes, an interesting, pithy, or novel take on pruning is not one of them, in my opinion.
Second, and most relevant to my overall rating, Section 4 does not get deeper than scratching the surface.  The figures do not offer much beyond the expected decay in performance as a percentage of neurons removed or gain value.  The experiments themselves are not particularly deep, covering a toy problem and MNIST, which does not convince me that I can draw lessons to the broader story of neural networks more generally.  
Third, there is no essential algorithmic, architectural, or mathematical insight, which I expect out of all but the most heavily experimental papers.