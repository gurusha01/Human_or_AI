Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:
- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. 
- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). 
- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the paper can be solved in a few second using a single core CPU. Hogwild! is suitable for sparse dataset because of its asynchronized nature. On data that are very sparse, the proposed approach is only slightly better or is worse than Hogwild. For dense dataset, it is unclear why we need to use SYMSGD instead of simply parallelizing the gradient computation using GPUs. Put them together, the experiment results are not convincing.