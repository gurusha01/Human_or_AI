Paper Summary
This paper makes two contributions -
(1) A model for next step prediction, where the inputs and outputs are in the
space of affine transforms between adjacent frames.
(2) An evaluation method in which the quality of the generated data is assessed
by measuring the reduction in performance of another model (such as a
classifier) when tested on the generated data.
The authors show that according to this metric, the proposed model works better
than other baseline models (including the recent work of Mathieu et al. which
uses adversarial training).
Strengths
- This paper attempts to solve a major problem in unsupervised learning
  with videos, which is evaluating them.
- The results show that using MSE in transform space does prevent the blurring
  problem to a large extent (which is one of the main aims of this paper).
- The results show that the generated data reduces the performance of the C3D
  model on UCF-101 to a much less extent than other baselines.
- The paper validates the assumption that videos can be approximated to quite a
  few time steps by a sequence of affine transforms starting from an initial
frame.
Weaknesses
- The proposed metric makes sense only if we truly just care about the performance
  of a particular classifier on a given task. This significantly narrows the
scope of applicability of this metric because arguably, one the important
reasons for doing unsupervised learning is to come up a representation that is
widely applicable across a variety of tasks. The proposed metric would not help
evaluate generative models designed to achieve this objective.
- It is possible that one of the generative models being compared will interact
  with the idiosyncrasies of the chosen classifier in unintended ways.
Therefore, it would be hard to draw strong conclusions about the relative
merits of generative models from the results of such experiments. One way to
ameliorate this would be to use several different classifiers (C3D,
dual-stream network, other state-of-the-art methods) and show that the ranking
of different generative models is consistent across the choice of classifier.
Adding such experiments would help increase certainty in the conclusions drawn
in this paper.
- Using only 4 or 8 input frames sampled at 25fps seems like very little context
  if we really expect the model to extrapolate the kind of motion seen in
UCF-101. The idea of working in the space of affine transforms would be much
more appealing if the model can be shown to really generated non-trivial motion
patterns. Currently, the motion patterns seem to be almost linear
extrapolations.
- The model that predicts motion does not have access to content at all. It only
  gets access to previous motion. It seems that this might be a disadvantage
because the motion predictor cannot use any cues like object boundaries, or
decide what to do when two motion fields collide (it is probably easier to argue
about occlusions in content space).
Quality/Clarity
The paper is clearly written and easy to follow. The assumptions are clearly
specified and validated. Experimental details seem adequate.
Originality
The idea of generating videos by predicting motion has been used previously.
Several recent papers also use this idea. However the exact implementation in
this paper is new. The proposed evaluation protocol is novel.
Significance
The proposed evaluation method is an interesting alternative, especially if it
is extended to include multiple classifiers representative of different
state-of-the-art approaches. Given how hard it is to evaluate generative models
of videos, this paper could help start an effort to standardize on a benchmark
set.
Minor comments and suggestions
(1) In the caption for Table 1: ``Each column shows the accuracy on the test set
when taking a different number of input frames as input" - ``input" here refers
to the input to the classifier (Output of the next step prediction model). However
in the next sentence ``Our approach maps 16 \times 16 patches into 8 \times 8
with stride 4, and it takes 4 frames at the input" - here ``input" refers to
the input to the next step prediction model. It might be a good idea to rephrase
these sentences to make the distinction clear.
(2) In order to better understand the space of affine transform
parameters, it might help to include a histogram of these parameters in the
paper. This can help us see at a glance, what is the typical range of these
6 parameters, should we expect a lot of outliers, etc.
(3) In order to compare transforms A and B, instead of ||A - B||^2, one
could consider A^{-1}B being close to identity as the metric. Did the authors
try this ?
(4) "The performance of the classifier on ground truth data is an upper bound on
the performance of any generative model." This is not strictly true. It is
possible (though highly unlikely) that a generative model might make the data
look cleaner, sharper, or highlight some aspect of it which could improve the
performance of the classifier (even compared to ground truth). This is
especially true if the the generative model had access to the classifier, it
could then see what makes the classifier fire and highlight those discriminative
features in the generated output.
Overall
This paper proposes future prediction in affine transform space. This does
reduce blurriness and makes the videos look relatively realistic (at least to the
C3D classifier). However, the paper can be improved by showing that the model can
predict more non-trivial motion flows and the experiments can be strengthened by
adding more classifiers besides than C3D.