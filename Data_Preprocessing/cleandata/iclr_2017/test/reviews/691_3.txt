This paper presents a valuable new collection of video game benchmarks, in an extendable framework, and establishes initial baselines on a few of them.
Reward structures: for how many of the possible games have you implemented the means to extract scores and incremental reward structures? From the github repo it looks like about 10 -- do you plan to add more, and when?
"rivalry" training: this is one of the weaker components of the paper, and it should probably be emphasised less. On this topic, there is a vast body of (uncited) multi-agent literature, it is a well-studied problem setup (more so than RL itself). To avoid controversy, I would recommend not claiming any novel contribution on the topic (I don't think that you really invented "a new method to train an agent by enabling it to train against several opponents" nor "a new benchmarking technique for agents evaluation, by enabling them to compete against each other, rather than playing against the in-game AI"). Instead, just explain that you have established single-agent and multi-agent baselines for your new benchmark suite.
Your definition of Q-function ("predicts the score at the end of the game given the current state and selected action") is incorrect. It should read something like: it estimates the cumulative discounted reward that can be obtained from state s, starting with action a (and then following a certain policy).
Minor:
* Eq (1): the Q-net inside the max() is the target network, with different parameters theta'
* the Du et al. reference is missing the year
* some of the other references should point at the corresponding published papers instead of the arxiv versions