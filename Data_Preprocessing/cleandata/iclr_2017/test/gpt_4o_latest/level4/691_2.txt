This paper presents a novel reinforcement learning environment called "The Retro Learning Environment," which integrates with the open-source LibRetro API to provide access to a variety of emulators and their associated games. The environment is positioned as a more general alternative to the Atari 2600 Arcade Learning Environment, with the first supported platform being the SNES and five initial games (with the potential for additional consoles and games in the future). The authors argue that SNES games present greater challenges compared to Atari games due to their more intricate graphics, AI, and game mechanics. The paper evaluates several DQN variants in experiments and proposes a benchmarking approach where learning algorithms compete against each other in multiplayer games.
I appreciate the effort to move toward more complex games than those available on the Atari 2600 and the flexibility of an environment that allows for the addition of new consoles and games. This is a promising direction. However, with the recent release of platforms like OpenAI Universe and DeepMind Lab, I am uncertain whether there is an immediate need for another such framework. Additionally, the legal concerns surrounding the use of ROMs for emulated games remain a potential issue. While this has not caused significant problems for Atari games, it could become more contentious as the community shifts to more advanced and commercially relevant games, particularly those from Nintendo, which still generates revenue from its intellectual property.
Beyond the introduction of the environment, the inclusion of DQN benchmarks on five games is a useful contribution but does not provide substantial novelty. The authors also highlight a "new benchmarking technique" that involves algorithms competing against each other rather than against in-game AI. However, this claim seems overstated, as the concept of pitting AIs against one another has been a central aspect of numerous AI competitions for decades. Similarly, the observation that reinforcement learning algorithms tend to specialize to their opponents is not particularly surprising.
In summary, while this paper is reasonable, I do not believe it offers enough significant contributions to warrant acceptance at a major conference. That said, the proposed environment may still carve out a niche in the increasingly crowded space of game-playing frameworks.
Additional comments:
- The paper contains numerous typos, far too many to list individually.
- The claim that Infinite Mario "still serves as a benchmark platform" appears inaccurate, as my understanding is that it was discontinued due to legal concerns raised by Nintendo.
- The statement that "RLE requires an emulator and a computer version of the console game (ROM file) upon initialization rather than a ROM file only. The emulators are provided with RLE" seems similar to ALE, which also requires the Stella emulator (provided with ALE). Clarification on how this differs would be helpful.
- There are no reported DQN or DDDQN results for Super Mario, which seems like a notable omission.
- It is unclear whether the results for F-Zero in Figure 2 include reward shaping or not.
- The reference to Du et al. appears incomplete and should be corrected.