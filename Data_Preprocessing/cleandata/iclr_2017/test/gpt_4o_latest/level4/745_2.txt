Overall, the paper presents an interesting idea and is well-written with a clear motivation. However, I believe it is not yet ready for publication at ICLR for the following reasons:
- The paper does not align closely with the theme of representation learning and may be more appropriate for a general machine learning or data mining conference.
- The proposed method is limited in scope, as it only applies to a narrow class of models and cannot be extended to widely-used formulations such as SVMs, logistic regression, or neural networks. The motivation for using SGD specifically for this type of formulation is unclear. For models like linear regression, the authors should compare their approach with linear programming methods. Furthermore, it is not evident why a parallel algorithm is necessary for linear regression problems, which are relatively straightforward to solve unless the dataset is extremely large (see the next point).
- The datasets used in the experiments are relatively small and appear to serve only as proof-of-concept examples. Most of the datasets considered can be solved within seconds on a single-core CPU. While Hogwild! is well-suited for sparse datasets due to its asynchronous nature, the proposed approach shows only marginal improvements or even underperforms compared to Hogwild! on very sparse data. For dense datasets, it is unclear why SYMSGD would be preferred over simply parallelizing gradient computations on GPUs. Taken together, the experimental results are not sufficiently compelling.