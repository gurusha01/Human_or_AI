This paper presents a novel approach for predicting future (unseen) video frames based on a set of known past frames. The proposed method utilizes a convolutional neural network (CNN) that operates in the space of affine transformations, diverging from most related works that focus on pixel or optical flow representations. In essence, the network takes as input a set of affine transformations that describe the motion of patches in the past frames and outputs a corresponding set of affine transformations predicting the motion of patches in future frames.
To achieve this, the authors make a few simplifying assumptions, primarily that a sequence of frames can be adequately modeled within their patch-affine framework. This assumption is reasonable, as many optical flow studies are grounded in similar hypotheses, such as modeling flow as a smoothly varying affine field (e.g., "Locally affine sparse-to-dense matching for motion and occlusion estimation" by Leordeanu et al., "EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow" by Revaud et al., and "Optical Flow With Semantic Segmentation and Localized Layers" by Sevilla-Lara et al.). These methods are considered state of the art, lending credibility to the validity of this approach. Furthermore, reformulating the prediction task to focus on motion rather than raw pixel prediction is a sound decision. The motion space (in this case, patch-affine transformations) is significantly smaller than the image space, making the problem more tractable, especially for high-resolution videos.
While I acknowledge the strengths of the paper, I find several critical flaws that need to be addressed:
- Lack of Quantitative Comparisons: The authors' decision not to compare their method with previous approaches in terms of pixel prediction error is problematic. Although the authors argue that the evaluation metric is imperfect, this does not justify completely avoiding quantitative comparisons. For example, the frames generated by the network on datasets like the moving digits dataset (Figure 4) appear reasonable and could be directly compared with other works. The absence of such comparisons raises concerns about the robustness of the proposed method.
- Issues with the Proposed Metric: The newly introduced evaluation metric has several shortcomings. First, action classification is performed using C3D, which is not a state-of-the-art method for this task. Second, the metric does not directly evaluate the network's claimed capability of next-frame prediction. Instead, it assesses whether another network, which was not trained to distinguish between real and synthetic frames, can classify actions from the predicted frames. This proxy metric is only weakly related to the actual task being evaluated. Moreover, training a network for a task other than the one it is ultimately evaluated on is conceptually flawed.
- Unclear Motion Estimation: The method for estimating the affine motion of patches is described vaguely, with only a brief mention that the problem is solved globally rather than treating each patch independently. Estimating the motion of all patches is akin to solving the optical flow problem, which remains an active area of research. The paper does not adequately address the potential inaccuracies in motion estimation, which are evident in the provided videos where motion is sometimes incorrectly estimated. Since the proposed approach relies heavily on accurate motion input, the impact of motion estimation errors on the network's performance should be thoroughly discussed. Additionally, the patch-affine hypothesis breaks down when patches are large enough to encompass multiple objects with conflicting motion, as observed in UCF101 videos.
- Training and Loss Function: The network is not trained end-to-end, which is a significant limitation. Instead of minimizing a loss in the actual output space (frame pixels) where exact ground-truth data is available, the network minimizes the difference between noisy ground-truth and output affine transformations. While it is true that pixel-based MSE loss can lead to blurry results, alternative loss functions, such as the gradient loss proposed by Mathieu et al., have been shown to mitigate this issue. As the authors themselves note, minimizing a loss in the transformation space introduces artifacts, including significant underestimation of motion. This is evident in Figure 5, where the predicted frames are nearly indistinguishable from the input frames.
- Insufficient Comparison with Related Work: The proposed approach is not adequately compared to prior methods. For instance, the approach is closely related to "SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY" by Taraucean et al., ICLR'15, which also predicts in the motion space. Experimental results should include a direct comparison with this work.
- Unfair Comparison with Optical Flow: The comparison with optical flow methods is not rigorous. For example, the approach of Brox et al. is over a decade old and does not represent the current state of the art. Additionally, assuming a constant flow for all frames is an overly simplistic baseline. At the very least, basic extrapolation techniques could be employed to incorporate the flow of all input frame pairs rather than relying solely on the last one. Overall, the proposed method is not benchmarked against sufficiently challenging baselines.
- Response to Reviewer Question: I disagree with the authors' response to a reviewer's question. Denoting the ground-truth frames as {X0, X1, ...} and the predicted frames as {Y1, Y2, ...}, the authors fail to adequately address... [Note: The continuation of this point is missing in the original review and would need to be completed based on the specific question and response.]
In conclusion, while the paper introduces an interesting approach and makes reasonable assumptions, it suffers from significant shortcomings in evaluation, clarity, and comparison with prior work. Addressing these issues would greatly strengthen the contribution and impact of the proposed method.