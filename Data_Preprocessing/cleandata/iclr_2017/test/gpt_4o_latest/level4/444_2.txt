This paper introduces a pattern extraction method aimed at both interpreting what a trained LSTM has learned and enabling the development of a hand-coded algorithm that achieves performance comparable to the LSTM. Promising results are demonstrated on a single dataset with one model architecture, but it remains uncertain how well this method will generalize. Nevertheless, the approach appears to offer a valuable tool for understanding and debugging models.
The questions in WikiMovies seem to be template-generated, which likely makes this pattern-matching approach effective. However, the experiments do not clarify whether the method can be extended to other Q&A tasks where answers may consist of free-form text rather than being substrings of the document. Does the model require the answer to form a continuous span within the original document?
The method also appears to exhibit limitations in handling specific word types, such as numbers or entity names. While these could potentially be encoded in the word embeddings, the algorithm's description suggests that the approach relies on an external entity detector. Does this imply that the method cannot identify entities solely from the decomposition of the LSTM's output? The results involving 'manual pattern matching,' where explicit year annotations are used, seem to indicate that the automatic method struggles with certain word types.
Additionally, it would be beneficial to include an attention-based model as a baseline for comparison alongside the gradient-based baseline.
Minor comments:
- The definitions of P and Q are missing.
- Some references appear to be incorrect, such as in section 5.1: 'in 1' should be 'in table 1.' Similarly, above section 7: 'as shown in 3' and in section 7.1.
- In the paragraph preceding section 6.3, 'adam' should be corrected to 'Adam.'