Clarity: The primary contribution of the paper, as presented in Section 2.2, was challenging to comprehend. The notation, particularly the use of l, p, and m, appeared inconsistent, and I remain uncertain about the specifics of the model being employed.
Originality: The novelty of the work lies in the application of the RFN model—incorporating ReLU non-linearity and dropout training—to the biclustering problem. This approach seems promising.
Significance: The proposed algorithm has the potential to serve as a valuable tool for unsupervised data modeling. The authors provide a compelling argument for its significance, particularly as it outperforms the widely-used state-of-the-art method, FABIA, while addressing some of its practical limitations.
Quality: The experimental results are of high quality.
Comments:
1) The introduction asserts that this method is significantly faster than FABIA due to the use of rectified units, which enable GPU acceleration. However, the mechanism behind this claim is unclear. Additionally, how many biclusters can this method support? The experiments seem to use only 3-5 biclusters—why is this the case?
2) The introduction suggests that employing dropout during training enhances sparsity in the bicluster assignments. While this hypothesis appears reasonable, it requires stronger justification, either through more robust arguments or additional experimental evidence.
3) In what sense is the model deep? Simply incorporating ReLU and dropout does not inherently make the model deep.