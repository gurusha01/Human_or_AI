This paper introduces a principled optimization approach for SGNS (word2vec).
While the method is theoretically elegant, its practical benefits remain unclear. For instance, does employing Riemannian optimization enable faster convergence compared to alternative methods? The empirical evaluation does not demonstrate a significant advantage for RO-SGNS; the 1% improvement on word similarity benchmarks falls within the variability typically caused by hyperparameter tuning (see "Improving Distributional Similarity with Lessons Learned from Word Embeddings," (Levy et al., 2015)). Nonetheless, the theoretical link to Riemannian optimization is compelling and could prove valuable for understanding related techniques in the future.