This paper introduces a novel approach for optimizing the objective function of CNNs. The proposed method employs a layer-wise optimization strategy, wherein at each step, the parameters of a single layer are optimized while keeping the parameters of other layers fixed. The central insight of the paper is that, for a broad class of CNNs, the optimization problem for a specific layer can be reformulated as the optimization of a piecewise linear (PL) function. Interestingly, this PL function optimization aligns with the optimization problem commonly encountered in latent structural SVMs. Leveraging this connection, the paper draws upon ideas from the latent structural SVM literature, particularly the concave-convex procedure, to facilitate the learning of CNN parameters.
Overall, the paper is well-written. Traditionally, CNNs and structural SVMs have been largely distinct research areas. The connection established between CNNs and latent structural SVMs is intriguing and has the potential to bridge the gap between these two communities, enabling the transfer of ideas and techniques across them.
That said, the proposed method has some notable limitations. 1) It is constrained to layer-wise optimization, which is essentially a coordinate descent algorithm. In modern deep learning, layer-wise optimization is not considered a competitive strategy for training CNNs. By adopting a layer-wise approach, the method sacrifices some advantages of global optimization (e.g., gradient descent). While the layer-wise strategy ensures that each coordinate descent step improves the objective, it remains unclear how the trade-off between the gains (guaranteed improvement at each step) and losses (suboptimal global optimization) plays out in practice. 2) The paper focuses on improving the optimization of the CNN objective. However, it is well-known that achieving a better objective value does not necessarily translate to a better model, particularly due to the risk of overfitting. While standard CNN training with SGD and backpropagation does not always guarantee objective improvement (unlike the proposed method), this might actually be beneficial, as it could help mitigate overfitting. After all, the ultimate goal of learning is not merely to optimize the objective but to develop a model with strong generalization performance.
The experimental evaluation is somewhat weak.  
1) The experiments are conducted solely on CIFAR-10, which is a relatively small dataset by contemporary standards. CNNs are typically evaluated on large-scale datasets like ImageNet, and it is unclear whether the conclusions drawn in this paper would hold in such settings.  
2) The comparisons are made against a restricted version of SGD (without batch normalization, dropout, etc.). While the paper justifies this by stating that the focus is on optimization, it is important to note that SGD is not designed solely to optimize the objective function. Instead, it aims to strike a balance between reasonable optimization and preventing overfitting. Thus, comparing the proposed method to SGD purely in terms of optimization performance may not be particularly meaningful.