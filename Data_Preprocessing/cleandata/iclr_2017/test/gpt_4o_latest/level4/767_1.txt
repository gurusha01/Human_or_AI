The authors propose a novel approach for adaptively determining the step size in SGD by framing the learning rate as an action within an MDP, where the reward corresponds to the change in the loss function. This method is evaluated against widely used adaptive first-order optimization techniques for training deep neural networks (e.g., Adagrad, Adam, RMSProp). While the results are intriguing, it is challenging to make a direct, apples-to-apples comparison. Below are some specific observations:
- What is the computational cost of the actor-critic algorithm compared to other optimization methods? The paper does not include plots showing the wall-time of optimization, even though the success of methods like Adagrad is largely attributed to their wall-time efficiency rather than the number of iterations.
- Why is only a single learning rate learned? To ensure a fair comparison with other popular first-order methods, why not train an independent RL model for each parameter, akin to how these methods adaptively adjust the learning rate for each parameter individually?
- Given that learning is inherently a non-stationary process, whereas RL algorithms typically assume a stationary environment, what justifies the expectation that an RL algorithm would be effective in determining a learning rate?
- In Figure 6, how does the proposed method compare to techniques like early stopping? It is possible that the actor-critic method appears to generalize better simply because it is less effective at optimization, thereby reducing overfitting.