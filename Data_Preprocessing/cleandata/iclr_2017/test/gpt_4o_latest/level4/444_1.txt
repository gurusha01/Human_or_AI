EDIT: The authors have made significant and thorough revisions to the paper, addressing many of my earlier concerns. The updated version is also much clearer and easier to follow. I now recommend this revised version for acceptance and have increased my score accordingly.
This paper introduces a novel method for interpreting LSTM models, which are often criticized for their lack of interpretability. Specifically, the authors propose a technique to decompose the LSTM's predictions for a QA task into word-level importance scores. These scores are then used to generate patterns that can identify answers through a straightforward matching algorithm. On the WikiMovies dataset, the proposed pattern-matching approach achieves accuracy levels comparable to those of a standard LSTM, demonstrating the effectiveness of the method.
I find the motivation behind this work compelling, as the interpretability of LSTMs remains an open challenge. Additionally, the strong performance of the pattern-matching approach was unexpected and impressive. However, certain aspects of the pattern extraction process remain insufficiently detailed, and the evaluation is limited to a highly specific task where predictions are made at the word level. Therefore, while I recommend the paper as a weak accept in its current form, I encourage the authors to provide further clarification on their approach, as I believe it has significant potential to benefit the NLP research community.
Comments:
- Please provide a more detailed introduction to the specific QA tasks being addressed before section 3.3, as it is unclear at that stage that the answers are entities within the document.
- Section 3.3: Is the softmax output predicting a binary 0/1 value (e.g., whether a word is the answer or not)?
- Section 3.3: Could you clarify what the P and Q vectors represent? Are you referring to a transformation of the hidden state into a 2-dimensional vector for binary classification?
- How does the performance of the pattern-matching method vary with different cutoff constant values?
- Section 5.2: Are there any questions in the dataset where the answers are not entities?
- How might the proposed approach be adapted for tasks where predictions are not made at the word level? For example, could it be extended to sentence-level tasks such as sentiment classification?