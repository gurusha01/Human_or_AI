This paper introduces a method for embedding data instances into a low-dimensional space while preserving a certain notion of similarity.
However, while the paper frames this idea as novel, it is essentially what most pre-trained embeddings (such as auto-encoders or word2vec) already achieve: mapping items into a low-dimensional space that inherently captures their similarities. Even in the specific context of word/context embeddings, the proposed approach lacks novelty, as it closely resembles one of the similarity functions described in "A Simple Word Embedding Model for Lexical Substitution" (Melamud et al., 2015). The authors should refine their novelty claims and clearly situate their work within the context of prior research.
Furthermore, I believe the evaluation could be improved. There are numerous established benchmarks for evaluating word embeddings in context, such as:  
*