This paper presents a correction technique designed to combine updates from multiple SGD processes, ensuring statistical equivalence to the sequential approach.
Comments  
1) The proposed method is both novel and intriguing, as it enables updates to be corrected even when they are delayed.  
2) However, the proposed theory is restricted to the square loss setting (with a linear update rule), which limits its applicability. Extending the technique to general objective functions and deep neural network settings would significantly enhance its relevance and appeal to the ICLR community.  
3) The method introduces the need to maintain a dimensionally reduced combiner matrix, which increases computational complexity. While the authors suggest that this overhead can be mitigated using SIMD support for symbolic updates, it is worth noting that standard SGD updates might also benefit from SIMD, particularly when dealing with dense datasets.
Overall, despite the practical limitations highlighted in points 2) and 3), the correction rule proposed in this work has potential value for researchers focused on scaling up learning. I encourage the authors to explore extending the method to non-linear objective functions, as this would make the contribution more compelling for the ICLR audience.