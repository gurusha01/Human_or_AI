This paper introduces a compare-aggregate model tailored for NLP tasks that involve semantic comparison of text sequences, such as question answering and textual entailment.  
The core structure of the model involves applying a convolutional neural network (aggregation) following an element-wise operation (comparison) performed on the attentive outputs of the LSTMs.  
A key contribution of this work lies in the comparison, where various methods for matching text sequences are evaluated, with element-wise subtraction and multiplication shown to consistently deliver superior performance across four distinct datasets.  
However, the main limitation is that the work is somewhat incremental and lacks significant innovation. Including a qualitative analysis of how subtraction, multiplication, and other comparison functions behave with different types of sentences would have added more depth and interest.