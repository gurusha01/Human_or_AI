The paper establishes a set of guidelines for designing convolutional neural network (CNN) architectures aimed at image processing and computer vision tasks. In essence, it resembles a review article summarizing contemporary CNN architectures while also introducing a few novel architectural concepts inspired by these guidelines. These proposed ideas are empirically tested on CIFAR-10 and CIFAR-100, but their performance on these datasets appears to be relatively weak (Table 1), leaving their value unclear.
I am uncertain whether compiling a set of rules derived from prior work justifies publication as a research paper. While summarizing such observations could be beneficial, particularly for newcomers to the field, given CNNs' dominance in computer vision for several years, much of the content seems to rest on common sense (e.g., rules 1, 3, 7, 11). The remainder might be more appropriate for an "introduction to training CNNs" course or a blog post. Additionally, the paper seems disproportionately focused on recent, incremental work, with significant attention devoted to the various ResNet variants.
The paper claims that "it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer," which is incorrect. As previously discussed regarding design pattern 5, the response provided ("the nature of design patterns is that they only apply some of the time") does not justify making such broad generalizations. This statement should likely be removed.
The sentence "We feel that normalization puts all the layer's input samples on more equal footing, which allows backprop to train more effectively" (section 3.2, 2nd paragraph) uses vague language open to multiple interpretations and requires clarification. Additionally, beginning the sentence with "we feel" is unusual, as this is not a matter of opinion. Such claims should be supported by experimental evidence or measurements. Similar issues with imprecise language appear throughout the paper.
The connection between Taylor series and the proposed Taylor Series Networks seems weak, and the naming does not seem appropriate. The resulting function is not even a polynomial, as each term represents a distinct function -- f(x) + g(x)2 + h(x)3 + ... is merely a nonlinear function of x, without any particularly compelling properties.
In summary, the paper comes across as a loosely organized collection of ideas and observations, with experimental results that fail to convincingly support its contributions.