In their response, the authors reference and compare their work to others, such as "Learning to Learn by Gradient Descent by Gradient Descent." However, the objectives of that work and the current study differ significantly. The former introduces a novel optimization algorithm, which is not the focus of this paper. Additionally, while Bayesian hyper-parameter optimization methods are designed to optimize multiple hyper-parameters, this work focuses solely on tuning a single hyper-parameter.
The network architecture employed for the CIFAR-10 experiments is outdated, and the reported performance is substantially lower than that of any recent publications in the field. Consequently, the comparisons presented in the paper are not valid. If the authors aim to demonstrate the advantages of their method, they should evaluate it using state-of-the-art network architectures to determine whether their claims hold in more competitive settings.
As previously noted, the additional computational cost of hyper-parameter optimization methods is only justified if the approach can achieve state-of-the-art results across multiple modern datasets.
In conclusion, while the concept of using an actor-critic network as a meta-learner is intriguing, the specific application proposed in this work appears to lack practical utility. The reported results are limited, making it difficult to draw meaningful conclusions about the method's effectiveness.