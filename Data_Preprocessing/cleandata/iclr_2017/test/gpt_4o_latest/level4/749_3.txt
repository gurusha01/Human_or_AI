The authors undertake the ambitious task of identifying a set of design patterns for modern deep learning architectures—essentially recurring themes observed in the literature. One could describe this as a distributed representation of deep architectures.
There are two aspects of this paper that I found particularly commendable. First, the authors provide an excellent review of recent works, which highlighted for me just how much I had overlooked in this rapidly evolving field. Second, the paper serves a "community service" role by helping newcomers establish a "coordinate system" for understanding deep architectures. This contribution could potentially outweigh the introduction of yet another optimization trick, which is the focus of many other submissions.
That said, I believe this work is still incomplete. While the idea behind the project is promising, the authors have not yet executed it effectively.
To begin with, the rationale behind the selection of these 14 patterns is unclear. For example, Maxout (pattern 14) is just one of many nonlinearities (e.g., PreLU, ReLU, etc.), and it is unclear how it holds the same level of generality as something like "3 Strive for simplicity."
Similarly, some patterns are overly vague, such as "Increase symmetry," which is supported by statements like "we noted a special degree of elegance in the FractalNet." It is unclear how this translates into a design pattern that can be applied to new architectures or whether it is specific to FractalNet alone.
Other patterns are phrased in an unconventional manner, such as "7 Cover the problem space," which seems to refer to dataset augmentation, or "6 Over-train," which lacks any supporting references. The authors appear to connect "over-train" to regularization in the preceding text, but this is inconsistent with their description of "over-train" as "training a network on a harder problem to improve generalization." If "harder problem" refers to adding a regularization term, the authors risk confusing inexperienced readers by conflating regularization with something that sounds like overfitting—the exact opposite concept.
The extensions proposed in Section 4 also feel somewhat disconnected. Specifically:
- It is unclear how Taylor Series networks relate to any of the design patterns introduced earlier in the paper.
- The purpose of the text between Sections 4.1 and 4.1.1 is ambiguous. Is this another architectural innovation? If so, why is it not grouped under 4.1.2 or 4.1.0?
- Most critically, the paper does not clearly explain how these design patterns can be practically applied to create new architectures.
For instance, the authors mention that they derive the "freeze-drop-path" variant from "symmetry considerations" related to "drop-path." Is this an application of the "increase symmetry" pattern? If so, how does "freeze-drop-path" achieve greater symmetry than "drop-path"? Can this be articulated concretely, or is it merely an intuitive guess? If it is the latter, it does not align with my understanding of applying a pattern. If it is the former, the explanation is missing.
What I would have appreciated more (and would like to see in a revised version) is a table that cross-references "design patterns" with "deep networks," showing which networks implement which patterns.
Additionally, much of the prior work is described in cryptic language. Providing minimal explanations of the mechanisms behind these alternative approaches would greatly enhance the paper's accessibility.