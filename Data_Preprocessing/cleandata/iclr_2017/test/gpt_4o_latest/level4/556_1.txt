The paper focuses on gaining a deeper understanding of the optimization landscape in deep learning, specifically when examined using various optimization algorithms, and consequently characterizes the behavior of these algorithms. The work extensively builds upon the approach of Goodfellow et al. (2015). However, I find it challenging to discern the paper's contributions. For instance, is it truly surprising that different algorithms converge to different solutions when initialized identically? It would be beneficial if the authors included such foundational intuition within the paper. Additionally, the question I posed to the reviewers—regarding how the findings of this work could inform future research on optimization in deep learning—remains unanswered. This, in my view, represents a critical gap. For example, while there are likely numerous ways to adapt the approach of Goodfellow et al. (2015) and similar studies to develop intriguing visualization techniques for deep learning, the key question is: how does this advance our ability to design better algorithms, enhance our intuition about the general structure of the optimization landscape, or contribute more broadly to the field? 
Overall, this is an interesting paper, but I am reasonably confident that it would be more suitable for a journal than for this conference. As a suggestion, it might be insightful and valuable, even as a sanity check, to plot the eigenspectra of the solutions obtained by the algorithms to examine the order of critical points identified.