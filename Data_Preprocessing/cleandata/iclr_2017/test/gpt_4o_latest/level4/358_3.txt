The authors present a modified version of the variational autoencoder (VAE) that incorporates dataset-level latent variables. The concept is well-motivated and effectively articulated. In my view, the primary contribution of this work lies in extending beyond the relatively straightforward graphical model framework of traditional VAEs and introducing more complex and intriguing structures to the deep learning field.
Comments:
- I am uncertain about the appropriateness of the term "statistician" in this context. Learning an approximate posterior over summary statistics is not the only conceivable method for summarizing a dataset using a neural network. For instance, one could adopt a maximum likelihood approach, among others. Overall, the paper might benefit from greater clarity by avoiding the introduction of new terminology like "statistic network" and instead using the more precise term "approximate posterior."
- The experimental results are compelling, and I appreciate the authors' response to my inquiry regarding "one-shot generation." However, I still believe the language requires refinement, particularly at the end of page 6. My interpretation of Figure 5 is as follows: An input set is provided, the approximate posterior over the context vector is computed, and samples from this posterior are then used to generate data points via the forward model. I would appreciate clarification on the following points:
(a) Are the data point-specific vectors z generated directly from the forward model, or are they sampled from the approximate posterior? 
(b) While I agree that the generated samples are of high quality, this observation is qualitative and lacks quantification. A key advantage of VAEs over GANs is the ability to compute log-probabilities in a principled manner. To evaluate "one-shot generation" performance rigorously, it would be valuable to report metrics such as log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that the log-probability performance of these models, compared to a standard VAE without the context latent variable, would be noteworthy. I still see no compelling reason to exclude these evaluations.