This paper explores the application of eligibility traces in conjunction with recurrent DQN agents. Similar to other recent advancements in deep reinforcement learning, the authors employ the forward view framework proposed by Sutton and Barto to make eligibility traces feasible for use with neural networks. Experimental results on the Atari games Pong and Tennis demonstrate that eligibility traces outperform standard Q-learning.
The manuscript is well-written, and the investigation of eligibility traces in deep reinforcement learning is a relatively underexplored area. However, the experimental scope is too narrow and fails to address some of the most compelling research questions.
As highlighted in the review comments, prior work has established that n-step returns generally outperform 1-step returns, both in classical reinforcement learning and in more recent applications involving deep networks. For instance, [1] demonstrates that employing n-step returns in the forward view with neural networks yields significant performance improvements on both Atari and TORCS. Their approach to n-step Q-learning combines returns of varying lengths in expectation, whereas eligibility traces achieve this explicitly. This paper does not provide a comparison between eligibility traces and n-step returns, instead merely showing that traces in the forward view improve performance on two Atari games. This is not a particularly impactful result. A more meaningful contribution would involve demonstrating whether eligibility traces offer improvements over methods already known to perform well with neural networks.
The second contribution claimed by the paper is the demonstration of the strong influence of optimization. However, as with eligibility traces, it is difficult to draw robust conclusions from experiments conducted on only two games with fixed hyperparameter settings. Similar findings have already been established in prior work through more comprehensive experimental studies. One could argue that the results presented here highlight the importance of hyperparameter selection rather than the optimization algorithm itself. Without a systematic exploration of optimization hyperparameters, it is challenging to make definitive claims about the relative advantages of the proposed methods.
[1] "Asynchronous Methods for Deep Reinforcement Learning," ICML 2016.