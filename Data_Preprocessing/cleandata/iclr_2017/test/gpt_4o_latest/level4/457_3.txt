Interesting concept, but the work feels incomplete. The model size reduction achieved does not match the significant factors reported in one of your references (Song 2016), where a 5-bit quantization is applied alongside pruning, resulting in an overall 49x reduction in VGG model size without accuracy degradation. Incorporating pruning into your approach could potentially yield comparable reductions (or even better, given your 4-bit quantization with no accuracy loss), but this needs to be demonstrated in the paper. As it stands, making a meaningful comparison is challenging.