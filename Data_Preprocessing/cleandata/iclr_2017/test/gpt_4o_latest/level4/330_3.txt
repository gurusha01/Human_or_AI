This paper introduces a method for learning document embeddings by summing the embeddings of constituent words, which are jointly trained and subjected to random dropout ('corruption') during training. While the individual components of the model are not particularly novel, the approach results in an efficient algorithm for document representation with strong empirical performance.
The joint training of word and document embeddings is a well-explored concept, as is the idea of representing a document as the sum of its word embeddings (e.g., see '"The Sum of Its Parts": Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Additionally, the corruption mechanism employed here is essentially standard dropout applied to the input layer. Combined with a word2vec-style loss function and training framework, the paper offers limited contributions in terms of novelty.
However, the method is highly efficient at generation time, requiring only the averaging of word embeddings rather than a more complex inference process, as seen in models like Doc2Vec. Furthermore, the embedding inherently captures salient global information about the document, specifically the information that supports local-context prediction. For such a straightforward model, the performance on tasks like sentiment analysis and document classification is notably promising.
In summary, while the model lacks significant novelty, its simplicity, efficiency, and strong performance make it a valuable contribution to the field. I recommend its acceptance for wider dissemination and further investigation.