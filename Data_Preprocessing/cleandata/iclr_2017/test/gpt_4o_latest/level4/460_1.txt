This paper investigates the off-policy learning of actor-critic algorithms with experience replay, addressing an important and challenging problem aimed at enhancing the sample efficiency of reinforcement learning methods. The authors tackle this issue by proposing a novel approach to truncating importance weights, introducing a modified trust region optimization, and incorporating the retrace method. The integration of these techniques demonstrates strong performance on Atari and MuJoCo benchmarks in terms of improving sample efficiency. My primary concern is understanding the individual contributions of each technique to the overall performance improvement. Conducting experiments to isolate and evaluate the gains from each of these components would provide valuable insights.