This paper introduces an innovative approach for deriving rule-based classifiers from trained LSTM models. The method is applied to a factoid question-answering task, where it is shown that the extracted rules achieve performance comparable to the original LSTM. The analysis of these rules provides insights into the features that the LSTM model captures.
Understanding and visualizing the computations performed by RNNs to uncover the functions they compute is a critical research area. Such analyses can shed light on the limitations of RNNs and inform strategies for their improvement. While the proposed approach has some limitations in flexibility—each rule is represented as an ordered sequence of words—the authors explore three scoring methods for identifying salient words (state-difference, cell-difference, and gradient). Their method achieves comparable performance, indicating that the extracted rules closely approximate the behavior of the RNN. The results are particularly intriguing, as most of the extracted rules consist of only two or three words.
It would be valuable to investigate how this approach generalizes to other natural language processing tasks, such as machine translation, where the extracted rules might differ significantly.
Other comments:
- Equation (12) appears to be over-parameterized with two vectors, $P$ and $Q$. The same functionality can be achieved with a single vector, which becomes evident when both the numerator and denominator are divided by $e^{P h_t}$.
- Section 4.1: If this section focuses solely on the forward LSTM, please make this explicit in the text.
- In Equation (13), define $c_0 = 0$ for clarity.
- Equation (13) and Equation (15) are identical. Please verify if this is an error.
- In Table 1, the third column should have the word "film" highlighted.
- Replace "are shown in 2" with "are shown in Table 2."
- To address issues with number representation, consider replacing each digit with the hashtag symbol ().