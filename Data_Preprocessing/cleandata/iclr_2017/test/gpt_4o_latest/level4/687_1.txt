The authors have made a commendable effort to explore the "fundamental nature of learning representations in neural networks," a subject of significant interest and relevance to our community. They aim to achieve this by employing a set of relatively simple pruning algorithms to examine performance degradation as a function of unit pruning. While the idea is intriguing and holds potential value, I do not believe the paper fully realizes this potential.
Firstly, the introduction to pruning feels overly detailed without offering much novelty or unexpected insights. For instance, Figure 1 appears unnecessary, as does much of the preamble in Section 3.3.0. The pruning algorithms themselves are reasonable, albeit quite basic, which would be acceptable if they effectively addressed the central research question. However, in terms of contributions, I do not find the paper to provide an innovative, concise, or particularly fresh perspective on pruning.
Secondly, and most critically in terms of my overall evaluation, Section 4 only scratches the surface of the topic. The figures primarily illustrate the anticipated decline in performance as neurons are pruned or gain values are removed, without offering deeper insights. The experiments are limited in scope, focusing on a toy problem and MNIST, which fails to convince me that the findings generalize to broader questions about neural networks.
Finally, the paper does not deliver any significant algorithmic, architectural, or mathematical insights, which I consider essential for all but the most empirically focused studies.