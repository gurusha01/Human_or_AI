This paper introduces a valuable new collection of video game benchmarks within an extendable framework and establishes initial baselines for a subset of them.
Reward structures: For how many of the possible games have you implemented mechanisms to extract scores and incremental reward structures? From the GitHub repository, it appears to be approximately 10. Do you plan to expand this, and if so, when?
"Rivalry" training: This is one of the weaker aspects of the paper and would benefit from reduced emphasis. There exists a substantial body of (uncited) multi-agent literature on this topic, as it is a well-studied problem setup—arguably more so than reinforcement learning itself. To avoid potential controversy, I suggest refraining from claiming any novel contributions in this area. For example, the claims about "a new method to train an agent by enabling it to train against several opponents" and "a new benchmarking technique for agent evaluation, by enabling them to compete against each other, rather than playing against the in-game AI" seem overstated. Instead, focus on clarifying that you have established both single-agent and multi-agent baselines for your new benchmark suite.
Your definition of the Q-function ("predicts the score at the end of the game given the current state and selected action") is inaccurate. A more precise definition would be: it estimates the cumulative discounted reward obtainable from state s, starting with action a (and subsequently following a specific policy).
Minor comments:
- Equation (1): The Q-net inside the max() operation refers to the target network, which uses different parameters, denoted as θ'.
- The Du et al. reference is missing the publication year.
- Some references should be updated to cite the corresponding published papers rather than the arXiv preprints.