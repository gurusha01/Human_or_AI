This paper presents a mechanism for active learning utilizing convolutional neural networks (CNNs). However, I hesitate to fully endorse the authors' characterization of these networks as "deep," given that the architecture described consists of only 2 hidden layers with 20 filters each. The proposed active learning criterion employs a greedy selection strategy based on variational free energy, accompanied by a series of approximations.
The manuscript is occasionally challenging to follow due to (a) numerous grammatical errors and (b) imprecise notation in certain sections (e.g., on page 5, line 1, the function f is referenced but not previously defined). While I lean toward recommending acceptance, my endorsement is weak due to these grammatical issues. If the paper is accepted, these should be addressed in the final version, ideally with the assistance of a native English speaker.
The topic is compelling, and the paper achieves its primary objective of demonstrating a proof of concept for active learning in CNNs, albeit on toy datasets. The new results on uncertainty sampling and curriculum learning are intriguing, but I am puzzled by the performance discrepancies on the USPS dataset. Specifically, uncertainty sampling performed exceptionally well on MNIST—outperforming the authors' proposed method—yet performed poorly on USPS. A detailed explanation for this discrepancy would be valuable.
I also have a question regarding the necessity of first sampling a larger subset \( D \subset U \), from which active learning is then applied. Is this step purely for computational efficiency, or does it contribute to improved results? If the latter, it would be helpful to see a comparison with results obtained when this step is omitted.
Some of the approximations described are quite intricate; is the code for your implementation available? Additionally, in Figures 1 and 2, could you clarify what is meant by "groundtruth"?
Finally, Section 5.2 examines the time complexity of the proposed approach, noting that it can take up to 30 seconds to select elements for a single minibatch. How does this compare to the time required to update the model using that minibatch via backpropagation?