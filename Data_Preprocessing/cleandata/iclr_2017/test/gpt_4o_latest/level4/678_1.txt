First, I would like to apologize for the delay in providing this review.
Summary: This paper investigates a series of experiments aimed at transferring training for a specific reading comprehension model (AS Reader) from a large, synthetic dataset to a smaller, target dataset.
Here is my understanding of the experiments on transfer learning, though I am not entirely certain about all the details:
1. The model is trained on the large synthetic dataset and subsequently evaluated on the smaller target datasets (Section 4.1).
2. The model is pre-trained on the large synthetic dataset as in the previous experiment, but it is then fine-tuned using a small subset of examples from the target dataset before being tested on the remaining target examples. Multiple models are trained using different subsets of fine-tuning examples, and their results are compared to those of models that are randomly initialized and then fine-tuned (Section 4.2).
3. The model is again pre-trained on the large synthetic dataset. The architecture of the model includes an embedding component and an encoder component. In this experiment, each component is alternately reset to a random initialization to evaluate the significance of pre-training for each component. The model is then fine-tuned on a small subset of examples from the target dataset and tested on the remaining target examples (Section 4.3).
One aspect that makes the paper challenging to follow is the composition of the test set, which includes multiple subtasks. At times, the paper reports the mean performance across these subtasks, while at other times, it focuses on the performance for specific subtasks. Additionally, there are instances where the reported results appear to reflect the mean performance across multiple models. I recommend reporting standard deviations to provide a clearer picture of the variability in the results. Furthermore, the term "best validation" needs to be clarifiedâ€”what exactly does this refer to?
This is an interesting and modest contribution. However, the clarity of the presentation could be improved, perhaps by simplifying the experimental setup. The most compelling conclusion, in my opinion, is presented at the end of Section 4.1, where the nuanced differences between the datasets are discussed.
Minor Comments:
- Some acronyms are not explained: GRU, BT, CBT.
- Typographical errors: "benfits" on page 2, "subsubset" on page 6.