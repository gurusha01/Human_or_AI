This paper presents dropout as a latent variable model (LVM). Using this framework, the authors examine the dropout "inference gap," defined as the discrepancy between a network's output during training (where a dropout instance is applied to each training sample) and testing (where expected dropout values are used to scale node outputs). They introduce the concept of expectation linearity and leverage it to derive bounds on the inference gap under certain (mild) assumptions. Additionally, they propose using a per-sample inference gap as a regularizer and provide an analysis comparing the accuracy of models with expectation-linearization constraints to those without.
One minor concern with the LVM perspective on dropout is that it appears to be tailored to probabilistic models, whereas dropout is broadly applicable to deep networks. However, I anticipate that the regularizer-based formulation of dropout would remain effective even in non-probabilistic models.
The term MC dropout on page 8 is not defined; please provide a definition.
On page 9, it is stated that the proposed regularizer enables standard dropout networks to achieve better results than Monte Carlo dropout. However, this seems to hold true only for the MNIST dataset and not for CIFAR.
From Tables 1 and 2, it also appears that MC dropout achieves the best performance across tasks and methods, though it is computationally expensive. Including a discussion on the computational efficiency of different dropout procedures alongside the accuracy results would be highly beneficial.
A couple of typos:
- Pg. 2: " … x is he input …" -> " … x is the input …"
- Pg. 5: " … as defined in (1), is …" -> The reference to (1) is incorrect in two instances within this paragraph.
Overall, this is a strong paper that I believe should be accepted and discussed at the conference.