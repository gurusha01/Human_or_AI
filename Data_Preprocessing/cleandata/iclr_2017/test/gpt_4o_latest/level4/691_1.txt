The paper introduces a novel platform for reinforcement learning research, named the Retro Learning Environment (RLE). While the primary focus is on Super Nintendo games, the authors assert that the interface is compatible with various other systems, including the Atari Learning Environment (ALE). Benchmarking results are provided for standard algorithms across five new Super Nintendo games, along with some findings using a novel "rivalry metric."
Standardized evaluation frameworks, such as public datasets, competitions, and environments, have historically played a pivotal role in advancing AI and machine learning research. A notable example is the Atari Learning Environment (ALE), which has become a widely adopted benchmark for algorithm comparison. In this context, the RLE has the potential to make a meaningful contribution to the field by introducing new, challenging domains for research exploration.
However, the primary emphasis of this paper is on presenting the RLE framework and highlighting the importance of exploring new domains. The experimental results themselves are based on existing algorithms. While there are some novel findings, such as the benefits of reward shaping and policy shaping (e.g., biasing toward moving right in Super Mario) during learning, these results are somewhat unsurprising, as the utility of domain knowledge is well established. The rivalry training concept is intriguingâ€”training against a different opponent leads to overfitting to that opponent and forgetting how to play against the in-game AI. However, it is peculiar that the evaluation is ultimately conducted against the in-game AI rather than the rival opponent.
Unfortunately, the section of the paper detailing the scientific results, particularly the rivalry training, lacks polish, which is disappointing. Overall, the paper does not evoke much excitement.
A more substantial scientific contribution accompanying the introduction of this new environment would have been desirable. While it is unclear whether such a contribution is essential for publication, it is also uncertain whether ICLR is the most appropriate venue for this work, given its primary focus on the new codebase. Alternative venues, such as mloss.org or the journal track associated with JMLR, might be better suited for this type of contribution.