The responses to the pre-review questions are not particularly compelling. Specifically, regarding the question about dataset density and the rationale for subsampling, the authors stated that subsampling is a common practice in recommender systems research, as evidenced by the cited papers. However, this explanation does not provide a strong justification for why subsampling is appropriate in this specific case. Moreover, it fails to address the critical question of "how the results would differ without subsampling," which is a question that could have been directly answered.
Given the paper's strong emphasis on addressing the cold-start problem, it seems counterintuitive to subsample the data in a way that reduces sparsity.
That said, the remaining pre-review questions appear to have been addressed satisfactorily.
The primary contribution of the paper lies in proposing user and item embedding methods to capture complex non-linear interactions between users and items. While this is conceptually aligned with recent work on deep recommender systems, the network formulation does exhibit some differences.
Overall, this is a reasonably well-constructed paper that contributes to an important area of research. However, there are notable shortcomings that should be addressed, including:
1) The evaluation methodology is unconventional. Recall@M is the sole metric reported, which is not a standard evaluation measure in recommender systems research. At a minimum, additional performance metrics, such as RMSE or AUC, should be included for completeness, even if the results on these metrics are not particularly strong.
2) Considering that the contribution is relatively straightforward (i.e., applying a new model to the "standard" recommender systems task), it is unfortunate that the authors rely on unusual data sampling. This should be a scenario where results can be reported directly against competing methods using exactly the same datasets and evaluation metrics for the most equitable comparison.
Without addressing these issues, it remains unclear whether the reported performance improvements are genuinely attributable to the proposed method or are instead influenced by the choice of datasets and loss functions.