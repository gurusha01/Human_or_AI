This paper introduces a parallel mechanism for the stochastic gradient descent (SGD) method, specifically for cases where gradients can be computed via linear operations (e.g., least squares linear regression and polynomial regression problems). The primary goal is to replicate the performance of sequential SGD by employing a proposed sound combiner. To enhance the efficiency of this combiner, the authors also incorporate a randomized projection matrix for dimensionality reduction. Experimental results demonstrate that the proposed method achieves better speedup compared to existing approaches such as Hogwild! and Allreduce.
However, I believe there may be a fundamental misunderstanding regarding the computational complexity of SGD.
"The combiner matrix M generated in the proposed method can be quite large and computationally expensive. Sequential SGD updates the weight vector w and requires O(f) space and time, where f is the number of features. In contrast, M is an f × f matrix, leading to a space and time complexity of O(f²) for parallel SGD. Practically, this would necessitate O(f) processors to achieve constant speedups, which is infeasible for datasets with thousands or millions of features."
I disagree with the claim that O(f²) space and time complexity is required for updating Mi * v, where v is an f-dimensional vector. Since Mi is a low-rank matrix of the form (I - ai ai'), the complexity and space requirements can be reduced to O(f) by computing it as O(v - ai (ai' v)). If M_i is expressed as the product of n rank-1 matrices, the complexity and space requirements become O(fn). In the context of this paper, n should be significantly smaller than f. This raises serious concerns about the validity of the authors' assumptions, experiments, and strategies, as they appear to be based on an incorrect understanding of the space and time complexity of SGD.
Additionally, it is unclear why the proposed method achieves speedup. The paper does not clearly explain which computations are performed in parallel or why the sequential algorithm would yield speedup if M_i * v is computed in the most efficient manner.
To improve the clarity and theoretical rigor of this paper, I recommend the following changes:
- Provide a detailed computational complexity analysis for each step of the proposed algorithm.
- Include a convergence rate analysis (not just convergence analysis) to evaluate how dimensionality reduction impacts computational complexity.