Dear authors,
The response provided by the authors has addressed some of my earlier concerns. However, I still have the following questions:
-- The response identifies a key contribution as a novel formulation: the word embedding learning process is divided into two steps. In step 1, a low-rank matrix X is obtained using Riemannian optimization, and in step 2, X is factorized into two matrices (W, C). The authors claim that this approach outperforms prior methods that directly optimize over (W, C). However, given that the final output (the factors) is the same, could the authors provide further intuition and justification as to why the proposed method achieves better performance?
From my perspective, despite the difference in parameterization, the first step of your approach and prior methods (e.g., SGD) both aim to optimize over low-rank matrices. While Riemannian optimization does eliminate the rotational degree of freedom (as discussed in Section 2.3 with the invertible matrix S), I am not entirely convinced that this is the sole reason for the observed improvement. Providing learning curves of the optimization objectives could help clarify whether Riemannian optimization is indeed more effective in this context.
-- Another detail I could not locate clearly is the following: you mention that a drawback of other methods is that their factors W and C do not directly encode similarity. Did you attempt to multiply the factors W and C obtained from other optimizers, then apply the factorization method described in Section 2.3, and subsequently use the resulting W for downstream tasks? I am curious whether this would lead to significant differences in performance.
Overall, I find it intriguing to see advanced optimization techniques applied to machine learning problems. However, from a machine learning perspective, the paper would be strengthened by providing a more comprehensive comparison and discussion, as outlined above. That said, as my expertise does not lie in NLP, I defer to other reviewers to assess the significance of the experimental results.