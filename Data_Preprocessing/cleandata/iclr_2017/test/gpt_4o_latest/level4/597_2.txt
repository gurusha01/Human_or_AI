This paper presents a methodologically intriguing approach, and based solely on its methodological contributions, I would lean toward recommending acceptance. However, the paper's broad claims of outperforming existing baselines for TSP (Traveling Salesman Problem) are not substantiated. The local search algorithm LK-H is able to solve all the instances presented by the authors to optimality within seconds on a CPU, whereas the authors' method produces clearly suboptimal results even after 25 hours on a GPU.
Given the evident superiority of the LK-H local search method, it is concerning that the authors chose to leave Figure 1 unaltered. The "local search" line in the figure refers to a subpar implementation by Google rather than the widely recognized LK-H method. For instance, at NIPS, I observed this figure being used in a talk (though I cannot recall by whom, and I do not believe it was the authors), with the narrative suggesting that "RNNs now clearly outperform local search." Such a figure is naturally prone to being misinterpreted in this way, and it is the authors' responsibility to prevent such misconceptions. 
The appropriate course of action, upon recognizing the true strength of LK-H, would have been to update the "local search" line to coincide with the "Optimal" line, thereby illustrating that their method remains significantly inferior to proper local search. Instead, the authors left the figure unchanged, perpetuating the misleading implication that their method outperforms local search. While this may not have been intentional, it risks misleading readers who might not delve deeply into the details. To experts outside the deep learning community, this could appear as an exaggerated and evidently incorrect claim. Consequently, despite the methodological novelty, I must recommend rejection.
---
Update after rebuttal and revisions:
I remain conflicted about this paper.
On one hand, the paper is exceptionally well-written, and the proposed method is both innovative and promising. I am personally interested in exploring and potentially improving upon it in the future. From this perspective, the paper merits acceptance.
On the other hand, the authors initially employed weak baselines, which made their method appear disproportionately strong. Through multiple rounds of reviewer feedback and revisions, it has become clear that the authors' method is significantly inferior to the state of the art. While this outcome was not surprising, the authors appear reluctant to fully acknowledge it. For example, they state, "We find that both greedy approaches are time-efficient and just a few percents worse than optimality." While this may be factually correct, it is widely understood in the TSP community that achieving results within a few percent of optimality is relatively straightforward. The real challenge lies in closing those final few percentage points. 
As an aside, it seems the authors may not have stopped LK-H after it found the optimal solution, as they do with their own method upon reaching a local optimum. LK-H is an anytime algorithm, meaning it can find the optimal solution in milliseconds and a near-optimal solution even faster, regardless of how long it continues to run.
Despite the toned-down claims in the revised version, my optimism about this paper diminished upon revisiting the section on Knapsack solving. The earlier version of the paper at least acknowledged two simple heuristics—ExpKnap and MinKnap—that yield optimal solutions. These were described as follows: "Two simple heuristics are ExpKnap, which employs branch-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be obtained by quantizing the weights to high precision and then performing dynamic programming with pseudo-polynomial complexity (Bertsimas & Demir, 2002)." The earlier version also demonstrated that these heuristics were already optimal, similar to the authors' method.
However, in a revision between December 11 and 14, this paragraph and the corresponding results for ExpKnap and MinKnap were removed. Instead, the authors introduced two weaker baseline methods (random search and greedy). This appears to be an attempt to find baselines that fail on these relatively simple instances, thereby making their method look better in comparison. Including random search seems particularly unhelpful, as it is not a method anyone would seriously consider for TSP. It is akin to comparing results on MNIST against a decision stump—while it may inflate the perceived performance of the proposed method, it is not meaningful. Although the results for the greedy baseline are somewhat informative, omitting the stronger baselines (ExpKnap and MinKnap) and their discussion is problematic. The revised table now misleadingly suggests that the authors' method outperforms all baselines. If the goal is simply to highlight bold numbers for one's own approach, this might achieve that, but it is not a responsible representation of the state of the art. Additionally, why did the authors not attempt to use the same OR-tools solver from Google that they employed for TSP? OR-tools appears to directly support Knapsack problems.
I appreciate the authors' statement that "Our model and training code will be made available soon." However, I would like clarification on what "soon" entails—will it be during the review period or in time for the conference?
In Table 3, what are the missing performance values for RL pretraining with 10,000 batches for Sampling T=1 and T=T? The performance improvement from 100 to 1,000 batches for RL pretraining Sampling T=T is much greater than for RL pretraining AS (e.g., 5.79 → 5.71 vs. 5.74 → 5.71 for TSP50). This suggests that RL pretraining Sampling T=T might outperform RL pretraining AS when using 10,000 samples. This could alter the qualitative conclusions in Table 2 and the overall findings of the paper. The authors briefly mention, "We sample 1,000 batches from a pretrained model, after which we do not see significant improvement," but given the steeper improvement gradient for Sampling T=T, I would be more convinced by seeing the results for 10,000 batches.
Lastly, what is the precise difference between RL pretraining Sampling T=1 and T=T*? If this is explained in the text, I may have missed it.