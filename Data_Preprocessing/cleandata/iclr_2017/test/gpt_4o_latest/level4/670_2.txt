This paper extends the method of Jonschkowski & Brock to learn state representations for multiple tasks, as opposed to a single task. The research direction of learning representations for multiple tasks is intriguing and remains relatively underexplored. The proposed approach involves learning separate representations and policies for each task, with task detection being performed automatically and integrated into the neural network.
The authors claim that their method is orthogonal to multi-task learning, even though the ultimate goal—solving multiple tasks—is shared. It would be valuable to see a more detailed discussion on this distinction in the paper, as highlighted during the pre-review question phase. Additionally, referencing other multi-task learning works, such as policy distillation and actor-mimic (both ICLR '16), could provide helpful context.
The method jointly learns a task classifier and a state representation learner by employing a differentiable gating mechanism to regulate information flow. A task coherence prior is introduced for the gating mechanism, ensuring that the learned task classifier maintains temporal coherence. This structural addition is what allows the method to outperform the standard, single-task approach.
The evaluation is conducted on two toy experimental scenarios. The first involves controlling one of two cars to navigate a track. In this scenario, task detection is straightforward, and the learned state representation is linear in the observation. The paper demonstrates the effectiveness of the proposed approach by comparing the learned policies to a standard non-multitask baseline, with sufficient evidence to support the method's utility.
In the second navigation scenario, only the state representation is qualitatively analyzed, without presenting the resulting control policy or comparisons to other learned state representations. Since the utility of multi-task state representation learning lies in its ability to improve control, the paper should also evaluate control performance in this scenario, using the same comparisons as in the first experiment. Without this evaluation, the experiment feels incomplete.
Finally, to meet the standards of a high-impact venue like ICLR, the method requires more comprehensive evaluation across a broader range of scenarios to demonstrate its generalizability. The current experiments do not establish the method's applicability to more complex tasks, such as scaling beyond MNIST-level images to 3D or real-world images, or handling higher-dimensional control problems. Evaluating the method in such scenarios is crucial, as scaling often introduces unforeseen challenges. If scaling is indeed straightforward, conducting and including these experiments should also be straightforward.
In summary, the pros and cons of the paper are as follows:
Cons:
- The approach does not explicitly leverage shared information across tasks to improve learning and requires separate policies for each task.
- Only one experimental setup evaluates the learned policy with multi-task state representation.
- No experiments are conducted on more realistic scenarios, such as 3D environments or high-dimensional control tasks.
Pros:
- The approach enables the use of a single network for multiple tasks, which is often not the case for transfer and multi-task learning methods.
- It introduces a novel way to learn a single policy for multiple tasks, incorporating a task coherence prior to ensure meaningful task classification.
- It is experimentally validated on two toy tasks, with one task demonstrating improvements over baseline methods.
Conclusion:
The paper would merit a higher rating if it included an evaluation of the control policy for the navigation task and demonstrated the method's applicability to a more challenging and realistic scenario.
Minor Comments and Suggestions:
Approach:
- Could this method be combined with other state representation learning techniques, such as those using autoencoders?
Experiments:
- An additional useful comparison would be to evaluate performance in a single-task setting (e.g., controlling only the red car) to establish an upper bound on policy performance. Does the learned multi-task policy achieve comparable performance? This upper bound would be tighter than the "known car position" baseline, which is also valuable.
- Does the "observations" baseline eventually achieve the same performance as the LRP approach? It would be helpful to clarify whether the proposed method primarily accelerates learning or also enables better final performance.
- If aliasing issues are present in the images, why not use higher-resolution images?