This paper demonstrates that augmenting a deep reinforcement learning (RL) approach with auxiliary tasks enhances performance in navigation within complex environments. Specifically, the authors employ the A3C algorithm for the RL problem, while simultaneously training the agent on two auxiliary tasks: an unsupervised depth prediction task and a self-supervised loop closure classification task. Although the use of auxiliary tasks to improve the training of models, including RL agents, is not novel, the primary contribution of this work lies in leveraging tasks that promote the learning of intrinsic spatial and motion representations, leading to notable improvements in maze navigation tasks.
The paper is well-written, the experimental results are compelling, and the utility of the auxiliary tasks for the problem is evident. However, the contribution is somewhat incremental in light of prior research on RL for navigation and the use of auxiliary tasks. The work would gain broader appeal with a more extensive analysis or insights, either on the optimal combination of tasks for visual navigation (e.g., exploring the utility of other visual or geometry-based tasks) or on the general role of auxiliary tasks in RL. As it stands, the paper provides a valuable demonstration of the benefits of geometry-based auxiliary tasks for navigation but may have limited appeal due to its relatively narrow focus.