The paper explores the use of Grassmannian SGD to optimize the skip-gram negative sampling (SGNS) objective for improved word embedding learning. However, it remains unclear why the proposed optimization method offers any advantage over the standard vanilla SGD-based approach. Neither method is supported by theoretical guarantees, and the empirical results demonstrate only marginal improvements. Moreover, the core concept—the projector splitting algorithm—has been widely applied in various machine learning contexts, as evidenced by prior work such as Vandereycken's contributions to matrix completion and Sepulchre's work on matrix factorization.
The computational cost analysis of the two methods is insufficiently detailed. For example, how computationally expensive is the SVD in equation (7)? Efficient low-rank updates to the SVD can be performed, with a rank-one update requiring O(nd) operations. What, then, is the computational cost per iteration of the proposed method?