There is significant ongoing interest in compressing neural network models. One prominent direction has explored the use of low-precision representations for model weights, sometimes reducing precision to just 1 or 2 bits. However, these methods have typically resulted in notable accuracy degradation. This paper introduces an iterative quantization method, where network weights are quantized progressively: the largest weights (by absolute value) are quantized and fixed, while the remaining unquantized weights are allowed to adjust to mitigate any induced errors. The experimental results demonstrate that this approach is highly effective, producing models with 4-bit or 3-bit weights while maintaining virtually the same accuracy. Although accuracy drops slightly at 2 bits, the performance still surpasses that of other quantization techniques.
In summary, the paper is well-written, the proposed technique appears novel, the experiments are comprehensive, and the results are highly convincing. I therefore recommend acceptance. However, the paper would benefit from an additional round of editing to improve writing style and grammar. Additionally, the explanation of the pruning-inspired partitioning strategy could be made clearer. For instance, the 50% splitting ratio is mentioned only in a figure caption and not explicitly discussed in the main text.