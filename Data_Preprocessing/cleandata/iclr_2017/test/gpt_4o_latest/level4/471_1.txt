This paper adapts neural conversational models to the batch reinforcement learning framework. The core idea is to leverage human scoring data for dialogue model responses, though such scores are costly to obtain. Consequently, it is logical to employ off-policy learning, where a base policy is trained on unsupervised data, deployed to gather human scores, and subsequently refined offline using those scores.
Although the overall contribution is incremental (applying off-policy actor-critic methods to dialogue generation), the approach is well-justified, and the paper is clearly written and easy to follow.
My primary concern lies with the dataset used for evaluation (restaurant recommendations), which is relatively small, comprising only 6,000 conversations. This is several orders of magnitude smaller than datasets commonly used in the field (e.g., Twitter, the Ubuntu Dialogue Corpus) for dialogue generation tasks. It is somewhat surprising that RNN-based chatbots (without additional structural enhancements) can produce reasonable utterances with such limited data. While Wen et al. (2016) demonstrated success with a similarly small restaurant dataset, their method directly maps dialogue states to surface forms rather than relying on an embedding-based representation of context. Therefore, it remains unclear whether the methods proposed in this paper would yield similar improvements when applied to much larger unsupervised datasets.
References:
Wen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. "A Network-based End-to-End Trainable Task-oriented Dialogue System." arXiv preprint arXiv:1604.04562 (2016).