This paper introduces a framework for generating document representations.  
The core concept involves representing a document as the average of its word embeddings, with a data-dependent regularization mechanism that emphasizes informative or rare words while constraining frequent words to values near zero.  
Empirical evaluations on sentiment analysis and document classification tasks demonstrate that the proposed approach achieves the lowest error rates when compared to baseline document embedding methods.  
Although I appreciate the motivation to identify an effective way to encode documents as vectors, the paper lacks substantial technical novelty.  
Most of the techniques employed are not original, and the primary appeal lies in the simplicity and computational efficiency of the method.  
Therefore, I would recommend evaluating the framework on a broader range of tasks to establish its superiority as a method for learning document representations.  
Regarding the RNN-LM, is the language model trained to minimize classification error, or is it optimized purely as a language model? Additionally, did you use the final hidden state as the document representation, or the average of all hidden states?  
A commonly adopted approach for document representation involves using a bidirectional LSTM and concatenating the final hidden states.  
It would be beneficial to compare the proposed method against this technique on tasks like document classification or sentiment analysis to provide a more comprehensive evaluation.