Summary:  
———  
The paper proposes a layer-wise optimization method for CNNs with ReLU activations and max-pooling, framed as a sequence of latent structured SVM problems. By employing CCCP-style optimization, the authors guarantee a monotonic decrease in the overall objective function.  
I find the insights presented in the paper intriguing but not entirely persuasive. Several claims are emphasized that may not hold in practical scenarios (e.g., lack of convergence guarantees due to mini-batch training). Additionally, some statements could be validated more rigorously (e.g., whether monotone convergence is advantageous or detrimental), and the experimental evaluation requires further elaboration. Overall, I believe the paper needs additional refinement to deliver a more compelling narrative.  
Quality: While some techniques are promising, their presentation could be improved for better clarity and intuition. At times, comparisons seem inconsistent, such as contrasting backpropagation with CCCP.  
Clarity: Certain derivations and underlying intuitions would benefit from more detailed explanations.  
Originality: The proposed approach is reasonable, though it relies on heuristic methods.  
Significance: The limited experimental setup makes it difficult to assess the broader significance of the work at this stage.  
Details:  
————  
1. While the theoretical guarantees for the optimization procedure are appealing, their practical utility remains unclear. For instance, mini-batch optimization disrupts monotonic decrease, which undermines the emphasis placed on this property in the paper. In my view, the current experimental evaluation does not adequately support the claims made.  
2. Conceptually similar work exists, such as that of B. Amos and J. Kolter on Input-Convex Deep Networks.