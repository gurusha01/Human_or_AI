In light of the detailed author responses and the subsequent updates to the manuscript, I am increasing my score to an 8 and reaffirming my support for this paper. I believe it will stand out as one of the strongest non-traditional applied deep learning contributions at ICLR, garnering significant interest and attention from attendees.
---
This paper presents a modern deep learning approach to the task of predicting the medications prescribed to a patient over a certain period, based solely on the sequence of ICD-9 codes assigned to the patient during that same timeframe. The authors frame this as a multilabel sequence classification problem, distinguishing it from language modeling, which typically involves multiclass classification. They employ standard LSTM and GRU architectures with embedding layers to process the sparse categorical inputs, following a methodology similar to that outlined in related work by Choi et al. Using a dataset of approximately 610,000 patient records, the authors demonstrate that RNN models outperform strong baselines, including an MLP, a random forest, and a common sense baseline. The performance gap between the recurrent models and the MLP appears to be statistically significant, given the size of the test set.
Strengths:
- The paper addresses a highly important problem. As the authors note, two of the key promises of electronic health records (EHRs)—which have been widely adopted across the U.S. due to legislative mandates and substantial federal incentives—are improved accuracy of records and reduced medication errors. These benefits have largely failed to materialize, presenting a significant opportunity for data mining and machine learning to make a meaningful impact.
- The manuscript is well-written, with a clear introduction and motivation, a comprehensive discussion of related work, a detailed description of experiments and metrics, and an engaging qualitative analysis of the results.
- The empirical results are robust, with RNNs achieving a clear advantage over strong baselines. This contrasts with some recent related work, such as Lipton & Kale et al. (ICLR 2016), where the performance gap between RNNs and MLPs was relatively small, and Choi et al. (MLHC 2016), which omitted several obvious baselines.
- The discussion is thorough and insightful. The authors' observations regarding the kidney code embedding results are particularly compelling and suggest a promising direction for future research.
Weaknesses:
- The authors make some unconventional choices in data preprocessing and experimental design, most notably their decision to use truncated patient sequences ending at randomly chosen time points rather than full patient sequences. While this does not invalidate their results, it feels somewhat unnatural and is not explained clearly, potentially diminishing the paper's overall impact. Additionally, this choice may limit the RNN's potential advantage.
- Although the chosen metrics are appropriate, they may be difficult for non-experts to interpret in terms of absolute and relative performance. The authors should consider dedicating some space to explaining (1) what level of performance—on each metric—would make the model practically useful in a clinical setting, and (2) whether the performance differences between models are "significant," even in an informal sense.
- The paper does not propose any novel methods, which is a notable limitation for a methods-focused conference like ICLR. Despite this, the empirical results and the application's relevance make the paper strong enough to merit acceptance. However, the authors could enhance its competitiveness by addressing this weakness. For instance, they might explore whether higher-capacity models are more prone to overfitting noisy targets and investigate this hypothesis by analyzing the types of errors made by each model.
Additional Comment:
From a clinical perspective, the paper has a significant limitation: the absence of ground truth labels for missing medications. Both training and testing rely on data with noisy labels. While the authors correctly argue that label noise during training is not a major issue (assuming the noise is random or even class-conditional), the lack of reliable labels during testing could skew the reported metrics. Furthermore, the assumption that label noise is not systemic seems unlikely, given that the data are recorded by human clinicians. The examples in Appendix C support this concern: in Case 1, 7 out of 26 actual medications received probabilities below 0.5. Clinical reviewers might view this issue with skepticism. To address this, the authors will need to devise creative evaluation strategies or invest significant resources in obtaining high-quality labeled data to convincingly demonstrate the model's effectiveness.
In conclusion, I hope this paper is accepted, as I believe it will be of great interest to the ICLR community. However, I remain borderline on whether I would actively advocate for its acceptance. If the authors can address the reviewers' critiques—particularly by investigating overfitting on noisy labels and providing additional insights—I would be willing to raise my score further and advocate for its inclusion.