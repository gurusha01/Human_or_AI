The paper introduces DRNN as a neural decoder specifically designed for tree structures. I find the model architecture appealing due to its two notable improvements over traditional methods: (1) it facilitates bidirectional information flow, incorporating data from both parent nodes and sibling nodes, which is particularly advantageous for tree structures, and (2) it employs a probability distribution to model tree boundaries (e.g., the last sibling or leaf node). This approach eliminates the need for special ending symbols, which tend to increase the parameter space and impose additional learning burdens on the model (as these symbols are shared with other tokens).
The authors evaluate DRNN on two tasks: recovering synthetic trees and recovering functional programs. The model outperforms traditional approaches, such as seq2seq models, in both cases.
However, I find the synthetic tree recovery task less compelling for two reasons: (1) the surface form inherently contains some topological information, which simplifies the task and reduces its difficulty, and (2) as illustrated in Figure 3, the model's performance declines significantly as the number of nodes increases, even when the node count is not particularly large. This raises questions about whether a simple baseline that only captures topological information from the surface string would perform much worse than DRNN. Additionally, in this scenario, DRNN seems unable to fully demonstrate its potential, as the length of the information flow within the model remains relatively short.
The experiments are intriguing, but I believe there are other, more challenging tasks where tree structure information plays a more critical role. For instance, consider the seq2seq parsing model (Vinyals et al., 2014). Could the DRNN proposed in this paper be applied to the decoder side of such a model? Tasks like this could better highlight the potential of DRNN and provide more compelling evidence that architectures like this are superior to traditional alternatives.