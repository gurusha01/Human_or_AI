The paper explores several innovations in deep reinforcement learning (RL) and assesses their impact on solving games in the Atari domain. While the paper is clearly written, it comes across as a collection of the authors' recent techniques rather than presenting a cohesive and compelling narrative. The work will likely appeal to practitioners already working on deep RL methods but may struggle to engage a broader audience.
The claims on page 1 suggest that the approach is both stable and sample efficient. This led me to expect some theoretical analysis supporting these properties. However, since this is an empirical claim, it would be helpful to clarify this distinction in the abstract.
The proposed innovations are grounded in sound methodologies. It is particularly noteworthy that the same approach demonstrates success in both discrete and continuous domains.
The empirical results are reasonably comprehensive. However, including confidence intervals on more of the plots would strengthen the presentation. Additionally, the results do not sufficiently disentangle the contributions of the various innovations, making it difficult to understand the individual impact of each component. For instance, it would be helpful to gain more intuition about why ACER outperforms A3C. Furthermore, it is unclear why the approach achieves only comparable results on discrete tasks but attains state-of-the-art performance on continuous tasks.
The paper provides thorough coverage of related work. It is commendable that this study brings more attention to Retrace, including the theoretical characterization presented in Section 7.