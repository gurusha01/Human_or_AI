This paper introduces an intriguing concept: mitigating the chaotic behavior of RNNs.  
While many papers proposing new RNN architectures tend to prioritize performance improvements while treating the underlying mechanisms of their success as a black-box, this work stands out by providing a clear rationale for why its proposed method might be effective.  
The paper includes extensive comparisons between chaotic systems (e.g., GRUs and LSTMs) and the stable system (the proposed CFN model). However, the reviewer remains unconvinced by the central claim of the paper, which suggests that while chaotic behavior enhances the representational richness of dynamic systems, it also renders them overly unstable. For instance, the paper demonstrates that LSTMs exhibit highly sensitive behavior when even minimal noise is introduced to the input. Despite this sensitivity, LSTMs still perform remarkably well under such chaotic conditions.  
Quantifying model complexity is inherently challenging, and many studies address this by either using the same number of hidden units or selecting models with approximately similar sizes. In this work, experiments were conducted using the same number of parameters for both the LSTM and CFN. However, the reviewer suspects that the CFN may have a significantly simpler computational graph. Building on this idea, could a stable dynamic system be developed that avoids being constrained to a single attractor?  
Another noteworthy aspect is that the CFN layers are updated on different timescales, with the decay rate decreasing as the layer depth increases. Could the authors provide additional statistics on this phenomenon? For instance, what is the average relaxation time of the hidden units at each layer?  
Batch normalization and layer normalization are known to enhance the stability of RNN training. How would a batch-normalized or layer-normalized LSTM behave in comparison? Additionally, implementing batch normalization or layer normalization in a new architecture is often non-trivial. It would be valuable to compare batch-normalized or layer-normalized versions of both the LSTM and CFN.  
The quality of the work is commendable, with clear explanations supported by thorough analyses and proofs. While the performance of the proposed model does not surpass that of LSTMs, the simplicity of the architecture makes it an interesting contribution. However, the reviewer is concerned about the model's potential performance on more challenging tasks, such as machine translation. Figure 4 is particularly compelling, as it illustrates that the hidden units in the second layer of the proposed architecture tend to retain information longer than those in the first layer.