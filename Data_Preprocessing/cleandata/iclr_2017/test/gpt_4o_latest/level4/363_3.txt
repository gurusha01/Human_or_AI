The paper introduces a general framework for addressing natural language understanding tasks involving two distinct textual inputs (e.g., a question and a source text) that can be aligned in some manner. The proposed approach employs soft attention to establish alignments between tokens in the two texts. These alignments, represented as pairs of attention queries and attention results, are then processed through a comparison function to derive representations. These representations are subsequently aggregated into a single vector using a convolutional neural network (CNN), from which the final output is computed. The paper not only positions this method as a viable and effective modeling strategy but also provides a thorough empirical analysis of the model's comparison component.
This work is timely and relevant. Language understanding tasks of this nature represent a significant challenge in NLP and are now becoming tractable through representation learning techniques. The proposed approach is both general and well-motivated, demonstrating its ability to achieve strong results. While the work is somewhat incremental when compared to the authors' prior research or related work by Parikh et al., it offers enough meaningful contributions to merit a strong recommendation for acceptance.
Details:  
- The model, as implemented for tasks involving longer sequences (all except SNLI), does not account for word order. While it performs competitively, this limitation imposes a significant upper bound on its potential performance. Although the paper acknowledges this, it might be worth briefly highlighting this limitation in the introduction or discussion sections.  
- If my understanding is correct, the attention mechanism employed is more closely aligned with the general/bilinear strategy described in Luong et al. (2015) than with the earlier work by Bahdanau et al. It would be appropriate to cite Luong et al. or another directly relevant reference for this strategy.  
- Given the risk of overfitting associated with the Neural Tensor Network (NTN) due to its large number of parameters, did you experiment with a variant that uses an input dimension \( l \) and a smaller output dimension \( m \) (resulting in an \( l \times l \times m \) tensor)?  
- It would be helpful to note that the SubMultNN approach bears a strong resemblance to the sentence-level matching strategy described in the Lili Mou paper you reference.  
- Is there a specific reason for using the same parameters to preprocess both the question and the answer in (1)? These components might benefit from weighting different aspects more heavily, depending on their roles.