This paper presents an actor-critic deep reinforcement learning (RL) framework with experience replay, integrating truncated importance sampling and trust region policy optimization. Additionally, the authors introduce a novel approach called stochastic duelling networks for estimating the critic in continuous action spaces. The proposed method is evaluated on Atari games and continuous control tasks, demonstrating performance comparable to state-of-the-art techniques.
As outlined in the introduction, the primary contributions of this work are the integration of 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks. These enhancements are effective and may prove valuable for advancing future RL research.
That said, the individual contributions feel somewhat incremental. Furthermore, the ACER framework appears significantly more complex and potentially fragile to implement compared to standard deep Q-learning with prioritized replay, which achieves similar performance on Atari games. For the Atari domain specifically, I would still favor prioritized replay due to its simplicity. Additionally, while improving sample efficiency in deep RL is an admirable objective, this goal should ideally be pursued in problem domains where sample efficiency is critical. Unfortunately, the paper evaluates sample efficiency only in the Atari and continuous control domains, where sample efficiency is less relevant. As a result, it remains unclear whether the proposed ACER method would generalize well to tasks where sample efficiency is a key concern.
Specific technical clarifications are needed on the following points:
- Regarding Retrace, do you compute $Q^{ret}$ recursively starting from the end of each trajectory? Please elaborate.
- The derivation of eq. (7) is unclear. Is there a missing approximation (double tilde) symbol?
- In Section 3.1, the paper claims that $Q^{ret}$ provides a lower-variance estimate of the action-value function. If so, why is it not used in eq. (8) for the bias correction term?
- The paper mentions using a replay memory of 50,000 frames to maintain comparability across threads with prior work. However, this is much smaller per thread compared to earlier experiments on Atari games, such as the one million transitions used in "Prioritized Experience Replay" by Schaul et al. This discrepancy could significantly impact the performance of both ACER and competing models. To properly evaluate ACER's improvements over prior methods, the authors should conduct experiments with larger replay memory sizes.
Additional comments:
- Consider moving Section 7 to the appendix.
- The statement "Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability" likely intended to refer to large values of lambda.
- Above eq. (6), specify that the squared error is used.
- Add the missing "t" subscript at the start of eq. (9).
- The explanation of stochastic duelling networks was difficult to follow; please rephrase for clarity.
- Clarify the sentence: "To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games."
- In Figure 2 (Bottom), add labels to the vertical axes for better interpretability.