This paper introduces a novel quantization approach for both weights and activations that eliminates the need for re-training. The proposed method achieves a compression ratio of 20x and a speed-up of 15x on the VGG-16 model. The manuscript is well-written, with a clear presentation of the methodology and results.
However, I have three main concerns. First, the results are not benchmarked against the numerous pruning methods discussed in Section 1.1, making it difficult to evaluate the relative performance of the proposed technique. Second, several existing compression schemes that incorporate pruning, re-training, and vector quantization [e.g., 1, 2, 3] appear to deliver significantly higher accuracies, compression ratios, and speed-ups. Consequently, for practical applications such as deploying networks on low-power, low-memory devices, these alternative methods seem better suited. The advantage of the proposed method—beyond potentially reducing the time required for network compression—remains unclear. Specifically, starting with a pre-trained network and fine-tuning a quantized model may not take substantially more time than the proposed method (though the authors could provide quantitative comparisons to clarify this). Lastly, much of the observed speed-up and memory reduction in the VGG-16 model stems from the three fully connected layers, particularly the final one. The speed-up achieved in the convolutional layers is relatively modest, raising questions about how well the method would perform on architectures with only convolutional layers, such as the Inception model.
[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,