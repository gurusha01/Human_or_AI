This paper presents an interpretation of dropout regularization that, while not entirely novel, remains insufficiently understood. The authors derive valuable theorems that estimate or bound key quantities relevant to analyzing dropout-regularized networks from their proposed perspective. Additionally, they introduce an explicit regularization term designed to have a well-defined impact on these key quantities. In the experimental section, they provide convincing evidence that the proposed regularization achieves the expected effects, supporting the utility and validity of their perspective on dropout.
The proposed regularization also appears to positively influence model performance; however, this is demonstrated only on relatively small-scale benchmark problems. As a result, I am skeptical that this approach will significantly influence how practitioners train models in practice. Nonetheless, their perspective aligns well with the recently proposed concept of "Dropout as a Bayesian approximation," and the insights and theorems presented in this paper could facilitate future research in this direction.