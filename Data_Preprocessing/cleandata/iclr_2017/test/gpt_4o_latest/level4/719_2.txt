The paper investigates a novel approach to classless association, a relaxed form of unsupervised learning where exact class labels are unknown, but there is prior knowledge about examples belonging to the same class. The authors propose a two-stream architecture comprising two neural networks, where each stream processes examples from the same class simultaneously. The streams depend on each other's targets (pseudo-classes or cluster indices), producing an intermediate representation, z, which is constrained to conform to a statistical distribution (assumed to be uniform in this case). The model is trained using an Expectation-Maximization (EM) framework, where the E-step estimates the current statistical distribution based on the output vectors z, and the M-step updates the network weights using z and the pseudo-classes. Experimental results on a reorganized MNIST dataset demonstrate improved performance over traditional clustering algorithms in terms of association accuracy and purity. Additionally, the authors compare their method to a supervised approach, where the proposed architecture understandably performs worse but still shows promising results.
The core idea of the architecture appears to rely on leveraging unlabeled data and ensuring agreement on pseudo-labels generated by the two streams. However, the paper is difficult to follow, and the motivation behind the proposed architecture is buried in the details. It is unclear what the authors aim to achieve by enforcing distribution matching and using the pseudo-targets of each stream. While the statistical distribution of classes is assumed to be uniform, it is not evident how this approach would generalize to other priors or scenarios where the prior is unknown. The current setup requires further justification.
An intriguing experiment would involve testing two examples from the same class, with one drawn from MNIST and the other from Rotated-MNIST or Background-MNIST. This would help clarify how the model handles differences between examples processed by the two streams.
In conclusion, the authors present an innovative approach to classless association that has the potential to be applied to a wide range of many-to-one problems. This is a noteworthy contribution. However, the current version of the paper lacks sufficient theoretical grounding and compelling experimental validation. I look forward to seeing this idea further developed with more extensive experiments on larger datasets and tasks. For now, I recommend this paper for presentation at the ICLR workshop.
Additional comments:
- Typo: Figure 1, second line of the caption: "that" -> "than"
- The necessity of Equation 2 is unclear.
- The batch size M is significantly larger than in classical models, but no explanation is provided for this choice.
- The choice of a uniform distribution should be clarified (while it is the simplest prior, a brief explanation would enhance completeness).
- Typo: Page 6, second paragraph, line 3: "that" -> "than"