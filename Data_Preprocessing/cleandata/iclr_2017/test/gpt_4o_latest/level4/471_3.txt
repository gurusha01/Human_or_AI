The paper explores a "batch" approach within a reinforcement learning (RL) framework to enhance chat-bot performance.  
The authors provide a comprehensive overview of the RL setup they employ and propose an algorithm that closely resembles a previously published online method for addressing the same problem. They compare their batch approach to the online version and investigate several modeling choices.  
I find the writing to be clear, and the proposed algorithm represents a logical extension of the online method.  
Below are a few constructive suggestions:  
- Comparison of the constant vs. per-state value function: In the artificial experiment, there was no observed difference between the two, whereas in the real-world task, a difference emerged. It would be beneficial to delve into the reasons behind this and include such insights in the discussion. One possible explanation is:  
  - In the artificial task, the constant value function may have an unfair advantage, as it can update all the model's weights, unlike the per-state value function, which is restricted to updating only the top layer.  
- Section 2.2:  
  - The sentence before the last: `s'` is not defined.  
  - The last sentence: Add "... in the stochastic case." at the end.  
- Section 4.1, last paragraph: Update "While Bot-1 is not significant ..." to "While Bot-1 is not significantly different from ML ..."