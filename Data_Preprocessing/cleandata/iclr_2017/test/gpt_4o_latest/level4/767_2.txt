The paper introduces an actor-critic reinforcement learning (RL) algorithm to train learning rate controllers for supervised learning tasks. Experimental results demonstrate that the proposed method surpasses standard optimizers such as SGD, ADAM, and RMSprop on benchmarks like MNIST and CIFAR-10.
However, I have two primary concerns. First, the paper does not compare its approach to other recently proposed methods, such as "Learning Step Size Controllers for Robust Neural Network Training" by Daniel et al. and "Learning to learn by gradient descent by gradient descent" by Andrychowicz et al. The work by Daniel et al. is particularly relevant since it also employs a policy search RL method (REPS). It is unclear what the limitations of their approach are compared to the proposed method. While the authors note that Daniel et al.'s method incorporates more prior knowledge, it is not evident why this should be considered a disadvantage.
Second, I am concerned about the experimental results. Some of the reported performance metrics for the baseline optimizers are unexpectedly low. For instance, RMSprop performs surprisingly poorly in Tables 2 and 3. This raises questions about whether the baseline methods were appropriately tuned. To address this, it would be beneficial to include comparisons using standard architectures and previously reported results. For example, the baselines could be evaluated on a more robust architecture like ResNet or, for simplicity, Network in Network from the following list: