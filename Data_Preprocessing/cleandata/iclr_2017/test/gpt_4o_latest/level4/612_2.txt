Review - Paper Summary
This paper presents two key contributions:  
1. A model for next-step prediction, where both inputs and outputs lie in the space of affine transforms between consecutive frames.  
2. An evaluation method that assesses the quality of the generated data by measuring the performance degradation of another model (e.g., a classifier) when tested on the generated data.  
The authors demonstrate that, based on this evaluation metric, their proposed model outperforms baseline models, including the recent work of Mathieu et al., which employs adversarial training.
Strengths
- The paper addresses a significant challenge in unsupervised video learning: the evaluation of generated data.  
- The results indicate that using MSE in the transform space effectively mitigates the blurring issue, which is a central goal of this work.  
- The generated data reduces the performance of the C3D model on UCF-101 to a significantly lesser extent compared to other baselines.  
- The paper validates the hypothesis that videos can be approximated over several time steps as a sequence of affine transforms originating from an initial frame.  
Weaknesses
- The proposed evaluation metric is only meaningful if the primary concern is the performance of a specific classifier on a given task. This limits the metric's applicability, as unsupervised learning often aims to produce representations that are broadly useful across diverse tasks. The proposed metric does not facilitate the evaluation of generative models designed with this broader objective in mind.  
- The interaction between a generative model and the peculiarities of the chosen classifier could lead to unintended effects, making it difficult to draw definitive conclusions about the relative merits of the generative models. To address this, the authors could evaluate their approach using multiple classifiers (e.g., C3D, dual-stream networks, and other state-of-the-art methods) and demonstrate that the ranking of generative models remains consistent across classifiers. Including such experiments would strengthen the paper's conclusions.  
- Using only 4 or 8 input frames sampled at 25fps provides limited context for the model to extrapolate the complex motion patterns observed in UCF-101. The approach of working in affine transform space would be more compelling if the model could generate more intricate motion patterns, as opposed to the near-linear extrapolations observed in the current results.  
- The motion prediction model does not have access to content information, relying solely on prior motion. This could be a disadvantage, as the motion predictor cannot leverage cues such as object boundaries or handle scenarios like colliding motion fields. Addressing occlusions or similar phenomena might be more intuitive in content space.  
Quality/Clarity
The paper is well-written and easy to follow. The assumptions are clearly articulated and validated, and the experimental details appear sufficient.
Originality
While the concept of generating videos by predicting motion has been explored in prior work, the specific implementation in this paper is novel. Additionally, the proposed evaluation protocol is original.
Significance
The proposed evaluation method offers an intriguing alternative, particularly if extended to incorporate multiple classifiers representing diverse state-of-the-art approaches. Given the challenges in evaluating generative video models, this paper could contribute to the development of a standardized benchmark.
Minor Comments and Suggestions
1. In the caption for Table 1: The phrase "Each column shows the accuracy on the test set when taking a different number of input frames as input" uses "input" to refer to the classifier's input (i.e., the output of the next-step prediction model). However, in the subsequent sentence, "Our approach maps 16 × 16 patches into 8 × 8 with stride 4, and it takes 4 frames at the input," "input" refers to the input to the next-step prediction model. Rephrasing these sentences to clarify the distinction would improve readability.  
2. To better understand the distribution of affine transform parameters, the authors could include a histogram of these parameters. This would provide insights into the typical range of the six parameters and whether outliers are common.  
3. Instead of using ||A - B||² to compare transforms A and B, the authors could consider using a metric based on A⁻¹B being close to the identity matrix. Did the authors explore this alternative?  
4. The statement "The performance of the classifier on ground truth data is an upper bound on the performance of any generative model" is not strictly accurate. It is conceivable, albeit unlikely, that a generative model could enhance data quality (e.g., by making it sharper or emphasizing discriminative features), thereby improving classifier performance beyond that achieved with ground truth data. This is particularly plausible if the generative model has access to the classifier and can optimize for features that activate the classifier.  
Overall
This paper introduces a method for future prediction in affine transform space, which reduces blurriness and produces videos that appear relatively realistic to the C3D classifier. However, the paper could be improved by demonstrating the model's ability to predict more complex motion patterns and by strengthening the experimental results through evaluations with additional classifiers beyond C3D.