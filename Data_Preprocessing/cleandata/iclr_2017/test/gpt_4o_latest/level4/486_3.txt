This paper introduces graph convolutional networks (GCNs), inspired by the idea of approximating graph convolutions. In a single propagation step, the model's operations can be summarized as follows: first, it applies a linear transformation to the node representations for each node, then multiplies the transformed representations by the normalized affinity matrix (augmented with self-loops), and finally applies a nonlinearity.
The model is applied to semi-supervised learning tasks on graphs, and the experimental results are impressive, significantly outperforming other baseline methods. The evaluation of the propagation model is also noteworthy, as it systematically examines and compares various model variants and design choices.
It is striking that such a simple model achieves such strong performance compared to the baselines. Given that most experiments rely on a two-layer model, this is particularly surprising, as a two-layer architecture is inherently localâ€”restricting the influence of a node's output to its 2-hop neighborhood, with no contributions from longer-range interactions. Since the computational efficiency of the model is well-documented (Section 6.3), it raises the question of whether adding more layers would yield any additional benefits.
Although the model is motivated by graph convolutions, its operations, as simplified in the paper, are quite straightforward. Compared to prior works such as Duvenaud et al. (2015) and Li et al. (2016), the proposed method is simpler and performs fewer operations overall. This leads to the question of how the proposed GCN would compare directly against these earlier methods.
In summary, I find the model to be simple yet compelling, with an interesting connection to graph convolutions and strong experimental results. While some questions remain unanswered, I believe this paper is worthy of acceptance.