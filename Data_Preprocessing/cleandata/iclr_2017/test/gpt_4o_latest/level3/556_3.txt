Review
Summary of Contributions
This paper provides an extensive empirical analysis of the geometry of loss functions in deep neural networks and investigates how different stochastic optimization methods interact with these loss surfaces. The authors explore the behavior of five popular optimization methods (SGD, SGDM, RMSprop, Adadelta, and ADAM) and propose an augmentation using a second-order Runge-Kutta integrator. The paper offers valuable insights into how optimization methods find different local minima, the characteristics of these minima, and the role of batch normalization in improving convergence. The authors employ innovative visualization techniques, including 2D and 3D projections of loss surfaces, to illustrate the differences between optimization methods. The paper is well-written, with clear experimental procedures and informative visualizations, making it a significant contribution to understanding optimization in deep learning.
Decision: Accept
The paper is recommended for acceptance due to its strong empirical contributions, clear presentation, and relevance to the field of deep learning optimization. However, there are areas where the paper could be improved, particularly in providing theoretical explanations for the observed phenomena.
Supporting Arguments
1. Strengths:
   - The paper addresses an important and underexplored problem: understanding the geometry of loss surfaces and the behavior of optimization methods in deep learning.
   - The experimental setup is robust, using state-of-the-art architectures (VGG, NIN, LSTM) and datasets (CIFAR-10, Penn Treebank), ensuring the results are broadly applicable.
   - The visualizations, especially the 2D and 3D projections of loss surfaces, are insightful and help elucidate the differences between optimization methods.
   - The analysis of batch normalization's impact on loss surfaces and convergence is particularly valuable, as it highlights its role in mitigating initialization sensitivity.
2. Weaknesses:
   - The paper lacks a theoretical explanation for why different optimization methods converge to different local minima, aside from the impact of batch normalization. This limits the generalizability of the findings.
   - Some figures, such as Figure 5, suffer from small font sizes, which detracts from their readability and accessibility.
   - While the authors propose a Runge-Kutta-based augmentation, its performance is inconsistent (e.g., it underperforms ADAM), and the reasons for this are not fully explored.
Additional Feedback for Improvement
1. Theoretical Insights: The authors should provide a deeper theoretical discussion on why optimization methods converge to different local minima and the implications of these differences for generalization. For example, what specific properties of the optimization algorithms or loss surfaces lead to these variations?
2. Visualizations: Improve the readability of figures, particularly by increasing font sizes and ensuring that all plots are clearly labeled. This will enhance the accessibility of the results.
3. Runge-Kutta Augmentation: The performance of the Runge-Kutta-based methods is not consistently better than existing methods like ADAM. The authors should investigate and clarify why this is the case and whether there are specific scenarios where their method excels.
4. Batch Normalization: While the paper discusses the role of batch normalization, it would benefit from a more detailed analysis of how it interacts with different optimization methods and loss surface geometries.
Questions for the Authors
1. Can you provide a theoretical explanation or hypothesis for why different optimization methods find qualitatively different local minima? Are there specific properties of the algorithms or loss surfaces that drive this behavior?
2. Why does the Runge-Kutta augmentation underperform ADAM in some cases? Are there specific conditions under which it is expected to perform better?
3. Did you explore the impact of other regularization techniques (e.g., dropout, weight decay) on the geometry of loss surfaces and the convergence of optimization methods?
4. Could you elaborate on the practical implications of the observed differences in the size of basins around local minima for generalization and robustness?
In conclusion, this paper makes a strong empirical contribution to understanding optimization in deep learning. Addressing the outlined weaknesses and questions would further strengthen its impact.