Review of the Paper
Summary of Contributions:  
The paper proposes a novel approach to optimize the Skip-Gram Negative Sampling (SGNS) objective by leveraging Riemannian optimization on the Grassmannian manifold. Specifically, the authors introduce a two-step framework: (1) optimizing the SGNS objective directly over a low-rank matrix using the projector-splitting algorithm and (2) recovering word and context embeddings from the optimized low-rank matrix. The authors claim that their method, termed RO-SGNS, achieves superior performance compared to standard SGNS optimization via stochastic gradient descent (SGD) and SVD-based approaches on linguistic similarity tasks. The paper also highlights the potential of this approach to serve as a foundation for future advancements in word embedding optimization.
Decision: Reject  
Key reasons for rejection:  
1. The advantage of the proposed Grassmannian SGD approach over vanilla SGD is not convincingly demonstrated, as the empirical improvements are marginal and lack theoretical guarantees.  
2. The projector-splitting algorithm used in the optimization is not novel and has been applied in prior works on matrix factorization and completion.  
3. The computational cost of the proposed method, particularly the SVD step in Equation (7), is not adequately analyzed, leaving doubts about its scalability for large datasets.
Supporting Arguments:  
1. Lack of Clear Advantage: While the paper reformulates SGNS optimization as a Riemannian optimization problem, the empirical results show only slight improvements in linguistic metrics over baseline methods. The paper does not provide theoretical guarantees for the proposed method, making it unclear whether the observed improvements are consistent or significant.  
2. Algorithm Novelty: The projector-splitting algorithm is not a novel contribution of this work, as it has been previously used in related optimization problems. The authors do not sufficiently differentiate their use of this algorithm from prior applications.  
3. Computational Complexity: The paper does not adequately discuss the computational cost of the proposed approach, particularly the SVD step, which can be computationally expensive for large-scale datasets. While the authors mention the possibility of efficient low-rank updates, they do not provide concrete details or experiments to support this claim.
Suggestions for Improvement:  
1. Theoretical Analysis: Provide theoretical guarantees or convergence analysis for the proposed method to strengthen its contribution. This would help justify the use of Grassmannian SGD over vanilla SGD.  
2. Empirical Validation: Conduct more extensive experiments, including ablation studies, to demonstrate the robustness and scalability of the proposed approach. For example, test the method on larger corpora or with varying hyperparameters to assess its generalizability.  
3. Computational Cost Analysis: Include a detailed analysis of the computational cost per iteration, particularly the SVD step, and compare it with baseline methods. This would clarify whether the proposed method is practical for large-scale applications.  
4. Step 2 Improvements: The paper acknowledges that Step 2 (recovering embeddings) is heuristic and suboptimal. Exploring more advanced techniques for this step could improve the overall performance and novelty of the approach.
Questions for the Authors:  
1. How does the computational cost of the proposed method scale with the size of the vocabulary and corpus? Can the method handle corpora with billions of tokens?  
2. Have you explored alternative retraction methods or optimizations that could reduce the reliance on SVD and improve scalability?  
3. Can the proposed method be extended to other embedding tasks (e.g., contextual embeddings) or evaluated on downstream tasks to demonstrate its broader applicability?  
4. How sensitive is the performance of your method to the choice of initialization (e.g., SVD-SPPMI vs. random initialization)?
In conclusion, while the paper presents an interesting reformulation of SGNS optimization, the lack of significant empirical improvements, theoretical guarantees, and computational analysis limits its impact. Addressing these issues could significantly strengthen the work.