Review
The paper introduces a novel TransGaussian model for parameterizing subject/object embeddings as Gaussian distributions, which is adaptable to both path and conjunctive queries in knowledge bases (KBs). It further combines these embeddings with an LSTM and attention mechanism to model distributions over relations for question answering (QA). The authors demonstrate their approach on a small, custom-built WorldCup2014 dataset, showcasing its ability to handle complex queries involving relation composition and conjunction.
Decision: Reject
The primary reasons for rejection are the limited experimental validation and weak empirical results. The paper lacks evaluation on standard benchmarks like FB15k or WebQuestions, which are critical for demonstrating generalizability and competitiveness. Additionally, the assumptions underlying conjunctive queries are not validated on real-world QA datasets, limiting the broader applicability of the proposed approach.
Supporting Arguments:
1. Novelty and Contribution: The TransGaussian model is an interesting extension of TransE, introducing Gaussian distributions to model uncertainty and relation composition. This is a meaningful contribution to the KB embedding literature. However, the term "Gaussian attention" is misleading, as it diverges from traditional attention mechanisms and is more aligned with KB embedding concepts.
   
2. Experimental Weakness: The evaluation is conducted solely on the WorldCup2014 dataset, which is small and synthetic. While the dataset allows controlled testing, it is insufficient to demonstrate the model's scalability or robustness. The weak results, particularly on certain query types, further diminish confidence in the model's practical utility.
3. Incomplete Methodology: The paper assumes an oracle for entity recognition but overlooks the need for an entity linker component, which is critical for real-world QA systems. This omission undermines the practicality of the proposed approach.
4. Clarity Issues: Figure 2 is poorly labeled, making it difficult to follow the flow of KB relations and natural language question words. This hinders the reader's understanding of the approach.
Suggestions for Improvement:
1. Benchmark Evaluation: Evaluate the model on standard datasets like FB15k, WebQuestions, or other large-scale KBs to demonstrate its generalizability and competitiveness.
2. Entity Linking: Incorporate an entity linker into the pipeline to address the practical challenges of real-world QA tasks.
3. Clarify Terminology: Avoid using "Gaussian attention" unless it aligns with traditional attention mechanisms. Consider renaming it to better reflect its role in KB embeddings.
4. Figure Improvements: Improve the clarity of Figure 2 by explicitly labeling KB relations, entities, and question words.
5. Conjunctive Query Validation: Test the assumptions of conjunctive queries on real QA datasets to ensure their applicability beyond synthetic settings.
Questions for the Authors:
1. How does the model perform on standard benchmarks like FB15k or WebQuestions? Are there plans to evaluate on such datasets?
2. Can you clarify why the term "Gaussian attention" was chosen, given its divergence from traditional attention mechanisms?
3. How does the model handle noise or ambiguity in entity recognition, especially in real-world QA scenarios?
4. What specific challenges led to the weak performance on certain query types (e.g., 10 and 12)? Could these be addressed with additional training data or architectural changes?
In summary, while the paper introduces an innovative approach to KB embeddings and QA, its limited experimental validation, weak results, and incomplete methodology prevent it from meeting the standards required for acceptance. Addressing these issues in future work could significantly strengthen the contribution.