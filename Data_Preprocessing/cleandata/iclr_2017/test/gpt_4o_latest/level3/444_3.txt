Review
Summary of Contributions
This paper introduces a novel method to extract interpretable rule-based classifiers from trained Long Short-Term Memory (LSTM) models, specifically applied to factoid question-answering tasks. By identifying salient word patterns using three scoring methods—state-difference, cell-difference, and gradient-based approaches—the authors demonstrate that these extracted rules can approximate the performance of the original LSTM. The extracted rules, often consisting of only two or three words, provide insights into the features that the LSTM focuses on. The method is validated on sentiment analysis and WikiMovies datasets, where the rule-based classifiers achieve competitive accuracy while offering interpretability. Additionally, the paper highlights the potential for extending this approach to other NLP tasks, such as machine translation, to uncover task-specific rule structures.
Decision: Accept  
The paper makes a significant contribution to the interpretability of LSTMs, which is a critical area in machine learning research. The proposed method is novel, scientifically rigorous, and well-supported by both quantitative and qualitative results. The ability to extract interpretable rules without a substantial drop in performance is particularly valuable for applications requiring transparency.
Supporting Arguments
1. Novelty and Relevance: The paper addresses a pressing issue in deep learning—model interpretability—by proposing a novel approach tailored to LSTMs. This is well-motivated and fills a gap in the literature.
2. Scientific Rigor: The experiments are thorough, spanning multiple datasets and tasks, and the results convincingly demonstrate the utility of the proposed method. The extracted rules are both interpretable and effective, as evidenced by their competitive performance.
3. Broader Impact: The method has potential applications beyond the demonstrated tasks, making it a valuable contribution to the NLP community. The discussion on extending the approach to other tasks is insightful and opens avenues for future research.
Suggestions for Improvement
1. Clarity and Notation: 
   - Equation (12) appears over-parameterized and could be simplified using a single vector. This should be addressed to improve clarity.
   - Equations (13) and (15) seem identical; the authors should verify and correct any errors.
   - Define \( c_0 = 0 \) explicitly in Equation (13) for completeness.
2. Inflexibility of Rules: The method's reliance on ordered word sequences limits its adaptability. Exploring unordered or partially ordered rules could enhance its applicability.
3. Presentation Issues: 
   - Highlight the word "film" in the third column of Table 1 for consistency.
   - Fix the phrasing "are shown in 2" to "are shown in Table 2."
   - Consider replacing digits with hashtags () to address number representation issues.
4. Generalization: While the method is demonstrated on sentiment analysis and question answering, additional experiments on tasks like machine translation or summarization would strengthen the paper's claims about generalizability.
Questions for the Authors
1. Could you clarify whether the phrase extraction in Section 4.1 focuses on the forward LSTM or includes bidirectional processing?
2. How does the method handle polysemous words or phrases that may have different meanings in different contexts? Are there plans to address this limitation?
3. What are the computational trade-offs of the proposed method compared to other interpretability techniques, especially for longer documents or larger datasets?
Overall, this paper is a strong contribution to the field of interpretable AI and is recommended for acceptance with minor revisions to address the above issues.