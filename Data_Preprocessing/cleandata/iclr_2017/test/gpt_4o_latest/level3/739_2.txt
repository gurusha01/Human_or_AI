Review of the Paper
Summary of Contributions
The paper proposes an algorithm for polynomial feature expansion directly on compressed sparse row (CSR) matrices, avoiding intermediate densification steps. This approach leverages the sparsity of the data to achieve a time complexity of \(O(d^kD^k)\), where \(d\) is the density, \(D\) is the dimensionality, and \(k\) is the polynomial expansion degree. The authors claim this is an improvement over the standard method, which has a time complexity of \(O(D^k)\). The paper also provides theoretical analysis of the algorithm's complexity and compares its performance empirically to the scikit-learn implementation for second-degree expansions.
Decision: Reject
The paper is rejected primarily due to (1) insufficient novelty in the algorithmic contribution and (2) significant weaknesses in experimental validation and contextualization within the literature.
Supporting Arguments for Decision
1. Algorithm Contribution: While the proposed algorithm reduces time complexity by a factor of \(d^k\), this improvement stems from leveraging sparsity, which is a well-established concept in computational optimization. The novelty of the algorithm itself is limited, as it primarily adapts existing ideas to CSR matrices without introducing fundamentally new techniques.
2. Background and References: The paper fails to adequately situate its work within the broader literature. Only three outdated references are cited, and recent advancements in sparse matrix operations and polynomial feature expansion are ignored. This lack of context weakens the motivation and significance of the contribution.
3. Experimental Weakness: The empirical results are incomplete and unconvincing. The authors do not provide results for higher-degree expansions (\(k > 2\)), which are critical for validating the scalability of the algorithm. Additionally, the performance gap between the proposed method and the baseline is attributed to differences in programming languages and environments, making the comparison unfair.
4. Presentation Issues: The paper suffers from several presentation flaws, including unclear notation (e.g., "pi:pi+1" in Section 2), missing algorithm titles and input/output details, and an unexplained colored area in Figure 1. These issues hinder the clarity and reproducibility of the work.
Suggestions for Improvement
1. Strengthen Novelty: Clearly articulate the novel aspects of the algorithm and differentiate it from existing methods. Consider extending the approach to other sparse matrix formats or higher-dimensional interactions to broaden its applicability.
2. Expand Literature Review: Incorporate recent and relevant works to provide a comprehensive background. This will help contextualize the contribution and highlight its significance.
3. Improve Experiments: Provide empirical results for higher-degree expansions (\(k > 2\)) and ensure fair comparisons by using consistent programming environments. Include additional benchmarks to validate the algorithm's scalability and robustness.
4. Enhance Presentation: Define all notations clearly, add titles and input/output details to algorithms, and explain all figures thoroughly. Correct typographical errors (e.g., "efter" in Section 3.1).
Questions for the Authors
1. How does the proposed algorithm perform for higher-degree expansions (\(k > 2\))? Can you provide empirical results to validate the scalability claims?
2. What recent works on sparse matrix operations and polynomial feature expansion were considered, and how does your approach compare to them?
3. Can you clarify the meaning of the colored area in Figure 1 and provide details on the experimental setup?
4. How would the algorithm generalize to other sparse matrix formats, such as compressed sparse column (CSC) or coordinate (COO) formats?
By addressing these issues, the paper could significantly improve its clarity, rigor, and contribution to the field.