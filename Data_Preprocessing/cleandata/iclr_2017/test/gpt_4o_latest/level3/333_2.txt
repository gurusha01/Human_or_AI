The paper presents a compelling analysis of the minimal conditions required for generating natural-looking textures, challenging the prevailing assumptions about the necessity of deep, hierarchical, and pre-trained neural networks for this task. By demonstrating that high-quality textures can be synthesized using single-layer convolutional networks with random filters, the authors provide a strong minimal baseline for texture synthesis. The work is notable for its surprising results, which suggest that neither depth nor supervised training of features is indispensable for natural texture generation. This finding has significant implications for the field, as it simplifies the requirements for texture synthesis and opens up new avenues for research.
Decision: Accept.  
The primary reasons for this decision are the novelty of the findings and the thoroughness of the analysis. The paper challenges established paradigms in texture synthesis and provides strong evidence to support its claims. While the evaluation methods could be improved, they do not undermine the validity of the results.
Supporting Arguments:  
1. Problem Tackled: The paper addresses a fundamental question in texture synthesis: what aspects of feature representations are crucial for generating natural-looking textures? This is a well-motivated problem, as it seeks to simplify and deepen our understanding of texture modeling.  
2. Novelty and Motivation: The approach is well-placed in the literature, as it builds on and contrasts with state-of-the-art methods like Gatys et al. (2015a). By questioning the indispensability of hierarchical depth and supervised training, the paper provides a fresh perspective.  
3. Scientific Rigor: The results are supported by extensive experiments comparing various single-layer architectures and filter types. The surprising finding that random filters can rival state-of-the-art methods is both robust and scientifically rigorous.  
Suggestions for Improvement:  
1. Evaluation Methods: The paper relies heavily on VGG-based loss as a proxy for perceptual similarity. While this is a reasonable choice, incorporating human evaluations or alternative perceptual metrics could strengthen the claims. For example, psychophysical assessments or diversity metrics could provide a more comprehensive evaluation of the synthesized textures.  
2. Computational Efficiency: The authors acknowledge the inefficiency of their approach due to the use of large filters. Exploring methods to improve computational efficiency, such as hierarchical architectures or optimized filter designs, would enhance the practical applicability of the model.  
3. Variability vs. Perceptual Similarity: The paper focuses primarily on perceptual similarity, but the trade-off with variability in synthesized textures is underexplored. Future work could delve deeper into balancing these two aspects, perhaps by introducing new metrics or optimization strategies.  
Questions for the Authors:  
1. How do the synthesized textures perform in tasks requiring human evaluation, such as distinguishing between real and generated textures?  
2. Could the proposed single-layer model be extended to handle more complex image generation tasks beyond texture synthesis?  
3. Have you considered alternative distance metrics or loss functions that might better capture the trade-off between perceptual similarity and variability?  
In conclusion, the paper is a strong contribution to the field of texture synthesis, offering both theoretical insights and practical implications. The work is promising and merits acceptance, with the potential for significant impact on future research.