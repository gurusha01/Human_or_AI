Review of "SteinGAN: Training Neural Samplers via Stein Variational Gradient Descent"
Summary of Contributions
This paper proposes a novel approach to train stochastic neural networks for probabilistic inference by leveraging Stein Variational Gradient Descent (SVGD). The authors reinterpret Generative Adversarial Networks (GANs) as energy-based models, where the discriminator models unnormalized likelihoods and the generator approximates the target distribution. The generator is optimized using SVGD, which incorporates a kernel-based repulsive force to encourage sample diversity. The paper introduces SteinGAN, an amortized MLE framework for training deep energy models, and demonstrates its ability to generate realistic images competitive with state-of-the-art GANs. The method is evaluated on datasets like MNIST, CIFAR-10, CelebA, and LSUN, showing promising empirical results.
Decision: Reject
While the paper introduces an interesting reinterpretation of GANs and a novel use of SVGD, it falls short in several critical areas that prevent it from meeting the standards for acceptance. The primary reasons for rejection are: (1) insufficient experimental rigor to support the claims, and (2) theoretical inconsistencies in the proposed method.
Supporting Arguments
1. Experimental Limitations:  
   - The experiments do not convincingly address the scalability of the kernel-based repulsion term to high-dimensional data, such as full-scale images. This is a significant limitation, as scalability is crucial for practical applications.  
   - The paper lacks a direct comparison between standard GANs and SteinGAN using the same architecture. Without this, it is unclear whether the improvements stem from the proposed method or other architectural or training differences.  
   - DCGAN, used as a baseline, is no longer considered a strong benchmark in the GAN literature. Comparisons with more recent GAN variants would strengthen the empirical claims.  
2. Theoretical Concerns:  
   - The generator and discriminator are updated alternately, rather than fully optimizing the generator at each step. This deviates from a strict Stein variational interpretation and raises questions about the theoretical soundness of the approach.  
   - The kernel for generator fitting depends on the discriminator's parameters, leading to a changing objective that cannot be interpreted as stochastic gradient descent on a single, well-defined objective. This undermines the theoretical foundation of the method.  
3. Clarity Issues:  
   - The use of "phi" to denote both the particle gradient direction and the energy function is confusing and detracts from the readability of the paper.  
   - The paper's presentation is dense and could benefit from clearer explanations of key concepts, particularly for readers unfamiliar with SVGD.
Suggestions for Improvement
1. Experimental Rigor:  
   - Include experiments that explicitly test the scalability of the kernel-based repulsion term to high-dimensional data.  
   - Compare SteinGAN directly with standard GANs using identical architectures to isolate the impact of the proposed method.  
   - Use stronger and more recent GAN baselines for evaluation, such as StyleGAN or BigGAN.  
2. Theoretical Clarifications:  
   - Provide a more detailed discussion of the implications of alternating updates on the theoretical validity of the Stein variational framework.  
   - Address the issue of the changing kernel-dependent objective and its impact on optimization stability.  
3. Clarity and Notation:  
   - Revise the notation to avoid ambiguity, particularly with the use of "phi."  
   - Simplify the presentation of the method and its connection to existing work to make it more accessible to a broader audience.
Questions for the Authors
1. How does the kernel-based repulsion term scale with the dimensionality of the data? Have you tested this on higher-resolution images?  
2. Can you provide results comparing SteinGAN and standard GANs using the same architecture to isolate the impact of the proposed method?  
3. How does the changing kernel-dependent objective affect the convergence and stability of the training process?  
In summary, while the paper presents an interesting reinterpretation of GANs and introduces a novel use of SVGD, it requires significant improvements in experimental rigor, theoretical clarity, and presentation to meet the standards for acceptance.