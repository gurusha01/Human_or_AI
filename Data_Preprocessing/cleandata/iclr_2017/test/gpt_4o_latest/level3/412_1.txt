Review of the Paper
Summary of Contributions
This paper introduces a novel layer-wise optimization algorithm for Piecewise-Linear Convolutional Neural Networks (PL-CNNs). The authors leverage the observation that parameter estimation for a single layer in PL-CNNs can be formulated as a Difference-of-Convex (DC) program, which is equivalent to solving a latent structured SVM problem. They propose using the Concave-Convex Procedure (CCCP) to optimize this DC program iteratively, with structured SVM problems solved via the Block-Coordinate Frank-Wolfe (BCFW) algorithm. The paper claims several advantages over backpropagation, including monotonic decreases in the learning objective and the elimination of learning rate tuning. Experimental results on MNIST, CIFAR-10, CIFAR-100, and ImageNet datasets suggest that the proposed method improves training objectives and test accuracies compared to state-of-the-art SGD-based optimizers.
Decision: Reject
While the paper presents a novel optimization approach with solid theoretical underpinnings, the experimental evaluation is insufficient to support the claims. The limited scope of experiments and subpar results on CIFAR-10 significantly weaken the paper's impact and practical relevance.
Supporting Arguments for Decision
1. Novelty and Theoretical Contribution:  
   The proposed layer-wise optimization for PL-CNNs is innovative and theoretically sound. The connection between PL-CNNs and latent structured SVMs is a valuable contribution, and the use of CCCP and BCFW is well-motivated. The theoretical guarantees of monotonic decreases in the objective function are a notable improvement over backpropagation.
2. Experimental Scope and Results:  
   The experimental evaluation is limited to a simple multi-class classification task, which does not fully demonstrate the scalability or generalizability of the proposed method. While the method shows improvements over SGD-based optimizers on training objectives, the reported test accuracy on CIFAR-10 (70.2%) is significantly lower than state-of-the-art benchmarks (90%+). This discrepancy raises concerns about the practical utility of the method and its ability to generalize to unseen data.
3. Clarity:  
   The paper is well-written and provides clear explanations of the methodology, including detailed derivations and algorithmic steps. However, the presentation of experimental results could be improved by providing more context and comparisons to other optimization methods.
Suggestions for Improvement
1. Expand Experimental Scope:  
   Evaluate the proposed method on more complex tasks and architectures, such as object detection or semantic segmentation, to demonstrate its scalability and broader applicability. Include comparisons with state-of-the-art optimizers on modern architectures like ResNet or Transformer-based models.
2. Address Performance Gap:  
   Investigate and address the significant performance gap on CIFAR-10. Provide insights into why the proposed method underperforms compared to backpropagation-based methods in terms of test accuracy.
3. Ablation Studies:  
   Conduct ablation studies to isolate the contributions of different components of the method (e.g., trust-region initialization, memory-efficient representation). This would help clarify the source of the observed improvements in training objectives.
4. Hyperparameter Sensitivity:  
   Include a detailed analysis of the sensitivity of the method to hyperparameters such as the regularization weight (λ) and the proximal term (µ). This would provide a clearer picture of the robustness of the approach.
Questions for the Authors
1. Can you provide an explanation for the low test accuracy on CIFAR-10 compared to state-of-the-art benchmarks? Is this due to limitations of the proposed method or experimental setup?
2. How does the proposed method handle overfitting, given the lack of dropout or other regularization techniques in the experiments?
3. Have you considered extending the method to non-piecewise-linear activations (e.g., Swish or GELU)? If so, what challenges do you foresee?
4. Could the proposed approach be adapted to unsupervised or semi-supervised learning settings? If yes, how would the optimization framework change?
In conclusion, while the paper presents a novel and theoretically sound optimization method, the limited experimental scope and subpar results on CIFAR-10 undermine its practical impact. Addressing these issues in future revisions could significantly strengthen the paper.