Review of the Paper
Summary of Contributions
This paper introduces a semi-supervised learning framework based on in-painting with an adversarial loss, termed Context-Conditional Generative Adversarial Networks (CC-GANs). The authors propose a generator that fills in missing image patches and a discriminator that distinguishes between real and in-painted images. The method is evaluated on STL-10 and Pascal VOC datasets, achieving state-of-the-art results on STL-10 and competitive results on Pascal VOC. The authors also demonstrate the ability of their generator to produce semantically meaningful in-paintings. The paper positions itself as an improvement over prior work by leveraging adversarial training for better feature learning and classification performance.
Decision: Reject
While the paper shows promise, it is not ready for publication in its current form. The decision is driven by the following key reasons:
1. Incomplete and Unfair Comparisons: The omission of AlexNet results and comparisons with relevant prior work (e.g., Doersch et al. and Noroozi & Favaro) undermines the validity of the claims. The use of the VGG architecture instead of AlexNet for Pascal VOC experiments makes comparisons with prior work unfair.
2. Limited Conceptual Novelty: The proposed method combines existing ideas (GAN-based feature learning and image in-painting) without introducing significant new concepts. While the results are strong, the lack of novelty weakens the paper's contribution.
Supporting Arguments
1. Missing AlexNet Results: AlexNet is a standard baseline in the literature, and its absence makes it difficult to assess the true performance gains of the proposed method. The authors' reliance on VGG without justification is unconvincing.
2. Incorrect Claims: Some claims, such as the source of performance gains and the relationship between tasks, are not well-supported by the reported results. For example, the claim that the gains stem primarily from the CC-GAN method rather than the architecture is not rigorously demonstrated.
3. Incomplete Inpainting Results: The qualitative in-painting results lack comparisons with prior methods, and no quantitative evaluation is provided. This diminishes the impact of the in-painting component of the paper.
Suggestions for Improvement
1. Include AlexNet Results: Re-run experiments with AlexNet to provide fair comparisons with prior work. This is critical for Pascal VOC experiments.
2. Compare with Relevant Work: Include comparisons with Doersch et al. and Noroozi & Favaro, as these are directly relevant to the proposed method.
3. Quantitative Inpainting Evaluation: Provide quantitative metrics (e.g., PSNR, SSIM) to evaluate the quality of in-painting results and compare them with prior methods.
4. Clarify Claims: Revisit claims about the source of performance gains and provide stronger evidence to support them.
5. Broaden Novelty: Consider exploring additional novel aspects of the method to strengthen its contribution beyond combining existing ideas.
Questions for the Authors
1. Why were AlexNet results omitted, and how do you justify the use of VGG for Pascal VOC experiments given that prior work uses AlexNet?
2. Can you provide quantitative evaluations of the in-painting results to better assess the generator's performance?
3. How does the proposed method compare to Doersch et al. and Noroozi & Favaro in terms of both methodology and results?
In summary, while the paper demonstrates strong results, it requires significant revisions to address the issues of fairness, completeness, and novelty before it can be considered for publication.