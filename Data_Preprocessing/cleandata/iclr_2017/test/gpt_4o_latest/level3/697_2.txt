The paper presents a novel two-step approach to training word embeddings for the Skip-Gram Negative Sampling (SGNS) model. The authors reformulate the SGNS optimization problem as a low-rank matrix optimization task, solved using Riemannian optimization techniques, followed by a matrix factorization step to extract word and context embeddings. This method is claimed to outperform existing approaches, including the original word2vec implementation and SVD-based methods, in terms of both the SGNS objective and linguistic similarity metrics. The authors also highlight the computational efficiency and flexibility of their approach, which leverages advanced optimization techniques.
Decision: Reject.  
While the paper introduces an interesting application of Riemannian optimization to SGNS, it lacks sufficient empirical and theoretical justification for its claims. The primary concerns are the unclear source of performance improvements and insufficient comparative analysis.
Supporting Arguments:  
1. Unclear Performance Gains: The authors claim that their two-step approach outperforms direct optimization over word and context embeddings (W and C). However, they do not provide a clear explanation for why this is the case, given that the end result (W and C) is the same. The avoidance of rotational degrees of freedom in Riemannian optimization is suggested as a potential reason, but this is not substantiated with evidence, such as learning curves or ablation studies.  
2. Limited Comparisons: The paper does not explore whether the observed improvements are specific to the proposed optimization method. For example, it would be valuable to test whether embeddings derived from other optimizers (e.g., SGD) could be factorized into W and C and still achieve comparable results in downstream tasks.  
3. Experimental Scope: While the method is evaluated on standard word similarity datasets, the results are not consistently superior (e.g., slightly worse performance on the MEN dataset). The authors also do not provide insights into the practical significance of these differences. Furthermore, the experiments are limited to a single corpus, which raises questions about generalizability.
Suggestions for Improvement:  
1. Provide Learning Curves: Include learning curves for the SGNS objective to illustrate how Riemannian optimization progresses compared to other methods. This would help clarify whether the proposed approach converges faster or avoids poor local minima.  
2. Broader Comparisons: Test the factorization of W and C obtained from other optimization methods (e.g., SGD) and evaluate their performance in downstream tasks. This would help isolate the contribution of the Riemannian optimization step.  
3. Theoretical Justification: Offer a more rigorous theoretical explanation for why the two-step approach improves performance. For instance, analyze the impact of avoiding rotational degrees of freedom on the optimization landscape.  
4. Extend Experiments: Evaluate the method on additional corpora and tasks to demonstrate its robustness and generalizability.  
Questions for the Authors:  
1. Why does the proposed method outperform direct optimization over W and C? Can you provide theoretical or empirical evidence to support this?  
2. Have you tested the performance of embeddings derived from other optimizers after factorizing their product into W and C?  
3. Can you provide learning curves for the SGNS objective to compare convergence behavior across methods?  
In summary, while the paper introduces a promising direction, it requires stronger empirical validation and theoretical grounding to justify its claims and contributions.