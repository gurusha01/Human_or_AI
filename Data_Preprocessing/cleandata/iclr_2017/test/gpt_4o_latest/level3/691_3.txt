Review of "Retro Learning Environment (RLE): A Novel Benchmark for Reinforcement Learning"
Summary of Contributions:
This paper introduces the Retro Learning Environment (RLE), a novel reinforcement learning (RL) benchmark that extends the capabilities of the widely used Arcade Learning Environment (ALE). RLE supports games from consoles such as the Super Nintendo Entertainment System (SNES) and Sega Genesis, offering a more complex and diverse set of challenges compared to Atari 2600 games. The environment is designed to be modular, extendable, and compatible with popular RL frameworks like Python and Torch. The paper also explores multi-agent reinforcement learning (MARL) by enabling agents to compete against each other, which could lead to more robust RL policies. Initial baselines using state-of-the-art algorithms are provided, demonstrating the increased difficulty of SNES games compared to Atari games.
Decision: Reject
While the paper introduces a promising benchmark with significant potential, several critical issues undermine its overall quality. The primary reasons for rejection are: (1) the incorrect definition of the Q-function, which is a fundamental flaw in the RL methodology, and (2) the weak and unoriginal implementation of the "rivalry" training component, which overlaps with existing multi-agent literature without offering sufficient novelty.
Supporting Arguments:
1. Benchmark Contribution: The introduction of RLE is a valuable contribution to the RL community, as it provides a more challenging and diverse set of games compared to ALE. The modular design and compatibility with existing frameworks are commendable. However, the current implementation of reward structures is limited to only 10 games, which restricts the immediate utility of the benchmark. While the authors plan to expand this, the current scope feels incomplete.
2. Technical Flaws: The incorrect definition of the Q-function in Equation (1) is a significant issue. This misalignment with standard RL terminology undermines the scientific rigor of the paper and raises concerns about the validity of the results.
3. Rivalry Training: The multi-agent "rivalry" training component lacks novelty and overlaps heavily with existing MARL methods. The results demonstrate limited generalization, and the approach does not advance the state-of-the-art in multi-agent learning. This component should be de-emphasized or significantly improved.
4. Minor Issues: Several minor issues detract from the paper's polish:
   - Equation (1) incorrectly labels the target network.
   - The reference to Du et al. is missing the publication year.
   - Some references cite arXiv preprints instead of published versions, which should be corrected.
Additional Feedback for Improvement:
- Reward Structures: Expanding the reward structures to cover more games would significantly enhance the benchmark's utility. Additionally, a more detailed discussion of how reward shaping impacts learning outcomes would be valuable.
- Q-Function Correction: The authors must revise the Q-function definition to align with standard RL practices. This is critical for ensuring the scientific validity of the work.
- Rivalry Training: To improve the multi-agent component, the authors could explore more sophisticated MARL techniques, such as minimax-Q learning or policy gradient methods for competitive settings. Alternatively, this section could be omitted to focus on the benchmark itself.
- Evaluation Metrics: The paper would benefit from a more comprehensive evaluation of the benchmark, including comparisons with other environments like OpenAI Gym or DeepMind Lab. Highlighting specific challenges unique to RLE would strengthen its case as a valuable contribution.
Questions for the Authors:
1. How do you plan to address the incorrect Q-function definition, and how might this impact the reported results?
2. What is the timeline for expanding the reward structures to more games, and how will this expansion be prioritized?
3. Could you provide more details on how RLE's challenges (e.g., delayed rewards, noisy backgrounds) translate to real-world applications?
4. Have you considered incorporating more advanced MARL techniques to improve the "rivalry" training component?
In conclusion, while the RLE benchmark has potential, the paper requires significant revisions to address technical flaws, expand its scope, and improve its scientific rigor.