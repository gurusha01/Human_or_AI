Review of the Paper
Summary
The paper aims to distill design principles for Convolutional Neural Network (CNN) architectures by analyzing recent advancements in the field. It proposes 14 design patterns, inspired by architectural concepts from software engineering and urban planning, to guide both novice and experienced practitioners in CNN design. Additionally, the paper introduces three novel architectures—Fractal of FractalNet (FoF), Stagewise Boosting Networks (SBN), and Taylor Series Networks (TSN)—as practical applications of these design principles. The authors evaluate their proposed architectures on CIFAR-10 and CIFAR-100 datasets, providing experimental results to support their claims. The paper is positioned as a resource for newcomers to CNN design, with the hope of inspiring further research.
Decision: Reject
The paper is not suitable for acceptance in its current form due to the following key reasons:
1. Weak Contribution: The primary contribution—summarizing CNN design principles—reads more like a review paper and lacks originality. Many of the proposed "design rules" are either common sense or better suited for educational materials like blog posts or introductory courses.
2. Unconvincing Experimental Results: The proposed architectures perform poorly compared to the baseline (FractalNet) on CIFAR-10/100, undermining the practical utility of the design principles and innovations.
Supporting Arguments
1. Lack of Novelty and Depth: While the idea of summarizing design principles is useful for newcomers, it does not meet the standards of a research paper. Many of the rules, such as "Normalize Layer Inputs" or "Strive for Simplicity," are well-known best practices in the field and do not provide new insights. The paper is heavily skewed toward incremental work, focusing disproportionately on ResNet variants, which limits its generalizability.
   
2. Incorrect and Misleading Claims: The claim about "universal downsampling and channel increase" in CNNs is incorrect and should be removed. Additionally, the connection between Taylor series and the proposed Taylor Series Networks (TSN) is weak and misleading, as the naming implies a mathematical rigor that is not substantiated.
3. Experimental Weaknesses: The experimental results are unconvincing. The proposed architectures (FoF, SBN, TSN) fail to outperform the baseline FractalNet, and the authors acknowledge that the results are preliminary. This raises questions about the practical utility of the proposed innovations.
4. Vague Language and Lack of Rigor: The paper uses vague language (e.g., "we feel") and makes unsubstantiated claims without sufficient experimental or theoretical support. For example, the benefits of "freeze-drop-path" are not convincingly demonstrated.
5. Structural Issues: The paper lacks a clear structure, making it difficult to follow. The presentation of the design principles is verbose and repetitive, while the experimental section is underdeveloped.
Suggestions for Improvement
1. Refocus the Contribution: If the goal is to provide a comprehensive summary of CNN design principles, the paper should explicitly position itself as a survey or tutorial. This would require a broader and more balanced review of the literature, beyond ResNet variants.
   
2. Strengthen Experimental Validation: The proposed architectures should demonstrate competitive performance on standard benchmarks. If the focus is on design principles, experiments should clearly isolate and validate the impact of each principle.
3. Clarify and Substantiate Claims: Remove incorrect claims (e.g., universal downsampling) and provide theoretical or empirical evidence for others. Avoid vague language and ensure that all claims are rigorously supported.
4. Improve Structure and Presentation: Reorganize the paper to clearly separate the design principles, proposed architectures, and experimental results. Avoid redundancy and ensure that the narrative is easy to follow.
Questions for the Authors
1. How do the proposed design principles differ from existing best practices in the field? Can you provide concrete examples of where they offer new insights?
2. Why do the proposed architectures perform poorly compared to the baseline? Are there specific design flaws or implementation issues that need to be addressed?
3. How do you justify the naming of Taylor Series Networks, given the weak connection to Taylor series approximations?
4. Have you considered evaluating the proposed architectures on larger datasets (e.g., ImageNet) to better demonstrate their scalability and utility?
In summary, while the paper addresses an important topic, it falls short in terms of originality, rigor, and practical impact. Significant revisions are needed to make it suitable for publication.