Review of "Retro Learning Environment (RLE)"
Summary of Contributions
This paper introduces the Retro Learning Environment (RLE), a novel reinforcement learning (RL) environment that interfaces with the LibRetro API, initially supporting SNES games with the potential for expansion to other consoles. The authors argue that SNES games present more complex challenges than Atari 2600 games due to richer graphics, more sophisticated AI, and intricate game mechanics. The paper benchmarks several Deep Q-Network (DQN) variants on five SNES games, demonstrating the difficulty of achieving human-level performance. Additionally, the authors explore multi-agent reinforcement learning (MARL) by pitting algorithms against each other, highlighting the specialization of RL agents to specific opponents. The paper also discusses challenges such as reward shaping and delayed rewards, emphasizing the potential of RLE as a platform for advancing RL research.
Decision: Reject
While the paper introduces a promising RL environment, it lacks sufficient novelty and rigor to warrant acceptance at a major AI conference. The key reasons for this decision are the limited originality of the benchmarking methodology and the lack of clarity in distinguishing RLE from existing environments like ALE and OpenAI Universe.
Supporting Arguments
1. Limited Novelty in Benchmarking: The benchmarking approach of pitting algorithms against each other is not novel, as it has been widely used in AI competitions. The experiments, while interesting, do not offer groundbreaking insights into RL algorithm development.
2. Unclear Distinction from Existing Platforms: The paper does not convincingly establish how RLE is fundamentally different or superior to existing environments like ALE, OpenAI Universe, or DeepMind Lab. Both RLE and ALE rely on emulators and ROMs, and the claimed advantages of RLE (e.g., support for SNES games) are incremental rather than transformative.
3. Limited Empirical Contributions: The absence of DQN/DDDQN results for key games like Super Mario and the unclear use of reward shaping in Figure 2 detract from the scientific rigor of the evaluation. Additionally, the finding that RL algorithms specialize to their opponents is unsurprising and does not significantly advance the field.
Suggestions for Improvement
1. Clarify Legal Concerns: The use of ROMs raises potential legal issues, especially for proprietary games from companies like Nintendo. The authors should address how these concerns will be mitigated to ensure the long-term viability of RLE.
2. Improve Benchmarking Rigor: Include results for missing games (e.g., Super Mario) and explicitly state whether reward shaping was used in all experiments (e.g., Figure 2). Additionally, consider benchmarking against more diverse RL algorithms to strengthen the empirical contributions.
3. Differentiate RLE from Existing Platforms: Clearly articulate the unique contributions of RLE compared to ALE, OpenAI Universe, and DeepMind Lab. Highlight specific features or challenges in RLE that are not addressed by these platforms.
4. Presentation and Typos: The paper contains numerous typos and an incomplete reference (Du et al.), which detract from its overall quality. These issues should be addressed in a revision.
Questions for the Authors
1. How does RLE address the legal concerns associated with using ROMs for emulated games, particularly for recent titles?
2. Can you provide a clearer comparison between RLE and ALE, specifically regarding the technical and methodological differences?
3. Were the results in Figure 2 obtained using reward shaping? If so, how does this impact the generalizability of the findings?
4. Why were DQN/DDDQN results for Super Mario omitted, and how might their inclusion affect the conclusions?
In summary, while the RLE has potential as a research tool, the paper does not sufficiently demonstrate its necessity or novelty compared to existing platforms. A revised version addressing the above concerns could significantly strengthen the paper's contributions.