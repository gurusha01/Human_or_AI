The paper proposes a novel document representation learning framework, Doc2VecC, which combines the skip-gram mechanism with a global context vector and dropout regularization. The authors claim that Doc2VecC is computationally efficient, generates high-quality document embeddings, and outperforms state-of-the-art methods in tasks such as sentiment analysis, document classification, and semantic relatedness. The key innovation lies in the corruption mechanism that introduces data-dependent regularization, favoring rare and informative words while suppressing common, non-discriminative ones.
Decision: Accept
The decision to accept is based on two primary reasons: (1) the proposed method is efficient and addresses a critical challenge in document representation learning by decoupling model complexity from the size of the training corpus, and (2) while the empirical evaluation has limitations, the combination of techniques and the simplicity of the model architecture make the approach valuable for the community.
Supporting Arguments:
1. Problem Tackled: The paper addresses the challenge of learning document representations that are both semantically meaningful and computationally efficient. By leveraging a corruption mechanism and averaging word embeddings, the authors propose a method that balances simplicity and performance.
   
2. Motivation and Placement in Literature: The approach is well-motivated, building on foundational works like Word2Vec and Paragraph Vectors. The authors clearly articulate the limitations of existing methods, such as the inefficiency of Paragraph Vectors for large corpora and the lack of semantic focus in simple averaging methods.
3. Scientific Rigor: The theoretical justification for the corruption mechanism as a form of data-dependent regularization is sound. However, the empirical evaluation, while interesting, is limited to a small set of tasks (e.g., sentiment analysis and document classification). The results are promising but would benefit from broader benchmarking.
Additional Feedback:
1. Empirical Evaluation: The evaluation is relatively narrow, focusing on sentiment analysis, document classification, and semantic relatedness. Including additional tasks, such as topic modeling or information retrieval, would strengthen the claims of generalizability.
   
2. t-SNE Projections: The paper dedicates significant space to t-SNE visualizations, which, while visually appealing, do not provide rigorous insights into the model's performance. This space could be better utilized to explore ablation studies or comparisons with more robust baselines.
3. Baselines: The baselines used in the experiments are somewhat soft. For instance, stronger baselines such as Transformer-based models (e.g., BERT) could provide a more competitive comparison.
4. Efficiency Claims: While the paper highlights the efficiency of Doc2VecC, a more detailed analysis of training and inference times across diverse hardware setups (e.g., GPUs) would make the claims more robust.
Questions for the Authors:
1. Could you elaborate on the choice of tasks for evaluation? How do you envision Doc2VecC performing on tasks like topic modeling or information retrieval?
2. The corruption mechanism is central to your approach. Did you explore alternative corruption strategies, and how do they compare to the one proposed?
3. How does Doc2VecC compare to modern Transformer-based models in terms of both performance and efficiency?
In conclusion, while the paper has some limitations in empirical evaluation and baseline comparisons, it introduces a simple, efficient, and potentially impactful method for document representation learning. With minor revisions, this work could make a valuable contribution to the field.