Review
Summary
The paper introduces a novel Batch Policy Gradient (BPG) method for reinforcement learning (RL) to improve chatbot performance in scenarios where rewards are noisy and expensive to obtain. Unlike prior work that relies on on-policy or online learning, this paper focuses on off-policy, batch RL, leveraging historical data to optimize chatbot responses. The authors compare their approach to an online version and explore two modeling choices for value function estimation: constant and per-state. Empirical results demonstrate the efficacy of BPG through synthetic experiments and real-world tests on a restaurant recommendation dataset. The paper also highlights the advantages of importance sampling and batch settings in reducing variance and improving convergence.
Decision: Accept
Key reasons:
1. Novelty and Relevance: The proposed batch RL method addresses a practical and underexplored problem in chatbot training, making it a valuable contribution to both the RL and NLP communities.
2. Empirical Validation: The paper provides rigorous experiments, showing clear improvements over baselines in both synthetic and real-world tasks. The discussion of modeling choices (constant vs. per-state value functions) adds depth to the analysis.
Supporting Arguments
1. Clarity and Motivation: The writing is clear, and the algorithm is a natural extension of existing online RL methods. The motivation for batch RL in noisy, expensive reward settings is well-articulated and grounded in real-world applications.
2. Scientific Rigor: The paper supports its claims with thorough experiments. The synthetic tasks effectively illustrate the advantages of BPG, while the restaurant recommendation dataset demonstrates its practical utility. The use of paired statistical tests (e.g., Wilcoxon signed-rank) strengthens the validity of the results.
3. Discussion of Modeling Choices: The comparison between constant and per-state value functions is insightful. While no difference is observed in synthetic tasks, the real-world experiments reveal advantages of the per-state approach, warranting further exploration.
Additional Feedback
1. Section 2.2 Issues: The notation for `s'` is undefined, and the last sentence is incomplete ("... in the stochastic case"). Clarifying these points would improve readability.
2. Section 4.1 Wording: The phrase "While Bot-1 is not significant ..." should be revised to "While Bot-1 is not significantly different from ML ..." for precision.
3. Fairness of Comparison: The authors note that the constant value function may have an unfair advantage in synthetic tasks by updating all model weights. This point could be elaborated further to ensure transparency in the experimental setup.
4. Hyperparameter Sensitivity: The choice of Î» and its impact on performance is briefly discussed, but more detailed analysis (e.g., sensitivity plots) would be helpful for practitioners.
Questions for the Authors
1. How does the method scale with larger datasets or more complex state-action spaces? Are there computational bottlenecks in the current implementation?
2. Could you provide more details on the rationale for clipping the importance sampling weights at 5? How sensitive is the algorithm to this threshold?
3. In real-world tasks, how robust is the method to variations in the quality of human-labeled rewards? Would noisy or biased labels significantly degrade performance?
Overall, the paper makes a strong contribution to the field and addresses a practical problem with a well-motivated and empirically validated approach. Minor revisions to address clarity and additional discussion points would further strengthen the work.