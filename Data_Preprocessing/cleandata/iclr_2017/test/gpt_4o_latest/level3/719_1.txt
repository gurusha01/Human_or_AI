The paper proposes a novel method to train neural networks without explicit labels, leveraging link/not-link information between pairs of examples. It introduces a "Classless Association" model with two parallel Multilayer Perceptrons (MLPs) that supervise one another via an Expectation-Maximization (EM) training rule. The authors draw inspiration from the Symbol Grounding Problem and infant sensory association learning, aiming to classify input pairs as belonging to the same unknown class. The model is evaluated on four classless datasets derived from MNIST, demonstrating better performance than clustering algorithms and promising results compared to supervised methods. The paper claims its contributions include a novel training rule, a unique architecture for classless association, and experimental validation in both supervised and unsupervised scenarios.
Decision: Reject.  
The primary reasons for rejection are the lack of clarity in the paper's presentation and insufficient justification for key design choices. Additionally, the practical applicability and novelty of the proposed method are questionable due to its similarities with existing semi-supervised learning approaches like co-training.
Supporting Arguments:  
1. Clarity and Writing: The paper's presentation is unclear, with convoluted explanations that hinder comprehension. For instance, the rationale behind using the power function in the E-step and the assumption of a uniform distribution is inadequately explained. This lack of clarity extends to the experimental setup, where some parameter choices (e.g., large mini-batch size) are not well-justified.  
2. Motivation and Novelty: While the idea of training without explicit labels is intriguing, the proposed method bears strong similarities to co-training and Siamese networks, which are well-established in the literature. The authors do not sufficiently differentiate their approach or provide a compelling argument for its novelty.  
3. Practical Applicability: The method is evaluated on synthetic datasets with uniform distributions and known class counts, which are idealized conditions. The authors acknowledge this limitation but fail to provide evidence that the model would generalize to real-world, more complex scenarios.  
Additional Feedback:  
1. The paper would benefit from a clearer explanation of the EM-training rule and its components. For example, why is the power function particularly suited for the E-step? Could alternative functions be explored?  
2. The assumption of a uniform distribution is restrictive. The authors should discuss how the method would perform with imbalanced or unknown distributions, as this is common in real-world data.  
3. The experimental evaluation should include more diverse datasets, especially multimodal or real-world datasets, to demonstrate broader applicability.  
4. The comparison with supervised and unsupervised baselines is appreciated, but the authors should also compare their method with semi-supervised approaches, given the conceptual overlap.  
Questions for the Authors:  
1. How does the model handle scenarios where the number of classes is unknown or the data distribution is non-uniform?  
2. Could you provide more justification for the choice of the power function in the E-step and its role in learning pseudo-classes?  
3. How does the model scale to larger datasets or deeper architectures? Have you considered computational efficiency?  
4. Can the proposed method be extended to multimodal datasets, as suggested in the conclusion? If so, what challenges do you anticipate?  
In summary, while the paper presents an interesting idea, it requires significant clarification, stronger justification for design choices, and more rigorous evaluation to be suitable for acceptance.