The paper introduces Doc2VecC, a novel framework for document representation learning that averages word embeddings with a corruption mechanism during training. This approach ensures that document embeddings capture semantic meaning while being computationally efficient. The authors highlight the model's simplicity, scalability, and ability to outperform state-of-the-art methods in sentiment analysis, document classification, and semantic relatedness tasks. The corruption mechanism acts as a data-dependent regularization, suppressing embeddings of frequent but uninformative words while emphasizing rare or informative ones. The model's ability to generate document embeddings efficiently at test time is particularly noteworthy.
Decision: Accept
Key Reasons:  
1. Strong Performance Despite Simplicity: Doc2VecC achieves competitive or superior results compared to more complex models across multiple tasks, demonstrating its effectiveness.  
2. Efficiency and Scalability: The model is computationally efficient, both in training and inference, making it practical for large-scale applications.  
Supporting Arguments:  
- Problem Tackled: The paper addresses the challenge of learning document representations that are both semantically meaningful and computationally efficient. By leveraging the simplicity of averaging word embeddings and introducing a corruption mechanism, the model avoids the parameter explosion seen in prior methods like Paragraph Vectors.  
- Motivation and Placement in Literature: The paper builds on established techniques like Word2Vec and Paragraph Vectors, but its use of corruption as regularization and its focus on efficiency distinguish it from prior work. While the idea of averaging word embeddings is not novel, the integration of corruption during training is a meaningful contribution.  
- Scientific Rigor: The experimental results are robust, with comparisons against strong baselines on diverse tasks. The authors provide detailed analyses of the model's performance, including its ability to suppress uninformative words and its scalability to large datasets.  
Suggestions for Improvement:  
1. Clarify Novelty: While the corruption mechanism is a key differentiator, the paper could better emphasize how this innovation advances the state of the art beyond existing methods like Denoising Autoencoders or Word2Vec with weighted averaging.  
2. Theoretical Insights: The paper provides a Taylor expansion-based explanation for the corruption mechanism's regularization effect. However, additional theoretical analysis or ablation studies could strengthen the argument.  
3. Comparison with Transformer-based Models: Given the growing popularity of transformer-based models for text representation, a discussion or comparison with these methods would provide a more comprehensive evaluation.  
4. Broader Applicability: While the paper focuses on specific tasks, it would be helpful to explore how Doc2VecC performs on other NLP tasks, such as summarization or question answering.  
Questions for the Authors:  
1. How does the corruption rate (q) affect the model's performance across different datasets? Is there an optimal range for q?  
2. Have you considered extending Doc2VecC to incorporate contextual embeddings (e.g., from BERT) instead of static Word2Vec embeddings?  
3. How does Doc2VecC handle extremely long documents or documents with sparse content?  
In conclusion, Doc2VecC is a well-executed contribution to document representation learning. Its simplicity, efficiency, and strong empirical performance make it a valuable addition to the field, and the paper warrants acceptance with minor clarifications and improvements.