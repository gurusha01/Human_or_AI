Review of the Paper
Summary of Contributions
This paper proposes a novel method for future frame prediction in video sequences by predicting transformations (affine transforms) between frames rather than directly predicting pixel values. The authors argue that this approach leads to sharper predictions and is computationally efficient. Additionally, the paper introduces a new evaluation protocol that uses a pre-trained classifier to assess the quality of generated frames based on their ability to preserve discriminative features. The method is evaluated on the UCF-101 dataset and compared against state-of-the-art baselines, showing favorable results in terms of both accuracy and computational efficiency.
Decision: Reject  
The primary reasons for rejection are:  
1. Lack of Novelty: The proposed approach is not sufficiently novel compared to existing literature. Similar transformation-based methods have been explored, and the paper does not address the critical challenge of multimodality in video prediction, despite claiming novelty.  
2. Misinterpretation of Reviewer Feedback: The authors misunderstood a key suggestion to test the importance of transformations by using RGB frames as input and transformations as output, which could have provided deeper insights into the proposed method's effectiveness.  
Supporting Arguments
1. Question/Problem Tackled: The paper addresses the problem of next-frame prediction in video sequences, focusing on reducing blurriness and improving efficiency by operating in the transformation space. While this is a relevant and challenging problem, the paper does not sufficiently differentiate itself from prior work. The use of affine transformations is not a new idea, and the paper does not convincingly demonstrate how its approach advances the state of the art.  
   
2. Motivation and Placement in Literature: The paper is well-written and provides a clear motivation for avoiding pixel-space predictions. However, the claims of novelty are overstated, as the method does not tackle multimodality, a key challenge in video prediction. The evaluation protocol is an interesting contribution, but it is not enough to compensate for the lack of innovation in the core methodology.  
3. Support for Claims: While the results on UCF-101 and Moving MNIST are promising, they do not provide strong evidence of the method's superiority. The evaluation protocol is useful but does not fully address the limitations of the proposed approach, such as its inability to handle diverse motion patterns or multimodal predictions. Additionally, the authors dismissed a reviewer suggestion to test with RGB frames, which could have provided critical insights into the importance of their transformation-based approach.  
Suggestions for Improvement
1. Address Multimodality: The paper should explicitly tackle the challenge of multimodality in video prediction. For example, incorporating stochastic modeling or adversarial training could help generate diverse and plausible future frames.  
2. Clarify Novelty: Clearly articulate the differences between this work and prior transformation-based approaches. Highlight any unique contributions beyond computational efficiency.  
3. Experimentation: Include experiments with RGB frames as input and transformations as output to validate the importance of using transformations. This would address the reviewer's suggestion and strengthen the empirical analysis.  
4. Broader Comparisons: Compare the method against a wider range of baselines, including recent multimodal approaches, to provide a more comprehensive evaluation.  
5. Factorization of Motion and Appearance: Explore methods to disentangle motion (transformations) from appearance, as this could lead to more robust and generalizable predictions.  
Questions for the Authors
1. How does the proposed method handle multimodality in video prediction? If it does not, how do you justify the claim of novelty?  
2. Why was the reviewer suggestion to test with RGB frames as input and transformations as output dismissed? Could this experiment provide additional insights into the effectiveness of the proposed approach?  
3. How does the method perform on datasets with more complex motion patterns or occlusions?  
In conclusion, while the paper presents an interesting direction and a useful evaluation protocol, the lack of a clear novel contribution and the dismissal of important reviewer feedback make it unsuitable for acceptance in its current form. Addressing these issues could significantly improve the paper's impact and clarity.