Review of the Paper
Summary of Contributions
This paper introduces a novel method to adaptively set the learning rate for stochastic gradient descent (SGD) using an actor-critic framework from reinforcement learning (RL). The proposed approach models the learning rate as an action in a Markov Decision Process (MDP), where the rewards are based on changes in the loss function. The authors claim that their method not only automates the learning rate adjustment process but also improves generalization performance by feeding different examples to the actor and critic networks. Experiments on MNIST and CIFAR-10 datasets demonstrate that the proposed method achieves comparable convergence speed to popular optimizers (e.g., Adam, Adagrad, RMSProp) while slightly improving test accuracy. The authors also compare their method with a prior adaptive learning rate approach (vSGD) and report superior performance across different neural network architectures.
Decision: Reject
While the paper presents an interesting idea and demonstrates some promising results, it falls short in several critical areas that are necessary for acceptance. The primary reasons for rejection are (1) the lack of a clear and rigorous comparison to baseline methods, and (2) insufficient analysis of computational overhead and wall-time performance, which are crucial for evaluating the practical utility of the method.
Supporting Arguments for the Decision
1. Lack of Rigorous Comparisons: The experimental results are intriguing but fail to provide a clear apples-to-apples comparison with baseline optimizers. For example, while test accuracy is slightly improved, the authors do not include wall-clock time or computational cost comparisons, which are critical for assessing the practical trade-offs of the proposed method.
   
2. Non-Stationarity vs. RL Assumptions: The authors do not adequately address the concern that the non-stationary nature of the learning process may conflict with the stationary assumptions of RL algorithms. This raises questions about the theoretical soundness of the approach.
3. Single Learning Rate Limitation: The method learns a single global learning rate for all parameters, unlike popular optimizers (e.g., Adam, RMSProp) that adapt learning rates for each parameter. This limitation is not thoroughly discussed or justified, and it may hinder the method's applicability to more complex models.
4. Unclear Comparison to Early Stopping: The comparison to early stopping in Figure 6 is ambiguous. It is unclear whether the observed performance gains are due to genuine improvements in learning rate adaptation or simply suboptimal optimization that prevents overfitting.
5. Missing Computational Overhead Analysis: The paper does not provide any analysis of the computational overhead introduced by the actor-critic framework. Given that the method involves training two additional networks (actor and critic), this omission makes it difficult to evaluate the method's practicality.
Suggestions for Improvement
1. Add Wall-Time and Overhead Analysis: Include plots or tables comparing the wall-clock time and computational overhead of the proposed method against baseline optimizers. This will help assess the trade-offs between accuracy and efficiency.
2. Parameter-Wise Learning Rates: Extend the method to learn parameter-wise learning rates, as this is a key feature of many successful optimizers like Adam and RMSProp. Discuss the challenges and potential benefits of this extension.
3. Address Non-Stationarity Concerns: Provide a detailed discussion or empirical analysis of how the method handles the non-stationary nature of the learning process and whether this impacts its effectiveness.
4. Clarify Early Stopping Comparison: Revisit the comparison to early stopping and provide clearer evidence that the performance gains are due to improved learning rate adaptation rather than suboptimal optimization.
5. Broader Dataset Evaluation: Test the method on additional datasets and more complex architectures to demonstrate its generalizability and scalability.
Questions for the Authors
1. How does the computational cost of training the actor and critic networks compare to the baseline optimizers in terms of wall-clock time and memory usage?
2. Have you considered extending the method to learn parameter-wise learning rates? If not, what are the challenges in doing so?
3. How does the method handle the non-stationary nature of the learning process, and does this conflict with the assumptions of the RL framework?
4. Can you provide more details on the comparison to early stopping in Figure 6? How do you ensure that the observed performance gains are not due to suboptimal optimization?
In conclusion, while the paper presents an innovative approach to learning rate adaptation, it requires significant improvements in experimental rigor, theoretical justification, and practical evaluation to be considered for acceptance.