Review of the Paper
Summary of Contributions
This paper aims to bridge the gap between the rapidly evolving field of convolutional neural network (CNN) architectures and the needs of inexperienced practitioners by identifying 14 design principles for CNNs in image classification. The authors provide a well-curated set of references and groupings, which could serve as a valuable resource for young researchers entering the field. Additionally, the paper introduces three novel architectures—Fractal-of-FractalNet (FoF), Stagewise Boosting Networks (SBN), and Taylor Series Networks (TSN)—and makes the implementation code publicly available, contributing positively to the research community. The authors hope these design principles and innovations will inspire further exploration and refinement of CNN architectures.
Decision: Reject
While the paper has potential in its conceptual framework and community contribution, it falls short in execution. The primary reasons for rejection are: (1) insufficient clarity and justification for the proposed design principles, and (2) weak experimental validation of the claims, with the novel architectures underperforming or offering only marginal improvements at significant computational costs.
Supporting Arguments
1. Clarity and Justification of Design Principles:  
   The 14 design principles are an ambitious attempt to distill best practices in CNN architecture design. However, many principles are poorly explained or insufficiently justified. For example, while "Proliferate Paths" and "Increase Symmetry" are intuitive, their practical implications and trade-offs are not clearly articulated. Similarly, "Over-train" and "Cover the Problem Space" are vaguely defined, lacking actionable insights for practitioners.
2. Experimental Validation:  
   The experimental results are scattered and fail to convincingly support the claims. The modifications to FractalNet, such as replacing fractal-joins with concatenation or Maxout, show negligible or negative impacts on performance. The novel architectures (FoF, SBN, TSN) either underperform or achieve only slight improvements (e.g., FoF's marginal accuracy gains) at the cost of increased parameters and complexity. The lack of rigorous experimentation around individual design principles further weakens the paper's contributions.
3. Community Contribution:  
   The publicly available code is commendable, but the lack of robust experimental results diminishes its practical utility. The authors' reliance on CIFAR-10/100 datasets without exploring more challenging benchmarks also limits the generalizability of their findings.
Suggestions for Improvement
1. Clarity in Design Principles:  
   Provide concrete, actionable guidelines for each principle, supported by theoretical or empirical justifications. For example, explain how practitioners can balance trade-offs like "simplicity vs. performance" or "depth vs. width" in real-world scenarios.
2. Focused Experiments:  
   Instead of introducing multiple novel architectures, conduct rigorous experiments to validate each design principle individually. This would strengthen the paper's core contribution and provide clearer insights for practitioners.
3. Benchmarking and Comparisons:  
   Test the proposed architectures on more diverse and challenging datasets (e.g., ImageNet) to demonstrate scalability and generalizability. Compare results with state-of-the-art models to contextualize the contributions.
4. Writing and Structure:  
   Improve the clarity and organization of the paper. For instance, separate the discussion of design principles from the architectural innovations to avoid conflating the two.
Questions for the Authors
1. How were the 14 design principles derived? Were they based on empirical observations, theoretical insights, or a combination of both?
2. Why were the experiments limited to CIFAR-10/100? Do you anticipate similar results on larger, more complex datasets?
3. Can you provide more detailed trade-off analyses for principles like "Proliferate Paths" and "Increase Symmetry"? How do these principles impact computational efficiency and model generalization?
In conclusion, while the paper has a promising premise, its lack of clarity, insufficient experimental rigor, and underwhelming results make it unsuitable for acceptance in its current form. Further refinement and focused validation could significantly enhance its impact.