Review of the Paper
Summary of Contributions
This paper investigates the optimization landscape of deep neural networks and characterizes the behavior of various stochastic optimization algorithms. The authors extend the visualization techniques introduced by Goodfellow et al. (2015) to explore the geometry of loss surfaces in higher dimensions and analyze the local minima found by different optimization methods. They also introduce a novel augmentation of optimization methods using second-order Runge-Kutta integrators and evaluate its performance. The paper provides empirical evidence that different optimization algorithms converge to distinct local minima, even when starting from the same initialization, and explores the implications of batch normalization and extreme weight initializations on the optimization process.
Decision: Reject
The primary reasons for rejection are the lack of originality in the methodology and the unclear significance of the findings. While the paper provides some interesting empirical observations, it heavily reuses the visualization approach of Goodfellow et al. (2015) without sufficient innovation. Additionally, the paper fails to provide meaningful intuition or practical utility for its findings, leaving the contributions to the field of optimization research unclear.
Supporting Arguments
1. Lack of Originality: The visualization techniques and experimental setup are largely borrowed from prior work, particularly Goodfellow et al. (2015). While the authors extend these techniques to higher dimensions, this extension feels incremental rather than a significant methodological innovation.
   
2. Unclear Contributions: The paper does not convincingly explain the significance of different algorithms finding different local minima. While the empirical results suggest that the local minima are qualitatively different, the implications for optimization research or algorithm design are not well articulated.
3. Practical Utility: The findings, such as the differences in the basins of attraction for various algorithms, are not tied to actionable insights for designing better optimization methods or improving training efficiency. The proposed Runge-Kutta augmentation does not outperform existing methods like ADAM, further limiting its practical relevance.
4. Lack of Intuition: The paper does not provide sufficient intuition-building for its results. For example, the implications of the observed differences in loss surface geometry or the role of batch normalization in shaping these surfaces are not adequately discussed.
5. Suitability for a Journal: The paper's exploratory nature and lack of immediate practical contributions make it more suitable for a journal, where there is more room for in-depth discussion, rather than a conference focused on impactful and novel contributions.
Suggestions for Improvement
1. Provide Intuition and Context: The authors should include more discussion on the implications of their findings. For example, why do different optimization algorithms converge to distinct local minima, and how does this affect generalization or robustness? What practical lessons can be drawn from the observed differences in loss surface geometry?
2. Enhance Originality: The paper could benefit from a more novel methodological contribution. For instance, introducing a new visualization technique or a fundamentally different approach to studying optimization landscapes would strengthen the paper.
3. Practical Insights: The authors should explore how their findings can inform the design of better optimization algorithms. For example, can the insights about basin sizes or transient phases be leveraged to improve convergence or generalization?
4. Additional Experiments: Plotting eigenspectra of the Hessians at critical points could provide deeper insights into the nature of the local minima and their stability. This could also help clarify the relationship between basin geometry and generalization performance.
Questions for the Authors
1. What is the practical significance of different algorithms finding distinct local minima? How does this affect generalization, robustness, or transfer learning?
2. Why does the proposed Runge-Kutta augmentation fail to outperform ADAM? Could this be due to the interaction between momentum and the RK2 integrator?
3. How do the observed differences in loss surface geometry translate to real-world training scenarios? For example, do larger basins correlate with better generalization in practice?
4. Can the authors clarify the role of batch normalization in reshaping the loss surface? How does this interact with the optimization process?
In summary, while the paper provides some interesting empirical observations, it lacks originality, clear contributions, and practical utility, making it unsuitable for acceptance in its current form.