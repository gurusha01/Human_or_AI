Review
Summary of Contributions
The paper introduces a novel combination of Deep Recurrent Q-Networks (DRQN) with eligibility traces to improve training in deep reinforcement learning (RL). Eligibility traces, a well-established concept in RL, are adapted for use with recurrent networks to address challenges like sparse rewards and partial observability. The authors evaluate their approach on two Atari games (Pong and Tennis) and compare it with DQN, demonstrating faster and more stable learning. Additionally, the paper investigates the impact of different optimizers (RMSprop and Adam) on performance, providing insights into their interaction with eligibility traces. The work is clearly written, well-structured, and grounded in related literature, making a meaningful contribution to the underexplored area of eligibility traces in deep RL.
Decision: Reject
While the paper presents an interesting idea and shows promising results, it lacks sufficient experimental depth and breadth to support its claims fully. The limited evaluation on only two games, both of which are relatively simple, undermines the generalizability of the proposed approach. Furthermore, the absence of comparisons with DRQN (without eligibility traces) and other state-of-the-art methods like Double DQN or Rainbow DQN weakens the empirical validation. These shortcomings make it difficult to assess the broader impact and significance of the work.
Supporting Arguments
1. Strengths:
   - The integration of eligibility traces with DRQN is a novel and underexplored idea in deep RL.
   - The paper demonstrates clear benefits of eligibility traces in accelerating and stabilizing learning, particularly in sparse reward settings.
   - The writing is clear, and the related work is well-covered, situating the contribution within the broader RL literature.
2. Weaknesses:
   - The experiments are limited to two Atari games (Pong and Tennis), which are not representative of the diversity of challenges in RL environments. More diverse benchmarks (e.g., other Atari games, continuous control tasks) are needed to validate the approach.
   - The comparison is primarily with DQN, which is not the most competitive baseline. Comparisons with DRQN (without eligibility traces) and more advanced methods like Double DQN, Prioritized Experience Replay, or Rainbow DQN are necessary.
   - The hyper-parameter settings (e.g., 位, learning rates) are fixed across experiments, leaving questions about the robustness of the method to different configurations.
Suggestions for Improvement
1. Expand Experimental Scope: Include evaluations on a broader range of environments, such as additional Atari games or continuous control tasks (e.g., MuJoCo). This would better demonstrate the generalizability of the approach.
2. Baseline Comparisons: Compare the proposed method with DRQN (without eligibility traces), DeepMind's DQN, and other state-of-the-art algorithms to contextualize the performance gains.
3. Hyper-Parameter Sensitivity: Conduct ablation studies and sensitivity analyses to understand the impact of key hyper-parameters like 位, learning rates, and optimizer configurations.
4. Frozen Network Frequency: Investigate the effect of the frozen network's update frequency on learning stability and performance, as suggested in the discussion.
Questions for the Authors
1. How does the proposed method perform in partially observable environments, where the benefits of recurrent networks are more pronounced?
2. Have you considered using more advanced baselines, such as Double DQN or Rainbow DQN, for comparison? If not, why?
3. How sensitive is the method to the choice of 位 and other hyper-parameters? Would different values of 位 lead to significantly different results?
4. Can the approach be extended to continuous action spaces or other RL paradigms, such as actor-critic methods?
In conclusion, while the paper addresses an interesting problem and provides initial evidence of the benefits of combining DRQN with eligibility traces, the limited experimental validation and lack of robust comparisons prevent it from meeting the standards for acceptance at this time. Expanding the scope of experiments and addressing the outlined weaknesses would significantly strengthen the paper.