Review of the Paper
Summary of Contributions
This paper introduces an actor-critic deep reinforcement learning (RL) framework, ACER (Actor-Critic with Experience Replay), designed to address the challenges of stability and sample efficiency in both discrete and continuous action spaces. The authors propose several innovations, including truncated importance sampling with bias correction, stochastic dueling networks (SDNs) for continuous action spaces, and an efficient trust region policy optimization method. The method is evaluated on the 57-game Atari domain and several MuJoCo continuous control tasks, demonstrating performance comparable to state-of-the-art approaches like prioritized DQN and A3C. The theoretical contributions include a novel interpretation of the Retrace operator as truncated importance sampling with bias correction, which is shown to be a contraction operator. The paper also provides ablation studies to highlight the importance of its individual components.
Decision: Reject
While the paper introduces several interesting ideas and demonstrates competitive performance, the contributions are incremental, and the proposed ACER framework is more complex and fragile compared to simpler alternatives like prioritized replay. Additionally, the evaluation domains do not emphasize sample efficiency, which undermines the claimed advantages of the method. Several technical aspects require clarification, and there are organizational and presentation issues that detract from the overall quality of the paper.
Supporting Arguments for the Decision
1. Incremental Contributions: The combination of Retrace with truncated importance sampling, trust region policy optimization, and stochastic dueling networks is novel but does not represent a significant leap over existing methods. The improvements in performance are marginal, and the added complexity of the ACER framework makes it less appealing compared to simpler methods like prioritized replay.
   
2. Evaluation Limitations: The evaluation focuses on domains (Atari and MuJoCo) where sample efficiency is not critical. This limits the generalizability of the method and its applicability to real-world scenarios where sample efficiency is paramount. The smaller replay memory size (50,000 frames) compared to prior work also raises concerns about the validity of performance comparisons.
3. Technical and Presentation Issues: Key technical details, such as the recursive computation of \( Q^{\text{ret}} \), the derivation of Eq. (7), and the use of \( Q^{\text{ret}} \) in Eq. (8), are insufficiently explained. Section 7 (theoretical analysis) should be moved to the appendix for better organization. Additionally, there are several minor issues, including unclear phrasing, typos, and missing labels in equations and figures.
Suggestions for Improvement
1. Clarify Technical Details: Provide a more detailed explanation of the recursive computation of \( Q^{\text{ret}} \), the derivation of Eq. (7), and the role of \( Q^{\text{ret}} \) in Eq. (8). This will help readers better understand the theoretical underpinnings of the proposed method.
2. Expand Evaluation: Include experiments in domains where sample efficiency is critical, such as robotics or real-world tasks with limited interaction data. This would strengthen the claims of sample efficiency and broaden the applicability of the method.
3. Simplify and Justify Design Choices: Highlight the necessity of each component in the ACER framework and justify the added complexity. Consider comparing ACER to simpler baselines like prioritized replay in a more comprehensive manner.
4. Improve Organization and Presentation: Move Section 7 to the appendix and streamline the main text to focus on the key contributions and results. Address minor issues such as typos, unclear phrasing, and missing labels in equations and figures.
Questions for the Authors
1. Can you provide more details on the recursive computation of \( Q^{\text{ret}} \) and its role in the policy gradient updates?
2. How does the smaller replay memory size (50,000 frames) affect the performance comparisons with prior work? Would larger replay memory sizes change the results?
3. Why were Atari and MuJoCo chosen as evaluation domains, given that sample efficiency is not critical in these settings? Do you have plans to evaluate ACER in more sample-constrained environments?
By addressing these issues and clarifying the technical contributions, the paper could be significantly improved for future submissions.