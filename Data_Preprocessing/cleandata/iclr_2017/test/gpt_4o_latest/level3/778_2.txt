Review of the Paper
Summary of Contributions
This paper proposes a novel network compression method that combines ternary matrix decomposition and binary activation encoding to reduce the test-time computational load of deep neural networks. Unlike prior approaches that focus solely on weight factorization, the authors introduce a method to factorize both weights and activations, enabling efficient feed-forward propagation using logical operations. The method is evaluated on three tasks—handwritten digit recognition (MNIST), ImageNet classification (VGG-16), and face recognition (VGG-Face)—demonstrating significant acceleration and memory compression with minimal accuracy degradation. For example, compressing fully connected layers in VGG-16 achieved a 15× speed-up and a 5.2% memory compression rate with only a 1.43% increase in top-5 error. The paper highlights the potential for deployment on low-power CPUs or specialized hardware, making it a promising contribution to the field of efficient AI.
Decision: Reject
While the paper presents an interesting approach to network compression, it falls short in key areas of evaluation and presentation. Specifically, the lack of clarity in result presentation and insufficient testing on state-of-the-art benchmarks make it difficult to fully assess the scientific rigor and practical feasibility of the proposed method.
Supporting Arguments for Decision
1. Results Presentation: The results are not presented in a clear, comparative table format as per the conference guidelines. While the paper provides figures and discussions, a consolidated table comparing original and compressed outcomes across tasks (e.g., accuracy, memory compression, and speed-up) is missing. This makes it challenging to evaluate the trade-offs between compression and performance degradation.
2. State-of-the-Art Benchmarks: The paper does not adequately address the feasibility of achieving competitive results on state-of-the-art networks where float representation is critical. For example, the MNIST accuracy degradation of 0.19% is within acceptable bounds, but the ImageNet top-5 error increase of 1.43% exceeds the benchmark guideline of 5%-10%. Additionally, the paper does not test the method on more recent architectures like ResNet or Transformer-based models, which are widely used in practice.
3. Scientific Rigor: While the proposed method is theoretically sound, the empirical results lack sufficient depth. For instance, the authors do not provide a detailed analysis of why certain configurations (e.g., kx = 4, kw = DO/2) perform better or how the method generalizes to other tasks. Furthermore, the feasibility of using restricted representations compared to float representations is not thoroughly addressed.
Suggestions for Improvement
1. Result Tables: Include clear tables comparing original and compressed outcomes across all tasks, detailing metrics such as accuracy, memory compression rate, and speed-up. This will make the results more accessible and easier to evaluate.
2. Broader Benchmarking: Test the method on more recent state-of-the-art architectures and datasets, such as ResNet, EfficientNet, or Vision Transformers, to demonstrate its applicability and competitiveness in modern deep learning.
3. Error Analysis: Provide a more detailed analysis of the trade-offs between compression and accuracy degradation. For example, explain why certain configurations work better and how the method handles error propagation in deeper layers.
4. Feasibility Discussion: Address the practical feasibility of the proposed method, particularly in scenarios where float representations are critical. Discuss potential limitations and how they might be mitigated.
Questions for the Authors
1. How does the proposed method compare to other state-of-the-art compression techniques, such as pruning or quantization-aware training, in terms of accuracy and computational efficiency?
2. Can the method be extended to modern architectures like ResNet or Transformer models? If so, what challenges might arise?
3. How does the method handle error propagation when compressing convolutional layers in deeper networks?
In summary, while the paper introduces an innovative approach to network compression, it requires significant improvements in result presentation, benchmarking, and analysis to meet the standards of the conference.