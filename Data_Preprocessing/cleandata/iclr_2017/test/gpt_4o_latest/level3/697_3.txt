The paper proposes a novel optimization method for Skip-Gram Negative Sampling (SGNS) in word embedding learning by leveraging Riemannian optimization. The authors reformulate the SGNS problem as a two-step procedure: first, optimizing the SGNS objective over a low-rank matrix manifold using a projector-splitting algorithm, and second, extracting embeddings from the optimized matrix. This approach is claimed to address limitations in existing methods, such as the non-uniqueness of matrix factorization and the indirect optimization of SGNS objectives in prior work. The paper demonstrates theoretical elegance in connecting SGNS optimization to Riemannian optimization and provides experimental results showing marginal improvements in word similarity benchmarks.
Decision: Reject
Key Reasons for Rejection:
1. Marginal Empirical Gains: The proposed method achieves only a 1% improvement on word similarity benchmarks compared to state-of-the-art methods. This small gain could be attributed to hyperparameter tuning rather than the inherent superiority of the method.
2. Lack of Tangible Benefits: While the theoretical contribution is appreciated, the paper does not convincingly demonstrate practical advantages, such as faster convergence or significant performance improvements over existing methods.
Supporting Arguments:
- The theoretical connection to Riemannian optimization is interesting and could inspire future work in related domains. However, the practical impact of this reformulation remains unclear. The marginal gains in linguistic metrics do not justify the added complexity of the proposed approach.
- The experimental evaluation lacks depth. For example, the authors do not provide a detailed analysis of computational efficiency or convergence speed compared to baselines like SGD or SVD-based methods. Without this, it is difficult to assess the practical utility of the method.
- The reliance on SVD-SPPMI embeddings for initialization raises questions about the standalone effectiveness of the proposed algorithm.
Suggestions for Improvement:
1. Stronger Empirical Validation: Include experiments that demonstrate faster convergence or reduced computational cost compared to existing methods. This would provide a more compelling case for the practical utility of the approach.
2. Broader Evaluation: Test the method on additional tasks (e.g., downstream NLP tasks) to show its generalizability and impact beyond word similarity benchmarks.
3. Ablation Studies: Provide a detailed analysis of how each component of the proposed method contributes to the overall performance. For instance, compare results with and without the Riemannian optimization step.
4. Hyperparameter Sensitivity: Clarify the role of hyperparameters (e.g., step size Î») and their impact on performance to address concerns about overfitting or tuning bias.
Questions for the Authors:
1. How does the computational complexity of the proposed method compare to SGD and SVD-based approaches in practice? Can you provide runtime benchmarks?
2. Could the marginal improvements in word similarity benchmarks be attributed to hyperparameter tuning rather than the proposed optimization framework?
3. Have you explored alternative initialization strategies beyond SVD-SPPMI embeddings? If so, how do they affect performance?
While the paper presents an elegant theoretical framework, the lack of substantial empirical evidence and practical benefits limits its impact. Addressing the above concerns could significantly strengthen the contribution.