Review of the Paper
Summary
The paper introduces SYMSGD, a parallel stochastic gradient descent (SGD) algorithm that aims to retain the sequential semantics of SGD in expectation. The authors propose a novel approach using sound combiners and probabilistically sound combiners to combine local models generated by parallel threads in a way that mimics sequential SGD. The paper claims that SYMSGD achieves significant speedups (up to 13Ã— on 16 cores) while maintaining the accuracy of sequential SGD for both sparse and dense datasets. The approach is limited to linear learners, such as linear regression and polynomial regression, and employs dimensionality reduction techniques to manage computational overhead. The authors evaluate SYMSGD on nine datasets and compare it with existing methods like HOGWILD! and ALLREDUCE.
Decision: Reject
The paper is well-written and presents an interesting idea, but it is not suitable for ICLR as it lacks direct relevance to representation learning. Additionally, there are significant limitations in the scope, experimental rigor, and relevance of the proposed approach.
Supporting Arguments for Rejection
1. Relevance to ICLR: The paper focuses on parallelizing SGD for linear learners, which does not align with the core themes of representation learning. The exclusion of popular models like SVMs, logistic regression, and neural networks further limits its applicability to the broader machine learning community.
   
2. Limited Scope: The proposed method is restricted to a small class of models with linear parameter dependencies. This excludes many widely used formulations, such as logistic regression and deep learning models, which are central to representation learning.
3. Unconvincing Experimental Results: The datasets used are small and serve primarily as proofs of concept. Most datasets can be solved in seconds on a single-core CPU, which undermines the need for parallelization. Furthermore, the reported speedups are modest compared to GPU-based parallelization for dense datasets, and the improvements over HOGWILD! for sparse datasets are limited.
4. Lack of Justification: The authors do not adequately justify the necessity of parallel algorithms for problems like linear regression, which are computationally inexpensive on modern hardware. Additionally, comparisons with linear programming methods for linear regression are missing, which would provide a stronger baseline.
5. Sparse vs. Dense Datasets: While SYMSGD shows some improvement over HOGWILD! for sparse datasets, it lacks a compelling argument for its use on dense datasets, where GPU-based approaches are more effective.
Suggestions for Improvement
1. Expand Applicability: Extend the approach to include more general models, such as logistic regression and neural networks, to make the method relevant to a broader audience.
2. Stronger Baselines: Compare SYMSGD with linear programming methods for linear regression to provide a more comprehensive evaluation. Additionally, include GPU-based parallelization methods for dense datasets.
3. Larger Datasets: Evaluate the method on larger, more realistic datasets that better reflect the challenges of modern machine learning tasks.
4. Theoretical Justification: Provide a clearer theoretical analysis of why SYMSGD is necessary and how it compares to existing methods in terms of computational complexity and scalability.
5. Relevance to Representation Learning: Highlight connections to representation learning or extend the approach to models that are central to this field, such as neural networks.
Questions for the Authors
1. Why is there no comparison with linear programming methods for linear regression? How does SYMSGD perform relative to these methods?
2. Can the proposed approach be extended to non-linear learners, such as logistic regression or neural networks? If not, what are the challenges?
3. Why were small datasets chosen for evaluation? Can SYMSGD scale to larger datasets, and how does it compare to GPU-based approaches in such scenarios?
4. What is the practical significance of retaining sequential semantics in parallel SGD? Are there specific applications where this property is critical?
While the paper presents an interesting algorithmic contribution, it requires significant improvements in scope, experimental rigor, and relevance to align with the goals of ICLR.