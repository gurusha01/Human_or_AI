Review
Summary of Contributions
This paper proposes a novel off-policy batch policy gradient (BPG) algorithm for reinforcement learning (RL) to improve chatbots in settings where rewards are noisy, expensive, and offline. The authors address a critical limitation of existing RL methods for natural language processing (NLP), which predominantly rely on on-policy or online learning. By leveraging historical data and importance sampling, the proposed method efficiently updates policies in batch settings, making it particularly suitable for customer service chatbots. The paper demonstrates the efficacy of the approach through synthetic experiments and real-world evaluations on a restaurant recommendation dataset. While the synthetic experiments highlight the algorithm's advantages over baselines, the real-world experiments show modest but statistically significant improvements in chatbot performance.
Decision: Reject
While the paper presents a well-motivated and technically sound approach, it fails to provide sufficient clarity and justification for why the batch version outperforms the online version. Furthermore, the real-world improvements, though statistically significant, are modest and do not convincingly demonstrate the practical impact of the method. These issues limit the paper's potential contribution to the field.
Supporting Arguments
1. Strengths:
   - The paper is well-written, with a clear explanation of the technical details and a solid grounding in the literature.
   - The motivation for using an off-policy batch RL approach is compelling, given the constraints of noisy and expensive rewards in chatbot training.
   - The synthetic experiments are instructive and effectively demonstrate the advantages of the proposed method over baselines.
   - The use of Amazon Mechanical Turk (AMT) for real-world evaluations adds credibility to the empirical results.
2. Weaknesses:
   - The paper does not sufficiently explain why the batch version of the algorithm outperforms the online version. While the authors suggest that the batch setting benefits from access to future actions and rewards, this claim is not rigorously substantiated with theoretical or empirical evidence.
   - The real-world experiments show only modest improvements, which may not justify the additional complexity of the proposed method. For example, the paired t-test results for Bot-1 are not statistically significant at the 10% level, and the improvements for Bot-2, while significant, are marginal.
   - The paper lacks a detailed ablation study to isolate the contributions of individual components, such as importance sampling and the choice of value function estimators.
Suggestions for Improvement
1. Clarify the Batch vs. Online Advantage: Provide a more rigorous analysis or additional experiments to explain why the batch approach outperforms the online approach. For instance, compare the variance of updates or convergence rates between the two settings.
2. Strengthen Real-World Results: Collect more labeled data to reduce noise in the AMT experiments and improve statistical significance. Additionally, provide qualitative examples that clearly demonstrate how the chatbot's performance has improved in meaningful ways.
3. Ablation Studies: Conduct experiments to evaluate the impact of key design choices, such as the use of importance sampling, the λ-return parameter, and the choice of value function estimators.
4. Broader Applicability: While the paper briefly mentions potential applications beyond chatbots (e.g., question answering, machine translation), it would benefit from a concrete demonstration of the method's generalizability to other tasks.
Questions for the Authors
1. Can you provide more theoretical or empirical evidence to explain why the batch version outperforms the online version?
2. How sensitive is the proposed method to the choice of hyperparameters, such as λ and the step size? Did you observe any stability issues during training?
3. Could you elaborate on the computational overhead introduced by the batch setting and how it compares to online methods in terms of scalability?
In summary, while the paper addresses an important problem and proposes a promising approach, the lack of clarity on key claims and the modest real-world improvements limit its impact. Addressing these issues could significantly strengthen the paper for future submissions.