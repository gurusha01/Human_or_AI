Review of "MT-LRP: Multi-Task State Representation Learning with Robotic Priors"
Summary of Contributions
The paper proposes a novel method, MT-LRP, for learning low-dimensional state representations in multi-task reinforcement learning (RL) settings. Unlike traditional multi-task learning approaches, MT-LRP learns task-specific state representations by solving tasks individually, without requiring explicit task labels or joint learning. The method extends the "learning with robotic priors" (LRP) framework by introducing a task coherence term to the loss function, which ensures representation consistency across training episodes. A gated neural network architecture is employed to detect tasks and extract task-specific state representations. The authors evaluate their approach on two simulated tasks—multi-task slot-car racing and mobile navigation—and demonstrate that MT-LRP outperforms baseline methods in extracting meaningful state representations and improving RL performance.
Decision: Weak Accept
The paper presents a promising approach to multi-task state representation learning, with a novel extension to robotic priors and a creative use of gated neural networks. However, the lack of benchmarking against traditional multi-task joint-learning methods and insufficient clarity in the mobile navigation results limit the paper's impact. Despite these shortcomings, the method's novelty and demonstrated effectiveness in the slot-car racing scenario justify a weak accept.
Supporting Arguments
1. Novelty and Motivation: The paper addresses the important problem of task-specific state representation learning in multi-task RL. The introduction of the task coherence term and the use of a gated neural network architecture are well-motivated and represent a meaningful extension to existing methods.
2. Empirical Results: The experiments on slot-car racing convincingly demonstrate the advantages of MT-LRP over baseline methods, particularly in scenarios with task-irrelevant distractions. The analysis of task detection and state representation further highlights the method's strengths.
3. Clarity of Methodology: The paper provides a detailed explanation of the MT-LRP framework, including the loss function, neural network architecture, and experimental setup.
Areas for Improvement
1. Benchmarking: The paper does not compare MT-LRP to traditional multi-task joint-learning approaches, such as policy distillation or shared representation learning. This omission makes it difficult to assess the broader applicability and competitiveness of the proposed method.
2. Mobile Navigation Results: The results for the mobile navigation task are underexplored and lack sufficient comparisons to alternative methods or baselines. This weakens the claim of general effectiveness across tasks.
3. Gated Architecture Baseline: While the use of a soft-gated architecture is justified, a comparison to a hard-gated baseline would strengthen the evaluation.
Questions for the Authors
1. How does MT-LRP compare to traditional multi-task joint-learning methods in terms of computational efficiency and performance?
2. Can you provide more details and quantitative results for the mobile navigation task? How does MT-LRP perform relative to baseline methods in this scenario?
3. What is the impact of the task-separation loss term on performance, especially in settings with short episodes or a mismatch between the number of tasks and gates?
Additional Feedback
1. Clarity of Results: The figures and tables could be better annotated to improve readability. For example, explicitly labeling the axes and providing more detailed captions would help readers interpret the results more easily.
2. Future Work: The discussion section raises interesting questions about knowledge transfer and shared layers before gating. Expanding on these ideas with preliminary experiments or theoretical insights would enhance the paper's contribution.
3. Reproducibility: While the implementation details are thorough, providing code or pseudocode for the MT-LRP algorithm would improve reproducibility.
In conclusion, the paper introduces a novel and promising approach to multi-task state representation learning. Addressing the benchmarking and evaluation gaps would significantly strengthen its impact and relevance to the RL community.