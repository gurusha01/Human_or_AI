Review of the Paper
The paper presents a novel approach to understanding the inner workings of Long Short-Term Memory (LSTM) networks by extracting patterns that contribute to their predictions. The authors propose a method to decompose LSTM outputs into interpretable components, enabling the identification of consistently important word sequences. These extracted patterns are then used to construct a simple, rule-based classifier, which approximates the performance of the original LSTM. The paper demonstrates the utility of this method on sentiment analysis tasks and the WikiMovies question-answering dataset.
Decision: Reject
The decision to reject is primarily based on two key reasons: (1) the limited generalizability of the proposed method, and (2) the lack of sufficient comparative baselines to validate the approach rigorously. While the method shows promise in specific, template-driven tasks, its applicability to more complex, free-form natural language processing (NLP) problems is unclear. Additionally, the absence of an attention-based baseline, which is a standard in interpretability research, weakens the empirical evaluation.
Supporting Arguments
1. Generalizability Concerns: The method performs well on structured datasets like WikiMovies but struggles with free-form Q&A tasks and specific word types such as numbers or entity names. The reliance on external entity detectors for handling such cases limits the method's broader applicability.
2. Baseline Comparisons: While the paper compares its method to gradient-based baselines, it omits comparisons to attention mechanisms, which are widely used for interpretability in LSTMs. Including such a baseline would provide a more comprehensive evaluation of the proposed approach.
3. Empirical Limitations: The results indicate that manual pattern matching outperforms the automatic method in certain cases (e.g., year annotations). This suggests that the proposed method may not fully capture the nuances of LSTM learning.
4. Minor Issues: The paper contains undefined variables (e.g., P and Q), incorrect references, and inconsistent formatting (e.g., "adam" instead of "Adam"). These issues detract from the paper's overall clarity and professionalism.
Additional Feedback
1. Improving Generalizability: The authors should explore how the method can be adapted to handle unstructured, free-form text. Incorporating external tools (e.g., entity detectors) into the pipeline should be explicitly discussed as part of the methodology.
2. Baseline Expansion: Adding an attention-based baseline would strengthen the empirical validation and provide a more robust comparison.
3. Clarity and Presentation: The paper would benefit from addressing the undefined variables and ensuring consistent formatting. Additionally, clearer explanations of equations and their relevance to the proposed method would improve readability.
Questions for the Authors
1. How does the method handle cases where extracted patterns overlap or conflict in their contributions to different classes?
2. Can the approach be extended to other neural architectures, such as transformers, which are becoming increasingly popular in NLP?
3. How does the method perform on datasets with significant linguistic variability, such as multilingual or code-mixed datasets?
In summary, while the paper introduces an interesting approach to LSTM interpretability, its limited generalizability, lack of comprehensive baselines, and minor presentation issues prevent it from meeting the standards for acceptance at this time. Addressing these concerns could significantly enhance the paper's impact and applicability.