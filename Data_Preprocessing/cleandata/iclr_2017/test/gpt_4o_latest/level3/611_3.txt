Review of the Paper
The paper proposes a novel method, Collaborative Deep Embedding (CDE), which employs dual deep neural networks to encode users and items for recommender systems. The authors claim that this approach addresses key challenges such as the cold-start problem and the semantic gap in user-item interactions. The dual networks are trained collaboratively, and the method introduces multi-level branching architectures to enhance expressive power. Experimental results on three real-world datasets demonstrate significant performance improvements over state-of-the-art methods, particularly in generalization to unseen items (cold-start scenarios).
Decision: Reject
While the paper introduces an interesting approach and demonstrates promising results, the decision to reject is based on two key concerns: (1) the weak justification for dataset subsampling, which undermines the validity of the results, and (2) the lack of standard evaluation metrics and dataset consistency, making it difficult to fairly compare the proposed method with competing approaches.
Supporting Arguments
1. Dataset Subsampling Concerns: The authors subsample the datasets to reduce sparsity, but this decision is poorly justified. Subsampling contradicts the paper's focus on addressing the cold-start issue, as sparsity is a natural characteristic of real-world recommendation datasets. The authors fail to explain how their method performs without subsampling, raising concerns about the generalizability of the results.
2. Evaluation Metrics and Dataset Consistency: The use of Recall@M as the sole evaluation metric is unusual for recommender systems. Standard metrics like RMSE or AUC should also be reported to provide a more comprehensive evaluation. Furthermore, the use of unusual data splits and subsampling limits the ability to fairly compare the proposed method with existing approaches. Results should ideally be reported on standard datasets and splits to ensure reproducibility and comparability.
3. Pre-Review Strengths: Aside from dataset-related concerns, the paper satisfactorily answers pre-review questions. The proposed user and item embedding methods are well-motivated and build on recent advancements in deep recommender systems. The multi-level branching design is a notable contribution that enhances the expressive power of the model.
Additional Feedback for Improvement
1. Dataset Subsampling: Provide a stronger justification for subsampling or, preferably, evaluate the method on the full datasets to demonstrate its robustness in handling sparsity.
2. Evaluation Metrics: Include standard metrics such as RMSE, AUC, or NDCG to complement Recall@M. This will provide a more holistic view of the method's performance.
3. Dataset Consistency: Use standard datasets and splits (e.g., MovieLens or CiteULike) without modifications to enable fair comparisons with existing methods.
4. Cold-Start Analysis: While the paper claims to address the cold-start issue, a more detailed analysis of its performance on truly unseen users and items is needed. This could include comparisons with methods explicitly designed for cold-start scenarios.
Questions for the Authors
1. How does the proposed method perform on the full datasets without subsampling? Does sparsity significantly impact its performance?
2. Why was Recall@M chosen as the primary evaluation metric? Can additional metrics like RMSE or AUC be included for completeness?
3. How does the method compare to existing approaches on standard dataset splits without modifications (e.g., MovieLens or CiteULike)?
By addressing the above concerns, the paper could make a stronger case for its contributions and improve its chances of acceptance in future submissions.