Review of "MT-LRP: Multi-Task State Representation Learning with Robotic Priors"
Summary of Contributions
This paper introduces MT-LRP, a novel approach for unsupervised state representation learning in multi-task reinforcement learning (RL). The method combines gated neural networks, robotic priors, and multi-task learning to jointly learn task-specific state representations and task detectors from raw observations. The key innovation lies in extending robotic priors with a task-coherence prior and leveraging gated neural networks to disentangle tasks and their corresponding state representations. The approach is evaluated on two simulated slot-car racing scenarios, demonstrating its ability to outperform baseline methods and extract meaningful state representations. The authors provide detailed insights into the learned representations and analyze the contribution of various loss terms to the performance of the method.
Decision: Reject
While the paper presents a promising and innovative approach, it falls short in certain critical areas. The primary reasons for rejection are the lack of rigorous parameter selection, the limited scope of multitask learning evaluation, and the absence of experiments on standardized, high-dimensional benchmarks like OpenAI Gym. These issues hinder the generalizability and reproducibility of the results, which are crucial for acceptance at a top-tier AI conference.
Supporting Arguments for Decision
1. Strengths:
   - The use of gated neural networks for task-specific state representation learning is well-motivated and novel.
   - Extending robotic priors with a task-coherence prior is a meaningful contribution, as it enables unsupervised task discovery without explicit task labels.
   - The combination of diverse loss terms (e.g., temporal coherence, proportionality, causality, repeatability, and task coherence) ensures robust state representation learning.
   - The experimental results on the slot-car racing scenarios demonstrate the method's ability to learn disentangled representations and improve RL performance, particularly in low-data regimes.
2. Weaknesses:
   - Arbitrary Parameter Choices: The paper lacks a systematic approach for selecting key hyperparameters (e.g., weights for loss terms, number of gate units). This raises concerns about the robustness and reproducibility of the results.
   - Limited Evaluation Scope: The experiments are confined to two toy scenarios (slot-car racing) with low-dimensional inputs (16x16 RGB images). These scenarios, while illustrative, do not reflect the complexity of real-world RL tasks or high-dimensional benchmarks like OpenAI Gym.
   - Lack of Standardized Tools: The use of custom experimental setups instead of widely accepted benchmarks makes it difficult to compare the proposed method with state-of-the-art approaches in multitask RL.
Suggestions for Improvement
1. Hyperparameter Selection: Adopt standardized methods for hyperparameter tuning, such as grid search or Bayesian optimization, and report the process in detail.
2. Evaluation on Standardized Benchmarks: Test the method on high-dimensional, widely-used RL environments (e.g., OpenAI Gym, DeepMind Control Suite) to demonstrate its scalability and generalizability.
3. Broader Multitask Learning Scope: Extend the evaluation to scenarios with more diverse tasks and higher task complexity to validate the method's robustness.
4. Ablation Studies: Provide more comprehensive ablation studies to quantify the individual contributions of the gating mechanism, task-coherence prior, and other loss terms.
5. Comparison with State-of-the-Art: Include comparisons with recent multitask RL methods, such as policy distillation or shared representation learning approaches, to contextualize the contributions.
Questions for the Authors
1. How were the weights for the loss terms (e.g., ωt, ωp, ωc, ωr, ωτ) chosen? Were they optimized for each experiment, or were they fixed across all scenarios?
2. How does the method scale to environments with higher-dimensional observations (e.g., 84x84 images) or a larger number of tasks?
3. Could the task-separation prior lead to over-segmentation in scenarios with overlapping tasks? How does the method handle such cases?
4. Why were OpenAI Gym or other standardized benchmarks not included in the evaluation? Are there specific challenges in applying MT-LRP to these environments?
In conclusion, while the paper introduces a creative and promising approach, addressing the outlined weaknesses and expanding the evaluation scope would significantly strengthen its impact and applicability.