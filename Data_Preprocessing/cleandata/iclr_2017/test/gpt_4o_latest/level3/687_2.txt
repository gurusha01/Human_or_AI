Review
Summary of Contributions
This paper proposes a novel algorithm for pruning entire neurons from trained neural networks using a second-order Taylor series approximation of the change in the loss function. The authors compare their method to a first-order approximation and a brute-force approach, which serves as a baseline for optimal pruning decisions. The paper also explores the implications of pruning on the fundamental nature of learning representations in neural networks, corroborating earlier hypotheses about the uneven distribution of learned representations across neurons. The authors present extensive experimental results on toy datasets, MNIST, and regression tasks, demonstrating that their method can prune up to 40-70% of neurons without significant performance degradation. They also highlight the limitations of first- and second-order approximations and discuss the computational challenges of brute-force pruning.
Decision: Reject
While the paper addresses an interesting and relevant problem, it suffers from several critical issues that undermine its overall contribution. The primary reasons for rejection are: (1) the lack of rigor in supporting some of its claims, particularly regarding the independence of neuron effects and the optimality of brute-force methods, and (2) the absence of a discussion on weight decay algorithms and their relation to Bayesian priors, which is crucial for understanding pruning in the broader context of regularization. Additionally, the paper is unnecessarily long and could benefit from significant condensation to improve clarity and focus.
Supporting Arguments for Rejection
1. Questionable Claims: The paper assumes neuron independence and treats pruning as a serial process, but these assumptions are flawed. The interdependence of neurons is well-documented, and the paper does not provide sufficient evidence to justify these simplifications. Similarly, while the brute-force method performs well in experiments, the claim that it is "optimal" is not rigorously substantiated.
   
2. Incomplete Literature Context: The omission of weight decay algorithms and their connection to Bayesian priors is a significant oversight. These methods are foundational to understanding pruning and regularization, and their absence weakens the theoretical grounding of the paper.
3. Presentation Issues: The paper is overly verbose, with many sections containing redundant or tangential information. For instance, the detailed derivations of Taylor approximations could be moved to supplementary material. Additionally, the Q&A section is insufficient and should be integrated into the main body to provide a cohesive narrative.
4. Empirical Limitations: While the experiments are thorough, they focus primarily on small-scale networks and toy datasets. The results on deeper networks and real-world datasets are limited, making it difficult to generalize the findings to modern deep learning architectures.
Suggestions for Improvement
1. Theoretical Rigor: Address the assumptions of neuron independence and serial pruning by either providing empirical evidence or revising the pruning criteria to account for interdependencies. Additionally, clarify the theoretical basis for the claim that brute-force pruning is optimal.
2. Contextualization: Include a discussion on weight decay and Bayesian priors to position the proposed method within the broader literature on regularization and model compression.
3. Conciseness: Condense the paper by removing redundant explanations and moving detailed derivations to supplementary material. Focus on the key contributions and results.
4. Experimental Scope: Extend the experiments to include deeper networks and more complex real-world datasets to demonstrate the scalability and applicability of the proposed method.
5. Integration of Q&A: Incorporate the Q&A section into the main body of the paper to ensure that key points are discussed in context.
Questions for the Authors
1. How do you justify the assumption of neuron independence, given the well-known interdependencies in neural networks?
2. Why was there no discussion on weight decay or Bayesian priors, which are closely related to pruning and regularization?
3. Can you provide more evidence or theoretical justification for the claim that brute-force pruning is optimal?
4. How does the proposed method scale to deeper networks or datasets with higher complexity, such as ImageNet?
5. Have you considered integrating re-training into the pruning process to mitigate the impact of suboptimal pruning decisions?
In summary, while the paper tackles an important problem and provides interesting insights, it requires significant revisions to address its theoretical, empirical, and presentation shortcomings.