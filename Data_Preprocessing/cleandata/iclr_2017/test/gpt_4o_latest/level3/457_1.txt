This paper introduces Incremental Network Quantization (INQ), a novel method for compressing pre-trained convolutional neural networks (CNNs) into low-precision versions with weights constrained to powers of two or zero. The approach addresses key challenges in the field of neural network quantization, including accuracy loss and training inefficiency, through an iterative process combining weight partitioning, group-wise quantization, and re-training. Experimental results demonstrate that INQ achieves state-of-the-art performance, with negligible or no accuracy loss for 5-bit weights and competitive results for 3-bit and 2-bit weights. The method is validated across a range of CNN architectures, including AlexNet, VGG-16, GoogleNet, and ResNets, and shows potential for deployment on resource-constrained devices.
Decision: Accept
The paper is recommended for acceptance due to its significant contributions to the field of neural network quantization. The method is both novel and well-supported by extensive experimental results, which demonstrate its effectiveness in achieving high compression rates without sacrificing accuracy. The iterative quantization scheme is a notable advancement over existing approaches, particularly in its ability to handle low-precision weights with minimal accuracy degradation.
Supporting Arguments
1. Novelty and Impact: The proposed INQ method introduces a unique iterative quantization framework that effectively balances compression and accuracy. Its ability to achieve lossless quantization for 5-bit weights and competitive performance for 3-bit and 2-bit weights represents a significant improvement over existing methods.
2. Experimental Rigor: The paper provides comprehensive experimental results on the challenging ImageNet dataset, covering a wide range of CNN architectures. The results are consistent and demonstrate the robustness of the proposed method.
3. Practical Relevance: By targeting low-precision weights constrained to powers of two, the method enables efficient hardware implementations, making it highly relevant for deployment in mobile and embedded systems.
Additional Feedback
While the paper is strong overall, there are areas for improvement:
1. Writing and Clarity: The writing style and grammar require refinement to improve readability. For example, some sections are overly verbose and could benefit from more concise explanations.
2. Explanation of Weight Partitioning: The pruning-inspired partitioning strategy, a key component of the method, is not clearly explained in the main text. Providing more intuitive descriptions or visualizations would enhance understanding.
3. Broader Comparisons: While the paper compares INQ to several state-of-the-art methods, additional discussion on its limitations (e.g., computational overhead during iterative re-training) would provide a more balanced perspective.
Questions for the Authors
1. Can you provide more details on the computational cost of the iterative quantization process compared to global quantization methods? How does this impact scalability to larger models?
2. The pruning-inspired partitioning strategy appears to play a critical role in the method's success. Could you elaborate on how the splitting ratio is determined and whether it is sensitive to hyperparameter tuning?
3. Have you explored extending INQ to quantize activations or gradients, as hinted in the supplementary materials? If so, how does this impact overall model performance and hardware efficiency?
In conclusion, this paper makes a strong contribution to the field of neural network compression and is well-suited for acceptance, provided the authors address the minor issues outlined above.