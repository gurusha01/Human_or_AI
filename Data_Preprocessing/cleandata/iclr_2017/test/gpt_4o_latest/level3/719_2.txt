Review
The paper introduces a novel technique for "classless association" using a two-stream neural network architecture, inspired by the Symbol Grounding Problem and infant learning mechanisms. The proposed model employs two parallel Multilayer Perceptrons (MLPs) trained using an Expectation-Maximization (EM) algorithm, where intermediate representations are matched to a uniform statistical distribution. The authors evaluate their method on re-organized MNIST datasets and demonstrate superior performance compared to classical clustering algorithms (e.g., K-means and Hierarchical Agglomerative Clustering) in terms of association accuracy and purity. While the method underperforms supervised approaches, it shows promise for classless association tasks, a relatively unexplored area in machine learning.
Decision: Reject
The primary reasons for rejection are the lack of clear motivation for the architectural choices and insufficient theoretical justification for key assumptions, such as the reliance on a uniform statistical distribution. Additionally, the experimental evaluation is limited to variations of MNIST, which restricts the generalizability of the findings. The paper also lacks large-scale experiments or evidence of applicability to more complex datasets, which would strengthen its claims.
Supporting Arguments:
1. Motivation and Assumptions: The paper does not adequately justify the reliance on pseudo-classes and the uniform statistical distribution. While the uniform prior is ideal for balanced datasets, its applicability to real-world scenarios with unknown or imbalanced distributions is questionable. The necessity of Equation 2 and the use of large batch sizes (M) are also unclear and require further explanation.
2. Limited Experimental Scope: The experiments are restricted to MNIST and its transformations (e.g., Rotated-MNIST, Inverted-MNIST). While these datasets are a good starting point, they are simplistic and do not reflect the challenges of real-world data. Evaluation on more diverse datasets, such as multimodal or noisy data, would provide stronger evidence of the model's utility.
3. Theoretical Gaps: The paper lacks a strong theoretical foundation for the proposed EM-based training rule and its convergence properties. Additionally, the absence of a discussion on the scalability of the approach to deeper architectures or larger datasets is a significant limitation.
Additional Feedback:
1. Clarity and Justification: The authors should provide a more detailed explanation of the motivation behind the architecture and the choice of the uniform prior. Exploring alternative priors or adaptive mechanisms for unknown distributions would enhance the paper's robustness.
2. Experimental Diversity: Including experiments on datasets with greater complexity, such as multimodal or real-world data (e.g., TVGraz or Wikipedia featured articles), would significantly improve the paper. Pairing MNIST with its rotated or background-augmented versions is insufficient to demonstrate generalizability.
3. Scalability and Efficiency: The use of large batch sizes (M = 5,250) compared to classical models is not justified. The authors should discuss the computational implications and explore whether smaller batch sizes could achieve comparable results.
4. Typographical Errors: Minor typos should be corrected, such as "that" â†’ "than" in Figure 1 caption and Page 6, second paragraph, line 3.
Questions for the Authors:
1. Why was a uniform statistical distribution chosen as the target? How would the model perform under different priors or in scenarios with imbalanced data?
2. What is the theoretical justification for Equation 2, and how does it contribute to the overall training process?
3. Have you considered testing the approach on larger or more complex datasets? If so, what challenges do you anticipate?
4. Can the model be extended to deeper architectures or multimodal datasets? If yes, what modifications would be necessary?
In conclusion, while the paper addresses an interesting and underexplored problem, it requires stronger theoretical grounding, broader experimental validation, and clearer motivation for its design choices to be suitable for acceptance.