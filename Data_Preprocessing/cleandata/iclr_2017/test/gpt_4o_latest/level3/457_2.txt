Review of "Incremental Network Quantization (INQ)" Paper
Summary of Contributions
This paper introduces Incremental Network Quantization (INQ), a novel method for converting pre-trained full-precision convolutional neural networks (CNNs) into low-precision models with weights constrained to powers of two or zero. The key contributions include a three-stage iterative process—weight partition, group-wise quantization, and re-training—that ensures minimal accuracy loss during quantization. The method is extensively validated on ImageNet across various CNN architectures (e.g., AlexNet, VGG-16, ResNet-18), demonstrating improved or comparable accuracy even at 5-bit, 4-bit, and 3-bit quantization levels. The paper also explores combining INQ with network pruning, achieving superior compression ratios compared to state-of-the-art methods like Han et al.'s Deep Compression. The authors claim that INQ is hardware-friendly, as it replaces floating-point multiplications with efficient bit-shift operations.
Decision: Accept
The paper makes a significant contribution to the field of model compression and quantization, addressing critical challenges such as accuracy loss and computational inefficiency. The proposed method is well-motivated, rigorously evaluated, and demonstrates state-of-the-art performance. However, minor clarifications and additional experiments could further strengthen the paper.
Supporting Arguments
1. Problem Relevance and Novelty: The paper tackles a pressing issue in deploying CNNs on resource-constrained devices by proposing a unique incremental quantization strategy. Unlike prior methods, INQ carefully partitions weights based on importance, quantizes them iteratively, and compensates for accuracy loss through re-training. This approach is novel and well-placed in the literature.
   
2. Scientific Rigor: The experimental results are comprehensive, covering multiple architectures and quantization levels. The reported accuracy improvements at low bit-widths (e.g., 5-bit and 4-bit) are compelling and demonstrate the method's robustness. The comparison with Han et al.'s Deep Compression is fair and highlights INQ's advantages in both accuracy and compression ratio.
3. Hardware Applicability: The focus on hardware efficiency (e.g., replacing floating-point operations with bit-shifts) is a practical and impactful contribution, making the method suitable for real-world deployment on mobile and embedded devices.
Suggestions for Improvement
1. Clarification on Encoding: The explanation of the "5-bit" representation (1 bit for zero, 4 bits for powers of two) is somewhat confusing. A more detailed and intuitive description of the variable-length encoding scheme would help readers better understand the computational implications.
2. Fair Comparison with Han et al.: While the paper briefly mentions combining INQ with pruning, it would be beneficial to include explicit results comparing INQ alone, pruning alone, and their combination against Han et al.'s method. This would provide a clearer picture of the individual and combined contributions of INQ and pruning.
3. Generalization to Other Quantization Aspects: The authors mention extending INQ to low-precision activations and gradients in future work. Including preliminary results or insights on this extension in the main paper would strengthen the contribution.
Questions for the Authors
1. Can you provide more details on how the variable-length encoding impacts computational efficiency during inference? How does it compare to fixed-point quantization in terms of hardware implementation?
2. How sensitive is the method to the choice of weight partitioning strategy? Could alternative strategies, such as those based on quantization error, further improve performance?
3. Have you tested INQ on tasks beyond image classification, such as object detection or semantic segmentation, to evaluate its generalizability?
Conclusion
The paper presents a well-motivated, innovative, and rigorously evaluated method for CNN quantization. While minor clarifications and additional experiments could enhance the paper, the contributions are substantial and merit acceptance.