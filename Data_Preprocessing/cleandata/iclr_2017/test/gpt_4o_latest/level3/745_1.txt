Review of the Paper
Summary of Contributions
The paper proposes SYMSGD, a novel parallel mechanism for Stochastic Gradient Descent (SGD) that aims to retain the sequential semantics of SGD in expectation. The key innovation lies in the use of a sound combiner matrix and randomized projection techniques to reduce the dimensionality of the combiner matrix, making the approach computationally feasible for high-dimensional datasets. The authors claim that SYMSGD achieves significant speedups (up to 13Ã—) over a heavily optimized sequential baseline on shared-memory machines with 16 cores. The paper also demonstrates that SYMSGD can achieve the same accuracy as sequential SGD while scaling well for both sparse and dense datasets.
Decision: Reject
While the paper introduces an interesting approach to parallelizing SGD, the fundamental issues with the computational complexity of the proposed method and the lack of clarity in key aspects of the methodology undermine its contributions. Below are the key reasons for this decision:
1. Unclear Computational Complexity and Practicality:  
   The combiner matrix \( M \) is described as having \( O(f^2) \) space and time complexity, which is impractical for high-dimensional datasets. While the paper proposes using randomized projection to reduce this complexity, the exact computational cost of \( M_i \cdot v \) is not clearly articulated. The reviewer suggests that this operation could potentially be reduced to \( O(f) \) with an efficient formulation, but the paper does not explore or validate this claim. This raises concerns about the validity of the experimental results and the practicality of the proposed approach.
2. Potential Misunderstanding of SGD:  
   The paper's claim that SYMSGD retains the sequential semantics of SGD in expectation is intriguing but raises concerns about a possible misunderstanding of SGD's convergence properties. The source of the speedup and parallelism in SYMSGD is not clearly explained, leaving open questions about whether the method truly preserves the theoretical guarantees of SGD or introduces unintended biases.
3. Lack of Convergence Analysis:  
   The paper does not include a convergence rate analysis to assess the impact of dimensionality reduction on the algorithm's performance. Without this, it is difficult to evaluate whether the proposed probabilistic combiners maintain the robustness and convergence properties of sequential SGD.
Supporting Arguments
- Experimental Results: While the experiments demonstrate speedups over methods like Hogwild! and Allreduce, the lack of clarity around the computational complexity of the combiner matrix \( M \) casts doubt on the validity of these results. If the \( O(f^2) \) assumption holds, the proposed method would be infeasible for large-scale datasets, contradicting the claims of scalability.
- Theoretical Justification: The paper does not provide sufficient theoretical analysis to support its claims. Specifically, the variance introduced by the randomized projection is not rigorously analyzed, and the paper does not address how this affects the algorithm's accuracy and convergence.
Suggestions for Improvement
1. Clarify Computational Complexity: Provide a detailed analysis of the computational complexity of the proposed method, including the cost of computing \( M_i \cdot v \) and how it scales with the number of features \( f \). If an efficient \( O(f) \) formulation is possible, it should be explicitly derived and validated.
2. Convergence Rate Analysis: Include a theoretical analysis of the convergence rate of SYMSGD, particularly in the context of the variance introduced by dimensionality reduction. This would help establish the robustness of the proposed method.
3. Explain Source of Speedup: Clearly articulate the source of the speedup achieved by SYMSGD and how it compares to existing methods like Hogwild! and Allreduce. Address whether the speedup comes from algorithmic improvements or hardware optimizations.
4. Empirical Validation: Conduct experiments on larger, high-dimensional datasets to demonstrate the scalability of the method. Additionally, compare SYMSGD's convergence behavior with sequential SGD to validate the claim of retaining sequential semantics.
Questions for the Authors
1. How is the computational complexity of \( M_i \cdot v \) reduced to \( O(f) \), as suggested by the reviewer? If this is not the case, how do you justify the feasibility of the proposed approach for high-dimensional datasets?
2. What is the impact of the variance introduced by the randomized projection on the convergence rate and accuracy of SYMSGD? Can you provide empirical or theoretical evidence to support your claims?
3. Can you explain the source of the observed speedup in SYMSGD? Is it primarily due to algorithmic improvements, hardware utilization, or other factors?
4. How does SYMSGD perform on datasets with millions or billions of features? Have you tested its scalability beyond the datasets mentioned in the paper?
In conclusion, while the paper introduces an innovative idea for parallelizing SGD, the lack of clarity and rigor in addressing key theoretical and practical concerns prevents it from being accepted in its current form.