Review of "Amortized SVGD and SteinGAN"
Summary of Contributions
The paper introduces amortized Stein Variational Gradient Descent (SVGD), a novel approach that trains a neural network to mimic the particle dynamics of SVGD for minimizing KL divergence. This method is applied to train energy-based models with unnormalized densities, offering an alternative to traditional MCMC or variational inference methods. As an application, the authors propose SteinGAN, a generative adversarial training framework that uses amortized SVGD to train a neural sampler alongside an energy-based model. The proposed method is evaluated on several datasets, including MNIST, CIFAR-10, CelebA, and LSUN, and claims to produce competitive or superior results compared to state-of-the-art GANs.
Decision: Reject
Key reasons for rejection:
1. Unclear Results and Insufficient Experiments: While the paper claims competitive performance with state-of-the-art GANs, the empirical results lack sufficient rigor. Metrics like inception score and classification accuracy are reported, but the comparisons with baselines (e.g., DCGAN) are not comprehensive or statistically robust.
2. Methodological Concerns: The dependency on an autoencoder for the kernel contradicts the authors' critique of prior work. Additionally, the autoencoder changes during training, making it unclear whether the observed performance improvements stem from the proposed amortized SVGD or the autoencoder itself.
3. Training Efficiency: The amortized SVGD approach introduces additional update steps, making it slower and more computationally expensive than non-amortized methods. This undermines its practical utility for large-scale applications.
Supporting Arguments
1. Proposed Method: The idea of amortizing SVGD is conceptually interesting, as it aims to generalize particle-based methods to neural samplers. However, the paper does not adequately address the scalability of the proposed method in high-dimensional spaces, where the effectiveness of the "repulsive force" in SVGD remains unclear.
2. Experimental Setup: The use of the kernel in the hidden representation of an autoencoder is integral to the method but raises concerns about the novelty of the contribution. The autoencoder's role is significant, yet its interaction with the proposed method is not thoroughly analyzed.
3. Training Time: The additional computational cost of amortized SVGD compared to non-amortized approaches is a significant drawback, especially since the paper does not demonstrate clear advantages in terms of performance or generalization.
Suggestions for Improvement
1. Clarify the Role of the Autoencoder: The paper should explicitly analyze the dependency of the proposed method on the autoencoder. Experiments isolating the contribution of amortized SVGD from the autoencoder are necessary to validate the method's novelty.
2. Expand Experimental Evaluation: Include more baselines (e.g., other particle-based methods, MCMC, or variational inference) and provide statistical significance tests for the reported results. Additionally, evaluate the method on tasks beyond image generation to demonstrate its generality.
3. Address Scalability: Provide a detailed analysis of how the method scales with dimensionality, particularly focusing on the effectiveness of the repulsive force in high-dimensional spaces.
4. Improve Training Efficiency: Explore ways to reduce the computational overhead of amortized SVGD, such as more efficient update rules or approximations.
Questions for the Authors
1. How does the performance of the proposed method compare to non-amortized SVGD in terms of both quality and computational cost? Are there specific scenarios where amortized SVGD outperforms non-amortized approaches?
2. How sensitive is the method to the choice of the kernel and its parameters (e.g., bandwidth)? Have alternative kernels been explored?
3. Can the authors provide ablation studies to disentangle the contributions of the autoencoder, the kernel, and the proposed amortized SVGD?
In conclusion, while the paper presents an interesting idea, it falls short in providing clear empirical evidence, addressing scalability concerns, and justifying its computational cost. Further refinement and additional experiments are needed to strengthen the contributions.