The paper presents a compelling exploration of minimalistic approaches to natural texture synthesis, challenging the prevailing reliance on deep, hierarchical networks and pre-trained filters. The authors demonstrate that single-layer convolutional neural networks (CNNs) with random filters can generate textures that often rival state-of-the-art methods like Gatys et al.'s VGG-19-based approach. This is a significant contribution, as it questions the necessity of depth, pooling, and supervised training in texture synthesis, offering a simpler yet effective alternative. The paper also highlights the advantages of shallow networks in optimization, particularly for regular and simple textures, and proposes initializing VGG-19-based synthesis with shallow network results to improve outcomes.
Decision: Accept
The paper is recommended for acceptance due to its innovative insights into texture synthesis, strong experimental results, and its challenge to established paradigms. However, the lack of systematic evaluation metrics and psychophysical validation weakens the conclusiveness of its claims. Addressing these limitations in future work would further strengthen the paper.
Supporting Arguments:
1. Novelty and Contribution: The paper introduces a minimal baseline for texture synthesis using single-layer CNNs with random filters, which is a novel and impactful contribution. It challenges the assumption that deep, pre-trained networks are indispensable for high-quality texture generation.
2. Experimental Rigor: The authors provide extensive comparisons between various single-layer architectures and the VGG-19-based model, demonstrating that shallow networks can outperform deeper ones for specific textures. The use of VGG-19 loss as a quantitative measure, while imperfect, adds a layer of objectivity to the evaluation.
3. Practical Implications: The findings suggest that simpler models can achieve competitive results, which could lead to more computationally efficient methods for texture synthesis.
Suggestions for Improvement:
1. Evaluation Metrics: The paper lacks systematic comparisons against quantifiable objectives beyond VGG-19 loss. While the authors acknowledge this limitation, incorporating additional metrics, such as entropy for texture diversity or psychophysical assessments, would provide a more comprehensive evaluation.
2. Visual Quality Assessment: The reliance on VGG-19 loss as a proxy for perceptual quality is reasonable but insufficient. Including human evaluations or alternative perceptual metrics would strengthen the claims about the quality of synthesized textures.
3. Psychophysical Validation: The authors suggest psychophysical assessments as future work, but even preliminary experiments in this direction would significantly enhance the paper's impact.
4. Computational Efficiency: While the paper discusses the inefficiency of large filters in shallow networks, it would benefit from a more detailed analysis of trade-offs between computational cost and synthesis quality.
Questions for the Authors:
1. How does the proposed method perform on more complex, non-regular textures compared to VGG-19-based synthesis? Are there specific limitations to the shallow network approach for such textures?
2. Could the authors elaborate on the variability of textures generated by their model? How does it compare to the diversity achieved by Gatys et al.'s approach?
3. The paper mentions that initializing VGG-19-based synthesis with shallow network results improves outcomes. Could the authors provide quantitative results or visual examples to illustrate this improvement?
In conclusion, this paper makes a strong case for rethinking the role of depth and training in texture synthesis. While some limitations remain, its contributions are significant enough to merit acceptance, and the suggested improvements could further elevate its impact.