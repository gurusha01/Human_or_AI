Review
Summary of Contributions
This paper introduces ACER (Actor-Critic with Experience Replay), a novel reinforcement learning (RL) algorithm that combines several innovations to achieve stability and sample efficiency in both discrete and continuous action spaces. Key contributions include truncated importance sampling with bias correction, stochastic dueling network architectures, and an efficient trust region policy optimization method. The authors demonstrate that ACER achieves state-of-the-art results in continuous control tasks and matches the performance of leading methods in discrete domains like Atari games. The paper also provides theoretical insights into the Retrace operator and its application to policy gradients, as well as an ablation study to evaluate the contributions of individual components.
Decision: Reject
While the paper is technically sound and introduces meaningful innovations, it lacks a compelling overarching message and fails to clearly articulate the impact of individual contributions. The empirical results, while strong in continuous domains, do not surpass state-of-the-art methods in discrete tasks, which limits the broader appeal of the work. Additionally, several areas of the paper require improvement in clarity and rigor, particularly in the presentation of empirical results.
Supporting Arguments
1. Strengths:
   - The proposed methods are well-motivated and grounded in the literature, with commendable attention to Retrace and its theoretical characterization.
   - The algorithm demonstrates strong performance in continuous control tasks, outperforming baselines by a significant margin.
   - The paper is clearly written and accessible to deep RL practitioners.
2. Weaknesses:
   - The empirical claims of stability and sample efficiency are not adequately emphasized in the abstract, and their scope is limited to the presented results.
   - The lack of confidence intervals in many plots undermines the statistical rigor of the empirical evaluation.
   - The impact of individual innovations (e.g., truncated importance sampling, stochastic dueling networks) is unclear, as the ablation study does not provide sufficient granularity.
   - The results on discrete tasks (Atari games) only match existing methods, which diminishes the novelty and significance of the contributions for the broader AI community.
Suggestions for Improvement
1. Abstract and Clarity:
   - Clarify that the claims of stability and sample efficiency are empirical and domain-specific.
   - Provide a more concise and compelling summary of the paper's contributions and results.
2. Empirical Evaluation:
   - Include confidence intervals or statistical significance tests in the plots to strengthen the empirical claims.
   - Provide a more detailed analysis of the ablation study to better quantify the impact of individual components.
3. Broader Impact:
   - Articulate a stronger overarching message that ties the contributions together and highlights their significance for the broader AI community.
   - Discuss potential applications of the proposed trust region optimization method beyond RL.
4. Questions for the Authors:
   - Can you provide confidence intervals or additional statistical evidence for the empirical results?
   - How do the proposed innovations generalize to other RL tasks or domains beyond Atari and MuJoCo?
   - Could you elaborate on the computational cost of ACER compared to baseline methods, particularly in large-scale environments?
Conclusion
While the paper introduces valuable innovations and achieves strong results in continuous control tasks, it falls short in terms of broader impact and clarity of contributions. Addressing the outlined weaknesses could significantly improve the paper's quality and relevance.