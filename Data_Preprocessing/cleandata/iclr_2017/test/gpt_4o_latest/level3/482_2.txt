Review of the Paper
Summary of Contributions
This paper presents a novel application of recurrent neural networks (RNNs), specifically GRUs and LSTMs, to predict patient medication classes based on sequences of ICD-9 billing codes. The authors frame this as a multilabel sequence classification problem and demonstrate the superiority of RNNs over baselines such as MLPs, random forests, and heuristic models. The work is motivated by the critical issue of incomplete medication records in electronic health records (EHRs), which can lead to medication errors. The study leverages a large dataset of 610,076 patient records, achieving strong empirical results (e.g., micro-AUC of 0.93 and label ranking loss of 0.076), and provides qualitative insights into the model's predictions. The embedding results for ICD-9 codes also reveal clinically meaningful groupings, particularly for kidney-related codes. This work is positioned as a significant step toward addressing the unmet promises of EHRs by improving medication reconciliation.
Decision: Accept
The paper is a strong candidate for acceptance due to its practical importance, empirical rigor, and clear writing. While it lacks methodological novelty, its empirical contributions and the critical nature of the problem it addresses make it highly valuable for the ICLR audience. The paper is likely to attract significant interest from both the machine learning and healthcare communities.
Supporting Arguments
1. Problem Importance: The paper tackles a critical healthcare challengeâ€”improving medication reconciliation in EHRs. This is a high-impact application of machine learning with the potential to reduce medication errors and improve patient outcomes.
2. Empirical Strength: The RNN models significantly outperform baselines, demonstrating the utility of sequence modeling for this task. The dataset size (~610K records) and the evaluation metrics (e.g., micro-AUC, label ranking loss) are robust and scientifically rigorous.
3. Writing and Analysis: The paper is well-written, with a clear motivation, thorough related work, and detailed experimental analysis. The qualitative evaluation of predictions adds depth to the empirical results.
4. Comparison to Related Work: The authors provide a comprehensive comparison to prior studies, highlighting the stronger performance of RNNs and the use of a larger dataset.
Suggestions for Improvement
1. Data Preprocessing: The decision to truncate patient sequences to the last 100 billing codes is unintuitive and may limit the RNN's ability to leverage long-term dependencies. The authors should justify this choice or explore alternative sequence lengths.
2. Metrics Explanation: While the chosen metrics (e.g., micro-AUC, label ranking loss) are appropriate, their practical significance is not easily interpretable for non-experts. The authors should provide more context on what these metrics mean in a clinical setting.
3. Label Noise: The lack of ground truth for missing medications introduces label noise, particularly during testing. This could skew the reported metrics and raise skepticism among clinical reviewers. The authors should investigate the impact of this noise and analyze potential overfitting to noisy labels.
4. Overfitting Analysis: A deeper analysis of model errors and overfitting to noisy labels would strengthen the claims. For example, the authors could explore how the model performs on subsets of the data with varying levels of label noise.
5. Broader Input Features: The authors suggest that including additional features (e.g., lab results, demographics) could improve performance. Exploring this in future work would make the approach more clinically robust.
Questions for the Authors
1. How does truncating patient sequences to the last 100 billing codes affect the model's ability to capture long-term dependencies? Have you compared this to using full sequences?
2. Can you clarify the clinical significance of the reported metrics (e.g., micro-AUC of 0.93)? How do these translate to real-world utility for clinicians?
3. How does label noise during testing impact the reported metrics? Have you considered methods to quantify or mitigate this noise?
4. Did you observe any systematic biases in the model predictions, such as over-predicting certain medication classes?
By addressing these critiques, the paper could further solidify its contributions and increase its impact. Overall, this is a compelling application of deep learning to a critical healthcare problem, and I strongly recommend its acceptance.