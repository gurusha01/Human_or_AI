Review of the Paper
Summary of Contributions
This paper introduces a novel Gaussian attention model for content-based neural memory access, which replaces traditional inner-product-based attention scoring with a multivariate Gaussian likelihood. The key contribution is the ability to control the spread of attention, enabling both sharp and broad attention distributions, which is particularly relevant for knowledge base (KB) modeling and question answering (Q&A). The proposed model, termed TransGaussian, is applied to KB embedding and Q&A tasks, demonstrating its ability to handle path queries and conjunctive queries. The authors also propose a new dataset, WorldCup2014, to evaluate the model's performance on these tasks. The paper claims that TransGaussian achieves competitive performance compared to simpler models like TransE, particularly when trained compositionally.
Decision: Reject
Key reasons for rejection:
1. Insufficient Justification for Gaussian Attention: The paper does not provide compelling theoretical or empirical evidence to justify the superiority of Gaussian attention over traditional inner-product-based methods. The motivation for the added complexity remains unclear.
2. Experimental Weaknesses: The experimental results are inconclusive. TransGaussian underperforms compared to simpler models like TransE in critical tasks (e.g., WordNet link prediction in Table 11 and certain Q&A queries in Table 2), raising concerns about the model's practicality and robustness.
Supporting Arguments
1. Lack of Justification: While the idea of controlling attention spread is intriguing, the paper does not adequately explain why Gaussian attention is preferable to inner products. A direct comparison through canonical experiments is missing, which weakens the claims of the paper.
2. Experimental Design: The experiments lack sufficient ablation studies to isolate the contributions of individual components of the model. For example, Table 8 in Appendix B partially addresses this but is insufficient for drawing strong conclusions.
3. Performance Concerns: The poor performance of TransGaussian (SINGLE) in Table 2 and its underperformance on WordNet link prediction compared to TransE (Table 11) suggest that the added complexity of the Gaussian attention model may not translate into practical benefits.
4. Dataset Limitations: While the introduction of the WorldCup2014 dataset is valuable, the lack of experiments on established benchmarks for KB completion and Q&A limits the generalizability of the results.
Suggestions for Improvement
1. Stronger Justification: Provide a more detailed theoretical or empirical analysis of why Gaussian attention is expected to outperform inner-product-based methods. A direct comparison on canonical tasks would strengthen the paper.
2. Ablation Studies: Conduct more comprehensive ablation experiments to disentangle the contributions of different components of the model (e.g., the role of learned variances in Gaussian attention).
3. Benchmark Experiments: Include experiments on well-established datasets for KB completion (e.g., FB15K, WN18) and Q&A to demonstrate the model's generalizability and competitiveness.
4. Clarity and Focus: The paper is dense and lengthy (16 pages with supplementary material). Splitting it into two separate papers—one focused on KB completion and another on Q&A—would improve readability and allow for deeper exploration of each topic.
5. Address Training Challenges: The poor performance of TransGaussian (SINGLE) highlights potential training difficulties. The authors should investigate and address these challenges to improve the model's practicality.
Questions for the Authors
1. Why is Gaussian attention expected to outperform inner-product-based attention in KB modeling and Q&A? Can you provide theoretical insights or empirical comparisons to support this claim?
2. How does the model handle scalability, given the added complexity of Gaussian attention? Have you tested it on larger KBs or Q&A datasets?
3. Can you elaborate on the training challenges faced by TransGaussian (SINGLE) and how they might be mitigated?
4. Why does TransGaussian underperform on certain tasks (e.g., WordNet link prediction and specific Q&A queries)? How do you explain these results?
While the paper introduces an interesting idea, the lack of sufficient justification, experimental rigor, and clarity limits its impact. Addressing these issues in a future submission could significantly strengthen the work.