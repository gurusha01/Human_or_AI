Review of the Paper
The paper proposes MT-LRP, a novel approach to learning state representations in multi-task reinforcement learning (RL). It addresses the largely unexplored problem of learning task-specific state representations from raw observations in an unsupervised manner, without prior knowledge of task labels or the number of tasks. The method introduces a gated neural network architecture and a task coherence prior to ensure temporal consistency in task detection. The authors demonstrate the effectiveness of their approach through experiments on toy tasks, including slot-car racing and navigation.
Decision: Reject
While the paper presents a promising direction and introduces novel contributions, it falls short in providing comprehensive evaluations and addressing scalability concerns. The lack of experiments on realistic, high-dimensional tasks and incomplete evaluation of the navigation task limit the paper's impact and generalizability.
Supporting Arguments for the Decision
1. Strengths:
   - The paper explores a novel direction by focusing on multi-task state representation learning, which is orthogonal to traditional multi-task learning approaches.
   - The introduction of a task coherence prior is innovative and improves task detection and state representation learning.
   - The slot-car racing experiment demonstrates the method's effectiveness in a controlled setting, showing improvements over baseline methods.
2. Weaknesses:
   - The evaluation is incomplete. The navigation task lacks a proper evaluation of the control policy and comparisons to baselines, leaving the results inconclusive.
   - Scalability is a significant concern. The method is only tested on toy tasks with low-dimensional observations, raising doubts about its applicability to more realistic scenarios such as 3D environments or high-dimensional control tasks.
   - The method requires separate policies for each task, which could be computationally expensive and impractical for scenarios with a large number of tasks.
   - The paper does not adequately discuss its relationship to related works such as policy distillation and actor-mimic approaches, missing an opportunity to position the contribution more clearly within the literature.
Suggestions for Improvement
1. Evaluate the control policy in the navigation task and provide comparisons to baseline methods to make the results more robust and complete.
2. Test the method on more challenging, realistic tasks (e.g., 3D environments or high-dimensional control) to demonstrate scalability and generalizability.
3. Include a discussion on how the approach could be combined with other state representation learning methods or multi-task learning techniques to address the need for separate policies.
4. Provide additional comparisons, such as single-task performance as an upper bound, and assess whether MT-LRP accelerates learning or improves final performance.
5. Address the aliasing issues in the experiments by using higher-resolution images to improve the quality of the evaluation.
6. Clarify the necessity and influence of the task-separation loss term, particularly in scenarios with short episode lengths or a mismatch between the number of expected and actual tasks.
Questions for the Authors
1. How does the method perform when scaled to more complex environments with high-dimensional observations? Have any preliminary experiments been conducted in this direction?
2. Can the task coherence prior be extended or modified to handle scenarios where tasks may switch within an episode?
3. How does MT-LRP compare to policy distillation or actor-mimic methods in terms of computational efficiency and performance?
4. Is there a way to reduce the computational overhead of maintaining separate policies for each task, such as by sharing parameters across tasks?
In conclusion, while the paper introduces a novel approach with potential, it requires more rigorous evaluation and broader applicability to be suitable for acceptance. The suggestions provided aim to strengthen the contribution and address the current limitations.