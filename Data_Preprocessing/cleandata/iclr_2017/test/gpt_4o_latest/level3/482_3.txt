Review
Summary of Contributions
This paper addresses the critical issue of incomplete medication lists in electronic medical records (EMRs) by proposing a computational approach to predict active medications based on sequences of diagnostic billing codes. The authors compare the performance of recurrent neural networks (GRUs and LSTMs), feed-forward networks, and random forests for this task. The study demonstrates that GRUs achieve the best performance, with a micro-averaged AUC of 0.93 and a Label Ranking Loss of 0.076, suggesting the utility of sequence-aware models in this domain. The authors also explore the embeddings learned by the models, revealing clinically meaningful groupings of diagnostic codes. The work highlights the potential of these models to assist in medication reconciliation and improve the quality of EMRs.
Decision: Reject
While the paper presents a well-executed study with significant implications for medical informatics, it is not well-aligned with the scope of ICLR. The focus on domain-specific applications and the lack of novel contributions to machine learning theory or methodology make it more suitable for a medical or data science venue.
Supporting Arguments
1. Alignment with ICLR Scope: The paper primarily applies existing machine learning techniques (GRUs, LSTMs, random forests) to a specific medical problem. While the application is impactful, the work does not introduce novel machine learning architectures, algorithms, or theoretical insights, which are central to ICLR.
   
2. Scientific Rigor and Results: The study is methodologically sound, with robust experiments and clear reporting of results. The authors provide a thorough comparison of models and justify their choice of evaluation metrics. The analysis of embeddings is a valuable addition, offering insights into the semantic relationships between diagnostic codes.
3. Motivation and Literature Context: The problem of incomplete medication lists is well-motivated, and the authors position their work effectively within the existing literature. However, the novelty lies more in the application domain than in advancing machine learning research.
Additional Feedback
1. Model Interpretability: While the authors discuss the embeddings and provide anecdotal examples of predictions, a more systematic evaluation of model interpretability would strengthen the paper. For example, how do the learned embeddings compare to existing medical ontologies?
2. Hardware Constraints: The authors note that the GRU and LSTM models were limited by hardware constraints. It would be helpful to discuss the potential impact of scaling these models and whether the observed performance gap between recurrent and feed-forward networks might widen with larger architectures.
3. Generalizability: The study focuses on data from a single institution. A discussion on the generalizability of the results to other healthcare systems with different coding practices or patient populations would be valuable.
4. Clinical Validation: While the paper suggests that the models could assist in medication reconciliation, a more detailed discussion of how these predictions would integrate into clinical workflows would enhance the practical relevance of the work.
Questions for the Authors
1. How do the embeddings learned by the GRU and LSTM models compare to existing medical ontologies like SNOMED or ICD-10 in terms of semantic relationships?
2. Did you explore any attention-based mechanisms (e.g., Transformer models) for this task, and if so, how did they perform relative to GRUs and LSTMs?
3. Can you provide more details on the potential clinical deployment of this model? For example, how would the predictions be presented to clinicians, and what level of accuracy would be required for adoption?
In conclusion, while this paper makes a valuable contribution to the medical domain, its lack of novelty in machine learning methodology and its domain-specific focus make it better suited for a medical informatics or applied data science conference.