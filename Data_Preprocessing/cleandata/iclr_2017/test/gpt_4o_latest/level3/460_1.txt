Review of the Paper
Summary
This paper introduces ACER (Actor-Critic with Experience Replay), a reinforcement learning (RL) algorithm designed to address the challenges of sample inefficiency and instability in off-policy learning. The authors propose three key innovations: truncated importance sampling with bias correction, stochastic dueling networks (SDNs), and an efficient trust region policy optimization method. These techniques are integrated with the Retrace algorithm, resulting in a stable and scalable actor-critic framework applicable to both discrete and continuous action spaces. The empirical results demonstrate significant improvements in sample efficiency on Atari and MuJoCo benchmarks, outperforming state-of-the-art methods like A3C and DQN in various settings. The paper also provides theoretical insights into the Retrace operator, proving its equivalence to a truncated importance sampling approach with bias correction.
Decision: Accept
The paper makes a substantial contribution to the field of reinforcement learning by addressing a critical problem—sample inefficiency in actor-critic methods—through a principled and innovative approach. The proposed techniques are well-motivated, rigorously evaluated, and demonstrate strong empirical performance across diverse benchmarks. However, further ablation studies to isolate the contributions of each individual component (e.g., SDNs, trust region updates) would strengthen the paper.
Supporting Arguments
1. Problem Relevance and Novelty: The paper tackles a well-recognized challenge in RL—improving sample efficiency in off-policy learning—by combining recent advances (e.g., Retrace) with novel contributions (e.g., truncated importance sampling with bias correction). The proposed approach is both innovative and practically impactful.
   
2. Empirical Rigor: The experiments are extensive, covering both discrete (Atari) and continuous (MuJoCo) action spaces. The results convincingly demonstrate that ACER achieves superior sample efficiency compared to baselines like A3C, DQN, and Prioritized Replay DQN. The ablation studies in the continuous domain further highlight the importance of the proposed components.
3. Theoretical Contributions: The paper provides a theoretical analysis of the Retrace operator, showing its equivalence to truncated importance sampling with bias correction. This analysis adds depth to the work and strengthens its scientific rigor.
4. Practical Applicability: The proposed trust region optimization method is computationally efficient and scalable, making it suitable for large-scale RL problems. This is a significant improvement over traditional TRPO, which is computationally expensive.
Suggestions for Improvement
1. Ablation Studies: While the paper includes some ablation studies in the continuous domain, it would be beneficial to evaluate the individual contributions of each proposed technique (e.g., truncated importance sampling, SDNs, trust region updates) in the discrete domain as well. This would provide a clearer understanding of the relative importance of each component.
2. Clarity in Presentation: The paper is dense with technical details, which may make it difficult for readers unfamiliar with the topic to follow. Simplifying or summarizing key equations and providing more intuitive explanations for the proposed methods (e.g., the trust region update) would improve accessibility.
3. Additional Benchmarks: While the Atari and MuJoCo benchmarks are standard, including results on other RL environments (e.g., robotics or real-world tasks) could further demonstrate the generalizability of ACER.
Questions for the Authors
1. How do the proposed techniques (e.g., truncated importance sampling, SDNs) interact with each other? Are there any synergies or trade-offs that arise when combining them?
2. Could you provide additional insights into the choice of hyperparameters (e.g., the truncation threshold `c`)? How sensitive is ACER to these hyperparameters across different environments?
3. In the ablation studies, why does the removal of truncation with bias correction have a significant impact in high-dimensional action spaces (e.g., Humanoid) but not in lower-dimensional tasks (e.g., Fish, Walker)?
Overall, this paper makes a strong contribution to reinforcement learning and is well-suited for acceptance at the conference.