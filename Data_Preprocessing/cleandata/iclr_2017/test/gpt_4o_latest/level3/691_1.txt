Review of "Retro Learning Environment (RLE)" Paper
Summary of Contributions
This paper introduces the Retro Learning Environment (RLE), a new reinforcement learning (RL) framework that extends the capabilities of the widely-used Arcade Learning Environment (ALE). RLE supports games from the Super Nintendo Entertainment System (SNES), Sega Genesis, and other platforms, offering a more complex and diverse set of challenges for RL algorithms. The authors benchmark standard RL algorithms on five SNES games and propose a "rivalry metric" to evaluate multi-agent reinforcement learning (MARL). The paper highlights the potential of RLE to advance RL research by providing a standardized, extensible environment with richer game dynamics, delayed rewards, and larger action spaces compared to ALE. Additionally, the authors explore reward shaping and multi-agent training as methods to improve learning outcomes.
Decision: Reject
While the RLE framework is a valuable contribution to the RL community, the paper lacks significant scientific contributions beyond the introduction of the environment. The experimental results, particularly on rivalry training, are underwhelming and fail to provide novel insights. Furthermore, the findings on reward shaping and domain knowledge, while relevant, are largely intuitive and do not advance the state of the art. The paper would be better suited for a venue like mloss.org or JMLR's journal track, which focus on tools and frameworks, rather than ICLR, which emphasizes scientific innovation.
Supporting Arguments
1. Novelty and Scientific Contribution: The primary contribution of the paper is the introduction of RLE, which is a valuable tool for the RL community. However, the paper does not provide groundbreaking scientific insights or novel methodologies. The findings on reward shaping and policy robustness are expected and do not push the boundaries of RL research.
   
2. Experimental Results: The benchmark results demonstrate that SNES games are more challenging than Atari games, but the analysis lacks depth. The "rivalry metric" and multi-agent experiments are interesting but suffer from issues like overfitting to opponents and inconsistent evaluation criteria. The catastrophic forgetting observed in rivalry training highlights a limitation but does not offer a clear solution or novel approach to address it.
3. Suitability for ICLR: ICLR prioritizes papers with strong theoretical or empirical contributions. While RLE is a useful framework, the paper does not offer sufficient scientific rigor or innovation to meet the conference's standards.
Suggestions for Improvement
1. Strengthen Scientific Contributions: The authors could explore novel RL algorithms or techniques specifically designed to address the challenges posed by RLE, such as delayed rewards, large action spaces, or stochastic opponent behavior. This would make the paper more suitable for ICLR.
   
2. Rivalry Metric and Multi-Agent Training: The rivalry metric is an intriguing concept but requires more robust evaluation and generalization strategies. The authors should address overfitting and propose methods to improve policy robustness in multi-agent settings.
3. Benchmarking and Analysis: The experimental results could be expanded to include comparisons with more advanced RL algorithms, such as those incorporating exploration strategies or hierarchical learning. Additionally, the authors could provide a deeper analysis of why certain games are particularly challenging for existing algorithms.
4. Positioning in the Literature: While the paper references ALE and other environments, it could better position RLE in the broader context of RL research by discussing how it addresses gaps or limitations in existing benchmarks.
Questions for the Authors
1. How does RLE handle the computational overhead of more complex games compared to ALE? Are there any performance trade-offs?
2. Can the authors provide more details on how the "rivalry metric" is calculated and how it could be standardized for broader use in MARL research?
3. Have the authors considered incorporating exploration-based strategies (e.g., intrinsic motivation) to address the challenges of sparse and delayed rewards in RLE?
4. What steps can be taken to mitigate the overfitting observed in rivalry training? Could curriculum learning or opponent diversity play a role?
In conclusion, while RLE is a promising tool for RL research, the lack of significant scientific contributions and polished experimental results limits its suitability for ICLR. Addressing the above points could substantially improve the paper's impact and relevance.