The paper introduces Doubly Recurrent Neural Networks (DRNNs), a novel architecture for decoding tree structures, with two key innovations: bidirectional information flow (parent-to-children and sibling-to-sibling) and a probability-based tree boundary model. The latter eliminates the need for special ending symbols, reducing parameter complexity and improving efficiency. DRNNs outperform traditional sequence-to-sequence (seq2seq) models in recovering synthetic trees and mapping natural language to functional programs. However, the synthetic tree recovery task is criticized for being overly simplistic due to inherent topological information in surface forms. Additionally, DRNN performance degrades significantly for trees with a large number of nodes, limiting its scalability for tasks requiring long-range dependencies.
Decision: Reject
Key Reasons:
1. Limited Task Complexity: While the DRNN demonstrates strong performance on synthetic tree recovery and functional program mapping, the synthetic task is too simple to convincingly showcase the model's advantages. The inherent topological cues in the data reduce the challenge, making the results less generalizable to real-world scenarios.
2. Scalability Concerns: The significant drop in performance as the number of nodes increases raises concerns about the model's applicability to tasks with longer information flows, such as large-scale parsing or machine translation.
Supporting Arguments:
- The probability-based boundary model is an elegant solution to avoid padding tokens, simplifying tree generation. However, the paper does not sufficiently explore how this innovation performs in more challenging, real-world tasks like seq2seq parsing or large-scale machine translation.
- The experimental results on the IFTTT dataset and synthetic trees are promising but limited in scope. The lack of exploration into more complex datasets or tasks undermines the broader applicability of the proposed architecture.
- The scalability issue is a critical limitation, as many real-world applications involve large and complex tree structures. Without addressing this, the utility of DRNNs remains constrained.
Additional Feedback:
1. The authors should explore more challenging tasks, such as seq2seq parsing or dependency tree generation for natural language, to better demonstrate the advantages of DRNNs over traditional models.
2. Investigate methods to improve scalability, such as hierarchical or coarse-to-fine decoding strategies, to address the performance drop for larger trees.
3. Provide a deeper analysis of the failure cases, particularly for large trees, to identify specific bottlenecks in the architecture.
4. Clarify the computational efficiency of DRNNs compared to other tree decoders, especially in terms of training and inference time.
Questions for the Authors:
1. How does the DRNN perform on tasks with more complex tree structures, such as dependency parsing or abstract syntax tree generation for programming languages?
2. Can the scalability issue be mitigated by architectural modifications or training strategies, such as curriculum learning or attention mechanisms?
3. How does the probability-based boundary model compare to padding-based approaches in terms of computational efficiency and accuracy on larger datasets?
While the paper introduces a novel and promising architecture, its limitations in task complexity and scalability prevent it from making a strong case for acceptance at this stage. Addressing these issues in future work could significantly enhance its impact.