Review of the Paper
Summary of Contributions
This paper aims to identify recurring design patterns in convolutional neural network (CNN) architectures, providing a conceptual "coordinate system" to guide both novice and experienced practitioners in designing deep learning models. The authors propose 14 design patterns distilled from a comprehensive review of recent CNN innovations. Additionally, the paper introduces novel architectural concepts, such as Fractal of FractalNet (FoF), Stagewise Boosting Networks (SBN), and Taylor Series Networks (TSN), which are evaluated on CIFAR-10 and CIFAR-100 datasets. The authors emphasize the "community service" aspect of their work, aiming to demystify CNN design principles and inspire further research in this area.
Decision: Reject
While the paper makes a commendable attempt to organize and distill CNN design principles, it falls short in terms of clarity, rigor, and practical applicability. The lack of actionable guidance for several patterns, unclear connections between proposed architectures and the identified patterns, and insufficient empirical validation undermine the overall contribution.
Supporting Arguments
1. Strengths:
   - The literature review is thorough, highlighting overlooked developments and providing a strong foundation for the proposed design patterns.
   - The paper's intent to serve as a resource for newcomers is valuable, addressing a genuine need in the deep learning community.
   - The novel architectural ideas (e.g., FoF, SBN, TSN) are intriguing and demonstrate creative thinking.
2. Weaknesses:
   - Lack of Clarity in Patterns: The selection of 14 patterns appears arbitrary, with some (e.g., "Maxout for Competition") being poorly justified and others (e.g., "Increase Symmetry") being vague and lacking actionable insights.
   - Confusing Terminology: Pattern names like "Cover the Problem Space" and "Over-train" are ambiguous and not well-explained, which could confuse readers.
   - Weak Empirical Validation: While the proposed architectures are tested on CIFAR datasets, the results are preliminary and do not convincingly demonstrate the practical utility of the design patterns. For example, the FoF network performs similarly to FractalNet, and SBN/TSN lag behind in final performance.
   - Unclear Connections: The relationship between the proposed architectures (e.g., TSN) and the identified design patterns is not well articulated. For instance, the connection between "freeze-drop-path" and "Increase Symmetry" is ambiguous.
   - Usability Issues: The absence of a clear mapping between design patterns and specific architectures (e.g., a summary table) reduces the paper's practical utility.
Additional Feedback for Improvement
1. Clarify Pattern Selection: Provide a more systematic rationale for the selection of the 14 patterns. Why were these patterns chosen, and how do they generalize across architectures?
2. Improve Pattern Definitions: Offer more precise and actionable descriptions for each pattern. For example, explain how "Increase Symmetry" can be operationalized in designing new architectures.
3. Refine Terminology: Use more intuitive and descriptive names for patterns to avoid confusion. For instance, "Over-train" could be renamed to reflect its focus on regularization techniques.
4. Strengthen Empirical Validation: Conduct more comprehensive experiments to demonstrate the utility of the design patterns in creating state-of-the-art architectures. Consider testing on diverse datasets and tasks beyond image classification.
5. Add a Mapping Table: Include a table mapping the 14 design patterns to specific architectures and their components. This would enhance the paper's usability for practitioners.
6. Explain Connections: Clearly articulate how the proposed architectures (FoF, SBN, TSN) embody the identified design patterns. For instance, provide concrete justification for the use of "freeze-drop-path" in SBN.
Questions for the Authors
1. What criteria were used to select the 14 design patterns? Could you provide examples of patterns that were considered but excluded, and why?
2. How do you envision practitioners applying these patterns to design new architectures? Could you include a step-by-step example?
3. Why do SBN and TSN underperform compared to FractalNet? Are there specific scenarios where these architectures might excel?
4. Could you elaborate on the connection between "Increase Symmetry" and "freeze-drop-path"? How does the latter operationalize the former?
In summary, while the paper addresses an important problem and demonstrates potential, significant revisions are needed to improve clarity, rigor, and practical impact.