Review
Summary of Contributions
This paper introduces a novel semi-supervised learning framework, CC-GAN, which integrates inpainting and adversarial loss within the GAN framework. The proposed approach leverages context conditioning to train a generator to fill in missing image patches and a discriminator to differentiate between real and inpainted images. The authors extend this to a combined CC-GAN2 model, which incorporates additional adversarial tasks. The framework is applied to larger datasets (STL-10 and Pascal VOC 2007), achieving state-of-the-art results in semi-supervised classification. The paper also demonstrates the ability to train large VGG-style networks directly using adversarial loss, which is a notable improvement over prior works. Additionally, the authors provide qualitative results showcasing semantically plausible inpainting outputs.
Decision: Reject
While the paper demonstrates strong empirical results and effective application of GAN-based semi-supervised learning to larger datasets, the work lacks sufficient novelty in its core ideas. The use of GAN discriminative features for semi-supervised learning is well-established in prior literature, and the proposed context conditioning approach appears to be an incremental extension rather than a fundamentally new contribution. Furthermore, the evaluation is incomplete, as critical baselines and comparisons are missing, particularly for the Pascal dataset.
Supporting Arguments for Decision
1. Lack of Novelty: The idea of leveraging GAN discriminative features for semi-supervised learning is not new. While the context conditioning approach is an interesting extension, it does not represent a significant conceptual leap. The CC-GAN2 model, in particular, seems more like a workaround to address challenges in scaling rather than a genuine improvement on the core CC-GAN idea.
   
2. Incomplete Evaluation: The paper does not provide an SSL-GAN baseline for the Pascal dataset, making it difficult to assess the relative contribution of the context conditioning approach. Additionally, the feasibility of applying SSL-GAN to downsampled Pascal datasets (e.g., 64x64, 96x96) is not explored, leaving questions about the scalability of the proposed method unanswered.
3. Empirical Rigor: While the results on STL-10 and Pascal are strong, the lack of comparisons on additional datasets limits the generalizability of the claims. A broader evaluation, including downsampled datasets, would strengthen the paper.
Suggestions for Improvement
1. Provide Baselines: Include an SSL-GAN baseline for the Pascal dataset to enable a direct comparison with the proposed CC-GAN framework. This would clarify the specific advantages of the context conditioning approach.
   
2. Explore Scalability: Investigate the feasibility of applying SSL-GAN to downsampled Pascal datasets and provide explanations if training challenges arise. This would address concerns about the scalability of the proposed method.
3. Broader Evaluation: Extend the evaluation to additional datasets, even if they are downsampled. This would help establish the generalizability of the proposed approach.
4. Clarify CC-GAN2 Motivation: Provide a stronger justification for the introduction of CC-GAN2. Explain how it represents a meaningful improvement over CC-GAN rather than a workaround for training challenges.
Questions for Authors
1. Why was an SSL-GAN baseline not included for the Pascal dataset? How does the context conditioning approach compare to standard SSL-GAN methods on this dataset?
2. What specific challenges prevent the application of SSL-GAN to downsampled Pascal datasets? Could these challenges also affect the scalability of CC-GAN?
3. How does the proposed CC-GAN2 model fundamentally improve upon CC-GAN, beyond introducing additional adversarial tasks?
In summary, while the paper demonstrates strong empirical results and effective application of GAN-based semi-supervised learning, the lack of novelty and incomplete evaluation limit its contribution. Addressing these issues could significantly strengthen the work.