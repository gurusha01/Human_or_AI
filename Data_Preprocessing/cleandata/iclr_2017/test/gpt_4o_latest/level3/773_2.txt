Review of the Paper
This paper introduces Rectified Factor Networks (RFNs) as a novel approach to biclustering, addressing the limitations of the widely-used FABIA model. The authors claim that RFNs, through the use of rectified linear units (ReLU) and dropout, offer significant improvements in computational efficiency, sparsity, and decorrelation of biclusters. The paper presents extensive experimental results on synthetic, gene expression, and genomic datasets, demonstrating RFN's superior performance compared to 13 other biclustering methods, including FABIA.
Decision: Reject
While the paper presents a promising idea and demonstrates strong empirical results, the lack of clarity in the explanation of the novel contribution (Section 2.2) and insufficient evidence for some key claims undermine its overall impact. Below, I provide detailed reasoning for this decision.
Supporting Arguments for the Decision
1. Clarity: The novel contribution in Section 2.2, which is central to the paper, is difficult to follow due to inconsistent notation and insufficient explanation of the RFN biclustering model. For instance, the process of rectifying and normalizing the posterior mean is not clearly described, and the connection between dropout and sparsity in bicluster assignments is not well articulated. This lack of clarity makes it challenging to fully understand and evaluate the proposed approach.
2. Originality and Significance: The application of RFNs with ReLU and dropout to biclustering is novel and has the potential to advance the field. The proposed method addresses practical challenges in FABIA, such as computational inefficiency and insufficient decorrelation. However, the claim that dropout increases sparsity in bicluster assignments lacks rigorous experimental evidence, which weakens the paper's overall contribution.
3. Scientific Rigor: While the experiments are extensive and well-conducted, some claims are not adequately supported. For example, the assertion that GPU compatibility leads to faster performance is not quantified, and the number of biclusters supported by RFNs is not clearly specified. Additionally, the depth of the model is questionable, as the use of ReLU and dropout alone does not necessarily qualify the model as "deep."
Suggestions for Improvement
1. Clarity: Revise Section 2.2 to provide a more detailed and consistent explanation of the RFN biclustering model. Clearly define the mathematical operations and their role in achieving sparsity and decorrelation.
2. Evidence for Claims: Strengthen the experimental evidence for key claims. For example:
   - Provide quantitative results comparing the speed of RFNs on GPUs versus CPUs.
   - Include experiments or ablation studies to demonstrate the impact of dropout on sparsity in bicluster assignments.
3. Depth of the Model: Address the concern about the model's depth. If the model is not deep in the traditional sense, clarify this in the paper to avoid confusion.
Questions for the Authors
1. Can you provide a more detailed explanation of how dropout contributes to increased sparsity in bicluster assignments? Have you conducted any experiments to isolate this effect?
2. What is the maximum number of biclusters that RFNs can support, and how does this compare to FABIA in practice?
3. How does the GPU implementation of RFNs quantitatively improve performance? Can you provide runtime comparisons for different dataset sizes?
In summary, while the paper introduces an interesting and potentially impactful approach to biclustering, the lack of clarity and insufficient evidence for key claims prevent it from meeting the standards for acceptance at this time. Addressing these issues in a revised submission could significantly strengthen the paper.