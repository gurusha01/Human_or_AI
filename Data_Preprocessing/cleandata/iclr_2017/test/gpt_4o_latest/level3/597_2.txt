Review of "Neural Combinatorial Optimization: A Framework for Tackling Combinatorial Optimization Problems Using Neural Networks and Reinforcement Learning"
Summary of the Paper
This paper introduces a novel framework, Neural Combinatorial Optimization, that leverages neural networks and reinforcement learning (RL) to tackle combinatorial optimization problems. The authors focus on the Traveling Salesman Problem (TSP) and the Knapsack Problem, demonstrating the flexibility of their approach. The framework employs a recurrent neural network (RNN) with a pointer network architecture, trained using policy gradient methods. The paper explores two approaches: RL pretraining and active search, and provides experimental results on 2D Euclidean TSP instances with up to 100 nodes, as well as Knapsack instances with up to 200 items. While the method does not achieve state-of-the-art results, it provides insights into the potential of neural networks as general-purpose tools for optimization problems.
Decision: Reject
Key reasons for rejection:
1. Invalid Baseline Comparisons: The claims of outperforming baselines for TSP are undermined by the omission of the state-of-the-art Lin-Kernighan-Helsgaun (LK-H) heuristic in key comparisons. The paper instead compares its method to outdated or weaker baselines, which misrepresents its performance.
2. Misleading Presentation of Results: Figure 1 is misleading, as it compares the proposed method to an outdated local search implementation rather than the stronger LK-H. The authors failed to update this figure in the revised version, perpetuating misconceptions about the method's relative performance.
Supporting Arguments
1. Baseline Issues: The exclusion of LK-H as a baseline for TSP and the removal of strong baselines (ExpKnap and MinKnap) for the Knapsack problem in favor of weaker alternatives (e.g., random search, greedy) is a significant flaw. This creates an inflated perception of the method's effectiveness and is scientifically irresponsible.
2. Misleading Claims: The statement that the method is "a few percents worse than optimality" downplays the well-known difficulty of achieving the last few percentage points of improvement in TSP. This claim is not sufficiently contextualized, especially given the superior performance of LK-H.
3. Inconsistencies in Methodology: The use of Google's OR-tools for TSP but not for the Knapsack problem raises questions about consistency and fairness in the evaluation process.
Additional Feedback for Improvement
1. Baseline Comparisons: The authors should include LK-H and other state-of-the-art solvers as baselines for TSP and restore strong baselines (ExpKnap, MinKnap) for the Knapsack problem. This would provide a more accurate assessment of the method's performance.
2. Clarify RL Pretraining Results: Missing performance values for RL pretraining with 10,000 batches in Table 3 should be included, as these are critical for evaluating the method's effectiveness.
3. Explain Sampling Temperature (T): The difference between RL pretraining Sampling T=1 and T=T* is unclear and needs further elaboration.
4. Code Release Timeline: The promise to release the model and training code is appreciated, but the authors should provide a clear timeline for availability to enhance reproducibility and community trust.
Questions for the Authors
1. Why was LK-H excluded from baseline comparisons for TSP, given its well-established state-of-the-art performance?
2. What motivated the removal of ExpKnap and MinKnap as baselines for the Knapsack problem in the revised version?
3. Can you clarify the difference in behavior and performance between Sampling T=1 and T=T* in RL pretraining?
4. When will the model and training code be made publicly available?
In summary, while the paper presents an interesting methodological contribution, its evaluation and presentation suffer from significant shortcomings that undermine its scientific rigor. Addressing these issues would greatly improve the paper's credibility and impact.