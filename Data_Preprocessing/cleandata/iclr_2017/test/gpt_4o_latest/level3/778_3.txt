Review
Summary of the Paper
This paper introduces a novel quantization method for compressing and accelerating deep neural networks without requiring retraining. The proposed approach factorizes both weights and activations into ternary and binary components, enabling significant reductions in memory usage and computational complexity. Specifically, the method achieves up to 20x compression and 15x speed-up on the VGG-16 model while maintaining a minimal increase in error rates. The paper also demonstrates the applicability of the method to various architectures and tasks, including handwritten digit recognition, ImageNet classification, and face recognition. The methodology is well-documented, and the results are clearly presented, showcasing the potential of the approach for deployment on low-power hardware.
Decision: Reject  
While the paper presents an interesting and novel approach, it falls short in several critical areas. The lack of comparison with other pruning and compression methods, as well as the unclear practical advantages of the proposed method, limits its impact and generalizability. Additionally, the method's limited effectiveness on convolutional layers raises concerns about its applicability to modern all-convolutional architectures.
Supporting Arguments for the Decision
1. Lack of Comparative Analysis: The paper does not adequately compare its method to other state-of-the-art pruning and quantization techniques mentioned in Section 1.1. Without such comparisons, it is difficult to assess the relative performance of the proposed approach in terms of accuracy, compression ratio, and speed-up.
   
2. Limited Applicability: Most of the reported speed-up and memory savings come from fully connected layers, which are less prominent in modern architectures like Inception and ResNet. The method's limited impact on convolutional layers raises concerns about its broader applicability.
3. Unclear Practical Advantages: While the method eliminates the need for retraining, the practical benefits of this feature are not quantified. For example, the paper does not provide a detailed analysis of how much time or computational resources are saved during the compression process compared to methods requiring retraining.
Additional Feedback for Improvement
1. Comparative Benchmarks: Include a thorough comparison with other state-of-the-art methods, such as pruning, re-training, and vector quantization, in terms of accuracy, compression, and speed-up. This would provide a clearer picture of the method's strengths and weaknesses.
   
2. Focus on Convolutional Layers: Since modern architectures rely heavily on convolutional layers, the paper should explore ways to improve the effectiveness of the proposed method on these layers. For example, optimizing the ternary decomposition for convolutional filters could make the method more broadly applicable.
3. Quantify Practical Benefits: Provide a detailed analysis of the time and computational savings achieved by avoiding retraining. This would help clarify the practical advantages of the method for real-world deployments.
4. Expand Experimental Scope: Test the method on all-convolutional networks like ResNet or Inception to evaluate its performance on architectures without fully connected layers.
Questions for the Authors
1. How does the proposed method compare to other pruning and quantization techniques in terms of accuracy, compression ratio, and speed-up?
2. Can the method be adapted or improved to achieve better results on convolutional layers?
3. What are the specific time and computational savings achieved by avoiding retraining, and how do these compare to the overall compression and acceleration benefits?
In conclusion, while the paper presents a novel and promising approach, addressing the above concerns would significantly enhance its impact and applicability.