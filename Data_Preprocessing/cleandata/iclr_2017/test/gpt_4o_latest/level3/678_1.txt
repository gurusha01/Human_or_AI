Review
This paper investigates transfer learning in reading comprehension using the AS Reader model, focusing on three key experiments. The authors explore the transfer of knowledge from large artificial datasets (BookTest and CNN/Daily Mail) to smaller target datasets (bAbI tasks and a subset of SQuAD). The study provides insights into the effectiveness of pre-training and fine-tuning strategies, as well as the contributions of specific model components (word embeddings and encoders) to transfer learning.
The paper makes several contributions: (1) It demonstrates that pre-training on large datasets alone results in limited transfer to target tasks, especially without fine-tuning. (2) It shows that fine-tuning on even small samples of target-domain data significantly improves performance, highlighting the value of pre-training. (3) It provides evidence that pre-training benefits both word embeddings and encoders, suggesting that transferable knowledge is distributed across the model. These findings are relevant to the NLP community, particularly for applications where labeled data is scarce.
Decision: Accept
The paper is well-motivated and addresses an important problem in NLPâ€”how to leverage data-rich domains to improve performance in data-scarce domains. The experiments are scientifically rigorous and provide valuable insights into transfer learning for reading comprehension. While the results do not advance state-of-the-art performance, the study's focus on generalization and transferability is a meaningful contribution to the field.
Supporting Arguments
1. Motivation and Placement in Literature: The paper is well-situated in the context of existing research on transfer learning and reading comprehension. It addresses a gap by focusing on transfer learning in reading comprehension, a less-explored area compared to text classification or parsing.
2. Scientific Rigor: The experiments are methodologically sound, with clear comparisons between pre-trained and randomly initialized models. The use of multiple datasets and fine-grained analyses (e.g., resetting components of the model) strengthens the validity of the conclusions.
3. Clarity of Results: The results are presented with sufficient detail, including mean performance, standard deviations, and statistical significance tests. The findings are interpretable and actionable for future research.
Suggestions for Improvement
1. Clarity of Experimental Setup: The experimental setup could be simplified for better readability. Terms like "best validation" should be explicitly defined, and the rationale for certain design choices (e.g., cloze-style conversion of bAbI tasks) should be clarified.
2. Reporting Metrics: While the paper reports mean performance, including standard deviations consistently across all experiments would improve clarity and allow readers to assess variability.
3. Unexplained Acronyms and Typos: Acronyms such as GRU, BT, and CBT should be defined upon first use. Minor typos (e.g., "benfits" on p. 2, "subsubset" on p. 6) should be corrected for professionalism.
4. Broader Implications: The paper could benefit from a discussion on the broader implications of the findings, particularly how this work might influence the design of future transfer learning models or datasets.
Questions for the Authors
1. How does the performance of the AS Reader compare to other transfer learning models on the same tasks? Would incorporating additional pre-training datasets improve transferability?
2. Could the poor transferability observed in Experiment 1 be attributed to the differences in task structure (e.g., cloze-style vs. natural questions)? If so, how might this be addressed in future work?
3. In Experiment 3, did resetting the encoder or embeddings impact convergence speed during fine-tuning? If so, how might this influence the practicality of transfer learning in real-world applications?
Overall, this paper provides a solid foundation for future research in transfer learning for reading comprehension and is a valuable contribution to the field.