Review of the Paper
Summary of Contributions
The paper investigates the geometry of loss functions in deep neural networks and the behavior of stochastic optimization methods in navigating these loss landscapes. Through empirical analyses, the authors aim to understand how different optimization algorithms find and interact with local minima. The paper introduces visualizations of loss surfaces in low-dimensional subspaces and explores the effects of switching optimization methods during training. Additionally, it examines the role of batch normalization and extreme parameter initializations in shaping the loss surface. The authors propose a novel augmentation of optimization methods using second-order Runge-Kutta integrators and compare their performance with standard methods.
Decision: Reject  
The primary reasons for rejection are the lack of clarity in the paper's presentation and insufficient scientific rigor in supporting its claims. While the topic is relevant and the empirical approach is interesting, the execution fails to meet the standards required for acceptance.
Supporting Arguments for Rejection
1. Lack of Clarity: The paper's presentation is convoluted, making it difficult to follow the experimental setup and interpret the results. Key ideas are buried in dense text, and the overuse of technical jargon without adequate explanation limits accessibility.
   
2. Overemphasis on Local Minima: The paper mentions "local minima" excessively (~70 times) but does not convincingly demonstrate their relevance or existence in the solutions. The authors fail to differentiate between local minima and saddle points, which are known to dominate high-dimensional loss landscapes.
3. Insufficient Context: The paper does not adequately address the possibility that slices of non-convex problems may resemble the examples shown. This oversight weakens the generalizability of the findings.
4. Superficial Analysis: Certain observations, such as incremental improvements during optimization, are presented without deep analysis. These statements come across as superficial and do not add meaningful insights.
5. Known Limitations of First-Order Methods: The failure of first-order methods on non-convex, ill-conditioned problems is well-documented in the literature. The paper does not provide novel insights into why these failures occur or how they relate to local minima.
Suggestions for Improvement
1. Clarify Presentation: The paper would benefit from a more structured and concise presentation. Clearly define the problem, hypotheses, and experimental setup. Use diagrams or flowcharts to explain complex ideas.
2. Strengthen Empirical Evidence: Provide rigorous evidence to support claims about local minima. For example, use Hessian-based methods to distinguish between local minima and saddle points.
3. Address Generalization: Discuss how the findings apply to broader classes of non-convex problems. Include experiments that test the robustness of the results across different architectures and datasets.
4. Deepen Analysis: Avoid superficial observations and delve deeper into the implications of the findings. For example, analyze why certain optimization methods lead to larger basins of attraction and whether this affects generalization.
5. Reduce Redundancy: The repeated focus on local minima detracts from the paper's overall impact. Streamline the discussion to focus on the most critical insights.
Questions for the Authors
1. How do you distinguish between local minima and saddle points in your experiments? Can you provide quantitative evidence for the existence of local minima?
2. Why do you believe the overparameterization of neural networks leads to qualitatively different local minima across optimization methods? How do you address the possibility of trivial reparameterizations?
3. Can you elaborate on why the Runge-Kutta integrator fails to outperform ADAM in your experiments? Could this be due to the interaction between momentum and adaptive learning rates?
In conclusion, while the paper addresses an important topic, its lack of clarity, overemphasis on local minima, and insufficient scientific rigor undermine its contributions. Significant revisions are necessary to make the work suitable for publication.