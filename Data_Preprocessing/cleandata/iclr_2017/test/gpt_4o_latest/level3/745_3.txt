Review of the Paper
Summary of Contributions
This paper introduces SYMSGD, a novel parallel stochastic gradient descent (SGD) algorithm that retains the sequential semantics of SGD in expectation. The method addresses the inherent sequential dependency in SGD by introducing probabilistically sound combiners, which allow local models learned by multiple threads to be combined into a global model equivalent to the result of sequential SGD. The approach is restricted to linear update rules and square loss functions but demonstrates significant speedups (up to 13Ã— on 16 cores) while maintaining accuracy comparable to sequential SGD. The paper also employs dimensionality reduction via the Johnson-Lindenstrauss lemma to mitigate the computational overhead of maintaining large combiner matrices. The authors provide extensive empirical evaluations on sparse and dense datasets, demonstrating the scalability and robustness of SYMSGD compared to existing methods like HOGWILD! and ALLREDUCE.
Decision: Reject
While the paper presents a novel and technically sound method for parallelizing SGD, its limited applicability to square loss functions with linear update rules significantly restricts its relevance to the broader machine learning community. Additionally, the computational overhead of maintaining combiner matrices, even with dimensionality reduction, may offset the practical benefits in real-world scenarios. Extending the method to non-linear objective functions would greatly enhance its impact and applicability.
Supporting Arguments
1. Novelty and Technical Soundness: The probabilistically sound combiner mechanism is innovative and mathematically rigorous. The use of dimensionality reduction to address scalability issues is well-motivated and effectively implemented. The paper also provides a thorough theoretical analysis and empirical validation, supporting its claims of accuracy and scalability.
   
2. Limited Applicability: The restriction to linear update rules and square loss functions is a significant limitation. Many real-world machine learning tasks, including classification problems, rely on non-linear objective functions such as logistic regression or deep learning models. While the authors claim that linear regression performs comparably to logistic regression on their datasets, this does not generalize to all tasks.
3. Overhead and Practicality: The computational cost of maintaining and reducing the combiner matrix, even with SIMD optimizations, may outweigh the benefits in scenarios with very high-dimensional datasets or limited computational resources. This is particularly concerning given the increasing prevalence of non-linear, high-dimensional models in modern machine learning.
Suggestions for Improvement
1. Extension to Non-Linear Objectives: Exploring methods to generalize SYMSGD to non-linear objective functions would significantly enhance its relevance and impact. For instance, the authors could investigate linear approximations (e.g., Taylor expansions) or other techniques to adapt their approach to broader classes of models.
2. Empirical Evaluation on Larger Datasets: While the paper evaluates SYMSGD on nine datasets, including both sparse and dense ones, it would be beneficial to include experiments on larger-scale, real-world datasets to better demonstrate its scalability and practical utility.
3. Comparison with Mini-Batch SGD: The paper does not explicitly compare SYMSGD with mini-batch SGD, which is a widely used approach for parallelizing SGD. Including such a comparison would provide a more comprehensive evaluation of the method's strengths and weaknesses.
4. Variance Control: The authors discuss variance introduced by dimensionality reduction but do not provide a detailed analysis of its impact on convergence rates. A deeper exploration of variance control mechanisms could strengthen the paper's contributions.
Questions for the Authors
1. Can the proposed method be adapted to support non-linear objective functions, and if so, how would this affect the computational overhead and accuracy?
2. How does SYMSGD compare to mini-batch SGD in terms of scalability, accuracy, and convergence rates?
3. What is the impact of increasing the number of features (e.g., millions or billions) on the computational cost of maintaining the reduced combiner matrix?
4. Could the initial "stuttering" in accuracy be mitigated without requiring additional communication overhead?
In conclusion, while the paper presents a novel and technically sound approach, its limited applicability and practical concerns reduce its relevance to the broader AI and machine learning community. Addressing these limitations in future work could make the method more impactful.