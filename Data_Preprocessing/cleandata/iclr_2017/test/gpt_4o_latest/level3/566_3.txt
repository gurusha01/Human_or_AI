Review of the Paper
Summary of Contributions
The paper introduces a novel active learning mechanism for Convolutional Neural Networks (CNNs) based on variational free energy and Fisher information approximations. The authors propose a batch active learning framework that scales to deep architectures, leveraging a Maximum Likelihood Estimation (MLE) approach to define prior and posterior distributions of network weights. The method uses Kronecker-factored approximations (KFAC) of Fisher matrices to make the approach computationally feasible. The paper demonstrates proof of concept on MNIST and USPS datasets, showing that the proposed method achieves competitive test accuracy using only 30% of the annotated training data. The work also highlights surprising results for uncertainty sampling and curriculum learning, particularly their failure on USPS, and provides a discussion on time complexity.
Decision: Reject
While the paper addresses an interesting and relevant problem in active learning for CNNs, it falls short in several critical areas. The primary reasons for rejection are the lack of clarity in the exposition and insufficient justification of key methodological choices and results.
Supporting Arguments
1. Clarity and Presentation: The paper is difficult to read due to numerous grammatical errors, inconsistent notation, and a lack of clear explanations for complex derivations. For example, the approximations in Section 3 are dense and poorly explained, making it challenging to follow the reasoning behind the proposed active learning criterion. Figures 1 and 2 also lack clarity, particularly the term "groundtruth," which is not adequately defined.
2. Depth of the Architecture: The proposed method is demonstrated on a shallow CNN architecture with only two hidden layers and 20 filters each. This limits the generalizability of the results to deeper architectures, which are more representative of modern deep learning applications.
3. Empirical Validation: The experiments are limited to toy datasets (MNIST and USPS), which are not sufficiently challenging to validate the scalability and robustness of the method. The surprising failure of uncertainty sampling and curriculum learning on USPS is not adequately explained, leaving key insights unaddressed.
4. Methodological Gaps: The necessity of sampling a larger subset before active learning selection is not well-justified, and its impact on results is unclear. Additionally, Section 5.2 discusses time complexity but does not compare it to the time required for minibatch backpropagation, which is critical for evaluating the practical utility of the method.
5. Code Availability: Given the complexity of the proposed approximations, the availability of code would significantly enhance reproducibility and understanding. The lack of code is a notable omission.
Suggestions for Improvement
1. Clarity and Grammar: The paper must undergo significant revisions to improve grammar, notation consistency, and the clarity of explanations. Key terms like "groundtruth" should be explicitly defined.
2. Experimental Validation: The method should be tested on more realistic and challenging datasets, such as CIFAR-10 or ImageNet, to demonstrate its applicability to modern deep learning tasks.
3. Architectural Depth: The experiments should include deeper CNN architectures to validate the scalability of the approach.
4. Explain Results: Provide a detailed explanation for the failure of uncertainty sampling and curriculum learning on USPS. This could offer valuable insights into the behavior of these methods under noisy conditions.
5. Time Complexity Comparison: Include a comparison of the active learning selection time with the time required for minibatch backpropagation to provide a complete picture of the computational trade-offs.
6. Code Release: Make the code publicly available to facilitate reproducibility and encourage further exploration of the proposed method.
Questions for the Authors
1. Why was a shallow CNN architecture chosen for the experiments? How do you expect the method to perform on deeper architectures?
2. Can you provide a detailed explanation for the failure of uncertainty sampling and curriculum learning on USPS?
3. What is the rationale behind sampling a larger subset before active learning selection, and how does it impact the results?
4. How does the time complexity of the proposed method compare to minibatch backpropagation in practice? 
In summary, while the paper tackles an interesting and relevant problem, significant improvements in clarity, experimental validation, and methodological justification are needed before it can be considered for acceptance.