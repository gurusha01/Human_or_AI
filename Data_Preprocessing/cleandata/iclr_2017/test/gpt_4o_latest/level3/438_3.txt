Review
Summary of Contributions  
This paper proposes augmenting reinforcement learning (RL) models with self-supervised auxiliary tasks, specifically depth prediction and loop closure detection, to improve internal representations for navigation tasks in complex 3D environments. The auxiliary tasks leverage 3D environment priors, enabling the agent to learn structured representations that enhance both data efficiency and task performance. The study demonstrates that these auxiliary tasks significantly accelerate learning and improve performance, approaching human-level navigation capabilities in certain scenarios. The authors also provide detailed analyses of the agent's behavior, localization abilities, and network dynamics, reinforcing the value of auxiliary tasks in RL. However, the paper's reliance on top-5 runs raises concerns about robustness, and its dismissal of navigation literature limits comparative insights.
Decision: Accept  
The paper makes a strong and well-supported contribution to the field of RL for navigation by demonstrating the utility of auxiliary tasks in improving representation learning and data efficiency. However, the authors should address concerns about robustness and engage more thoroughly with related navigation literature.
Supporting Arguments  
1. Well-Motivated Approach: The use of auxiliary tasks is well-justified, leveraging 3D priors to address the challenges of sparse rewards and memory requirements in navigation. The tasks are carefully chosen to align with navigation-relevant features like spatial geometry and trajectory history.
2. Rigorous Empirical Results: The experiments convincingly show that auxiliary tasks lead to faster convergence and better asymptotic performance across multiple 3D maze environments. The detailed analysis of position decoding and agent behavior strengthens the claims about structured internal representations.
3. Broader Implications: The study highlights the broader potential of using auxiliary tasks to enhance RL models, not just for navigation but also for other tasks requiring rich internal representations.
Additional Feedback  
1. Robustness Concerns: The reliance on top-5 runs for reporting results raises questions about the robustness of the approach. It would be helpful to report mean and variance across all runs or provide a more detailed analysis of hyperparameter sensitivity.
2. Engagement with Navigation Literature: The dismissal of classical navigation approaches like SLAM as "not RL" is overly reductive. A more nuanced comparison with SLAM-based methods or hybrid approaches could yield valuable insights and strengthen the paper's positioning.
3. Auxiliary Task Design: While the chosen auxiliary tasks are effective, the paper could explore whether other tasks (e.g., reward prediction) might yield similar or complementary benefits. This would provide a more comprehensive understanding of the role of auxiliary tasks in RL.
4. Scalability: The paper briefly mentions limitations in memory capacity for larger, procedurally generated mazes. Future work could explore architectures with external memory to address this limitation.
Questions for the Authors  
1. How robust is the proposed approach to variations in hyperparameters? Could you provide more details on the distribution of performance across all runs?  
2. How do the auxiliary tasks compare to other potential tasks, such as reward prediction or semantic segmentation, in terms of their impact on learning efficiency and performance?  
3. Could you clarify why SLAM-based approaches were not included in the experimental comparisons? How do you envision integrating or comparing your method with such approaches?  
4. Have you considered the computational overhead introduced by the auxiliary tasks, and how does it scale with larger environments or more complex tasks?
By addressing these points, the paper could further solidify its contributions and provide a more comprehensive perspective on the role of auxiliary tasks in RL for navigation.