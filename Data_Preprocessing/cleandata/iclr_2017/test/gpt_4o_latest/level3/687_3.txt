Review of the Paper
Summary of Contributions
This paper introduces a second-order Taylor expansion-based pruning method for neural networks, aiming to remove entire neurons rather than individual weights. The authors compare this method against a first-order Taylor approximation and a brute-force pruning approach. The paper also explores the implications of pruning on neural network learning representations, revisiting hypotheses by Mozer & Smolensky (1989a) about the dualistic roles of neurons. The experimental results demonstrate that while the second-order method performs better than the first-order method in some cases, it consistently underperforms compared to brute-force pruning. The authors highlight that brute-force pruning can remove up to 40-70% of neurons in shallow networks without significant performance degradation, though its effectiveness diminishes in deeper networks. The paper is well-written, with clear explanations and detailed responses to pre-review questions.
Decision: Reject
The primary reasons for rejection are the lack of focus and clarity regarding the paper's contributions to the literature, as well as the weak motivation and performance of the proposed second-order pruning method. While the paper provides interesting insights into neural representations and pruning, it fails to establish a concrete and novel contribution or actionable conclusions that advance the state of the art.
Supporting Arguments
1. Lack of Focus and Contribution Clarity: The paper attempts to address two orthogonal topics—benchmarking pruning methods and studying neural representations—but does not adequately focus on either. The title and abstract suggest an emphasis on neural representations, yet the experiments and results focus heavily on pruning algorithms. This disconnect dilutes the paper's impact and leaves its primary contribution unclear.
   
2. Weak Motivation for the Second-Order Method: The proposed second-order Taylor expansion method is not well-motivated. The authors acknowledge that similar insights could be derived from existing methods, and the second-order method consistently underperforms brute-force pruning. The computational expense of the brute-force method is noted, but no practical alternatives or improvements are proposed to make it tractable.
3. Failure to Address Neural Representations: Despite claims in the title and abstract, the paper does not adequately investigate or contribute to the understanding of neural representations. While the experiments provide anecdotal support for Mozer & Smolensky's hypothesis, they do not offer rigorous or novel insights into this topic.
4. Experimental Limitations: The experiments are primarily conducted on toy datasets (e.g., MNIST), and the results may not generalize to more complex, real-world datasets or deeper networks. The paper acknowledges this limitation but does not attempt to address it.
Suggestions for Improvement
1. Clarify the Paper's Focus: The authors should decide whether the paper aims to benchmark pruning methods or investigate neural representations. If the focus is on pruning, the paper should emphasize practical contributions, such as improving the computational efficiency of brute-force pruning. If the focus is on neural representations, the experiments should be designed to rigorously test hypotheses about learning representations.
2. Strengthen the Motivation for the Second-Order Method: The authors should provide a stronger theoretical or practical justification for the second-order method. Alternatively, they could explore hybrid approaches that approximate brute-force pruning more efficiently.
3. Expand Experimental Scope: The experiments should include deeper networks and more complex datasets to validate the generalizability of the findings. Additionally, the authors could explore the impact of re-training after pruning, as this is a common practice in real-world applications.
4. Streamline the Abstract and Conclusion: The abstract and conclusion should clearly articulate the paper's contributions and avoid mixing orthogonal aspects. For example, the abstract should not overemphasize neural representations if the primary focus is on pruning methods.
Questions for the Authors
1. What is the practical advantage of the second-order method over existing pruning techniques, given its underperformance compared to brute-force pruning?
2. How do the findings on neural representations extend or challenge existing theories beyond corroborating Mozer & Smolensky's hypothesis?
3. Have you considered hybrid approaches that approximate brute-force pruning while reducing computational expense? If so, what were the results?
4. How do you envision the second-order method scaling to deeper networks or more complex datasets? Would re-training improve its performance?
This review aims to provide constructive feedback to help the authors refine their work and better align their contributions with the expectations of the research community.