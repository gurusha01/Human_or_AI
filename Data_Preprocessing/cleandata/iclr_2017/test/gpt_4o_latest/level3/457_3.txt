Review of "Incremental Network Quantization (INQ)"
Summary of Contributions
This paper introduces Incremental Network Quantization (INQ), a novel method for converting pre-trained full-precision convolutional neural networks (CNNs) into low-precision models with weights constrained to powers of two or zero. The authors propose a three-step iterative process—weight partition, group-wise quantization, and re-training—to minimize accuracy loss during quantization. The method is tested on popular CNN architectures (e.g., AlexNet, VGG-16, ResNet-18) and achieves comparable or improved accuracy at reduced precision levels (e.g., 5-bit, 4-bit). The paper also explores the combination of INQ with pruning for model compression, claiming superior results compared to prior work, such as Han et al. (2016). The authors argue that INQ is particularly suited for deployment on resource-constrained hardware due to its ability to replace floating-point operations with binary bit-shifts.
Decision: Reject
Key reasons for rejection:
1. Incomplete Exploration of Model Compression: While the paper demonstrates promising results for quantization, it does not adequately explore pruning, a critical component of model compression. Han et al. (2016) achieved a 49× reduction in VGG-16 using pruning and quantization, whereas this paper does not demonstrate comparable reductions, making it difficult to assess its competitiveness.
2. Insufficient Comparison with Prior Work: The paper lacks direct experimental results comparing INQ with pruning-based methods. Without this, the claim of superior performance remains unsubstantiated.
Supporting Arguments
1. Problem Definition and Motivation: The paper addresses an important problem—reducing the computational and memory requirements of CNNs for deployment on edge devices. The iterative quantization strategy is well-motivated and builds on prior work in pruning and quantization. However, the novelty is incremental, as the method primarily combines existing techniques (e.g., pruning-inspired partitioning) in a structured manner.
   
2. Scientific Rigor and Results: While the results on quantization (e.g., 5-bit and 4-bit models) are promising, the lack of pruning results undermines the paper's claims of achieving state-of-the-art compression. The authors mention combining INQ with pruning but fail to provide detailed results or comparisons with Han et al. (2016), which limits the paper's impact.
3. Clarity and Completeness: The paper is well-written and provides detailed explanations of the INQ process. However, the absence of critical data (e.g., pruning results) and the lack of a comprehensive comparison with prior work make the contributions appear incomplete.
Suggestions for Improvement
1. Incorporate Pruning Results: To strengthen the claims, the authors should include experiments demonstrating the effectiveness of INQ when combined with pruning. This would allow for a direct comparison with Han et al. (2016) and other state-of-the-art methods.
2. Provide Compression Ratios: Report detailed compression ratios (e.g., model size reductions) for all tested architectures, especially in comparison to prior work.
3. Expand on Hardware Implications: While the paper briefly mentions the hardware benefits of binary bit-shift operations, a more thorough analysis of computational and power efficiency on real-world hardware would enhance the paper's practical relevance.
4. Clarify Missing Results: The authors should explain why pruning results were omitted and provide a roadmap for future work addressing this gap.
Questions for the Authors
1. How does INQ compare to Han et al. (2016) in terms of compression ratios for VGG-16 and other architectures when pruning is included?
2. Can the authors provide results for INQ's performance on hardware platforms (e.g., FPGA, mobile devices) to validate its practical benefits?
3. How does the iterative nature of INQ affect training time compared to global quantization methods?
In conclusion, while the paper introduces a promising quantization method, its incomplete exploration of pruning and lack of direct comparisons with prior work limit its contributions. Addressing these gaps would significantly enhance the paper's impact.