Review of the Paper
Summary of Contributions
The paper proposes a novel actor-critic reinforcement learning (RL) framework to automate the learning rate adjustment for stochastic gradient descent (SGD)-based machine learning algorithms. The authors aim to address the challenge of manually tuning learning rates, which is often tedious and problem-specific. By leveraging RL, the proposed method trains an actor network to predict the learning rate at each step and a critic network to evaluate the long-term impact of these decisions. The paper claims that this approach improves generalization, prevents overfitting, and achieves better performance compared to baseline optimizers such as Adam, Adagrad, and RMSprop. Experiments on MNIST and CIFAR-10 datasets are presented to validate the method, with additional comparisons to vSGD, an adaptive learning rate method.
Decision: Reject
Key reasons for rejection:
1. Outdated Experimental Setup: The network architectures used for CIFAR-10 experiments are outdated and fail to reflect the current state-of-the-art (SOTA) in deep learning. This undermines the validity of the comparisons and the claimed advantages of the method.
2. Limited Scope and Practical Value: The method focuses on tuning a single hyperparameter (learning rate), which is already well-addressed by modern optimizers. The proposed approach does not demonstrate sufficient improvement over SOTA methods to justify its additional computational cost.
Supporting Arguments
1. Outdated Architectures: The CNN architecture used for CIFAR-10 is overly simplistic and does not align with modern practices, such as ResNet or Vision Transformers. This limits the generalizability of the results and raises concerns about the relevance of the findings.
2. Lack of Robust Validation: The experiments are limited to two datasets (MNIST and CIFAR-10), both of which are relatively small-scale and well-studied. The absence of experiments on larger, more complex datasets (e.g., ImageNet) weakens the claim of broad applicability.
3. Computational Cost: The actor-critic framework introduces significant computational overhead compared to traditional optimizers. This cost is not justified, as the method does not outperform SOTA optimizers in a meaningful way.
4. Inconclusive Results: While the actor-critic idea is interesting, the reported improvements in test accuracy are marginal and do not convincingly demonstrate the practical value of the method.
Suggestions for Improvement
1. Use Modern Architectures: To validate the effectiveness of the proposed method, experiments should be conducted on SOTA architectures such as ResNet, EfficientNet, or Transformers. This would provide a more realistic assessment of the method's performance.
2. Expand Dataset Scope: Include experiments on larger and more diverse datasets, such as ImageNet or COCO, to demonstrate the scalability and generalizability of the approach.
3. Compare Against SOTA: The method should be benchmarked against modern adaptive optimizers like AdamW and learning rate schedulers such as cosine annealing or cyclical learning rates.
4. Analyze Computational Overhead: Provide a detailed analysis of the computational cost introduced by the actor-critic framework and justify its trade-offs in terms of performance gains.
5. Clarify Practical Use Cases: The paper should address scenarios where the proposed method offers clear advantages over existing approaches, particularly in real-world applications.
Questions for the Authors
1. How does the method perform on modern architectures like ResNet or Transformers? Are the claimed advantages still observed in these settings?
2. Can the computational overhead of the actor-critic framework be reduced, and how does it compare to lightweight optimizers like AdamW in terms of training time?
3. Why were MNIST and CIFAR-10 chosen as the primary datasets? Would the method generalize to larger and more complex datasets?
4. Have you considered extending the framework to tune multiple hyperparameters (e.g., momentum, weight decay) simultaneously? If not, why?
In conclusion, while the paper presents an interesting application of RL to learning rate optimization, it falls short of demonstrating its relevance and effectiveness in modern deep learning contexts. Addressing the outlined concerns could significantly strengthen the paper.