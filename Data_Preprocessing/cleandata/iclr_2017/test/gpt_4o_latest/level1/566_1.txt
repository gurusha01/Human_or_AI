Review of the Paper
Summary of Contributions
This paper introduces a novel batch active learning framework for deep neural networks, particularly Convolutional Neural Networks (CNNs), leveraging a variational inference-based approach. The authors propose a scalable and computationally efficient active learning criterion that combines Maximum Likelihood Estimation (MLE) and Bayesian inference principles. By approximating the posterior and prior distributions of network weights using Fisher information and employing Kronecker-factored approximations, the method avoids the computational burden of backpropagation during the active learning process. The paper demonstrates the effectiveness of the proposed method on MNIST and USPS datasets, achieving near-optimal performance with only 30% of the annotated training data. The work is positioned as the first of its kind to scale batch active learning to deep networks using variational free energy, and it shows promise for both active and curriculum learning.
Decision: Accept
The paper is well-motivated, scientifically rigorous, and makes a significant contribution to the intersection of active learning and deep learning. The key reasons for acceptance are:
1. Novelty and Impact: The proposed method is the first to scale batch active learning to deep networks using variational inference, addressing a critical gap in the literature.
2. Empirical Validation: The experiments on MNIST and USPS datasets convincingly demonstrate the method's effectiveness, scalability, and robustness compared to baseline approaches.
Supporting Arguments
1. Problem Motivation and Placement in Literature: The paper clearly identifies the challenges of combining active learning with deep learning, particularly the computational inefficiencies of traditional methods. The proposed approach is well-grounded in prior work on variational inference and Fisher information, and the authors effectively position their contribution within the existing literature.
2. Scientific Rigor: The derivation of the active learning criterion is thorough and grounded in statistical theory, with appropriate approximations to ensure computational feasibility. The use of Kronecker-factored approximations for Fisher matrices is a clever adaptation to deep architectures.
3. Experimental Results: The results demonstrate that the method achieves competitive or superior performance compared to random sampling, uncertainty sampling, and curriculum learning. The scalability of the approach is validated through time complexity experiments, and the method performs well on both small and large datasets.
Suggestions for Improvement
1. Clarity of Presentation: While the theoretical derivations are comprehensive, they are dense and may be difficult for readers unfamiliar with variational inference or Fisher information. Including a more intuitive explanation or visual representation of the key ideas would improve accessibility.
2. Hyperparameter Sensitivity: The paper mentions that hyperparameters were not optimized for different training set sizes. A discussion or analysis of the sensitivity of the method to hyperparameter choices would strengthen the empirical evaluation.
3. Comparison with More Baselines: While the paper compares the proposed method to random sampling, uncertainty sampling, and curriculum learning, additional comparisons with recent submodular or Bayesian active learning methods would provide a more comprehensive evaluation.
4. Generalization to Other Architectures: The focus is on CNNs, but it would be valuable to discuss how the method might generalize to other architectures, such as transformers or recurrent networks.
Questions for the Authors
1. How sensitive is the method to the choice of the hyperparameter Î³ in the variational free energy formulation? Could this parameter be learned adaptively during training?
2. The experiments focus on image classification tasks. Have you considered applying the method to other domains, such as natural language processing or time-series data?
3. How does the method perform when the dataset contains a higher proportion of noisy or mislabeled samples, as seen in real-world scenarios?
In conclusion, this paper makes a strong contribution to the field of active learning for deep networks. While there are areas for improvement, the novelty, rigor, and empirical results justify its acceptance.