The paper introduces Doc2VecC, a novel and efficient framework for document representation learning. The key contribution of Doc2VecC is its simplicity: it represents documents as an average of word embeddings, augmented by a corruption model that introduces data-dependent regularization. This regularization penalizes common or non-informative words while emphasizing rare or informative ones, ensuring that the learned document representations capture semantic meaning. The model is computationally efficient, capable of training on billions of words per hour on a single machine and generating representations for unseen documents quickly. Empirical results demonstrate that Doc2VecC outperforms state-of-the-art methods in tasks such as sentiment analysis, document classification, and semantic relatedness, while being significantly faster to train and test.
Decision: Accept
The paper should be accepted due to its strong empirical performance, computational efficiency, and well-motivated approach. Two key reasons for this decision are:
1. Novelty and Simplicity: Doc2VecC introduces a simple yet effective corruption mechanism that improves document representations without adding significant computational overhead.
2. Empirical Rigor: The paper provides extensive experimental results across diverse tasks, demonstrating that Doc2VecC consistently outperforms or matches state-of-the-art methods while being faster and more scalable.
Supporting Arguments
1. Well-Motivated Approach: The authors build on the success of Word2Vec and Paragraph Vectors, addressing their limitations (e.g., scalability and inefficiency for unseen documents). The corruption model is a novel addition that is theoretically justified as a form of data-dependent regularization.
2. Empirical Validation: The experiments are thorough, covering multiple datasets and tasks. Doc2VecC achieves state-of-the-art performance in sentiment analysis and document classification, while also producing superior word embeddings for word analogy tasks. The results are consistent and reproducible, with code provided.
3. Efficiency: The simplicity of the model architecture allows for significant speedups during training and testing, making it practical for large-scale applications.
Suggestions for Improvement
1. Clarity of Theoretical Justification: While the corruption mechanism is well-motivated, the derivation of its regularization effect could be simplified or clarified for broader accessibility.
2. Comparison with More Recent Methods: The paper does not compare Doc2VecC with some newer transformer-based models (e.g., BERT). While these models may not focus on efficiency, their inclusion would provide a more comprehensive evaluation.
3. Ablation Studies: An ablation study isolating the impact of the corruption mechanism versus simple averaging of word embeddings would strengthen the claims about its importance.
Questions for the Authors
1. How does Doc2VecC perform on tasks requiring fine-grained semantic understanding, such as question answering or summarization, compared to transformer-based models?
2. Can the corruption probability \(q\) be dynamically adjusted during training to further optimize performance?
3. Is the model robust to variations in vocabulary size or domain-specific corpora? For example, how does it handle highly specialized or low-resource datasets?
Overall, the paper makes a significant contribution to the field of document representation learning, balancing simplicity, efficiency, and performance. With minor clarifications and additional comparisons, it would be even stronger.