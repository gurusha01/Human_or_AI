Review
Summary of Contributions
This paper investigates the geometry of loss functions in deep neural networks and examines how various stochastic optimization methods interact with these loss surfaces. The authors conduct a series of empirical experiments using state-of-the-art network architectures (e.g., VGG, NIN, and LSTM) and optimization methods (e.g., SGD, ADAM, RMSprop, and others) to explore the properties of local minima. The paper introduces novel visualization techniques, such as barycentric and bilinear interpolation, to project the high-dimensional loss surfaces into lower dimensions for better interpretability. A key contribution is the empirical demonstration that different optimization algorithms converge to distinct local minima, even when starting from the same initialization, and that these minima exhibit qualitatively different properties. Additionally, the authors propose an augmentation of optimization methods using second-order Runge-Kutta integrators and analyze their performance. The paper also examines the effects of batch normalization and extreme initializations on the loss surface and training outcomes.
Decision: Reject
While the paper addresses an important problem and provides interesting empirical insights, it falls short in several areas. The primary reasons for rejection are: (1) insufficient theoretical grounding or explanation for the observed phenomena, and (2) a lack of clarity and rigor in connecting the empirical results to the broader implications for optimization and generalization in deep learning.
Supporting Arguments for Decision
1. Insufficient Theoretical Motivation: The paper provides extensive empirical results but does not adequately explain why different optimization algorithms find distinct local minima or why the shapes of loss surfaces around these minima differ. The lack of theoretical grounding limits the generalizability of the findings.
   
2. Limited Practical Implications: While the paper demonstrates that different optimization methods lead to different local minima, it concludes that these minima have similar generalization performance. This raises questions about the practical significance of the observed differences in loss surface geometry.
3. Runge-Kutta Augmentation: The proposed use of Runge-Kutta integrators is intriguing, but the results show mixed performance. For example, ADAM outperforms its RK2-augmented counterpart, which undermines the claim that this method provides a meaningful improvement.
4. Clarity and Focus: The paper is dense and difficult to follow in places, particularly in the experimental setup and results sections. The connection between the empirical findings and the broader research questions is not always clear.
Suggestions for Improvement
1. Theoretical Context: Provide a stronger theoretical foundation for the observed phenomena, such as why different optimization methods lead to distinct local minima and how this relates to the geometry of the loss surface.
2. Practical Implications: Clarify the practical significance of the findings. For example, explore whether the differences in loss surface geometry have implications for robustness, transfer learning, or other real-world applications.
3. Runge-Kutta Methods: Investigate why RK2 augmentation underperforms in some cases (e.g., with ADAM). Consider whether alternative formulations or parameter tuning could improve its effectiveness.
4. Clarity in Presentation: Simplify the presentation of the experimental setup and results. Use concise summaries and highlight key findings more clearly.
5. Batch Normalization Analysis: The discussion on batch normalization is interesting but underdeveloped. Consider expanding this analysis to provide deeper insights into its role in shaping the loss surface.
Questions for the Authors
1. Why do the local minima found by different optimization algorithms have similar generalization performance despite being qualitatively different in terms of loss surface geometry?
2. Can the differences in loss surface geometry (e.g., basin size) be linked to practical benefits, such as robustness to adversarial attacks or better performance on unseen data?
3. What is the theoretical basis for the observed differences in loss surface geometry between optimization methods? Could these differences be predicted or quantified in advance?
4. Why does the RK2 augmentation fail to improve performance with ADAM? Could this be due to an incompatibility between the RK2 method and ADAM's adaptive learning rate?
In summary, while the paper makes some valuable contributions, it requires significant improvements in theoretical grounding, clarity, and practical relevance to meet the standards of the conference.