The paper addresses the critical problem of inaccuracies in electronic medical records (EMRs), particularly the omission of active medications from patient records. The authors propose using recurrent neural networks (RNNs), specifically Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTM) networks, to predict the therapeutic classes of medications a patient is likely taking based on the sequence of their most recent 100 billing codes. The study demonstrates that GRUs outperform other models, achieving a micro-averaged AUC of 0.93 and a Label Ranking Loss of 0.076, suggesting the potential for these models to assist in medication reconciliation and improve EMR accuracy.
Decision: Accept.  
The paper is well-motivated, addressing a significant and underexplored issue in computational medicine. The proposed approach is scientifically rigorous, leveraging large-scale data (3.3 million instances) and demonstrating strong empirical results. The use of RNNs to capture temporal dependencies in billing codes is innovative and well-placed in the literature, building on prior work while addressing a unique prediction problem. The results are compelling, with clear evidence that the models can identify missing medications and potentially improve clinical workflows.
Supporting Arguments:  
1. Novelty and Impact: The paper tackles a critical healthcare challenge with a novel application of RNNs, demonstrating their utility in a domain where temporal data is key. The focus on predicting therapeutic classes rather than individual medications is a practical and scalable approach.  
2. Scientific Rigor: The experiments are well-designed, comparing RNNs to feed-forward networks and random forests, and the evaluation metrics (e.g., AUC, Label Ranking Loss) are appropriate for the multi-label prediction task. The authors also acknowledge label noise in the data and interpret their results conservatively.  
3. Empirical Results: The GRU model's strong performance, coupled with qualitative examples of its predictions, supports the paper's claims. The embedding analysis further highlights the potential for broader applications of the learned representations.
Suggestions for Improvement:  
1. Label Noise Analysis: While the authors discuss label noise qualitatively, a more systematic analysis (e.g., quantifying its impact on model performance) would strengthen the paper.  
2. Clinical Validation: The paper would benefit from a discussion of how the model could be integrated into clinical workflows and validated in real-world settings.  
3. Model Scalability: The authors note hardware constraints on model size. Including a discussion of how the approach could scale with improved computational resources would enhance the paper's practical relevance.  
4. Additional Features: Exploring the inclusion of other patient data (e.g., demographics, lab results) could improve predictions and should be mentioned as future work.
Questions for the Authors:  
1. How does the model handle rare billing codes or therapeutic classes with low prevalence in the dataset?  
2. Can the authors provide more details on the random search for hyperparameter optimization, including the range of values tested?  
3. How might the model's predictions be validated against a gold standard in a clinical setting, given the acknowledged label noise?  
4. Could the embedding learned in this study be applied to other tasks, such as disease progression modeling or patient similarity analysis?  
Overall, this paper makes a meaningful contribution to the field, and the proposed approach has the potential to significantly improve EMR accuracy and patient care.