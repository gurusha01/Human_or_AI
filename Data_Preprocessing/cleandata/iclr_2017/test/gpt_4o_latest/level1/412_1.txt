Review of the Paper
Summary of Contributions:  
This paper introduces a novel layerwise optimization algorithm for Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a class of CNNs employing piecewise linear non-linearities (e.g., ReLU, max-pool) and an SVM classifier as the final layer. The authors reformulate the parameter estimation problem for a layer as a difference-of-convex (DC) program, solved using the concave-convex procedure (CCCP). This approach eliminates the need for learning rate tuning and guarantees a monotonic decrease in the learning objective. The algorithm is extended to improve memory efficiency, initialization, and time complexity, making it scalable to large datasets and networks. Empirical results on MNIST, CIFAR-10/100, and ImageNet demonstrate improvements over state-of-the-art backpropagation variants, achieving better training objectives and test accuracies.
Decision: Accept  
The paper makes a significant contribution by establishing a novel connection between CNN optimization and latent structured SVMs, offering a principled alternative to backpropagation. The proposed method is well-motivated, theoretically grounded, and empirically validated on diverse datasets, demonstrating scalability and performance gains. The elimination of learning rate tuning is a practical advantage, addressing a long-standing challenge in deep learning optimization.
Supporting Arguments:  
1. Problem Significance and Novelty: The paper addresses the critical issue of learning rate sensitivity in backpropagation, proposing a robust alternative for PL-CNNs. The connection to latent structured SVMs is novel and opens avenues for leveraging established optimization techniques in deep learning.  
2. Theoretical Rigor: The reformulation of layerwise optimization as a DC program is well-justified, and the use of CCCP ensures a monotonic decrease in the objective function. The extensions to the BCFW algorithm for memory and computational efficiency are thoughtfully designed.  
3. Empirical Validation: The experiments convincingly demonstrate the method's superiority over backpropagation variants across datasets and architectures. The scalability to large networks like VGG-16 on ImageNet further strengthens the contribution.  
Suggestions for Improvement:  
1. Clarity of Presentation: While the theoretical contributions are substantial, the paper's exposition is dense and could benefit from clearer explanations. For instance, the derivation of the DC program and the role of latent variables could be simplified for accessibility.  
2. Comparison with Other Optimization Methods: The paper briefly mentions alternative structured SVM solvers (e.g., cutting-plane methods) but does not empirically compare them with the proposed approach. Including such comparisons would strengthen the claim of superiority.  
3. Practical Implications: While the paper highlights the elimination of learning rate tuning, it would be helpful to discuss the computational trade-offs of the proposed method compared to backpropagation in more detail.  
Questions for the Authors:  
1. How does the proposed method handle overfitting, especially in the absence of techniques like dropout or batch normalization during optimization?  
2. Can the algorithm be extended to handle non-piecewise-linear activations (e.g., sigmoid, tanh)? If not, what are the limitations?  
3. How does the computational cost of CCCP compare to backpropagation for very deep networks, especially in terms of convergence time?  
Overall, the paper presents a well-motivated and impactful contribution to deep learning optimization, with both theoretical and practical significance. Addressing the suggestions above would further enhance its clarity and impact.