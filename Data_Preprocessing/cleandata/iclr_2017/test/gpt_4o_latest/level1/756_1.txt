The paper introduces two novel neural network architectures: Similarity Encoders (SimEc) and Context Encoders (ConEc). SimEc addresses the problem of creating similarity-preserving embeddings that can handle large datasets, unknown similarity functions, and out-of-sample data points. It extends the capabilities of spectral methods like kernel PCA by providing a scalable, flexible, and computationally efficient solution. ConEc builds upon SimEc to enhance word2vec embeddings by incorporating local context, enabling the creation of embeddings for out-of-vocabulary words and distinguishing between multiple meanings of words. The paper demonstrates the utility of these methods through experiments on image and text datasets, as well as a named entity recognition (NER) task.
Decision: Accept.  
The key reasons for this decision are the novelty and practical significance of the proposed methods. SimEc offers a compelling alternative to traditional dimensionality reduction techniques, addressing scalability and generalization challenges. ConEc provides a meaningful extension to word2vec, solving practical issues like handling out-of-vocabulary words and polysemy. The experimental results convincingly support the claims, showing improvements in embedding quality and task performance.
Supporting Arguments:  
1. Well-Motivated Approach: The paper is well-placed in the literature, clearly identifying limitations in existing methods (e.g., kernel PCA, word2vec) and presenting SimEc and ConEc as innovative solutions. The connections drawn between SimEc and spectral methods provide a strong theoretical foundation.  
2. Scientific Rigor: The experiments are thorough, spanning multiple datasets and tasks. The results demonstrate that SimEc can replicate and extend the capabilities of kernel PCA and isomap, while ConEc improves word embeddings in practical NLP tasks like NER. The use of both quantitative metrics and qualitative visualizations strengthens the evidence.  
3. Practical Relevance: The ability to handle large datasets, unknown similarity functions, and out-of-vocabulary words makes the proposed methods highly applicable in real-world scenarios.
Suggestions for Improvement:  
1. Clarity on Hyperparameters: While the paper provides general recommendations, more details on hyperparameter tuning (e.g., number of hidden layers, regularization strength) would help practitioners replicate the results.  
2. Scalability Analysis: A more detailed discussion of computational efficiency, particularly for very large datasets, would be valuable. For example, how does SimEc compare to kernel PCA in terms of runtime and memory usage?  
3. Broader Evaluation: While the experiments are compelling, additional benchmarks (e.g., other NLP tasks or datasets) could further validate the generalizability of ConEc.  
4. Error Analysis: Including an analysis of failure cases (e.g., when embeddings fail to preserve similarities or distinguish word senses) would provide insights into the limitations of the methods.
Questions for the Authors:  
1. How sensitive are SimEc and ConEc to the choice of neural network architecture (e.g., depth, activation functions)?  
2. Could SimEc be extended to other types of similarity measures (e.g., cosine similarity) beyond those explored in the paper?  
3. For ConEc, how does the performance scale with the size of the training corpus? Are there diminishing returns with larger datasets?  
4. Could the authors elaborate on the potential for using convolutional or hierarchical architectures in SimEc, as mentioned in the conclusion?  
Overall, the paper makes a significant contribution to the fields of dimensionality reduction and representation learning, and I recommend its acceptance.