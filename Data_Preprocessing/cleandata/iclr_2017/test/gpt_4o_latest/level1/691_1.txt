The paper introduces the Retro Learning Environment (RLE), a novel reinforcement learning (RL) platform that extends the capabilities of the widely used Arcade Learning Environment (ALE). RLE supports games from more advanced gaming consoles such as the Super Nintendo Entertainment System (SNES) and Sega Genesis, offering new challenges due to the increased complexity and diversity of these games. The authors highlight key contributions, including the modular design of RLE, which allows for easy integration of new games and consoles, and the introduction of multi-agent reinforcement learning (MARL) capabilities. The paper also presents experimental results demonstrating the limitations of state-of-the-art RL algorithms on SNES games, emphasizing the need for more robust and generalizable approaches.
Decision: Accept
Key reasons for acceptance:
1. Novel Contribution: The introduction of RLE as an RL environment with significantly more complex challenges than ALE is a meaningful contribution to the field. The modularity and extensibility of RLE ensure its long-term relevance for RL research.
2. Scientific Rigor: The experiments are well-designed and provide valuable insights into the limitations of current RL algorithms. The inclusion of multi-agent scenarios and reward shaping experiments adds depth to the evaluation.
Supporting Arguments:
1. Well-Motivated Approach: The paper is well-situated in the literature, building on the success of ALE while addressing its limitations. The authors provide a thorough comparison with existing environments such as OpenAI Gym, Universe, and DeepMind Lab, clearly establishing the need for RLE.
2. Experimental Results: The results are robust and highlight the challenges posed by SNES games, such as delayed rewards, large action spaces, and noisy backgrounds. The experiments on multi-agent learning and reward shaping are particularly insightful and demonstrate the potential of RLE for advancing RL research.
3. Broader Impact: By enabling research on more complex games, RLE has the potential to drive the development of RL algorithms that generalize better to real-world tasks, such as autonomous driving and robotics.
Suggestions for Improvement:
1. Clarity on Generalization: While the paper emphasizes the challenges of generalization, it would benefit from a more detailed discussion on how RLE could be used to benchmark generalization across different games or levels within a game.
2. Expanded Evaluation: The experiments focus on a limited set of SNES games. Including results from other supported consoles (e.g., Sega Genesis) would strengthen the case for RLE's versatility.
3. Algorithmic Insights: The paper could explore potential algorithmic modifications or enhancements (e.g., exploration strategies, hierarchical RL) that might address the challenges identified in RLE.
4. Multi-Agent Learning: The multi-agent experiments are promising but underexplored. Future work could investigate more sophisticated MARL algorithms, such as those that explicitly model opponent behavior.
Questions for the Authors:
1. How does RLE handle the computational overhead of emulating more complex consoles compared to ALE? Are there any performance benchmarks for training RL agents in RLE?
2. Could you elaborate on the process of adding new games or consoles to RLE? How much effort is required, and are there any limitations?
3. Have you considered integrating RLE with other popular RL frameworks, such as TensorFlow or JAX, to broaden its accessibility?
In conclusion, the paper makes a significant contribution to the RL community by introducing a challenging and extensible environment. While there are areas for improvement and further exploration, the novelty and potential impact of RLE justify its acceptance.