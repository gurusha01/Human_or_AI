Review of the Paper
Summary of Contributions
This paper introduces a novel algorithm for training stochastic neural networks to sample from target distributions, addressing a key challenge in probabilistic inference. The proposed method leverages Stein Variational Gradient Descent (SVGD) to iteratively adjust neural network parameters, enabling the generation of samples that minimize the KL divergence with the target distribution. The authors extend this approach to propose SteinGAN, a framework for amortized maximum likelihood estimation (MLE) of deep energy models. SteinGAN employs an adversarial setup where the neural sampler and energy model iteratively refine each other, resulting in realistic image generation competitive with state-of-the-art GANs. The paper makes significant contributions by addressing the limitations of traditional variational inference methods, such as the need for explicit proposal densities, and by demonstrating the practical utility of the approach on benchmark datasets like MNIST, CIFAR-10, CelebA, and LSUN.
Decision: Accept
The paper should be accepted due to its innovative approach to probabilistic inference, which combines the theoretical rigor of SVGD with practical applications in generative modeling. The method is well-motivated, addresses a critical gap in the literature, and is supported by empirical results that demonstrate its effectiveness in generating high-quality images.
Supporting Arguments
1. Novelty and Significance: The paper tackles a fundamental problem in probabilistic inference—efficiently sampling from complex distributions—by introducing a method that avoids the computational bottlenecks of traditional variational inference. The concept of "amortized SVGD" is both novel and impactful, with potential applications beyond generative modeling.
2. Empirical Validation: The authors provide extensive experimental results, showing that SteinGAN generates realistic images competitive with GANs while capturing more information from training datasets, as evidenced by higher classification accuracy. This dual evaluation (visual and quantitative) strengthens the claims.
3. Theoretical Rigor: The paper is grounded in a solid theoretical framework, with clear derivations of the proposed updates and connections to existing methods like black-box variational inference and the reparameterization trick.
Suggestions for Improvement
1. Clarity in Presentation: While the theoretical sections are thorough, they can be dense for readers unfamiliar with SVGD or variational inference. Including a high-level diagram or flowchart summarizing the algorithmic steps would improve accessibility.
2. Comparative Baselines: The paper compares SteinGAN primarily to DCGAN. Including additional baselines, such as other energy-based models or recent advances in GANs, would provide a more comprehensive evaluation.
3. Ablation Studies: The paper would benefit from ablation studies to isolate the contributions of key components, such as the kernel choice in SVGD or the stabilization techniques used during training.
4. Scalability Discussion: While the method is promising, its scalability to higher-dimensional distributions or larger datasets is not explicitly discussed. Addressing this would strengthen the practical relevance of the approach.
Questions for the Authors
1. How sensitive is the performance of SteinGAN to the choice of kernel in SVGD? Have alternative kernels been tested?
2. The paper mentions that SteinGAN achieves higher classification accuracy than DCGAN. Could you elaborate on why this might be the case? Does SteinGAN capture more diverse or structured features?
3. How does the computational cost of SteinGAN compare to DCGAN or other generative models? Are there trade-offs between quality and efficiency?
4. Could the proposed method be extended to other probabilistic inference tasks, such as Bayesian optimization or reinforcement learning? If so, what modifications would be required?
Overall, this paper makes a strong contribution to the field of probabilistic inference and generative modeling, and I recommend its acceptance. The suggestions provided aim to further enhance the clarity and impact of the work.