Review of the Paper: "SYMSGD: A Parallel SGD Algorithm Retaining Sequential Semantics in Expectation"
Summary of Contributions
This paper introduces SYMSGD, a novel parallel stochastic gradient descent (SGD) algorithm that retains the sequential semantics of SGD in expectation. The authors address a fundamental limitation of existing parallel SGD methods, such as HOGWILD! and ALLREDUCE, which fail to honor inter-step dependencies, often resulting in suboptimal convergence rates or scalability issues. SYMSGD achieves its goal by employing probabilistic model combiners that allow local models from parallel threads to be combined into a global model equivalent to sequential SGD in expectation. The approach leverages dimensionality reduction via the Johnson-Lindenstrauss lemma to make the computation of combiner matrices feasible for high-dimensional datasets. Experimental results on nine datasets demonstrate up to 13× speedup on 16 cores compared to a heavily optimized sequential baseline, with no loss in accuracy.
Decision: Accept
The paper presents a significant and well-motivated contribution to the field of parallel machine learning, addressing a critical bottleneck in parallel SGD methods. The proposed SYMSGD algorithm is novel, theoretically sound, and empirically validated, making it a valuable addition to the literature.
Supporting Arguments for the Decision
1. Well-Motivated Problem: The paper identifies a clear limitation in existing parallel SGD methods—namely, their inability to preserve the sequential semantics of SGD. This is a critical issue for achieving both scalability and accuracy in parallel machine learning.
   
2. Novelty and Innovation: The introduction of probabilistically sound combiners and the use of dimensionality reduction for scalability are innovative. The approach is theoretically grounded, with detailed derivations provided for the combiner matrices and their probabilistic approximations.
3. Empirical Validation: The experimental results are robust, covering nine datasets with varying sparsity and dimensionality. SYMSGD consistently achieves significant speedups without sacrificing accuracy, outperforming state-of-the-art methods like HOGWILD! and ALLREDUCE.
4. Practical Usefulness: The algorithm is applicable to a wide range of linear learners and demonstrates scalability on both sparse and dense datasets, making it highly relevant for real-world applications.
5. Thoroughness: The paper provides detailed theoretical analysis, implementation optimizations, and a comprehensive evaluation. The discussion of trade-offs, such as the impact of variance on accuracy, adds depth to the work.
Suggestions for Improvement
1. Clarity on Initial Stuttering: The paper mentions that SYMSGD experiences "stuttering" in accuracy during the initial iterations. While the authors suggest combining models more frequently as a potential solution, additional experiments or analysis on this trade-off would strengthen the paper.
2. Broader Applicability: The current approach is limited to SGD algorithms with linear inter-step dependencies. Exploring extensions to non-linear dependencies (e.g., logistic regression) or providing a roadmap for future work in this direction would enhance the paper's impact.
3. Comparison with Mini-Batch SGD: While the paper focuses on single-example SGD, a comparison with mini-batch SGD (a common parallelization strategy) would provide additional context for the performance of SYMSGD.
4. Distributed Systems: The authors briefly mention the potential for extending SYMSGD to distributed systems. Including preliminary results or a discussion of the challenges in this setting would be valuable.
Questions for the Authors
1. How does the choice of the dimensionality reduction parameter \( k \) affect the trade-off between computational overhead and accuracy? Are there guidelines for selecting \( k \) based on dataset characteristics?
2. Can SYMSGD be extended to handle non-linear update rules (e.g., logistic regression) without significant loss in accuracy or scalability?
3. How does SYMSGD perform on datasets with extreme sparsity or very high feature dimensionality (e.g., >1 billion features)?
4. Have you explored hybrid approaches that combine SYMSGD with mini-batch SGD for further scalability?
Conclusion
SYMSGD represents a significant step forward in parallelizing SGD while retaining its sequential semantics. The paper is well-written, theoretically rigorous, and empirically validated, making it a strong candidate for acceptance. Addressing the suggested improvements and questions could further enhance the paper's impact and applicability.