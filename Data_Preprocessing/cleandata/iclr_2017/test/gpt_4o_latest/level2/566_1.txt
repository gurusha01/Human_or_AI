Review of the Paper
Summary of Contributions
This paper introduces a novel batch active learning framework for deep neural networks, particularly Convolutional Neural Networks (CNNs), leveraging a variational inference approach. The authors propose a scalable active learning criterion based on Maximum Likelihood Estimation (MLE) and Bayesian inference, utilizing Fisher information matrices and their Kronecker-factored approximations (KFAC). The paper claims to achieve computational efficiency by avoiding backpropagation during active selection and demonstrates the framework's efficacy on MNIST and USPS datasets. The results show that the proposed method outperforms baseline approaches, such as random sampling and uncertainty sampling, achieving near-optimal test accuracy using only 30% of the annotated training data.
Decision: Accept
The paper is recommended for acceptance due to its novel contribution to the intersection of active learning and deep learning, its rigorous theoretical grounding, and its demonstrated practical utility. The proposed method addresses a significant challenge—scaling active learning to deep networks—while offering promising results on benchmark datasets.
Supporting Arguments
1. Novelty and Innovation: The paper presents a unique combination of variational inference and active learning for deep networks, which has not been explored before. The use of KFAC approximations for Fisher matrices in this context is innovative and addresses the computational challenges associated with deep architectures.
   
2. Theoretical Rigor: The authors provide a detailed derivation of their active learning criterion, grounded in statistical principles and variational free energy. The approximations and assumptions are well-explained, and the methodology is supported by prior literature.
3. Empirical Validation: The experimental results on MNIST and USPS datasets demonstrate the method's effectiveness in reducing the required labeled data while maintaining high accuracy. The scalability of the approach is also validated through time complexity analysis.
4. Practical Relevance: The framework has clear practical implications for scenarios where labeled data is scarce or expensive to obtain, such as medical imaging or autonomous driving.
Additional Feedback for Improvement
1. Clarity of Presentation: While the theoretical sections are thorough, they can be overwhelming for readers unfamiliar with variational inference or Fisher information. Simplifying some explanations or including a high-level overview of the derivations would improve accessibility.
2. Comparison with More Baselines: The paper primarily compares its method to random sampling, uncertainty sampling, and curriculum learning. Including comparisons with more recent active learning methods, particularly those designed for deep networks, would strengthen the empirical evaluation.
3. Discussion of Limitations: The authors briefly mention limitations related to the asymptotic nature of their approximations and potential instability on small datasets. A more detailed discussion of these limitations, along with suggestions for future work, would enhance the paper's impact.
4. Hyperparameter Sensitivity: The paper does not discuss the sensitivity of the proposed method to hyperparameters, such as the size of the query batch or the scaling factor γ in the variational free energy. Including such an analysis would provide deeper insights into the robustness of the approach.
Questions for the Authors
1. How does the method perform on datasets with higher dimensionality or more complex distributions, such as CIFAR-10 or ImageNet? 
2. Can the proposed framework be extended to other types of neural networks, such as transformers or recurrent networks? 
3. How sensitive is the method to the choice of the initial labeled dataset? Does the performance vary significantly with different initializations?
Conclusion
This paper makes a significant contribution to the field of active learning for deep networks by proposing a scalable and theoretically grounded framework. While there is room for improvement in presentation and empirical comparisons, the novelty and practical relevance of the work justify its acceptance.