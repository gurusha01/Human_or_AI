This paper presents a novel framework, Neural Combinatorial Optimization (NCO), which leverages neural networks and reinforcement learning (RL) to address combinatorial optimization problems, with a focus on the Traveling Salesman Problem (TSP) and the KnapSack problem. The authors propose a pointer network architecture trained using policy gradient methods, achieving near-optimal solutions for 2D Euclidean TSP instances with up to 100 nodes and optimal solutions for KnapSack instances with up to 200 items. The framework is positioned as a generalizable alternative to traditional handcrafted heuristics, offering flexibility across problem domains.
Decision: Accept
Key reasons: (1) The paper introduces a novel and well-motivated approach to a challenging problem, demonstrating competitive results. (2) The methodology is rigorously evaluated with clear comparisons to baselines, showing promise for generalization to other combinatorial problems.
Supporting Arguments:
1. Claims and Support: The paper claims that NCO can achieve near-optimal solutions for TSP and optimal solutions for KnapSack problems using RL-trained neural networks. These claims are supported by extensive experiments, including comparisons to supervised learning baselines, heuristic algorithms (e.g., Christofides), and state-of-the-art solvers like Concorde. The results are statistically significant and demonstrate the scalability of the approach.
   
2. Novelty and Placement in Literature: The work builds on prior research in sequence-to-sequence learning and pointer networks, extending these ideas to combinatorial optimization. The use of RL for training, as opposed to supervised learning, is a significant innovation, addressing challenges such as the lack of labeled data for NP-hard problems. The paper also provides a thorough review of related work, situating its contributions within the broader context of optimization and machine learning.
3. Usefulness and Generalization: The proposed framework is practically useful, as it reduces reliance on problem-specific heuristics and shows potential for application to a wide range of optimization problems. The inclusion of experiments on the KnapSack problem further underscores its flexibility.
4. Limitations and Acknowledgment: The authors acknowledge that their method is not yet competitive with state-of-the-art solvers in terms of speed and performance for large-scale problems. They also discuss the challenges of ensuring solution feasibility for more complex constraints, providing a balanced view of the framework's current capabilities.
Additional Feedback:
1. Reproducibility: While the paper provides sufficient architectural and algorithmic details, including pseudocode, the authors should ensure that their code is publicly available as promised. This would enhance reproducibility and encourage further research.
2. Scalability: The paper could benefit from a more detailed discussion on the computational trade-offs of the proposed approach, particularly for larger problem instances. For example, how does the runtime scale with the number of nodes/items?
3. Comparison to Metaheuristics: While the paper compares NCO to traditional heuristics and solvers, a deeper analysis of why it underperforms against advanced metaheuristics (e.g., Guided Local Search) would be valuable. This could help identify avenues for improvement.
4. Exploration of Constraints: The discussion on handling constraints (e.g., TSP with time windows) is intriguing but underexplored. Future work could focus on integrating constraint satisfaction mechanisms into the framework.
Questions for Authors:
1. How does the performance of NCO vary with different RL training configurations, such as alternative reward structures or baseline estimators?
2. Could the framework be extended to handle dynamic or stochastic optimization problems, where the problem instance evolves over time?
3. How does the choice of hyperparameters (e.g., softmax temperature, logit clipping) impact the quality of solutions and convergence speed?
In conclusion, this paper makes a significant contribution to the field of combinatorial optimization by introducing a flexible and generalizable framework. While there are areas for improvement, the work is well-executed, and its potential impact justifies acceptance.