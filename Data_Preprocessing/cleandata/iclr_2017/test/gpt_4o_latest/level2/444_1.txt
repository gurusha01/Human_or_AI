The paper presents a novel method for visualizing the importance of specific inputs in determining the output of Long Short-Term Memory (LSTM) networks, with applications in sentiment analysis and question answering tasks. The authors propose a decomposition technique to assign importance scores to words, enabling the extraction of representative phrases that summarize the behavior of trained LSTMs. These extracted phrases are validated by constructing a simple, rule-based classifier that approximates the LSTM's performance. The paper demonstrates the utility of this approach through experiments on sentiment analysis datasets (Yelp and Stanford Sentiment Treebank) and the WikiMovies question-answering dataset, achieving competitive results while offering interpretability.
Decision: Accept
The key reasons for this decision are the novelty of the proposed method and its potential impact on improving the interpretability of LSTMs. The paper introduces a well-motivated and technically sound approach to address the "black-box" nature of LSTMs, which is a significant challenge in deep learning. The method is validated through both quantitative results and qualitative insights, demonstrating its effectiveness and practical relevance.
Supporting Arguments:
1. Novelty and Contribution: The decomposition of LSTM outputs into interpretable importance scores is a novel contribution. The ability to extract meaningful patterns and phrases from LSTMs addresses a critical gap in understanding neural network behavior, particularly in NLP tasks.
2. Experimental Validation: The proposed method is rigorously evaluated on multiple datasets, showcasing its generalizability across tasks. The rule-based classifier, while simple, achieves reasonable performance, validating the utility of the extracted patterns.
3. Interpretability: The extracted patterns provide insights into the decision-making process of LSTMs, which is valuable for both researchers and practitioners. The qualitative examples and heatmaps further enhance the interpretability of the results.
Additional Feedback:
1. Limitations and Approximation Gap: While the paper acknowledges the approximation gap between the LSTM and the rule-based classifier, a more detailed analysis of why certain patterns fail in specific contexts would strengthen the discussion. Exploring ways to reduce this gap could be a valuable extension.
2. Scalability: The two-step phrase extraction process is computationally efficient, but the scalability to larger datasets or more complex tasks could be discussed in greater detail.
3. Comparison with Baselines: While the paper compares its method to prior work, additional baselines, such as attention-based interpretability methods, could provide a more comprehensive evaluation.
Questions for the Authors:
1. How does the proposed method perform on datasets or tasks with highly imbalanced classes or noisy data?  
2. Could the approach be extended to other neural architectures, such as Transformers, and if so, what modifications would be required?  
3. Are there any insights into the trade-offs between interpretability and performance when using the rule-based classifier instead of the LSTM?  
Overall, this paper makes a significant contribution to the field of interpretable AI and provides a promising direction for future research. With minor improvements, it has the potential to become a highly impactful work.