The paper presents a novel approach to the "classless association" problem, inspired by the Symbol Grounding Problem and infant sensory association learning. The authors propose a model with two parallel Multilayer Perceptrons (MLPs) trained using an Expectation-Maximization (EM) approach to associate two input instances of the same unknown class. The model is evaluated on four classless datasets derived from MNIST, demonstrating promising results compared to supervised and unsupervised baselines. The contributions include a novel training rule based on statistical distribution matching, a unique architecture for classless association, and a comparative evaluation framework.
Decision: Accept with Minor Revisions
Key reasons for this decision are the novelty of the proposed approach and its demonstrated effectiveness in addressing the classless association task. However, there are areas where the paper could be improved for greater clarity and impact.
Supporting Arguments:
1. Novelty and Contribution: The paper introduces a unique training paradigm and architecture for classless association, which is a relatively unexplored area. The EM-based training rule and the use of pseudo-classes are innovative and well-motivated by the Symbol Grounding Problem.
2. Experimental Validation: The authors provide a thorough evaluation on four datasets, demonstrating that their model outperforms unsupervised clustering methods and achieves competitive results compared to supervised baselines. The use of metrics like Association Accuracy and Purity adds rigor to the evaluation.
3. Theoretical Grounding: The model builds on established concepts like statistical distribution matching and EM training, which are well-explained and contextualized within the literature.
Suggestions for Improvement:
1. Clarity of Presentation: The paper is dense and could benefit from clearer explanations, particularly in the methodology section. For instance, the role of the weighting vector Î³ and its update mechanism could be elaborated further for better understanding.
2. Limitations and Future Work: While the authors acknowledge limitations (e.g., reliance on uniform distributions and known class counts), they could discuss these in more depth. For example, how does the model perform when the number of classes or the distribution is unknown? This would strengthen the discussion on generalizability.
3. Comparison with Related Work: The paper briefly mentions similarities with Siamese Networks but does not provide a detailed comparison. A more in-depth discussion of how the proposed model differs from or improves upon existing approaches like dual autoencoders or clustering-based methods would enhance its positioning in the literature.
4. Reproducibility: While the experimental setup is detailed, the paper does not explicitly state whether the code or datasets will be made available. Providing these would greatly enhance the paper's impact and reproducibility.
Questions for the Authors:
1. How does the model handle scenarios where the number of classes or the statistical distribution is unknown? Have you considered adaptive methods for determining the number of pseudo-classes?
2. The paper mentions that the model performs better with larger mini-batches. Could you elaborate on why this is the case and how it might affect scalability to larger datasets?
3. Have you tested the model on datasets with more complex transformations or multimodal inputs? If so, how does it generalize beyond the MNIST-based datasets?
Conclusion:
The paper addresses an interesting and underexplored problem with a novel and well-supported approach. While the work is promising, addressing the suggested improvements would enhance its clarity, impact, and generalizability. I recommend acceptance with minor revisions to address these points.