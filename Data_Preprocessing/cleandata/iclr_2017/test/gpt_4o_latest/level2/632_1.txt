The paper proposes the Gaussian attention model for content-based neural memory access, introducing a novel scoring function based on multivariate Gaussian likelihoods. This model enables neural networks to flexibly adjust their attention from narrow to broad focus, depending on the semantic distance in latent space. The authors demonstrate its application in embedding knowledge bases into continuous vector spaces and training question-answering models. The proposed model effectively handles uncertainty propagation and conjunctive queries, achieving significant improvements over baseline models like TransE. Experiments on the WorldCup2014 dataset validate the model's ability to answer path and conjunctive queries with higher accuracy.
Decision: Accept
The paper is well-motivated, presents a novel contribution, and demonstrates strong empirical results. The Gaussian attention model offers a meaningful advancement over existing approaches, particularly in its ability to handle uncertainty and conjunctions in question answering. However, there are areas for improvement in clarity and evaluation.
Supporting Arguments:
1. Novelty and Contribution: The Gaussian attention model extends prior work by introducing a quadratic energy function for attention, which allows for more nuanced control over attention spread. This is a significant improvement over inner-product-based attention mechanisms.
2. Empirical Validation: The model achieves state-of-the-art results on the WorldCup2014 dataset, particularly in handling complex queries involving relation composition and conjunctions. The compositional training approach further strengthens its performance.
3. Theoretical Soundness: The paper provides a clear mathematical formulation of the Gaussian attention model and its integration into knowledge base embeddings and question answering.
Additional Feedback:
1. Clarity of Presentation: While the technical details are thorough, the paper could benefit from a clearer explanation of the intuition behind the Gaussian attention model, particularly for readers less familiar with attention mechanisms. Visualizations of the attention mechanism in action would enhance understanding.
2. Dataset Limitations: The WorldCup2014 dataset is relatively small and domain-specific. Future work should evaluate the model on larger, more diverse datasets to establish its generalizability.
3. Baseline Comparisons: While the paper compares the model to TransE, additional comparisons with more recent or complex models (e.g., neural tensor networks) would strengthen the claims.
4. Handling Weak Queries: The paper acknowledges weaker performance on certain queries (e.g., "is in country" relations). A deeper analysis of these limitations and potential solutions would be valuable.
Questions for the Authors:
1. How does the Gaussian attention model scale to larger knowledge bases with millions of entities and relations? Are there computational bottlenecks?
2. Could the model be extended to handle non-commutative relations or more complex query types, such as disjunctions or negations?
3. How sensitive is the model to the choice of hyperparameters, such as embedding dimensions and the structure of the covariance matrix?
Overall, the paper presents a compelling and innovative approach to improving neural memory access and question answering. With minor revisions to address clarity and broader evaluation, it has the potential to make a strong impact in the field.