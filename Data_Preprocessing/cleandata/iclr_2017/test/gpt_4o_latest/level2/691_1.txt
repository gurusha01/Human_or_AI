Review of "The Retro Learning Environment (RLE): A New Benchmark for Reinforcement Learning"
This paper introduces the Retro Learning Environment (RLE), a novel reinforcement learning (RL) benchmark that expands upon the widely-used Arcade Learning Environment (ALE). RLE supports games from more advanced gaming consoles like the Super Nintendo Entertainment System (SNES) and Sega Genesis, offering a unified interface for RL research. The authors claim that RLE presents significant challenges for state-of-the-art RL algorithms due to the complexity, delayed rewards, and stochasticity of SNES games. Additionally, the paper introduces a multi-agent training feature and demonstrates its potential to improve agent robustness. The authors also provide open-source implementation details and highlight RLE's extensibility to other gaming consoles.
Decision: Accept
The paper makes a compelling case for the significance of RLE as a new benchmark for RL research. The primary reasons for acceptance are:  
1. Novelty and Contribution: RLE addresses limitations in existing benchmarks like ALE by incorporating more complex games and multi-agent capabilities. This represents a meaningful advancement for the RL community.  
2. Comprehensive Evaluation: The authors provide rigorous experiments comparing state-of-the-art RL algorithms on SNES games, demonstrating the challenges posed by RLE. The inclusion of reward shaping and multi-agent experiments further strengthens the paper's contributions.  
Supporting Arguments
The paper is well-motivated, as it builds on the success of ALE while addressing its limitations. The authors effectively demonstrate the increased complexity of SNES games compared to Atari games, including larger action spaces, richer visuals, and delayed rewards. The experiments are thorough, with clear evaluation methodologies and comparisons to human performance. The results highlight the inability of current algorithms to generalize or outperform humans in most SNES games, underscoring the need for further research. The multi-agent experiments are particularly interesting, as they reveal challenges like catastrophic forgetting and the potential for improved generalization through alternating opponents.
The implementation details, including the use of the LibRetro interface and open-source availability, ensure reproducibility and encourage adoption by the RL community. The paper also acknowledges limitations, such as the reliance on reward shaping for certain games, and outlines future challenges, making it a constructive contribution to the field.
Suggestions for Improvement
1. Clarity on Generalization: While the experiments demonstrate challenges in generalization (e.g., agents failing to adapt to unseen levels or policies), the paper could provide more insights into why these failures occur and suggest potential solutions.  
2. Broader Algorithm Comparisons: The experiments focus primarily on DQN variants. Including results from other RL paradigms, such as policy-gradient methods or model-based RL, could provide a more comprehensive evaluation of RLE.  
3. Human Baseline Details: The methodology for human performance evaluation could be elaborated, particularly regarding the training duration and expertise of the human players.  
4. Reward Shaping Dependency: While reward shaping improves performance, it introduces domain-specific knowledge. The authors could discuss how to design more general reward functions or explore intrinsic motivation techniques to address this dependency.
Questions for the Authors
1. How does RLE handle games with highly stochastic elements or dynamic difficulty adjustments? Are there mechanisms to ensure fair comparisons across algorithms?  
2. Have you considered integrating RLE with other popular RL frameworks like OpenAI Gym or RLlib to increase accessibility?  
3. Can you elaborate on the scalability of RLE to newer consoles or more computationally intensive games, such as modern 3D titles?  
Conclusion
Overall, this paper makes a strong contribution to the RL field by introducing a challenging and extensible benchmark. While there are areas for improvement, the novelty, thorough evaluation, and open-source implementation make this work a valuable resource for advancing RL research. I recommend acceptance.