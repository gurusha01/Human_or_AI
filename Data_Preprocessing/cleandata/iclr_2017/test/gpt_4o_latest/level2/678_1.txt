The paper investigates transfer learning in text comprehension, focusing on pre-training neural models on large datasets and evaluating their ability to generalize to low-resource target domains. The authors use the BookTest and CNN/Daily Mail datasets for pre-training and assess transfer performance on bAbI tasks and a subset of SQuAD. The study reveals limited transfer when no target-domain examples are provided, but demonstrates significant improvements when even a small number of target-domain examples are introduced. Additionally, the paper explores the contributions of pre-trained word embeddings and context encoders, finding that both components contribute to transfer performance.
Decision: Accept (with minor revisions)  
The paper makes a valuable contribution to the field of NLP by addressing the underexplored topic of transfer learning in reading comprehension. Its findings, particularly the positive impact of pre-training followed by target adjustment, are relevant and could stimulate further research. However, the paper has some limitations that need to be addressed to strengthen its impact.
Supporting Arguments:  
1. Novelty and Contribution: This is one of the first studies to examine transfer learning in reading comprehension, distinguishing it from prior work focused on text classification and parsing. The exploration of pre-training benefits beyond word embeddings is particularly insightful.  
2. Experimental Rigor: The authors conduct extensive experiments, training over 3,700 models, and provide detailed analyses of results across multiple datasets and tasks. The inclusion of both artificial (bAbI) and real-world (SQuAD) datasets enhances the generalizability of the findings.  
3. Practical Relevance: The study addresses a critical challenge in NLP—generalization to low-resource domains—making it highly relevant to the community.
Suggestions for Improvement:  
1. Clarity of Results: While the paper provides extensive data, the presentation of results could be streamlined. For instance, the tables and figures are dense and could benefit from clearer summaries or visualizations to highlight key trends.  
2. Baseline Comparisons: The paper compares its results to state-of-the-art models like MemN2N and DCR but does not fully contextualize the performance gap. A discussion on why the AS Reader underperforms and how it could be improved would be helpful.  
3. Acknowledgment of Limitations: The paper acknowledges that its model does not achieve state-of-the-art results but could further discuss the implications of this for real-world applications. For example, how might the findings generalize to other NLP tasks or architectures beyond the AS Reader?  
4. Reproducibility: While the authors mention plans to release datasets and code, providing these resources upfront would enhance the paper's impact and reproducibility.
Questions for Authors:  
1. How do you envision the findings of this study influencing the design of future NLP models for low-resource domains?  
2. Could the poor transfer performance without target adjustment be attributed to the specific architecture of the AS Reader? Would other architectures (e.g., transformers) yield different results?  
3. Did you explore any alternative pre-training tasks or datasets beyond cloze-style question answering? If not, how might such alternatives impact the results?
Overall, the paper addresses an important problem and provides meaningful insights, but minor revisions to improve clarity and contextualization would enhance its contribution.