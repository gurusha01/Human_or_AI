The paper proposes a novel method for compressing and accelerating deep neural networks by factorizing both weights and activations into integer and non-integer components. Specifically, it introduces a ternary matrix decomposition for weights and binary encoding for activations, enabling efficient test-time computations using logical operations (AND, XOR, and bit count). The method achieves significant memory compression and computational acceleration, as demonstrated on three networks: a CNN for MNIST, VGG-16 for ImageNet, and VGG-Face for face recognition. Notably, the method achieves up to 15× acceleration and a 5.2% memory footprint with minimal accuracy degradation (e.g., 1.43% increase in top-5 error for VGG-16). The approach does not require retraining, making it a practical post-processing step for pre-trained models.
Decision: Accept
The paper is well-motivated, presents a clear and innovative contribution, and provides strong empirical evidence to support its claims. The key reasons for acceptance are: (1) the novelty of combining ternary matrix factorization and binary activation encoding, which bridges the gap between matrix factorization and integer decomposition methods, and (2) the practical utility of the method, demonstrated across diverse tasks with significant improvements in computational efficiency and memory usage.
Supporting Arguments:
1. Novelty and Contribution: The paper introduces a unified framework that combines the strengths of matrix/tensor factorization and integer decomposition. The use of ternary matrices ({−1, 0, +1}) for weights and binary encoding for activations is a novel approach that outperforms binary-only methods in terms of approximation quality.
2. Empirical Validation: The experiments are thorough, covering three distinct networks and tasks (classification and feature embedding). Results demonstrate the method's effectiveness in balancing compression, acceleration, and accuracy, with detailed analysis of parameter settings (e.g., kw and kx).
3. Practicality: The method is a post-training step, requiring no changes to training algorithms or retraining, which makes it highly applicable to existing pre-trained models.
Additional Feedback for Improvement:
1. Clarity of Presentation: While the technical details are comprehensive, the paper could benefit from a clearer explanation of key equations (e.g., Eq. (3)) and algorithms (e.g., Algorithm 1). Including more intuitive descriptions or visualizations would help readers unfamiliar with matrix factorization.
2. Limitations and Future Work: The paper briefly mentions limitations (e.g., approximation errors) and future directions but could elaborate further. For instance, how does the method perform on transformer-based architectures or tasks beyond vision (e.g., NLP)?
3. Comparison with State-of-the-Art: While the paper references related work, a more direct empirical comparison with recent methods like XNOR-Net or pruning techniques would strengthen the evaluation.
Questions for the Authors:
1. How does the method scale with larger models, such as transformers or modern vision architectures (e.g., Vision Transformers)?
2. Can the proposed approach be extended to tasks requiring dynamic inference, such as object detection or sequence generation?
3. How sensitive is the method to the choice of kw and kx, and are there guidelines for selecting these parameters for new architectures?
In summary, the paper presents a significant and practically useful contribution to the field of network compression and acceleration. With minor improvements in clarity and additional comparisons, it has the potential to make a strong impact.