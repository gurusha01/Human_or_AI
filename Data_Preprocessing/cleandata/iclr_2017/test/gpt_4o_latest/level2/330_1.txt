The paper introduces Doc2VecC, a novel document representation learning framework that efficiently generates semantic document embeddings by averaging word embeddings while incorporating a corruption model. The corruption mechanism introduces data-dependent regularization, favoring rare and informative words while suppressing frequent and non-discriminative ones. The authors claim that Doc2VecC matches or outperforms state-of-the-art methods in sentiment analysis, document classification, and semantic relatedness tasks, while being computationally efficient for both training and inference.
Decision: Accept
Key reasons: (1) The paper presents a significant improvement in efficiency and performance over existing methods, with strong empirical results across diverse tasks. (2) The simplicity of the model architecture, combined with the corruption mechanism, is a novel and practical contribution to the field.
Supporting Arguments:
1. Claims and Empirical Validation: The authors substantiate their claims with rigorous experiments on sentiment analysis (IMDB dataset), document classification (Wikipedia dataset), and semantic relatedness (SICK dataset). Doc2VecC consistently outperforms baselines like Word2Vec, Paragraph Vectors, and Denoising Autoencoders in accuracy while being significantly faster, especially during test-time inference. The corruption mechanism is well-justified as a form of regularization, and its impact is quantitatively demonstrated.
2. Novelty and Practicality: The corruption model and the simplicity of averaging word embeddings during training and inference are innovative. Unlike Paragraph Vectors, which require expensive inference for unseen documents, Doc2VecC generates embeddings efficiently, making it highly practical for real-world applications.
3. Efficiency: The reported training and inference times highlight the scalability of Doc2VecC, especially for large datasets. This is a crucial advantage over computationally intensive methods like Skip-thought Vectors and LSTM-based approaches.
4. Field Knowledge and Completeness: The paper demonstrates a strong understanding of prior work, situating Doc2VecC within the broader context of document representation learning. The authors provide comprehensive comparisons with baselines and state-of-the-art methods, ensuring reproducibility through publicly available code.
Suggestions for Improvement:
1. Ablation Studies: While the corruption mechanism is central to the model, a more detailed ablation study isolating its effect on performance (e.g., varying corruption rates) would strengthen the argument.
2. Error Analysis: The paper could benefit from a qualitative analysis of failure cases to better understand the limitations of Doc2VecC, particularly in tasks where it underperforms LSTM-based methods.
3. Broader Evaluation: Including additional datasets or tasks, such as machine translation or question answering, would further demonstrate the generalizability of Doc2VecC.
Questions for the Authors:
1. How does the performance of Doc2VecC vary with different corruption rates (q)? Is there an optimal range for q across tasks?
2. Could the corruption mechanism be extended to incorporate semantic or syntactic information, rather than random word removal?
3. How does Doc2VecC handle out-of-vocabulary (OOV) words in low-resource languages or domains with specialized vocabularies?
Overall, the paper makes a compelling case for Doc2VecC as a practical and effective solution for document representation learning. With minor improvements, it has the potential to make a significant impact in the field.