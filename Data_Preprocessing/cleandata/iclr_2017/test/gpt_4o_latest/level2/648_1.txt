The paper proposes a novel semi-supervised learning framework based on in-painting using adversarial loss, termed Context-Conditional Generative Adversarial Networks (CC-GANs). The authors claim that their approach enables the training of large discriminative models, such as VGG-style networks, in a semi-supervised fashion, and achieves competitive or superior performance on STL-10 and PASCAL VOC datasets compared to existing methods. The primary contribution lies in leveraging in-painting as a regularization task for the discriminator, which learns features useful for object classification. The paper also introduces a combined GAN and CC-GAN approach (CC-GAN2) to improve the diversity of negative examples for the discriminator.
Decision: Accept
Key Reasons for Decision:
1. Novelty and Contribution: The paper presents a significant innovation in semi-supervised learning by repurposing in-painting with adversarial loss to train discriminative models. The CC-GAN framework is a clear improvement over existing methods, particularly in its ability to train large models directly with adversarial loss.
2. Empirical Validation: The experimental results on STL-10 and PASCAL VOC datasets demonstrate that CC-GAN achieves state-of-the-art performance, outperforming prior methods by a notable margin. The inclusion of ablation studies and comparisons with baselines strengthens the validity of the claims.
Supporting Arguments:
- The authors provide a well-motivated approach by highlighting the limitations of existing semi-supervised methods and positioning their work within the broader context of adversarial learning and representation learning.
- The use of adversarial loss for in-painting as a regularization task is both innovative and practical, as it aligns closely with the target task of object classification.
- The paper includes rigorous experiments, such as comparisons with supervised baselines, semi-supervised GANs, and related in-painting methods (e.g., Pathak et al., 2016). The results convincingly demonstrate the superiority of CC-GAN, particularly in terms of classification accuracy and feature quality.
- The inclusion of qualitative results (e.g., in-painting examples) further supports the robustness of the proposed method.
Additional Feedback:
1. Reproducibility: While the authors provide architectural details and training procedures, a more explicit discussion of hyperparameter sensitivity and computational requirements would enhance reproducibility.
2. Limitations: The paper briefly acknowledges challenges in scaling to high-resolution images but does not explore potential solutions in depth. A more detailed discussion of this limitation and future directions would strengthen the paper.
3. Clarity: While the technical content is well-presented, certain sections (e.g., the combined CC-GAN2 objective) could benefit from additional clarification or visual aids to improve accessibility for readers unfamiliar with GANs.
Questions for Authors:
1. How sensitive is the performance of CC-GAN to the choice of hole size and location during in-painting? Would a more structured masking strategy (e.g., object-aware masks) improve results?
2. Have you explored the impact of alternative architectures for the generator and discriminator, particularly for scaling to higher-resolution images?
3. Could the proposed method be extended to other domains (e.g., video or text) where context-based reconstruction is relevant?
Overall, the paper makes a strong contribution to semi-supervised learning and adversarial training, and I recommend its acceptance with minor revisions to address the above feedback.