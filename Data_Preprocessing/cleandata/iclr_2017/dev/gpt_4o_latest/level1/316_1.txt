Review of the Paper: "Private Aggregation of Teacher Ensembles (PATE)"
Summary of Contributions
This paper addresses the critical problem of protecting sensitive training data in machine learning models, particularly in applications involving private information such as medical records. The authors propose the PATE framework, which combines knowledge aggregation and transfer from an ensemble of teacher models trained on disjoint datasets to a student model. The student learns through noisy voting among the teachers, ensuring differential privacy. The paper claims several contributions: (1) a general learning strategy for differential privacy, (2) improved privacy analysis using the moments accountant, (3) application of semi-supervised learning with GANs to reduce privacy loss, and (4) state-of-the-art privacy/utility trade-offs on MNIST and SVHN datasets. The approach is algorithm-agnostic and applicable to various machine learning models, including deep neural networks. The results demonstrate strong privacy guarantees with minimal utility loss, making this work a significant advancement in privacy-preserving machine learning.
Decision: Accept
The paper makes a strong case for acceptance due to its novel contributions to differential privacy in machine learning, rigorous theoretical analysis, and empirical validation. The key reasons for this decision are: 
1. Novelty and Impact: The PATE framework introduces a practical, generalizable, and theoretically sound approach to achieving differential privacy in machine learning, addressing a critical need in privacy-sensitive applications.
2. Scientific Rigor: The paper provides a detailed privacy analysis using the moments accountant and demonstrates state-of-the-art results on benchmark datasets, validating its claims.
Supporting Arguments
1. Well-Motivated Problem: The paper is well-placed in the literature, building on prior work in differential privacy and knowledge transfer. It addresses a pressing issue—protecting sensitive training data—while improving upon existing methods like noisy SGD and privacy-preserving random forests.
2. Theoretical and Empirical Strength: The authors provide a rigorous privacy analysis, leveraging the moments accountant to bound privacy loss dynamically. The empirical results on MNIST and SVHN demonstrate that PATE achieves superior privacy/utility trade-offs compared to prior methods, such as noisy SGD.
3. Generality and Applicability: The framework is algorithm-agnostic, making it applicable to a wide range of machine learning models and datasets, including medical data and random forests, as demonstrated in the appendices.
Suggestions for Improvement
1. Clarity of Presentation: While the paper is thorough, some sections, particularly the privacy analysis, are dense and could benefit from additional explanatory figures or examples to aid understanding for non-expert readers.
2. Broader Evaluation: The experiments are primarily focused on MNIST and SVHN. While the appendices include results on additional datasets (e.g., UCI Adult and Diabetes), further evaluation on more diverse datasets, particularly in real-world privacy-sensitive domains like healthcare, would strengthen the paper.
3. Scalability Discussion: The paper could elaborate on the scalability of PATE to larger datasets and tasks with more complex output spaces, as the number of teachers required may grow significantly.
4. Comparison with Other Semi-Supervised Methods: The paper briefly mentions semi-supervised learning with GANs but does not compare PATE-G to other state-of-the-art semi-supervised methods in terms of utility and privacy guarantees.
Questions for the Authors
1. How does the PATE framework perform on tasks with a large number of output classes, where the number of teachers required may be prohibitively high? Are there strategies to mitigate this?
2. Can the authors provide more details on the computational overhead introduced by the moments accountant and noisy aggregation mechanisms?
3. How sensitive are the results to the choice of the Laplacian noise scale (γ)? Could this parameter be adaptively tuned during training?
4. Have the authors considered applying PATE to sequence-based models like RNNs or transformers? If so, what challenges arise?
In conclusion, this paper makes a significant contribution to the field of privacy-preserving machine learning and is well-suited for acceptance at the conference. The suggestions provided are intended to further enhance the clarity and impact of the work.