The paper introduces the Input-Switched Affine Network (ISAN), a novel recurrent neural network (RNN) architecture that replaces nonlinear dynamics with input-dependent affine transformations. The authors claim that ISAN achieves comparable performance to traditional RNNs, such as LSTMs and GRUs, on character-level language modeling tasks while offering significant advantages in interpretability and computational efficiency. By leveraging the linearity of its architecture, the paper demonstrates how ISAN enables detailed analyses of contributions from past inputs, facilitates word-level aggregation, and supports computational optimizations like precomputing transformations for input sequences. The work is positioned as a step toward designing intelligible neural networks without sacrificing performance.
Decision: Accept
The key reasons for this decision are:  
1. Novelty and Practical Relevance: The ISAN architecture is a fresh and innovative approach to improving the interpretability of RNNs, addressing a critical challenge in deploying neural networks in high-stakes applications.  
2. Scientific Rigor: The paper provides thorough empirical and theoretical evidence to support its claims, including performance comparisons, interpretability analyses, and computational efficiency evaluations.
Supporting Arguments:  
1. The paper is well-motivated and grounded in the literature. It situates ISAN within the context of prior work on interpretable neural networks and linear dynamical systems, clearly articulating its contributions.  
2. The results are robust and scientifically rigorous. The authors demonstrate ISAN's comparable performance to nonlinear RNNs on the Text8 dataset, analyze its interpretability using linear decomposition methods, and validate its computational benefits through detailed experiments.  
3. The interpretability analyses are particularly compelling. The ability to decompose predictions into contributions from past inputs and analyze word-level dynamics is a significant advancement over traditional RNNs.  
Suggestions for Improvement:  
1. Broader Benchmarking: While the Text8 dataset is a standard benchmark, additional experiments on diverse tasks (e.g., word-level language modeling or tasks with larger vocabularies) would strengthen the generalizability of the results.  
2. Scalability Discussion: The paper briefly mentions challenges in scaling ISAN to large vocabularies. A more detailed exploration of potential solutions, such as tensor factorization or hierarchical switching, would enhance the paper's practical relevance.  
3. Comparison to Simpler Baselines: While ISAN is compared to nonlinear RNNs, a more detailed comparison to simpler baselines (e.g., n-gram models) could clarify its relative advantages in terms of performance and interpretability.  
Questions for the Authors:  
1. How does ISAN handle long-term dependencies compared to nonlinear RNNs? Are there tasks where its linear dynamics might be a limitation?  
2. Can the computational benefits of ISAN, such as precomputing affine transformations, be realized in practice for large-scale datasets with high input diversity?  
3. How sensitive is ISAN to hyperparameter choices, such as the number of hidden units or the size of the input vocabulary?  
Overall, this paper makes a strong contribution to the field of interpretable neural networks and is well-suited for acceptance at the conference.