Review of the Paper
Summary of Contributions
This paper addresses the critical challenge of exploration in reinforcement learning (RL) environments with sparse rewards, proposing a novel intrinsic motivation framework based on surprise. Specifically, the authors introduce two intrinsic reward formulations—surprisal and k-step learning progress—derived from the KL-divergence between the true transition probabilities of the environment and a learned dynamics model. The contributions include:
1. Empirical evidence demonstrating that these intrinsic rewards enable efficient exploration in sparse reward environments, outperforming several heuristic methods and achieving comparable performance to state-of-the-art approaches like VIME, but at a lower computational cost.
2. The introduction of a new sparse reward benchmark task (sparse Swimmer) and evaluation of existing benchmarks to assess exploration incentives.
3. A computationally efficient method for concurrently learning the dynamics model and policy.
The paper is well-motivated, situating its contributions within the broader literature on intrinsic motivation and exploration in RL. The results are compelling, showing robust performance across continuous control and Atari RAM tasks, with surprisal emerging as a particularly effective and computationally efficient exploration incentive.
Decision: Accept
The paper is recommended for acceptance due to its significant contributions to intrinsic motivation in RL, its rigorous empirical evaluation, and its practical implications for improving exploration in sparse reward environments. The key reasons for this decision are:
1. Novelty and Practicality: The surprisal-based intrinsic rewards are conceptually elegant, computationally efficient, and empirically effective, offering a viable alternative to existing methods like VIME.
2. Comprehensive Evaluation: The authors evaluate their methods across diverse and challenging benchmarks, providing strong evidence of generalizability and robustness.
Supporting Arguments
1. Well-Motivated Approach: The paper builds on prior work in intrinsic motivation, clearly distinguishing its contributions from existing methods (e.g., VIME, L2 prediction error). The use of surprisal and learning progress as intrinsic rewards is well-justified, both theoretically and empirically.
2. Empirical Rigor: The experiments are thorough, covering both continuous control and discrete action tasks with deterministic and stochastic dynamics. The results demonstrate consistent improvements over baseline methods and competitive performance with state-of-the-art approaches.
3. Computational Efficiency: The analysis of computational costs highlights a significant advantage of the proposed methods over VIME, making them more practical for large-scale RL applications.
Suggestions for Improvement
1. Clarify Limitations: While the paper discusses potential limitations (e.g., the pathological behavior of surprisal in fully learned dynamics models), it would benefit from a more explicit analysis of failure cases or environments where the proposed methods might underperform.
2. Broader Comparisons: The empirical evaluation could include comparisons with other recent intrinsic motivation methods, such as those based on pseudo-counts or empowerment, to further contextualize the contributions.
3. Ablation Studies: Additional ablation studies on the dynamics model architecture, hyperparameters (e.g., η, κ), and the choice of k in learning progress would provide deeper insights into the sensitivity of the proposed methods.
4. Theoretical Guarantees: While the empirical results are strong, the paper could benefit from a discussion of potential theoretical guarantees or bounds for the proposed intrinsic rewards, particularly in relation to exploration efficiency.
Questions for the Authors
1. How sensitive are the results to the choice of hyperparameters, particularly η (the explore-exploit trade-off coefficient) and κ (the KL-divergence step size)?
2. Can the proposed methods handle environments with highly stochastic dynamics as effectively as deterministic ones? If not, what modifications might be necessary?
3. How do the surprisal and learning progress bonuses compare in terms of stability and convergence in more complex, hierarchical tasks?
4. Could the proposed methods be extended to multi-agent RL settings, where exploration incentives might need to account for interactions between agents?
In conclusion, this paper makes a valuable contribution to the field of reinforcement learning by introducing scalable and effective intrinsic motivation strategies for sparse reward environments. With minor clarifications and additional experiments, it has the potential to further strengthen its impact.