The paper introduces Variational Canonical Correlation Analysis (VCCA), a deep generative model for multi-view representation learning, which extends the probabilistic latent variable model interpretation of linear CCA to nonlinear settings using deep neural networks (DNNs). The authors also propose a variant, VCCA-private, which disentangles shared and private latent variables in multi-view data. The paper claims that VCCA provides a scalable, efficient, and generative approach to multi-view learning, while VCCA-private improves reconstruction quality and disentanglement. Empirical results demonstrate competitive or superior performance on tasks involving image, speech, and text data, with the added ability to generate high-quality samples.
Decision: Accept
The paper is well-motivated and makes a significant contribution to the field of multi-view learning by combining the strengths of variational inference and deep generative models. The proposed methods are novel, scientifically rigorous, and demonstrate strong empirical performance. The ability to disentangle shared and private variables without supervision is particularly compelling, as it addresses a key limitation of prior work. The paper is also well-placed in the literature, building on foundational work in CCA, deep learning, and variational inference.
Supporting Arguments:
1. Novelty and Contribution: The extension of CCA to a deep generative framework is novel and well-justified. The introduction of VCCA-private to model private variables is a meaningful addition that addresses practical challenges in multi-view data.
2. Empirical Validation: The experiments are comprehensive, spanning multiple domains (e.g., image, speech, and text), and the results consistently show that VCCA and VCCA-private perform competitively or better than existing methods. The disentanglement of shared and private variables is convincingly demonstrated.
3. Scientific Rigor: The derivation of the variational lower bound and the use of Monte Carlo sampling with the reparameterization trick are technically sound. The connection to prior work, such as VAEs and multi-view autoencoders, is clearly articulated.
Suggestions for Improvement:
1. Clarity in Derivations: While the mathematical derivations are thorough, they can be dense for readers unfamiliar with variational inference. Adding a high-level explanation or diagram summarizing the key steps in deriving the VCCA objective would improve accessibility.
2. Ablation Studies: The paper could benefit from more detailed ablation studies to isolate the contributions of specific components, such as the KL divergence term or the private variable modeling in VCCA-private.
3. Generative Quality: While the paper mentions the ability to generate high-quality samples, more qualitative examples (e.g., reconstructed images or generated text) would strengthen this claim.
4. Computational Efficiency: A discussion of the computational cost of VCCA and VCCA-private compared to other methods would be helpful, especially given the scalability claims.
Questions for Authors:
1. How sensitive is the performance of VCCA and VCCA-private to the choice of hyperparameters, such as the dimensionality of the latent space or the dropout rate?
2. Can the authors provide more examples or metrics to evaluate the quality of generated samples, particularly for complex datasets like MIR-Flickr?
3. How does VCCA-private handle cases where the private variables dominate the shared variables in terms of variance? Are there scenarios where this disentanglement fails?
In conclusion, the paper makes a substantial contribution to multi-view learning and is well-suited for acceptance, with minor improvements to clarity and additional experiments to further strengthen its impact.