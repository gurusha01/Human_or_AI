Review of the Paper
The paper investigates the potential for sparsity-centric optimization in training LSTM-based RNNs, a domain where such techniques have been underexplored compared to CNNs. The authors identify sparsity in the gradients during backward propagation and propose a simple thresholding technique, termed "sparsified SGD," to induce additional sparsity. The experimental results demonstrate that the proposed method achieves over 80% sparsity in linear gate gradients without degrading model performance, leading to a reduction of more than 50% in redundant multiply-accumulate (MAC) operations during training. The paper positions its contributions as a step toward improving the energy efficiency and training speed of LSTM-based RNNs, with implications for hardware acceleration.
Decision: Accept
The paper is well-motivated and makes a clear and valuable contribution to the field of sparsity-centric optimization for LSTM-based RNNs. The key reasons for acceptance are: (1) the novelty of exploring sparsity in LSTM training gradients, which has been less studied compared to CNNs, and (2) the strong experimental evidence supporting the claims, including validation across multiple applications and datasets. The proposed technique is simple yet effective, and the results are scientifically rigorous.
Supporting Arguments
1. Problem Relevance and Novelty: The paper addresses a significant gap in the literature by extending sparsity-centric optimization techniques to LSTM-based RNNs. The identification of sparsity in backward propagation gradients is novel and well-motivated by the skewed distribution of gate activations.
2. Scientific Rigor: The experimental results are thorough, spanning multiple applications (language modeling, image captioning, and machine translation) and datasets. The authors demonstrate the generality of their approach through sensitivity tests with varying network topologies and sequence lengths.
3. Impact and Practicality: The proposed sparsified SGD technique is simple to implement and has practical implications for hardware acceleration. The reduction in redundant MAC operations is significant, making the work highly relevant for energy-efficient deep learning.
Suggestions for Improvement
1. Dynamic Thresholding: The paper mentions the potential for a dynamic thresholding approach in future work. Including preliminary results or a discussion of how such a method might be designed could strengthen the paper.
2. Hardware Implementation Details: While the paper discusses the potential for hardware acceleration, it would benefit from more concrete details or simulations demonstrating how the sparsity could be exploited in real-world hardware systems.
3. Comparison with Related Techniques: The paper could include a more detailed comparison with other sparsity-inducing methods, such as reduced precision techniques, to better contextualize its contributions.
Questions for the Authors
1. How does the proposed sparsified SGD technique compare to other gradient sparsification methods, if any exist, in terms of computational overhead and performance trade-offs?
2. Could the thresholding technique be extended to other types of RNN cells (e.g., GRUs) or even to non-recurrent architectures? If so, what challenges might arise?
3. How sensitive is the proposed method to the choice of the threshold across different datasets and tasks? Would an adaptive thresholding mechanism be necessary for more complex applications?
Overall, the paper presents a compelling contribution to the field and is recommended for acceptance with minor revisions to address the above suggestions.