Review of the Paper
Summary of Contributions
This paper introduces a novel training procedure for learning a generative model as the transition operator of a Markov chain. The key innovation lies in the "infusion training" method, which progressively denoises random noise into high-quality samples matching the target distribution. Unlike prior approaches, such as those based on inverting slow diffusion processes or adversarial training, this method uses a single network and avoids the instability of GANs. The authors demonstrate competitive results on several datasets, including MNIST, CIFAR-10, and CelebA, and provide both qualitative and quantitative evaluations of their model's performance. The paper also situates its approach within the broader literature, highlighting connections to and differences from related methods like VAEs, Generative Stochastic Networks, and diffusion-based models.
Decision: Accept
The paper presents a well-motivated and novel approach to generative modeling, with clear empirical evidence supporting its claims. The infusion training procedure is a meaningful contribution to the field, addressing limitations of existing methods (e.g., slow sampling in diffusion models, instability in GANs). The results, particularly on natural image datasets, are competitive and demonstrate the practical utility of the approach. The paper is well-written, thorough in its experimental evaluation, and provides sufficient theoretical grounding for its methodology.
Supporting Arguments
1. Novelty and Motivation: The infusion training procedure is a creative solution to the problem of learning a generative model that efficiently produces high-quality samples. By infusing target information into the training chain, the method avoids the pitfalls of prior approaches, such as the need for equilibrium distributions or adversarial balancing.
   
2. Empirical Rigor: The experiments are comprehensive, covering multiple datasets and evaluation metrics (e.g., Parzen-window estimates, likelihood bounds, Inception scores). The qualitative results, including sample generation and inpainting, convincingly demonstrate the model's ability to generate diverse and realistic outputs.
3. Positioning in Literature: The paper does an excellent job of situating its contributions within the broader landscape of generative modeling. It clearly distinguishes its approach from related work, such as VAEs, GANs, and diffusion models, while also acknowledging limitations and areas for future exploration.
Suggestions for Improvement
1. Clarity on Theoretical Guarantees: While the paper acknowledges that the denoising-based infusion training lacks theoretical guarantees, it would be helpful to include a more detailed discussion of the implications of this limitation. For instance, how does this affect the robustness or generalizability of the model?
2. Comparison with GANs: Although the paper briefly compares its results to GANs, a more detailed analysis would strengthen the argument. For example, how does the model perform in terms of mode collapse, training stability, or computational efficiency compared to GANs?
3. Ablation Studies: The paper could benefit from additional ablation studies to better understand the impact of key design choices, such as the infusion rate schedule or the number of denoising steps. This would provide deeper insights into the method's behavior and potential trade-offs.
4. Evaluation Metrics: While the paper uses a variety of evaluation metrics, the reliance on Parzen-window estimates (known to be unreliable) could be mitigated by incorporating more robust metrics, such as Fr√©chet Inception Distance (FID), especially for natural image datasets.
Questions for the Authors
1. How sensitive is the model to the choice of the infusion rate schedule? Would a more adaptive schedule improve performance?
2. Can the infusion training procedure be extended to conditional generative tasks, such as text-to-image generation or image segmentation? If so, what modifications would be required?
3. How does the computational cost of the proposed method compare to GANs or diffusion models, particularly in terms of training time and memory requirements?
Conclusion
This paper makes a significant contribution to the field of generative modeling by introducing a novel and effective training procedure. While there are areas for further exploration, the proposed approach is well-motivated, empirically validated, and positioned to inspire future research. I recommend acceptance.