Review of the Paper
Summary of Contributions
The paper proposes a novel approach for structure discovery in undirected graphical models, particularly Gaussian Graphical Models (GGMs), by framing the problem as a learning task. Instead of relying on manually designed estimators like the graphical lasso, the authors introduce a neural network-based method that learns to map empirical covariance matrices to graph structures. This approach is trained on synthetic data and generalizes well to real-world problems in genetics, neuroimaging, and simulation data. The method is computationally efficient, competitive with state-of-the-art techniques, and adaptable to various structural priors (e.g., sparsity, small-world networks). The authors also demonstrate the scalability of their method to high-dimensional settings (p > n) and its robustness to variations in data distributions.
Decision: Accept
The paper presents a well-motivated and innovative approach to a challenging problem, with strong experimental results and significant practical implications. The key reasons for acceptance are:
1. Novelty and Impact: The idea of learning a graph structure estimator using neural networks is novel and has the potential to influence future research in structure discovery.
2. Empirical Rigor: The method is thoroughly evaluated on synthetic and real-world datasets, demonstrating superior performance and computational efficiency compared to established methods like graphical lasso and MCMC-based approaches.
Supporting Arguments
1. Problem Relevance and Motivation: The paper addresses a critical challenge in probabilistic graphical models: structure discovery from limited data. The authors highlight the limitations of existing methods, such as the difficulty of incorporating domain-specific priors and the computational inefficiency of optimization-based approaches. The motivation for a learnable estimator is well-justified.
2. Methodological Soundness: The proposed framework is grounded in empirical risk minimization, and the use of convolutional neural networks (CNNs) is well-motivated by the compositional structure of the problem. The authors provide theoretical insights into the consistency of their method and justify their architectural choices (e.g., dilation for receptive field growth).
3. Experimental Validation: The method is extensively tested on synthetic data, including small-world networks and non-Gaussian distributions, as well as real-world datasets in genetics and neuroimaging. The results consistently show that the proposed method outperforms baselines in terms of accuracy, stability, and computational speed.
4. Practical Utility: The method's ability to generalize across domains and its computational efficiency make it highly practical for real-world applications.
Suggestions for Improvement
1. Clarity of Presentation: While the paper is comprehensive, certain sections (e.g., the neural network architecture and training procedure) are dense and could benefit from additional visual aids or simplified explanations. For example, a more detailed diagram of the D-Net architecture would help readers unfamiliar with convolutional networks.
2. Ablation Studies: It would be helpful to include ablation studies to isolate the impact of key design choices, such as the use of dilation in the CNN or the ensembling approach via input permutations.
3. Comparison with More Baselines: The paper primarily compares against graphical lasso and MCMC-based methods. Including additional baselines, such as recent advancements in Bayesian structure learning or other deep learning-based approaches, would strengthen the evaluation.
4. Theoretical Guarantees: While the authors provide some theoretical insights, a more formal analysis of the method's generalization bounds or its behavior under different priors would enhance the paper's rigor.
Questions for the Authors
1. How sensitive is the method to the choice of hyperparameters, such as the number of convolutional layers or the dilation sequence? Could the authors provide guidance on selecting these parameters for new datasets?
2. The paper mentions the potential for transfer learning to scale the method to larger graphs. Have the authors conducted preliminary experiments to validate this claim?
3. How does the method handle cases where the true graph structure deviates significantly from the training priors (e.g., highly dense graphs)?
4. Could the authors elaborate on the computational complexity of the proposed method compared to graphical lasso and MCMC-based methods, particularly for very large graphs?
In conclusion, the paper makes a significant contribution to the field of structure discovery in graphical models. Addressing the above suggestions would further enhance its clarity and impact.