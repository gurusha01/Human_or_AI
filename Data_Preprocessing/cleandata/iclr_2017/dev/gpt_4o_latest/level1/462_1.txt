Review
Summary of Contributions
This paper addresses the critical problem of adversarial perturbations in deep learning systems, which pose a significant challenge to their deployment in safety-critical applications. The authors propose augmenting deep neural networks with a small "detector" subnetwork that can distinguish genuine inputs from adversarial examples. This approach is orthogonal to existing methods that focus on hardening the classifier itself. The paper demonstrates that adversarial perturbations, though imperceptible to humans, can be detected with high accuracy using this method. Furthermore, the detector generalizes well to weaker and similar adversaries, and the authors introduce a novel training procedure to counteract a dynamic adversary capable of fooling both the classifier and the detector. Empirical results on CIFAR10 and a subset of ImageNet validate the effectiveness of the approach, showing high detection rates and robustness against various adversarial attacks.
Decision: Accept
The paper makes a compelling contribution to the field of adversarial robustness by introducing a novel detection mechanism that complements existing defenses. The key reasons for acceptance are:
1. Novelty and Practical Relevance: The proposed detector subnetwork is a well-motivated and innovative solution to the problem of adversarial attacks, with practical implications for safety-critical systems.
2. Empirical Rigor: The experimental results are thorough, demonstrating the effectiveness of the method across datasets, adversarial attack types, and scenarios involving both static and dynamic adversaries.
Supporting Arguments
1. Well-Motivated Approach: The paper is grounded in literature, addressing gaps in existing methods that primarily focus on classifier robustness. The authors provide a clear rationale for why detecting adversarial perturbations is feasible and valuable.
2. Strong Empirical Results: The experiments convincingly show that the detector achieves high accuracy in identifying adversarial examples, even for dynamic adversaries. The generalization to weaker adversaries and transferability across attack types further strengthens the case for the proposed method.
3. Scientific Rigor: The methodology is well-documented, with clear descriptions of the adversarial attacks, detector architecture, and training procedures. The results are presented comprehensively, with appropriate baselines and ablation studies.
Suggestions for Improvement
1. Clarify Detector Design Choices: While the detector architecture is described, the rationale behind specific design choices (e.g., attachment points, number of layers) could be elaborated. This would help readers understand the trade-offs and potentially replicate the results.
2. Broader Adversary Evaluation: The paper focuses on a subset of adversarial attacks. Including evaluations against more recent or diverse attack methods would strengthen the findings and demonstrate broader applicability.
3. Computational Overhead: The paper does not discuss the computational cost of training and deploying the detector. A comparison of the overhead introduced by the detector relative to the baseline classifier would be valuable for practical adoption.
4. Real-World Scenarios: While the results are promising, the paper could benefit from a discussion on how the detector would perform in real-world, large-scale systems where adversarial attacks might be more complex and varied.
Questions for the Authors
1. How does the detector perform when adversarial examples are generated using more recent attacks, such as AutoAttack or adversarial patch methods?
2. What are the computational trade-offs of deploying the detector in real-time systems? Can the detector operate efficiently in latency-sensitive applications?
3. Could the proposed method be extended to detect adversarial examples in domains beyond computer vision, such as natural language processing or speech recognition?
In conclusion, this paper makes a significant contribution to the field of adversarial robustness and is well-suited for acceptance. Addressing the suggested improvements and questions could further enhance its impact.