The paper presents a novel framework for training AI agents in the first-person shooter (FPS) game Doom, combining the Asynchronous Advantage Actor-Critic (A3C) reinforcement learning model with curriculum learning. The authors claim their approach is simple, does not rely on opponent-specific information, and achieves state-of-the-art performance, winning the Track 1 competition at the ViZDoom AI Competition 2016 by a significant margin. The paper also introduces adaptive curriculum training and post-training rules to enhance performance and stability.
Decision: Accept
The paper is well-motivated, demonstrates strong empirical results, and contributes meaningfully to the field of reinforcement learning in partially observable 3D environments. The key reasons for acceptance are: (1) the effective integration of curriculum learning with A3C to address sparse rewards and adversarial settings, and (2) the demonstrated superiority of the proposed method in a competitive benchmark, achieving a 35% higher score than the runner-up.
Supporting Arguments:
1. Problem and Motivation: The paper addresses the challenging problem of training AI agents in partially observable, adversarial 3D environments, where sparse rewards and long-term dependencies make direct application of reinforcement learning difficult. The use of curriculum learning to progressively increase task difficulty is well-motivated and aligns with prior literature.
   
2. Scientific Rigor: The authors provide detailed descriptions of their methodology, including network architecture, training pipeline, and reward shaping techniques. The results are rigorously evaluated through ablation studies, internal tournaments, and competition performance, supporting the claims of the paper. The adaptive curriculum approach and post-training rules further enhance the robustness of the model.
3. Empirical Results: The proposed framework achieves state-of-the-art performance, winning the ViZDoom AI Competition Track 1 by a large margin. The ablation studies convincingly demonstrate the importance of curriculum learning, adaptive training, and post-training rules in achieving these results.
Suggestions for Improvement:
1. Clarify Adaptive Curriculum Details: While the adaptive curriculum is a key innovation, the paper could provide more details on how the probability distribution shifts dynamically based on performance. Including a visualization or algorithmic pseudocode would improve clarity.
   
2. Comparison with Track 2: The paper focuses on Track 1 but does not discuss why the proposed method was not applied to Track 2 (unknown maps). A discussion of the challenges and potential adaptations for Track 2 would strengthen the paper.
3. Generalization to Other Games: While the paper claims the framework is suitable for closed-source games, it would benefit from a discussion or preliminary experiments on other FPS games to demonstrate generalizability.
4. Explain Computational Costs: The training process involves 255 parallel processes and multiple GPUs, which may not be feasible for all researchers. A discussion of computational efficiency or potential optimizations would be helpful.
Questions for the Authors:
1. How does the adaptive curriculum handle scenarios where the agent's performance fluctuates significantly? Is there a mechanism to prevent oscillations in task difficulty?
2. Could the proposed framework be extended to handle unknown maps (as in Track 2)? If not, what modifications would be necessary?
3. How does the model's performance compare to human players over a larger sample size, beyond the brief human-AI match mentioned?
In conclusion, the paper makes a significant contribution to reinforcement learning in complex 3D environments and demonstrates state-of-the-art performance in a competitive setting. With minor clarifications and additional discussion, the paper would be even stronger.