Review of the Paper
The paper introduces the Generative Paragraph Vector (GPV) and its supervised extension, the Supervised Generative Paragraph Vector (SGPV), as novel approaches for learning distributed text representations. The authors address a critical limitation of the Paragraph Vector (PV) model—its inability to infer representations for unseen texts—by proposing a probabilistic generative framework. The GPV model incorporates a prior distribution over paragraph vectors, enabling inference for new texts, while the SGPV model integrates label information to guide representation learning for prediction tasks. The paper demonstrates the effectiveness of these models through experiments on five text classification benchmarks, showing competitive or superior performance compared to state-of-the-art methods, including deep learning models.
Decision: Accept
The paper is well-motivated, introduces a meaningful contribution to the field of text representation learning, and provides strong empirical evidence to support its claims. The key reasons for acceptance are: (1) the novel extension of PV to a generative framework, which addresses a significant limitation of the original model, and (2) the competitive performance of the proposed models, particularly the simplicity and efficiency of SGPV compared to complex deep learning architectures.
Supporting Arguments
1. Problem Tackled: The paper identifies a clear and important problem—the lack of generalization to unseen texts in PV—and proposes a well-justified solution through a probabilistic generative process. This aligns well with prior work in the field, such as the evolution of LDA from PLSI.
2. Scientific Rigor: The authors provide a detailed mathematical formulation of GPV and SGPV, including their generative processes, optimization methods, and inference mechanisms. The experimental results are robust, with evaluations on diverse datasets and comparisons to strong baselines.
3. Empirical Results: The models achieve state-of-the-art or competitive performance on multiple benchmarks. The inclusion of SGPV-bigram demonstrates the importance of capturing word order, further validating the approach.
Suggestions for Improvement
1. Clarity in Model Explanation: While the mathematical formulation is thorough, the paper could benefit from additional intuitive explanations or visualizations to make the generative process more accessible to a broader audience.
2. Comparison with Deep Models: Although the paper highlights the simplicity of SGPV compared to deep models, it would be helpful to include computational efficiency metrics (e.g., training time, parameter count) to substantiate this claim.
3. Ablation Studies: An ablation study analyzing the impact of key components (e.g., the prior distribution, the inclusion of n-grams) would strengthen the paper's findings and provide insights into the model's design choices.
4. Future Work: The authors briefly mention exploring alternative probabilistic distributions for paragraph vectors. Expanding on this direction with preliminary experiments or theoretical insights would enhance the paper's forward-looking contributions.
Questions for the Authors
1. How sensitive are the models to hyperparameters such as the embedding dimensionality, the choice of prior distribution, or the number of negative samples? Did you observe any trade-offs between performance and computational cost?
2. Could the proposed models be extended to multilingual or cross-lingual text representation tasks? If so, what challenges might arise?
3. How does the inference time for unseen texts in GPV/SGPV compare to other methods like PV or deep models? This would provide a clearer picture of the practical utility of the proposed approach.
In conclusion, the paper makes a significant contribution to the field of text representation learning by addressing a critical limitation of PV and demonstrating the effectiveness of the proposed models. With minor improvements in clarity and additional analyses, the paper could have even broader impact.