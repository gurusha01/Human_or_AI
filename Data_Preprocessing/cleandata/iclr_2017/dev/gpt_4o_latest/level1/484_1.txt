Review of the Paper
Summary of Contributions  
The paper presents a theoretical and empirical investigation into the inductive bias of convolutional networks, particularly focusing on their ability to model correlations among input regions. The authors introduce the concept of separation rank as a measure of correlation strength and analyze convolutional arithmetic circuits to show that deep networks can achieve exponentially high separation ranks for certain input partitions, while shallow networks are limited to linear separation ranks. The paper also demonstrates that pooling geometry plays a critical role in determining which input partitions are favored, thereby controlling the inductive bias. Empirical experiments validate the theoretical findings, showing how different pooling geometries perform better on tasks with specific correlation structures. The work provides insights into the utility of depth in convolutional networks and offers a framework for tailoring inductive bias to specific tasks.
Decision: Accept  
The paper is recommended for acceptance due to its significant theoretical contributions, rigorous analysis, and practical insights into the inductive bias of convolutional networks. The key reasons for this decision are:  
1. The introduction of separation rank as a novel and meaningful measure of correlation modeling in convolutional networks.  
2. The theoretical depth and clarity in connecting pooling geometry to inductive bias, which has practical implications for network design.  
Supporting Arguments  
1. Well-Motivated Problem: The paper addresses a fundamental gap in understanding the inductive bias of convolutional networks, particularly the role of pooling geometry and depth. This is a timely and important contribution to the field.  
2. Theoretical Rigor: The analysis is mathematically rigorous, with clear proofs and derivations. The connection between separation rank and the L2 distance from separable functions is particularly compelling.  
3. Empirical Validation: The experiments are well-designed and validate the theoretical claims, demonstrating the practical relevance of the findings. The use of synthetic tasks with controlled correlation structures is a strength.  
4. Broader Impact: The findings have implications for designing convolutional networks tailored to specific tasks, particularly in domains where natural image statistics do not apply.  
Suggestions for Improvement  
1. Clarity in Definitions: While the paper is mathematically rigorous, some definitions (e.g., separation rank) could be presented more intuitively for a broader audience. A visual illustration of separation rank and its implications would be helpful.  
2. Connection to Practical Architectures: The paper primarily focuses on convolutional arithmetic circuits. While the experiments include convolutional rectifier networks, a deeper discussion of how the findings generalize to widely used architectures (e.g., ResNets, Transformers) would strengthen the paper.  
3. Pooling Geometry Design: The paper suggests that pooling geometry can be tailored to specific tasks but does not provide a systematic method for designing such geometries. Future work could explore automated methods for optimizing pooling geometry based on task requirements.  
4. Empirical Complexity: The experiments could include a broader range of tasks and datasets to further validate the generality of the findings. For example, real-world datasets with known correlation structures could be used.  
Questions for the Authors  
1. How sensitive are the theoretical results to the choice of representation functions (fÎ¸d)? Would the results hold for non-linear activations like ReLU or sigmoid?  
2. Can the concept of separation rank be extended to other architectures, such as attention-based models, where correlations are not localized?  
3. The experiments focus on synthetic tasks. How do the findings translate to real-world datasets, particularly those with non-natural image statistics?  
4. Is there a way to quantify the trade-off between pooling geometry complexity and computational efficiency?  
Overall, the paper makes a significant contribution to the understanding of convolutional networks and their inductive biases. With minor improvements in clarity and broader applicability, it has the potential to influence both theoretical research and practical applications in deep learning.