The paper introduces COCONET, a convolutional neural network in the NADE family of generative models, and explores its application to polyphonic music generation using a blocked Gibbs sampling approach. The authors argue that this method, inspired by the nonlinear compositional process of human composers, improves sample quality compared to the traditional NADE ancestral sampling procedure. They provide evidence that the improvement stems from addressing poorly modeled conditional distributions and demonstrate the versatility of their approach through unconditioned polyphonic music generation tasks. The paper positions its contributions as a novel combination of blocked Gibbs sampling with convolutional architectures for music generation, supported by qualitative and quantitative results.
Decision: Accept
The paper is well-motivated, presents a novel and significant improvement over existing methods, and provides sufficient evidence to support its claims. However, some areas could benefit from additional clarification and further exploration.
Supporting Arguments:
1. Novelty and Motivation: The use of blocked Gibbs sampling as an analogue to human nonlinear composition is a creative and compelling idea. The paper effectively situates its work within the literature, referencing both the NADE family of models and prior work on approximate Gibbs sampling.
2. Experimental Evidence: The authors provide qualitative results (e.g., inpainting of Bach chorales) and quantitative evidence to demonstrate the superiority of blocked Gibbs sampling over ancestral sampling. The visualizations in Figure 1 are particularly helpful in illustrating the model's performance.
3. Practical Usefulness: The proposed method has clear applications in music generation and could be of interest to both researchers and practitioners in generative modeling and computational creativity.
Additional Feedback:
1. Reproducibility: While the paper provides a solid overview of the method, it would benefit from more details on the experimental setup, including hyperparameters, training data, and evaluation metrics. This would enhance reproducibility.
2. Limitations: The paper does not explicitly discuss the limitations of the approach. For example, how does the model handle more complex musical structures or genres beyond Bach chorales? Addressing these questions would strengthen the paper.
3. Comparison with Baselines: While the paper compares blocked Gibbs sampling to ancestral sampling, it would be helpful to see comparisons with other state-of-the-art music generation models to contextualize the performance gains.
4. Subjective Evaluation: Music generation often benefits from human evaluation. Including a user study or expert feedback on the quality of the generated music would provide additional support for the claims.
Questions for the Authors:
1. How does the model perform on more diverse datasets or genres beyond Bach chorales? Are there any limitations in its generalizability?
2. Could you provide more details on the computational cost of blocked Gibbs sampling compared to ancestral sampling? Is the improvement in quality worth the additional cost?
3. Have you explored conditioning the model on specific musical attributes (e.g., style, tempo) to guide the generation process?
In summary, the paper presents a novel and promising approach to music generation that is well-motivated and supported by experimental evidence. With additional details and broader evaluations, it could make a strong contribution to the field.