The paper presents an end-to-end automatic speech recognition (ASR) system that combines a convolutional neural network (ConvNet) acoustic model with a novel Auto Segmentation Criterion (ASG) and a simple beam-search decoder. The authors claim that their approach eliminates the need for force alignment of phonemes, simplifies the architecture compared to existing end-to-end systems, and achieves competitive word error rates (WER) on the LibriSpeech dataset. The system is evaluated using three input feature types: MFCCs, power spectrum, and raw waveform, with MFCCs yielding the best results. The paper also highlights the computational efficiency of ASG compared to the widely used Connectionist Temporal Classification (CTC) criterion.
Decision: Accept  
Key reasons:  
1. The paper introduces a novel segmentation criterion (ASG) that is simpler and computationally more efficient than CTC while achieving comparable accuracy.  
2. The proposed system demonstrates competitive WER on a standard benchmark (LibriSpeech) with a smaller model and less training data compared to state-of-the-art systems.
Supporting Arguments:  
The paper provides strong experimental evidence for its claims. The ASG criterion is shown to match CTC in accuracy while being faster, particularly for longer sequences. The authors also demonstrate that their model achieves a WER of 7.2% on clean speech using MFCC features, which is competitive with existing methods despite using significantly less training data. The simplicity of the architecture (23 million parameters vs. 100 million in comparable systems) and the elimination of force alignment are notable contributions. Furthermore, the paper provides a detailed comparison of input features, showing the potential of raw waveform features with sufficient training data.
Additional Feedback:  
1. While the ASG criterion is a key contribution, the paper could benefit from a more detailed theoretical comparison with CTC, particularly regarding the implications of removing blank labels and using global normalization.  
2. The decoder implementation is described as simple, but additional details on its scalability and performance in more complex scenarios (e.g., larger vocabularies) would strengthen the paper.  
3. The results for raw waveform features are promising but underexplored. Future work could investigate whether additional data or architectural modifications could close the gap with MFCC-based models.  
4. The paper could include a discussion of potential limitations, such as the reliance on a fixed stride in the ConvNet, which might impact performance on variable-length inputs.
Questions for Authors:  
1. How does the ASG criterion handle cases of highly imbalanced letter distributions in transcriptions?  
2. Could the proposed system be extended to handle larger vocabularies or multilingual datasets?  
3. How does the system perform in real-time scenarios with noisy or accented speech?  
4. What are the trade-offs between using MFCCs, power spectrum, and raw waveform features in terms of training time and model generalization?
Overall, the paper makes a significant contribution to end-to-end ASR by proposing a simpler and efficient alternative to existing methods. The results are compelling, and the approach has the potential for broader applicability with further refinement.