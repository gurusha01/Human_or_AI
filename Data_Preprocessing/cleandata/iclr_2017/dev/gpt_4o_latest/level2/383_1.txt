Review of "MetaQNN: Reinforcement Learning for Neural Architecture Search"
The paper introduces MetaQNN, a reinforcement learning-based algorithm for automating the design of convolutional neural network (CNN) architectures. The authors propose a Q-learning agent that sequentially selects CNN layers to optimize performance on image classification tasks. The key contributions include the novel application of reinforcement learning to architecture search, the use of an -greedy exploration strategy and experience replay to expedite learning, and the demonstration of competitive performance on standard datasets (CIFAR-10, SVHN, MNIST). The paper also highlights the transferability of the discovered architectures to other tasks, showcasing their utility in transfer learning.
Decision: Accept
The paper is well-motivated, presents a novel approach, and demonstrates strong empirical results. The primary reasons for acceptance are:
1. Novelty and Contribution: The use of reinforcement learning for CNN architecture search is innovative and represents a significant advancement over prior meta-modeling approaches.
2. Empirical Validation: The proposed method outperforms existing automated design methods and is competitive with state-of-the-art handcrafted architectures on multiple datasets.
Supporting Arguments
1. Claims and Support: The paper claims that MetaQNN can automatically design high-performing CNN architectures and outperforms existing meta-modeling methods. These claims are supported by rigorous experiments on CIFAR-10, SVHN, and MNIST. The results demonstrate that MetaQNN-designed architectures achieve lower error rates than comparable methods and are competitive with state-of-the-art handcrafted networks.
   
2. Usefulness: The method is practically useful for automating neural architecture design, reducing the reliance on human expertise. The transferability of the discovered architectures further enhances its applicability.
3. Novelty and Field Knowledge: The paper builds on established reinforcement learning techniques (e.g., Q-learning, -greedy strategy, experience replay) and adapts them innovatively for neural architecture search. The related work section is comprehensive, showing a solid understanding of prior methods.
4. Limitations and Completeness: The authors acknowledge limitations, such as the coarse discretization of the state-action space and the fixed hyperparameters during exploration. They also suggest future directions, such as combining MetaQNN with hyperparameter optimization.
Suggestions for Improvement
1. Scalability: While the paper demonstrates strong results, the computational cost (8-10 GPU days per dataset) is significant. The authors could discuss strategies to improve scalability, such as parallelizing the Q-learning process or leveraging more efficient search techniques.
2. Exploration of Larger State Spaces: The paper constrains the state-action space to ensure tractability. Future work could explore methods like Q-function approximation to handle larger, continuous spaces.
3. Comparison with Neural Architecture Search (NAS): While the paper compares MetaQNN with traditional meta-modeling methods, it would be valuable to include comparisons with other recent neural architecture search methods, such as differentiable NAS.
4. Interpretability of Results: The analysis of Q-values and layer selection patterns is insightful. However, further discussion on how these insights can guide manual architecture design would enhance the paper's impact.
Questions for the Authors
1. How does the performance of MetaQNN compare with more recent neural architecture search methods, such as those based on differentiable optimization or evolutionary algorithms?
2. Can the proposed method be extended to incorporate more complex layer types (e.g., residual connections, attention mechanisms)?
3. How sensitive is the method to the choice of hyperparameters (e.g., learning rate, discount factor) during Q-learning?
Conclusion
Overall, the paper presents a novel and well-executed approach to automating CNN architecture design using reinforcement learning. The strong empirical results, combined with the method's flexibility and transferability, make it a valuable contribution to the field. With minor improvements and further exploration of scalability, MetaQNN has the potential to become a foundational method in neural architecture search.