Review of the Paper
This paper investigates sparsity-centric optimization for training LSTM-based RNNs, a topic that has received less attention compared to CNNs. The authors identify a skewed distribution in the gradients of LSTM gates during backward propagation, which they leverage to propose a sparsified stochastic gradient descent (SGD) technique. By applying a static threshold to round small gradient values to zero, the paper demonstrates that more than 80% sparsity can be induced in linear gate gradients without performance degradation. This sparsity reduces over 50% of multiply-accumulate (MAC) operations, which can be exploited by hardware accelerators to improve energy efficiency and training speed.
Decision: Accept
The paper makes a compelling case for acceptance due to its novel contribution to sparsity-centric optimization for LSTM training and its strong experimental validation. The key reasons for this decision are:  
1. Novelty and Relevance: The work addresses a significant gap in the field by extending sparsity-centric techniques, previously successful for CNNs, to LSTM-based RNNs. The proposed sparsified SGD is a simple yet effective approach that demonstrates substantial energy efficiency improvements.  
2. Scientific Rigor: The claims are well-supported by theoretical analysis and extensive empirical results across multiple tasks (language modeling, image captioning, and machine translation), datasets, and network configurations. The experiments convincingly show that the proposed technique maintains performance while achieving high sparsity.
Supporting Arguments
1. Well-Motivated Problem: The paper clearly identifies the lack of sparsity-centric optimization techniques for LSTM training and provides a thorough background on why existing CNN-based methods are not directly applicable. The skewed distribution of gate activations is a novel insight that justifies the proposed approach.  
2. Experimental Validation: The authors conduct a comprehensive set of experiments to validate their method. They demonstrate consistent results across different network topologies and datasets, ensuring the generality of the approach. The correlation analysis between sparsified and original gradients further supports the robustness of the method.  
3. Practical Usefulness: The proposed technique has clear implications for hardware acceleration, making it highly relevant for practitioners aiming to improve the efficiency of LSTM training on GPUs and other accelerators.
Suggestions for Improvement
1. Dynamic Thresholding: While the static thresholding approach is effective, it is suboptimal. The authors acknowledge this limitation and propose dynamic thresholding as future work. A preliminary exploration of dynamic thresholds based on learning rate or gradient norms would strengthen the paper.  
2. Hardware Implementation: Although the paper discusses potential hardware benefits, it lacks a concrete implementation or simulation of the proposed sparsified SGD on hardware accelerators. Including such results would make the claims more tangible.  
3. Impact on Convergence: While the experiments show no significant performance loss, a deeper analysis of how sparsification affects convergence speed and gradient noise would provide additional insights.  
Questions for the Authors
1. How does the sparsified SGD technique interact with other regularization methods, such as dropout or weight decay?  
2. Could the proposed method be extended to other RNN variants, such as GRUs or Transformer architectures?  
3. Have you considered the impact of sparsification on gradient accumulation in distributed training setups?  
Conclusion
This paper presents a well-motivated, novel, and rigorously evaluated contribution to the field of sparsity-centric optimization for LSTM training. While there is room for improvement in dynamic thresholding and hardware validation, the current work is a significant step forward and has the potential to inspire further research in this area. I recommend acceptance.