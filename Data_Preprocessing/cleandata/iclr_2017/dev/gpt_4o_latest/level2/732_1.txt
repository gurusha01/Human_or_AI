Review of the Paper: "Generative Paragraph Vector and Supervised Generative Paragraph Vector"
This paper introduces two novel models, Generative Paragraph Vector (GPV) and Supervised Generative Paragraph Vector (SGPV), as extensions to the widely used Paragraph Vector (PV) model. The authors address a key limitation of PV—its inability to infer distributed representations for unseen texts—by proposing a probabilistic framework with a complete generative process. The supervised variant, SGPV, incorporates text labels to guide representation learning, making it suitable for prediction tasks. The paper demonstrates the effectiveness of these models through experiments on five benchmark text classification datasets, showing superior or comparable performance to state-of-the-art methods.
Decision: Accept
Key Reasons:
1. Novelty and Contribution: The paper presents a significant improvement over the original PV by enabling inference for unseen texts and integrating supervision for prediction tasks. The introduction of a generative framework and the ability to incorporate n-grams for capturing word order information are meaningful contributions.
2. Empirical Validation: The experimental results are robust, demonstrating that GPV and SGPV outperform or match state-of-the-art methods across multiple datasets. The models achieve this while maintaining simplicity and computational efficiency compared to deep learning-based approaches.
Supporting Arguments:
- The paper is well-motivated, addressing a critical limitation of PV and situating the work effectively within the broader literature on text representation methods. The comparison to both traditional (e.g., LDA, TF-IDF) and modern (e.g., CNN, RNN) baselines is comprehensive.
- The generative framework is clearly articulated, with a detailed explanation of the mathematical formulation and optimization process. The use of a multivariate normal distribution as a prior for paragraph vectors is a thoughtful design choice.
- The inclusion of supervised learning (SGPV) and the extension to n-grams (SGPV-bigram) demonstrate the flexibility and scalability of the proposed models.
- The experimental results are convincing, with GPV outperforming unsupervised baselines and SGPV achieving competitive results against deep learning models, particularly on datasets like SST-1 and SST-2.
Additional Feedback:
1. Clarity and Presentation: While the paper is generally well-written, some sections, particularly the mathematical derivations, could benefit from additional clarity or illustrative examples. For instance, the MAP estimation process for paragraph vectors might be challenging for readers unfamiliar with probabilistic modeling.
2. Limitations: The paper does not explicitly discuss potential limitations of the proposed models. For example, the reliance on a fixed prior distribution (multivariate normal) might limit flexibility in certain applications. Exploring alternative priors could be an interesting direction for future work.
3. Comparison with Deep Models: While the simplicity of GPV and SGPV is a strength, it would be helpful to provide a more detailed analysis of the trade-offs between simplicity and performance compared to deep models.
4. Scalability: The paper claims computational efficiency, but no runtime or scalability analysis is provided. Including such details would strengthen the argument for the practicality of the models.
Questions for the Authors:
1. How sensitive are the models to the choice of prior distribution for paragraph vectors? Have you experimented with alternatives to the multivariate normal distribution?
2. Can the proposed models handle very large-scale datasets effectively? If so, could you provide runtime comparisons with other methods?
3. How does the performance of SGPV-bigram vary with the inclusion of higher-order n-grams (e.g., trigrams)? Are there diminishing returns?
In conclusion, this paper makes a strong contribution to the field of text representation learning by addressing a key limitation of PV and demonstrating the effectiveness of a generative approach. With minor improvements in clarity and additional discussion of limitations, this work has the potential to significantly impact both research and practical applications.