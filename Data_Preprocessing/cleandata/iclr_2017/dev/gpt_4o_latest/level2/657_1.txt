Review
This paper addresses the problem of reducing the memory footprint of text classification models while maintaining competitive accuracy. The authors propose a novel approach built upon product quantization (PQ) to compress word embeddings, combined with techniques such as vocabulary pruning, hashing, and retraining. Their method, implemented as an extension of the fastText library, achieves significant memory savings—up to two orders of magnitude—while incurring only minor accuracy losses. Experimental results on multiple benchmarks demonstrate the effectiveness of the proposed method, which outperforms state-of-the-art approaches in terms of the trade-off between memory usage and classification accuracy.
Decision: Accept
Key reasons for this decision include:  
1. Strong empirical support: The paper provides extensive experimental evidence across diverse datasets, demonstrating that the proposed approach achieves substantial memory savings with minimal accuracy degradation.  
2. Practical impact: The proposed techniques are highly relevant for real-world applications, particularly for memory-constrained environments like mobile devices. The authors also commit to releasing code, which will benefit the broader research and engineering communities.
Supporting Arguments  
The paper makes a clear and compelling case for the need to balance memory efficiency and accuracy in text classification. The authors leverage well-established techniques such as PQ and pruning but adapt them innovatively for text classification tasks. For example, their bottom-up retraining strategy for PQ is well-motivated and empirically validated. The inclusion of text-specific tricks, such as pruning based on feature norms and hashing, further enhances the practicality of the approach. The experiments are thorough, covering both small and large datasets, and the results consistently support the claims made in the paper. The comparison with existing methods, including CNN-based classifiers, is fair and highlights the advantages of the proposed method.
Additional Feedback  
1. Clarity of Presentation: While the technical content is strong, some sections (e.g., the explanation of PQ) could benefit from additional clarity and simplification for readers unfamiliar with the technique. Including a high-level diagram of the compression pipeline might help.  
2. Limitations: The paper could more explicitly discuss potential limitations, such as the trade-offs between pruning aggressiveness and coverage, or the scenarios where the method might underperform (e.g., datasets with extremely short text).  
3. Future Work: The proposed directions for future work, such as conditioning vector sizes on frequency and combining entropy and norm-based pruning, are promising. However, it would be helpful to provide more concrete ideas or preliminary results to validate these directions.
Questions for the Authors  
1. How does the proposed method perform on tasks with highly imbalanced datasets or rare classes? Does the pruning strategy affect the ability to classify rare labels?  
2. Have you considered applying these compression techniques to other architectures, such as transformer-based models? If so, what challenges do you anticipate?  
3. Could you provide more details about the computational overhead introduced by retraining after quantization? How does this scale with dataset size?
In summary, this paper makes a significant contribution to the field of efficient text classification. The proposed techniques are innovative, well-supported by experiments, and have clear practical utility. With minor improvements in presentation and discussion of limitations, this paper will be a valuable addition to the conference.