Review of the Paper
This paper presents a novel framework for training AI agents in First-Person Shooter (FPS) games, specifically Doom, by combining the Asynchronous Advantage Actor-Critic (A3C) reinforcement learning model with curriculum learning. The authors claim their approach is simpler than prior methods, as it does not rely on opponent information, and achieves state-of-the-art performance, evidenced by winning Track 1 of the ViZDoom AI Competition 2016 with a 35% higher score than the second-place entry. The paper also introduces adaptive curriculum training, which dynamically adjusts task difficulty, and demonstrates its effectiveness through ablation studies and competitive results.
Decision: Accept
Key reasons for this decision are the paper's strong empirical results and its meaningful contribution to reinforcement learning in partially observable 3D environments. The proposed combination of A3C and curriculum learning is well-motivated and demonstrates significant performance improvements over baseline methods. The work is novel, practically useful, and scientifically rigorous.
Supporting Arguments
1. Claims and Support: The paper's claims are well-supported by extensive experiments, including ablation studies, internal tournaments, and competition results. The use of curriculum learning to address sparse rewards and adversarial environments is particularly compelling, as it demonstrates clear performance gains over pure A3C. The adaptive curriculum training further enhances the agent's robustness, showing thoughtful innovation.
2. Novelty and Usefulness: The framework introduces a simpler yet effective approach to training agents in FPS games without requiring opponent information, making it generalizable to closed-source games. The use of curriculum learning in this domain is novel and addresses a critical challenge in reinforcement learning for complex environments.
3. Scientific Rigor: The paper provides detailed descriptions of the architecture, training pipeline, and evaluation metrics, ensuring reproducibility. The experiments are thorough, and the results are statistically significant, with clear comparisons to prior work.
Additional Feedback
1. Limitations and Future Work: While the paper acknowledges its limitations, such as the reactive nature of the agent and reliance on short-term memory, it could benefit from a more detailed discussion of how these limitations impact performance in unseen scenarios, such as Track 2 of the competition. Suggestions for addressing these issues, such as incorporating long-term memory or map-building capabilities, are promising but underexplored.
2. Clarity and Presentation: The paper is generally well-written, but some sections, such as the explanation of adaptive curriculum learning, could be more concise. Additionally, the inclusion of more visualizations, such as gameplay footage or decision-making heatmaps, would enhance the reader's understanding of the agent's behavior.
3. Broader Impact: The authors could discuss the potential applications of their framework beyond gaming, such as robotics or autonomous navigation, to highlight its broader relevance.
Questions for the Authors
1. How does the agent's performance generalize to unseen maps or scenarios, such as those in Track 2 of the competition? Could the proposed framework be extended to handle such cases?
2. Could you elaborate on the computational efficiency of your approach compared to other methods, particularly in real-time applications?
3. How sensitive is the agent's performance to hyperparameter choices, such as the reward shaping parameters or the number of curriculum levels?
In summary, this paper makes a significant contribution to reinforcement learning in complex 3D environments and is a strong candidate for acceptance. With minor improvements in clarity and additional discussion of limitations, it could further enhance its impact.