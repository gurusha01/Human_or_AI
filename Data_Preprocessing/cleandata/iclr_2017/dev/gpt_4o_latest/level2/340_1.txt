Review of the Paper
The paper presents a novel approach to the problem of unsupervised domain transfer using the proposed Domain Transfer Network (DTN). The authors aim to map samples from a source domain (S) to a target domain (T) while preserving a given perceptual function \( f \). The contributions include a compound loss function combining GAN-based adversarial loss, \( f \)-preservation loss, and a regularization term to ensure identity mapping for samples already in the target domain. The method is applied to visual tasks such as digit transformation (SVHN to MNIST) and photo-to-emoji generation, demonstrating its ability to generate convincing results while preserving identity. The paper also highlights DTN's potential for unsupervised domain adaptation and style transfer.
Decision: Accept
Key reasons for acceptance:
1. Novelty and Contribution: The paper addresses a novel problem formulation in unsupervised domain transfer, which is distinct from related tasks like style transfer or domain adaptation. The proposed DTN framework introduces meaningful innovations, such as the combination of \( f \)-preservation and GAN-based losses, and demonstrates its utility across diverse visual domains.
2. Experimental Validation: The results are compelling, with quantitative and qualitative evaluations supporting the claims. The experiments on SVHN-to-MNIST transfer and photo-to-emoji generation are well-designed and demonstrate the effectiveness of the method.
Supporting Arguments:
1. Strong Theoretical Foundation: The paper builds on established concepts like GANs and \( f \)-preserving mappings but extends them in a novel way. The compound loss function is well-motivated and addresses key challenges in domain transfer, such as ensuring perceptual consistency and identity preservation.
2. Empirical Rigor: The experiments are thorough, including ablation studies to evaluate the contribution of individual loss components. The results show that DTN outperforms baseline methods and achieves state-of-the-art performance in unsupervised domain adaptation.
3. Practical Usefulness: The ability to generate personalized emoji and perform domain adaptation without labeled data has clear practical applications. The method also demonstrates versatility by supporting tasks like style transfer.
Suggestions for Improvement:
1. Clarity in Presentation: While the theoretical framework is robust, some sections, particularly the mathematical formulations, could benefit from additional explanation for accessibility to a broader audience.
2. Comparison with More Baselines: The paper could strengthen its claims by comparing DTN with additional state-of-the-art methods in domain transfer and style transfer tasks.
3. Limitations and Future Work: Although limitations are briefly mentioned, a more detailed discussion of scenarios where DTN might fail (e.g., highly dissimilar domains or limited data in \( T \)) would provide a balanced perspective.
Questions for the Authors:
1. How sensitive is the method to the choice of the perceptual function \( f \)? Would using a different \( f \) (e.g., one trained on a different dataset) significantly affect performance?
2. Can the method generalize to non-visual domains, such as text or audio? If so, what modifications would be required?
3. How does the method handle cases where the target domain \( T \) has significant intra-class variability (e.g., diverse artistic styles in emoji)?
In conclusion, the paper presents a significant contribution to the field of unsupervised domain transfer. While there is room for improvement in presentation and broader comparisons, the novelty, well-motivated approach, and strong experimental results justify acceptance.