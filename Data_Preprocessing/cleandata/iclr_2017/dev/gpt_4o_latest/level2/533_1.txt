Review of the Paper
Summary of Contributions
This paper addresses the critical challenge of exploration in reinforcement learning (RL) environments with sparse rewards. The authors propose a novel intrinsic motivation strategy based on surprise, formulated as the KL-divergence between the true transition probabilities and a learned dynamics model. Two approximations of this surprise—surprisal and k-step learning progress—are derived to make the approach computationally efficient and scalable. The paper demonstrates the efficacy of these intrinsic rewards across a variety of challenging benchmarks, including continuous control tasks and Atari RAM games. The results indicate that the proposed methods outperform existing heuristic exploration strategies and are competitive with state-of-the-art methods like VIME, while being computationally cheaper.
Decision: Accept
The paper makes a significant contribution to the field of reinforcement learning by introducing a scalable and computationally efficient intrinsic motivation mechanism. The proposed methods are empirically validated on diverse and challenging benchmarks, demonstrating their robustness and utility. The work is well-motivated, grounded in existing literature, and presents a clear improvement over prior approaches in terms of both performance and computational cost. These factors justify acceptance.
Supporting Arguments
1. Novelty and Significance: The paper introduces a novel formulation of surprise as intrinsic motivation, distinguishing it from prior work by using a learned dynamics model and avoiding the computational overhead of Bayesian approaches like VIME. The approximations (surprisal and k-step learning progress) are innovative and practical, addressing a key limitation in existing exploration methods.
   
2. Empirical Validation: The proposed methods are rigorously evaluated on a wide range of benchmarks, including sparse reward continuous control tasks and Atari RAM games. The results consistently show that surprisal outperforms naive exploration and other intrinsic motivation baselines, while being competitive with VIME at a fraction of the computational cost.
3. Computational Efficiency: The paper highlights a significant reduction in computational overhead compared to VIME, with a reported speedup factor of 3 in practical settings. This is a crucial contribution for scaling RL methods to real-world applications.
4. Clarity and Reproducibility: The paper is well-written and provides sufficient implementation details, including algorithmic descriptions and hyperparameters, to facilitate reproducibility.
Additional Feedback
1. Discussion of Limitations: While the paper briefly mentions potential limitations (e.g., pathological noise-seeking behavior in surprisal), a more detailed discussion of failure cases and how they might be mitigated would strengthen the work.
2. Comparison with Other Methods: Although the paper compares its methods with VIME and naive exploration, additional comparisons with other recent intrinsic motivation techniques (e.g., pseudo-counts or empowerment) would provide a more comprehensive evaluation.
3. Ablation Studies: The paper could benefit from more extensive ablation studies to isolate the contributions of individual components, such as the choice of dynamics model architecture or the effect of hyperparameters like η.
4. Scalability to Larger Domains: While the proposed methods are computationally efficient, it would be valuable to discuss their scalability to even larger state-action spaces or real-world robotics tasks.
Questions for the Authors
1. How sensitive are the results to the choice of dynamics model architecture? Would more complex models (e.g., ensembles) improve performance?
2. Can the proposed methods handle environments with highly stochastic dynamics, where the learned model may struggle to approximate the true transition probabilities?
3. Have you considered combining surprisal with other intrinsic motivation techniques, such as pseudo-counts, to further enhance exploration?
In conclusion, this paper presents a well-motivated and empirically validated contribution to intrinsic motivation in reinforcement learning. While there are areas for further exploration and improvement, the work is a valuable addition to the field and merits acceptance.