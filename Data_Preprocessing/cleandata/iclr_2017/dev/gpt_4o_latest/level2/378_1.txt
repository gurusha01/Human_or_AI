Review of the Submitted Paper
Summary of Contributions
This paper introduces a novel reinforcement learning (RL) algorithm, Under-Appreciated Reward Exploration (UREX), which addresses the challenge of sparse rewards in high-dimensional spaces. The authors propose a directed exploration strategy that identifies and promotes action sequences underappreciated by the current policy, defined as sequences where the log-probability underestimates the resulting reward. UREX is implemented as a modification to the REINFORCE algorithm, and its effectiveness is demonstrated on algorithmic tasks such as multi-digit addition, sequence reversal, and binary search. Notably, UREX achieves state-of-the-art performance on these tasks, solving multi-digit addition with generalization to sequences of up to 2000 digits, a feat not previously achieved by model-free RL methods. The paper also highlights UREX's robustness to hyper-parameter tuning and its ability to balance exploration and exploitation effectively.
Decision: Accept
The paper is well-motivated, presents a significant contribution to the field of RL, and demonstrates strong empirical results. The key reasons for this decision are:
1. Novelty and Innovation: The proposed UREX algorithm introduces a principled and novel approach to exploration in RL, addressing a critical limitation of existing methods in sparse reward settings.
2. Empirical Rigor: The experimental results convincingly demonstrate UREX's superiority over baseline methods (e.g., entropy-regularized REINFORCE and Q-learning) across a range of challenging tasks. The generalization to longer sequences is particularly impressive.
3. Practical Impact: The algorithm's robustness to hyper-parameters and ease of implementation make it a practical contribution for RL practitioners.
Supporting Arguments
1. Motivation and Placement in Literature: The paper is well-situated within the RL literature, addressing the limitations of undirected exploration strategies like entropy regularization. The authors provide a thorough review of related work and clearly articulate the gap that UREX fills.
2. Theoretical Soundness: The derivation of UREX is grounded in a solid theoretical framework, leveraging importance sampling to approximate optimal policies. The authors also provide insights into the trade-offs between exploitation and exploration.
3. Experimental Validation: The evaluation spans six algorithmic tasks, with UREX consistently outperforming baselines. The robustness to hyper-parameters is convincingly demonstrated through extensive grid searches, and the generalization results highlight the algorithm's ability to learn meaningful policies.
4. Clarity and Reproducibility: The paper is well-written, with clear explanations of the methodology and experimental setup. The use of open-source environments (e.g., OpenAI Gym) and detailed implementation notes enhance reproducibility.
Suggestions for Improvement
1. Analysis of Limitations: While the paper acknowledges that UREX does not achieve logarithmic complexity on the binary search task, a more detailed discussion of this limitation and potential remedies would strengthen the paper.
2. Comparison with More Baselines: Including additional baselines, such as actor-critic methods or intrinsic motivation approaches, would provide a more comprehensive evaluation.
3. Scalability: The paper could explore the scalability of UREX to more complex environments beyond algorithmic tasks, such as robotics or high-dimensional continuous control.
4. Ablation Studies: While the paper demonstrates the effectiveness of UREX, ablation studies isolating the contributions of different components (e.g., importance sampling, removal of entropy regularization) would provide deeper insights.
Questions for the Authors
1. How does UREX perform on tasks with dense rewards? Would the directed exploration strategy still provide benefits, or is its utility limited to sparse reward settings?
2. Could the authors clarify how the importance weights are normalized during training and their impact on the stability of the algorithm?
3. For the binary search task, what specific modifications to the reward signal or policy architecture might enable the agent to achieve logarithmic complexity?
In conclusion, this paper makes a significant contribution to the field of reinforcement learning by addressing a critical challenge in sparse reward settings. The proposed UREX algorithm is both theoretically sound and empirically validated, with strong potential for practical applications. I recommend acceptance with minor revisions to address the suggestions above.