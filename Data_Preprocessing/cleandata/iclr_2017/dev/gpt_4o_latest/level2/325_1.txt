This paper introduces a novel training procedure for generative models, where the transition operator of a Markov chain is learned to progressively denoise unstructured noise into samples matching the target distribution. The key innovation lies in the "infusion training" technique, which biases the training chain toward the target distribution by infusing information from the training examples. The proposed approach demonstrates competitive performance compared to Generative Adversarial Networks (GANs) while requiring fewer denoising steps than related methods like reverse diffusion processes. Experimental results on datasets such as MNIST, CIFAR-10, and CelebA show promising sample quality and diversity, as well as effective inpainting capabilities.
Decision: Accept.  
Key reasons: (1) The paper presents a novel and well-motivated training procedure that advances the state of generative modeling. (2) The experimental results are comprehensive and demonstrate the practical utility of the proposed method.
Supporting Arguments:  
1. Novelty and Contribution: The infusion training procedure is a creative and significant contribution to generative modeling. By progressively infusing target information into the training chain, the method overcomes limitations of prior approaches, such as the slow convergence of reverse diffusion processes. The paper also provides a clear distinction from related methods, such as VAEs and GANs, and highlights its advantages, such as simpler training objectives and the use of a single network.  
2. Experimental Validation: The experiments are thorough, spanning multiple datasets and evaluation metrics, including Parzen window estimates, likelihood bounds, and Inception scores. The results demonstrate that the method produces high-quality samples and outperforms traditional GANs on CIFAR-10 in terms of the Inception score. The inpainting experiments further validate the model's ability to generate coherent and diverse outputs.  
3. Theoretical Insights: The paper provides a detailed explanation of the infusion training process and its relationship to other generative modeling techniques, offering valuable theoretical insights into its design and effectiveness.
Additional Feedback for Improvement:  
1. Clarity of Presentation: While the paper is detailed, some sections, particularly the mathematical formulations in Section 2.3, could benefit from clearer explanations or visual aids to improve accessibility for readers unfamiliar with Markov chains or variational methods.  
2. Comparison with GANs: Although the paper mentions the instability of GAN training, a more detailed discussion of how the proposed method addresses these issues would strengthen the argument. Additionally, comparisons with state-of-the-art GANs trained with similar resources would provide a more robust evaluation.  
3. Limitations and Future Work: The paper briefly mentions limitations, such as the lack of theoretical guarantees for the denoising-based infusion training. A more explicit discussion of potential drawbacks, such as scalability to larger datasets or higher-dimensional data, would provide a balanced perspective.  
Questions for the Authors:  
1. How sensitive is the model's performance to the choice of the infusion rate schedule (e.g., constant vs. increasing)? Could this parameter tuning limit the method's generalizability?  
2. Have you considered extending the infusion training procedure to conditional generative tasks, such as image-to-image translation?  
3. How does the computational cost of the proposed method compare to GANs or VAEs, particularly in terms of training time and memory requirements?  
Overall, this paper makes a meaningful contribution to the field of generative modeling, and its novel approach has the potential to inspire further research. With minor revisions to improve clarity and address limitations, it is well-suited for acceptance at the conference.