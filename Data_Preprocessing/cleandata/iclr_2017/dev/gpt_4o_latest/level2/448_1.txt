The paper presents a theoretical framework for understanding the trainability of deep neural networks through the lens of mean field theory. It identifies key depth scales, ξq and ξc, that govern the propagation of signals and gradients in untrained, randomly initialized networks. The authors demonstrate that networks are trainable when their depth does not exceed these scales, with ξc diverging at the "edge of chaos," enabling arbitrarily deep networks to be trained near criticality. The work also explores the impact of dropout, showing that it destroys the order-to-chaos critical point, thereby limiting trainable depth. The paper provides empirical validation using MNIST and CIFAR10 datasets, linking theoretical predictions to practical hyperparameter constraints.
Decision: Accept
Key Reasons for Decision:
1. Novelty and Contribution: The paper provides a significant theoretical advancement by formalizing the relationship between network depth, initialization, and trainability. The introduction of depth scales and their connection to criticality is both novel and impactful.
2. Empirical Validation: The theoretical claims are rigorously tested through experiments, demonstrating strong alignment between predictions and observed results.
Supporting Arguments:
- The paper builds on prior work (e.g., Poole et al., 2016) but extends it meaningfully by introducing depth scales and analyzing the effects of dropout. The duality between forward signal propagation and backpropagation of gradients is particularly insightful.
- The experiments are well-designed, confirming the universality of the proposed framework across datasets, training durations, and optimizers. The connection between ξc and trainable depth is convincingly established.
- The analysis of dropout is a valuable addition, as it highlights practical limitations and provides theoretical grounding for its effects on trainability.
Additional Feedback:
- Clarity: While the theoretical derivations are thorough, the paper could benefit from a more intuitive explanation of key equations (e.g., eqs. 3, 4, 7, 9) for readers less familiar with mean field theory.
- Practical Implications: The discussion section mentions potential pre-training schemes and connections to Gaussian Processes, but these ideas could be expanded to provide actionable insights for practitioners.
- Limitations: The authors acknowledge that the framework does not apply to unbounded activations like ReLU or structured architectures like convolutional networks. Explicitly discussing how these limitations might be addressed in future work would strengthen the paper.
Questions for Authors:
1. How sensitive are the results to the choice of activation function? Would similar depth scales emerge for other bounded nonlinearities?
2. The experiments focus on fully connected networks. Do you anticipate significant deviations in ξc for convolutional or transformer-based architectures?
3. Could the framework be extended to analyze the effects of other regularization techniques, such as batch normalization or weight decay?
Overall, this paper makes a strong theoretical and empirical contribution to understanding the trainability of deep networks. Its insights are likely to influence both research and practice in neural network design.