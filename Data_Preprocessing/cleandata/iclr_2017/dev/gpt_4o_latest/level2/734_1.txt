The paper introduces Deep Variational Canonical Correlation Analysis (VCCA), a novel deep generative model for multi-view representation learning that extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by deep neural networks (DNNs). The authors also propose a variant, VCCA-private, which disentangles shared and private information across views. The key contributions include deriving a variational lower bound for the data likelihood using a DNN-parameterized posterior, leveraging Monte Carlo sampling for approximation, and demonstrating the model's ability to generate high-quality samples and disentangle shared/private variables without hard supervision.
Decision: Accept
The paper is well-motivated, presents a significant innovation over existing methods, and provides strong empirical evidence supporting its claims. The primary reasons for acceptance are:
1. Novelty and Contribution: The extension of linear CCA to a deep generative framework with variational inference is a meaningful advancement. The introduction of VCCA-private to disentangle shared and private variables is particularly innovative.
2. Empirical Validation: The paper demonstrates competitive or superior performance on diverse tasks (e.g., noisy MNIST, XRMB speech-articulation, and MIR-Flickr datasets) compared to state-of-the-art methods, while also enabling sample generation, which many prior methods cannot achieve.
Supporting Arguments:
1. Claims and Support: The paper's claims are well-supported by theoretical derivations and rigorous experiments. The use of variational inference and Monte Carlo sampling is clearly explained, and the results are statistically significant across multiple datasets.
2. Usefulness: The proposed models are practically useful for multi-view learning tasks, particularly in scenarios requiring disentanglement of shared and private variables or generative capabilities.
3. Novelty and Placement in Literature: The work builds on and extends prior methods like DCCA and DCCAE, addressing their limitations (e.g., lack of generative modeling and difficulty in optimization). The references are comprehensive, and the authors demonstrate a strong understanding of the field.
4. Completeness: The paper provides sufficient details for reproducibility, including model architectures, training procedures, and hyperparameter tuning.
Suggestions for Improvement:
1. Clarity in Presentation: While the theoretical derivations are rigorous, some sections (e.g., the variational lower bound derivation) could benefit from additional visual aids or simplified explanations for accessibility to a broader audience.
2. Ablation Studies: While the experiments are thorough, an ablation study isolating the contributions of key components (e.g., the KL divergence term, private variables in VCCA-private) would strengthen the empirical analysis.
3. Limitations: The paper does not explicitly discuss potential limitations, such as scalability to extremely high-dimensional data or sensitivity to hyperparameter tuning. Including this discussion would provide a more balanced perspective.
Questions for Authors:
1. How sensitive is the performance of VCCA and VCCA-private to the choice of prior distributions (e.g., Gaussian)? Have alternative priors been explored?
2. Can the proposed models handle more than two views? If so, how would the architecture and training procedure generalize?
3. How does the computational cost of VCCA compare to simpler methods like DCCA or DCCAE, particularly for large datasets?
In conclusion, the paper makes a significant contribution to the field of multi-view representation learning, and its strengths outweigh its minor shortcomings. I recommend acceptance with minor revisions to improve clarity and address the above questions.