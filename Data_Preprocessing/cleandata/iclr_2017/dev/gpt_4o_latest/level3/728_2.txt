Review of the Paper
Summary of Contributions
This paper introduces the Semi-Aggregated Markov Decision Process (SAMDP), a novel approach to analyzing trained reinforcement learning (RL) policies by creating spatiotemporal abstractions. SAMDP combines the strengths of Semi-Markov Decision Processes (SMDPs) and Aggregated MDPs (AMDPs), aiming to simplify the analysis of complex policies trained on large state spaces. The authors propose a five-stage process for constructing SAMDPs, including feature selection, spatiotemporal clustering, skill identification, inference, and model selection. The approach is demonstrated on both a toy gridworld problem and Atari2600 games, where SAMDP is used to monitor and improve policy robustness. The paper also presents an "eject mechanism" for shared autonomy systems, enabling intervention when a policy's performance is predicted to deteriorate.
Decision: Reject
While the paper introduces an interesting concept, it suffers from significant issues in formalism, theoretical rigor, and experimental evaluation. These shortcomings undermine the potential impact of the proposed SAMDP model.
Supporting Arguments for Rejection
1. Unclear Formalism: The SAMDP model is not rigorously defined. Key concepts such as spatiotemporal clustering and skill identification rely heavily on examples rather than precise mathematical definitions. This lack of clarity makes it difficult to evaluate the novelty and correctness of the approach.
   
2. Insufficient Theoretical Rigor: The paper does not provide a formal analysis of the SAMDP model's properties, such as its consistency, convergence, or generalizability. The reliance on heuristic methods (e.g., modified K-means clustering) without theoretical justification weakens the scientific foundation of the work.
3. Outdated Experimental Domains: The experiments primarily focus on 2D gridworlds and Atari2600 games, which are considered simplistic and outdated benchmarks in modern RL research. The lack of evaluation on more realistic and challenging domains limits the paper's relevance and applicability.
4. Empirical Validation: While the authors provide qualitative insights and some quantitative metrics (e.g., VMSE), the results are not compelling. The evaluation lacks comparisons to baseline methods or alternative modeling approaches, making it unclear whether SAMDP offers any significant advantage.
Suggestions for Improvement
1. Formal Definitions: Provide precise mathematical definitions of the SAMDP model, including the clustering process, skill identification, and transition dynamics. This would improve the clarity and rigor of the paper.
   
2. Theoretical Analysis: Include a formal analysis of SAMDP's properties, such as its ability to approximate the underlying MDP, its scalability, and its robustness to noise in the data.
3. Modern Benchmarks: Replace the gridworld and Atari2600 experiments with evaluations on more realistic domains, such as continuous control tasks (e.g., MuJoCo) or real-world robotics applications. This would demonstrate the practical utility of SAMDP.
4. Baseline Comparisons: Compare SAMDP against existing state abstraction and temporal abstraction methods to highlight its advantages and limitations.
5. Clustering Methodology: Explore more advanced clustering techniques, such as spectral clustering or graph-based methods, and justify their use theoretically and empirically.
Questions for the Authors
1. How does SAMDP handle stochastic environments or policies? The paper assumes deterministic transitions and policies, which may not generalize to more realistic settings.
2. How sensitive is the SAMDP model to the choice of clustering parameters (e.g., number of clusters, window size)? Have you conducted an ablation study to evaluate this?
3. Can SAMDP be extended to continuous action spaces, and if so, how would the clustering and skill identification processes change?
4. Why were Atari2600 games chosen as benchmarks, given their limited relevance to modern RL challenges? Would SAMDP perform well on more complex domains?
In summary, while the idea of SAMDP is intriguing, the paper requires significant improvements in formalism, theoretical grounding, and experimental evaluation to be considered for acceptance.