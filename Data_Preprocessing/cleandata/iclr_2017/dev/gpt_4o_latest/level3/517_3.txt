Review of the Paper
Summary of the Paper
The paper proposes a novel method for structure discovery in undirected graphical models by combining graphical models and neural networks. Specifically, it introduces a learnable edge-estimation framework that maps empirical covariance matrices to graph structures using convolutional neural networks (CNNs). The authors argue that this approach bypasses the challenges of manually designing priors and optimization algorithms, as required in traditional methods like graphical lasso. The method is trained on synthetic data and evaluated on both synthetic and real-world datasets (e.g., genetics and neuroimaging). The authors claim competitive or superior performance compared to existing methods, with significant improvements in execution speed.
Decision: Reject
The paper presents an interesting idea of leveraging neural networks for graph structure discovery, but it falls short in several key areas. The primary reasons for rejection are: (1) insufficient clarity and organization in the methodology, particularly in Algorithm 1, and (2) lack of convincing evidence that the proposed method provides a significant advantage over existing approaches like graphical lasso.
Supporting Arguments for the Decision
1. Clarity and Organization: 
   - Algorithm 1 is poorly explained. Key steps, such as how the pair \((Yi, \hat{\Sigma}i)\) is constructed from \((Gi, Xi)\), are ambiguous. 
   - The role and definition of \(\Sigma\) and \(\Sigma_i\) are unclear, leading to confusion about their usage in the algorithm.
   - The input data \(X\) (not sampled data) is mentioned but not explicitly integrated into the algorithm, raising questions about its role.
2. Model Selection Concerns:
   - The authors criticize graphical lasso for its reliance on model selection but fail to address the fact that their method also implicitly involves model selection through the sparse prior \(P(G)\) and hyperparameter tuning. This undermines the claimed advantage over existing methods.
3. Receptive Field and Propositions:
   - The definitions of the receptive field in Propositions 2 and 3 are unclear, making it difficult to assess the theoretical validity of the proposed CNN architecture.
4. Empirical Evidence:
   - While the experimental results show promise, the paper does not convincingly demonstrate a significant advantage over graphical lasso or other baselines. For example, the performance improvement in real-world datasets is modest and does not justify the added complexity of the neural network approach.
   - The paper lacks a thorough comparison of computational costs, especially during training, which could offset the claimed speed advantage at inference time.
Suggestions for Improvement
1. Algorithm Clarity:
   - Provide a detailed and step-by-step explanation of Algorithm 1, ensuring that all variables (e.g., \(\Sigma\), \(\Sigmai\)) and processes (e.g., constructing \((Yi, \hat{\Sigma}_i)\)) are clearly defined.
   - Clarify how the input data \(X\) is used in the algorithm.
2. Model Selection:
   - Address the issue of hyperparameter tuning for the sparse prior \(P(G)\) and discuss how it compares to the model selection challenges in graphical lasso.
3. Receptive Field:
   - Clearly define the receptive field and its role in the CNN architecture. Provide intuitive explanations for Propositions 2 and 3 to improve accessibility.
4. Experimental Validation:
   - Include ablation studies to isolate the contributions of different components of the proposed method.
   - Provide a more detailed comparison of computational costs, including training time, to justify the claimed efficiency.
5. Writing and Organization:
   - Improve the overall organization of the paper, particularly in the methods section, to enhance readability and logical flow.
Questions for the Authors
1. How is the pair \((Yi, \hat{\Sigma}i)\) constructed from \((Gi, Xi)\)? Can you provide a detailed explanation or example?
2. What is the role of the input data \(X\) in Algorithm 1, and how does it differ from the sampled data?
3. How does the hyperparameter tuning for the sparse prior \(P(G)\) differ from the model selection challenges in graphical lasso?
4. Can you clarify the definition of the receptive field in Propositions 2 and 3 and its significance in the CNN architecture?
5. How does the computational cost of training the proposed method compare to graphical lasso and other baselines?
In conclusion, while the paper introduces an innovative approach to graph structure discovery, it requires significant improvements in clarity, theoretical justification, and empirical validation to be competitive with existing methods.