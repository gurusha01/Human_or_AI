Review
Summary of Contributions
This paper investigates the use of a Siamese convolutional neural network with a contrastive loss function to learn invariant geometric representations of planar curves under Euclidean and Similarity transformations. The authors propose a data-driven approach to approximate geometric invariants, such as curvature, which are traditionally derived through differential or integral methods. The paper claims that the learned representations are more robust to noise, sampling irregularities, and occlusions compared to classical methods. Additionally, the authors introduce a multi-scale representation paradigm by leveraging the design of negative samples in the contrastive loss framework. This work bridges the fields of numerical differential geometry and deep learning, presenting an interesting application of neural networks for geometric invariant learning.
Decision: Reject  
Key Reasons: (1) Insufficient experimental rigor and validation of design choices, and (2) limited novelty in the proposed approach.
Supporting Arguments
1. Experimental Rigor: While the paper demonstrates promising initial results, the experiments are limited in scope and lack robust statistical validation. For instance, the evaluation is primarily conducted on a small set of toy examples and specific cases, which does not provide strong evidence of generalizability. Quantitative metrics, such as statistical comparisons across a larger dataset of curves, are missing, making it difficult to assess the reliability of the proposed method.
2. Negative Sampling Concerns: The choice of negative samples in the contrastive loss framework is a significant weakness. The network appears to primarily learn to distinguish shapes at different scales rather than capturing intrinsic geometric properties. The lack of discussion on hard negative sampling, a critical component for effective contrastive learning, further undermines the robustness of the approach.
3. Incremental Novelty: While the application of Siamese networks to geometric invariants is novel in its specific context, the approach itself is incremental. Siamese networks and contrastive loss are well-established techniques, and the paper does not introduce significant methodological innovations beyond adapting these tools to planar curve analysis.
Additional Feedback for Improvement
1. Hard Negative Sampling: The authors should explore and discuss strategies for constructing hard negative samples, which are known to improve the performance of contrastive loss-based models. This could help the network focus on learning more nuanced geometric features rather than trivial distinctions.
2. Broader Evaluation: The experimental section should include quantitative evaluations on larger and more diverse datasets, along with statistical metrics (e.g., mean accuracy, standard deviation, and confidence intervals). Additionally, comparisons with state-of-the-art methods for curve analysis would strengthen the claims.
3. Ablation Studies: The paper would benefit from ablation studies to validate the design choices, such as the architecture of the Siamese network, the choice of hyperparameters (e.g., margin in the contrastive loss), and the impact of multi-scale representations.
4. Theoretical Insights: While the paper provides some geometric context, a deeper theoretical analysis of how the learned representations relate to classical invariants (e.g., curvature) would enhance the scientific contribution.
5. Clarity on Multi-Scale Representations: The multi-scale representation section is interesting but underdeveloped. The authors should provide clearer explanations and visualizations of how the learned representations evolve across scales and how this behavior compares to classical scale-space methods.
Questions for the Authors
1. How does the network handle hard-to-distinguish negative samples, such as curves with similar local neighborhoods but different global structures? Have you considered incorporating hard negative mining techniques?
2. Can you provide quantitative metrics (e.g., retrieval accuracy, robustness to noise) across a larger dataset to support the claim that the learned invariants are more robust than classical methods?
3. How does the learned representation generalize to transformations beyond Euclidean and Similarity groups, such as affine or projective transformations?
4. Could you elaborate on the computational efficiency of your approach compared to classical numerical methods for invariant computation?
In conclusion, while the paper presents an interesting application of Siamese networks to geometric invariants, it falls short in experimental rigor and novelty. Addressing the identified weaknesses could significantly improve the quality and impact of the work.