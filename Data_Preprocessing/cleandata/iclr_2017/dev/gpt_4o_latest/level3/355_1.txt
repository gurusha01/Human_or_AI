Review of "Training Vision-Based Agents for First-Person Shooter Game Doom Using Curriculum Learning"
Summary of Contributions
This paper presents a novel framework for training AI agents to play the 3D first-person shooter game Doom using a combination of Asynchronous Advantage Actor-Critic (A3C) reinforcement learning and curriculum learning. The authors propose a progressive training pipeline, starting with simpler tasks and gradually increasing difficulty, to address the challenges of sparse rewards and adversarial environments. They also introduce adaptive curriculum learning, reward shaping, and post-training rules to further enhance performance. The proposed agent, named F1, achieved first place in Track 1 of the ViZDoom AI Competition 2016, outperforming competitors by a significant margin. The paper highlights the use of attention frames and motion features, as well as the development of emergent tactics by the agent. The authors claim their approach is simpler and more generalizable than prior work, as it does not rely on opponent-specific information.
Decision: Accept
The paper is well-written, presents clear contributions, and demonstrates strong empirical results. The key reasons for acceptance are:
1. Practical Impact: The proposed framework is effective, as evidenced by the ViZDoom competition results. The use of curriculum learning and post-training rules provides a practical solution for training agents in complex 3D environments.
2. Novelty for the Domain: While the techniques themselves (e.g., curriculum learning, A3C) are not novel, their application to Doom and the specific problem setting is innovative and demonstrates the potential of combining these methods.
Supporting Arguments
1. Problem Motivation and Placement in Literature: The paper is well-motivated, addressing the challenges of sparse rewards and adversarial settings in 3D environments. It builds on prior work in reinforcement learning and FPS AI, providing a clear comparison to existing methods like Arnold and IntelAct.
2. Scientific Rigor: The results are empirically validated through ablation studies, internal tournaments, and competition performance. The use of curriculum learning is shown to improve stability and convergence, though the evidence for adaptive curriculum being superior to pure A3C is somewhat weak.
3. Practical Contributions: The post-training rules and reward shaping techniques are practical additions that significantly enhance performance, as demonstrated in Table 5.
Additional Feedback for Improvement
1. Clarity of Figure 6: The visualization of the convolutional kernels in Figure 6 is unclear and lacks concrete metrics or higher-resolution images. This diminishes the interpretability of the agent's learned features. The authors should address this in the final version.
2. Overreliance on Domain Knowledge: While the framework is effective, it heavily relies on domain-specific reward shaping and post-training rules. This reduces its alignment with the principles of pure reinforcement learning and may limit generalizability to other games or environments.
3. Adaptive Curriculum Evidence: The claim that adaptive curriculum learning is more stable than pure A3C is not strongly supported by the results. Additional experiments or statistical analysis would strengthen this argument.
4. End-to-End Training: The authors mention that post-training rules could be replaced with end-to-end training involving delta buttons. Exploring this in future work would make the approach more elegant and generalizable.
Questions for Authors
1. Can you provide more quantitative evidence to support the claim that adaptive curriculum learning is more stable than pure A3C? For example, statistical tests or additional metrics.
2. How generalizable is the proposed framework to other 3D games or non-FPS environments? Have you tested it in other domains?
3. Could you elaborate on the limitations of using only the last 4 frames for decision-making? How might incorporating longer-term memory (e.g., LSTMs) improve performance?
In conclusion, this paper offers a strong contribution to training AI agents in 3D adversarial environments, with practical techniques and competitive results. While there are areas for improvement, the overall quality and impact of the work justify its acceptance.