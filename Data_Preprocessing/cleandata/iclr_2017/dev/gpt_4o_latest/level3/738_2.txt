Review of the Paper
Summary of Contributions
This paper investigates the potential for sparsity-centric optimization in LSTM-based RNN training, a topic that has received less attention compared to CNNs. The authors identify a skewed distribution in the activation values of LSTM gates during backward propagation, which leads to sparsity in the gradients. They propose a simple thresholding technique, termed "sparsified SGD," to induce further sparsity by rounding small gradient values to zero. Experimental results demonstrate that this method can achieve over 80% sparsity in linear gate gradients without performance loss, making more than 50% of MAC operations redundant. The authors argue that this redundancy can be exploited by hardware techniques to improve energy efficiency and training speed. The paper also validates the generality of the approach across different datasets, tasks, and network configurations.
Decision: Reject
While the paper presents an interesting observation and a novel technique, it falls short of the rigor and comprehensiveness required for a full conference paper. The main reasons for rejection are the lack of benchmarking on well-known datasets and the absence of detailed analysis of the speedup and energy efficiency impacts. The work is more akin to a technical report than a fully developed research paper.
Supporting Arguments for Decision
1. Novelty and Significance: The observation that sparsity can be induced in LSTM training gradients is novel but not entirely surprising, given prior work on sparsity in neural networks. While the idea of exploiting this sparsity is interesting, the paper does not provide sufficient evidence to demonstrate its broader impact on real-world applications or hardware implementations.
   
2. Experimental Rigor: The experiments lack benchmarking on widely recognized datasets (e.g., Penn Treebank, IMDB, or WMT benchmarks) and fail to report standard performance metrics such as perplexity, BLEU scores, or accuracy. This omission makes it difficult to assess the practical relevance of the proposed method.
3. Hardware Implications: The claim that the sparsified gradients can improve energy efficiency and training speed is speculative. The paper does not include any concrete implementation or simulation of hardware speedups, leaving the central claim unsubstantiated.
4. Scope of Contribution: While the sparsified SGD technique is well-explained, the paper does not explore its impact on other architectures (e.g., GRUs) or tasks beyond the limited examples provided. This limits the generalizability of the findings.
Suggestions for Improvement
1. Benchmarking: Include experiments on widely used datasets and report standard metrics to establish the practical relevance of the method.
2. Hardware Validation: Provide a proof-of-concept hardware implementation or simulation to quantify the claimed energy efficiency and speedup benefits.
3. Broader Analysis: Extend the analysis to other RNN architectures, such as GRUs, and evaluate the method on a wider range of tasks.
4. Dynamic Thresholding: The proposed static thresholding is a good starting point, but exploring adaptive thresholding methods could improve the robustness and applicability of the approach.
5. Comparison with Related Work: Situate the work more clearly within the existing literature on efficient RNN training and sparsity-centric optimization techniques. A discussion of how this method compares to pruning or quantization techniques would strengthen the paper.
Questions for the Authors
1. How does the sparsified SGD method affect training time when implemented on existing hardware (e.g., GPUs)?
2. Can the proposed method be applied to other RNN architectures, such as GRUs, or to transformer-based models?
3. Why were well-known datasets and benchmarks not included in the experiments? Would the results generalize to these datasets?
4. How does the induced sparsity affect the convergence rate of the training process? Are there any trade-offs in terms of stability or robustness?
In summary, while the paper presents an interesting idea, it requires significant additional work to meet the standards of a full conference paper. The authors are encouraged to address the above points to strengthen their contribution.