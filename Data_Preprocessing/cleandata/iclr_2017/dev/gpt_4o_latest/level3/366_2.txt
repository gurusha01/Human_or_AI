Review of "Autoencoded Variational Inference for Topic Models (AVITM)"
Summary of Contributions
This paper introduces a novel neural variational inference method for topic modeling, addressing key challenges in applying Autoencoding Variational Bayes (AEVB) to Latent Dirichlet Allocation (LDA). The authors propose a Dirichlet prior approximation using a Gaussian in the softmax basis, which resolves the difficulty of reparameterizing the Dirichlet distribution. Additionally, the paper tackles the issue of component collapsing in continuous latent variables by employing high-momentum training with batch normalization. The proposed method, AVITM, is computationally efficient, black-box in nature, and capable of generating interpretable topics. The authors further demonstrate the flexibility of AVITM by introducing ProdLDA, a novel topic model that replaces the mixture model in LDA with a product of experts, yielding superior topic coherence. Experimental results on benchmark datasets validate the effectiveness of the proposed methods, showing improvements in topic coherence and inference speed.
Decision: Accept
The paper makes a significant contribution to the field of topic modeling by addressing long-standing challenges in neural variational inference for LDA. The proposed solutions are well-motivated, experimentally validated, and have practical implications for scalable and interpretable topic modeling. The introduction of ProdLDA further highlights the flexibility and utility of AVITM. These contributions, combined with the clarity of the paper, justify acceptance.
Supporting Arguments
1. Problem Tackled: The paper effectively addresses two critical challengesâ€”handling the Dirichlet prior in AEVB and mitigating component collapsing. These are well-known issues in applying neural variational inference to topic models, and the proposed solutions are innovative and impactful.
2. Motivation and Placement in Literature: The work is well-situated in the context of prior research, building on foundational works in variational inference and neural topic models. However, citations to foundational works on neural variational inference and VAEs (e.g., [1], [2]) should be added for completeness.
3. Empirical Validation: The experiments are thorough and scientifically rigorous. The results demonstrate that AVITM achieves comparable perplexity to traditional methods while significantly improving topic coherence and inference speed. The introduction of ProdLDA further strengthens the paper's contributions, as it outperforms LDA in topic coherence with minimal implementation changes.
Additional Feedback for Improvement
1. Citations: Add references to foundational works on neural variational inference and VAEs ([1], [2]) to strengthen the theoretical grounding of the paper.
2. Typo: Correct the sentence describing the Dirichlet prior approximation (Section 3.2) for clarity.
3. Experimental Clarifications:
   - Explain why DMFVI results were not reported after 24 hours of training in Table 2. This omission raises questions about the fairness of the comparison.
   - Provide intuition for the observed discrepancy in Table 3, where baseline models exhibit lower perplexity but worse topic coherence compared to ProdLDA. This could help clarify the trade-offs between perplexity and coherence.
   - Discuss the impact of learning-rate and momentum scheduling on training speed and convergence (Figure 1). This analysis could provide insights into the robustness of the proposed optimization strategy.
4. Analysis of Component Collapsing: Include additional analysis of latent variables to better illustrate how the proposed method mitigates component collapsing.
Questions for Authors
1. Why were DMFVI results omitted for RCV1 after 24 hours of training? Could this be due to implementation inefficiencies, or is it a fundamental limitation of DMFVI?
2. Can you provide more intuition or theoretical justification for why ProdLDA achieves better topic coherence despite higher perplexity in some cases?
3. How sensitive is the model to hyperparameter choices, such as the learning rate, momentum, and dropout? Would these require significant tuning for new datasets?
Overall, the paper is a strong contribution to the field, and addressing the minor issues outlined above would further enhance its clarity and impact.