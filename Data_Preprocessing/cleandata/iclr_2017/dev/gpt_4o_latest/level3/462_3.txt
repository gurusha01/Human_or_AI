Review
Summary of Contributions
This paper proposes a novel approach to detecting adversarial examples by augmenting classification networks with a detector subnetwork. The detector is trained to distinguish between genuine and adversarial inputs, enabling the system to flag adversarial cases for human intervention. The authors further introduce the concept of "dynamic adversaries," which attempt to fool both the classifier and the detector, and propose a training procedure to harden the detector against such adversaries. The empirical results demonstrate that adversarial examples can be detected with high accuracy under certain conditions, and the proposed dynamic detector improves robustness against adversarial attacks, increasing detection rates from 55% to 70%. The paper also explores the generalizability of detectors across adversaries and adversarial strengths, providing insights into the regularities of adversarial perturbations.
Decision: Reject  
While the paper introduces a promising approach to adversarial detection, it falls short in several critical areas that limit its contribution to the field. The primary reasons for rejection are the insufficient reliability of the dynamic detector for robust security and the lack of generalizability across different network architectures and adversarial examples.
Supporting Arguments
1. Limited Detection Reliability: Although the dynamic detector improves detection rates, achieving 70% accuracy is insufficient for real-world safety-critical applications. The paper acknowledges this limitation but does not propose a clear path to achieving the reliability required for robust security.
   
2. Generalization Concerns: The detector's ability to generalize to adversarial examples from different networks is not adequately addressed. The experiments focus primarily on specific architectures (e.g., ResNet for CIFAR10 and VGG16 for ImageNet), leaving open the question of whether the proposed method can scale to diverse architectures or datasets.
3. Model Transferability Definition: The paper's definition of "model transferability" is narrower than expected. The reviewer emphasizes that detecting adversarial examples across all models, not just the trained one, is critical for practical deployment. This broader perspective is not sufficiently explored.
4. Experimental Weaknesses: The paper does not verify whether the constructed adversarial examples actually cause misclassification in all cases, which undermines the validity of the results. Additionally, unclear labeling in figures (e.g., X-axis in Figure 2 and \(\sigma\) in Figure 5) detracts from the clarity of the experimental findings.
Suggestions for Improvement
1. Enhance Detection Reliability: Future work should focus on improving the dynamic detector's accuracy to a level suitable for deployment in safety-critical applications. Exploring ensemble methods or hybrid approaches could be beneficial.
   
2. Broaden Generalization Experiments: The authors should evaluate the detector's performance across a wider range of architectures and datasets to demonstrate its robustness and scalability.
3. Clarify Definitions and Results: The paper should align its definition of "model transferability" with broader community standards and explicitly verify that adversarial examples cause misclassification. Additionally, improve figure labeling for better interpretability.
4. Address Adversarial Diversity: Investigate whether the detector can reliably identify adversarial examples generated by a broader range of attack methods, including those with randomized or less regular perturbations.
Questions for the Authors
1. How does the detector perform on adversarial examples generated from networks with architectures significantly different from those used in training?
2. Can the detector's accuracy be improved beyond 70% for dynamic adversaries, and if so, what methods would you recommend?
3. Have you verified that all adversarial examples used in the experiments cause misclassification in the classifier? If not, how might this affect your results?
4. How does the detector handle adversarial examples crafted with randomized perturbations or using less structured attack strategies?
While the paper presents an interesting approach, addressing the above concerns will be essential to make it a stronger contribution to the field.