Review of the Paper
Summary of Contributions
This paper introduces the Private Aggregation of Teacher Ensembles (PATE) approach, a novel method for achieving differential privacy in deep learning. The method combines teacher models trained on disjoint subsets of sensitive data with a student model trained on aggregated, noisy outputs of the teachers. PATE is notable for its general applicability across various machine learning models, including non-convex architectures like deep neural networks. The paper demonstrates state-of-the-art privacy-utility trade-offs on MNIST and SVHN datasets, leveraging semi-supervised learning with GANs and an improved privacy analysis using the moments accountant. The authors also highlight the intuitive and formal privacy guarantees provided by their approach, making it accessible to both expert and non-expert audiences.
Decision: Accept
The paper makes a significant contribution to the field of differentially-private deep learning by advancing the state of the art in privacy-utility trade-offs and introducing a generalizable framework. However, some issues, such as the lack of theoretical guarantees for learning performance and the need for more robust epsilon reporting, should be addressed in future work.
Supporting Arguments for Decision
1. Advancement of State-of-the-Art: The paper achieves notable improvements in privacy-utility trade-offs compared to prior work, such as Abadi et al. (2016). The results on MNIST and SVHN are compelling, with the student models achieving high accuracy while maintaining strict privacy guarantees.
2. Thoroughness and Clarity: The paper is well-written and provides a comprehensive discussion of related work, situating its contributions effectively within the literature. The use of semi-supervised learning and GANs to reduce privacy loss is innovative and well-executed.
3. Generality: The PATE framework is broadly applicable, as demonstrated by its use with both convolutional neural networks and random forests on diverse datasets (e.g., MNIST, SVHN, UCI Adult, and Diabetes datasets).
Suggestions for Improvement
1. Theoretical Guarantees: While the empirical results are strong, the approach lacks theoretical guarantees for learning performance. Future work should explore theoretical bounds or conditions under which PATE is guaranteed to perform well.
2. Epsilon Reporting: The reported epsilon values are not privately releasable, which undermines meaningful comparisons with related work. The authors should adopt a mechanism to release privacy budgets in a differentially-private manner.
3. Generality Across Data Types: The experiments are limited to image and tabular datasets. Testing the method on other natural data types, such as text or time-series data, would strengthen the claim of generality.
4. Clarity Issues: Specific sections require revision for improved clarity:
   - Rephrase the last paragraph of Section 3.1 to clarify the role of teacher training data.
   - Formalize the trade-off statement in Section 4.1 regarding the number of teachers.
   - Revise the discussion of Figure 3 to ensure consistency and clarity.
5. Related Work: The discussion of related work should include differentially-private semi-supervised learning approaches, such as teacher-learner methods from the 2013 literature.
Questions for the Authors
1. How does the PATE approach generalize to tasks with more complex data types, such as natural language or sequential data?
2. Can the authors provide a differentially-private mechanism for releasing epsilon values to enable fair comparisons with other methods?
3. What are the limitations of the approach in terms of scalability to larger datasets or tasks with more output classes?
In conclusion, while there are areas for improvement, the paper makes a strong contribution to the field and warrants acceptance. The PATE approach is a promising step forward in achieving differential privacy for deep learning models.