Review of the Paper
Summary of Contributions
This paper proposes a novel approach to induce sparsity in the training of LSTM-based RNNs by thresholding small gradient values to zero, termed "sparsified SGD." The authors demonstrate that this method can achieve up to 80% sparsity in the linear gate gradients during backward propagation without significantly affecting model performance. The induced sparsity translates to over 50% redundant multiply-accumulate (MAC) operations, which could be eliminated using specialized hardware, potentially enabling faster and more energy-efficient training. The paper provides extensive experimental validation across multiple applications, including language modeling, image captioning, and machine translation, and shows that the approach generalizes well across different network topologies and datasets.
Decision: Accept
The paper makes a strong case for acceptance due to its novel contribution, practical implications, and rigorous experimental validation. The key reasons for this decision are:
1. Novelty and Practical Relevance: The proposed sparsified SGD is a novel and effective technique for inducing sparsity in LSTM training, addressing a gap in the literature where sparsity-centric optimizations have been primarily focused on CNNs.
2. Strong Empirical Support: The results convincingly demonstrate the efficacy of the method across diverse tasks and configurations, with minimal impact on model performance.
Supporting Arguments
1. The paper addresses a well-defined and important problem: enabling sparsity-centric optimizations for LSTM training, which has been underexplored compared to CNNs. The motivation is clear and grounded in both theoretical insights and practical considerations for hardware acceleration.
2. The proposed thresholding technique is simple yet effective, achieving significant sparsity without compromising performance. The experiments are comprehensive, covering multiple datasets, tasks, and network configurations, which strengthens the generalizability of the findings.
3. The analysis of the skewed distribution of gate activations and its connection to gradient sparsity is insightful and scientifically rigorous. The authors also provide a clear explanation of how the method integrates into the standard SGD framework.
Suggestions for Improvement
While the paper is strong overall, there are areas where it could be improved:
1. Missing Citations: The paper overlooks relevant prior work on low-precision arithmetic for RNNs, such as Hubara et al.'s "Quantized Neural Networks." Including these citations would better situate the work within the broader literature.
2. Dynamic Thresholding: The authors acknowledge that the static thresholding approach is suboptimal. A preliminary exploration of dynamic thresholding, even if limited, would strengthen the contribution and provide a clearer path for future work.
3. Hardware Implications: While the paper discusses the potential for hardware acceleration, it would benefit from a more detailed analysis or simulation of how the proposed sparsity could be exploited in practice, particularly for GPUs or custom accelerators.
Questions for the Authors
1. How does the choice of the threshold parameter interact with other hyperparameters, such as the learning rate or batch size? Could this interaction affect the generalizability of the approach?
2. Have you considered the potential impact of sparsified gradients on convergence speed or stability in more complex tasks, such as those involving attention mechanisms or multi-task learning?
3. Could the proposed method be extended to other RNN variants, such as GRUs, or even non-recurrent architectures like Transformers?
In conclusion, this paper makes a significant and novel contribution to the field of efficient neural network training, and its findings are likely to have practical implications for both researchers and hardware designers. With minor revisions and additional context, the paper would be even stronger.