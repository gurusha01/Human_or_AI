Review of the Paper: Variational Canonical Correlation Analysis (VCCA)
Summary of Contributions
This paper proposes a novel deep generative model, Variational Canonical Correlation Analysis (VCCA), which extends the probabilistic latent variable interpretation of linear CCA to nonlinear settings using deep neural networks (DNNs). By leveraging the Variational Autoencoder (VAE) framework, the authors derive a variational lower bound for the data likelihood and optimize it via stochastic gradient descent. The paper also introduces VCCA-private, a variant designed to disentangle shared and private latent variables in multi-view data. The authors evaluate their models on tasks spanning multiple domains, including image-image, speech-articulation, and image-text, demonstrating competitive performance compared to existing methods. The paper highlights the potential of VCCA to generate high-quality samples and disentangle shared/private information, which is a promising direction for multi-view learning.
Decision: Reject
While the paper introduces a novel combination of multi-view modeling and VAEs, it falls short in several critical areas. The lack of theoretical and empirical rigor, limited connection to canonical correlation analysis (CCA), and suboptimal experimental results compared to the state-of-the-art methods make it unsuitable for acceptance in its current form.
Supporting Arguments for the Decision
1. Insufficient Connection to CCA: Unlike methods like DCCA and DCCAE, which explicitly optimize for canonical correlation, VCCA does not provide a clear theoretical or empirical connection to CCA. This weakens the paper's claim of extending CCA to nonlinear settings.
   
2. Limited Discussion of Related Work: The paper does not adequately discuss prior work on nonlinear multi-view models, especially probabilistic and Bayesian approaches, which are highly relevant. This omission makes it difficult to contextualize the novelty and significance of the proposed method.
3. Practical and Optimization Challenges: The VCCA-private model appears ill-posed, requiring manual tuning of dimensionalities and relying on ad-hoc dropout mechanisms to disentangle private and shared variables. These design choices lack justification and raise concerns about the model's scalability and generalizability.
4. Underwhelming Experimental Results: Neither VCCA nor VCCA-private surpasses state-of-the-art methods like DCCAE in key tasks. Additionally, the approximate posterior is parameterized from only one view, limiting the model's utility for generic multi-view tasks beyond classification.
Suggestions for Improvement
1. Strengthen Connection to CCA: Provide a more explicit theoretical or empirical analysis connecting VCCA to CCA, particularly in terms of optimizing canonical correlation in the latent space.
2. Expand Related Work: Discuss prior work on nonlinear multi-view models, including probabilistic and Bayesian approaches, to better position the proposed method in the literature.
3. Refine Methodology: Address the ad-hoc nature of VCCA-private by providing a principled approach to disentangle shared and private variables. Consider alternatives to manual dimensionality tuning and dropout mechanisms.
4. Improve Experimental Validation: Conduct more comprehensive experiments to demonstrate the practical utility of VCCA. Include ablation studies to isolate the contributions of key components, and compare against a broader range of baselines.
5. Clarify Practical Utility: Highlight specific scenarios where VCCA offers advantages over existing methods, such as its generative capabilities or disentanglement of shared/private variables.
Questions for the Authors
1. How does VCCA optimize for canonical correlation in the latent space, and how does this compare to methods like DCCA/DCCAE?
2. What is the rationale behind parameterizing the approximate posterior from only one view, and how does this limitation affect the model's applicability to multi-view tasks?
3. Can you provide more details on the optimization challenges encountered during training, particularly for VCCA-private?
4. How does the proposed method handle cases where the private variables dominate the shared variables, and how does this affect the quality of the learned representations?
This paper presents an interesting idea but requires significant refinement to address its methodological and empirical limitations. The suggestions provided aim to help the authors strengthen their work for future submissions.