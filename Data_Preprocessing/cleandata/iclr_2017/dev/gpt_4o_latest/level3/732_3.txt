Review
Summary of Contributions
This paper introduces the Generative Paragraph Vector (GPV) and its supervised extension, the Supervised Generative Paragraph Vector (SGPV), as probabilistic extensions of the Distributed Bag of Words version of Paragraph Vector (PV-DBOW). The authors claim that these models address the limitation of PV-DBOW in inferring representations for unseen texts by incorporating a complete generative process. The supervised variant, SGPV, integrates text labels into the model, enabling direct application to prediction tasks. The paper also proposes an extension, SGPV-bigram, to capture word order information. Experimental results are presented on five benchmark datasets, with claims of superior performance compared to state-of-the-art methods.
Decision: Reject  
Key reasons for rejection:
1. Incorrect Claims and Lack of Novelty: The paper incorrectly claims that PV-DBOW cannot infer representations for unseen texts, which is explicitly addressed in the original PV paper. The primary contribution of the generative framing appears to be limited to L2 regularization on embeddings, which is incremental rather than groundbreaking.
2. Weak Empirical Results: The proposed models underperform compared to the original PV on key datasets (e.g., SST-1 and SST-2). This undermines the claim of achieving state-of-the-art results.
Supporting Arguments
1. Motivation and Placement in Literature: While the paper reframes PV-DBOW from a generative perspective, this reframing does not lead to significant theoretical or practical advancements. The generative process primarily introduces regularization, which is a well-known technique. Furthermore, the failure to cite Li et al. (2015) for n-gram-based approaches suggests incomplete engagement with relevant literature.
2. Empirical Validation: The experimental results are poorly presented. Key details, such as the dimensionality of paragraph vectors, are missing, and the tables are poorly formatted. The results themselves do not convincingly support the claims, as the proposed models fail to outperform the original PV on critical benchmarks.
3. Technical Issues: The paper suffers from citation and bibliography formatting issues, which detract from its professionalism and readability. The use of BibTeX is recommended for consistency.
Suggestions for Improvement
1. Clarify Novelty: Clearly articulate the novel contributions of the generative framing beyond regularization. Address the incorrect claim about PV-DBOW's inability to infer unseen paragraph vectors.
2. Engage with Literature: Cite all relevant prior work, including Li et al. (2015), to situate the contributions within the broader context of text representation learning.
3. Improve Experiments: Provide more comprehensive experimental details, such as paragraph vector sizes, and ensure that tables are well-formatted and interpretable. Address the performance gap with PV on SST-1 and SST-2.
4. Bibliography: Use BibTeX or another consistent citation management tool to resolve formatting issues.
Questions for Authors
1. How does the generative framing fundamentally improve the quality of paragraph vectors beyond introducing regularization?  
2. Why are the results on SST-1 and SST-2 worse than the original PV? Could this be due to hyperparameter tuning or other implementation details?  
3. Why was Li et al. (2015) not cited for n-gram-based approaches? How does SGPV-bigram differ from their work?
While the paper introduces an interesting perspective on PV-DBOW, the lack of novelty, weak empirical results, and technical issues prevent it from meeting the standards of this conference. Addressing these concerns could significantly improve the work.