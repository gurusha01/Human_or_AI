Review of the Paper: MetaQNN - Reinforcement Learning for Automatic CNN Architecture Design
Summary of Contributions:
The paper introduces MetaQNN, a reinforcement learning-based approach using Q-learning to automate the design of convolutional neural network (CNN) architectures. The method allows a learning agent to sequentially select CNN layers, optimizing for high performance on image classification tasks. The authors demonstrate that MetaQNN outperforms existing automated network design methods and achieves competitive results against state-of-the-art (SOTA) hand-crafted architectures, even when restricted to standard layer types. The approach is validated on three benchmark datasets (CIFAR-10, SVHN, and MNIST), showing strong results and transfer learning capabilities. The paper also highlights the scalability of the method and its potential for constraint-based network design.
Decision: Accept
Key Reasons for Decision:
1. Novelty and Contribution: The paper addresses a significant problem in deep learning: automating the design of CNN architectures. By leveraging Q-learning, the authors propose a novel approach that is well-motivated and distinct from prior methods such as genetic algorithms and Bayesian optimization.
2. Empirical Validation: The results on CIFAR-10, SVHN, and MNIST demonstrate the effectiveness of MetaQNN, with the method outperforming previous automated approaches and achieving competitive results against SOTA hand-crafted models. The inclusion of transfer learning experiments further strengthens the claims.
3. Scientific Rigor: The methodology is clearly described, and the experiments are thorough, including ablation studies, Q-value analysis, and stability evaluations.
Supporting Arguments:
- The use of Q-learning with experience replay and an Îµ-greedy strategy is well-justified and aligns with reinforcement learning literature. The constraints on the state-action space are practical and ensure tractability without compromising generality.
- The paper provides detailed insights into the learned architectures, showing that the agent discovers design motifs similar to those in hand-crafted networks. This adds interpretability to the results.
- The experiments are reproducible, with code and model files made publicly available, which is a strong positive for the research community.
Suggestions for Improvement:
1. Evaluation on Larger Datasets: While the results on CIFAR-10, SVHN, and MNIST are promising, the method's scalability to larger datasets like ImageNet remains unexplored. Testing on such datasets would significantly enhance the paper's impact.
2. Comparison with Modern NAS Methods: The paper does not compare MetaQNN with more recent neural architecture search (NAS) methods, such as those based on differentiable architecture search (DARTS). Including such comparisons would provide a clearer picture of MetaQNN's relative performance.
3. Complex Layer Types: The method is currently restricted to standard layer types (convolution, pooling, fully connected). Exploring the inclusion of more complex layers (e.g., residual connections, attention mechanisms) could further improve performance and applicability.
4. Computational Efficiency: The method requires significant computational resources (e.g., 8-10 days on 10 GPUs for CIFAR-10). Discussing strategies to reduce the computational cost, such as leveraging surrogate models or early stopping, would be beneficial.
Questions for the Authors:
1. How does the performance of MetaQNN scale with larger datasets or tasks beyond image classification? Have you considered testing on datasets like ImageNet or COCO?
2. Can the proposed method be extended to include more complex layer types, such as residual connections or attention mechanisms? If so, what modifications would be required?
3. How does MetaQNN compare to recent NAS methods, such as DARTS or ENAS, in terms of performance and computational efficiency?
Conclusion:
The paper makes a valuable contribution to the field of automated machine learning by introducing a reinforcement learning-based approach for CNN architecture design. While there are areas for improvement, particularly in scalability and comparisons with modern NAS methods, the novelty and empirical results justify acceptance. The paper is well-written, scientifically rigorous, and provides a strong foundation for future research in automated network design.