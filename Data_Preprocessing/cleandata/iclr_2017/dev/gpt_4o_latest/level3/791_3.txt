Review of the Paper
Summary of Contributions
The paper introduces a novel unsupervised pretraining method for convolutional neural networks called Spatial Contrasting (SC). The method leverages the inherent spatial properties of convolutional networks by building triplets of image patches and learning representations that score patches from the same image higher than those from different images. The proposed approach is straightforward to implement, compatible with standard training techniques like SGD and backpropagation, and does not require architectural modifications. Empirical results demonstrate that SC achieves state-of-the-art performance on the STL10 dataset and competitive results on CIFAR-10. The paper also highlights the potential of SC as a pretraining technique for supervised tasks, particularly in scenarios with abundant unlabeled data.
Decision: Reject
While the paper presents an interesting and promising idea, it falls short in several key areas that are critical for acceptance. The primary reasons for rejection are the lack of mathematical clarity in some sections and insufficient exploration of hyper-parameters and empirical studies to fully validate the method's robustness and generalizability.
Supporting Arguments
1. Clarity Issues: 
   - The use of "P(fi^1 | fi^2)" in Section 4.1 is unclear and lacks a rigorous mathematical explanation. This makes it difficult to fully understand the underlying probabilistic framework of the proposed method.
   - Section 4.2, which discusses the use of "batch statistics," is underexplained. The sampling process and its implications for training dynamics are not sufficiently detailed.
2. Empirical Limitations:
   - The number of patches sampled in Algorithm 1 is fixed, but it should be treated as a hyper-parameter. Exploring different values could provide insights into the method's sensitivity and performance.
   - The impact of patch size is not discussed, even though it is a critical factor in the effectiveness of spatial contrasting. This omission limits the reproducibility and applicability of the method.
3. Performance Concerns:
   - While the results on STL10 are impressive, the CIFAR-10 results are less so. The authors should consider pretraining on larger datasets like ImageNet or combining CIFAR-10 with additional unlabeled data to demonstrate scalability and broader applicability.
Suggestions for Improvement
1. Mathematical Clarity: Provide a more rigorous explanation of "P(fi^1 | fi^2)" and its role in the loss function. Rephrase or expand Section 4.1 to ensure that the probabilistic interpretation is accessible to readers.
2. Hyper-Parameter Exploration: Treat the number of patches sampled and patch size as hyper-parameters. Conduct experiments to analyze their impact on performance and include these results in the paper.
3. Empirical Studies: Extend the experiments to include pretraining on larger datasets and evaluate the method's ability to scale. Additionally, conduct ablation studies to isolate the contributions of different components of the SC loss.
4. State-of-the-Art Comparison: Make a stronger effort to beat state-of-the-art results on CIFAR-10 and other benchmarks. This would strengthen the empirical validation of the method.
Questions for the Authors
1. Can you clarify the probabilistic interpretation of "P(fi^1 | fi^2)" in Section 4.1 and how it is computed?
2. How does the choice of patch size affect the performance of the SC loss? Did you experiment with different sizes?
3. Why was the number of patches sampled in Algorithm 1 fixed? Would varying this parameter improve performance or training stability?
4. Have you considered pretraining on larger datasets like ImageNet or combining CIFAR-10 with additional unlabeled data? If not, why?
While the paper introduces a novel and interesting idea, addressing the above issues is necessary to strengthen its scientific rigor and practical impact.