The paper presents a novel application of recurrent neural networks (RNNs) to model the spiking activity of primate retinal ganglion cells (RGCs) in response to natural images, outperforming the traditional generalized linear model (GLM). The authors propose a multitask RNN framework that shares features across neurons, enabling robust performance even with limited data. They also introduce a GLM-RNN hybrid model to dissect the contributions of spatial and temporal nonlinearities to the improved predictions. The paper claims that such predictive models can advance our understanding of retinal processing and serve as tools for neuroscience research and applications like retinal prosthetics.
Decision: Reject. While the paper demonstrates technical improvements in spike prediction, it falls short in providing biological insights into retinal processing, which is central to its stated goal. The novelty of using RNNs for this task is incremental, given the known complexity of retinal dynamics and the established utility of RNNs for time series prediction.
Supporting Arguments:
1. The paper is well-motivated, addressing the limitations of GLMs in capturing the nonlinear and recurrent nature of retinal responses to natural stimuli. The use of multitask learning to leverage limited data is a strength.
2. The results are scientifically rigorous, with RNNs consistently outperforming GLMs across metrics. However, the performance gains are modest, and the lack of improvement with more complex models (e.g., LSTMs) suggests data or architectural constraints.
3. The primary weakness lies in the interpretation of results. While the RNN's superior performance is unsurprising, the paper does not elucidate what this reveals about retinal computations. The discussion of spatial and temporal nonlinearities is limited, and the "black-box" nature of the RNN is acknowledged but not sufficiently addressed.
Suggestions for Improvement:
1. Biological Insights: The paper should focus on extracting interpretable features from the RNN to provide insights into retinal processing. For instance, analyzing the learned weights or dynamics could reveal novel computational principles.
2. Model Complexity: The lack of improvement with LSTMs raises questions about the data's adequacy or the model's design. Exploring alternative architectures or regularization techniques might yield better results.
3. Parameter Sharing: The decision to share parameters across neurons should be justified more rigorously. The authors should compare shared and free-parameter models under varying data conditions to assess the trade-offs.
4. Title Revision: Adding "activity" to the title (e.g., "Modeling Retinal Ganglion Cell Activity") would enhance formal correctness.
Questions for Authors:
1. Can you provide a deeper analysis of the RNN's learned features? For example, what do the first-layer filters reveal about spatial or temporal processing in the retina?
2. Why do you think LSTMs fail to outperform simpler RNNs? Could this be due to insufficient data or suboptimal hyperparameter tuning?
3. How does parameter sharing impact performance as the dataset size increases? Would free-parameter models eventually outperform shared ones with more data?
In summary, while the paper makes a technical contribution to spike prediction, its lack of biological interpretability and incremental novelty limit its impact. Addressing these issues could significantly strengthen the work.