Review of the Paper
Summary of Contributions
This paper introduces Deep Variational Canonical Correlation Analysis (VCCA), a deep probabilistic model for multi-view learning that extends the latent variable interpretation of linear CCA to nonlinear settings using deep neural networks (DNNs). The authors employ variational inference with reparameterization gradients to derive a lower bound on the data likelihood, enabling efficient optimization via stochastic gradient descent. A variant called VCCA-private is also proposed, which disentangles shared and private latent variables for each view. The paper highlights connections to multi-view autoencoders but distinguishes itself through the inclusion of probabilistic sampling and regularization at the bottleneck layer. The experimental results demonstrate the model's ability to disentangle shared and private representations effectively across diverse datasets, including noisy MNIST, XRMB speech-articulation, and MIR-Flickr.
Decision: Reject
While the paper demonstrates strong experimental results and provides a clear extension of probabilistic CCA to deep generative models, it lacks sufficient novelty and methodological innovation. The reliance on standard variational inference techniques and the absence of more expressive approximations (e.g., normalizing flows) limit its contribution to the field.
Supporting Arguments
1. Strengths:
   - The paper is well-written and provides a clear explanation of the methodology, including its connection to prior work on multi-view autoencoders and probabilistic CCA.
   - The experiments are thorough and demonstrate the model's ability to disentangle shared and private latent variables effectively. The inclusion of multiple datasets strengthens the empirical evaluation.
   - The introduction of VCCA-private is a useful extension for modeling private variables, which improves reconstruction quality and interpretability.
2. Weaknesses:
   - The methodological contribution is incremental. The proposed model is a straightforward application of variational autoencoders (VAEs) to the probabilistic CCA framework, using standard techniques like mean-field approximation and black-box variational inference.
   - The paper does not leverage recent advancements in variational inference, such as normalizing flows or more expressive posterior approximations, which could have significantly improved the model's capacity.
   - The connection to multi-view autoencoders is acknowledged but offers limited novelty, as the insights align with well-established findings in the literature.
   - A comparison to full Bayesian inference methods (e.g., MCMC) for linear CCA is missing, which would have strengthened the evaluation and contextualized the performance of the proposed approach.
Additional Feedback
1. Experiments:
   - The experiments could be improved by including comparisons to more recent probabilistic inference methods or generative models. For example, evaluating against models that use normalizing flows or hierarchical priors would provide a more comprehensive analysis.
   - The reliance on standard approximations (e.g., mean-field family) should be explicitly tested against more robust inference techniques, such as non-amortized variational inference or MCMC, to assess the trade-offs in performance and scalability.
2. Broader Impact:
   - While the disentanglement of shared and private variables is a valuable feature, the paper could explore more practical downstream tasks (e.g., semi-supervised learning or domain adaptation) to demonstrate the broader applicability of the proposed model.
3. Clarity:
   - The connection to multi-view autoencoders could be elaborated further to clarify the key distinctions and advantages of VCCA over existing methods.
   - The paper could benefit from a more detailed discussion of the limitations of the proposed approach, particularly regarding scalability and the choice of posterior approximations.
Questions for the Authors
1. Why were more expressive variational approximations (e.g., normalizing flows) not considered? Would incorporating such techniques improve the model's performance or flexibility?
2. How does VCCA compare to full Bayesian inference methods for linear CCA in terms of performance and computational efficiency?
3. Could the authors provide more insights into the scalability of VCCA and VCCA-private for larger datasets or higher-dimensional views?
In conclusion, while the paper presents a clear and well-executed extension of probabilistic CCA, the lack of methodological novelty and missed opportunities to leverage recent advancements in variational inference limit its impact. Addressing these issues in a future revision could significantly strengthen the contribution.