Review of the Paper
Summary of Contributions
This paper introduces Autoencoded Variational Inference for Topic Models (AVITM), a novel application of Autoencoding Variational Bayes (AEVB) to Latent Dirichlet Allocation (LDA). The authors address two significant challenges in applying AEVB to topic models: the Dirichlet prior and component collapsing. The proposed method offers computational efficiency, black-box inference, and competitive topic coherence compared to traditional methods like collapsed Gibbs sampling and mean-field variational inference. Additionally, the paper presents a new topic model, ProdLDA, which replaces the mixture model in LDA with a product of experts, yielding more interpretable topics with minimal code changes. The authors demonstrate the effectiveness of AVITM and ProdLDA through experiments on real-world datasets, highlighting improvements in training time, topic coherence, and scalability.
Decision: Reject
While the paper presents a promising approach, it falls short in several key areas that hinder its acceptance. The primary reasons for rejection are: (1) the conflation of model and inference in the experiments, which makes it difficult to isolate the contributions of AVITM, and (2) insufficient discussion of hyper-parameter selection and its impact on performance.
Supporting Arguments
1. Experimental Design: The experimental results conflate the effects of the proposed inference method (AVITM) with the new model (ProdLDA). This makes it challenging to determine whether the observed improvements stem from the inference method, the model, or their combination. A clearer separation of these factors is necessary for a rigorous evaluation.
   
2. Hyper-Parameter Discussion: The lack of a detailed discussion on hyper-parameter selection limits the reproducibility and generalizability of the results. For example, the choice of learning rates, momentum, and batch normalization parameters appears critical to avoiding component collapsing, yet these details are not adequately explored.
3. Statistical Rigor: The results are presented without error bars, raising concerns about the statistical significance of the findings. This omission undermines confidence in the reported improvements.
4. Clarity Issues: Several aspects of the paper are unclear. For instance, the term "unimodal in softmax basis" in Section 3.2 is not well-explained, and Figure 1 is ambiguous regarding whether it represents prior or posterior distributions. Such issues detract from the paper's readability and scientific rigor.
Suggestions for Improvement
1. Synthetic Datasets: To better assess the parameter recovery capabilities of AVITM, the authors should include experiments on synthetic datasets with known Dirichlet distributions. This would provide a controlled setting to evaluate the inference method independently of the model.
2. Error Bars and Statistical Analysis: Include error bars in all experimental results to demonstrate statistical significance and robustness.
3. Hyper-Parameter Sensitivity: Provide a detailed analysis of hyper-parameter sensitivity, including guidelines for selecting optimal values.
4. Terminology and Clarity: Revise unclear terminology (e.g., "unimodal in softmax basis") and ensure consistency between text and figures (e.g., Figure 1).
5. Inference Network Credit: Acknowledge prior work on inference networks, such as the Helmholtz machine, to provide proper historical context.
Questions for the Authors
1. How does AVITM perform on synthetic datasets with varying Dirichlet priors? Can it recover the true parameters accurately?
2. Could you clarify the meaning of "unimodal in softmax basis" in Section 3.2? The provided explanation and counterexample are unclear.
3. How sensitive is AVITM to the choice of hyper-parameters, particularly learning rate and momentum? Can you provide guidelines for practitioners?
4. Does ProdLDA represent a novel contribution to the topic modeling literature, or is it primarily an illustration of AVITM's flexibility?
By addressing these issues, the paper could significantly improve its clarity, rigor, and impact.