Review
Summary of Contributions:
This paper addresses the critical problem of adversarial example detection in machine learning systems and proposes a novel approach to improve the robustness of neural networks. The authors introduce a detector subnetwork trained to distinguish between genuine and adversarial inputs, which is integrated into the primary classification network. This joint system is evaluated against both static and dynamic adversaries, demonstrating significant improvements in adversarial detection and robustness. The paper also introduces a novel training procedure for the detector to counteract dynamic adversaries, which adapt to both the classifier and detector. The experimental results, conducted on CIFAR10 and a subset of ImageNet, are thorough and well-analyzed, showing high detectability of adversarial examples and generalization across adversarial methods. The paper is well-written, clear, and concise, making it accessible to a broad audience.
Decision: Accept
The paper makes a strong contribution to the field of adversarial robustness by introducing a novel and effective adversarial detection mechanism. The key reasons for acceptance are:
1. The proposed detector subnetwork and training procedure are innovative and well-motivated, addressing a significant gap in the literature.
2. The experimental results are robust, scientifically rigorous, and convincingly support the claims made in the paper.
Supporting Arguments:
1. Problem Tackled: The paper addresses a well-defined and important problemâ€”detecting adversarial examples and improving the robustness of neural networks. The motivation is clear, particularly for safety-critical applications like autonomous driving.
2. Novelty and Motivation: The approach of augmenting a classifier with a detector subnetwork is novel and orthogonal to existing methods that focus solely on hardening the classifier. The dynamic adversary training procedure is a significant innovation that elevates the adversary-model competition.
3. Experimental Rigor: The results are comprehensive, covering multiple datasets (CIFAR10 and ImageNet), adversarial methods (static and dynamic), and evaluation metrics (detectability and generalization). The experiments are well-designed, and the findings are well-supported by detailed analyses.
4. Clarity and Presentation: The paper is well-structured, with clear explanations of methods, experiments, and results. The figures and tables effectively illustrate the findings.
Suggestions for Improvement:
1. Broader Generalization: While the detector generalizes well to similar adversaries, its performance degrades for dissimilar ones. Future work could explore methods to improve generalization across a wider range of adversaries.
2. Computational Efficiency: The paper does not discuss the computational overhead introduced by the detector subnetwork. A discussion of the trade-offs between robustness and efficiency would strengthen the paper.
3. Real-World Scenarios: While the results are promising, the paper could benefit from a discussion on the applicability of the proposed method to real-world adversarial attacks, including those that may not rely on gradient-based methods.
4. Ablation Studies: Additional ablation studies on the detector's architecture and hyperparameters would provide deeper insights into the design choices.
Questions for the Authors:
1. How does the computational cost of training and inference with the detector subnetwork compare to that of the baseline classifier?
2. Can the proposed detector handle adversarial attacks that are not gradient-based, such as black-box or physical-world attacks?
3. How does the detector perform when integrated with other types of classifiers beyond ResNet and VGG16?
In conclusion, this paper makes a significant contribution to adversarial robustness research and is a strong candidate for acceptance. With minor improvements and additional discussions, the work could have even broader impact.