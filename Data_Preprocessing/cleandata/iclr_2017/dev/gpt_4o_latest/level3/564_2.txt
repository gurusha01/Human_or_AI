Review of the Paper
Summary of Contributions
This paper introduces Higher Order Recurrent Neural Networks (HORNNs), a novel extension of traditional RNNs aimed at improving their ability to model long-term dependencies in sequential data. By incorporating multiple memory units that track preceding states and feed them back through weighted paths, HORNNs address the vanishing gradient problem that often limits the effectiveness of RNNs. The authors draw an analogy to digital filters in signal processing, which is a creative and insightful connection. They also propose pooling mechanisms (e.g., max-based, FOFE-based, and gated pooling) to normalize feedback signals and improve learning stability. Experimental results on two language modeling datasets, Penn Treebank (PTB) and English text8, demonstrate that HORNNs achieve state-of-the-art performance, outperforming RNNs, LSTMs, and other baselines.
Decision: Reject
While the paper presents an interesting and promising idea, it falls short in several critical areas that prevent it from being accepted in its current form. The primary reasons for rejection are the lack of sufficient experimental depth and the need for significant improvements in organization and clarity.
Supporting Arguments
1. Novelty and Potential: The idea of extending RNNs with higher-order memory units and connecting this to signal processing concepts is innovative. The proposed pooling mechanisms are also well-motivated and address practical challenges in training HORNNs.
2. Experimental Results: The results on PTB and text8 are compelling, showing that HORNNs outperform existing models. However, the experiments are limited to language modeling tasks, and the generalizability of HORNNs to other sequential modeling tasks (e.g., speech recognition, machine translation) remains unexplored.
3. Connections to Literature: While the paper references relevant prior work, the discussion of how HORNNs relate to or improve upon these methods is somewhat superficial. For example, the comparison with LSTMs and GRUs could be more rigorous, particularly in terms of computational efficiency and scalability.
4. Clarity and Organization: The paper is dense and difficult to follow in places. Key ideas, such as the motivation for specific pooling mechanisms or the computational trade-offs of HORNNs, are not presented clearly. The writeup would benefit from better structuring and more concise explanations.
Additional Feedback for Improvement
1. Expand Experimental Scope: To strengthen the claims of general applicability, the authors should evaluate HORNNs on a broader range of tasks, such as speech recognition or sequence-to-sequence modeling. This would provide stronger evidence of the model's versatility.
2. Ablation Studies: The paper would benefit from more detailed ablation studies to isolate the contributions of different components (e.g., the impact of pooling mechanisms, the choice of order in HORNNs).
3. Computational Analysis: While the authors mention that HORNNs are computationally efficient, a more detailed analysis of training time, memory usage, and scalability compared to LSTMs and GRUs would be valuable.
4. Clarity in Presentation: The writeup should be refined to improve readability. For example, the introduction could be more concise, and the technical details in Section 3 could be better organized with clear subsections and diagrams.
Questions for the Authors
1. How do HORNNs perform on tasks beyond language modeling, such as speech recognition or machine translation? Have you considered evaluating them on such tasks?
2. Could you provide a more detailed comparison of computational efficiency between HORNNs, LSTMs, and GRUs, especially for larger datasets or longer sequences?
3. How sensitive are HORNNs to the choice of pooling mechanisms and hyperparameters (e.g., the forgetting factor in FOFE pooling)?
4. Have you explored the impact of dropout or other regularization techniques on the performance of HORNNs?
In summary, while the paper introduces an intriguing approach with promising results, it requires more experimental depth, better organization, and clearer presentation to reach the standard for acceptance. The authors are encouraged to address these issues and resubmit.