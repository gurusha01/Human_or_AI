Review of the Paper
The paper explores intrinsic motivation in deep reinforcement learning (RL) by leveraging surprisal and k-step learning progress as intrinsic rewards, derived from the KL-divergence between true and learned transition probabilities. The authors claim that these intrinsic rewards enable efficient exploration in sparse reward environments, outperforming naive exploration methods and achieving competitive performance with state-of-the-art methods like VIME, while being computationally more efficient.
Decision: Reject
The primary reasons for rejection are the lack of novelty and insufficient scientific rigor in supporting the claims. While the paper addresses an important problem in RL, the proposed intrinsic motivation techniques (surprisal and learning progress) are not novel and have been explored in prior works, such as Kompella et al. (2012) and Srivastava et al. (2012). Furthermore, the application of these techniques to deep RL lacks significant innovation compared to existing methods like VIME. The empirical results, while promising, do not convincingly demonstrate superiority over VIME, as the claimed computational efficiency mainly arises from faster initialization rather than per-step improvements.
Supporting Arguments
1. Novelty: The concepts of surprisal and learning progress as intrinsic motivators have been well-studied in the literature. The paper does not sufficiently differentiate its approach from prior works, such as Stadie et al. (2016) and Houthooft et al. (2016). The use of KL-divergence for intrinsic rewards is incremental rather than groundbreaking.
2. Empirical Results: While the paper demonstrates competitive performance with VIME on several tasks, the results are not consistently superior. For example, surprisal performs worse than VIME on CartpoleSwingup and exhibits higher variance on SwimmerGather. The computational efficiency claim is overstated, as the speedup primarily comes from parallelized forward passes, which is a hardware-dependent advantage rather than an algorithmic innovation.
3. Limitations: The approach of targeting states with uncertain transition models is sensible but limited in environments like Go, where the transition model is trivial. The paper does not address how its methods generalize to such tasks.
Suggestions for Improvement
1. Novelty: The authors could explore alternative forms of intrinsic motivation, such as agent competence-based measures (e.g., Srivastava et al., 2012), to introduce more novelty.
2. Empirical Rigor: Provide a more thorough comparison with VIME, including statistical significance tests and ablation studies to isolate the contributions of surprisal and learning progress.
3. Generalization: Discuss the limitations of the proposed methods in environments with trivial or deterministic dynamics and explore potential solutions.
4. Clarity: The paper could benefit from a clearer explanation of the differences between surprisal and learning progress, as well as their respective advantages and disadvantages.
Questions for the Authors
1. How does the proposed method handle environments with trivial or deterministic transition models, where the KL-divergence may not provide meaningful intrinsic rewards?
2. Can the authors provide a detailed breakdown of the computational cost comparison with VIME, including the impact of hardware parallelization?
3. Have the authors considered alternative intrinsic motivation measures, such as those based on agent competence or empowerment, and how do these compare to surprisal and learning progress?
While the paper addresses an important challenge in RL and presents some promising results, the lack of novelty and insufficient empirical rigor prevent it from making a significant contribution to the field. Addressing these issues could improve the paper's impact in future iterations.