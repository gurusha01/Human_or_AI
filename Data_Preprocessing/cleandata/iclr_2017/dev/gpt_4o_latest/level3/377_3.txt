Review of the Paper
Summary of Contributions
This paper investigates the ability of reinforcement learning (RL) agents to infer physical properties of objects through interaction, focusing on two tasks: determining which block is heavier and counting rigid bodies in a tower. The authors argue that interaction, rather than passive observation, is crucial for understanding hidden object properties. The paper demonstrates that RL agents trained with a recurrent A3C model can learn experimentation strategies to solve these tasks. The authors also explore how agents balance the cost of gathering information against the risk of making mistakes. The work is inspired by developmental psychology and aims to bridge the gap between human-like intuitive physics and artificial agents.
Decision: Reject
The primary reasons for this decision are the limited novelty of the approach and the insufficient contributions to advancing the field. While the problem is well-motivated and the paper is well-written, it lacks significant architectural or theoretical innovations. The tasks presented, while interesting, are simplistic and do not serve as robust benchmarks for more advanced agents. Furthermore, the paper does not sufficiently differentiate its contributions from prior work, particularly in the domain of learning physical properties through interaction.
Supporting Arguments for the Decision
1. Limited Novelty: The approach of learning physical properties through interaction is not new and has been explored in prior work (e.g., Wu et al., 2015; Galileo). The use of a recurrent A3C model is standard, and there are no significant architectural or algorithmic contributions.
   
2. Task Simplicity: The two tasks—"Which is Heavier" and "Towers"—represent a narrow scope of physical understanding. The difficulty of these tasks is unclear due to the absence of comparisons with simpler RL agents or baselines. For example, the "Which is Heavier" task appears relatively easy, as it primarily involves computing differences in block positions.
3. Insufficient Related Work Discussion: The paper does not adequately highlight why its tasks are more interesting or challenging than those in prior approaches. The related work section lacks a detailed comparison, making it difficult to assess the novelty and significance of the contributions.
4. Limited Contribution to the Field: While the paper demonstrates that agents can learn to experiment and gather information, the results do not significantly advance our understanding of how agents can learn physical properties. The tasks are unlikely to generalize to more complex scenarios or serve as benchmarks for future research.
Suggestions for Improvement
1. Expand Task Scope: Introduce more challenging and diverse tasks that better represent the complexity of physical reasoning. For example, tasks involving friction, deformability, or tool use could provide a richer testbed for evaluating agent capabilities.
2. Baseline Comparisons: Include comparisons with simpler RL agents or heuristic baselines to better contextualize the difficulty of the tasks and the performance of the proposed approach.
3. Detailed Model Description: Provide a more thorough description of the model architecture, including a diagram illustrating inputs, outputs, and the overall workflow. This would improve clarity and replicability.
4. Future Directions: Include a discussion on how the proposed tasks and methods could be extended to more complex environments or used to study transfer learning, theory building, or data efficiency.
5. Related Work: Strengthen the discussion of related work by explicitly comparing the proposed approach to prior methods and highlighting the unique contributions of this paper.
Questions for the Authors
1. How do the proposed tasks compare in difficulty to existing benchmarks for learning physical properties? Could simpler baselines (e.g., random policies or non-recurrent models) solve these tasks?
2. Why were these specific tasks (mass inference and counting rigid bodies) chosen? How do they generalize to broader notions of physical reasoning?
3. Could the learned experimentation strategies transfer to new tasks or environments? If not, how might this limitation be addressed in future work?
In summary, while the paper addresses an interesting and relatively unexplored area, its contributions are incremental, and the tasks are too simplistic to have a significant impact on the field. Addressing the above suggestions could greatly strengthen the paper and its potential for acceptance in future iterations.