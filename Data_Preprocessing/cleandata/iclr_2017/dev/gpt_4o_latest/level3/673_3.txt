Review of "Hierarchical Memory Networks with Maximum Inner Product Search for Scalable Memory Access"
Summary of Contributions
The paper proposes a Hierarchical Memory Network (HMN) that integrates Maximum Inner Product Search (MIPS) for scalable memory access. The authors argue that this hybrid approach combines the scalability of hard attention mechanisms with the ease of training associated with soft attention. The paper explores various approximate MIPS techniques—clustering, hashing, and tree-based approaches—to improve scalability during both training and inference. Empirical results on the SimpleQuestions dataset suggest that exact K-MIPS improves accuracy over soft attention, while approximate K-MIPS methods provide computational speedups at the cost of some accuracy. The authors also propose strategies to mitigate the approximation bias introduced by clustering-based MIPS.
Decision: Reject
The primary reasons for rejection are:
1. Lack of Novelty: The core idea of using approximate nearest-neighbor search (NNS) or MIPS for memory-augmented networks has already been explored by Rae et al. (2016). The paper does not sufficiently differentiate itself from prior work or provide a compelling advancement over existing methods.
2. Incomplete Benchmarking: The authors use their own neighbor searcher instead of standardized methods and fail to benchmark their approach against well-established baselines in the approximate NNS/MIPS literature. This omission weakens the empirical rigor of the paper.
Supporting Arguments
1. Lack of Novelty: While the hierarchical memory structure and K-MIPS integration are presented as contributions, these ideas are incremental extensions of existing work. Rae et al. (2016) already demonstrated the use of approximate search methods for memory access in neural networks. The authors' claim that their approach is fundamentally different due to its focus on external knowledge bases is not convincingly substantiated.
   
2. Misaligned Problem Setting: The problem tested—factoid question answering on the SimpleQuestions dataset—does not clearly necessitate vector-based fast-nearest-neighbor search. Text hashing or simpler retrieval mechanisms could suffice for this task, making the use of approximate MIPS seem unnecessary and over-engineered.
3. Unnecessary Distinction Between "MIPS" and "NNS": The paper introduces a distinction between MIPS and NNS, which is largely unnecessary and distracts from the main contributions. Most modern libraries handle both tasks or allow easy conversions between them.
4. Empirical Weaknesses: The clustering-based approximate MIPS approach, which is the best-performing method among the approximations, still underperforms compared to exact K-MIPS. This raises concerns about the practical utility of the proposed methods for real-world scalability.
Suggestions for Improvement
1. Clarify Novelty: Clearly articulate how the proposed method advances beyond Rae et al. (2016) and other related works. Highlight specific technical or empirical contributions that distinguish this paper.
   
2. Benchmark Against Standard Methods: Include comparisons with well-established approximate NNS/MIPS methods (e.g., FAISS, Annoy, HNSW) to provide a more rigorous evaluation of the proposed approach.
3. Reconsider Task Selection: Choose a task where vector-based nearest-neighbor search is demonstrably necessary and cannot be replaced by simpler methods like text hashing. This would better justify the use of MIPS-based approaches.
4. Address Approximation Bias: The paper acknowledges the performance degradation caused by approximate K-MIPS but does not propose a robust solution. Future work should focus on reducing this bias, potentially through dynamic memory updates or hybrid retrieval strategies.
Questions for the Authors
1. How does your approach compare to Rae et al. (2016) in terms of both accuracy and computational efficiency? Could you provide direct comparisons?
2. Why was the SimpleQuestions dataset chosen for evaluation, given that simpler retrieval mechanisms could suffice for this task?
3. Did you evaluate your approximate MIPS methods against widely used libraries like FAISS or HNSW? If not, why?
4. Could you elaborate on why the distinction between MIPS and NNS is necessary for this work?
In summary, while the paper addresses an important challenge in scaling memory-augmented networks, it fails to provide sufficient novelty, empirical rigor, or justification for its design choices. Addressing these issues could significantly strengthen the work.