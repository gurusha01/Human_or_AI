Review of the Paper
Summary of Contributions
This paper introduces two novel extensions to the Paragraph Vector (PV) model: the Generative Paragraph Vector (GPV) and the Supervised Generative Paragraph Vector (SGPV). The authors aim to address a perceived limitation of PV—the inability to infer representations for unseen texts—by proposing a probabilistic generative framework. GPV introduces a prior distribution over paragraph vectors, enabling inference for new texts, while SGPV incorporates text labels into the model for supervised learning tasks. The paper claims that these models are simple yet effective, achieving state-of-the-art performance on several text classification benchmarks. The authors further extend SGPV to include n-grams, demonstrating the importance of word order in text representation.
Decision: Reject
The primary reason for rejection is that the paper's core premise—that the original PV model cannot infer representations for unseen texts—is incorrect. This misunderstanding undermines the motivation for the proposed models and calls into question the necessity of the contributions. While the paper does present some novel ideas, the invalid premise weakens the overall argument and the subsequent discussion.
Supporting Arguments for the Decision
1. Invalid Premise: The authors assert that PV cannot infer representations for unseen texts, but this is not accurate. PV can infer representations for new texts by fixing the word embeddings and optimizing the paragraph vector for the new text. This misunderstanding undermines the paper's motivation and the need for the proposed generative framework.
   
2. Methodological Concerns: While the generative approach is novel, it is not clear that it provides significant advantages over existing methods. The paper does not convincingly demonstrate why the added complexity of the generative framework is necessary or beneficial compared to simpler alternatives like PV with inference.
3. Empirical Evidence: Although the experimental results show competitive performance, they do not provide compelling evidence that the proposed models are superior to existing methods. The improvements over PV and other baselines are marginal in some cases, and the paper does not adequately explore why these improvements occur.
Suggestions for Improvement
1. Clarify the Motivation: The authors should revisit their understanding of PV and explicitly address how their approach differs from and improves upon the standard inference process for unseen texts. If the goal is to provide a probabilistic framework, this should be clearly articulated as the primary contribution, rather than addressing a non-existent limitation.
2. Focus on Novel Contributions: The paper introduces interesting ideas, such as incorporating n-grams into the generative framework. These aspects could be explored further in a standalone paper, emphasizing their novelty and practical implications.
3. Empirical Analysis: The authors should provide more detailed analysis to justify the benefits of their approach. For example, they could compare the quality of inferred representations for unseen texts between PV and GPV/SGPV, or analyze the impact of the prior distribution on performance.
Questions for the Authors
1. How does the proposed inference process for GPV/SGPV differ from the standard inference process in PV, where paragraph vectors for unseen texts are optimized with fixed word embeddings?
2. Can you provide a detailed comparison of the computational complexity of GPV/SGPV versus PV with inference? Is the added complexity justified by the performance gains?
3. How sensitive are the results to the choice of prior distribution for paragraph vectors? Have you explored alternatives to the multivariate normal distribution?
In conclusion, while the paper introduces some novel ideas, the invalid premise regarding PV's limitations undermines the core argument. I encourage the authors to refine their understanding of PV, focus on the novel aspects of their approach, and resubmit a revised version that better positions their contributions within the existing literature.