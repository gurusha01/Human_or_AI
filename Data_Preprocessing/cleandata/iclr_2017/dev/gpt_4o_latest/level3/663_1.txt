Review of the Paper
Summary of Contributions
This paper proposes a large-scale multi-modal product classification system that combines text and image inputs using a decision-level fusion approach. The authors train state-of-the-art CNNs for text and image inputs separately and introduce a policy network to combine their outputs. The experimental results show that the text CNN significantly outperforms the image CNN on a real-world dataset of 1.2 million products from Walmart.com. While the multi-modal fusion approach achieves a slight improvement in top-1 accuracy over the text CNN, feature-level fusion underperforms, which is unexpected. The paper provides clear writing, practical insights into large-scale model training, and a valuable error analysis highlighting the potential of multi-modality. 
Decision: Reject
Key reasons for this decision are the limited technical novelty and the minimal performance improvement achieved by the proposed approach. While the paper is well-written and provides practical insights, the novelty of the fusion methods is limited, as decision- and feature-level fusion techniques have been explored extensively in prior work. Furthermore, the performance gain from the proposed multi-modal fusion approach is marginal, raising questions about its practical utility. The lack of additional datasets also limits the reproducibility of the results.
Supporting Arguments
1. Limited Technical Novelty: The decision-level and feature-level fusion techniques used in the paper are well-documented in prior literature. While the authors provide a novel application to a large-scale e-commerce dataset, the methodological contributions are incremental rather than groundbreaking.
   
2. Minimal Performance Improvement: The multi-modal fusion approach only slightly improves top-1 accuracy over the text CNN, which already outperforms the image CNN by a significant margin. This raises concerns about whether the added complexity of multi-modal fusion is justified in practice.
3. Reproducibility Concerns: The experiments rely solely on a proprietary Walmart dataset, and no additional datasets are reported. This makes it difficult for other researchers to validate or extend the findings.
Suggestions for Improvement
1. Broader Evaluation: The authors should evaluate their approach on additional publicly available datasets to enhance reproducibility and demonstrate the generalizability of their method.
   
2. Deeper Analysis of Feature-Level Fusion: The unexpected underperformance of feature-level fusion warrants further investigation. The authors could explore alternative architectures, optimization strategies, or regularization techniques to improve its performance.
3. Quantify Practical Benefits: Since the text CNN already performs well, the authors should provide a more detailed discussion of scenarios where multi-modal fusion would be practically beneficial, such as cases with non-informative text or images.
4. Novelty in Fusion Techniques: To enhance the technical contribution, the authors could explore more sophisticated fusion methods, such as attention mechanisms or transformer-based architectures, which are gaining traction in multi-modal learning.
Questions for the Authors
1. Can you provide more details on why feature-level fusion underperforms? Did you explore using pre-trained embeddings or alternative architectures for feature concatenation?
2. How sensitive is the performance of the policy network to its architecture and hyperparameters? Could deeper or more complex policy networks yield better results?
3. Have you considered using publicly available datasets for evaluation? If not, could you release a subset of the Walmart dataset to facilitate reproducibility?
In conclusion, while the paper addresses a relevant problem and provides practical insights, the limited novelty and marginal performance gains do not meet the bar for acceptance at this conference. The authors are encouraged to address the reproducibility concerns and explore more innovative fusion techniques in future work.