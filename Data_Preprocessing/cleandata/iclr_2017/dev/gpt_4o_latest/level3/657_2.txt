Review
Summary of Contributions
The paper introduces a novel approach to compressing fast linear text classification models, specifically targeting memory-constrained environments such as mobile devices. The authors build upon the fastText framework and employ product quantization (PQ) as the primary compression technique, combined with additional text-specific tricks like vocabulary pruning and hashing. The proposed method achieves significant memory savings—up to two orders of magnitude—while maintaining comparable classification accuracy. The authors also demonstrate the effectiveness of their approach across multiple benchmarks, outperforming state-of-the-art methods in terms of the trade-off between memory usage and accuracy. The paper is well-written, and the experimental results are compelling, providing strong evidence for the claims.
Decision: Accept
Key reasons for acceptance:
1. Strong Results: The paper demonstrates substantial memory savings (up to ×1000 compression) with minimal loss in accuracy, which is highly relevant for real-world applications.
2. Clarity and Reproducibility: The methodology is clearly described, and the authors commit to releasing code, making the work reproducible and impactful for the community.
Supporting Arguments
1. Problem Relevance: The paper addresses a critical problem in NLP—balancing model size and accuracy for text classification—particularly for resource-constrained environments. This is a timely and important contribution, given the increasing deployment of machine learning models on edge devices.
2. Well-Motivated Approach: The use of product quantization is a logical choice for model compression, and the authors adapt it effectively to the text classification domain. The inclusion of text-specific tricks like vocabulary pruning and hashing further strengthens the approach.
3. Rigorous Evaluation: The experiments are thorough, covering multiple datasets and comparing against strong baselines. The results convincingly support the claims, with detailed analysis of trade-offs between memory usage and accuracy.
Suggestions for Improvement
1. Citations to Related Work: While the paper provides a strong review of prior work, it would benefit from citing additional relevant literature, such as "Quantized Convolutional Neural Networks for Mobile Devices" (CVPR 2016), which discusses compression techniques in a different domain but shares conceptual similarities.
2. Further Analysis of Compression Limits: The paper briefly explores extreme compression scenarios (e.g., models under 64KiB), but a deeper analysis of the trade-offs in these cases would add value. For instance, how does the performance degrade across different datasets as compression becomes more aggressive?
3. Comparison with Other Compression Techniques: While PQ is the primary focus, a more detailed comparison with alternative compression methods (e.g., pruning without retraining, binary quantization) would provide additional context for the results.
Questions for the Authors
1. How does the proposed method perform on datasets with highly imbalanced classes or rare labels? Does the pruning strategy affect the model's ability to handle such cases?
2. Could the authors elaborate on the choice of parameters for product quantization (e.g., number of subquantizers, bits per quantization index)? How sensitive are the results to these hyperparameters?
3. Have the authors considered combining PQ with other compression techniques, such as low-rank approximations or structured sparsity, to achieve even greater memory savings?
Overall, this paper makes a significant contribution to the field of text classification and model compression. With minor refinements and additional citations, it has the potential to serve as a strong baseline for future research.