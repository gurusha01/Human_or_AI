Review of the Paper
Summary of Contributions
This paper introduces the Dynamic Chunk Reader (DCR), a neural reading comprehension model designed to predict answers of varying lengths, addressing a gap in existing models that primarily focus on single-token or entity-based answers. The model combines recurrent neural networks, a novel word-by-word attention mechanism, and convolutional layers to dynamically construct and rank candidate answer chunks. The authors claim that DCR is an end-to-end system capable of handling both factoid and non-factoid questions, as demonstrated on the SQuAD dataset. The paper highlights three main contributions: (1) a joint candidate chunking and ranking mechanism, (2) a novel attention mechanism for question-aware passage representations, and (3) the integration of linguistic features to improve ranking performance.
Decision: Reject
While the paper addresses an important problem and proposes a novel approach, it falls short in several key areas, including clarity, scientific rigor, and empirical performance. The main reasons for rejection are:
1. Underwhelming Performance: The model, while showing improvements over baseline methods, performs significantly below state-of-the-art systems and ranks low on the SQuAD leaderboard (12th out of 15).
2. Misaligned Claims: The reliance on linguistic features (e.g., POS patterns) undermines the claim of an "end-to-end" system, as these features are manually engineered and not learned during training.
Supporting Arguments
1. Proposed Model: The DCR model is innovative in its ability to handle variable-length answers, which is a meaningful contribution to the reading comprehension domain. However, the reliance on brute-force enumeration and POS pattern trie trees for chunk generation deviates from the end-to-end paradigm. This hybrid approach weakens the novelty of the model, as it combines traditional rule-based methods with neural components.
   
2. Evaluation: The results on the SQuAD dataset (66.3% EM, 74.7% F1) are modest and fail to compete with state-of-the-art systems. The low leaderboard ranking suggests that the model's practical impact is limited. Additionally, the performance drop for longer answers and certain question types (e.g., "why" questions) highlights the model's limitations in handling complex queries.
3. Clarity and Rigor: The description of the attention mechanism is difficult to follow and lacks clear differentiation from standard seq2seq attention methods. Furthermore, the paper does not provide sufficient theoretical justification for some design choices, such as the use of convolutional layers for n-gram representations.
Suggestions for Improvement
1. Clarity: The attention mechanism and chunk representation layers need clearer explanations, with diagrams or pseudocode to aid understanding. Additionally, the distinction between DCR and prior models should be more explicitly articulated.
2. End-to-End Learning: To strengthen the claim of an end-to-end system, the authors should explore methods to replace manually engineered features (e.g., POS patterns) with learned representations.
3. Empirical Validation: The authors should benchmark DCR against more state-of-the-art systems and provide ablation studies to better isolate the contributions of each component.
4. Error Analysis: While the paper includes some analysis (e.g., performance by question type), a deeper investigation into failure cases, particularly for longer answers, could guide future improvements.
Questions for the Authors
1. How does the proposed attention mechanism differ from standard seq2seq attention beyond the word-by-word computation? Could you provide a formal comparison or empirical evidence for its superiority?
2. Why was brute-force enumeration chosen for chunk generation, and how does it scale for longer passages or datasets with larger contexts?
3. Have you considered alternative methods for chunking (e.g., boundary prediction models) that could reduce reliance on linguistic features?
In conclusion, while the paper introduces a novel approach to reading comprehension, it does not meet the bar for acceptance due to its limited performance, reliance on non-learned features, and lack of clarity in key sections. Further refinement and experimentation are needed to make the proposed model competitive and scientifically rigorous.