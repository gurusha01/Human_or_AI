Review
Summary of Contributions
The paper introduces Higher Order Recurrent Neural Networks (HORNNs), a novel extension to standard RNNs aimed at improving the modeling of long-term dependencies in sequential data. The key innovation lies in incorporating multiple memory units that track preceding states and feed them back into the hidden layer through weighted paths, analogous to digital filters in signal processing. The authors also explore various pooling functions—max-based, FOFE-based, and gated pooling—to normalize and aggregate information from these paths. The proposed HORNNs are evaluated on two language modeling tasks using the Penn Treebank (PTB) and English text8 datasets, achieving state-of-the-art performance in terms of perplexity. The paper claims that HORNNs alleviate the vanishing gradient problem, improve training efficiency compared to LSTMs, and are computationally efficient on GPUs.
Decision: Reject  
Key Reasons:  
1. Insufficient Evidence for Claims: The paper's claim that HORNNs improve long-term dependency modeling lacks rigorous analysis or ablation studies to isolate the impact of the proposed architecture. Additionally, the assertion that LSTMs are slow to train and hard to scale contradicts both empirical evidence and their widespread adoption in production systems.  
2. Limited Scope of Experiments: The experiments are confined to NLP tasks, specifically language modeling, without exploring other domains like audio, video, or time-series data, where long-term dependencies are equally critical. This limits the generalizability of the proposed approach.
Supporting Arguments
1. Unfair Comparisons to SOTA: The paper benchmarks HORNNs against older models, such as LSTMs and RNNs, but does not compare them to more recent state-of-the-art (SOTA) architectures like Transformers or their derivatives, which dominate NLP tasks. This makes it difficult to contextualize the reported results.  
2. Overlooked Analysis: While the authors claim that HORNNs alleviate vanishing gradients and improve long-term dependency modeling, no empirical evidence (e.g., gradient flow visualizations or attention maps) is provided to substantiate these claims.  
3. Contradictory Assertions: The claim that LSTMs are slow and hard to scale is not well-supported, especially given that LSTMs are widely used in large-scale production systems. This undermines the credibility of the paper's motivation.
Suggestions for Improvement
1. Expand Experimental Scope: Evaluate HORNNs on tasks beyond NLP, such as speech recognition, video classification, or time-series forecasting, to demonstrate their general applicability.  
2. Fair Comparisons: Compare HORNNs to more recent SOTA models, such as Transformers or hybrid architectures, to provide a clearer picture of their relative performance.  
3. Ablation Studies: Include experiments to isolate the contributions of individual components, such as the pooling functions or the higher-order connections, to validate their effectiveness.  
4. Gradient Analysis: Provide empirical evidence, such as gradient flow visualizations, to substantiate the claim that HORNNs mitigate the vanishing gradient problem.  
5. Clarify Claims: Reassess and justify the claim regarding LSTMs being slow and hard to scale, or remove it entirely if it cannot be substantiated.
Questions for the Authors
1. How do HORNNs perform on tasks involving sensory input data, such as audio or video, where long-term dependencies are critical?  
2. Could you provide empirical evidence (e.g., gradient flow analysis) to support the claim that HORNNs alleviate the vanishing gradient problem?  
3. Why were more recent SOTA models, such as Transformers, excluded from the comparisons?  
4. How do the proposed pooling functions compare in terms of computational overhead and their impact on model performance?  
In conclusion, while the core idea of HORNNs is interesting and potentially impactful, the paper requires significant additional work to substantiate its claims, broaden its experimental scope, and ensure fair comparisons.