The paper introduces the "Dynamic Chunk Reader" (DCR), an end-to-end neural model for machine reading comprehension (RC) that extracts and ranks answer candidates from a document. Unlike prior models, DCR predicts answers of variable lengths, addressing the challenge of identifying arbitrary answer spans. The proposed model incorporates novel techniques, including a convolutional layer for n-gram phrase representation and a dynamic chunking mechanism for candidate generation. Experimental results on the SQuAD dataset demonstrate competitive performance, achieving 66.3% Exact Match (EM) and 74.7% F1 scores.
Decision: Reject
While the paper presents innovative ideas, the lack of thorough analysis and comparisons limits the ability to assess the significance of the proposed techniques. Specifically, the following concerns warrant rejection:
1. Insufficient Justification for Convolutional Networks: The use of convolutional layers for n-gram phrase representation is novel, but the paper does not provide adequate comparisons to alternative methods, such as LSTMs or attention-based models. The role of tri-gram information in improving performance is unclear, and the authors fail to analyze its contribution relative to other components.
2. Dynamic Chunking Needs Further Validation: While dynamic chunking is a promising approach, the paper does not sufficiently justify its effectiveness in capturing long answer phrases. The analysis of its impact on performance, particularly for non-factoid questions, is limited.
3. Limited Discussion on Pre-trained Models: The reliance on pre-trained embeddings (e.g., GloVe) is noted, but the paper does not compare their advantages and disadvantages against character-based embeddings or other alternatives. This omission leaves a gap in understanding the trade-offs of the chosen approach.
Supporting Arguments:
- The experimental results are promising, but the lack of ablation studies on the convolutional layer and dynamic chunking makes it difficult to isolate their contributions.
- The paper does not address whether the proposed model generalizes to datasets beyond SQuAD, which limits its applicability.
Suggestions for Improvement:
1. Provide a detailed comparison of convolutional networks with LSTMs or other sequence models for phrase representation. Include ablation studies to quantify the contribution of tri-gram information.
2. Analyze the performance of dynamic chunking on longer answers and non-factoid questions. Consider providing qualitative examples to illustrate its effectiveness.
3. Discuss the trade-offs between pre-trained embeddings and alternative representations, such as character-based embeddings, and include empirical comparisons.
4. Expand the evaluation to additional datasets to demonstrate the generalizability of the model.
Questions for the Authors:
1. How does the convolutional layer compare to LSTMs in terms of computational efficiency and performance? Can you provide empirical evidence?
2. What specific challenges does dynamic chunking address that are not handled by prior chunking methods? How does it perform on non-factoid questions compared to factoid ones?
3. Have you considered using character-based embeddings or fine-tuning pre-trained models like BERT? How might these affect the model's performance?
In summary, while the paper introduces valuable ideas, the lack of rigorous analysis and broader evaluation limits its contributions. Addressing these gaps could significantly strengthen the work.