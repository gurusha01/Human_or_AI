Review of the Paper
Summary of Contributions
This paper introduces a novel model-based reinforcement learning (RL) approach called Predictive Reinforcement Learning (PRL) that leverages a residual recurrent neural network (RRNN) to predict future rewards. The authors claim that their approach is particularly suited for multitask learning, demonstrating transfer learning benefits across three ATARI games (Breakout, Pong, and Demon Attack). Additionally, the paper proposes a new recurrent network architecture inspired by residual networks, which decouples memory from computation. The authors argue that their approach can generalize across strategies and environments, and they highlight the potential for supervised training of predictive models in RL tasks.
Decision: Reject
The paper presents an interesting idea of combining model-based RL with multitask learning and introduces a novel architecture. However, the work suffers from significant methodological and presentation issues that undermine its contributions. The key reasons for rejection are (1) a lack of rigorous evaluation and comparison to existing RL methods, and (2) inconsistencies between the proposed algorithm and its implementation, which raise concerns about the validity of the results.
Supporting Arguments
1. Evaluation and Benchmarks: The experiments are limited to three ATARI games, which are hand-picked to suit the hardcoded strategy. There is no comparison to state-of-the-art RL methods such as DQN or A3C, making it difficult to assess the competitiveness of the proposed approach. Additionally, the lack of evaluation metrics like classification error or mean squared error (MSE) for the predictive model further weakens the empirical rigor.
   
2. Algorithm-Implementation Mismatch: The proposed algorithm outputs a single scalar (expected reward), but the implementation outputs two values (probability of dying and probability of higher score). This discrepancy is not adequately justified and raises concerns about the validity of the results and the claims made in the paper.
3. Lack of Reinforcement Learning: Despite being framed as an RL paper, the approach relies on supervised learning with hardcoded policies rather than demonstrating improvements in RL algorithms. This undermines the paper's claim of contributing to the RL field.
4. Clarity and Notation Issues: The paper suffers from unclear notation (e.g., "a" for state, inconsistent definitions of \( ri \) and \( ci \)) and unusual mathematical formulations. Observation 1 contains errors, and the explanation of the RRNN is unnecessarily convoluted.
5. Related Work and Positioning: The related work section is sparse and omits key references, such as "Action-Conditional Video Prediction using Deep Networks in Atari Games." The paper does not adequately position itself within the broader RL literature.
6. Experimental Results: The results are unclear and contradictory. For example, the claim of being "7 times better" is ambiguous and unsupported. The degradation in Pong and Demon Attack is not sufficiently analyzed, and the oscillations in performance are not addressed adequately.
Suggestions for Improvement
1. Expand Experiments: Include a broader range of games and compare the proposed approach against state-of-the-art RL methods. Evaluate the quality of predictions using metrics like MSE or classification error.
2. Clarify Algorithm-Implementation Gap: Align the algorithm and implementation or provide a strong justification for the discrepancy.
3. Strengthen Related Work: Include missing references and clearly articulate how the proposed approach advances the state of the art.
4. Improve Clarity: Revise the notation and mathematical formulations for consistency and readability. Address errors in Observation 1 and clarify ambiguous claims.
5. Demonstrate RL Contributions: Show how the proposed approach improves RL performance, rather than relying on supervised learning with hardcoded policies.
Questions for the Authors
1. Why does the implementation output two values instead of the single scalar described in the algorithm? How does this affect the validity of the results?
2. How does the proposed approach compare to existing RL methods like DQN or A3C in terms of performance and sample efficiency?
3. Can you provide quantitative metrics (e.g., MSE, classification error) to evaluate the quality of the predictive model?
4. How does the model handle long-term dependencies, and why were the selected games chosen for evaluation?
In conclusion, while the paper introduces a novel idea and architecture, the lack of rigorous evaluation, clarity, and alignment with RL principles makes it unsuitable for acceptance in its current form.