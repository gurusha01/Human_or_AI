Review
Summary of Contributions
This paper introduces the Input-Switched Affine Network (ISAN), a novel recurrent architecture with linear dynamics designed for enhanced interpretability and computational efficiency in language modeling. By eschewing non-linear activation functions, the ISAN allows for detailed analysis of its internal mechanisms, including the decomposition of predictions into contributions from past inputs and the exploration of latent state dynamics in alternate bases. The authors demonstrate that ISAN achieves comparable performance to standard RNNs, GRUs, and LSTMs on the Text8 dataset for character-level language modeling, while offering potential computational benefits such as parameter sparsity and caching of affine transformations for common input sequences. The paper also highlights the model's ability to provide insights into word-level contributions and semantic clustering, which are challenging to achieve with non-linear RNNs. Despite its simplicity, the ISAN is shown to be a promising step toward interpretable and efficient sequence modeling.
Decision: Accept  
The paper is recommended for acceptance due to its significant contribution to the research dialogue on interpretable recurrent architectures, even though its quantitative results are modest. The work provides a valuable framework for understanding linear recurrent models and opens avenues for future exploration in computational efficiency and interpretability.
Supporting Arguments
1. Novelty and Contribution: The ISAN's linear dynamics and input-switched architecture represent a novel approach to language modeling, emphasizing interpretability without sacrificing performance on the studied task. The ability to decompose predictions and analyze latent state dynamics in arbitrary bases is a key strength.
2. Scientific Rigor: The paper provides a thorough analysis of the ISAN, including comparisons with fully linear RNNs and non-linear architectures. The experiments are well-executed, and the insights into the model's behavior are compelling.
3. Research Dialogue: While the results on Text8 are not state-of-the-art, the paper's focus on interpretability and efficiency addresses an important gap in the literature, making it a valuable contribution to the field.
Suggestions for Improvement
1. Dataset Choice: The reliance on the Text8 dataset, which is relatively uncommon, limits the generalizability of the findings. Future work should include evaluations on more widely used benchmarks or datasets with rigid combinatorial structures to better contextualize the results.
2. Comparison with Non-Linear Models: While the ISAN is compared to non-linear RNNs on Text8, additional experiments on tasks with rigid combinatorial structures (e.g., parentheses counting) would provide deeper insights into its strengths and limitations.
3. Related Work: The discussion of related sequence models should be expanded. Specifically, the paper should include a detailed comparison with Belanger and Kakade (2015), whose work on linear dynamics and singular vector analysis is directly relevant. A more explicit connection to Linear Dynamical Systems (LDS), including concepts like bias vectors and Kalman gain matrices, would further strengthen the theoretical grounding.
4. Kalman Smoothing: Exploring Kalman smoothing in the ISAN, where future observations are used for state inference, could enhance its applicability and interpretability.
5. Sparse/Convex Combinations: Introducing sparse or convex combinations of dictionary matrices could improve the scalability and interpretability of the ISAN for word-level tasks.
Questions for the Authors
1. How does the ISAN perform on datasets with more complex combinatorial structures, such as the parentheses counting task, compared to non-linear RNNs?
2. Can the caching mechanism for affine transformations be practically implemented for large-scale language models, and what are the trade-offs in terms of memory usage and computational overhead?
3. How would the ISAN handle continuous-valued inputs, and what modifications to the architecture would be required to scale to larger vocabularies or word-level modeling?
In conclusion, the ISAN is a compelling contribution to the field of interpretable sequence modeling, and the paper's insights into linear dynamics are likely to inspire further research. While there are areas for improvement, the work is well-positioned to advance the dialogue on interpretable and efficient recurrent architectures.