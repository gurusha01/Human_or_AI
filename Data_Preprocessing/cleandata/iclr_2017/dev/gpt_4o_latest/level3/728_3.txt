Review of the Paper
Summary of Contributions
The paper introduces the Semi-Aggregated Markov Decision Process (SAMDP) model, a novel approach for analyzing trained reinforcement learning (RL) policies by leveraging spatiotemporal abstractions. SAMDP combines concepts from Semi-MDPs (SMDPs) and Aggregated MDPs (AMDPs) to simplify the analysis of complex policies by clustering states and identifying higher-level skills. The authors propose a modified k-means clustering algorithm that incorporates temporal coherence and demonstrate the utility of SAMDP in both toy problems (e.g., gridworld) and real-world scenarios (e.g., Atari2600 games). The paper highlights SAMDP's potential for improving policy interpretability, monitoring agent performance, and enhancing robustness in high-stakes applications such as robotics and shared autonomy systems.
Decision: Reject  
While the paper presents an interesting idea with potential, it falls short in several critical areas, including clarity, novelty, and empirical rigor. These shortcomings limit its impact and suitability for acceptance at this time.
Supporting Arguments for Decision
1. Clarity and Presentation: The paper is difficult to follow due to dense writing, excessive use of acronyms, and insufficiently detailed explanations of key concepts. The lack of clear algorithms, formulas, and intuitive descriptions makes it challenging to grasp the proposed method. Improved formatting and a more structured presentation would significantly enhance readability.
   
2. Novelty and Positioning: Although SAMDP is positioned as a novel approach, its distinction from existing hierarchical RL methods and prior work on SMDPs and AMDPs is not sufficiently emphasized. The paper does not clearly articulate how SAMDP advances the state of the art or addresses gaps in the literature.
3. Empirical Evaluation: The evaluation section lacks depth and rigor. While the authors provide qualitative insights and some quantitative metrics (e.g., VMSE, entropy), the results are not comprehensive enough to convincingly demonstrate the method's effectiveness. For example, comparisons with baseline methods (e.g., standard SMDPs or AMDPs) are missing, and the evaluation is limited to a small set of tasks.
4. Practical Utility: The paper claims SAMDP can improve policy robustness and interpretability, but these claims are not well-supported by experimental evidence. The "eject button" experiment is intriguing but lacks sufficient detail and broader validation to establish its practical utility.
Suggestions for Improvement
1. Clarity and Accessibility: Simplify the writing, reduce acronyms, and include clear algorithms and formulas to explain the SAMDP construction process. Use diagrams and pseudocode to illustrate key steps, such as the modified k-means clustering and skill identification.
2. Positioning and Novelty: Clearly differentiate SAMDP from existing hierarchical RL methods and provide a detailed comparison with related work. Highlight the unique contributions of SAMDP and its advantages over traditional SMDPs and AMDPs.
3. Empirical Rigor: Expand the evaluation to include comparisons with baseline methods and additional tasks. Provide more detailed quantitative results and statistical analyses to validate SAMDP's performance and robustness claims.
4. Practical Applications: Strengthen the discussion of SAMDP's practical utility, particularly in high-consequence systems. Provide concrete examples or case studies to illustrate how SAMDP can be applied in real-world scenarios.
Questions for the Authors
1. How does SAMDP compare quantitatively with traditional SMDPs and AMDPs in terms of interpretability and performance metrics?
2. Could you provide more details on the modified k-means clustering algorithm, including its computational complexity and sensitivity to hyperparameters (e.g., window size, number of clusters)?
3. How consistent are the SAMDP models when built from different trajectories or under varying levels of noise in the data?
4. Can SAMDP be extended to continuous state-action spaces, and if so, what modifications would be required?
5. How does SAMDP handle stochastic environments or policies with significant exploration noise?
While the SAMDP framework shows promise, addressing the above concerns and providing a more rigorous and accessible presentation would significantly strengthen the paper.