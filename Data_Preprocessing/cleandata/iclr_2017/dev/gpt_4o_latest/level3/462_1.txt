Review
Summary of Contributions
The paper proposes a novel approach to detecting adversarial examples by augmenting deep neural networks with a small "detector" subnetwork. This detector is trained to distinguish between genuine and adversarially perturbed inputs, showing that adversarial perturbations, despite being quasi-imperceptible to humans, can be reliably identified. The authors demonstrate that the detector generalizes well to similar or weaker adversarial examples, and they introduce a training method to counteract dynamic adversaries that target both the classifier and the detector. The simplicity of the approach, combined with its non-trivial implications, offers a promising new direction for defensive systems in machine learning.
Decision: Accept
The paper makes a significant contribution to the field of adversarial robustness by introducing a complementary detection mechanism that is orthogonal to existing defenses. While there are limitations, particularly in addressing dynamic adversaries and the need for improved clarity, the novelty and potential impact of the work justify acceptance.
Supporting Arguments
1. Novelty and Contribution: The idea of using a detector subnetwork to identify adversarial examples is simple yet innovative. The empirical results, particularly the detector's ability to generalize across adversaries, highlight its practical value. The introduction of dynamic adversary training is another noteworthy contribution that strengthens the system's robustness.
   
2. Scientific Rigor: The experiments are comprehensive, covering both static and dynamic adversaries, and are conducted on standard datasets like CIFAR-10 and a subset of ImageNet. The results are compelling, showing high detectability of adversarial examples and reasonable generalization across adversaries.
3. Potential Impact: The proposed method could be a valuable addition to safety-critical applications, where detecting adversarial attacks is crucial. The approach also opens up avenues for future research, such as using the detector's gradients for regularization or developing stronger adversaries.
Suggestions for Improvement
1. Clarity and Completeness: The paper would benefit from a more detailed discussion of dynamic adversaries and their implications. Specifically, the authors should elaborate on the challenges posed by dynamic adversaries and provide a deeper analysis of the limitations of their current approach.
2. Training Details: The training methodology for the detector, particularly for dynamic adversary training, needs further explanation. For instance, how does the random selection of the parameter Ïƒ affect convergence, and what are the implications for scalability to larger datasets?
3. Broader Evaluation: While the results on CIFAR-10 and a subset of ImageNet are promising, evaluating the method on additional datasets or tasks (e.g., speech recognition or natural language processing) would strengthen the paper's claims about generalizability.
4. Theoretical Insights: The paper could benefit from a theoretical analysis of why adversarial perturbations are detectable. While the authors provide some discussion based on the boundary tilting perspective, a more formal treatment would enhance the paper's scientific depth.
Questions for the Authors
1. How does the detector's performance scale with increasing dataset complexity or higher-resolution images? Are there any computational bottlenecks in training or inference?
2. Have you considered adversaries that employ randomization or non-gradient-based methods? How would the detector perform against such attacks?
3. Could the detector be combined with other defense mechanisms, such as adversarial training or input preprocessing, to further enhance robustness?
In conclusion, while there are areas for improvement, the paper makes a significant and novel contribution to adversarial robustness research. Its simplicity, empirical validation, and potential impact make it a valuable addition to the field.