Review
Summary of Contributions
The paper introduces the Semi-Aggregated Markov Decision Process (SAMDP) model, aimed at analyzing trained reinforcement learning (RL) policies by leveraging spatial and temporal abstractions. The authors propose a novel combination of the Aggregated MDP (AMDP) and Semi-MDP (SMDP) frameworks, which simplifies policy analysis by reducing the dimensionality of the state space and the planning horizon. SAMDP is constructed using spatiotemporal clustering, with features derived from the transformed state space. The paper demonstrates the utility of SAMDP through experiments on gridworld and Atari2600 games, showcasing its ability to monitor and improve policy robustness. A key contribution is the use of SAMDP in shared autonomy systems, where it identifies suboptimal agent behavior and enables human intervention. The authors also provide evaluation metrics to assess the quality of SAMDP models and highlight potential directions for future work.
Decision: Reject
While the paper addresses an important problem in RL—interpretable policy analysis through spatiotemporal abstractions—it falls short in several critical areas. The primary reasons for rejection are (1) insufficient justification for key abstraction choices and (2) a lack of formalization and clarity in the methodology, which hinders reproducibility and scientific rigor.
Supporting Arguments
1. Motivation and Importance: The paper is well-motivated, addressing the pressing need for interpretable RL models, especially in safety-critical applications. However, while the SAMDP framework is an interesting direction, the restrictive abstraction choices (e.g., single initiation/termination states, clustering, and temporal coherence) are not adequately justified. These choices may limit the generality and scalability of the approach.
   
2. Methodological Gaps: The paper lacks formal definitions and precise mathematical descriptions for key components, such as AMDP, SAMDP, and the clustering algorithm. Algorithmic details are sparse, making it difficult to evaluate the correctness and reproducibility of the proposed method. For example, the clustering process and its temporal coherence modifications are described only qualitatively, with critical details deferred to the supplementary material, which is incomplete.
3. Experimental Evaluation: While the experiments on gridworld and Atari games are well-executed, the significance and generalizability of the results remain unclear. The reliance on specific design choices (e.g., t-SNE for feature extraction) raises concerns about the robustness of SAMDP across diverse RL tasks. Additionally, the computational and space overhead of constructing SAMDPs is not explicitly addressed, leaving questions about its scalability.
4. Presentation Issues: Minor typos and errors (e.g., a wrong sign in the equation after Eq. 2) detract from the paper's overall polish. The supplementary material, which is referenced for critical explanations, is incomplete, further reducing the paper's clarity.
Suggestions for Improvement
1. Justification of Abstraction Choices: Provide a stronger theoretical or empirical basis for the restrictive assumptions in SAMDP, such as single initiation/termination states and the specific clustering approach. Consider exploring more flexible abstractions to enhance generalizability.
2. Formalization and Clarity: Include formal definitions, mathematical descriptions, and pseudocode for SAMDP construction and evaluation. This will improve the paper's rigor and reproducibility.
3. Overhead Analysis: Explicitly discuss the computational and space overhead of constructing SAMDPs, particularly for large-scale RL problems.
4. Supplementary Material: Ensure the supplementary material is complete and provides detailed explanations for all referenced design choices and clustering methods.
5. Experimental Diversity: Evaluate SAMDP on a broader range of RL tasks, including continuous-action environments, to demonstrate its general applicability.
Questions for the Authors
1. How sensitive is the SAMDP model to the choice of feature extraction method (e.g., t-SNE)? Have you considered alternatives, and how do they impact the results?
2. Can you provide a quantitative analysis of the computational and space overhead involved in constructing SAMDPs for large-scale environments?
3. How does SAMDP handle stochastic environments or policies with high variability? Would the deterministic assumptions in the inference stage break down in such cases?
4. Could you elaborate on the rationale behind the specific clustering modifications for temporal coherence? How do they compare to other clustering methods in terms of performance?
In summary, while the SAMDP framework is a promising step toward interpretable RL, the paper requires significant improvements in methodological rigor, justification of design choices, and clarity to meet the standards of the conference.