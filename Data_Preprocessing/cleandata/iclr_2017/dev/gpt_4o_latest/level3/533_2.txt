Review
Summary of Contributions
This paper proposes a surprise-based intrinsic reward mechanism for reinforcement learning (RL) and introduces two practical estimation algorithms: surprisal and k-step learning progress. The authors aim to address the challenge of exploration in sparse-reward environments by formulating surprise as the KL-divergence between true transition probabilities and a learned dynamics model. The proposed methods are computationally efficient and scalable, and the paper demonstrates their application across a variety of continuous control tasks and Atari RAM games. The authors claim that their methods achieve competitive performance with state-of-the-art approaches, such as Variational Information Maximizing Exploration (VIME), while being computationally less expensive.
Decision: Reject
While the paper presents an interesting and computationally efficient approach to intrinsic motivation in RL, the contribution is incremental and falls short in several critical areas. The performance of the proposed methods is inconsistent across tasks, and the claims of substantial speed-up over VIME are not strongly supported by numerical evidence. Furthermore, the novelty of the approach is limited, as it closely resembles prior work in intrinsic motivation, particularly VIME.
Supporting Arguments
1. Problem and Motivation: The paper addresses an important problem—efficient exploration in sparse-reward environments—and situates itself well in the context of the literature on intrinsic motivation. However, the proposed methods are conceptually similar to VIME, and the novelty is primarily in the implementation details (e.g., avoiding Bayesian neural networks and using simpler dynamics models). This limits the paper's contribution to the field.
2. Empirical Results: The experiments demonstrate that the surprisal-based method performs robustly across tasks, but it is often outperformed by VIME in terms of overall performance. The k-step learning progress method is less consistent and struggles in certain environments, such as Swimmer. While the authors highlight computational efficiency as a key advantage, the reported speed-up (a factor of 3) is modest and not rigorously analyzed across diverse settings.
3. Claims and Evidence: The claim of "massive speed-up" is overstated, as the analysis in Appendix C suggests that the theoretical speed-up is highly dependent on batch size and hardware parallelism. Additionally, the paper does not provide a thorough comparison of computational costs in real-world scenarios, which weakens the argument for the practical utility of the proposed methods.
Suggestions for Improvement
1. Stronger Empirical Validation: Provide more comprehensive comparisons with VIME and other baseline methods across a wider range of environments. Include metrics such as sample efficiency and computational cost in real-world settings to substantiate the claims of speed-up.
2. Theoretical Insights: Offer a deeper theoretical analysis of the proposed methods, particularly the conditions under which surprisal and k-step learning progress are expected to outperform existing approaches. This would strengthen the paper's contribution beyond empirical results.
3. Novelty and Positioning: Clearly articulate the conceptual differences between the proposed methods and prior work, such as VIME. Highlight any unique insights or advantages that the proposed methods bring to the field, beyond computational efficiency.
4. Numerical Evidence: Include detailed numerical results to support the claim of computational efficiency. For instance, compare wall-clock training times for VIME and the proposed methods across multiple tasks and hardware setups.
Questions for the Authors
1. How does the performance of the proposed methods scale with the complexity of the environment (e.g., increasing state/action space dimensions or stochasticity)?
2. Can you provide more detailed numerical evidence to support the claim of computational efficiency, particularly in real-world scenarios?
3. What are the key factors that contribute to the inconsistent performance of the k-step learning progress method across tasks? How might these issues be addressed?
In conclusion, while the paper presents a computationally efficient approach to intrinsic motivation, the incremental nature of the contribution and the lack of strong empirical and numerical support for the claims make it unsuitable for acceptance in its current form.