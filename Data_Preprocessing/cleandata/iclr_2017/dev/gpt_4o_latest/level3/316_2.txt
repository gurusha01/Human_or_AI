Review of the Paper
The paper introduces the Private Aggregation of Teacher Ensembles (PATE) framework, a novel privacy-preserving approach for machine learning that combines multiple "teacher" models trained on disjoint datasets with a "student" model trained via noisy voting. The authors claim that PATE provides strong differential privacy guarantees for sensitive training data while maintaining high utility, as demonstrated on MNIST and SVHN datasets. The approach is generalizable, independent of the underlying learning algorithms, and achieves state-of-the-art privacy/utility trade-offs. The use of semi-supervised learning and improved privacy analysis are key contributions.
Decision: Accept
The decision to accept this paper is based on two primary reasons:  
1. Novelty and Practical Relevance: The PATE framework addresses a critical issue in machine learning—protecting sensitive training data—using an intuitive and theoretically grounded approach. Its general applicability and independence from specific learning algorithms make it a valuable contribution to the field.  
2. Empirical Validation: The experiments on MNIST and SVHN demonstrate strong privacy guarantees and competitive accuracy, showcasing the practical utility of the proposed method.
Supporting Arguments
1. Problem Definition and Motivation: The paper tackles the important problem of privacy-preserving machine learning, particularly for sensitive data such as medical records. The motivation is well-grounded in recent privacy attacks and the limitations of existing methods, making the work timely and impactful.  
2. Theoretical and Empirical Rigor: The theoretical analysis, while relying on empirical parameters, is intuitive and builds on established differential privacy techniques. The empirical results on MNIST and SVHN are robust, with clear improvements over prior work in privacy/utility trade-offs.  
3. Generality and Applicability: The framework's independence from specific learning algorithms and its applicability to non-convex models like deep neural networks enhance its versatility. The use of semi-supervised learning further strengthens its practical utility.  
Suggestions for Improvement
1. Testing on Sensitive Data: While the experiments on MNIST and SVHN are solid, testing on real-world sensitive datasets, such as medical records, would significantly strengthen the paper's claims and demonstrate its applicability in high-stakes domains.  
2. Clarity on Security Guarantees: The theoretical guarantees rely on empirical parameters, which may limit their robustness. A more detailed discussion of the potential limitations and conditions under which the guarantees hold would improve the paper's rigor.  
3. Comparison with Related Work: While the paper discusses related work, a more detailed comparison of privacy bounds and accuracy with state-of-the-art methods, especially on datasets beyond MNIST and SVHN, would provide a clearer picture of the framework's advantages.  
Questions for the Authors
1. How does the PATE framework perform on datasets with inherently noisy or imbalanced data, such as medical records or financial transactions?  
2. Can the authors provide more details on the scalability of the approach, particularly for tasks with a large number of output classes or limited availability of non-sensitive auxiliary data?  
3. How sensitive are the privacy guarantees to the choice of the Laplacian noise parameter, and how can practitioners determine an optimal setting for their use case?  
Overall, this paper presents a significant contribution to privacy-preserving machine learning and has the potential to inspire further research in this critical area.