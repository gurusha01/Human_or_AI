This paper introduces an end-to-end speech recognition system that combines convolutional neural networks (ConvNets) with a novel sequence training criterion, termed AutoSegCriterion (ASG). The system eliminates the need for phonetic alignment and leverages grapheme-based outputs, simplifying the traditional HMM/GMM-based pipeline. The authors demonstrate competitive results on the LibriSpeech dataset, achieving a word error rate (WER) of 7.2% with MFCC features and promising performance with raw waveform inputs. A key contribution is the ASG criterion, which removes the need for a blank symbol (as in CTC) and directly computes sequence-level loss, offering computational efficiency and flexibility. The paper also highlights the simplicity of its beam-search decoder and its ability to integrate unnormalized acoustic scores.
Decision: Reject
While the paper presents an interesting and promising approach, it lacks sufficient rigor and clarity in several critical areas. The absence of baseline comparisons with hybrid NN/HMM systems and state-of-the-art models (e.g., Deep Speech 2) limits the ability to contextualize the results. Additionally, Table 2 does not clearly demonstrate how the proposed system compares to existing benchmarks, leaving the reader uncertain about its relative performance. Furthermore, the paper does not adequately discuss its relationship to LF-MMI, a well-known sequence training approach, despite clear conceptual overlaps. These omissions weaken the paper's positioning in the broader literature.
Supporting Arguments:
1. Strengths: The use of a purely ConvNet-based approach for end-to-end speech recognition is novel and computationally efficient. The ASG criterion is a notable contribution, simplifying the training process while achieving performance comparable to CTC. The exploration of raw waveform inputs is also a valuable addition to the field.
2. Weaknesses: The lack of baseline comparisons with hybrid NN/HMM systems and state-of-the-art models is a significant oversight. Additionally, while the authors claim computational efficiency, the discrepancy between theoretical and empirical results (e.g., faster training despite higher complexity) raises questions about the implementation details, such as sampling rate and step size. The paper would benefit from experiments with larger step sizes (e.g., 30ms or 40ms) to validate its claims of efficiency.
Suggestions for Improvement:
1. Baseline Comparisons: Include results comparing the proposed system to hybrid NN/HMM systems and state-of-the-art models. This would provide a clearer understanding of the system's performance.
2. Clarify Table 2: Clearly indicate how the proposed system compares to existing benchmarks, including Deep Speech 2 and other end-to-end systems.
3. Theoretical Discussion: Discuss the relationship between ASG and LF-MMI in greater depth, providing references and highlighting similarities and differences.
4. Efficiency Analysis: Provide a detailed explanation of why the proposed method is faster than frame-level CTC training despite its theoretical complexity. Experimenting with larger step sizes could further validate computational efficiency claims.
Questions for the Authors:
1. How does the proposed system compare to hybrid NN/HMM systems in terms of WER and computational efficiency?
2. Can you provide a more detailed explanation of the implementation that leads to faster training despite the higher theoretical complexity of ASG?
3. How does the ASG criterion perform with larger step sizes (e.g., 30ms or 40ms), and does this impact computational cost or WER?
In summary, while the paper introduces a promising approach, it requires stronger contextualization, clearer comparisons, and additional experiments to substantiate its claims.