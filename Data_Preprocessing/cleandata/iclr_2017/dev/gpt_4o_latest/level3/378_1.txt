Review of the Paper
Summary of Contributions
This paper introduces a novel exploration strategy for reinforcement learning (RL) called Under-Appreciated Reward Exploration (UREX). UREX leverages importance sampling to encourage exploration of action sequences that are under-appreciated by the current policy, defined as sequences where the log-probability underestimates the resulting reward. The method is a simple modification of the REINFORCE algorithm and is shown to outperform both REINFORCE and Q-learning on a set of algorithmic tasks, including multi-digit addition and binary search. The paper claims that UREX improves robustness to hyper-parameters, reduces sensitivity to reward sparsity, and achieves better generalization to longer sequences. Notably, the authors highlight that UREX is the first RL method to solve multi-digit addition using only reward feedback.
Decision: Accept
The decision to accept is based on the novelty of the proposed exploration strategy, its demonstrated empirical improvements over baseline methods, and its contribution to advancing RL in sparse reward settings. However, the paper has some limitations, particularly in the evaluation setup, which should be addressed in future revisions.
Supporting Arguments
1. Novelty and Simplicity: UREX introduces a principled and computationally efficient modification to REINFORCE, making it accessible for practitioners. The use of importance sampling to identify under-appreciated action sequences is a creative approach to directed exploration.
   
2. Empirical Performance: The experiments demonstrate that UREX significantly outperforms REINFORCE and Q-learning on challenging algorithmic tasks, particularly in sparse reward settings. The ability to solve multi-digit addition and generalize to sequences of up to 2000 digits is a noteworthy achievement.
3. Robustness: The paper provides evidence that UREX is more robust to hyper-parameter variations compared to REINFORCE, which is a practical advantage for RL applications.
Suggestions for Improvement
1. Motivation and Clarity: The paper's motivation is somewhat unclear. It is not evident whether the primary goal is to improve exploration for policy gradient methods or to achieve better performance on algorithmic tasks. Clarifying this distinction would strengthen the narrative.
2. Evaluation on Standard Benchmarks: The experiments are limited to toy algorithmic tasks with small action spaces. While these tasks are challenging, the lack of evaluation on standard RL benchmarks (e.g., Atari, MuJoCo) limits the generalizability of the findings. Future work should include comparisons on larger action spaces and more diverse environments.
3. Baselines: The baselines used in the experiments (REINFORCE and one-step Q-learning) are relatively weak. Incorporating stronger baselines, such as Proximal Policy Optimization (PPO) or Soft Actor-Critic (SAC), would provide a more rigorous evaluation.
4. Variance Reduction: The authors acknowledge that the variance of importance weights can be high during early training. Incorporating variance-reduction techniques, such as baselines or reward normalization, could improve stability and performance.
Questions for the Authors
1. How does UREX perform on tasks with larger action spaces or continuous action spaces? Would the importance sampling approach scale effectively in such scenarios?
2. Can UREX be combined with variance-reduction techniques, such as advantage estimation, to further stabilize training?
3. How does UREX compare to modern exploration methods, such as intrinsic motivation or curiosity-driven exploration, in sparse reward settings?
Conclusion
The paper presents a novel and effective exploration strategy that addresses a critical challenge in RL: exploration in sparse reward environments. While the evaluation could be more comprehensive, the contributions are significant and warrant acceptance. The proposed UREX method has the potential to inspire further research in exploration strategies for RL.