Review
Summary of Contributions
This paper proposes a simple yet effective method for sentence embeddings, leveraging a reweighting scheme called Smooth Inverse Frequency (SIF) and a common component removal step using PCA. The method is entirely unsupervised and demonstrates significant performance improvements over unweighted baselines and even some supervised models like RNNs and LSTMs on textual similarity tasks. The authors provide a theoretical justification for their method using a latent variable generative model, extending prior work by Arora et al. (2016). They also show that their approach can enhance existing embeddings, such as those from GloVe and PSL, and is robust across different corpora and parameter settings. The paper argues that this method should serve as a strong baseline for future work, especially in low-resource or domain adaptation scenarios.
Decision: Reject
While the paper introduces an interesting and practical method, several critical issues undermine its clarity and scientific rigor. The lack of precise definitions, incomplete justifications for key components, and insufficient explanation of results make it difficult to fully assess the contribution's validity and impact.
Supporting Arguments for Decision
1. Ambiguity in Terminology: The term "discourse" is used extensively but ambiguously throughout the paper. It is unclear whether it refers to syntactic, semantic, or contextual information, and this lack of clarity affects the interpretability of the theoretical model and its connection to the proposed method.
   
2. Unjustified Model Components: The introduction of the common component \( c0 \), which is said to relate to syntax, lacks a clear theoretical or empirical justification. The authors claim to have "discovered" \( c0 \) empirically but do not provide sufficient evidence or explanation for its role in improving embeddings.
3. Unclear Explanation of Results: The results in Table 2, particularly for sentiment analysis, are inadequately discussed. The authors acknowledge that their method underperforms on sentiment tasks due to the downweighting of critical words like "not," but they do not propose any solutions or explore alternative weighting schemes.
4. Unclear Method Discovery: The statement about discovering the new model through detecting the common component \( c_0 \) is vague and does not provide a clear methodological pathway. This weakens the credibility of the theoretical justification.
5. Connection to Prior Work: While the paper links its method to prior work (e.g., TF-IDF, Word2Vec subsampling), the connections are not explored in sufficient depth. For example, the discussion of Word2Vec subsampling as a stochastic approximation of the proposed weighting scheme is intriguing but lacks a rigorous comparative analysis.
Suggestions for Improvement
1. Clarify Terminology: Define "discourse" explicitly and consistently, and explain how it is modeled in different contexts (e.g., syntactic vs. semantic).
2. Justify \( c0 \): Provide a theoretical or empirical basis for the inclusion of \( c0 \) and its relationship to syntax. For example, analyze the impact of \( c_0 \) on performance across different tasks and datasets.
3. Expand on Results: Discuss the results in greater detail, particularly for Table 2. Explain why the method performs well on textual similarity tasks but struggles with sentiment analysis, and propose potential remedies.
4. Improve Theoretical Clarity: Clearly articulate the process of discovering \( c_0 \) and its integration into the generative model. Provide more rigorous evidence for the theoretical claims.
5. Broaden Comparisons: Include a more comprehensive comparison with other reweighting schemes (e.g., TF-IDF) and supervised methods, especially in terms of computational efficiency and robustness.
Questions for the Authors
1. How exactly was the common component \( c_0 \) discovered, and why is it assumed to relate to syntax? Can you provide empirical evidence to support this claim?
2. Why does the method perform poorly on sentiment analysis tasks? Could alternative weighting schemes address this issue?
3. How does the proposed method compare to existing reweighting techniques like TF-IDF in terms of computational cost and scalability?
By addressing these issues, the paper could significantly improve its clarity, rigor, and impact.