Review
This paper introduces a set of techniques to compress wide and shallow text classification models, focusing on reducing memory usage while maintaining classification accuracy. The proposed methods include optimized product quantization (OPQ), vocabulary pruning, and hashing, which are applied to the fastText library. The authors claim their approach achieves a memory reduction of up to 1000x with minimal accuracy loss, outperforming state-of-the-art methods in terms of the trade-off between memory usage and accuracy. The paper also promises to release the code as part of the fastText library, aiming to provide reproducible baselines for compact text classifiers.
Decision: Reject
Key Reasons for Rejection:
1. Marginal Machine Learning Contributions: While the problem of compressing neural models is important, the proposed techniques—optimized product quantization, vocabulary pruning, and hashing—are not novel. OPQ, in particular, has been extensively studied in prior work, and the vocabulary pruning approach relies on straightforward heuristics (L2 norm and coverage) without significant innovation.
2. Weak Experimental Validation: The experiments lack convincing results and focus on uncommon benchmarks. The paper does not evaluate its methods on state-of-the-art recurrent neural network (RNN) models or more widely recognized benchmarks, leaving the broader implications unclear. Additionally, critical baselines, such as relaxed binary coefficients with L1 regularization or subword-based vocabulary reduction, are missing.
Supporting Arguments:
- Limited Novelty: The use of OPQ for inner product approximation is well-documented in the literature, and the vocabulary pruning approach is a straightforward application of existing heuristics. The hashing trick and Bloom filters are also standard techniques in text classification.
- Experimental Gaps: The paper does not compare its methods against practical baselines, such as subword-based models, which are known to reduce vocabulary size effectively. Furthermore, the lack of evaluation on modern RNN or transformer-based models makes it difficult to assess the relevance of the proposed techniques in contemporary NLP.
- Uncommon Benchmarks: While the paper evaluates its methods on datasets from Zhang et al. (2015) and FlickrTag, these benchmarks are not widely used to assess state-of-the-art text classification models, limiting the generalizability of the results.
Suggestions for Improvement:
1. Broader Evaluation: Evaluate the proposed methods on state-of-the-art RNN or transformer-based models and widely recognized benchmarks (e.g., AG News, IMDB, or GLUE tasks). This would provide stronger evidence of the methods' applicability to modern NLP.
2. Stronger Baselines: Include comparisons with baselines that use relaxed binary coefficients with L1 regularization or subword-based vocabulary reduction. These are practical and competitive approaches for reducing model size.
3. Clarify Contributions: Clearly distinguish the novel aspects of the proposed methods from prior work. For example, if the bottom-up retraining strategy for OPQ is a key contribution, this should be emphasized and rigorously evaluated.
4. Theoretical Insights: Provide theoretical justifications for the proposed heuristics, such as the use of L2 norm and coverage for vocabulary pruning. This would strengthen the scientific rigor of the paper.
5. Practical Implications: Discuss the trade-offs between memory savings and computational overhead introduced by the proposed methods, particularly in real-world deployment scenarios.
Questions for the Authors:
1. How do the proposed methods compare to subword-based approaches for vocabulary reduction in terms of both memory savings and accuracy?
2. Why were state-of-the-art RNN or transformer-based models not included in the experiments? Could the proposed techniques be extended to these architectures?
3. How does the computational overhead of the proposed methods (e.g., hashing, retraining after quantization) compare to the baseline fastText model?
In conclusion, while the paper addresses an important problem and demonstrates significant memory savings, the lack of novelty, weak experimental validation, and limited scope of evaluation make it unsuitable for acceptance in its current form. Addressing the aforementioned issues could significantly strengthen the paper.