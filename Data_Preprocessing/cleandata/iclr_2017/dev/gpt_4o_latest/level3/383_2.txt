Review of "MetaQNN: Reinforcement Learning for Neural Architecture Search"
Summary of Contributions
The paper introduces MetaQNN, a reinforcement learning-based approach for automating the design of convolutional neural network (CNN) architectures. By employing Q-learning with an ε-greedy exploration strategy and experience replay, the method sequentially selects CNN layers to discover high-performing architectures. MetaQNN is evaluated on CIFAR-10, SVHN, and MNIST datasets, demonstrating competitive performance against state-of-the-art handcrafted and automated architectures. The paper also highlights the transferability of discovered architectures to other tasks, showcasing their potential for broader applications. The work represents a significant step towards automating neural architecture design, reducing reliance on human expertise.
Decision: Reject
While the paper presents a promising idea with strong empirical results, the high computational cost and lack of scalability to larger or more diverse datasets limit its practical applicability. Additionally, the paper would benefit from further experiments to address concerns about low-data regimes and architectural generalization.
Supporting Arguments for Decision
1. Strengths:
   - The proposed method is well-motivated, addressing the challenge of manual CNN design by leveraging reinforcement learning.
   - Results on CIFAR-10, SVHN, and MNIST are competitive, outperforming existing automated methods and even some handcrafted architectures.
   - The use of Q-learning with experience replay is novel in the context of neural architecture search (NAS), and the analysis of Q-values provides valuable insights into layer selection.
2. Weaknesses:
   - High Computational Cost: The method requires 8–10 days on 10 GPUs for relatively small datasets, making it impractical for larger datasets or real-world applications. This issue is not addressed adequately in the paper.
   - Scalability Concerns: The paper does not explore the scalability of MetaQNN to larger images or datasets with more complex characteristics. This omission raises questions about the method's generalizability.
   - Low-Data Regimes: The lack of experiments on datasets like Caltech-101, which represent low-data regimes, limits the paper's claim of broad applicability. Demonstrating the ability to discover competitive architectures for such datasets would strengthen the work.
   - Incomplete Comparisons: ResNet architectures, which are widely used benchmarks, are missing from the comparison tables, making it difficult to fully contextualize the results.
Suggestions for Improvement
1. Address Computational Cost: Explore methods to reduce the computational burden, such as leveraging proxy tasks, early stopping, or more efficient search strategies.
2. Evaluate Scalability: Conduct experiments on larger datasets (e.g., ImageNet) or higher-resolution images to demonstrate the method's scalability.
3. Low-Data Regimes: Include experiments on Caltech-101 or similar datasets to validate the approach's performance in low-data settings.
4. Comparison with ResNets: Add ResNet architectures to the performance comparison tables to provide a more comprehensive evaluation.
5. Broader Analysis: Provide insights into why certain architectures perform better and whether the discovered designs align with known best practices in CNN design.
Questions for the Authors
1. How does the method scale to larger datasets or higher-resolution images? Are there any plans to optimize the computational cost?
2. Can the approach be adapted for low-data regimes, where overfitting is a significant concern? If so, how?
3. Why were ResNet architectures excluded from the comparison tables? Would the inclusion of ResNets alter the conclusions of the paper?
While the paper presents a strong conceptual foundation, addressing the above concerns would significantly enhance its impact and practical relevance.