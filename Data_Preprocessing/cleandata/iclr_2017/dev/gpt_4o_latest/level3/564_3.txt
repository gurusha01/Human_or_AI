Review
Summary of Contributions
The paper introduces Higher Order Recurrent Neural Networks (HORNNs), an extension of traditional RNNs designed to better model long-term dependencies in sequential data. By incorporating multiple past hidden states as feedback through weighted paths, HORNNs aim to address the vanishing gradient problem and enhance the short-term memory mechanism. The authors evaluate HORNNs on language modeling tasks using the Penn Treebank (PTB) and English text8 datasets, claiming state-of-the-art performance. Additionally, the paper explores pooling functions (e.g., max-based, FOFE-based, and gated pooling) to normalize feedback signals and improve learning efficiency.
Decision: Reject
The paper is rejected primarily due to (1) its incremental contribution with limited novelty and (2) weak empirical and theoretical support for its claims.
Supporting Arguments
1. Incremental Contribution:  
   While the proposed HORNN architecture introduces weighted feedback paths to aggregate multiple past hidden states, this idea is not novel. Similar concepts, such as hierarchical RNNs and gated feedback mechanisms, have been explored in prior works (e.g., LSTMs, GRUs, and clockwork RNNs). The paper does not sufficiently differentiate HORNNs from these existing models, nor does it provide compelling evidence that the proposed approach represents a significant advancement.
2. Weak Analysis:  
   The paper lacks rigorous theoretical analysis to justify why HORNNs are better suited for modeling long-term dependencies compared to existing architectures. The experimental results, while promising, are not robust enough to substantiate the claims. For example, the reported improvements in perplexity are marginal and could be attributed to hyperparameter tuning or dataset-specific optimizations rather than the proposed architecture.
3. Baseline Comparisons:  
   The baseline models used for comparison (e.g., standard RNNs and older LSTM implementations) are outdated and do not reflect the current state of the art. More recent and competitive baselines, such as Transformer-based models or modern LSTM variants with dropout regularization, should have been included to validate the claims.
4. Writing Issues:  
   The paper's writing is verbose and unfocused, particularly in the introduction and abstract. Misleading statements, such as overly harsh criticisms of LSTMs (e.g., "complicated and slow to learn"), undermine the paper's credibility. Additionally, the experimental section lacks clarity, making it difficult to assess the validity of the results.
Suggestions for Improvement
1. Clarify Novelty:  
   Clearly articulate how HORNNs differ from and improve upon existing architectures like hierarchical RNNs, clockwork RNNs, or gated feedback RNNs. Highlight the unique contributions of the proposed pooling functions and justify their necessity.
2. Stronger Baselines:  
   Include comparisons with more competitive baselines, such as modern LSTM implementations with dropout, GRUs, or Transformer-based models. This would provide a more convincing case for the effectiveness of HORNNs.
3. Theoretical Justification:  
   Provide a deeper theoretical analysis of HORNNs, particularly regarding their ability to mitigate the vanishing gradient problem and capture long-term dependencies. This could include mathematical proofs or visualizations of gradient flow during backpropagation.
4. Writing Improvements:  
   Streamline the introduction and abstract to focus on the key contributions. Avoid hyperbolic or misleading statements about existing models. Ensure that the experimental methodology and results are presented clearly and concisely.
Questions for the Authors
1. How do HORNNs compare to more recent architectures, such as Transformers or modern LSTM variants with regularization techniques?  
2. What specific challenges do HORNNs address that are not already solved by LSTMs or GRUs?  
3. Can you provide a more detailed analysis of the computational complexity and scalability of HORNNs, particularly for larger datasets or tasks beyond language modeling?  
4. How sensitive are the results to hyperparameter choices, such as the order of HORNNs or the type of pooling function used?  
While the paper presents an interesting idea, it falls short in terms of novelty, rigor, and clarity. Addressing these issues could significantly strengthen the work for future submissions.