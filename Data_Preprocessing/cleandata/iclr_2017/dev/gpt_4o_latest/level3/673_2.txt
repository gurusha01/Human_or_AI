Review of the Paper
Summary of Contributions
The paper proposes a novel hierarchical memory network (HMN) that leverages Maximum Inner Product Search (MIPS) for scalable memory access in large-scale tasks, such as factoid question answering. The authors argue that this approach serves as a hybrid between soft and hard attention mechanisms, aiming to combine the scalability of hard attention with the ease of training associated with soft attention. The key contributions include: (1) introducing K-MIPS-based attention for memory networks, (2) demonstrating that exact K-MIPS improves accuracy compared to soft attention, and (3) exploring approximate K-MIPS techniques for scalability, albeit with some performance trade-offs. The authors present experimental results on the SimpleQuestions dataset, comparing exact and approximate K-MIPS methods, and propose strategies to mitigate the biases introduced by approximation.
Decision: Reject  
Key Reasons:  
1. Limited Practical Utility: The proposed approximate K-MIPS methods, while faster, result in significant performance degradation, undermining the paper's primary goal of achieving both scalability and accuracy.  
2. Insufficient Experimental Rigor: The paper lacks critical evaluations, such as speed comparisons with alternative fast nearest neighbor methods (e.g., FLANN), and does not provide a comprehensive analysis of the trade-offs between speed and accuracy.
Supporting Arguments
1. Fixed Memory Slots and Adaptability: The use of K-MIPS inherently limits the adaptability of the memory network, particularly for tasks requiring multi-hop reasoning or dynamic memory updates. The paper acknowledges this limitation but does not propose a solution, leaving the approach less suitable for complex reasoning tasks.  
2. Performance Degradation with Approximation: While exact K-MIPS demonstrates improved accuracy over soft attention, approximate K-MIPS—intended to address scalability—suffers from significant performance loss. This trade-off is not convincingly justified, and the proposed strategies to reduce approximation bias yield only marginal improvements.  
3. Critique of Heuristics: The paper critiques dataset-dependent heuristics but introduces its own non-data-dependent heuristics (e.g., clustering-based K-MIPS), which are not convincingly superior. This undermines the motivation for the proposed approach.  
4. Experimental Gaps: The absence of speed comparisons with established fast nearest neighbor methods (e.g., FLANN) weakens the claim of scalability. Additionally, the paper does not explore the impact of memory size variations or provide a detailed analysis of computational complexity.
Suggestions for Improvement
1. Address Adaptability: Explore mechanisms to make the memory network more adaptable, such as dynamic memory updates or multi-hop reasoning capabilities, to broaden the applicability of the approach.  
2. Comprehensive Benchmarking: Include speed and accuracy comparisons with alternative methods like FLANN or other fast nearest neighbor techniques to substantiate claims of scalability.  
3. Mitigate Approximation Bias: Investigate more sophisticated approximate K-MIPS algorithms or hybrid approaches that balance speed and accuracy more effectively.  
4. Expand Experimental Scope: Evaluate the approach on additional datasets and tasks to demonstrate generalizability. Include ablation studies to isolate the impact of each proposed component.  
5. Clarify Motivation: Provide a stronger justification for the use of approximate K-MIPS over existing methods, addressing the trade-offs more explicitly.
Questions for the Authors
1. How does the proposed method compare in terms of speed and accuracy to state-of-the-art fast nearest neighbor methods like FLANN?  
2. Can the hierarchical memory structure be adapted dynamically during training or inference to improve adaptability?  
3. What is the impact of memory size on the performance of both exact and approximate K-MIPS methods?  
4. How does the proposed approach perform on tasks requiring multi-hop reasoning or more complex memory access patterns?  
5. Could more advanced clustering techniques or hybrid methods (e.g., combining clustering with hashing) reduce the performance degradation observed with approximate K-MIPS?
In its current form, the paper presents an interesting idea but fails to convincingly demonstrate its practical utility or superiority over existing methods. Addressing the above concerns could significantly strengthen the contribution and impact of the work.