Review
Summary
This paper proposes a novel Variational Autoencoder (VAE)-based framework for topic modeling, termed Autoencoded Variational Inference for Topic Models (AVITM). The authors address key challenges in applying autoencoding variational Bayes (AEVB) to latent Dirichlet allocation (LDA), such as handling the Dirichlet prior and mitigating component collapsing. The framework is computationally efficient, achieving faster inference through amortized inference, and is adaptable to new topic models with minimal effort. The paper also introduces ProdLDA, a new topic model that replaces LDA's mixture model with a product of experts, resulting in significantly improved topic coherence. Experimental results demonstrate that AVITM matches traditional inference methods in accuracy while being much faster, and ProdLDA outperforms LDA in topic quality.
Decision: Accept
The paper makes a strong case for acceptance due to its significant contributions to the field of topic modeling. The proposed AVITM framework is both innovative and practical, addressing long-standing challenges in applying variational inference to topic models. The introduction of ProdLDA demonstrates the potential of the framework to enable new and improved topic models. The empirical results are compelling, showing both computational efficiency and improved topic coherence.
Supporting Arguments
1. Well-Defined Problem and Motivation: The paper clearly identifies the challenges of applying AEVB to LDA, including the difficulty of handling the Dirichlet prior and the issue of component collapsing. The motivation for a black-box inference method is well-articulated, and the proposed solutions are logical and innovative.
2. Empirical Rigor: The experiments are thorough, comparing AVITM and ProdLDA against established baselines (e.g., mean-field variational inference, collapsed Gibbs sampling, and NVDM). The results convincingly demonstrate the computational efficiency and improved topic coherence of the proposed methods.
3. Broader Impact: The black-box nature of AVITM makes it highly adaptable, lowering the barrier for practitioners to experiment with new topic models. The introduction of ProdLDA highlights the framework's potential for advancing the state of the art in topic modeling.
Suggestions for Improvement
1. Notation in Equation 5: The notation \( P(\theta(h)|\alpha) \) is awkward and could be simplified to \( P(h|\alpha) \) for clarity, as suggested in the review guidelines.
2. Document Length Sensitivity: The recognition model's behavior appears to vary with document length, which could be undesirable. Normalizing the input representations could mitigate this issue and improve robustness.
3. Relationship to Exponential Family PCA: The paper briefly mentions that ProdLDA is related to exponential family PCA but does not explore this connection in depth. A more detailed discussion could provide additional theoretical insights.
4. Clarity of Results: While the results are promising, the paper could benefit from a clearer presentation of the quantitative improvements in topic coherence and computational efficiency. For instance, including more detailed tables or visualizations would enhance the reader's understanding of the results.
Questions for the Authors
1. How does the performance of AVITM scale with larger datasets and higher numbers of topics? Are there any limitations to its scalability?
2. Can the authors elaborate on the choice of the Laplace approximation for the Dirichlet prior? How does it compare to alternative approximations in terms of accuracy and computational efficiency?
3. Have the authors considered the potential impact of document preprocessing (e.g., stopword removal, vocabulary size) on the performance of ProdLDA? Would the model's coherence scores hold across different preprocessing pipelines?
Overall, this paper makes a substantial contribution to the field of topic modeling and provides a practical framework for advancing research in this area. With minor revisions, it has the potential to be a highly impactful addition to the conference.