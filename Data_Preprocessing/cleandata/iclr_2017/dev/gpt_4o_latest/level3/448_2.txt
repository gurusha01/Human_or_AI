Review of the Paper
Summary of Contributions
This paper provides a significant theoretical advancement in understanding the behavior of deep random neural networks by extending the mean-field approximation. The authors introduce depth scales that govern information propagation, analyze the effects of dropout, and develop a corresponding mean-field model for backpropagation. Their findings reveal the critical role of the order-to-chaos transition in determining the trainability of deep networks, with the depth scales providing bounds on the maximum trainable depth. The paper also highlights how dropout disrupts the critical point, limiting trainable depth. The theoretical results are validated through numerical experiments, which demonstrate strong alignment between theory and empirical observations. Overall, the paper offers a comprehensive framework for predicting trainability based on network architecture and hyperparameters, contributing both theoretical insights and practical implications for neural network design.
Decision: Accept
The paper is well-suited for acceptance due to its rigorous mathematical analysis, clear exposition, and strong empirical validation. It addresses a fundamental problem in deep learning—the relationship between network depth, trainability, and initialization—while providing a universal framework applicable across datasets and architectures. The novelty of extending mean-field theory to include dropout and backpropagation further strengthens its contribution.
Supporting Arguments
1. Specific Problem Tackled: The paper addresses the critical problem of understanding how information propagates through deep random neural networks and how this impacts trainability. By identifying depth scales and analyzing their dependence on hyperparameters, the authors provide a theoretical foundation for designing trainable networks.
   
2. Motivation and Placement in Literature: The work builds upon prior research (e.g., Poole et al., 2016) on mean-field theory and the order-to-chaos transition, extending it to include dropout and backpropagation. The authors position their work well within the context of existing literature, addressing gaps and offering novel insights.
3. Scientific Rigor and Validation: The paper provides a thorough mathematical analysis, deriving key results with clarity and precision. The theoretical claims are supported by numerical experiments, which show strong agreement with predictions. The experiments are well-designed, and the results are robust across datasets, training methods, and hyperparameters.
Additional Feedback for Improvement
1. Clarity of Exposition: While the paper is well-written, certain sections, such as the derivation of depth scales and the impact of dropout, could benefit from additional intuitive explanations to make the results more accessible to a broader audience.
   
2. Practical Implications: While the theoretical framework is compelling, the paper could elaborate further on how practitioners might leverage these insights in real-world scenarios, such as designing architectures for specific tasks or optimizing training pipelines.
3. Limitations and Future Work: The authors briefly mention that their framework does not apply to unbounded activations like ReLU. Expanding on this limitation and discussing potential extensions to convolutional or residual networks would strengthen the paper's impact.
Questions for the Authors
1. How sensitive are the depth scales to variations in the activation function beyond the examples considered in the paper? Could the framework be extended to unbounded activations like ReLU?
2. The paper suggests that networks closer to criticality train more effectively. Could you provide more insights into how this observation might guide hyperparameter tuning in practice?
3. Have you considered the implications of your framework for structured architectures like convolutional or transformer networks, where weight matrices are not fully connected?
In conclusion, this paper makes a valuable contribution to the theoretical understanding of deep neural networks and provides actionable insights for their design and training. With minor refinements, it has the potential to become a cornerstone in the study of information propagation and trainability in deep learning.