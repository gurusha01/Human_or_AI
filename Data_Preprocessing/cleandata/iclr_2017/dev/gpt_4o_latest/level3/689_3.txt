Review of the Paper
Summary of Contributions:
This paper introduces Tensorial Mixture Models (TMMs), a novel generative model designed to address classification problems with missing data. TMMs extend conventional mixture models by incorporating a "priors tensor" to capture dependencies between local structures, such as image patches. The authors propose using tensor decomposition techniques (CP and Hierarchical Tucker) to make the model tractable, transforming it into a Convolutional Arithmetic Circuit (ConvAC). The paper highlights TMMs' ability to perform tractable marginalization, enabling optimal classification regardless of the missingness distribution. Empirical results on MNIST and NORB datasets demonstrate the model's robustness to missing data and its performance advantages over competing methods.
Decision: Reject
Key Reasons for Rejection:
1. Unclear Theoretical Interpretations: The paper's interpretation of CP and HT decompositions on the priors tensor is unclear and potentially flawed. This undermines the theoretical foundation of the proposed method.
2. Dimensional Inconsistencies: The generative model in Figure 5 exhibits dimensional inconsistencies, and the authors incorrectly use multiplication where a Tucker decomposition is more appropriate.
3. Presentation and Technical Errors: The paper contains several technical typos (e.g., in the diagonal Gaussian Mixture Model example), which obscure key derivations and reduce the clarity of the work.
Supporting Arguments:
1. Theoretical Concerns: The CP and HT decompositions are central to the tractability of TMMs. However, the paper does not convincingly explain how these decompositions align with the probabilistic interpretation of the priors tensor. This lack of clarity raises doubts about the validity of the proposed approach.
2. Dimensional Issues: The dimensional inconsistencies in Figure 5 suggest a lack of rigor in the mathematical formulation of the generative model. This weakens the credibility of the results and the overall contribution.
3. Impact of Convolution Operators: The use of convolution operators for inner product computation might compromise the invariance properties of CNNs, although the practical implications remain uncertain. This trade-off is not adequately discussed.
Additional Feedback for Improvement:
1. Sample Complexity Analysis: The authors should provide a detailed discussion of the sample complexity of TMMs, given the model's high complexity due to its tensorial structure.
2. Fix Technical Errors: Address the typos and inconsistencies in the derivations, particularly in the diagonal Gaussian Mixture Model example. This will improve the clarity and rigor of the paper.
3. Empirical Validation: While the results on MNIST and NORB are promising, additional experiments on more complex datasets would strengthen the empirical validation of the model.
4. Discussion on ConvAC Limitations: The authors should explicitly discuss the potential limitations of using ConvACs, particularly regarding the trade-offs between expressivity and invariance.
Questions for the Authors:
1. Can you clarify the probabilistic interpretation of the priors tensor under CP and HT decompositions? How does this align with the generative nature of TMMs?
2. Why was a Tucker decomposition not used in Figure 5, given its suitability for addressing dimensional inconsistencies?
3. How does the use of convolution operators affect the invariance properties of the model? Have you tested this empirically?
Conclusion:
While the paper introduces innovative ideas and demonstrates promising results, the theoretical and presentation issues significantly detract from its overall quality. Addressing these concerns could make the paper a strong candidate for future acceptance.