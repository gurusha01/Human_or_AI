The paper introduces a simple yet effective probabilistic approach for sentence embeddings, leveraging a weighted average of word vectors followed by principal component removal. This method, termed Smooth Inverse Frequency (SIF), is computationally efficient and unsupervised, making it particularly valuable in domain adaptation and low-resource settings. The authors provide both empirical evidence and theoretical justification for their approach, demonstrating significant improvements over unweighted baselines and even outperforming some supervised methods like RNNs and LSTMs on textual similarity tasks. The paper also highlights the robustness of the method across different corpora and parameter settings, further solidifying its utility as a baseline for future research.
Decision: Reject.  
While the paper presents an interesting and practical contribution, it falls short in several critical areas. The lack of comparison to Wang and Manning's work is a significant oversight, as their NB-SVM method is a well-established baseline for tasks like sentiment analysis. Additionally, the proposed method underperforms NB-SVM on large datasets, which is disappointing given the claims of its generalizability. Furthermore, the issue of large co-occurrence counts, addressed by Glove embeddings' weighting function, is not sufficiently differentiated in this work. These gaps weaken the paper's positioning within the broader literature.
Supporting Arguments:  
1. Strengths: The method is simple, interpretable, and computationally efficient, making it a strong candidate for unsupervised settings. The theoretical grounding provided through the generative model is a valuable addition, offering insights into why the method works. The empirical results on textual similarity tasks are compelling, showing consistent improvements over baselines.  
2. Weaknesses: The omission of a direct comparison to Wang and Manning's NB-SVM is a critical flaw, as it limits the paper's ability to contextualize its contributions. The underperformance on large datasets raises questions about the scalability and robustness of the method. Additionally, the novelty of addressing large co-occurrence counts is diminished by the existing work on Glove embeddings, which already tackles this issue effectively.
Suggestions for Improvement:  
1. Include a detailed comparison to Wang and Manning's NB-SVM method, particularly on tasks like sentiment analysis, to better position the contribution within the existing literature.  
2. Address the underperformance on large datasets by exploring potential enhancements to the weighting scheme or incorporating task-specific adaptations.  
3. Clarify how the proposed method differs from or improves upon Glove's weighting function for large co-occurrence counts.  
4. Fix minor issues such as the typo in the introduction ("The capturing the similarities") and use `\citet` for the Wieting et al. (2016) citation to adhere to proper formatting guidelines.
Questions for the Authors:  
1. Can you provide a more detailed explanation of why the proposed method underperforms on large datasets compared to NB-SVM?  
2. How does the method handle tasks where word order is critical, such as sentiment analysis?  
3. Could the weighting parameter `a` be learned dynamically for better performance across diverse datasets?  
4. How does the proposed method compare to other unsupervised baselines like TF-IDF in tasks beyond textual similarity, such as classification or clustering?
While the paper has potential, addressing these gaps is essential to strengthen its contribution and impact.