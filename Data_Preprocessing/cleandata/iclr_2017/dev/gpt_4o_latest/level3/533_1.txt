Review of the Paper
Summary of Contributions
This paper introduces a novel approach to surprise-based intrinsic motivation in deep reinforcement learning (RL). The authors propose a framework that uses the KL-divergence between the true transition probabilities and a learned dynamics model to generate intrinsic rewards. Two tractable approximations are developed: one based on surprisal and the other on k-step learning progress. The paper demonstrates that these intrinsic rewards improve exploration efficiency in sparse reward environments, outperforming several heuristic exploration techniques and achieving competitive results with state-of-the-art methods like VIME. The proposed approach is computationally efficient and validated across diverse tasks, including continuous control and Atari RAM games.
Decision: Accept
The paper presents a well-motivated and novel contribution to intrinsic motivation in RL, with empirical results supporting its claims. However, the lack of thorough comparisons with recent work on challenging benchmarks like Montezuma's Revenge is a notable limitation. Despite this, the paper's contributions are significant and merit acceptance.
Supporting Arguments
1. Novelty and Motivation: The formulation of surprise as the KL-divergence between true and learned transition probabilities is a novel and theoretically grounded approach. The authors clearly differentiate their method from prior work, such as Bellemare et al. (2016) and Stadie et al. (2016), and address limitations in computational cost and scalability.
   
2. Empirical Validation: The experiments demonstrate that the proposed intrinsic rewards improve exploration in sparse reward environments, outperforming naive exploration and other heuristic methods. The results are robust across diverse tasks, including continuous control and discrete action domains.
3. Computational Efficiency: The proposed method is computationally efficient, with a reported speedup of up to 3x compared to VIME. This is a significant advantage for scaling intrinsic motivation methods to complex environments.
4. Clarity and Organization: The paper is well-written and provides detailed descriptions of the methodology, implementation, and experimental setup, making it easy to reproduce the results.
Suggestions for Improvement
1. Comparisons with Bellemare et al. (2016): The paper does not evaluate its method on challenging benchmarks like Montezuma's Revenge, where intrinsic motivation methods like pseudo-counts have shown dramatic improvements. Including such comparisons would strengthen the empirical claims.
2. Analysis of Failure Cases: While the paper discusses some failure cases (e.g., premature convergence of the dynamics model in MountainCar), a more detailed analysis of when and why the proposed method underperforms (e.g., on CartpoleSwingup compared to VIME) would be valuable.
3. Broader Benchmark Coverage: The evaluation is limited to a subset of Atari games and continuous control tasks. Expanding the benchmark suite to include more diverse and challenging environments would provide a more comprehensive assessment of the method's generality.
4. Theoretical Guarantees: While the paper provides a strong empirical foundation, a discussion of theoretical guarantees (e.g., convergence properties of the intrinsic rewards) would enhance its impact.
Questions for the Authors
1. How does the method perform on environments with extremely sparse rewards, such as Montezuma's Revenge? Are there specific challenges in applying the proposed approach to such tasks?
2. Can the authors provide more insights into the choice of hyperparameters, particularly the KL-divergence step size (κ) and the bonus coefficient (η0)? How sensitive are the results to these parameters?
3. The paper mentions that surprisal incentives may lead to noise-seeking behavior in the limit. Could this be mitigated by combining surprisal with other intrinsic motivation signals, such as novelty or empowerment?
Conclusion
This paper makes a significant contribution to the field of intrinsic motivation in deep RL by proposing a novel, computationally efficient framework for surprise-based exploration. While there are areas for improvement, the strengths of the paper outweigh its limitations, and it is a valuable addition to the conference.