Review of "Predictive Reinforcement Learning"
This paper proposes a model-based reinforcement learning (RL) approach, Predictive Reinforcement Learning (PRL), which predicts rewards and life-loss probabilities to achieve superhuman performance in three Atari games (Breakout, Pong, and Demon Attack). The authors claim that PRL not only performs well in single-task setups but also benefits from multi-task learning, demonstrating transfer learning capabilities. The paper introduces a novel recurrent neural network architecture, the Residual Recurrent Neural Network (RRNN), designed to decouple memory from computation, which is argued to be particularly effective for modeling environments with minimal memory requirements.
Decision: Reject
While the paper presents promising ideas and achieves notable results, it lacks sufficient scientific rigor and clarity in several critical areas. The primary reasons for rejection are: (1) insufficient evidence to support the claims of model improvement and architecture necessity, and (2) a lack of proper contextualization within the existing RL literature.
Supporting Arguments for Rejection:
1. Model Learning Concerns: The paper does not provide evidence that the model is improving during training. It lacks exploratory experiments, such as predictions from specific start states or training loss on a holdout set, to validate the model's predictive capabilities. This omission raises questions about the robustness of the proposed approach.
2. Architecture Justification: The necessity of the RRNN architecture is not convincingly demonstrated. The authors claim that RRNN outperforms LSTMs in initial tests but do not provide comparative results or ablation studies to substantiate this claim. Simpler architectures might suffice, but this is unexplored.
3. Literature Context: The paper fails to acknowledge foundational works in model-based RL, such as Dyna (Sutton, 1990) and Lin & Mitchell's approaches. This omission weakens the motivation for the proposed method and its placement within the broader RL landscape.
4. Ad-hoc Design: The combination of predicted rewards and life-loss probabilities using game-specific rules is not generalizable. A unified Q-value or learned policy would have been more scientifically rigorous.
5. Overfitting and Memory Decoupling: The claims about memory decoupling and overfitting risks are inconsistent with the described RNN-based approach. The paper does not adequately address how overfitting is mitigated or provide evidence to support the decoupling claim.
Additional Feedback for Improvement:
- Notation: The paper uses inconsistent and unconventional notation for observations, rewards, and actions, which hampers readability. Standardizing notation would improve clarity.
- Exploratory Experiments: Including ablation studies, comparisons with simpler architectures, and visualizations of predictions (e.g., from specific start states) would strengthen the paper's claims.
- Reference Errors: The paper incorrectly cites Morimoto et al. instead of Ioffe and Szegedy for Batch Normalization. This should be corrected.
- Observation 1: The proof of Observation 1 appears unnecessary. Citing the layer normalization paper (Ba et al., 2016) would suffice for motivation.
- Code Release: The authors state that the code will be released before ICLR 2017, but no link is provided. Ensuring code availability would enhance reproducibility.
Questions for the Authors:
1. Can you provide quantitative evidence (e.g., training loss on a holdout set) to demonstrate that the model is improving during training?
2. Why is the RRNN architecture necessary? Could simpler alternatives, like standard LSTMs, achieve similar results?
3. How does your approach compare to foundational model-based RL methods like Dyna? Why were these works not discussed in the introduction?
4. How do you address the risk of overfitting when using a fixed dataset for training? Have you considered online learning or data augmentation techniques?
In summary, while the paper introduces interesting ideas and achieves promising results, it lacks the experimental rigor, contextualization, and clarity required for acceptance. Addressing these issues in future work could significantly improve the paper's contribution to the field.