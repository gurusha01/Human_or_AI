Review of the Paper
Summary of Contributions
This paper introduces a novel reinforcement learning (RL) exploration strategy, termed Under-Appreciated Reward Exploration (UREX). UREX aims to address the challenge of sparse rewards in high-dimensional environments by encouraging exploration of action sequences that are under-appreciated by the current policy. The method is a simple modification to the REINFORCE algorithm, leveraging importance sampling to balance exploitation and exploration. The authors demonstrate the effectiveness of UREX on a set of algorithmic tasks, including multi-digit addition and binary search, where it outperforms entropy-regularized REINFORCE (MENT) and double Q-learning. Notably, UREX is shown to generalize to longer input sequences and exhibits robustness to hyper-parameter changes, reducing the tuning burden. The paper also introduces a new benchmark task, BinarySearch, to encourage further research on algorithmic tasks.
Decision: Accept
The paper is recommended for acceptance due to its novel and well-motivated contribution to RL exploration strategies, its clear empirical advantages over baseline methods, and its potential to inspire further research in RL for algorithmic tasks. However, some areas of improvement are noted, particularly in experimental depth and clarity of reward descriptions.
Supporting Arguments
1. Novelty and Motivation: The proposed UREX method is a novel and principled approach to exploration in RL, addressing a critical limitation of existing methods in sparse reward settings. The use of under-appreciated rewards as a mechanism for directed exploration is both innovative and well-motivated.
   
2. Empirical Performance: The experimental results convincingly demonstrate that UREX outperforms or matches baseline methods on challenging tasks, particularly in environments with sparse rewards and long action sequences. The ability to solve multi-digit addition and generalize to sequences of up to 2000 digits is a significant achievement.
3. Robustness: The method's robustness to hyper-parameter changes is a practical advantage, as hyper-parameter tuning is often a significant bottleneck in RL research.
4. Clarity and Simplicity: The proposed method is a minor modification to REINFORCE, making it easy to implement and integrate into existing RL pipelines.
Suggestions for Improvement
1. Reward Description: The paper lacks consistency in describing the reward structure. The distinction between sparse and per-step rewards should be clarified earlier in the text to avoid confusion.
   
2. Hyper-parameter Exploration: The limited experimentation with the temperature parameter (\(\tau\)) is a concern. Testing a broader range of \(\tau\) values or employing random search instead of grid search could provide stronger evidence of robustness.
3. Comparison with Value-Based Exploration: While UREX addresses the exploration question effectively, the lack of comparison with value-function-based exploration methods (e.g., intrinsic motivation or curiosity-based approaches) is a missed opportunity to position the method more comprehensively within the literature.
4. Experimental Depth: The empirical evaluation could be strengthened with additional experiments on toy tasks or more diverse environments to better illustrate the method's generality. For example, tasks with continuous action spaces or real-world applications could highlight the broader applicability of UREX.
5. Reward-Trajectory Imbalance: The potential issue of trajectory length affecting exploration is noted but not thoroughly addressed. Future work could explore reward shaping techniques to mitigate this imbalance.
Questions for the Authors
1. How does UREX compare to intrinsic motivation or curiosity-driven exploration methods in terms of performance and computational cost?
2. Could the authors provide more insights into the failure cases of UREX, particularly on tasks like BinarySearch where the agent does not fully generalize to logarithmic complexity?
3. How sensitive is UREX to the choice of the reward baseline (\(b(h)\))? Would alternative baseline strategies improve performance?
4. Have the authors considered applying UREX to tasks with continuous action spaces or real-world RL benchmarks?
Conclusion
This paper makes a valuable contribution to the RL exploration literature by introducing a novel and effective method for directed exploration. While there are areas for improvement, the strengths of the proposed approach and its empirical results justify its acceptance. The work is likely to inspire further research in RL for algorithmic tasks and exploration strategies.