Review of the Paper
Summary of Contributions
This paper addresses the critical question of why deep convolutional networks outperform shallow ones by exploring the role of pooling in modeling correlations among input regions. It introduces the concept of separation rank to quantify function complexity relative to input partitions and demonstrates that deep networks can achieve exponentially high separation ranks for favored partitions, while shallow networks are limited to linear ranks. The theoretical analysis is conducted on convolutional arithmetic circuits, with empirical validation extending to convolutional rectifier networks. The paper also highlights how pooling geometry influences the inductive bias, enabling networks to model specific types of correlations efficiently. This insight is experimentally validated using synthetic tasks, showing how different pooling schemes optimize performance for distinct tasks.
Decision: Reject
While the paper addresses an important problem and provides novel insights into the role of pooling and depth, it falls short in several critical areas. The theoretical results are overly focused on convolutional arithmetic circuits, limiting their general applicability to more widely used architectures. Additionally, the analysis of deep vs. shallow networks is misleading, as it primarily contrasts pooling vs. no pooling. The lack of illustrative examples for max pooling and insufficient discussion on the practical implications of depth further weaken the paper's impact.
Supporting Arguments
1. Strengths:
   - The paper tackles a fundamental and underexplored question about the inductive biases of deep networks.
   - The use of separation rank to formalize correlations is innovative and provides a new lens to understand network behavior.
   - The empirical validation, particularly the experiments with different pooling geometries, demonstrates the practical relevance of the theoretical findings.
2. Weaknesses:
   - The theoretical results rely heavily on product pooling, raising concerns about their applicability to networks with max pooling, which is more common in practice. The lack of illustrative examples for max pooling limits the paper's relevance.
   - The comparison between deep and shallow networks is not entirely fair, as shallow networks lack hierarchical pooling, making the analysis more about pooling vs. no pooling rather than depth per se.
   - The tensor analysis is terse and assumes familiarity with prior work, making it inaccessible to a broader audience.
   - The paper does not sufficiently address why depth is crucial in practical settings, especially given the success of very deep networks in real-world applications.
Suggestions for Improvement
1. Provide illustrative examples and theoretical extensions for max pooling to make the results more broadly applicable.
2. Clarify the distinction between depth and pooling in the analysis to avoid conflating the two concepts.
3. Include a more detailed discussion of how the findings align with practitioners' success using very deep networks, particularly in the context of modern architectures like ResNets or Transformers.
4. Expand the tensor analysis with additional explanations or an appendix summarizing the necessary background to make the paper more accessible.
5. Explore hybrid pooling geometries or combinations of pooling schemes to extend the practical implications of the findings.
Questions for the Authors
1. How do the results generalize to architectures with max pooling or average pooling, which are more commonly used than product pooling?
2. Can you provide empirical evidence or theoretical insights on how separation rank behaves in very deep networks (e.g., ResNets)?
3. How does the choice of pooling geometry interact with other architectural decisions, such as skip connections or attention mechanisms?
In conclusion, while the paper makes valuable contributions to understanding the role of pooling and depth, it requires significant revisions to broaden its applicability and strengthen its practical relevance.