Review
Summary of Contributions
This paper investigates the impact of different action parameterizations on the learning performance and robustness of control policies for dynamic articulated figure locomotion using deep reinforcement learning (DeepRL). The authors compare four actuation models—torques (Tor), muscle activations (MTU), target joint angles (PD), and target joint velocities (Vel)—on a gait-cycle imitation task across multiple planar characters and gaits. The paper demonstrates that higher-level action parameterizations incorporating local feedback, such as PD and Vel, significantly improve learning speed, policy robustness, and motion quality compared to low-level torque control. The authors also propose a heuristic optimization approach for MTU parameters and provide a visually appealing video showcasing the results. The work contributes to understanding the role of action representations in DeepRL and highlights the importance of co-designing actuation mechanics and control strategies.
Decision: Reject
While the paper is interesting and provides valuable insights into the effect of action parameterizations, its limited generalizability and restricted applicability to broader tasks weaken its overall impact. The results are primarily demonstrated on planar simulations and a specific reward function, leaving open questions about their applicability to more complex 3D scenarios or diverse tasks. Additionally, the slight performance advantage of proportional control (PD) raises concerns about whether it stems from inherent biases in the reward function rather than fundamental differences in parameterizations.
Supporting Arguments
1. Strengths:
   - The paper is well-written, straightforward, and presents clear results, making it accessible to readers.
   - The experiments are methodologically sound, with a thorough comparison of four action parameterizations across multiple metrics (learning speed, robustness, motion quality, and query rates).
   - The accompanying video effectively demonstrates the policies' performance, adding clarity to the results.
2. Weaknesses:
   - The generalizability of the results is limited. The findings may not hold for tasks beyond locomotion, such as spinning a top or tasks requiring impedance control, as acknowledged by the authors.
   - The use of a tracking cost in the reward function may inherently favor PD and Vel parameterizations, raising questions about the neutrality of the evaluation framework.
   - The experiments are restricted to planar simulations, leaving the extension to 3D scenarios as future work. This limitation significantly reduces the paper's applicability to real-world robotics or animation tasks.
   - The slight advantage of PD over other parameterizations is not convincingly explained, especially given the known function for converting between representations. This raises questions about whether the observed differences are meaningful or artifacts of the experimental setup.
Suggestions for Improvement
1. Expand Generalizability: Extend the experiments to 3D articulated figures and evaluate the parameterizations on a broader range of tasks, including those requiring non-locomotive control (e.g., manipulation or spinning tasks).
2. Reward Function Analysis: Provide a detailed analysis of how the reward function influences the results. Consider testing alternative reward functions, such as those focused solely on locomotion costs, to assess the robustness of the findings.
3. Clarify PD Advantage: Investigate and explain why PD parameterization shows a slight advantage despite the existence of a known function for converting between representations.
4. Broader Applicability: Discuss how the findings could inform the design of control systems in real-world robotics or animation, particularly in scenarios with complex actuation mechanics.
Questions for the Authors
1. How would the results change if the tracking cost in the reward function were replaced with a locomotion-only cost? Would PD and Vel still outperform Tor and MTU?
2. Can you provide more insight into why PD parameterization shows a slight advantage? Could this be an artifact of the reward function or experimental setup?
3. Have you considered testing the parameterizations on tasks beyond locomotion, such as manipulation or spinning tasks? If so, what were the results?
4. How do you envision extending this work to 3D scenarios, and what challenges do you anticipate in doing so?
In conclusion, while the paper presents interesting findings, the limited scope and generalizability of the results prevent it from making a strong contribution to the field. Addressing the above concerns could significantly enhance the paper's impact and relevance.