Review of "Hierarchical Multiscale Recurrent Neural Networks"
Summary of Contributions
This paper introduces a novel recurrent neural network (RNN) architecture, the Hierarchical Multiscale Recurrent Neural Network (HM-RNN), which is designed to learn hierarchical structures in sequential data without requiring explicit boundary information. The model employs a layered architecture with discrete boundary detectors that enable operations such as COPY, UPDATE, and FLUSH, allowing it to capture different levels of abstraction in temporal data. The authors address the challenge of training with discrete variables using the straight-through estimator and introduce a slope annealing trick to improve training stability. Experimental results demonstrate state-of-the-art performance on the Text8 dataset for character-level language modeling, competitive results on the Penn Treebank and Hutter Prize Wikipedia datasets, and superior performance on handwriting sequence generation tasks. Qualitative results also illustrate the model's ability to discover natural hierarchical boundaries in data.
Decision: Accept
The paper presents a compelling and novel contribution to the field of sequential data modeling, with strong experimental results and a well-motivated approach. However, the decision to accept is contingent on addressing minor issues related to writing quality, attribution errors, and missing references.
Supporting Arguments
1. Novelty and Significance: The HM-RNN introduces a unique mechanism for learning hierarchical structures in sequential data without explicit boundary information, which is a longstanding challenge in RNNs. The use of discrete boundary detectors and operations like COPY and FLUSH is innovative and well-justified.
2. Empirical Validation: The model achieves state-of-the-art results on the Text8 dataset and demonstrates competitive performance on other benchmarks. The qualitative analysis further supports the claim that the model can capture meaningful hierarchical structures.
3. Scientific Rigor: The paper provides a detailed description of the model, training techniques, and experimental setup, ensuring reproducibility and clarity.
Suggestions for Improvement
1. Writing and Attribution Issues:
   - The introduction makes claims about the human brain's hierarchical learning without sufficient evidence or citations. Foundational references such as Mikolov et al. (2010) and Lin et al. (1996) are missing.
   - The related work section misattributes the introduction of gradient clipping to Pascanu et al. (2012), while it was first introduced by Mikolov et al. (2010). Similarly, the lineage of the CW-RNN model is inaccurately described.
   - Several important references, such as Fern√°ndez et al. (2007) and Ring (1993), are omitted, which weakens the contextualization of the proposed work.
2. Clarity and Writing Style:
   - The paper's writing could benefit from more concise explanations, particularly in the technical sections. For instance, the description of the COPY, UPDATE, and FLUSH operations is overly detailed and could be streamlined.
   - Some sections, such as the introduction and related work, lack a coherent narrative, making it harder to follow the progression of ideas.
3. Evaluation and Analysis:
   - While the experimental results are strong, the paper could include additional baselines or ablation studies to isolate the contributions of specific components, such as the slope annealing trick or the boundary detector mechanism.
   - The visualization of hierarchical boundaries is insightful, but further quantitative analysis of the discovered boundaries (e.g., comparison with ground truth segmentation) would strengthen the claims.
Questions for the Authors
1. How sensitive is the model's performance to the choice of hyperparameters, such as the slope annealing schedule or the number of layers?
2. Can the proposed HM-RNN generalize to other types of sequential data, such as speech or video, where hierarchical structures are less explicit?
3. How does the computational efficiency of the HM-RNN compare to other multiscale models, particularly in terms of training time and memory usage?
In conclusion, the HM-RNN is a promising and well-executed contribution to the field of hierarchical sequence modeling. Addressing the minor issues outlined above will further strengthen the paper and its impact.