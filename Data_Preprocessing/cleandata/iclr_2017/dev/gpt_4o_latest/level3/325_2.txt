Review
Summary of Contributions
This paper introduces a novel training method for generative models based on iterative denoising, where a Markov chain progressively refines unstructured noise into high-quality samples. The proposed "infusion training" technique biases the training chain toward the target data distribution, enabling the model to learn a transition operator that effectively denoises input samples over a small number of steps. The method addresses key challenges in generative modeling, such as mode collapse in GANs and the need for efficient sampling. Experimental results demonstrate competitive performance across multiple datasets, including MNIST, CIFAR-10, and CelebA, with qualitative evaluations showing sharp and varied sample generation. The paper also highlights the simplicity of its approach compared to GANs, as it requires only a single network and avoids adversarial training instability.
Decision: Accept
The paper makes a significant contribution to generative modeling by proposing a novel and practical training method that is both conceptually simple and empirically effective. While there are areas for improvement, such as theoretical guarantees and more comprehensive comparisons, the overall value of the work warrants acceptance.
Supporting Arguments
1. Core Contribution and Novelty: The infusion training approach is an innovative extension of denoising-based generative models, offering a simpler alternative to adversarial training and diffusion-based methods. The paper builds on prior work (e.g., Sohl-Dickstein et al., 2015) but introduces key distinctions, such as fewer denoising steps and an adaptive training chain.
2. Empirical Validation: The experiments are thorough, covering multiple datasets and evaluation metrics (e.g., log-likelihood bounds, Inception scores). The generated samples are visually compelling, and the inpainting results demonstrate the model's ability to handle structured outputs.
3. Research Importance: The method addresses critical challenges in generative modeling, such as efficient sampling and multi-modal distribution learning, making it a valuable addition to the field.
Areas for Improvement
1. Theoretical Guarantees: The paper lacks formal guarantees regarding the convergence of the Markov chain or the impact of parameter choices (e.g., infusion rate). Establishing connections to MCMC or energy-based models could strengthen the theoretical foundation.
2. Comparison to Alternatives: While the paper compares its method to GANs and diffusion-based models, direct quantitative comparisons to other state-of-the-art methods, such as Hamiltonian Monte Carlo with variational inference, are missing.
3. Clarity in Training Details: The roles of parameters like alpha and omega in the infusion chain are not fully explained. Additional experiments to analyze their impact would improve reproducibility and understanding.
4. Visualization: The paper would benefit from visualizations of the infusion and sampling chains, particularly for complex datasets like CIFAR-10 and CelebA, to better illustrate the model's behavior.
Questions for the Authors
1. How sensitive is the method to the choice of infusion rate (alpha) and its schedule? Could you provide more detailed ablation studies on this parameter?
2. How does the method perform when scaled to higher-resolution datasets or more complex architectures, such as those used in state-of-the-art GANs?
3. Can the infusion training approach be extended to conditional generative tasks, such as image-to-image translation or text-to-image generation?
Additional Feedback
- Address minor issues in the paper, such as errors in references and unclear explanations of variance parameterization.
- Include a discussion on computational efficiency compared to GANs and diffusion-based models.
- Consider adding a qualitative evaluation of failure cases or limitations of the method, such as potential biases in the generated samples.
Overall, this paper makes a valuable contribution to generative modeling and provides a promising direction for future research. With minor revisions and additional clarity, it has the potential to significantly impact the field.