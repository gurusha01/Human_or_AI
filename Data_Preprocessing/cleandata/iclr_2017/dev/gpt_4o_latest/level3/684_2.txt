Review of "Predictive Reinforcement Learning: A Model-Based Approach to Multi-Task Reinforcement Learning"
Summary of Contributions
This paper introduces a model-based reinforcement learning (RL) approach, termed Predictive Reinforcement Learning (PRL), aimed at addressing the challenges of multi-task learning in RL. The authors propose a novel recurrent neural network architecture inspired by residual networks, which decouples memory from computation, and claim that this architecture is particularly suited for environments with complex dynamics but limited memory requirements. The paper highlights the potential of PRL to achieve transfer learning benefits across tasks and demonstrates its application to three ATARI games. The authors also argue that their approach generalizes better than model-free methods, such as Q-learning, and can predict outcomes independent of the strategy being played. While the results do not achieve state-of-the-art performance, the authors claim that PRL opens new research directions for model-based RL in complex environments.
Decision: Reject
The primary reasons for rejection are (1) insufficient clarity and rigor in the theoretical and empirical contributions, and (2) limited experimental validation that fails to provide compelling evidence for the proposed method's effectiveness.
Supporting Arguments
1. Ambiguity in Key Concepts and Terminology: The term "strategy" is used ambiguously throughout the paper and lacks a formal definition. Similarly, it is unclear whether "r" refers to the discounted return or the immediate reward at time \( t \), which creates confusion in interpreting the proposed model and its results. Misuse of RL terminology, such as "model-based" and "strategy," further detracts from the paper's clarity.
2. Unclear Distinction from Existing Methods: The distinction between PRL and Q-learning, particularly for deterministic policies, is inadequately explained. The rationale for calling PRL a "model-based" approach is also weak, as the paper does not explicitly describe how the model leverages environment dynamics in a way that aligns with standard model-based RL definitions.
3. Insufficient Experimental Validation: The experiments are limited to only three ATARI games, and the learning curves are restricted to 19 iterations, providing an incomplete picture of the method's performance. The results are not compared to prior work, such as Temporal Difference (TD) learning or other multi-task RL baselines, making it difficult to assess the proposed method's relative effectiveness.
4. Poor Writing and Informal Language: The paper is vague in several sections, with informal language and imprecise descriptions of the methodology. For example, the explanation of the recurrent network architecture lacks sufficient mathematical rigor, and the experimental setup is described in a way that leaves many implementation details unclear.
5. Limited Generalizability: The experiments rely on hard-coded strategies and environments that do not require long-term planning, which undermines the claim that PRL is broadly applicable to complex environments. Additionally, the instability in training and oscillations in performance (e.g., in Demon Attack) suggest that the method is not robust.
Suggestions for Improvement
1. Clarify Terminology and Definitions: Provide a formal definition of "strategy" and explicitly clarify whether "r" refers to the discounted return or immediate reward. Ensure that RL terms are used consistently and align with standard definitions.
2. Strengthen Theoretical Foundations: Clearly articulate how PRL differs from Q-learning and other model-free methods, particularly for deterministic policies. Justify why PRL should be considered a model-based approach and explain deviations from standard RL frameworks.
3. Expand Experimental Validation: Test the method on a broader range of tasks and compare it to established baselines, such as TD learning, Q-learning, and other multi-task RL methods. Extend the learning curves beyond 19 iterations to provide a more comprehensive evaluation.
4. Improve Writing and Rigor: Revise the paper to eliminate vague and informal language. Provide a more detailed and rigorous description of the proposed architecture and training process, including mathematical formulations where appropriate.
5. Address Training Instability: Investigate and address the causes of instability during training, such as the model's tendency to "forget" certain outcomes. Consider techniques like curriculum learning or oversampling rare events to improve robustness.
Questions for the Authors
1. How does PRL differ from Q-learning in deterministic environments, and why is it more effective for multi-task learning?
2. Why is PRL referred to as a model-based approach, given that it does not seem to explicitly model environment dynamics in the traditional sense?
3. Can you provide a comparison of PRL's performance with standard baselines, such as TD learning or other multi-task RL methods?
4. Why were the experiments limited to only three games, and why were the learning curves restricted to 19 iterations? Would extending these experiments yield more conclusive results?
In its current form, the paper lacks the clarity, rigor, and experimental evidence required for acceptance. However, with significant revisions and additional experiments, it could potentially make a meaningful contribution to the field of multi-task RL.