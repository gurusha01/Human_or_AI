The paper introduces a novel generative function \( G \) for domain transfer, ensuring \( f \)-constancy, and evaluates it on two visual domain adaptation tasks: digit transformation (SVHN to MNIST) and photo-to-emoji generation. The key contribution lies in the integration of \( f \)-constancy into the generative process, which preserves semantic features across domains. The authors propose a Domain Transfer Network (DTN) with a compound loss function combining adversarial, \( f \)-preserving, and regularization terms. The experimental results demonstrate the method's ability to generate visually appealing and semantically consistent outputs, outperforming baseline methods in both qualitative and quantitative evaluations.
Decision: Reject.  
While the paper presents an interesting approach with promising results, several critical issues limit its acceptance. The counter-intuitive design of \( G = g \circ f \), the lack of clarity in baseline performance, and the need for retraining across tasks raise concerns about generalizability and robustness.
Supporting Arguments:  
1. Strengths:  
   - The paper is well-structured and provides extensive experimental results, showcasing the potential of \( f \)-constancy in domain transfer tasks.  
   - The application to emoji generation is novel and demonstrates practical utility.  
   - The inclusion of multiple loss components (e.g., \( L{CONST}, L{TID} \)) is well-motivated and empirically validated.  
2. Weaknesses:  
   - The design of \( G = g \circ f \) is counter-intuitive, as the restricted \( f \) may lose critical information, potentially limiting the expressiveness of \( G \). This design choice is not adequately justified.  
   - The approach requires retraining for each new task, which undermines its generalizability across diverse domain adaptation scenarios.  
   - The poor performance of the baseline method (Equations 1 and 2) is not sufficiently explained, leaving doubts about the robustness of the proposed improvements.  
   - Figure 5 lacks clarity in comparing methods, and the exploration of style transfer as a baseline is underdeveloped.  
Additional Feedback:  
- The authors should provide a more detailed analysis of why \( G = g \circ f \) is advantageous despite its potential limitations. For example, how does \( f \)-constancy balance against the loss of information in \( f \)?  
- Clarify why the baseline method performs poorly. Is it due to architectural limitations, loss function design, or insufficient training? This would strengthen the case for the proposed DTN.  
- Improve Figure 5 by explicitly marking which method performs better and providing quantitative metrics for comparison.  
- Explore the potential of style transfer methods (e.g., Gatys et al., 2016) as baselines for tasks like emoji generation to better contextualize the novelty of DTN.  
Questions for Authors:  
1. How does the restricted \( f \) in \( G = g \circ f \) impact the diversity and quality of generated samples? Could a more flexible \( G \) improve results?  
2. Why does the baseline method fail, and how does the inclusion of \( f \)-constancy address these shortcomings?  
3. Can the proposed method be extended to avoid retraining for new tasks, thereby improving generalizability?  
In summary, while the paper introduces an interesting concept and achieves promising results, the outlined concerns regarding design choices, clarity, and generalizability need to be addressed for acceptance.