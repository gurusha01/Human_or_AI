The paper introduces Variational Canonical Correlation Analysis (VCCA) and its variant, VCCA-private, as deep generative models for multi-view representation learning. These models extend the latent variable model interpretation of linear CCA to nonlinear settings using deep neural networks (DNNs). The authors claim that VCCA can disentangle shared and private information between views, generate high-quality samples, and improve downstream tasks like classification. The paper compares VCCA and VCCA-private with existing methods such as CCA, DCCA, DCCAE, and MVAE across three datasets: synthetic MNIST, XRMB, and MIR-Flickr, demonstrating competitive or superior performance.
Decision: Accept.  
The key reasons for acceptance are the novelty of the proposed method and its strong empirical performance across diverse datasets. The paper makes a meaningful contribution to multi-view learning by combining the strengths of variational inference and DNNs, and it provides a scalable framework with practical utility.
Supporting Arguments:  
1. Novelty and Motivation: The paper is well-motivated, addressing limitations of prior methods like DCCA and MVAE, particularly their inability to generate samples or disentangle shared/private variables effectively. The use of variational inference to extend CCA is a significant contribution.  
2. Empirical Rigor: The experiments are thorough, covering synthetic and real-world datasets. The analysis of dropout and private variables is insightful, and the results demonstrate clear advantages of VCCA-private in disentangling shared/private variables and improving reconstruction quality.  
3. Scientific Rigor: The derivations are correct and align with established principles of variational inference. However, some derivations (e.g., Equations 3 and 13) are overly detailed and could be moved to the Appendix for clarity.
Suggestions for Improvement:  
1. Missing Discussion on Linear VCCA vs. Linear CCA: The paper lacks a quantitative comparison between linear VCCA and linear CCA, which could provide deeper insights into the advantages of the proposed method.  
2. Notation Clarity: There is some inconsistency in notation between Equations 8 and 9, which could confuse readers. This should be clarified.  
3. Overstated Claim in Section 3: The claim that generating realistic samples necessarily indicates discovery of the underlying structure is incorrect and should be revised.  
4. Derivations Placement: The detailed derivations in Equations 3 and 13 could be moved to the Appendix to improve readability.  
5. Broader Contextualization: While the paper positions itself well within the literature, a more detailed discussion of how VCCA compares to adversarial methods or other generative models could strengthen its impact.
Questions for the Authors:  
1. Could you provide a quantitative comparison between linear VCCA and linear CCA to highlight the benefits of your approach in simpler settings?  
2. How sensitive is the performance of VCCA-private to the choice of dimensionality for private variables (dhx, dhy)?  
3. Have you explored the use of alternative priors (e.g., mixtures of Gaussians) for the latent variables, and if so, how do they impact performance?  
4. Could you elaborate on how the proposed models handle cases where the views are not perfectly aligned or contain significant noise?  
In conclusion, the paper presents a novel and impactful approach to multi-view representation learning, with strong empirical results and a solid theoretical foundation. Addressing the minor issues raised above would further enhance the clarity and completeness of the work.