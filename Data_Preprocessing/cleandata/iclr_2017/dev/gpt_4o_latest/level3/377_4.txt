Review
The paper introduces a novel problem of agents learning latent environmental properties through reinforcement learning (RL), specifically focusing on tasks that require active interaction with objects to infer hidden properties such as mass and cohesion. The authors propose a unified framework that combines convolutional layers for perception and later layers for action learning. The study is motivated by findings in developmental psychology, drawing parallels between infant experimentation and agent learning. Two environments, "Which is Heavier" and "Towers," are presented to evaluate the agents' ability to infer physical properties through interaction. The results demonstrate that RL agents can learn effective experimentation strategies, outperforming randomized baselines, and adapting their behavior based on task difficulty and cost of information gathering. The study has potential applications in robotics, particularly in scenarios involving image-based inputs and fist-like actuator actions.
Decision: Reject
While the paper addresses an interesting and underexplored problem, it falls short in several critical areas. The primary reasons for rejection are the lack of algorithmic innovation and the absence of crucial experimental results and baseline comparisons.
Supporting Arguments
1. Lack of Algorithmic Innovation: The paper relies on existing RL methods, such as LSTMs and A3C, without introducing significant extensions or novel algorithms. While the problem formulation is novel, the methodology does not push the boundaries of RL research. The use of standard techniques limits the paper's contribution to the field.
2. Incomplete Experimental Validation: The paper does not provide experimental results for the "Fist Pixels" setting, which is particularly relevant for real-world robotics applications. This omission weakens the practical applicability of the proposed approach. Additionally, the lack of baseline comparisons makes it difficult to assess the true benefits of the proposed framework. For example, comparing the performance of the proposed agents against simpler heuristic-based methods or alternative RL architectures would provide a clearer picture of its efficacy.
Additional Feedback
1. Baseline Comparisons: Including comparisons with alternative approaches, such as heuristic-based methods or other RL architectures, would strengthen the empirical evaluation. This would help clarify whether the observed performance gains are due to the proposed framework or simply the use of RL.
2. Real-World Relevance: The omission of results for the "Fist Pixels" setting is a significant gap. Addressing this would enhance the paper's relevance to robotics and real-world applications.
3. Data Efficiency: The paper acknowledges that it does not optimize for data efficiency, which is a critical aspect of RL in real-world settings. Future work could explore methods to reduce the number of samples required for learning.
4. Theory Building and Transfer: The authors mention the potential for agents to transfer learned knowledge to new tasks but do not explore this in the current work. Including preliminary experiments or discussions on transfer learning would add depth to the study.
Questions for the Authors
1. How does the proposed framework compare to simpler heuristic-based methods or alternative RL architectures in terms of performance and efficiency?
2. Why were experimental results for the "Fist Pixels" setting omitted, and do the authors plan to include these in future work?
3. How scalable is the proposed approach to more complex environments or real-world robotics tasks?
4. Could the authors elaborate on how the learned experimentation strategies might generalize to unseen tasks or environments?
In summary, while the paper addresses an intriguing problem and demonstrates promising results, the lack of algorithmic novelty, incomplete experimental validation, and absence of baseline comparisons limit its impact. Addressing these issues in future iterations could significantly enhance the paper's contribution to the field.