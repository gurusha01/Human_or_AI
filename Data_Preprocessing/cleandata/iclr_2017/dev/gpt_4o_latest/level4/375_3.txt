This paper introduces a novel, theoretically-grounded approach for representing sentences as vectors. The experimental results demonstrate that the vectors generated by this method achieve strong performance on similarity and entailment benchmarks, even outperforming certain RNN-based approaches.
In general, this is a compelling empirical finding, particularly given that the model appears to lack sensitivity to word order (based on my understanding). I would, however, appreciate a deeper discussion on why such a straightforward model outperforms LSTMs in capturing similarity and entailment. Could this outcome be influenced by specific characteristics of these benchmarks?