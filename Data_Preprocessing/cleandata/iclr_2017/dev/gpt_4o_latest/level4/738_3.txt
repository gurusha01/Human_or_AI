CONTRIBUTIONS  
During the training of LSTMs, many intermediate gradients approach zero due to the flat regions of the tanh and sigmoid activation functions when far from the origin. This paper demonstrates that rounding these small gradients to zero leads to matrices with up to 80% sparsity during training. Furthermore, it shows that applying this sparsification while training character-level LSTM language models does not significantly impact the model's final performance. The authors propose that this sparsity could be leveraged by specialized hardware to enhance the energy efficiency and speed of training recurrent networks.
NOVELTY  
To the best of my knowledge, the use of gradient thresholding to induce sparsity and improve efficiency during RNN training represents a novel contribution.
MISSING CITATIONS  
Previous research has investigated the use of low-precision arithmetic for training recurrent neural network language models:  
Hubara et al., "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations".