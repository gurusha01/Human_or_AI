The paper introduces a modified RNN architecture with multiple layers, where higher layers receive lower layer states only when a FLUSH operation is predicted. This operation involves passing the state upward and resetting the lower layer's state. To select one of three operations at each time step, the authors employ the straight-through estimator combined with a slope-annealing technique during training. Empirical results and visualizations demonstrate that the proposed architecture excels at boundary detection.
Pros:
- The paper is well-motivated and exceptionally well-written.
- It presents promising initial results in learning hierarchical representations, supported by visualizations and comprehensive experiments on tasks such as language modeling and handwriting generation.
- The annealing technique with the straight-through estimator appears to have potential applications in other tasks involving discrete variables, and the trade-off introduced in the flush operation is innovative.
Cons:
- The paper falls short in a few areas. It does not provide empirical results on computational savings, and it does not convincingly demonstrate hierarchy beyond a single level, particularly in cases where the data includes separators like spaces or pen up/down signals.
- It remains unclear whether the improved downstream performance stems from leveraging hierarchical information or if it is a result of the architectural modifications acting as a form of regularization, an issue that could benefit from further clarification.