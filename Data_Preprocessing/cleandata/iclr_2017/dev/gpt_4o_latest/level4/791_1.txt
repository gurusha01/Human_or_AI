This paper introduces an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. Specifically, the approach encourages feature representations of patches from the same image to be closer than those from different images. The optimization focuses on the distance ratios of positive training pairs. Empirical results demonstrate the effectiveness of the proposed method as an initialization strategy for supervised training.
Strengths:
- The training objective is well-motivated, particularly as high-level features exhibit translation invariance.
- The proposed methods show effectiveness in initializing neural networks for supervised training across multiple datasets.
Weaknesses:
- The methods bear significant technical resemblance to the "exemplar network" (Dosovitskiy, 2015). Cropping patches from a single image can be viewed as a form of data augmentation, analogous to the data augmentation of positive samples (the exemplar) in Dosovitskiy (2015).
- The experimental comparisons presented in the paper are potentially misleading. The reported results are based on fine-tuning the entire network with supervision, whereas in Table 2, the results for exemplar convnets (Dosovitskiy, 2015) are derived from unsupervised feature learning (where the network is not fine-tuned with labeled samples, and only a classifier is trained on the features). This makes the comparison unfair. It is plausible that exemplar convnets (Dosovitskiy, 2015) would achieve similar improvements with fine-tuning. Without direct comparisons (i.e., head-to-head evaluations with and without fine-tuning using the same architecture but different loss functions), the experimental results are not entirely convincing.
- Regarding the comparison with the "What-where" autoencoder (Zhao et al., 2015), it would be valuable to evaluate the proposed method in large-scale settings, as demonstrated by Zhang et al. (ICML 2016, "Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification"). Training an AlexNet is computationally feasible with modern GPUs (e.g., TITAN-X level), so such experiments could strengthen the paper.
- The proposed method appears to be primarily useful for natural images, where patches from the same image are likely to share similarities.