The authors introduce a character-level language model that achieves a degree of interpretability without significantly compromising predictive performance.
CONTRIBUTION:
This paper can be described as an experimental exploration of an interesting insight. It builds on the idea that multi-class logistic regression allows for the attribution of prediction outcomes to input featuresâ€”some features increase the likelihood of the correct class, while others decrease it. The authors demonstrate that a sufficiently simple RNN architecture exhibits a similar log-linear property, enabling the attribution of prediction outcomes to elements of the preceding input sequence.
PROS:
The paper is well-written and enjoyable to read. It is refreshing to see that a straightforward architecture can still perform reasonably well.  
The proposed model has potential utility in educational settings, such as classroom assignments.  
Its simplicity makes it easy to implement, and students could replicate the authors' analysis to explore the factors influencing the network's predictions.  
The authors provide appealing visualizations to support their findings.  
Additionally, Section 5.2 highlights some computational advantages of the proposed approach.
CAVEATS ON PREDICTIVE ACCURACY:
- Figure 1 claims that the ISAN achieves "near identical performance to other architectures." However, this seems to hold true only for the largest models.
Explanation: For smaller parameter sizes, the GRU outperforms the proposed model by 22% to 39% in terms of perplexity per word (ppw), which is the standard metric in language modeling. (In this field, a 10% reduction in ppw is traditionally considered a significant achievement, such as in a Ph.D. dissertation. I estimated an average of 7 characters per word when converting cross-entropy per character to perplexity per word.)
- It is unclear whether this model family will remain competitive in more complex scenarios.
Explanation: The experiments are limited to character-based language modeling on a relatively small 10M-character dataset, resulting in very high ppw values (2135 for the best models presented). In comparison, word-based RNN language models achieve much lower ppw values: 133 when trained on 44M words and 51 when trained on 800M words. [These reference numbers are taken from a previously cited paper.]