Paraphrased Review:
Overview:  
This paper introduces a method to connect trajectory log-probabilities with rewards by defining under-appreciated rewards. The authors argue that there exists a linear relationship between trajectory rewards and their log-probabilities, which can be leveraged by quantifying the resulting mismatch. Specifically, when an action sequence under-appreciates its reward, its log-probability is increased.  
The proposed approach is a straightforward modification of the well-known REINFORCE algorithm, requiring only one additional hyperparameter, \(\tau\). It intuitively provides a more effective exploration mechanism compared to \(\epsilon\)-greedy or random exploration.  
The method is evaluated on algorithmic environments and compared against entropy-regularized REINFORCE and double Q-learning. It demonstrates comparable or superior performance to these baselines, particularly in more complex environments.  
Remarks:  
- The emphasis in the introduction on algorithmic tasks is a double-edged sword. While it is an intriguing domain to validate the hypothesis and benchmark the method, it may detract from the broader applicability of the proposed approach, which (in my opinion) is more general.  
- In the introduction, the reward is described as sparse, but in Section 6 (tasks 1-5), it is stated that a reward is given at each correct emission, i.e., at every time step. This is only clarified as being adjusted to end-of-episode rewards in Section 7.4, after the results are discussed. I suggest moving or mentioning this clarification earlier in Section 6.  
- The approach appears sensitive to \(\tau\) being within the same range as \(\log \pi(a|h)\), yet the authors only experiment with \(\tau = 0.1\) for UREX. I find this choice of experimentation unclear and somewhat questionable.  
- Instead of grid search, random search (as proposed by Bergstra & Bengio, 2012) could be an alternative. It might better demonstrate hyperparameter robustness and allow for broader exploration within the same experimental budget.  
Opinion:  
- This is undoubtedly an interesting contribution to policy-gradient methods, addressing the critical question of "how should agents explore?"  
- I am hesitant to accept claims of hyperparameter robustness based solely on performance within a selected hyperparameter range. The results merely show that the method works well under certain conditions. Could it be that MENT requires a different set of hyperparameters? (Just playing devil's advocate here.)  
- While matching \(1/\tau\) with \(\log \pi\) is a natural choice, it introduces a strong prior: that the reward (scaled by \(1/\tau\)) resides in the same space as the log-policy. A potential issue I foresee (correct me if I'm wrong) is that as trajectory lengths increase, rewards are expected to grow linearly. This could lead to shorter trajectories being underexplored compared to longer ones with equivalent rewards, unless the reward is shaped to favor shorter trajectories (which is only the case in task 6).  
- It would have been valuable to compare the proposed method with approaches that explicitly aim to improve exploration using value functions (e.g., prioritized experience replay, Schaul et al., 2015).  
- While I don't want to sound repetitive, \(\tau\) plays a pivotal role in this method, yet its impact is not thoroughly analyzed in the experiments.  
Conclusion:  
The methodology and reasoning are clearly articulated, and the paper effectively communicates its central idea. While the proposed modification to a well-known algorithm is relatively minor, it is novel, well-motivated, and a meaningful addition to the literature on exploration in reinforcement learning.  
The experiments are well-aligned with the hypothesis, and the results support the authors' claims.  
That said, I believe the paper would benefit from additional (or more innovative) experimentation, as well as a more explicit demonstration of how the method impacts exploration. While this paper convinced me of the cleverness and motivation behind measuring the mismatch between a trajectory's observed reward and its probability under the current policy, the empirical evidence could be made more compelling, even if only through toy tasks.