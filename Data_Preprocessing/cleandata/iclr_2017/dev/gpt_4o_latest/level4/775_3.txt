This paper explores a question that is frequently overlooked in reinforcement learning or locomotion experiments.
My primary critique is that it is challenging to draw broader conclusions or reason beyond the experimental results presented. The authors focus exclusively on a single neural network architecture and a single reward function. For instance, does the torque controller face limitations imposed by the policy network?  
I recommend varying the number of neurons or demonstrating that the results remain consistent with an alternative state representation (e.g., training on pixel data). In its current form, the use of the term "DeepRL" feels somewhat arbitrary.
On the positive side, the paper is well-organized and easy to follow. The experiments are robust, clearly presented, and straightforward to interpret.  
This is undoubtedly an intriguing line of research, and beyond extending the work to 3D, I suggest that incorporating more realistic physical constraints (e.g., actuator limitations, communication delays, etc., on real robots) could significantly enhance the impact of this study.