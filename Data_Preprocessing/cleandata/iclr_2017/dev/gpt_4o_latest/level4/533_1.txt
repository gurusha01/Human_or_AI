The authors propose a novel method for surprise-based intrinsic motivation within the context of deep reinforcement learning. They effectively differentiate their approach from other recent intrinsic motivation techniques and support their claims with experimental results spanning a wide range of discrete and continuous action domains. The paper introduces two practical approximations to their framework: one that disregards the stochastic nature of true environmental dynamics and another that estimates the rate of information gain, drawing parallels to Schmidhuber's formal theory of creativity, fun, and intrinsic motivation. When integrated with TRPO, the exploration bonus derived from their method generally outperforms standard TRPO. Nonetheless, a more comprehensive comparison with other recent intrinsic motivation approaches would have strengthened the paper. For example, Bellemare et al. (2016) demonstrated notable performance improvements on challenging Atari games, such as Montezuma's Revenge, by combining DQN with an exploration bonus. However, Montezuma's Revenge is not included as an experimental benchmark in this work. Incorporating such comparisons would significantly enhance the paper's overall impact.