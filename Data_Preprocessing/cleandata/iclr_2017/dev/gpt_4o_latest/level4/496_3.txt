This paper introduces a novel variant of recurrent networks designed to learn hierarchical structures in sequential data (e.g., character → word). The proposed approach eliminates the need for explicit boundary information to segment sequences into meaningful groups, unlike prior work such as Chung et al. (2016).
The model architecture is structured as a series of layers, each intended to capture information at a distinct "level of abstraction." The lower layers activate the upper ones and determine when to update them via a controller (or state cell, referred to as c). A notable aspect of the model is that c is a discrete variable, which has the potential to enable faster inference. However, this discrete nature introduces challenges for training, necessitating the use of the straight-through estimator (Hinton, 2012).
The experimental evaluation is comprehensive, and the proposed model achieves competitive performance across several challenging tasks. Additionally, the qualitative results demonstrate the model's ability to identify natural boundaries in sequential data.
In summary, this paper presents a robust and innovative model, supported by promising experimental results.
---
On a minor note, I have a few comments and suggestions regarding the writing and related work:
- In the introduction:
  - "One of the key principles of learning in deep neural networks as well as in the human brain": Please provide evidence to substantiate the claim about the "human brain."
  - "For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances": The reference to Mikolov et al. (2010) appears to be missing here.
  - "in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data": Please include a reference to Lin et al. (1996) to support this statement.
- In the related work:
  - "A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchical RNN (El Hihi & Bengio, 1995)": This statement is inaccurate; the CW-RNN extends the NARX model (Lin et al., 1996), not the work of El Hihi & Bengio (1995).
  - "While the above models focus on online prediction problems, where a prediction needs to be made...": There are several missing references here, particularly to Socher's work and earlier recursive network models.
  - "The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)": Gradient clipping was not introduced in Pascanu et al. (2012); the earlier work of Mikolov et al. (2010) should be cited as the origin of this technique.
- Missing references:
  - "Recurrent neural network based language model," Mikolov et al. (2010)
  - "Learning long-term dependencies in NARX recurrent neural networks," Lin et al. (1996)
  - "Sequence labelling in structured domains with hierarchical recurrent neural networks," Fernandez et al. (2007)
  - "Learning sequential tasks by incrementally adding higher orders," Ring (1993)