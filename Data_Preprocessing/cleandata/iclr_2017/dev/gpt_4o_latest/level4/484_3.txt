This paper explores the question of which functions are better suited for deep networks as opposed to shallow networks. The central intuition is compelling and relatively straightforward: pooling operations aggregate information, and when this information is correlated, it can be utilized more effectively if the geometry of the pooling regions aligns with the correlations, enabling more efficient aggregation. Shallow networks, which lack layers of localized pooling, do not possess this mechanism to efficiently combine correlated information.
The theoretical analysis is centered on convolutional arithmetic circuits, building upon the authors' prior theoretical contributions. The results leverage the intriguing technical concept of separability, which quantifies the extent to which a function can be expressed as the composition of independent functions. Since separability is defined relative to a partition of the input, it serves as an appropriate framework for evaluating the complexity of functions in relation to specific pooling geometries. While many of the technical concepts are intuitive, the tensor analysis is dense and challenging to follow without familiarity with the authors' earlier work.
In some respects, the comparison between deep and shallow networks presented in the paper is somewhat misleading, as the shallow networks lack a hierarchical pooling structure. For instance, a shallow convolutional network with ReLU and max pooling is not a meaningful construct, as max pooling would operate over the entire image. Consequently, the paper seems to primarily analyze the impact of pooling versus the absence of pooling. For example, it remains unclear from this work whether a deep CNN without pooling would be more efficient than a shallow network.
The dependence of the theoretical results on the use of product pooling raises questions about their generalizability to more commonly used max pooling. Even if deriving theoretical results for max pooling is challenging, simple illustrative examples could provide valuable insights. If the authors were to prepare an extended version of the paper for a journal, it would be helpful to include a toy example demonstrating a function that can be efficiently represented with a convolutional arithmetic circuit when the pooling structure aligns with the correlations. Additionally, they could illustrate how this function might be represented using a convolutional network with ReLU and max pooling.
A more explicit discussion of how the depth of a deep network influences the separability of representable functions would also be appreciated. Since shallow networks lack local pooling, the distinction between deep and shallow networks in this work seems primarily to hinge on the presence or absence of pooling. However, practitioners observe that very deep networks often outperform "deep" networks with only a few convolutional and pooling layers. The paper does not directly address whether its findings shed light on this phenomenon.
In summary, the paper tackles an important problem using an interesting approach. However, it falls short of convincingly addressing why depth is so critical, due to its theoretical focus on arithmetic circuits and its comparison to shallow networks without localized pooling.