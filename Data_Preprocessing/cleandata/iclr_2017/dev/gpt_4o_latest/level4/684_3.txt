This paper introduces a novel approach to model-based reinforcement learning and evaluates its performance on three ATARI games. The method involves training a model to predict a sequence of rewards and probabilities of losing a life, conditioned on a context of frames and a sequence of actions. The controller samples random action sequences and selects the one that optimally balances the probabilities of earning a point and losing a life, based on predefined thresholds. The proposed system demonstrates the ability to play three ATARI games at a super-human level, both individually and in a multi-task training setup.
The results presented are highly promising; however, the system design includes several ad-hoc choices. Additionally, the paper provides limited analysis of the relative importance of the system's various components.
Main concerns:
- The combination of predicted rewards and life-loss probabilities is handled in an ad-hoc manner. A more natural approach would involve learning a Q-value, whereas the current method employs different rules for different games.
- It is unclear whether the model is genuinely being learned and improved. Visualizing predictions for multiple action sequences from carefully selected starting states would provide valuable insight. This should be demonstrated for both a game where the approach succeeds and one where it fails. Furthermore, tracking the training loss on a fixed holdout set of sequences could help illustrate the learning progress.
- The significance of the proposed RRNN architecture is not well-explored. Would the system still perform effectively without the residual connections? Could a standard LSTM achieve similar results?
Minor points:
- Introduction, paragraph 2: There is a substantial body of earlier work on using models in reinforcement learning. For instance, consider Dyna and "Memory approaches to reinforcement learning in non-Markovian domains" by Lin and Mitchell, among others.
- Section 3.1: Using \(ai\) to represent observations is unconventional. It would be clearer to use \(oi\) for observations and \(a_i\) for actions.
- Section 3.2.2: The notation is inconsistentâ€”\(r_i\) was previously used to denote the reward at time \(i\), but it is reused here for a different purpose.
- Observation 1 appears somewhat misplaced. Citing the layer normalization paper should suffice for providing motivation.
- Section 3.2.2, second-to-last paragraph: It is unclear how memory is decoupled from computation in this approach. Models like neural Turing machines achieve this by utilizing external memory, but this appears to be an RNN with skip connections.
- Section 3.3, second paragraph: Whether the model overfits depends on the dataset. The approach's inability to work with demonstrations is precisely due to overfitting concerns.
- Figure 4: The reference for Batch Normalization should cite Ioffe and Szegedy rather than Morimoto et al.
In summary, while the paper presents innovative ideas and encouraging results, it would benefit from additional exploratory and ablation experiments, as well as further refinement.