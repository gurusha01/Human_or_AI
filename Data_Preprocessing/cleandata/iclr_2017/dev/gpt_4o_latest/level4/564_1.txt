The paper investigates the concept of integrating skip connections over time within RNNs. While the core idea is not particularly groundbreaking, the authors present several approaches for incorporating this information into the current hidden state using different pooling functions. These models are evaluated on two widely-used text benchmarks.
Some observations:
1) The experiments are limited to NLP tasks and focus solely on prediction problems. It would have been beneficial to see the models applied to other domains, such as modeling a conditional distribution p(y|x) rather than just p(x). Additionally, testing on sensory input data like audio or video could have provided deeper insights.
2) As noted by other reviewers, the comparisons to alternative models do not seem entirely fair. Given the rapid evolution of SOTA in NLP, it is challenging to contextualize the experiments within the broader landscape.
3) The authors claim that the proposed approach aids long-term prediction. However, the paper lacks a thorough analysis to substantiate this claim, as I previously highlighted in an earlier query.
4) The claim that LSTMs train slowly and are difficult to scale does not align with my personal experience. Moreover, the widespread adoption of LSTM systems in production environments (e.g., Google, Baidu, Microsoft, etc.) contradicts this assertion.
While I find the underlying idea of the paper interesting, the aforementioned issues lead me to conclude that it is not yet ready for publication.