This paper aims to explore the capability of reinforcement learning (RL) agents to conduct "physics experiments" within an environment to infer the physical properties of objects. The problem is well-motivated; understanding the physical properties of objects is a fundamental skill for intelligent agents, and there has been relatively limited research in this area, particularly within the realm of deep RL. Additionally, the paper is well-written.
While the paper does not claim any architectural or theoretical contributions, the primary novelty lies in its task application—utilizing a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer object properties. Specifically, the two tasks involve moving blocks to estimate their mass and poking towers to determine the number of rigid bodies they comprise. These tasks, while relevant, represent a narrow subset of the broader abilities required for an agent to understand physics. This limitation is not inherently problematic, but the lack of comparisons with alternative (simpler) RL agents makes it challenging to assess the difficulty of the proposed tasks. As noted in the pre-review question, the "Which is Heavier" task appears relatively straightforward due to the actuator setup and the fact that the model primarily learns to compute the difference between successive block positions, which are directly encoded as features in most experiments. Consequently, it is unsurprising that the RL agent can solve the proposed tasks.
The paper's main claim, beyond solving these two physics-related tasks, is that "the agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes." The "cost of gathering information" is modeled by scaling the reward with a gamma value less than 1. While this behavior is somewhat interesting, it is not particularly surprising given the problem's design.
The authors emphasize that their approach—learning about physical object properties through interaction—differs from many prior approaches that rely on visual cues. However, they also acknowledge that this idea is not novel and has been explored in previous work (e.g., Agrawal et al. (2016)). It is essential for the authors to provide a more detailed discussion of these prior approaches (potentially removing less relevant content from the related work section) and explicitly articulate why the proposed tasks are compelling compared to, for instance, learning to manipulate objects toward specific goals by poking them.
To assess the paper's contribution, two key questions arise:
1) To what extent do these two tasks advance the goal of enabling agents to learn object properties through interaction, beyond prior work?  
2) To what extent do the results of the RL agent on these tasks enhance our understanding of agents that interact with their environment to learn physical properties of objects?
Based on the concerns outlined above, it is unclear whether the answers to (1) or (2) are "to a significant extent." For (1), since the proposed agent effectively solves both tasks, it is unclear whether these tasks can serve as benchmarks for more advanced agents (e.g., they may not function as a set of bAbI-like tasks).
Another issue, as noted by Reviewer 3, is that the model description is overly concise. Including a diagram to illustrate the inputs and outputs of the model at each time step would greatly facilitate replication.
In summary, while advancing agents' ability to learn physical properties through interaction is an important goal, the technical contributions of this paper are somewhat limited. As a result, it is unclear how much this work advances research in this area beyond the prior studies cited. It would be beneficial for the authors to include a discussion about the future of agents learning physics through interaction, speculate on more challenging extensions of the current tasks, and clarify how their approach fits into this broader vision.