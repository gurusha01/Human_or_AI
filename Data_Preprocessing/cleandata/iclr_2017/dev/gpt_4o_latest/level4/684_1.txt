This paper introduces a model-based reinforcement learning methodology aimed at predicting future rewards based on the current state and future actions. The approach employs a "residual recurrent neural network" that predicts the expected reward increase at various future time steps. To validate the proposed method, experiments are conducted on Atari games using a straightforward playing strategy: evaluating random action sequences and selecting the one with the highest expected reward while maintaining a low probability of dying. Notably, in one of the three tested games, the agent performs better when trained in a multitask setting (i.e., learning all games simultaneously), suggesting the occurrence of transfer learning.
The submission is well-written and presents an original and technically sound reward prediction architecture. However, I believe several issues prevent this work from meeting the standards of a top-tier conference like ICLR, as outlined below.
The first concern lies in the inconsistency between the algorithm described in Section 3 and its implementation in Section 4 (experiments). In Section 3, the model output is defined as the expected accumulated reward over future time steps (a single scalar). However, in the experiments, the output consists of two values: the probability of dying and the probability of achieving a higher score without dying. While this alternative implementation might perform better, it deviates from the main idea presented in the paper, which is not directly evaluated. This raises the question of whether the original formulation would work as intended, given the decision to implement it differently.
Additionally, the experimental evaluation is limited. The method is tested on only three games, which were seemingly selected for their simplicity, and there is no comparison with other reinforcement learning techniques (e.g., DQN and related methods). While I understand that the paper's primary goal is not to achieve state-of-the-art performance but to demonstrate the utility of the model's predictions for decision-making, the experiments fall short of showcasing how this model could lead to improved reinforcement learning algorithms. The current approach does not involve reinforcement learning per se, as the model is trained using supervised learning and applied within a manually defined, hardcoded policy. Furthermore, the paper does not evaluate the quality of the model's predictions (e.g., classification error for dying probability, mean squared error for future rewards) or compare them to simpler baseline methods.
Finally, the related work section is insufficient, focusing primarily on DQN while providing minimal discussion on model-based reinforcement learning. For example, the paper "Action-Conditional Video Prediction using Deep Networks in Atari Games" is a highly relevant work that should have been cited.
Minor Comments:
- The notation is unconventional, with "a" representing a state rather than an action. This is potentially confusing and deviates from standard RL notation without justification.
- Using a dot for tensor concatenation is suboptimal, as the dot typically denotes a dot product.
- The variable \( ri \) in Section 3.2.2 is a residual but is unrelated to \( ri \), the reward, which could cause confusion.
- \( c_i \) is defined as "the control performed at time \( i \)," but it appears to represent the control performed at time \( i-1 \).
- There is recurring confusion between the terms "mean" and "median" in Section 3.2.2.
- The variable \( x \) in Observation 1 should not be used, as the \( x \) in Figure 3 does not pass through layer normalization.
- The inequality in Observation 1 should involve \( |xi| \) rather than \( xi \).
- Observation 1 and its proof occupy too much space for a relatively simple result.
- In Section 3.2.3, the first \( rj \) should be \( ri \).
- The probability of dying is introduced abruptly in Section 3.3 without prior explanation that it will be an output of the model.
- The statement "Our approach is not able to learn from good strategies" seems to imply "only from good strategies." Please clarify.
- In Figure 4, clarify that "fc" refers to "fully connected."
- It would be helpful to explain how the architecture in Figure 4 differs from the classical DQN architecture in Mnih et al. (2015).
- Clarify the meaning of \( r_{j2} \) as per your response in the OpenReview comments.
- Table 3 is confusing: it states "After one iteration" but includes "PRL Iteration 2."
- The statement "Figure 5 shows that not only is there no degradation in Pong and Demon Attack" seems inaccurate, as the performance appears slightly worse.
- The claim "A model that has learned only from random play is able to play at least 7 times better" is unclear—please specify the origin of this "7."
- The reference to "a potential problem we mentioned earlier" in Figure 5c (Demon Attack) is unclear—please specify where this issue was previously discussed.