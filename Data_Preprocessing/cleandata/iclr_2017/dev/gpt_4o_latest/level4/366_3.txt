This paper presents an intriguing approach to topic modeling using a VAE framework. The core contribution lies in training a recognition model for the inference phase, which leverages "amortized inference" to achieve significantly faster inference compared to traditional methods that require iterative inference for each document. Below are some comments:
Equation 5: The notation p(theta(h)|alpha) feels somewhat cumbersome. Wouldn't P(h|alpha) be a more straightforward choice?
The generative model appears to be indifferent to document length, as the latent variables only produce probabilities over the word space. However, the recognition model seems to adjust the probabilities q(z|x) drastically when the document length changes, given that the input to q is affected. This behavior seems suboptimal. Perhaps normalizing the input to the recognition network could address this issue?
It is worth noting that the ProdLDA model might essentially correspond to exponential family PCA or a related variant.