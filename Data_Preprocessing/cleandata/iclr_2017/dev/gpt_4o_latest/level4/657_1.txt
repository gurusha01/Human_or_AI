The paper introduces several techniques to compress a wide and shallow text classification model that relies on n-gram features. These techniques include: (1) applying (optimized) product quantization to compress embedding weights, (2) pruning certain vocabulary elements, and (3) leveraging hashing to reduce vocabulary storage (though this is a relatively minor aspect of the paper). The focus is on models with extremely large vocabularies, and the authors demonstrate a reduction in model size with only a modest decrease in accuracy.
The task of compressing neural models is both significant and engaging. The methods section is well-articulated, featuring clear high-level explanations and relevant references. However, the paper's contributions to machine learning appear to be limited. The experimental results are not particularly compelling, as they primarily center on benchmarks that are not widely adopted. Furthermore, the paper's relevance to state-of-the-art RNN-based text classification models remains ambiguous.
The use of (optimized) product quantization for approximating inner products is not especially innovative, as similar approaches have been explored in prior work. Most of the model size reduction is achieved through vocabulary pruning. The proposed pruning method is based on the assumption that embeddings with higher L2 norms are more significant, with an additional coverage heuristic factored in. From a machine learning perspective, a more appropriate baseline for addressing this problem would involve introducing a set of (relaxed) binary coefficients for each embedding vector and jointly learning these coefficients alongside the weights. Sparsity could be encouraged by applying an L1 regularizer to the coefficients. From a practical standpoint, an important baseline is missing: what if one simply reduces the vocabulary size, for example, by using subword units (as discussed in...)?