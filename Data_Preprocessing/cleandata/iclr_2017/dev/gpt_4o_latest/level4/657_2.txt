The paper introduces a set of techniques aimed at compressing fast (linear) text classification models. The manuscript is well-written, and the results are compelling. The primary compression method leverages product quantization, a strategy that has been previously investigated in other contexts within the neural network model compression domain. While the authors cite the work by Gong et al., it would also be valuable to reference "Quantized Convolutional Neural Networks for Mobile Devices" (CVPR 2016).