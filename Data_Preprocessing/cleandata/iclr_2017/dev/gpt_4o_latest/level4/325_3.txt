Summary:  
This paper presents a heuristic method for training a deep directed generative model, where each layer samples from the same conditional distribution, akin to the transition operator of a Markov chain. The method approximates the gradient by substituting the posterior over latents with an alternative distribution, similar to optimizing a variational lower bound. However, unlike variational optimization, the approximating distribution is heuristically constructed at each step rather than updated to improve the lower bound. Additionally, the conditional distributions are optimized greedily instead of following the gradient of the joint log-likelihood.
Review:  
The proposed method is intriguing and appears to warrant further investigation. That said, given the existence of alternative approaches for training the same class of models that are 1) theoretically more robust, 2) of comparable computational complexity, and 3) empirically effective (e.g., Rezende & Mohamed, 2015), I am uncertain about the potential impact of this work. My primary concern, however, lies in the limited scope of the empirical evaluation.
I appreciate that the authors provided proper log-likelihood estimates, which will facilitate and encourage future comparisons of this method on continuous MNIST. However, the authors should clarify that the log-likelihood values cited from Wu et al. (2016) are not representative of a VAE's performance. (From the paper: "Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model." "Such observation models can easily achieve much higher log-likelihood scores, […].")
It would have been valuable to include comparisons with inpainting results from other methods. How does the practicality of the proposed approach compare to alternatives? Similar to the diffusion method by Sohl-Dickstein et al. (2015), the proposed approach appears both efficient and effective for inpainting. Neglecting to emphasize this aspect and conduct the necessary evaluations feels like a missed opportunity.
Minor Comments:  
– A citation for "ordered visible dimension sampling" is missing.  
– There are typos and frequent incorrect usage of \citet and \citep.