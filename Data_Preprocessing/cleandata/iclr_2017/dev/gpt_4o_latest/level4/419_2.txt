This paper introduces TopicRNN, a hybrid model that integrates LDA and RNN by enhancing a standard RNN with latent topic information. The model employs a switching variable to selectively include or exclude additive contributions from latent topics during word generation. 
The authors evaluate TopicRNN on two tasks: language modeling using the PTB dataset and sentiment analysis on the IMDB dataset. Their results demonstrate that TopicRNN outperforms a vanilla RNN on PTB and achieves state-of-the-art performance on IMDB.
Some questions and comments:
- In Table 2, could you clarify how LDA features are incorporated into the RNN (RNN LDA features)?
- It would be helpful to include results from an LSTM model for comparison, even if its perplexity is lower than that of TopicRNN. This would provide insight into how much latent topics help narrow the performance gap between RNN and LSTM.
- The generated text in Table 3 does not appear meaningful. Could you elaborate on its purpose? Is this text generated for the "trading" topic? Additionally, what about the generated text for the IMDB dataset?
- How well does the proposed method scale to large vocabulary sizes (e.g., >10K)?
- What is the accuracy on IMDB if the extracted features are used directly for classification, rather than being passed through a neural network with one hidden layer? This seems like a more direct comparison to the BoW, LDA, and SVM baselines presented.