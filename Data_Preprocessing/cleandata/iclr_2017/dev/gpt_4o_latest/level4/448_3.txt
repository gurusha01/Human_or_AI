This paper provides a mathematical examination of the mechanisms by which information flows through deep feed-forward neural networks, offering a novel analysis that tackles the challenges of vanishing and exploding gradients during the backward pass of backpropagation, as well as the application of the dropout algorithm. The manuscript is well-structured and clearly articulated, the analysis is comprehensive, and the experimental findings, which align well with the proposed model, are highly commendable.