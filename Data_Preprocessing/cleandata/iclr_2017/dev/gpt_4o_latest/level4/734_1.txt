This paper explores the scenario where multiple data views are modeled using a probabilistic deep neural network framework. While this approach introduces non-linearity (unlike, for instance, CCA), it also complicates inference. To address this, the authors employ the VAE framework for inference.
In [Ref 1], it is demonstrated that maximum likelihood estimation using a linear latent model yields canonical correlation directions. However, in the non-linear case with DNNs, it remains unclear (based on the current analysis) what the solution corresponds to in terms of canonical directions. The paper does not provide such an analysis, which makes it difficult to justify referring to this model as a CCA-like approach. By contrast, methods such as DCCA and DCCAE explicitly incorporate canonical correlation between features into their objectives and offer clearer interpretability.
[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.
Additionally, the paper does not engage with a substantial body of related work on non-linear multi-view models. For example, there are probabilistic non-linear multi-view models [Ref 2, 3], extensions to Bayesian frameworks with shared and private spaces [Ref 4], and applications of variational and deep learning methods [Ref 5].
[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.  
[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.  
[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.  
[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.
The integration of multi-view modeling with VAEs is a promising idea, and to the best of my knowledge, this combination has not been explored before. This makes the proposed model potentially valuable.
However, the critical question is how to effectively extend VAEs to handle multiple views. The paper does not convincingly demonstrate that the straightforward construction presented here works well for this purpose. For instance, VCCA does not appear to advance the state of the art (and, in fact, performs below it overall), while VCCA-private seems to be a poorly defined model. Specifically, the dimensionalities \(d\) must be manually tuned through exhaustive search, and the model lacks a robust mechanism to ensure that private and shared variables do not capture redundant information. Using dropout alone to address this issue feels ad hoc (and, as shown in Fig. 4 (ver2), the dropout rate is highly influential). While better performance might be achievable with extensive tuning (possibly explaining the improved FLICKR results in ver2 without changes to the model), optimization remains challenging for the reasons outlined. From an experimental perspective, VCCA-private also does not appear to advance the state of the art. While it is not necessary for every new paper to outperform all prior baselines, the extension of VAEs to multiple views is a compelling idea that warrants further investigation into more effective implementations.
Another limitation is that the approximate posterior is parameterized using only one of the views. This design choice reduces the model's utility as a general multi-view framework, as it may perform poorly on tasks beyond classification. If classification is the primary focus, the model should be compared against dedicated classification methods, such as feedforward neural networks.
The visualizations in Fig. 8 are well-presented and insightful. Overall, the paper successfully demonstrates the potential of combining VAEs with multi-view modeling. However, it does not convincingly address a) the practicality of the proposed approach and b) the connection to CCA beyond the shared focus on multiple views. In summary, while the paper is interesting and has merit, it requires further refinement.