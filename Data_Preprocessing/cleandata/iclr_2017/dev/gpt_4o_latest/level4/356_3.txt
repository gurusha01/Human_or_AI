This paper introduces a model capable of inferring programs from input/output example pairs, focusing on a constrained domain-specific language that supports a broad range of string transformations, akin to the one employed by Flash Fill in Excel. The proposed method involves modeling successive "extensions" of a program tree, conditioned on an embedding of the input/output pairs. Extension probabilities are derived as a function of leaf and production rule embeddings. A key contribution is the "Recursive-Reverse-Recursive Neural Net" (R3NN), which computes globally informed embeddings of leaves by performing an operation resembling belief propagation on a tree, while training this process in an end-to-end differentiable manner.
This paper has several notable strengths. Unlike some related work in the deep learning community, this approach seems practical and could be applied in real-world scenarios in the near future. The R3NN concept is innovative, and the authors provide strong motivation for its use. Additionally, the authors have conducted an extensive exploration of model variants to identify what works effectively and what does not. The exposition is clear and well-structured (even though the paper is lengthy), making it an enjoyable read. However, there are some weaknesses. The results are not highly accurate, potentially because the model is trained on small programs but tasked with inferring longer ones. It is unclear why the authors did not train on longer programs. Furthermore, the model appears to use a fixed number of I/O pairs, which raises concerns about scalability. For instance, if additional I/O pairs were provided, the model might not leverage them effectively (and experiments suggest that more pairs could even degrade performance). Despite these limitations, I strongly support accepting this paper at ICLR.
Additional comments:
- The expansion probability expression contains too many e's; it might be clearer to simply write "Softmax."
- There is a mention of adding a bidirectional LSTM to process global leaf representations before scoring, but no implementation details are provided (at least none that I could find).
- The authors emphasize the importance of using hyperbolic tangent activation functions, but further discussion on why alternatives like ReLU would not work as well would be valuable.
- The batching process is unclear, given that each program has a unique tree topology. More explanation on this aspect would be helpful, along with details about the optimization algorithm (e.g., SGD, Adagrad, Adam), learning rate schedules, and weight initialization. These omissions make the results less reproducible.
- In Figure 6 (unsolved benchmarks), it would be helpful to include the program sizes for the harder examples. This would clarify whether the failures are due to the benchmarks requiring longer programs or some other factor.
- The paper overlooks a related work by Piech et al. ("Learning Program Embeddingsâ€¦"), where a recursive neural network was trained to match abstract syntax trees for programs submitted to an online course, predicting program outputs but not synthesizing programs. Including this reference would strengthen the discussion of related work.