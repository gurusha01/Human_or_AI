This paper employs tensors to construct generative models. The core idea involves partitioning the input into regions represented by mixture models, with the joint distribution of the mixture components captured using a tensor. By constraining the tensors to those with efficient decompositions, the authors train convolutional arithmetic circuits to estimate the probability of the input and class label, thereby creating a generative model for both the input and labels.
The proposed approach is conceptually elegant. However, it is not entirely clear how the authors determine the specific architecture of their model and how these design choices correspond to the class of joint distributions the model can represent. While these choices may be somewhat heuristic, the overall framework offers a compelling mechanism for controlling the generality of the distributions being modeled.
The experiments are conducted on simple, synthetic datasets involving missing data, which represents a limitation. The paper would be more persuasive if it included experiments on real-world problems with missing data. A key challenge here is the need to know which elements of the input are missing, which constrains the method's applicability. Could the authors consider experiments on problems like the Netflix challenge, a canonical example of prediction with missing data? Despite these limitations, the experiments provide meaningful comparisons to prior work and serve as a reasonable initial evaluation.
I found the handling of missing data in the experiments somewhat unclear. From the introductory discussion, I understood that the generative model operates over region patches in the image, suggesting that missing regions would be marginalized. However, when missing data consists of IID randomly missing pixels, it seems likely that every region would contain some missing information. Why is it valid to marginalize over missing pixels in this case? Specifically, in Equation 6, $x_i$ represents a local region, and the subsequent discussion outlines how to marginalize over missing regions. How does this process work when only part of a region is missing? Additionally, the summation in the equation following Equation 6 appears potentially large. What is the computational cost of this operation?
The paper also appears inconsistent regarding the broader applicability of its results beyond images. The probabilistic model is primarily motivated in the context of images, yet the authors note in the experiments that they avoid using state-of-the-art inpainting algorithms because their method is not restricted to images and they wish to compare to image-specific methods. This argument would be more convincing if the paper included experiments in non-image domains.
It is also unclear whether the proposed network leverages translation invariance. A key factor in the success of CNNs is their encoding of translation invariance through weight sharing. Does the authors' network incorporate such invariance? If not, why should we expect it to perform well in challenging image-related tasks?
Finally, as a minor issue, the paper would benefit from more thorough proofreading. For instance, a few errors from the first page include:
- "significantly lesser" → "significantly less"  
- "the the"  
- "provenly" → "provably"