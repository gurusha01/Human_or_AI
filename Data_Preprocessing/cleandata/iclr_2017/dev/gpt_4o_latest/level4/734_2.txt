The authors introduce deep VCCA, an extension of the probabilistic CCA model that employs likelihoods parameterized by nonlinear functions (neural networks). Variational inference is utilized via an inference network and reparameterization gradients. Additionally, the paper presents a variant called VCCA-private, which incorporates local latent variables specific to each data point (view). The authors also establish a connection to the multi-view autoencoder framework.
While the methodology is technically sound, it is not particularly novel given the advancements in black-box variational inference and variational autoencoders. The proposed model is a straightforward adaptation of probabilistic CCA with neural network parameterized likelihoods. The inference process follows the standard black-box approach, relying on reparameterization gradients and inference networks. Moreover, the use of a mean-field approximation is somewhat outdated, considering recent progress in more expressive variational approximations (e.g., Rezende and Mohamed (2015); Tran et al. (2016)).
The connection to multi-view autoencoders is initially insightful but ultimately boils down to the distinction between MAP and variational inference. This is a well-established observation: the authors argue in the abstract that the key difference lies in the additional sampling, but the critical factor is the KL regularizer. Even with noisy samples, the variances in a normal variational approximation would collapse to zero, effectively reducing it to a point mass approximation, akin to optimizing a point estimate under the MVAE objective. While the authors seem to acknowledge this to some extent in their remarks, the discussion remains unclear.
Despite these concerns, the paper demonstrates significant strengths in its application. The experimental results are robust, offering comparisons to alternative multi-view approaches across diverse and interesting datasets. The inclusion of "private variables," though conceptually simple, is effectively leveraged to disentangle per-view latent representations from the shared view. However, the paper would have benefited from comparisons to methods employing probabilistic inference, such as full Bayesian approaches for linear CCA.
The paper also adopts several approximations, such as the mean-field family and inference networks, as standard practice, though these may not be strictly necessary. To better understand the impact of approximate inference on model performance, I strongly recommend conducting experiments with MCMC or non-amortized variational inference for at least one dataset.
+ Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.  
+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.