Altogether, this is a strong paper, an engaging read, and highly interesting. The work contributes meaningfully to advancing the state of the art in differentially-private deep learning. It is well-written and relatively comprehensive.
One limitation is that while the proposed approach is designed to be general, it does not provide theoretical guarantees regarding learning performance. Privacy-preserving machine learning research often includes analyses of both privacy (in the worst-case DP setting) and learning performance (often under specific assumptions). Since learning performance may depend on the choice of architecture, future experimentation with different architectures—even on the same datasets—is encouraged. If such experimentation is not included, the authors should justify their choice of architecture and/or clarify what aspects of the observed learning performance can be generalized.
Another limitation is that the reported epsilon values are not those that can be privately released. The authors acknowledge that their technique for privately releasing epsilon would alter the resulting value. However, this issue must be addressed to enable meaningful comparisons with the epsilon-delta values reported in related work.
Lastly, as noted in the paper, the proposed approach may not generalize to other natural data types. Additional experiments on other datasets are strongly encouraged. Furthermore, the datasets used in the paper should be explicitly cited.
Other comments:
The discussion of certain parts of the related work is thorough. However, the paper would benefit from including a survey or discussion of related work on differentially-private semi-supervised learning. For instance, in the context of random forests, the following paper proposed a differentially-private semi-supervised learning approach using a teacher-learner framework (though not explicitly referred to as "teacher-learner"). In this method, private labeled data is only used to train the "primary ensemble," while a "secondary ensemble" is trained on unlabeled (non-private) data using pseudo-labels generated by the primary ensemble:
G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.
Section C. provides a useful comparison of approaches. However, please ensure that the quantitative results presented here constitute an apples-to-apples comparison with the GAN results.
The paper is exceptionally well-written overall, but a few areas require clarification:
- Last paragraph of Section 3.1: The sentence "all teachers…get the same training data…" should be rephrased to clarify that the "same training data" refers to the same teacher on neighboring databases, not across all teachers.
- Section 4.1: The statement "The number n of teachers is limited by a trade-off between the classification task's complexity and the available data" is imprecise, as the trade-off is not formalized. In the i.i.d. setting, this trade-off would likely also depend on the relationship between the target hypothesis and the data distribution.
- Discussion of Figure 3: The explanation in the text and caption is unclear and should be revised. Initially, the text seems to suggest that a larger gap is better (as indicated in the caption). However, it later states that the gap remains under 20%, which appears contradictory. This inconsistency should be clarified.