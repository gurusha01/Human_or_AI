Thank you for conducting additional experiments and providing more detailed explanations in the manuscript. The authors have addressed most of my concerns, and I have updated my score accordingly.
This paper focuses on learning a generative function \( G \) that maps inputs from a source domain to a target domain while ensuring that a given representation function \( f \) remains invariant across both domains. This property is referred to as \( f \)-constancy. The proposed approach is evaluated on two visual domain adaptation tasks.
The manuscript is relatively clear and easy to follow, and the authors have presented extensive experimental results on the two datasets. The concept of \( f \)-constancy represents the primary novelty of this work.
However, it seems counter-intuitive to constrain \( G \) to be of the form \( g \circ f \), as this starts with a restricted function \( f \) that may have already discarded information. For example, in the face dataset, \( f \) is trained to optimize performance on a specific task using an external dataset. It remains unclear whether inputs from the source or target domain can be reconstructed by applying \( G \), as described in equations (5) and (6). Additionally, the function \( f \) is task-specific. In the two experiments, \( f \) is trained to recognize digits in the source SVHN dataset or to identify individuals in the face dataset. Consequently, the entire process would need to be repeated for domain adaptation involving the same domains but targeting different tasks, such as recognizing facial expressions instead of identities.
Do the authors have any insights into why the baseline method described in equations (1) and (2) performs so poorly?
Figure 5 provides a visual comparison between the proposed method and style transfer. However, it is not immediately clear which method performs better. Would it be possible to apply style transfer to generate emojis from photos and then replicate the experiments presented in Table 4?