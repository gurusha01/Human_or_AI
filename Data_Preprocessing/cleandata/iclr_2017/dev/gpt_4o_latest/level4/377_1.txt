This paper explores the problem of leveraging physical interactions to infer physical outcomes, a topic that aligns with a growing trend in the machine learning (ML) community focused on understanding physics. The authors take this exploration a step further by designing experimental setups where no prior knowledge of physical properties or rules is assumed, employing a deep reinforcement learning (DRL) approach to tackle the problem. My overall assessment of this paper is that it presents an intriguing idea and effort, but its contribution remains unclear.
The experimental setups are notably engaging. The objective is to determine which blocks are heavier or which blocks are glued together solely by interacting with the objects (e.g., pushing and pulling) without any prior knowledge. The paper demonstrates reasonable performance across these tasks, providing detailed scenarios to support its findings.
Despite the interesting experiments and results, the contribution of the work is ambiguous. My primary concern is whether the results offer any novel insights. While the scenarios are focused on physical experiments, they do not appear significantly different (and may even be simpler) than existing DRL tasks, such as learning from playing games like Atari. This raises the question: are these tasks genuinely distinct from other standard DRL benchmarks? To address this, I would have been more enthusiastic if the authors had provided deeper insights or additional experiments, such as analyzing the learned representations or exploring connections between the results and human behavior or physical laws. At present, the paper primarily reports the experimental setup and the agent's performance metrics. For instance, it describes what was done and how well the agent performed, but it does not delve into the underlying representations or broader implications. The authors could enhance the work by dissecting the learned representations further or discussing how the results relate to fundamental physical principles or human cognition.
I am conflicted about my overall rating. While the paper has merit due to its innovative idea, it could benefit from a more thorough analysis. Nonetheless, I recommend acceptance based on the strength of its concept.
Below are some detailed questions and comments, which do not significantly influence my overall rating:
1. Page 2: The authors state, "we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties." Why is interaction with objects necessary to learn about their properties? Couldn't observation alone suffice?
2. Figure 1 (right): The Y-axis label is missing.
3. Page 3: The connection to bandit problems is intriguing, but the formal approach is entirely based on DRL. Could this connection be explored further?
4. Page 5: The statement "which makes distinguishing between the two heaviest blocks very difficult" is unclear. Why does a small mass gap make the task harder (unless the gap is nearly zero)? Shouldn't a machine be capable of detecting even minimal differences in speed? If not, could this limitation stem from the network architecture?
5. Page 5: The authors state, "Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train." Would it be possible to show a correlation between performances at the instance level rather than relying on average performance? Even so, this conclusion seems somewhat overreaching.
6. General: Throughout the paper, many conclusions (e.g., task difficulty) appear to rely on a specific training distribution. For example, how does the agent determine when an instance is more difficult? Doesn't this depend heavily on the empirically learned distribution of training samples (e.g., P(m₃ | m₁, m₂), where mᵢ represents the masses of objects 1, 2, and 3)? In other words, does the notion of difficulty hold much significance without more extensive testing across diverse distributions?
7. Baselines: Are there any baseline approaches included for comparison? If not, incorporating them could strengthen the paper.