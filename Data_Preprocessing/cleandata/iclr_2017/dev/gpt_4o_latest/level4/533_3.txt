This paper investigates the role of intrinsic motivation within the framework of deep reinforcement learning (RL). It introduces several variants based on an auxiliary model-learning process (e.g., prediction error, surprise, and learning progress) and demonstrates their utility in enhancing exploration across various continuous control tasks (as well as the Atari game "Venture," potentially).
Novelty: The proposed intrinsic motivation mechanisms are not novel, and it is debatable whether their application to deep RL constitutes a novel contribution (see, for instance, Kompella et al., 2012).
Potential: The concept of targeting states where a transition model exhibits uncertainty is reasonable but inherently constrained. I recommend that the authors address these limitations explicitly. For instance, in environments like the game of Go, where the transition model can be learned trivially, this approach would likely default to random exploration. Alternative forms of learning progress or surprise, derived from the agent's competence, might offer greater promise over time. For further insights, see Srivastava et al., 2012.
Computation time: The paper's claim of superiority over VIME appears overstated. The reported advantage seems to arise primarily from faster initialization, while the per-step computational cost is comparable. Given that VIME also achieves highly competitive performance, what specific arguments can you provide to justify the advantages of your proposed method(s)?