This paper introduces a letter-level decoder based on a variation of the CTC approach, termed ASG. In this method, the blank symbol is replaced with letter repetition symbols, and explicit normalization is omitted. While the letter-level model itself is not novel, both its description and the proposed CTC variant are intriguing.
The approach is tested on the LibriSpeech dataset, with the authors asserting that their method is competitive. They compare their ASG variant to CTC but do not provide a comparison of the letter-level approach against existing word-level results. When compared to Panayotov et al. (2015), the performance here appears to be on par with word-level GMM/HMM models but falls short of word-level hybrid DNN/HMM models. However, it should be noted that Panayotov et al. employed speaker adaptation, which does not appear to have been applied in this work. I recommend including a comparison to Panayotov's results (in addition to referencing Baidu's results on LibriSpeech, which are not directly comparable due to their use of significantly larger training datasets) to provide readers with a clearer quantitative perspective. As the authors themselves note, Baidu's GPU implementation for CTC is optimized for larger vocabularies, making the GPU comparisons in Tables 1a-c less relevant unless further details about the implementations are discussed.
The analysis window used in this work is quite large (nearly 2 seconds). While other studies have employed windows of up to 0.5 to 1 second (e.g., MRASTA features), it would be valuable to include a discussion on the rationale behind using such a large window and the observed advantages.
The paper is well-written overall, but additional details on the use of non-normalized transition scores and beam pruning would enhance the submission. Table 1 could also be improved by including the units of the numbers directly within the table rather than only in the caption.
Prior publications of this work, such as the NIPS end-to-end workshop paper, should be explicitly mentioned and cited.
Clarification is needed on what is meant by transition "scalars."
I will not reiterate comments already provided during the pre-review phase.
Minor Comments:
- Section 2.3, end of the second sentence: "train properly the model" → "train the model properly."
- End of the same paragraph: "boostrap" → "bootstrap." Such errors could be avoided with an automatic spell check.
- Section 2.3: "Bayse" → "Bayes."
- The definition of logadd is incorrect (see comment); this also applies to the NIPS end-to-end workshop paper.
- Line before Equation (3): "all possible sequence of letters" → "all possible sequences of letters" (plural).
- Section 2.4, first line: "threholding" → "thresholding" (spell check).
- Figure 4: Specify the corpus used here—e.g., dev?
A more concise version of this work is set to be presented at the NIPS end-to-end workshop on December 10, 2016. This NIPS submission appears to be a clear subset of the current paper and should be explicitly referenced here.
When normalization of acoustic model scores is omitted, the resulting score range may vary, potentially affecting beam pruning and its interaction with normalized LM scores. Did you analyze this?
In Section 2.3, digits are used to label character repetitions. How are numbers handled in this context?
There seems to be inconsistent notation: the variable 't' is used for different time scales. For instance, in Equation (1), 't' represents strided time frames, whereas in x_t above, it directly enumerates frames.