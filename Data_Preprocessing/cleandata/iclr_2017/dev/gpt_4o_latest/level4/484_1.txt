This paper explores why deep networks perform effectively in practice and examines how altering the geometry of pooling enables polynomially sized deep networks to compute functions with exponentially high separation rank (under specific partitioning).
In their prior work, the authors demonstrated the advantages of deep networks over shallow ones when using ReLU as the activation function and max/mean pooling. However, in this paper, the convolutional layers lack activation functions, and pooling is implemented as a product of node values. That said, the experimental results consider both scenarios.
While providing a general theoretical explanation for this phenomenon is inherently challenging, this limitation is not a major concern. The current contribution meaningfully enhances the existing body of knowledge.
The paper focuses on convolutional arithmetic circuits and illustrates how this model captures inductive biases and how pooling mechanisms can be adjusted to influence these biases.
This compelling contribution offers insights into how deep networks can capture correlations between input variables, even when the network size is polynomial and the correlations are exponential.
It is worth noting that, although the authors have made commendable efforts to clearly define their notations and concepts, further elaboration on their definitions, expressions, and conclusions would enhance accessibility and improve clarity.