I reviewed the manuscript on December 5th.
Summary:  
The authors explore the phenomenon of adversarial perturbations and pose the question of whether it is possible to design a system capable of independently detecting adversarial data points. If adversarial examples could be reliably detected, this would allow for intervention before the machine processes them. A key focus of the study is whether an adversarial detector can be constructed to remain robust against adversarial examples crafted to target both the classifier and the detector. The findings indicate that training a detector under this more challenging scenario leads to improvements, though it does not fully solve the problem of detecting adversarial examples.
Major comments:  
The authors propose a novel security-oriented approach to addressing adversarial examples: building an independent detection system to flag adversarial inputs, enabling human intervention in such cases.  
One potential limitation of this approach is the possibility that an adversary could adapt by creating adversarial examples capable of deceiving both the original classifier and the detector. If this were achievable, the proposed method would lose its effectiveness, as the attacker could circumvent the entire system. To their credit, the authors demonstrate that constructing a 'dynamic' detector, which is designed to detect adversarial examples while also being resilient to adversarial attacks, helps mitigate this escalation (improving detection rates from 55% to 70% in the worst-case scenario). While these gains are promising, I am concerned about the overall detection rates. Achieving these levels of detection would not suffice for a reliable security mechanism.
My second concern pertains to the concept of 'model transferability.' The definition of 'transferability' used in the paper differs from my understanding of the term. In my view, 'model transferability' refers to the process of generating adversarial examples on one model and evaluating their effectiveness in attacking a second model trained with different initial conditions. (The authors, however, define 'transferability' as the ability of the detector to generalize across different training methods.) My definition of 'model transferability' is critical because it assesses the generality of adversarial examples across models, rather than being limited to a specific trained model. Prior work (e.g., Kurakin et al., 2016) has shown that different methods exhibit varying levels of 'model transferability.' I am concerned about how well the proposed detector would perform in identifying adversarial examples across all models rather than just the specific model it was trained on. A robust detector should be capable of identifying adversarial examples generated for any network, not just a single trained network. This issue appears to be largely unaddressed in the paper, though it is possible that I may have overlooked a subtle explanation in the text.
Minor comments:  
- If there are any data points in the bottom-left region of Figure 2 (left), this would be important to highlight. Consider repositioning the legend to make this area more visible.  
- The X-axis label in Figure 2 (right) is incorrect.  
- Could the authors evaluate the transferability of the detector?  
- How is \(\sigma\) defined or labeled in Figure 5?  
- For each constructed adversarial image, has it been explicitly verified that the image indeed causes a misclassification by the original network? In other words, are all adversarial images confirmed to be effective against the original classifier?