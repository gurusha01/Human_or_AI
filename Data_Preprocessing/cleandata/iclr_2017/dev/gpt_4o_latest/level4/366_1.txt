The authors introduce NVI as a novel inference method for LDA variants. They benchmark NVI-LDA against standard inference techniques such as CGS and online SVI. Additionally, they apply NVI to another model, ProdLDA, though it is unclear whether this model has been previously introduced in the topic modeling literature.
Overall, I find the direction of this work compelling, and NVI appears to be a promising approach for LDA. However, the experimental results conflate the effects of the model and the inference method, making it difficult to assess the true impact of NVI. Moreover, the paper lacks a discussion on hyper-parameter selection, which is known to significantly influence the performance of topic models. This omission raises questions about the scenarios in which the proposed method is expected to perform well.
Would it be possible to generate synthetic datasets with varying Dirichlet distributions to evaluate whether the proposed method can reliably recover the true parameters under different conditions?
Figure 1: Does this represent the prior or the posterior? The text discusses sparsity, but the y-axis is labeled "log p(topic proportions)," which creates some ambiguity.
Section 3.2: The term "unimodal in softmax basis" is unclear. For instance, a Dirichlet distribution on a K-dimensional simplex with a concentration parameter of α/K (where α < 1) is multimodal. Wouldn't the softmax basis still exhibit multimodality in such cases?
The reported results lack error bars. Are the differences statistically significant?
---
Minor comments:
- The last term in equation (3) should not be referred to as "error"; perhaps "reconstruction accuracy" or "negative reconstruction error" would be more appropriate.
- The concept of using an inference network predates this work, as seen in the Helmholtz machine.