This paper presents a generative model that converts noise into model samples through a progressive denoising process. The approach shares similarities with diffusion-based generative models. However, in contrast to the diffusion framework:
- It employs only a limited number of denoising steps, making it significantly more computationally efficient.
- Instead of following a reverse trajectory, the conditional chain for the approximate posterior directly transitions to q(z(0) | x) and then proceeds in the same direction as the generative model. This design enables the inference chain to act as a perturbation around the generative model, guiding it toward the data. (This aspect also appears to have some conceptual overlap with ladder networks.)
- There is no tractable variational bound on the log likelihood.
I found the core idea intriguing and was impressed by the visual sample quality achieved with a short chain. The inpainting results stood out, as one-shot inpainting is generally not feasible in most generative modeling frameworks. However, the work would be more compelling with a log likelihood comparison that does not rely on Parzen likelihoods.
Detailed comments:
Sec. 2:  
- "theta(0) the" → "theta(0) be the"  
- "theta(t) the" → "theta(t) be the"  
- "what we will be using" → "which we will be doing"  
I appreciate the approach of inferring q(z^0|x) and running inference in the same direction as the generative chain. This aspect reminds me somewhat of ladder networks.  
- "q. Having learned" → "q. [paragraph break] Having learned"  
Sec. 3.3:  
- "learn to inverse" → "learn to reverse"  
Sec. 4:  
- "For each experiments" → "For each experiment"  
How sensitive are the results to the infusion rate?  
Sec. 5:  
- "appears to provide more accurate models" — I don't believe this was demonstrated, as there is no direct comparison to the Sohl-Dickstein paper.  
Fig. 4:  
- Neat!