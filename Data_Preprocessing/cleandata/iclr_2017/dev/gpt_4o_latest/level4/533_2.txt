This paper introduces a surprise-based intrinsic reward framework for reinforcement learning, accompanied by two practical algorithms for estimating these rewards. The proposed ideas bear resemblance to prior work on intrinsic motivation, such as VIME and other related approaches.  
On the positive side, the methods are straightforward to implement and demonstrate advantages across various tasks.  
However, they are frequently outperformed by VIME, and none of the proposed methods consistently emerge as the best among them (though surprisal appears to be the most consistent, it is unfortunately not asymptotically equivalent to the true reward). While the authors claim significant speed improvements, the numerical results indicate that VIME is slower during initialization but not substantially slower per iteration thereafter (a big O analysis might help clarify these claims).  
In summary, this is a reasonable and simple technique, albeit somewhat incremental compared to the existing state of the art.