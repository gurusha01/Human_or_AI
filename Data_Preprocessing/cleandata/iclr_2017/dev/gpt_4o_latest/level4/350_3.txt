Review - Paper Summary  
This paper introduces an unsupervised learning framework where the network predicts its future state at the next time step (both at the input layer and potentially at other layers). Upon observing these states, an error signal is generated by comparing the predictions with the actual observations. This error signal is then propagated back into the model. The authors demonstrate that their approach achieves accurate predictions on a toy dataset of rotating 3D faces as well as on natural video sequences. Additionally, they show that the learned features can be effectively utilized for supervised tasks.
Strengths  
- The model is a compelling implementation of the predictive coding concept, realized through an end-to-end differentiable recurrent neural network architecture.  
- The use of a forward-propagated error signal is an underexplored idea, and this work provides a strong example of its application.  
- The empirical results are robust, and the comparisons with relevant baselines convincingly demonstrate the model's effectiveness.  
- The paper includes a thorough ablative analysis of the proposed model, adding depth to the evaluation.  
Weaknesses  
- The model, particularly as depicted in Fig. 1, is presented as a generalized predictive framework where predictions are made at every layer. However, the experiments reveal that only the predictions at the input layer are significant, and the optimal configuration involves disabling the error signal from higher layers. While the authors acknowledge this as a direction for future work, this discrepancy warrants further discussion in the current paper, given the way the model is framed.  
- The network does not incorporate stochasticity and does not explicitly model the future as a multimodal distribution. Although the authors mention this as a potential avenue for future work, it remains a limitation of the current approach.  
Quality  
The experiments are well-executed, and the appendix provides a comprehensive analysis of the results.  
Clarity  
The paper is clearly written and easy to understand.  
Originality  
While some prior deep learning models have explored predictive coding, the proposed model appears to be novel in its approach to feeding back the error signal and implementing the entire framework as a fully differentiable network.  
Significance  
This work is likely to be of significant interest to the growing community of researchers focused on unsupervised learning for time series data. It also highlights predictive coding as a valuable paradigm for learning in this domain.  
Overall  
This is a strong paper with well-designed experiments and detailed analyses. The idea of propagating the error signal forward is underutilized in the field, and this work effectively brings attention to its potential.