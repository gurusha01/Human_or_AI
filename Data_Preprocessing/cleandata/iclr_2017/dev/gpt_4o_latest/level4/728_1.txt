The paper begins by highlighting the necessity for methods that integrate both state and temporal representation learning in reinforcement learning (RL) while also offering interpretability, potentially enabling human operators to intervene when needed. This is an important and practically relevant objective, and it is encouraging to see research being directed toward this area. For this reason, I would like to commend the authors and encourage them to continue pursuing this line of work. However, I am not convinced that the current version of the paper provides the optimal solution to this problem. Some of the issues stem from presentation, while others may require a more fundamental rethinking of the approach.
To achieve "interpretability," the authors adopt specific mechanisms for abstraction. For instance, their skills are designed to always begin in a single skill initiation state and terminate in a single state. This design choice appears overly restrictive, and the rationale for imposing this constraint (beyond convenience) is unclear. Similarly, the use of clustering as the basis for constructing higher-level states relies on a specific type of clustering, but it is not evident why this method was chosen over alternative approaches. Additionally, the method used to ensure temporal coherence appears unnecessarily limiting. Although the authors refer to supplementary material for explanations of these design choices, I was unable to locate these details in the provided version of the paper. The authors should either clearly justify why these specific choices are essential or, ideally, explore whether these constraints can be relaxed while still maintaining interpretability.
From a presentation perspective, the paper would benefit from formal definitions of AMDP and SAMDP, as well as precise descriptions of the algorithms used to construct these representations (e.g., Bellman equations for the models and update rules for the learning algorithms). While the paper provides intuitive explanations, the mathematical details are not explicitly stated. Additionally, the computational overhead (in terms of time and space) associated with constructing an SAMDP should be clarified.
The experiments are well-executed, and it is commendable that the authors include both gridworld experiments, which facilitate easy visualization and interpretation, and Atari games. Despite some opinions to the contrary, gridworld experiments remain valuable for this type of research. The results are promising, but the proposed approach involves numerous components that rely on specific design choices, making its overall significance and general applicability unclear at this stage. A more complete supplementary document might have helped address some of these concerns.
Minor comment: There are typos in the notation and an incorrect sign in the equation in the two lines following Eq. 2.