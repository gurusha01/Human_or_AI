This paper introduces a generative model for mixtures of basic local structures, where the dependencies between these structures are represented as a tensor. The authors employ tensor decomposition, leveraging results from their prior work on the expressive power of CNNs and the hierarchical Tucker format to derive an inference mechanism. However, this approach hinges on the assumption that such a decomposition exists. The paper does not address the general applicability of their method, nor does it specify the subspace where this decomposition is feasible, efficient, or exhibits low approximation error. When questioned on this point, the authors argue that such theoretical analysis is unnecessary in the deep learning era. While this stance is debatable, the paper does not substantiate this claim or outline the limitations of their approach. Consequently, from a theoretical standpoint, the paper has shortcomings, and its claims are not fully justified. Some assertions cannot be supported by the current state of tensor literature, as the authors themselves acknowledge in their discussion. Therefore, the claims should have been revised, with explicit clarifications that the proposed method is restricted to a well-defined subclass of tensors.
From an empirical perspective, the experiments presented in the paper are insufficient to warrant acceptance. The use of MNIST and CIFAR-10 as benchmarks is inadequate, as these datasets are overly simplistic, and more comprehensive experiments are necessary. Additionally, the experiments on missing data are overly synthetic and fail to address real-world scenarios. The paper also lacks experiments extending beyond image data, despite the authors' repeated assertions that their approach is applicable to domains beyond images. Given the incomplete theoretical framework and the emphasis on broader applicability, these additional experiments are crucial for the paper's acceptance.