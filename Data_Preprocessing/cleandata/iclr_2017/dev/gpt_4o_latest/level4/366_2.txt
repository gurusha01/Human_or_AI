This paper introduces the application of the neural variational inference (NVI) method for topic modeling. The authors present an elegant approach to approximate the Dirichlet prior by employing a softmax basis with a Gaussian, followed by training the model to optimize the variational lower bound. Additionally, the paper addresses the component collapsing issue, a common challenge for continuous latent variables governed by Gaussian distributions, and proposes an improved solution. The results are promising, and the experimental methodology appears sound.
Minor comments:  
- Please include citations to [1] or [2] for neural variational inference, and [2] for variational autoencoders (VAE).  
- There is a typo in the sentence: "This approximation to the Dirichlet prior p(θ|α) is results in the distribution." It should be corrected to: "This approximation to the Dirichlet prior p(θ|α) results in the distribution."  
- In Table 2, it is mentioned that DMFVI was trained for over 24 hours but failed to produce any results. Why not wait until the training completes and report the corresponding numbers?  
- In Table 3, the perplexities of LDA-Collapsed Gibbs and NVDM are lower, yet the proposed models (ProdLDA) generate more coherent topics. Could the authors elaborate on their intuition behind this observation?  
- How does the training speed (until convergence) vary when using different learning-rate and momentum scheduling strategies, as shown in Figure 1?  
- It might be valuable to include additional analysis of the latent variables z, particularly regarding component collapsing. While the results indirectly suggest that the learning-rate and momentum scheduling approach mitigates this issue, a more detailed exploration would strengthen the paper.  
Overall, the paper effectively introduces its core idea, provides a clear rationale for employing NVI, and demonstrates experimental results that support the main claims. The challenges are well-articulated, and the proposed solutions are convincingly validated.  
[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML'14  
[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML'14