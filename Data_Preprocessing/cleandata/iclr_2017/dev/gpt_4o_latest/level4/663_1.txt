This paper presents a large-scale multi-modal product classification system composed of three main modules: an Image CNN (based on the VGG 16 architecture), a text CNN (following Kim 2014), and decision-level fusion strategies. The authors explore several fusion approaches, including methods that combine probabilities from the text and image CNNs, select predictions from one of the CNNs, average the outputs, and perform end-to-end training. Experimental results indicate that the text CNN outperforms the image CNN individually, while multi-modal fusion provides a modest improvement in accuracy. Interestingly, end-to-end feature-level fusion performs worse than the standalone text CNN, which is somewhat unexpected. The paper is well-written and provides valuable insights into the practical challenges of training large-scale models. However, I am inclined to recommend rejection for the following reasons:
1) The study is limited to a single dataset. The authors do not report results on any other dataset, and without releasing the Walmart dataset, reproducing the findings will be extremely difficult.  
2) The technical contribution is incremental. The decision-level fusion strategies employed have already been explored in prior work.  
3) The performance improvement achieved is marginal.