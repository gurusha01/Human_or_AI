SUMMARY  
The paper introduces a reading-comprehension question answering system tailored for a recent QA task where answers to a question can consist of either single tokens or spans within a given text passage.  
The proposed model begins by encoding both the passage and the query using a recurrent neural network (RNN).  
An attention mechanism is then employed to compute the relevance of each word in the passage with respect to each word in the question.  
The attention-weighted passage words are concatenated with their original encodings, and the resulting vectors are subsequently re-encoded using another RNN.  
To capture local features, three convolutional neural networks (CNNs) with varying filter sizes (1, 2, and 3-gram) are applied.  
Candidate answers are generated by either matching part-of-speech (POS) patterns observed in the training set or selecting all possible text spans up to a predefined length.  
Each candidate answer is represented in three forms, corresponding to the n-gram representations. The compatibility between these representations and the question representation is then computed.  
The resulting scores are combined linearly and used to estimate the probability of each candidate being the correct answer to the question.  
The method is evaluated on the SQuAD dataset, where it demonstrates superior performance compared to the proposed baselines.  
---
OVERALL JUDGMENT  
The approach presented in this paper is intriguing but lacks sufficient justification in certain aspects.  
For instance, the paper does not adequately explain why concatenating the original passage encoding with the attention-weighted encoding is advantageous in the attention mechanism.  
The contributions of the paper are moderately novel, primarily focusing on the attention mechanism and the convolutional re-encoding.  
However, the general framework of combining questions and passages to compute their compatibility has already become a fairly standard practice in QA models.  
---
DETAILED COMMENTS  
- In Equation (13), the variable "i" should replace "s^l."  
- The sentence, "The best function is to concatenate the hidden state of the first word in a chunk in the forward RNN and that of the last word in the backward RNN," remains unclear. Does the RNN process all the words in the chunk or all the words in the passage? The authors' response did not sufficiently clarify this point.