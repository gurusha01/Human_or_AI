Summary: The authors propose a simple RNN with linear dynamics for language modeling. The linear dynamics significantly improve the interpretability of the model and offer the potential to enhance performance by caching dynamics for frequently occurring sub-sequences. However, the quantitative results on a benchmark task are not particularly strong. The choice of dataset is somewhat unconventional, and only a single dataset is evaluated. That said, the authors introduce several well-executed techniques for analyzing the model's behavior, many of which would not be feasible for a non-linear RNN.
Overall, I recommend accepting the paper despite its modest results. It makes for an engaging read and represents a meaningful contribution to the ongoing research dialogue.
Feedback:
The paper could be strengthened by reducing the number of analysis experiments and expanding the discussion of related sequence models. While some experiments were highly insightful, others (e.g., Section 4.6) seemed to primarily demonstrate that the model fits the data well, rather than highlighting any particularly novel or important property. Given the reasonable perplexity results, it is already clear that the model fits the data adequately.
LSTMs and GRUs are well-suited for language modeling tasks involving rigid combinatorial structures, such as nested parentheses. It would have been valuable to compare your model against non-linear methods on such datasets. Don't shy away from reporting negative resultsâ€”if non-linear methods outperform your model on these tasks, it would still be an interesting finding.
You should include a discussion of Belanger and Kakade (2015) in the related work section. While their motivations (developing fast, scalable learning algorithms) differ from yours (focusing on interpretable latent state dynamics and simple credit assignment for future predictions based on past observations), their work also employs linear dynamics and analyzes the model using the singular vectors of the transition matrix. This connection is worth highlighting.
More generally, it would benefit readers if you engaged more directly with the literature on linear dynamical systems (LDS). This comparison was discussed in the openreview forum, and I recommend incorporating it into the paper. For instance, it would be helpful to emphasize that the bias vectors in your model correspond to the columns of the Kalman gain matrix.
On the topic of LDS, your model aligns with Kalman filtering, but in an LDS framework, one can also perform Kalman smoothing, where state vectors are inferred using both past and future observations. Could your model be extended to include a similar capability?
Finally, consider exploring whether each matrix in your model could be represented as a sparse or convex combination of a set of dictionary matrices. This type of parameter sharing could enhance interpretability, as characters would then be represented by the low-dimensional weights used to combine the dictionary elements. Additionally, this approach might improve scalability for word-level tasks.