There has been substantial prior work on learning directly from raw waveforms and training letter-based CTC networks for speech recognition. However, only a limited number of studies have explored combining these approaches with a purely ConvNet architecture, as presented in this paper. The results on a large-scale corpus like Librispeech, as used here, are intriguing. That said, the inclusion of baseline results from hybrid NN/HMM systems would provide a more comprehensive comparison. From Table 2 alone, it remains unclear to readers how this system compares to state-of-the-art methods.
The primary contribution of this paper appears to be the end-to-end sequence training criterion for their CTC variant, where the blank symbol is omitted. This can be interpreted as a form of sequence training for CTC, akin to the approach described in H. Sak et al., "Learning acoustic frame labeling for speech recognition with recurrent neural networks," 2015. However, unlike the traditional approach of first generating denominator lattices using a frame-level trained CTC model, this paper directly computes the sequence-level loss by accounting for all competing hypotheses in the normalizer. This enables end-to-end model training. From this perspective, the method shares similarities with D. Povey's LF-MMI for sequence training of HMMs. As another reviewer has noted, additional references and discussions on this connection should be included.
While this approach is expected to be more computationally expensive than frame-level CTC training, the authors' implementation, as shown in Table 1, appears to be significantly faster. It would be helpful to clarify whether the systems compared in Table 1 used the same sampling rate. For instance, at the end of Section 2.2, it is mentioned that the step size for the proposed model is 20ms. Is this also the case for Baidu's CTC system? Additionally, have the authors experimented with increasing the step size to 30ms or 40ms? Prior research suggests that larger step sizes may yield comparable performance while substantially reducing computational costs.