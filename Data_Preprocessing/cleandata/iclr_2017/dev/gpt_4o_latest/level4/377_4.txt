This paper reports intriguing experimental results demonstrating that state-of-the-art deep reinforcement learning techniques enable agents to learn latent (physical) properties of their environment. The authors formulate the problem of an agent labeling environmental properties after interacting with its surroundings through its actions and utilize a deep reinforcement learning model to assess the feasibility of such learning. The proposed approach integrates convolutional layers for pixel-based perception with subsequent layers for learning actions driven by reinforcement signals.
Our assessment of this paper is mixed. While the paper is well-written and presents compelling experimental findings, it introduces and formulates a problem that could have significant implications for robotics applications. However, it falls short in terms of algorithmic innovation and lacks some critical experiments necessary to validate its claims fully.
Pros:
+ The paper introduces a novel problem of learning latent environmental properties through agent interaction.
+ It provides a framework that effectively combines existing methodologies to address the proposed problem.
+ The study explores reinforcement learning with image inputs and fist-like actuator actions, paving the way for direct applicability to robotic systems.
Cons:
- Limited algorithmic contribution: The paper primarily applies existing tools and methods to address the problem rather than proposing novel algorithms or extending existing ones. Specifically, the approach involves training LSTMs with convolutional layers using the established Asynchronous Advantage Actor-Critic framework.
- Missing results for a critical setting: In the Towers experiment, results for the "Fist Pixels" setting—arguably the most important configuration—are absent. This setting, which involves pixel inputs and the Fist actuator in a continuous action space, is the closest approximation to real-world robotic scenarios. Its omission raises concerns about the applicability of the proposed approach to real-world robots. For example, Figure 5 does not include results for this setting. Is there a specific reason for this omission?
- Lack of baseline comparisons: The paper does not compare its approach to any baseline methods, making it difficult to discern what the agent is genuinely learning and which aspects of the proposed approach contribute to the task's success. For instance, in the Towers task, how would an agent with a fixed action policy—randomly pushing or hitting the tower with the Fist actuator and passively observing the consequences—perform in generating labels compared to the proposed deep reinforcement learning approach? A comparison with such a baseline (keeping all other components constant) would provide valuable insights into the benefits of the proposed method.