The paper presents an intriguing application of generative models to tackle the problem of classification with missing data. The proposed tensorial mixture models (TMM) address the broader issue of dependent samples, which marks a significant improvement over traditional mixture models that typically assume sample independence. Notably, the TMM framework reduces to conventional latent variable models under certain conditions. While I find the core ideas of the paper compelling, I am disappointed by the lack of rigor in the presentation (e.g., missing notations) and some flaws in the technical derivations. Before delving into the technical specifics, I outline my primary concerns as follows:
1. The joint density across all samples is modeled using a tensorial mixture generative framework. However, the interpretation of the CP decomposition or HT decomposition applied to the prior density tensor is unclear. The authors provide an interpretation of TMM as a product of mixture models under the assumption of independent samples, but I find this interpretation problematic, as I will elaborate on in the detailed technical comments below.
2. The authors utilize convolution operators to compute an inner product. While this can be implemented via zero-padding, it compromises the invariance structure, which is a key advantage of convolutional neural networks (CNNs) over feed-forward neural networks. That said, I am uncertain about the extent to which this limitation impacts practical performance.
3. The authors could provide some discussion in the paper regarding the sample complexity of their method, given the inherent complexity of the model.
Because I found the ideas in the paper highly appealing, I referred to the arXiv version (as suggested by the authors) to better understand the technical details, which were not well-presented in the conference submission due to issues such as inconsistent notations. Below, I highlight a few technical issues and typos I encountered (equation references are from the arXiv version):
1. The generative model depicted in Figure 5 appears to have a flaw. Specifically, \( P(xi|di;\theta{di}) \) are vectors of length \( s \), so the product of these vectors is not well-defined. The dimensional inconsistency between the terms on either side of the equation is evident. This should instead be expressed as a Tucker decomposition rather than a simple multiplication. The correct formulation should be:  
   \[
   P(X) = \sum{d1,\ldots,dN} P(d1,\ldots,dN) \cdot P(x1|d1;\theta{d1}) \cdot P(x2|d2;\theta{d2}) \cdots P(xN|dN;\theta{d_N}),
   \]
   which represents a summation of a multi-linear operation on the tensor \( P(d1,\ldots,dN) \), where each mode is projected onto \( P(xi|di;\theta{di}) \).
2. I suspect there are typos in the special case for diagonal Gaussian Mixture Models, as I was unable to derive the third-to-last equation on page 6. However, it is also possible that I misunderstood this example.
3. The claim that TMM reduces to a product of mixture models is not entirely accurate. The first equation on page 7 holds only when the "sum of product" operation is equivalent to the "product of sum" operation. Similarly, in Equation (6), the second equality is valid only under specific conditions, which are not generally true. While these may simply be typographical errors, it would be beneficial for the authors to correct them. I also suspect that addressing these issues might lead to improved performance on the MNIST dataset.
In summary, I am highly impressed by the conceptual contributions of this paper. However, I recommend that the authors address the technical inaccuracies and typos if the paper is accepted.