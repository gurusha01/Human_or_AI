This paper introduces a novel exploration strategy aimed at encouraging exploration of under-explored reward regions. The proposed importance sampling-based approach represents a straightforward modification to REINFORCE, and experimental results on several algorithmic toy tasks demonstrate that the proposed method outperforms both REINFORCE and Q-learning.
The study presents promising results in the domain of automated algorithm discovery using reinforcement learning. However, the primary motivation of the paper remains unclear. Is the main goal to enhance exploration for policy gradient methods? If so, the authors should have evaluated their approach on standard reinforcement learning benchmarks. While there is extensive literature on improving REINFORCE, the authors have focused on a simplified version of REINFORCE applied to a non-standard task and concluded that UREX performs better. If the primary motivation is to improve performance in algorithm learning tasks, the baselines used are still relatively weak. The authors should clarify their main objective.
Additionally, the action space considered is quite small. Early in the paper, the authors highlight concerns about the scalability of entropy regularization to larger action spaces. Therefore, a comparison of MENT and UREX in a task with a larger action space would provide valuable insights into whether UREX is robust to larger action spaces.
---
After rebuttal:
I overlooked the argument regarding action sequences when raising the issue of small action spaces.
Regarding the concern about weak baselines, there are several techniques in the literature to address the high-variance challenge in REINFORCE. For instance, see Mnih & Gregor, 2014.
I have updated my rating from 6 to 7. However, I still encourage the authors to strengthen their baselines.