Based on prior work, including stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, this paper proposes a Leaky-ReLU model for generative learning.
Pro:  
What stands out is that, unlike the traditional approach of first defining an energy function and then deriving the conditional distributions, this paper takes the reverse approach by proposing the forms of the conditionals first and then deriving the energy function. However, while this formulation is intriguing, it is not novel to this paper, as it was previously generalized to exponential family GLMs.
Con:  
Due to the emphasis on specifying the conditionals, the joint probability density function and the marginal \( p(v) \) become complex and computationally challenging.
Regarding the experiments, it would have been beneficial to include a Restricted Boltzmann Machine (RBM) with binary visible units and leaky ReLU hidden units. This would better demonstrate the advantages of the leaky ReLU hidden units. Additionally, there are more results available for binary MNIST modeling that could serve as a basis for comparison. While the authors correctly note that the annealing distribution is no longer Gaussian, experiments such as CD-25 or (Fast) PCD could be conducted to compare against baseline RBMs trained using (Fast) PCD.
This paper is compelling as it combines a novel hidden unit activation function with the simplicity of annealed AIS sampling. However, baseline comparisons to models such as Stepped Sigmoid Units (Nair & Hinton) or other alternatives like spike-and-slab RBMs are absent. Without these comparisons, it is difficult to assess whether leaky ReLU RBMs outperform existing approaches, even in the continuous visible domain.