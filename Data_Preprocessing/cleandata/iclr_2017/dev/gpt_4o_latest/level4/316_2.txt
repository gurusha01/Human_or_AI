This paper addresses the challenge of ensuring privacy for training data. The proposed method involves utilizing multiple models trained on disjoint datasets as "teacher" models, which collectively guide the training of a "student" model. The student model predicts outputs determined through noisy voting among the teacher models.
The theoretical contributions are solid and intuitive. Since the teachers' outputs are filtered through noisy voting, the student model is unlikely to replicate the exact behavior of the teachers. However, the probabilistic guarantees rely on several empirical parameters, making it challenging to definitively assess whether complete security is ensured.
The experiments conducted on MNIST and SVHN datasets are well-executed. That said, given the paper's claim that the approach is particularly valuable for sensitive data, such as medical records, it would be beneficial to include one or two experiments demonstrating its application in such domains.