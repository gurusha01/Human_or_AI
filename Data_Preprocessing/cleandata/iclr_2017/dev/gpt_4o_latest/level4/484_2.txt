The paper introduces a sophisticated algebraic framework to analyze the class of functions represented by convolutional networks. As is common in similar works in the literature, the idealized networks considered in the paper—interpretable as polynomials over tensors—differ from the CNNs typically used in practice. For example, the ReLU non-linearity is replaced by a product of linear functions (or a sum of logarithms).
While the paper is highly technical and dense, the authors have clearly defined every concept and introduced the mathematical terminology appropriately. That said, I believe the authors could make an effort to present the key ideas in a more accessible manner and provide an intuitive explanation of the separation rank before delving into its various mathematical interpretations. Personally, my familiarity with the separation rank framework is limited, and my understanding would have benefited from a simpler, more illustrative example—such as the shallow network case in Section 5.3—before tackling the general deep network case.
To summarize my understanding of the main result in Theorem 1:
- For the shallow case, the upper bound on the separation rank demonstrates that this rank grows at most linearly with the network size (measured by the single hidden layer). Exponential network sizes arise when the separation rank must grow exponentially, as dictated by the partition.
- For the deep case, the upper bound remains linear in the network size (measured by the last hidden layer). However, this result depends on the choice of a partition \((I^{\text{low}}, J^{\text{high}})\), and the maximal rank induced by this partition is inherently linear. As a result, the network size can remain linear.
I have done my best to summarize the key insights of this paper, but I still feel I may not have fully captured the complexity of the partition rank concept. Its linear growth with network size can be seen as either advantageous or disadvantageous, depending on the context. Hopefully, someone will eventually provide a more concise and intuitive explanation of these ideas—perhaps one that could fit on a single slide.
While I believe this paper is suitable for publication at the conference in its current form, I have two suggestions that could enhance its impact:
1. Theoretical Improvements: The theoretical framework presented in this paper is still far from the completeness achieved by PAC-bound papers during the "shallow network era." Specifically, the non-probabilistic lower and upper bounds in Theorem 1 appear to be loose, and the paper lacks a PAC-like framework to determine which bound is more relevant and how it impacts performance beyond intuition. Additionally, the paper only addresses one aspect of the inductive bias—the maximal representation capacity of a DNN under bounded network size constraints. However, one primary reason for bounding network size is to mitigate overfitting, which is typically justified by PAC or VC-dimension bounds. If we consider the expected risk as the sum of empirical risk and structural risk, this paper focuses entirely on empirical risk minimization while leaving structural risk unaddressed.
2. Practical Considerations: The experimental results in the paper largely confirm intuitive conclusions or outcomes that could be derived from simpler reasoning. For instance, the use of convolutions to capture symmetry in images by joining symmetrical pixels is a well-known concept that has been employed in computer vision for decades using handcrafted pattern detectors. To better motivate the use of this framework, the authors could address questions that are not easily resolved by human intuition and remain poorly understood. For example, one intriguing application could be the recent use of gated convolutions with dilations ("à trous") for 1D speech signals, as popularized in Google WaveNet.