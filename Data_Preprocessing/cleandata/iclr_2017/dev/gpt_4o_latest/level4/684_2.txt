The term "strategy" is somewhat unclear. Could the authors provide a more formal definition or explanation of what is meant by "strategy"?  
Is "r" intended to represent the discounted return at time t, or is it the immediate reward at time t?  
Could the authors provide a comparison of their method with Temporal-Difference (TD) learning?  
The paper lacks clarity and employs numerous reinforcement learning (RL) terms with varying meanings without adequately explaining these deviations.  
The statement, "So, the output for a given state-action pair is always the same," raises questions. By definition, the Q-function represents the value of a (state, action) pair. For a deterministic policy, the output would indeed always be the same. How does this differ from standard Q-learning?  
The model description does not clearly define the policy, mentioning it only in the context of data generation.  
What justifies categorizing this as a model-based approach?  
The learning curves presented span only 19 iterations, which is insufficient to provide meaningful insights. Furthermore, the final results are not comparable to prior works. The model has been tested on only three games.  
Overall, the paper is vague, uses informal language, and occasionally misuses standard RL terminology. The experiments are conducted on a very small scale and yield poor performance even in this limited setting. Additionally, it remains unclear why the approach is considered model-based.