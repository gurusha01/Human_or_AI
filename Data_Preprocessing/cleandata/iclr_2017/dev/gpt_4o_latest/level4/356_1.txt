The paper introduces a method for synthesizing string manipulation programs using a set of input-output examples. It focuses on a restricted class of programs defined by a simple context-free grammar, which is sufficient to address string manipulation tasks from the FlashFill benchmark. The authors propose a probabilistic generative model, the Recursive-Reverse-Recursive Neural Network (R3NN), which assigns probabilities to program parse trees through a combination of bottom-up and top-down passes. Experimental results are provided on a synthetic dataset and the Microsoft Excel FlashFill benchmark.
Program synthesis is a significant problem that has garnered substantial interest from the deep learning community in recent years. The paper's approach, which leverages parse trees and recursive neural networks, is intriguing and holds promise. However, the model's complexity and lack of clarity in several sections (detailed below) raise concerns. On the downside, the experimental results are particularly weak, and the paper does not appear ready for publication in its current form. Initially, I was optimistic about the paper, but my concerns grew upon realizing that the method achieves only 38% accuracy on the FlashFill benchmark with 5 input-output examples, and this performance drops further to 29% when 10 examples are used. This unexpected behavior surprised the authors as well, and they proposed hypotheses to explain it. However, I see this as a critical issue, suggesting either a bug in the implementation or a fundamental limitation of the model. For a program synthesis model to be practical, it must handle multiple input-output examples effectively, as complex programs often require several examples to resolve ambiguities.
Given the experimental shortcomings, I do not believe the paper is ready for publication at this time. Therefore, I recommend a weak reject. That said, I encourage the authors to address the concerns outlined below and resubmit, as the core idea of the paper is promising.
Additional comments:
I found the model unclear in several areas:
- How is the probability distribution normalized? Given the bottom-up and top-down evaluation of potentials, does this require enumerating all possible completions of a program and comparing their exponentiated potentials? If so, does this limit the model's applicability to longer programs due to the computational cost of enumerating completions?
- What happens if only 1 input-output pair is used per program instead of 5? Does this improve the results?
- Section 5.1.2 is difficult to follow. Could you clarify this section, perhaps with examples? Does your input-output representation assume a fixed number of examples across all tasks (e.g., always 5 or 10 examples per task)?
Regarding the experimental results:
- Could you provide baseline results on the FlashFill benchmark from prior work for comparison?
- Is your method restricted to short programs, given the choice of 13 as the maximum number of instructions?
- Is a program considered correct only if it matches the test program exactly, or is it deemed correct if it produces the correct outputs on a held-out set of input-output examples?
- When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e., recall), or do you first filter programs based on training input-output pairs and then evaluate the selected program?
Finally, the paper exceeds the recommended 8-page limit. Please consider condensing it to improve readability and adhere to the guidelines.