This paper presents a model that combines concepts from generative topic models with those from recurrent neural network (RNN) language models. The authors evaluate their proposed method on both a document-level classification benchmark and a language modeling benchmark, demonstrating promising results. Additionally, the paper includes an analysis of the topics learned by the model and its ability to generate text. Overall, the paper is well-written, and with the authors committing to releasing the code, others should be able to replicate the approach. However, I have two potentially significant concerns that I would like the authors to address:
1 - Traditional LDA topic models assume exchangeability (bag-of-words assumption). The paper should explicitly clarify whether this assumption is also made in the generative story of TopicRNN. At first glance, it appears to be the case since \( yt \) is sampled using only the document topic vector and \( ht \). However, in practice, \( ht \) is derived from a recurrent model that observes \( y{t-1} \). The relationship between the clean generative model specification and the actual implementation is unclear. In the "Generating Sequential Text" section, it is evident that the topic model cannot generate words without using \( y1 \) to \( y{t-1} \), which appears inconsistent with the generative model description. This discrepancy needs to be addressed in the paper to ensure a complete and coherent presentation.
2 - The topic model restricts interactions of the topic vector \( \theta \) to be linear. While this might be necessary to maintain the tractability of the generative model, it seems like a limiting assumption. Intuitively, one would expect the topic representation to interact more richly with the language model, enabling nonlinear adjustments to word probabilities within a document. The authors should discuss the rationale behind this modeling choice and, if possible, suggest how future work could relax this assumption. Alternatively, they should provide justification for why this assumption may not be as limiting as it initially appears.
Lastly, the colors in Figure 2 are difficult to distinguish and should be improved for better readability.