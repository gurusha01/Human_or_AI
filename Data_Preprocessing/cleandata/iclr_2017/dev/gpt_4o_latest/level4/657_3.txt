This paper presents a method to approximate the FastText approach in a way that significantly reduces its memory footprint—by several orders of magnitude—while maintaining its classification accuracy. The original FastText method relies on a linear classifier applied to bag-of-words embeddings. While this approach is extremely efficient in terms of training and testing speed, it tends to result in a large model size.
The focus of this paper is on approximating the original method using lossy compression techniques. Specifically, the embeddings and classifier matrices, A and B, are compressed using Product Quantization (PQ), and an aggressive pruning of the dictionary is performed. The authors conduct experiments across various datasets, including those with both small and large numbers of classes, to tune the parameters and validate the approach's effectiveness. The results demonstrate that with minimal loss in classification accuracy, the model size can be reduced significantly—by a factor of 100 to 1000 compared to the original size.
The paper is generally well written. The objectives are clearly defined and effectively executed, and the experiments are thorough. Additionally, the paper evaluates and compares different techniques for compressing the model data (e.g., PQ versus LSH), which adds value. However, the paper does not introduce any fundamentally novel ideas for text classification. Instead, it focuses on adapting existing lossy compression techniques, which is not necessarily a drawback. Specifically, the contributions include:
  - a simple adaptation of PQ for unnormalized vectors,
  - framing dictionary pruning as a set covering problem (an NP-hard problem), with a greedy approach shown to work effectively,
  - the use of hashing tricks and bloom filters borrowed from prior research.
These techniques are fairly generic and could be applied to other contexts as well.
The paper has some minor shortcomings:
  - The computation of the full model size is not clearly explained. What exactly constitutes the model? What proportion of the total size is attributable to the A and B matrices, the dictionary, and other components? It is difficult to pinpoint the size bottleneck, which also appears to vary depending on the target application (e.g., small versus large numbers of test classes). A formula to compute the total model size as a function of parameters (e.g., k, b for PQ, and K for the dictionary, as well as the number of classes) would have been helpful.
  
  - Certain parts of the paper lack clarity. For example, the greedy approach for dictionary pruning is described in fewer than four lines (top of page 5), even though it is not straightforward. Similarly, it is unclear why the binary search used in the hashing trick would introduce an overhead of a few hundred kilobytes.
  
Overall, this is a solid piece of work, though its research impact may be somewhat limited.