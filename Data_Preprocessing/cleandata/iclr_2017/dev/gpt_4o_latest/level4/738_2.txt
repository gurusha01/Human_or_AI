This paper demonstrates that sparse operations can be employed during the training of LSTM networks without compromising accuracy. While this finding is novel (albeit not entirely unexpected), I should note that I do not have extensive familiarity with the literature on fast RNN implementations.
Minor comment:  
The LSTM language model described does not incorporate a 'word2vec' layer. Instead, it uses a straightforward linear embedding layer. It is worth clarifying that 'word2vec' refers to a specific model and is not directly applicable to character-level language models.
The paper articulates its main observation clearly. However, its impact would be significantly enhanced if the authors employed a widely recognized dataset and experimental setup, such as those used in Graves (2013) or Sutskever et al. (2014), and provided detailed training, validation, and test performance metrics.
While the core observation is intriguing, it falls short of justifying a full conference paper in its current form without accompanying implementation (or simulation) and benchmarking of the proposed speedups across multiple tasks. For instance, it would be valuable to explore how the reported gains vary with different architectural choices.
As it stands, this work represents an interesting technical report, and I look forward to seeing more comprehensive results in the future.