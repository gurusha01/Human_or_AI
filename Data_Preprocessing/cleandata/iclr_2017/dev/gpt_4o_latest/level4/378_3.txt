The paper introduces a novel algorithm based on REINFORCE, designed to explore underappreciated action sequences. The core idea involves comparing the probability of an action sequence under the current policy with its estimated reward. Actions for which the current policy underestimates the reward receive higher feedback, thereby promoting the exploration of specific action sequences. The UREX model is evaluated on six algorithmic RL problems, demonstrating intriguing properties when compared to the standard regularized REINFORCE (MENT) model and Q-Learning.
The proposed model is compelling, well-defined, and clearly articulated. To the best of my knowledge, the UREX model is an original contribution that holds significant potential for the RL community. However, the main limitation of the paper lies in confining the evaluation of the algorithm to specific algorithmic problems. Given the ease with which the model could be tested on other standard RL tasks, expanding the evaluation to include such tasks would strengthen the paper considerably. I strongly encourage the authors to incorporate additional tasks in their work.