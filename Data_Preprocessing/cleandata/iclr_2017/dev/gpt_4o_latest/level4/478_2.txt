The authors demonstrate that a contrastive loss within a Siamese architecture can be employed to learn representations for planar curves. Using their proposed framework, they achieve a representation that is comparable to traditional differential or integral invariants, as assessed on a few toy examples.
The manuscript is generally well-written and presents an intriguing application of the Siamese architecture. However, the experimental evaluation and results suggest that these are preliminary findings, as many design choices remain unvalidated. My primary concern lies in the selection of negative samples, as the network appears to primarily learn to distinguish between shapes at different scales rather than identifying distinct shapes. It is well established that achieving strong performance with contrastive loss requires careful selection of hard negative samples, as relying on overly simple negatives can lead to suboptimal results. Could this explain the choice of negatives in this work? Unfortunately, this critical aspect is not addressed in the paper.
Additionally, the paper lacks a more comprehensive quantitative evaluation, focusing instead on specific examples rather than providing robust statistical analyses across multiple curves (e.g., invariance to noise and sampling artifacts).
Overall, the paper represents an interesting initial exploration in this domain. However, it is unclear whether the experimental section is sufficiently robust and thorough for acceptance at ICLR. Furthermore, the novelty of the proposed approach is somewhat limited, as Siamese networks have been widely used for years, and this work primarily demonstrates their applicability to a new task.