I lack sufficient expertise in mean-field techniques to fully assess the validity of Eq. 2, but I am inclined to accept it as presented.
Minor comment on clarity: Referring to the "evolution" of \( x{i;a} \) as it propagates through the network might provide useful intuition for some readers. However, I found it somewhat confusing because \( x{*;a} \) represents the fixed input vector, while the newly introduced variables \( z \) and \( y \) are the ones that actually describe its so-called evolution. Is that correct?
Regarding the interpretation of this analysis: A network might be trainable even if information initially does not pass through it, provided that the training process alters the weights in such a way that information begins to flow through the networkâ€”and does not subsequently modify the weights to block this flow again. Would it be possible to make this clearer by explicitly defining what is meant by a "training algorithm"?
Comments on the primary claims:
Prior research on initializing neural networks to encourage information flow (e.g., Glorot & Bengio,