This paper introduces an approach that involves looking n-steps backward when modeling sequences with RNNs. Instead of relying solely on the previous hidden state (t-1), the proposed RNN incorporates additional hidden states from further back in time ((t-k) steps, where k=1,2,3,4). Additionally, the paper explores several methods for aggregating multiple hidden states from the past.
The reviewer has identified several concerns with this paper.
First, the writing quality of the paper needs improvement. Both the introduction and abstract devote excessive space to explaining unrelated facts or reiterating well-known concepts from the literature. Some statements in the paper are misleading. For example, the paper claims, "Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long-term dependency in sequential data using a simple mechanism of recurrent feedback," but later contradicts itself by stating that RNNs struggle to capture long-term dependencies effectively. In reality, RNNs are valued primarily for their ability to handle variable-length sequences and model temporal relationships between symbols in a sequence. The critique of LSTMs is also unconvincing, as the paper argues that LSTMs are slow and difficult to scale for larger tasks. However, it is well-known that some companies already deploy large-scale seq2seq models in production, with LSTMs serving as core components. This demonstrates that LSTMs can indeed be scaled effectively for large-scale applications.
Second, the proposed idea in the paper is incremental and lacks novelty. Prior work has already explored the use of direct connections to earlier hidden states [1]. While the aggregation of multiple hidden states is a distinguishing feature of this paper, the authors fail to provide a thorough analysis of whether this contribution meaningfully addresses the problem posed. The paper claims that the new architecture improves the handling of long-term dependencies, but it does not offer rigorous evidence or intuitive reasoning to support this claim. Based on the architecture's design, it appears, at a high level, that the model may mitigate the vanishing gradient problem to some extent, potentially by a linear factor. However, the lack of detailed analysis, such as a dedicated section discussing the empirical findings, weakens the paper's contribution.
Third, the baseline models used in the experiments are outdated and weak. Numerous models have been trained and evaluated on the Penn Treebank corpus for word-level language modeling, yet the paper only compares its results to a limited set of older models. The claim, "To the best of our knowledge, this is the best performance on PTB under the same training condition," is difficult to accept. Modern RNN-based methods typically achieve test perplexity scores below 80 on this task, whereas the proposed method achieves a perplexity of 100, which is significantly higher.
[1] Zhang et al., "Architectural Complexity Measures of Recurrent Neural Networks," NIPS'16