Summary
The paper proposes a novel approach to exploration in complex domains with sparse rewards using intrinsic motivation. The authors formulate surprise as the KL-divergence of the true transition probabilities from a learned model and derive two approximations: surprisal and k-step learning progress. They demonstrate the effectiveness of these incentives in a range of environments, including continuous control tasks and Atari games, outperforming other heuristic exploration techniques.
Decision
I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and grounded in the literature, and (2) the empirical results demonstrate the effectiveness of the proposed incentives in various environments.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of exploration in complex domains and motivates the use of intrinsic motivation. The authors provide a thorough review of related work and clearly distinguish their approach from existing methods. The empirical results are extensive and demonstrate the robustness of the proposed incentives across different environments. The use of Trust Region Policy Optimization (TRPO) as the base reinforcement learning algorithm and the comparison to other intrinsic motivation methods, such as Variational Information Maximizing Exploration (VIME), add to the paper's strength.
Additional Feedback
To further improve the paper, I suggest the authors provide more insight into the choice of hyperparameters, particularly the KL-divergence step size and the sub-sample factor. Additionally, it would be interesting to see a more detailed analysis of the speedup compared to VIME, including a breakdown of the time costs incurred by each method. Finally, I would like the authors to clarify the relationship between the proposed incentives and Bayesian surprise, as mentioned in Section 3.1.
Questions for the Authors
1. Can you provide more details on the choice of hyperparameters, particularly the KL-divergence step size and the sub-sample factor?
2. How do you plan to extend this work to more complex environments, such as those with high-dimensional state and action spaces?
3. Can you provide a more detailed analysis of the speedup compared to VIME, including a breakdown of the time costs incurred by each method?