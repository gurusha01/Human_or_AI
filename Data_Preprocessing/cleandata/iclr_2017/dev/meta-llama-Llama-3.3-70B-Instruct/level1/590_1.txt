Summary
The paper proposes a novel neural reading comprehension model, called Dynamic Chunk Reader (DCR), which can extract and rank answer candidates from a given document to answer questions. DCR is capable of predicting answers of variable lengths and can handle both factoid and non-factoid questions. The model uses a joint deep learning approach, enhanced with a novel attention mechanism and five simple yet effective features. The authors evaluate DCR on the Stanford Question Answering Dataset (SQuAD) and achieve a 66.3% Exact match and 74.7% F1 score.
Decision
I decide to Accept this paper with minor revisions. The paper tackles a specific and important problem in the field of natural language processing, and the approach is well-motivated and well-placed in the literature. The results are impressive, and the authors provide a thorough analysis of the model's performance.
Supporting Arguments
The paper addresses a significant challenge in reading comprehension, which is predicting answers of variable lengths. The authors propose a novel attention mechanism and a joint deep learning approach, which allows the model to dynamically construct and rank answer candidates. The results on the SQuAD dataset are impressive, and the authors provide a thorough analysis of the model's performance, including error analysis and ablation tests.
Additional Feedback
To further improve the paper, I suggest the authors provide more details on the implementation of the model, such as the hyperparameter settings and the training procedure. Additionally, it would be helpful to include more visualizations, such as plots of the attention weights, to provide a better understanding of the model's behavior. Finally, the authors may want to consider evaluating the model on other datasets, such as the CNN/Daily Mail dataset, to demonstrate its generalizability.
Questions for the Authors
1. Can you provide more details on the implementation of the attention mechanism, such as the choice of hyperparameters and the training procedure?
2. How do you plan to address the issue of predicting longer answers, which are usually non-factoid in nature?
3. Have you considered evaluating the model on other datasets, such as the CNN/Daily Mail dataset, to demonstrate its generalizability?