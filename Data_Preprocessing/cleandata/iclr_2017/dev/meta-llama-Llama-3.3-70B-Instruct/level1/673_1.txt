Summary
The paper proposes a Hierarchical Memory Network (HMN) that uses Maximum Inner Product Search (MIPS) to efficiently access a large external memory. The HMN is designed to overcome the limitations of soft attention mechanisms, which are computationally expensive and prone to dispersed gradients, and hard attention mechanisms, which are challenging to train. The authors demonstrate the effectiveness of HMN on the SimpleQuestions dataset, showing that exact K-MIPS attention outperforms soft attention and that approximate K-MIPS techniques can provide a speedup at the cost of some accuracy.
Decision
I decide to Accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and important problem in the field of neural networks with memory, and (2) the approach is well-motivated and supported by empirical results.
Supporting Arguments
The paper clearly identifies the limitations of existing attention mechanisms and proposes a novel solution that addresses these limitations. The authors provide a thorough analysis of the HMN architecture and its components, including the memory structure and the reader module. The empirical results demonstrate the effectiveness of HMN on a large-scale question answering task, and the authors provide a detailed comparison of different approximate K-MIPS techniques.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational complexity of the HMN architecture and its components. Additionally, it would be helpful to include more analysis on the trade-offs between accuracy and speedup achieved by the approximate K-MIPS techniques. Finally, the authors may want to consider exploring other applications of HMN beyond question answering, such as image captioning or dialogue systems.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the memory structure and how it is organized in a hierarchical way?
2. How do you select the hyper-parameters for the approximate K-MIPS techniques, and what is the effect of these hyper-parameters on the performance of HMN?
3. Have you considered using other attention mechanisms, such as hard attention or reinforcement learning-based attention, in conjunction with HMN?