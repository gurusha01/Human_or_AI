The paper proposes a novel language model called TopicRNN, which integrates the strengths of recurrent neural networks (RNNs) and latent topic models to capture both local and global dependencies in language. The authors claim that TopicRNN outperforms existing contextual RNN baselines on word prediction tasks and achieves competitive results on sentiment analysis.
I decide to accept this paper with two key reasons: (1) the approach is well-motivated and grounded in the literature, and (2) the paper provides convincing empirical results to support its claims.
The authors provide a clear and thorough explanation of the background and related work, demonstrating a good understanding of the strengths and limitations of existing language models. The proposed TopicRNN model is well-designed, and the use of variational inference to learn the model parameters is a nice touch. The empirical results on word prediction and sentiment analysis are impressive, and the authors provide a detailed analysis of the results, including visualizations of the learned topics.
To further improve the paper, I would like to see more discussion on the following points: (1) how the choice of topic model (e.g., LDA vs. correlated topic model) affects the performance of TopicRNN, and (2) whether the model can be applied to other natural language processing tasks beyond word prediction and sentiment analysis. Additionally, it would be helpful to see more examples of generated text from the model to better understand its capabilities.
Some questions I would like the authors to answer to clarify my understanding of the paper are: (1) how do the authors determine the optimal number of topics for a given dataset, and (2) whether the model can handle out-of-vocabulary words or require additional preprocessing steps. Overall, the paper is well-written, and the proposed model has the potential to make a significant impact in the field of natural language processing.