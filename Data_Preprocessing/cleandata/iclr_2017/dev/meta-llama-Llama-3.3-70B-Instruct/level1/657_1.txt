Summary
The paper proposes a method to reduce the memory complexity of text classification models while maintaining their accuracy. The authors extend the fastText library by applying product quantization to store word embeddings, which leads to a significant reduction in memory usage. They also introduce a pruning technique to select the most important features, resulting in a further reduction in model size. The authors demonstrate the effectiveness of their approach on several benchmarks, achieving a compression factor of up to 1000 with minimal loss in accuracy.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by thorough experiments. The authors provide a clear and detailed explanation of their method, and the results demonstrate a significant improvement in memory efficiency without sacrificing accuracy.
Supporting Arguments
The paper tackles a specific and relevant problem in natural language processing, namely reducing the memory complexity of text classification models. The authors provide a thorough review of related work and motivate their approach by highlighting the limitations of existing methods. The proposed method is well-explained, and the experiments are comprehensive and well-designed. The results demonstrate a significant reduction in memory usage while maintaining accuracy, making the approach a valuable contribution to the field.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational cost of their method, particularly in terms of training time. Additionally, it would be interesting to see a comparison with other state-of-the-art methods, such as convolutional neural networks, in terms of both accuracy and memory efficiency. The authors may also consider providing more insights into the trade-offs between different hyperparameters, such as the number of subquantizers and the pruning threshold.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to handle out-of-vocabulary words in the proposed method?
2. Can the authors provide more details on the implementation of the pruning technique, particularly in terms of the online parallelizable greedy approach?
3. How do the authors envision the proposed method being applied to other natural language processing tasks, such as language modeling or machine translation?