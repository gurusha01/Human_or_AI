This paper proposes a novel neural network structure, called Higher Order Recurrent Neural Networks (HORNNs), to better model long-term dependency in sequential data. The authors extend the traditional Recurrent Neural Network (RNN) structure by using more memory units to keep track of preceding states, which are fed to the hidden layers as feedback through different weighted paths. The paper also explores various pooling functions to calibrate the signals from different feedback paths.
I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper supports its claims with thorough experiments and results.
The approach is well-motivated as it addresses the vanishing gradient problem in traditional RNNs, which hinders their ability to capture long-term dependencies. The authors provide a clear explanation of the limitations of traditional RNNs and LSTMs, and propose a novel solution that extends the traditional RNN structure. The paper is also well-placed in the literature, as it references relevant works in the field and provides a thorough comparison with existing models.
The paper supports its claims with thorough experiments and results on two popular language modeling datasets, Penn Treebank (PTB) and English text8. The results show that the proposed HORNNs outperform traditional RNNs and LSTMs, and achieve state-of-the-art performance on both datasets. The authors also provide a detailed analysis of the effects of different pooling functions and model configurations, which demonstrates the effectiveness of their approach.
To further improve the paper, I suggest that the authors provide more insights into the computational complexity of HORNNs and their scalability to larger datasets. Additionally, it would be interesting to see more comparisons with other state-of-the-art models, such as attention-based models and transformer-based models.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend HORNNs to other sequential modeling tasks, such as speech recognition and sequence-to-sequence modeling? (2) Can the authors provide more details on the implementation of HORNNs, including the specific architectures and hyperparameters used in the experiments? (3) How do the authors plan to address the potential issue of overfitting in HORNNs, especially when using larger models and more complex pooling functions?