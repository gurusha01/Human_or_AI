Summary
This paper introduces a novel approach to learning about physical properties of objects through active interaction in a simulated environment. The authors draw inspiration from developmental psychology and propose a deep reinforcement learning framework that enables agents to conduct experiments to infer hidden properties such as mass and cohesion. The paper presents two environments, "Which is Heavier" and "Towers", where agents learn to manipulate objects and observe the consequences to answer questions about physical properties. The results demonstrate that agents can learn different strategies to balance the cost of gathering information and making mistakes in various situations.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and well-motivated question of how to learn physical properties of objects through active interaction, and (2) the approach is well-supported by empirical results and rigorous analysis.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem, motivation, and related work. The authors design two environments that require agents to interact with objects to infer physical properties, and the results show that agents can learn effective strategies to solve these tasks. The paper also provides a thorough analysis of the agents' behavior, including the trade-off between information gathering and risk of making mistakes. The use of deep reinforcement learning and the design of the environments are well-suited to the problem, and the results demonstrate the efficacy of the approach.
Additional Feedback
To further improve the paper, I suggest that the authors consider the following points: (1) provide more details on the implementation of the environments and the reinforcement learning algorithm, (2) discuss the potential limitations and challenges of scaling the approach to more complex environments, and (3) explore the possibility of transferring the learned knowledge to new tasks or environments.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the agents' strategies change as the problem difficulty increases, and what are the implications for real-world applications? (2) Can the authors provide more insights into the agents' decision-making process, such as how they balance exploration and exploitation? (3) How do the results generalize to other physical properties, such as friction or deformability, and what are the potential applications of this approach in robotics or computer vision?