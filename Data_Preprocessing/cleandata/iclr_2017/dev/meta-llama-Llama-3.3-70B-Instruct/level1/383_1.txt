Summary
The paper introduces MetaQNN, a meta-modeling algorithm based on reinforcement learning that automatically generates high-performing convolutional neural network (CNN) architectures for image classification tasks. The algorithm uses Q-learning with an epsilon-greedy exploration strategy and experience replay to explore a large but finite space of possible architectures. The authors demonstrate the effectiveness of MetaQNN on three image classification benchmarks, CIFAR-10, SVHN, and MNIST, and show that the generated architectures outperform existing hand-crafted networks and other automated network design methods.
Decision
I decide to Accept this paper with minor revisions.
Reasons
The paper tackles a specific and important problem in the field of deep learning, namely the automation of CNN architecture design. The approach is well-motivated and well-placed in the literature, drawing on recent advances in reinforcement learning and meta-learning. The results are impressive, with MetaQNN generating architectures that outperform state-of-the-art methods on several benchmarks.
Supporting Arguments
The paper provides a clear and detailed description of the MetaQNN algorithm, including the Q-learning formulation, the epsilon-greedy exploration strategy, and the experience replay mechanism. The authors also provide a thorough analysis of the results, including a comparison with other automated network design methods and an analysis of the generated architectures. The paper is well-written and easy to follow, with clear and concise language.
Additional Feedback
To further improve the paper, I suggest the following:
* Provide more details on the computational resources required to run MetaQNN, including the number of GPUs and the training time.
* Consider adding more benchmarks or datasets to demonstrate the generality of MetaQNN.
* Provide more insight into the learned architectures, such as visualizations of the generated networks or analysis of the layer types and connections.
* Consider comparing MetaQNN with other reinforcement learning-based methods for neural architecture search.
Questions for the Authors
I would like the authors to clarify the following points:
* How did the authors choose the hyperparameters for MetaQNN, such as the learning rate and the epsilon schedule?
* Can the authors provide more details on the implementation of the experience replay mechanism, including the size of the replay buffer and the sampling strategy?
* How do the authors plan to extend MetaQNN to other domains, such as natural language processing or speech recognition?