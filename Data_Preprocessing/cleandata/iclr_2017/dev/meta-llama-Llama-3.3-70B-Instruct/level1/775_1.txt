Summary
The paper explores the impact of different action parameterizations on learning to control dynamic articulated figure locomotion using deep reinforcement learning. The authors compare four action parameterizations: torques, muscle activations, target joint angles, and target joint velocities, and evaluate their effects on learning time, policy robustness, motion quality, and policy query rates. The results show that action parameterizations with local feedback, such as target joint angles and muscle activations, can improve policy performance and learning speed across different motions and character morphologies.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-motivated question, namely the impact of action parameterization on learning to control dynamic articulated figure locomotion. Secondly, the paper provides a thorough and rigorous evaluation of the different action parameterizations, including a comprehensive analysis of the results and a discussion of the limitations and potential future work.
Supporting Arguments
The paper is well-motivated, as it addresses a significant gap in the current understanding of deep reinforcement learning, namely the impact of action parameterization on learning to control complex systems. The authors provide a clear and concise introduction to the problem, and motivate the choice of action parameterizations evaluated in the paper. The paper is also well-placed in the literature, as it builds on existing work in deep reinforcement learning and biomechanics. The authors provide a thorough evaluation of the different action parameterizations, including a comprehensive analysis of the results and a discussion of the limitations and potential future work. The results are scientifically rigorous, as they are based on a thorough and well-designed experimental evaluation.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the implementation of the actor-critic learning algorithm, including the specific hyperparameters used and the optimization technique employed. Additionally, the authors could provide more discussion on the potential applications of the results, such as the use of action parameterizations in robotics or computer animation. Furthermore, the authors could consider evaluating the performance of the policies on more complex tasks, such as 3D locomotion or manipulation tasks.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How did the authors choose the specific action parameterizations evaluated in the paper, and what were the criteria used to select them?
* Can the authors provide more details on the optimization technique used to optimize the actuator parameters for the muscle activation model, and how it was implemented?
* How do the authors plan to extend the results to 3D locomotion, and what are the potential challenges and limitations of doing so?