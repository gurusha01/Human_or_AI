Summary
This paper studies the behavior of untrained neural networks with randomly distributed weights and biases using mean field theory. The authors identify two depth scales, ξq and ξc, that control the propagation of information about the magnitude of a single input and the correlation between two inputs, respectively. They show that ξc diverges at the order-to-chaos transition, allowing for the training of arbitrarily deep networks near criticality. The authors also develop a mean field theory for backpropagation, demonstrating a duality between the forward propagation of signals and the backpropagation of gradients. They validate their theoretical results with experiments on MNIST and CIFAR10, showing that the depth scale ξc accurately predicts the trainable region of hyperparameters.
Decision
I decide to Accept this paper, with the main reasons being:
1. The paper tackles a specific and well-defined problem in the field of neural networks, namely the understanding of the behavior of untrained networks and the identification of depth scales that control information propagation.
2. The approach is well-motivated and grounded in mean field theory, which provides a solid foundation for the analysis.
3. The paper provides a clear and thorough derivation of the theoretical results, including the identification of the depth scales and the development of a mean field theory for backpropagation.
Supporting Arguments
The paper presents a comprehensive and well-structured analysis of the behavior of untrained neural networks. The authors provide a clear and concise introduction to the problem, followed by a detailed derivation of the theoretical results. The experiments on MNIST and CIFAR10 provide strong evidence for the validity of the theoretical predictions, demonstrating that the depth scale ξc accurately predicts the trainable region of hyperparameters.
Additional Feedback
To further improve the paper, I suggest that the authors:
1. Provide more intuition and explanation for the physical meaning of the depth scales ξq and ξc, and how they relate to the behavior of the network.
2. Consider extending the analysis to more complex network architectures, such as convolutional neural networks, to demonstrate the generality of the results.
3. Provide more discussion on the implications of the results for the design and training of neural networks, including potential applications and future directions.
Questions for the Authors
1. Can you provide more insight into the relationship between the depth scales ξq and ξc, and how they relate to the behavior of the network?
2. How do the results generalize to more complex network architectures, such as convolutional neural networks?
3. What are the potential implications of the results for the design and training of neural networks, and what are some potential future directions for research in this area?