Summary of the Paper's Claims and Contributions
The paper proposes a novel training procedure for learning a generative model as a Markov chain, which progressively denoises an initial random noise sample into a high-quality sample that matches the target distribution. The approach, called "infusion training," involves sampling from a slightly different chain than the model chain used for generation, where information from the training target example is infused into the chain. The authors claim that this approach can produce high-quality and varied samples in a small number of steps, and demonstrate its effectiveness through experiments on several datasets, including MNIST, Toronto Face Database, CIFAR-10, and CelebA.
Decision and Key Reasons
Based on the review, I decide to Accept this paper. The key reasons for this decision are:
1. The paper tackles a specific and well-motivated problem in the field of generative models, which is to learn a transition operator that can efficiently generate high-quality samples.
2. The approach proposed in the paper is well-placed in the literature, building on previous work on denoising autoencoders, generative stochastic networks, and Markov chain Monte Carlo methods.
3. The paper provides a clear and detailed explanation of the infusion training procedure, including the mathematical formulation and the experimental setup.
Supporting Arguments
The paper provides several supporting arguments for the effectiveness of the infusion training approach, including:
1. Theoretical analysis: The authors provide a theoretical analysis of the infusion training procedure, including a derivation of the lower bound on the log-likelihood of the generative model.
2. Experimental results: The authors present experimental results on several datasets, demonstrating the effectiveness of the infusion training approach in generating high-quality and varied samples.
3. Comparison to previous work: The authors compare their approach to previous work on generative models, including GANs and VAEs, and demonstrate that their approach can produce competitive results.
Additional Feedback and Questions
To further improve the paper, I would like to see:
1. More detailed analysis of the infusion rate schedule: The authors mention that the infusion rate schedule is important for the success of the approach, but do not provide a detailed analysis of how to choose the optimal schedule.
2. More experiments on larger datasets: While the authors demonstrate the effectiveness of the approach on several datasets, it would be interesting to see how the approach scales to larger datasets.
3. More comparison to other generative models: The authors compare their approach to GANs and VAEs, but it would be interesting to see how the approach compares to other generative models, such as normalizing flows or autoregressive models.
Some questions I would like the authors to answer are:
1. How do the authors choose the optimal infusion rate schedule for a given dataset?
2. Can the authors provide more insight into why the infusion training approach is able to produce high-quality samples in a small number of steps?
3. How does the authors' approach relate to other work on generative models, such as the recent work on denoising diffusion models?