Summary of the Paper
The paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. The proposed approach, called Under-Appreciated Reward Exploration (UREX), promotes exploration of action sequences that yield rewards larger than what the model expects. UREX is a combination of a mode-seeking objective (standard REINFORCE) and a mean-seeking term, which provides a well-motivated trade-off between exploitation and exploration. The authors evaluate UREX on a set of algorithmic tasks and demonstrate significant improvements over baseline methods, including entropy-regularized REINFORCE and one-step Q-learning.
Decision
I decide to Accept this paper with minor revisions. The paper tackles a specific and important problem in RL, and the proposed approach is well-motivated and supported by theoretical and empirical evidence.
Supporting Arguments
1. Specific question/problem tackled: The paper addresses a clear and specific problem in RL, namely, the need for more effective exploration strategies in high-dimensional spaces with sparse rewards.
2. Well-motivated approach: The authors provide a clear and well-motivated explanation of the UREX approach, including its relationship to existing methods and its advantages over them.
3. Empirical evidence: The authors provide extensive empirical evidence to support the effectiveness of UREX, including results on a range of algorithmic tasks and comparisons to baseline methods.
Additional Feedback
To further improve the paper, I suggest the following:
* Provide more detailed explanations of the UREX algorithm and its implementation, including any hyperparameter settings and tuning procedures used in the experiments.
* Consider adding more theoretical analysis of the UREX approach, including its convergence properties and relationships to other RL methods.
* Provide more discussion of the potential applications and limitations of the UREX approach, including its potential for use in more complex and realistic RL environments.
Questions for the Authors
1. Can you provide more details on the hyperparameter settings and tuning procedures used in the experiments?
2. How do you plan to extend the UREX approach to more complex and realistic RL environments, such as those with high-dimensional state and action spaces?
3. Can you provide more discussion of the potential limitations and challenges of the UREX approach, including any potential biases or weaknesses in the algorithm?