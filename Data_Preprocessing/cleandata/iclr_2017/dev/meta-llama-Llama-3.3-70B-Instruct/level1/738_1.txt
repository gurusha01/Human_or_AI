Summary
The paper proposes a novel approach to induce sparsity in the gradients of Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs) during training, leveraging the skewed distribution of gate activations. The authors introduce a simple yet effective thresholding technique, "sparsified" Stochastic Gradient Descent (SGD), which rounds small gradient values to zero, resulting in increased sparsity without loss of performance. The experimental results demonstrate that the proposed technique can achieve over 80% sparsity in linear gate gradients, leading to more than 50% redundant multiply-accumulate (MAC) operations in the entire LSTM training process.
Decision
I decide to Accept this paper, with the primary reason being that the approach is well-motivated, and the results are scientifically rigorous and correct. The paper provides a clear and thorough analysis of the problem, and the proposed solution is simple yet effective.
Supporting Arguments
The paper tackles a specific and relevant problem in the field of deep learning, namely, improving the energy efficiency and reducing the memory footprint of LSTM-based RNNs. The approach is well-placed in the literature, building upon existing work on sparsity-centric optimization techniques for Convolutional Neural Networks (CNNs). The authors provide a thorough analysis of the skewed distribution of gate activations and its implications for the gradients, which is a key insight that enables the proposed solution. The experimental results are convincing, demonstrating the effectiveness of the proposed technique across various applications and datasets.
Additional Feedback
To further improve the paper, I suggest that the authors consider providing more detailed analysis of the trade-offs between sparsity and performance, as well as exploring the applicability of the proposed technique to other types of RNNs and deep learning models. Additionally, it would be interesting to see a more detailed discussion of the potential hardware implications of the proposed technique, including the design of specialized hardware accelerators to exploit the induced sparsity.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more insight into the choice of threshold value and its impact on the trade-off between sparsity and performance?
2. How do you envision the proposed technique being integrated into existing deep learning frameworks and hardware accelerators?
3. Have you considered exploring the applicability of the proposed technique to other types of RNNs, such as Gated Recurrent Units (GRUs) or Bidirectional RNNs?