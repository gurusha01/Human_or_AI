Summary
The paper proposes a decision-level fusion approach for multi-modal product classification using text and image inputs. The authors train input-specific state-of-the-art deep neural networks for each input source and demonstrate that their multi-modal network improves the top-1 accuracy over both networks on a real-world large-scale product classification dataset collected from Walmart.com.
Decision
I decide to Accept this paper with the following key reasons:
1. The paper tackles a significant problem in e-commerce, which is product classification, and proposes a novel approach to fuse text and image inputs.
2. The authors provide a thorough analysis of the errors made by the different networks and demonstrate the potential gain of multi-modality.
Supporting Arguments
The paper is well-motivated, and the approach is well-placed in the literature. The authors provide a clear overview of the challenges in product classification and the limitations of existing methods. The proposed decision-level fusion approach is novel and shows promising results. The experiments are well-designed, and the results are thoroughly analyzed. The authors also provide a detailed discussion of the potential and limitations of their approach.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the dataset collection process and the preprocessing steps. Additionally, it would be interesting to see more analysis on the types of products that benefit the most from the multi-modal approach. The authors may also consider providing more comparisons with other state-of-the-art methods and discussing the potential applications of their approach in other domains.
Questions for the Authors
1. Can you provide more details on the policy network architecture and the training process?
2. How do you plan to address the issue of overfitting in the policy network?
3. Can you provide more insights on the types of products that benefit the most from the multi-modal approach?
4. Have you considered using other modalities, such as audio or video, in your approach?
5. How do you plan to extend your approach to other domains and applications?