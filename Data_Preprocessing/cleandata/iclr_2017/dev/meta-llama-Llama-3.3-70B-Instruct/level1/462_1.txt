Summary
The paper proposes a novel approach to detect adversarial perturbations in deep neural networks by augmenting the classification network with a small "detector" subnetwork. The detector is trained on a binary classification task to distinguish between genuine data and data containing adversarial perturbations. The authors demonstrate that adversarial perturbations can be detected surprisingly well, even though they are quasi-imperceptible to humans. The detector generalizes to similar and weaker adversaries, and the authors propose a novel training procedure to counteract dynamic adversaries that aim to fool both the classifier and the detector.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and important problem in the field of deep learning, namely the vulnerability of neural networks to adversarial perturbations, and (2) the approach proposed by the authors is well-motivated, and the experimental results demonstrate the effectiveness of the detector in detecting adversarial perturbations.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of adversarial perturbations and the limitations of existing approaches to address this issue. The authors propose a novel approach that is orthogonal to prior work, and the experimental results demonstrate the effectiveness of the detector in detecting adversarial perturbations. The paper also provides a thorough analysis of the results, including the generalizability of the detector to different adversaries and the robustness of the detector to dynamic adversaries.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the architecture of the detector subnetwork and the hyperparameters used to train the detector. Additionally, it would be interesting to see more experiments on the transferability of the detector to different datasets and network architectures. Furthermore, the authors may want to consider exploring the use of the detector as a regularization technique to improve the robustness of the classifier against adversarial examples.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend the approach to detect adversarial perturbations in more complex datasets, such as ImageNet? (2) Can the authors provide more insights into the types of features that the detector learns to detect adversarial perturbations? (3) How do the authors plan to address the potential issue of overfitting of the detector to the specific adversary used to train it?