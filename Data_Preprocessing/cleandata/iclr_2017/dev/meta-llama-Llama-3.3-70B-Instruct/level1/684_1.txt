Summary
The paper presents a novel model-based approach to deep reinforcement learning, called Predictive Reinforcement Learning (PRL), which learns a predictive model of the environment and uses it to make decisions. The approach is designed to overcome the limitations of traditional Q-learning methods, which struggle with multi-task learning and require a large amount of interaction with the environment. The authors demonstrate the effectiveness of PRL in learning multiple tasks simultaneously, including three Atari games, and show that it can surpass human performance without degrading.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a significant problem in reinforcement learning, namely multi-task learning, and presents a well-motivated and novel approach to address it; and (2) the experimental results demonstrate the effectiveness of the approach in learning multiple tasks simultaneously, which is a significant contribution to the field.
Supporting Arguments
The paper provides a clear and well-structured presentation of the PRL approach, including a detailed description of the model architecture and the training procedure. The authors also provide a thorough analysis of the results, including a comparison with individual models trained on each task separately. The use of a new kind of recurrent neural network, called Residual Recurrent Neural Network (RRNN), is also a significant contribution, as it allows for efficient learning of complex environments with minimal memory requirements.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the sensitivity of the results to different hyperparameter settings. Additionally, it would be interesting to see more experiments on other environments and tasks, to demonstrate the generality of the PRL approach. Finally, the authors may want to consider discussing the potential applications of PRL in real-world scenarios, such as robotics or autonomous driving.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How did you select the specific Atari games used in the experiments, and what criteria did you use to evaluate the performance of the PRL approach? (2) Can you provide more details on the training procedure, including the batch size, learning rate, and number of iterations used in the experiments? (3) How do you plan to address the instability issue mentioned in the discussion section, and what potential solutions do you propose to mitigate this problem?