This manuscript explores the underlying reasons for the effectiveness of deep networks in practice, as well as the impact of modifying pooling geometry on the separation rank of polynomially sized deep networks, particularly for specific partitioning schemes.
The authors' prior research demonstrated the advantages of deep networks over shallow ones when utilizing ReLu activation functions and max/mean pooling. However, the current paper examines a scenario without an activation function following convolution and employs a pooling method based on node value multiplication, while also considering both cases in the experimental results.
Given the inherent complexity of this problem, the lack of generalizability is not a significant limitation, and the present contribution meaningfully expands the existing body of knowledge. 
The paper delves into convolutional arithmetic circuits, illustrating how this model can capture inductive biases and how pooling can be used to adjust these biases. 
A key insight provided by this work is an understanding of how deep networks can efficiently capture correlations between input variables, even when the network size is polynomial but the correlation is exponential.
It is noteworthy that, although the authors have made a concerted effort to clearly define their notation and terminology, further elaboration on these aspects would enhance the accessibility of their definitions, expressions, and conclusions.