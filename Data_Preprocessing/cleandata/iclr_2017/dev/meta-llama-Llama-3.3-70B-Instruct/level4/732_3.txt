This study reinterprets paragraph vectors from a generative perspective, thereby providing a motivation for the existing approach of inferring paragraph vectors and applying an L2 regularizer to the paragraph embeddings. Additionally, it motivates the joint learning of a classifier on paragraph vectors for text classification purposes.
However, the paper is plagued by citation inconsistencies, both in-text and in the bibliography, such as the inconsistent inclusion of first names. Utilizing a software package like BibTex could help achieve a more uniform bibliography. A major concern is the apparent lack of novelty in this work.
The authors' claim that there is no existing method for inferring paragraph vectors for unseen documents is inaccurate. The original paragraph vector paper demonstrates that a new vector can be obtained by holding the rest of the model parameters fixed and performing gradient descent on the new paragraph vector, eliminating the need for the original dataset during inference. Essentially, this work replicates this process when finding the MAP estimate for a new vector, with the only notable contribution being the regularization of the embedding matrix stemming from the generative paragraph vector framework.
The supervised generative paragraph vector approach involves jointly training a linear classifier on the paragraph vectors, with no changes to the paragraph vector inference process. For the n-gram-based approach, a citation of Li et al.'s 2015 work is warranted.
The experimental results are marred by poorly formatted tables, with truncated values, and the authors fail to specify the size of the paragraph vector. Furthermore, the SGPV results are inferior to those reported in the original paragraph vector paper, where SST-1 achieved 48.7 and SST-2 achieved 86.3. Relevant literature, such as the work by Bofang Li et al. in 2015 on learning document embeddings by predicting n-grams for sentiment classification, should be properly cited to contextualize the contributions of this study.