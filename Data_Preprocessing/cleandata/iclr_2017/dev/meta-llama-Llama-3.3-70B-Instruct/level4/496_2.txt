This paper presents a novel RNN architecture, featuring multiple layers where higher layers receive lower layer states only when a FLUSH operation is predicted, which involves passing the state upwards and resetting the lower layer's state. To select from three possible operations at each time step, the authors employ the straight-through estimator with a slope-annealing technique during training. The empirical results and visualizations demonstrate the effectiveness of this modified architecture in boundary detection.
The strengths of this paper include:
- A well-motivated and exceptionally well-written presentation
- Promising initial results on learning hierarchical representations, supported by thorough experiments and visualizations on language modeling and handwriting generation
- The introduction of the annealing trick with the straight-through estimator, which may have broader applications in tasks involving discrete variables, and the innovative trade-off in the flush operation
However, there are some limitations:
- The paper falls short in certain aspects, notably the lack of empirical results on computational savings and the inability to demonstrate hierarchy beyond a single level, particularly in cases where the data contains separators like spaces and pen up/down events
- It remains unclear whether the improved downstream performance is attributed to the utilization of hierarchical information or the architectural changes acting as a regularizer, an issue that warrants further investigation.