This paper investigates a crucial aspect of adversarial examples, focusing on the identification of adversarial images and leveraging them to enhance the robustness of neural networks. By introducing a detector subnetwork specifically designed to target adversaries, the study elevates the adversarial model competition to a new dimension. The authors provide compelling evidence that training a detector subnetwork to identify adversarial examples can effectively robustify networks, rather than solely relying on making the networks themselves more resilient to such attacks. The paper evaluates the jointly trained primary and detector system under various conditions, including scenarios where the adversary has access to the model and those where generic attacks are employed. The results demonstrate significant improvements with this approach, supported by thorough and well-motivated analyses that reinforce the central argument. Overall, the writing is clear, concise, and effectively communicates the research findings.