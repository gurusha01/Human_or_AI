This paper proposes a Leaky-ReLU model for generative learning, building upon prior research on stepped sigmoid units and ReLU hidden units in discriminatively trained supervised models.
A notable aspect of this work is its approach to deriving the energy function from the conditional distributions, rather than the traditional method of first defining the energy function. Although this formulation is not entirely new, having been previously generalized to exponential family GLMs, it is still an interesting perspective.
However, the focus on specifying conditionals leads to complexities in computing the joint pdf and marginal p(v), making these calculations challenging.
In terms of experiments, a comparison with a Restricted Boltzmann Machine (RBM) featuring binary visibles and Leaky ReLU hidden units would be beneficial in demonstrating the superiority of Leaky ReLU hidden units. Additionally, the authors could compare their results to existing research on binary MNIST modeling. While the authors correctly note that the annealing distribution is non-Gaussian, running Contrastive Divergence-25 (CD-25) or Fast Persistent Contrastive Divergence (Fast PCD) experiments would provide a useful baseline comparison to a standard RBM trained using Fast PCD.
The paper's combination of a new hidden function with the ease of annealed Importance Sampling (AIS) is intriguing. Nevertheless, the lack of baseline comparisons to other models, such as Stepped Sigmoid Units (Nair & Hinton) or spike-and-slab RBMs, makes it difficult to assess whether Leaky ReLU RBMs offer superior performance, even in the continuous visible domain.