The authors propose a character language model that achieves a balance between interpretability and predictivity. 
CONTRIBUTION:
This paper can be viewed as an exploratory study of an intriguing idea. It builds upon the concept that multi-class logistic regression enables the attribution of prediction credit to input features, where certain features increase the probability of the correct class while others decrease it. The authors extend this idea by demonstrating that a sufficiently simple RNN model architecture exhibits log-linearity, allowing for the attribution of prediction credit to elements of the past history.
PROS:
The paper is well-structured and engaging to read. The use of a simple architecture that still yields respectable results is noteworthy. 
This model has the potential to be a valuable educational tool, as it is easy to implement and replicate, making it an ideal candidate for a classroom assignment. 
The authors provide informative visualizations that facilitate understanding.
Additionally, Section 5.2 highlights some computational advantages of this approach.
CAVEATS ON PREDICTIVE ACCURACY:
* Although Figure 1 suggests that the proposed model, ISAN, has performance comparable to other architectures, this seems to hold true only for the largest models. 
Explanation: Upon closer examination, it appears that for smaller parameter sizes, a GRU outperforms the authors' model by a significant margin of 22% to 39% in terms of perplexity per word (ppw), a common metric for evaluating language models. 
* Furthermore, it remains uncertain whether this model will maintain its competitiveness in more complex scenarios beyond the toy situations presented in this study.
Explanation: The authors have only tested their model on character-based language modeling with a relatively small 10M-char dataset, resulting in a high perplexity per word (ppw) of 2135 for the best models. In contrast, word-based RNN language models trained on larger datasets, such as 44M words or 800M words, achieve significantly lower ppw values of 133 and 51, respectively. [These numbers are based on the previously cited paper.]