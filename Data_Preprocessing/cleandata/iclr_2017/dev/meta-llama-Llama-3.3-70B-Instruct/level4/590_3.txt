SUMMARY.
This paper presents a reading-comprehension question answering system designed for a recent QA task, where answers can be either single tokens or spans within a given text passage. The proposed model employs a recurrent neural network to encode both the passage and the query. An attention mechanism is utilized to determine the importance of each word in the passage relative to each word in the question. The encoded passage words are then concatenated with the attention weights, and the resulting vector is further encoded using another RNN. To capture local features, three convolutional neural networks with varying filter sizes (1, 2, and 3-grams) are applied. Candidate answers are identified by either matching part-of-speech patterns from the training set or selecting all possible text spans up to a certain length. Each candidate answer has three representations, corresponding to each n-gram representation, and their compatibility with the question representation is calculated. The scores are combined linearly to determine the probability of each candidate answer being correct. The method is evaluated on the SQUAD dataset, where it outperforms the proposed baselines.
OVERALL JUDGMENT
While the method presented in this paper is intriguing, it lacks motivation in certain aspects. For instance, the benefits of concatenating the original passage encoding with the attention-weighted encoding in the attention mechanism are not clearly explained. The paper's contributions are moderately novel, primarily introducing the attention mechanism and convolutional re-encoding. However, combining questions and passages to score their compatibility has become a standard approach in most QA models.
DETAILED COMMENTS
In Equation (13), "i" should be replaced with "s", not "s^l". 
The explanation of "the best function is to concatenate the hidden state of the first word in a chunk in the forward RNN and that of the last word in the backward RNN" remains unclear. Specifically, it is uncertain whether the RNN is applied to all words in the chunk or the entire passage. Unfortunately, the authors' response does not provide sufficient clarification on this point.