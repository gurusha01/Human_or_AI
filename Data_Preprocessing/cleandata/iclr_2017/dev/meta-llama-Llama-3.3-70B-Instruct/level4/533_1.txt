The authors introduce a innovative method for surprise-based intrinsic motivation in the context of deep reinforcement learning, providing a clear distinction from other recent approaches to intrinsic motivation. They substantiate their methodology with empirical results from a diverse range of domains, encompassing both discrete and continuous action spaces. Two viable approximations of their framework are presented: one that neglects the stochastic nature of environmental dynamics and another that estimates the rate of information acquisition, bearing some resemblance to Schmidhuber's theoretical framework on creativity, fun, and intrinsic motivation. The incorporation of an exploration bonus into TRPO yields superior performance compared to standard TRPO. Nevertheless, a more comprehensive evaluation against other contemporary studies on intrinsic motivation would be beneficial. For example, the work by Bellemare et al. in 2016 demonstrated substantial performance improvements on complex Atari games, such as Montezuma's Revenge, by integrating DQN with an exploration bonus; however, this particular game is not included in the experimental results. The inclusion of such comparisons would significantly enhance the paper's validity and impact.