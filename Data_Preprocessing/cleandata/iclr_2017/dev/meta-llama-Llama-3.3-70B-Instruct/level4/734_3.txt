Summary:
This paper explores the application of variational autoencoders for multi-view representation learning, presenting alternatives to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). The authors investigate two variants of variational autoencoders, referred to as VCCA and VCCA-private, and evaluate their performance on synthetic MNIST, XRMB speech-articulation, and MIR-Flickr datasets.
Review:
The utilization of variational autoencoders for multi-view representation learning is a topic of significant interest to the ICLR community, given their widespread adoption. The paper is well-structured and clearly written, with thorough experiments that provide valuable insights. Notably, the comparison between MVAE and VCCA reveals distinct performance differences, despite similarities in their objective functions. The analysis of dropout and private variables' effects is also a noteworthy contribution.
As highlighted by the authors, VCCA differs fundamentally from linear or nonlinear CCA in terms of optimization criteria and solution outcomes. A more in-depth discussion on the distinctions between a linear variant of VCCA and linear CCA, accompanied by quantitative comparisons, would have been beneficial in enhancing understanding of these differences. Although variational inference may not be applicable in the linear case, such an analysis would still provide valuable context.
The detailed derivations in Equations 3 and 13 appear unnecessary, given that VCCA and VCCA-p are specialized instances of VAE with specific architectural constraints. Consider relocating these derivations to the Appendix to improve the paper's overall flow.
In Section 3, the claim that generating realistic samples from the learned distribution implies discovery of the underlying data structure is inaccurate. As demonstrated by Theis et al. (2016), a model can produce realistic samples without capturing the underlying structure. This sentence should be revised or removed.
Minor suggestion: The equation between Equations 8 and 9 would benefit from using the notation N(x; g(z, theta), I), consistent with Equation 6, to enhance clarity.