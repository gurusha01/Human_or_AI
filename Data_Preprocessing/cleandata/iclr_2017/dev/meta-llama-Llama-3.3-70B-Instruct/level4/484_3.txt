This manuscript explores the suitability of deep versus shallow networks for various functions, presenting a compelling and straightforward intuition. The pooling operations effectively consolidate information, and when the geometry of pooling regions aligns with correlated data, it enables more efficient utilization of this information. In contrast, shallow networks lacking localized pooling layers are unable to combine correlated information efficiently.
The theoretical framework focuses on convolutional arithmetic circuits, building upon the authors' prior work. The concept of separability is introduced, which measures the extent to which a function can be represented as a composition of independent functions. This notion is particularly relevant for assessing the complexity of functions in relation to a specific pooling geometry. While many technical concepts are intuitive, the tensor analysis is concise and may be challenging to follow without familiarity with the authors' previous research.
The comparison between deep and shallow networks may be somewhat misleading, as shallow networks typically lack a hierarchical pooling structure. For instance, a shallow convolutional network with RELU and max pooling is not practical, as the max operation would be applied to the entire image. Therefore, the paper can be seen as an analysis of the impact of pooling versus the absence of pooling. It is unclear whether a deep CNN without pooling would offer any advantages over a shallow network based on this work.
The dependence of the theoretical results on the use of product pooling, and their potential extension to max pooling, is not explicitly stated. Providing simple illustrative examples, even if theoretical results are difficult to derive, would be beneficial. In a potential journal version of the paper, adding a toy example demonstrating the efficient representation of a function using a convolutional arithmetic circuit with a suitable pooling structure, and comparing it to a convolutional network with RELU and max pooling, could enhance the intuitiveness of the results.
A more explicit discussion on how the depth of a deep network affects the separability of representable functions would be appreciated. The difference between deep and shallow networks may be largely attributed to the presence or absence of pooling. However, practitioners have observed that very deep networks tend to outperform "deep" networks with only a few convolutional layers and pooling. The paper does not explicitly address whether the results provide insight into this phenomenon.
Overall, the manuscript tackles an important problem in an interesting manner. While it is not entirely convincing that the work fully captures the essence of why depth is crucial, due to the theoretical limitations to arithmetic circuits and the comparison to shallow networks without localized pooling, the paper presents a valuable contribution to the field.