This manuscript explores the capability of reinforcement learning (RL) agents to conduct 'physics experiments' within an environment to deduce physical properties of objects. The problem statement is well-motivated, as inferring physical properties is essential for intelligent agents, and there is a notable lack of research in this area, particularly within deep RL. The paper is also well-written and clear in its presentation.
The novelty of the paper primarily lies in its application of a recurrent A3C model to two tasks. These tasks involve an agent interacting with its environment to infer physical properties: determining the mass of blocks by moving them and assessing the number of rigid bodies in towers by poking them until they fall. Although these tasks represent a limited subset of the abilities required for an agent to understand physics, this focus is not inherently problematic. However, the absence of comparisons with simpler RL agents on these tasks makes it challenging to assess their difficulty level. As previously noted, the 'Which is Heavier' task appears relatively straightforward due to the actuator setup and the direct encoding of block positions as features, which may explain why the RL agent can successfully solve the proposed tasks without surprise.
Beyond solving the tasks, the paper claims that the agents develop strategies that balance information gathering costs with mistake costs. This behavior, achieved by introducing a reward multiplier with a gamma value less than 1, is somewhat interesting but not unexpected given the problem's constraints.
The authors differentiate their approach, which learns physical object properties through interaction, from previous methods that rely on visual cues. While they acknowledge this approach is not novel and has been explored (e.g., by Agrawal et al. in 2016), a more detailed discussion of these approaches and a clearer justification for the interest and uniqueness of the proposed tasks compared to other works (like learning to move objects to specific positions by poking) would be beneficial.
To evaluate the paper's contribution, two key questions must be considered: 
1. What is the significance of these two tasks in advancing the goal of agents learning object properties through interaction beyond existing work?
2. How do the results of the RL agent on these tasks enhance our understanding of agents learning physical properties by interacting with their environment?
Given the concerns raised, it's challenging to conclude that the answers to these questions indicate a significant contribution. Specifically, since the proposed agent can essentially solve both tasks, it's unclear whether these tasks can serve as benchmarks for more advanced agents.
Additionally, as Reviewer 3 pointed out, the model description is very concise and could be improved with illustrative diagrams showing the inputs and outputs at each time step to facilitate replication.
In conclusion, while progress toward agents that can discover physical properties of their environment is crucial, and this paper contributes to this direction, its technical contributions are somewhat limited. It would be beneficial to include discussions on the future of agents learning physics from interaction, such as speculating on more challenging versions of the tasks presented and how the proposed approach fits into the broader research landscape.