The authors introduce NVI for LDA variants and conduct comparisons with established inference schemes, including CGS and online SVI. Additionally, they assess NVI's performance on the ProdLDA model, although its novelty in the topic modeling literature is unclear.
Overall, the paper's direction is commendable, and NVI shows promise for LDA. However, the experimental results conflate model and inference performance, making it challenging to interpret the significance of the findings. Moreover, the authors omit discussions on hyper-parameter selection, which is known to substantially impact topic models' performance, making it difficult to determine when the proposed method is expected to be effective.
To strengthen the results, generating synthetic datasets with varying Dirichlet distributions and evaluating the proposed method's ability to recover true parameters could be beneficial.
Figure 1 is ambiguous, as it refers to sparsity while the y-axis label "log p(topic proportions)" suggests a different concept. 
In Section 3.2, the term "unimodal" in the context of softmax basis is unclear. Considering a Dirichlet distribution on the K-dimensional simplex with a concentration parameter alpha/K, where alpha < 1 yields a multimodal distribution, it is uncertain whether the softmax basis remains multimodal.
The results lack error bars, raising questions about their statistical significance.
Minor suggestions include revising the last term in equation (3) to "reconstruction accuracy" or "negative reconstruction error" for clarity. Furthermore, the concept of using an inference network has been explored previously, as seen in the Helmholtz machine.