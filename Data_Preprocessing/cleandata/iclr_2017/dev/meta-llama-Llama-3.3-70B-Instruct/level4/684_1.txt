This paper presents a novel model-based reinforcement learning approach that predicts future rewards based on the current state and future actions, utilizing a "residual recurrent neural network" to output the expected reward increase at various future time steps. The approach is demonstrated through experiments on Atari games, where a simple playing strategy is employed, involving the evaluation of random move sequences and selection of the sequence with the highest expected reward and low death probability. Notably, one of the three tested games exhibits improved performance when the agent is trained in a multitask setting, suggesting the occurrence of transfer learning.
The submission is generally readable, and the proposed reward prediction architecture appears to be an original and sound concept. However, several issues prevent this work from meeting the ICLR standards, as outlined below.
Firstly, there is a discrepancy between the algorithm described in Section 3 and its actual implementation in Section 4. The output is initially stated to be the expected accumulated reward in future time steps as a single scalar, but in the experiments, it is implemented as two separate values: the probability of dying and the probability of achieving a higher score without dying. Although this modification may yield better results, it deviates from the original idea presented in the paper and raises questions about the effectiveness of the initial proposal.
Furthermore, the experimental results are limited, focusing on only three hand-picked games and lacking comparisons to other reinforcement learning techniques, such as DQN. While the primary focus of the paper is not to showcase state-of-the-art results, given the simple heuristic policy used, the experiments should have explored the potential of the model in enhancing reinforcement learning algorithms. Currently, the model is essentially a supervised algorithm used within a manually defined, hardcoded policy, rather than being integrated into a reinforcement learning framework. Additional experiments could have investigated the accuracy of the predictions, such as classification error for death probability or mean squared error for future rewards, in comparison to simpler baselines.
Additionally, the "previous work" section is overly concise, primarily discussing DQN and neglecting to provide a comprehensive overview of model-based reinforcement learning. A notable omission is the lack of citation for relevant papers, such as "Action-Conditional Video Prediction using Deep Networks in Atari Games", which should have been included.
Minor comments include:
- Unconventional notation, where "a" denotes a state instead of an action, potentially causing confusion and deviating from standard reinforcement learning notation.
- The use of a dot for tensor concatenation is not ideal, as it typically represents a dot product.
- The residual term \(ri\) in Section 3.2.2 is distinct from the reward \(ri\).
- The definition of \(c_i\) as "The control that was performed at time i" seems to actually refer to the control performed at time \(i-1\).
- There is a recurring confusion between mean and median in Section 3.2.2.
- The variable \(x\) should not be used in Observation 1, as the \(x\) from Figure 3 does not undergo layer normalization.
- The inequality in Observation 1 should pertain to \(|xi|\), not \(xi\).
- Observation 1, along with its proof, occupies excessive space for a relatively simple result.
- In Section 3.2.3, the first \(rj\) should be \(ri\).
- The introduction of the death probability in Section 3.3 seems abrupt, as it is not previously established as an output of the model.
- The statement "Our approach is not able to learn from good strategies" may be misleading; it might be more accurate to say "only from good strategies".
- It would be beneficial to clarify that "fc" in Figure 4 stands for "fully connected" and to highlight how the architecture differs from the classical DQN architecture by Mnih et al. (2015).
- Further clarification on \(r_j2\) is needed, as per the response in OpenReview comments.
- Table 3's reference to "After one iteration" is confusing, given the presence of "PRL Iteration 2".
- The claim in Figure 5 that there is no degradation in Pong and Demon Attack appears to be somewhat inaccurate.
- The statement "A model that has learned only from random play is able to play at least 7 times better" lacks clear justification for the factor of 7.
- The mention of a potential problem in Demon Attack's plot in Figure 5c seems unrelated to any previous discussion.