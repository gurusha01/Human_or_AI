This manuscript presents a collection of techniques for compressing rapid linear text classification models, yielding a well-structured and readable paper with notable results. The primary compression method employed is product quantization, a technique previously investigated in the context of neural network model compression. While the authors cite the work of Gong et al., it would be beneficial to also reference Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016), as it provides additional relevant context within the field.