The authors demonstrate the effectiveness of a contrastive loss function in a Siamese architecture for learning representations of planar curves, achieving results comparable to traditional differential or integral invariants, albeit on a limited set of toy examples. The paper is well-structured and presents an intriguing application of the Siamese architecture. However, the experimental evaluation appears preliminary, with several key choices lacking validation. A major concern lies in the selection of negative samples, where the network primarily learns to differentiate between shapes at varying scales rather than recognizing distinct shapes. It is a well-established fact that careful hard negative sampling is crucial for achieving optimal performance with contrastive loss, as relying on overly simplistic negatives can yield subpar results. Unfortunately, this aspect is not addressed in the paper, potentially underlying the choice of negatives.
Moreover, the paper falls short in providing a comprehensive quantitative evaluation, instead focusing on showcasing specific examples rather than assessing more robust statistical measures across multiple curves, such as invariance to noise and sampling artifacts. In summary, while the paper presents interesting initial steps in this direction, it is uncertain whether the experimental section is sufficiently rigorous and thorough for the standards of the ICLR conference. Additionally, the novelty of the proposed concept is somewhat limited, given the longstanding use of Siamese networks and the fact that this work primarily demonstrates their applicability to a new task.