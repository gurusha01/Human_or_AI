This manuscript presents a novel approach to inferring programs from input/output example pairs, focusing on a domain-specific language that encompasses a broad range of string transformations. The proposed method involves modeling successive extensions of a program tree, conditioned on the embedding of input/output pairs. The extension probabilities are calculated based on the embeddings of leaf nodes and production rules, leveraging the introduced Recursive-Reverse-Recursive Neural Net (R3NN) to compute globally aware leaf embeddings through a differentiable, end-to-end trainable process.
The paper has several notable strengths. Unlike some related work in the deep learning community, this approach has the potential for practical application in the near future. The R3NN concept is well-motivated and explored, with the authors investigating various model variants to identify what works and what doesn't. The clear exposition, despite the paper's length, makes it an enjoyable read. However, there are some weaknesses, including the model's limited accuracy, potentially due to training on small programs and being applied to longer ones. It is unclear why the authors didn't train on longer programs. Additionally, the number of input/output pairs seems fixed, which may limit the model's ability to utilize additional pairs.
Overall, this paper is worthy of acceptance at ICLR. Some minor suggestions include simplifying the expansion probability expression, providing more details on the bidirectional LSTM used for global leaf representations, and discussing the importance of hyperbolic tangent activation functions. Clarification on batching, optimization algorithms, learning rate schedules, and weight initialization would improve reproducibility. Adding program sizes for unsolved benchmarks in Figure 6 would provide insight into the approach's limitations. Finally, the related work section could be enhanced by including the study by Piech et al. on learning program embeddings.