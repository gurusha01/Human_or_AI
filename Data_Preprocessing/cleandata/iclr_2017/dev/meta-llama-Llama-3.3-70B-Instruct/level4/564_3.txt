This paper presents a concept of incorporating n-step backward analysis when modeling sequences using Recurrent Neural Networks (RNNs). The proposed RNN architecture not only utilizes the preceding hidden state (t-1) but also considers additional previous states (t-k, where k=1,2,3,4). Furthermore, the paper introduces several methods for aggregating multiple past hidden states.
However, several concerns are raised regarding this paper. 
Firstly, the writing quality needs significant improvement. The introduction and abstract devote excessive space to explaining unrelated concepts or reiterating well-established literature, and some statements are misleading. For example, the paper initially states that RNNs are suitable for modeling sequential data due to their ability to capture long-term dependencies through recurrent feedback, but later contradicts this by asserting that RNNs struggle with capturing long-term dependencies. In reality, RNNs are appealing because they can handle variable-length sequences and model temporal relationships between symbols. The criticism of LSTMs being slow and difficult to scale is also questionable, as companies have successfully utilized large-scale seq2seq models (which often employ LSTMs) in production environments.
Secondly, the proposed idea is incremental and not particularly novel, as previous works have suggested using direct connections to previous hidden states [1]. Although the paper's approach of aggregating multiple previous hidden states is distinct, it fails to provide a thorough analysis of the effectiveness of its primary contribution. The new architecture is claimed to better handle long-term dependencies, but lacks rigorous proof or intuitive design to support this assertion. At a high level, the architecture may potentially mitigate the vanishing gradients issue by a linear factor. A more in-depth analysis of the empirical findings, ideally spanning at least one page, would be beneficial.
Thirdly, the baseline models used in the paper are weak. Numerous other models have been trained and tested on word-level language modeling tasks using the Penn Treebank corpus, but the paper only includes a few outdated models. The claim of achieving the best performance on PTB under the same training conditions is questionable, as contemporary RNN-based methods typically achieve test perplexity scores below 80, significantly lower than the 100 reported in this paper.
[1] Zhang et al., "Architectural Complexity Measures of Recurrent Neural Networks", NIPS'16