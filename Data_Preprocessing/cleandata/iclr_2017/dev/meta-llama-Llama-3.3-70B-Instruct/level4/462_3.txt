I reviewed the manuscript on December 5th.
Summary:
The authors explore the concept of adversarial perturbations, inquiring about the feasibility of developing a standalone system to identify adversarial data points, which could potentially prevent automated processing by a machine. Notably, they investigate the possibility of creating an adversarial detector that is resilient to adversarial examples designed to deceive both the classifier and the detector itself. Their findings indicate that training a detector in this more challenging setting yields improvements, but does not entirely resolve the issue of detecting adversarial examples.
Major comments:
The authors propose a novel approach to addressing adversarial examples from a security perspective, involving the development of an independent system to detect adversaries, enabling human intervention in such cases. 
A potential limitation of this approach is that an adversary could construct adversarial examples to deceive both the original classifier and the new detector, rendering this approach ineffective. However, the authors demonstrate that building a 'dynamic' detector, which can detect adversarial examples while being resilient to adversarial attacks, mitigates this potential escalation, resulting in improved detection rates (from 55% to 70%). Although the 'dynamic' detector shows positive gains, I am concerned about the overall detection rates, as detecting adversarial examples at this rate may not provide a reliable security procedure.
My second comment pertains to 'model transferability', which I define as the construction of an adversarial example on one network and the measurement of its effectiveness in attacking a second, differently trained model. The authors' definition of 'transferability' differs, focusing on the detector's generalizability across training methods. 'Model transferability' (as I define it) is crucial, as it assesses the generality of an adversarial example across all models, rather than being specific to a given trained model. Different methods exhibit varying levels of 'model transferability' (Kurakin et al, 2016), and I am concerned about the detector's ability to detect adversarial examples across all models, not just the specific trained model in question. A robust detector should be able to detect adversarial examples from any network, not just a particular trained network. This question appears to be largely unaddressed in the paper, although I may have overlooked a subtle point in the authors' descriptions.
Minor comments:
If there are any data points in the bottom-left region of Figure 2, it would be essential to visualize them, potentially by repositioning the legend to highlight the presence or absence of points in this area.
- The x-axis label in Figure 2 (right) is incorrect.
It would be beneficial to measure the transferability of the detector.
- The labeling of \sigma in Figure 5 is unclear.
- Whenever an image is constructed as an 'adversary', has it been verified that the image is indeed adversarial, i.e., does it result in misclassification by the original network?