Overview:
This study introduces a novel approach to connecting trajectory log-probabilities and rewards by defining under-appreciated rewards, implying a linear relationship between trajectory rewards and their log-probability that can be leveraged by measuring the resulting mismatch. Specifically, when an action sequence under-appreciates its reward, its log-probability increases. This method represents a straightforward modification to the well-established REINFORCE method, requiring only an additional hyperparameter τ, and provides an intuitive improvement over ε-greedy or random exploration mechanisms. The approach is evaluated on algorithmic environments and compared to entropy-regularized REINFORCE and double Q-learning, demonstrating comparable or superior performance, particularly in more complex environments.
Remarks:
- The introduction's focus on algorithmic tasks may have both positive and negative consequences, as it offers an interesting domain for testing the hypothesis and benchmarking the method, but also diverts the reader's attention from the proposed method's generality.
- There appears to be an inconsistency in the description of the reward structure, initially stated as sparse, but later clarified as providing a reward at each correct emission, which is only corrected to an end-of-episode reward in a subsequent section.
- The approach seems sensible to τ being in the same range as logπ(a|h), but the experimentation choice of only trying τ=0.1 for UREX is unclear and may not fully explore the hyperparameter space. An alternative to grid search, such as random search, could provide a more comprehensive understanding of hyperparameter robustness.
- The experimentation could be improved by exploring a wider range of hyperparameters and using alternative methods, such as random search, to demonstrate the robustness of the approach.
Opinion:
- The proposed approach to policy-gradient methods is intriguing, as it addresses the crucial question of how agents should explore their environment. However, the claim of robustness to hyperparameters may be overstated, as the results only demonstrate performance within a specific hyperparameter range.
- The assumption that the reward lies in the same space as the log policy, implied by matching 1/τ with logπ, may be overly strong and could lead to an imbalance in exploration, where shorter trajectories are less explored than longer ones, unless the reward is carefully shaped.
- Additional comparisons with methods that explicitly aim to improve exploration using value functions, such as prioritized experience replay, could provide a more comprehensive understanding of the approach's strengths and limitations.
- The impact of τ on the experiments is not thoroughly analyzed, which is a critical aspect of the method, and further investigation is necessary to fully understand its effects.
The methodology and reasoning are clearly explained, and the paper effectively communicates its message, which is novel and well-motivated, despite being a minor modification to a well-known algorithm. The experiments are well-chosen, and the results support the authors' hypothesis. However, more extensive or clever experimentation, as well as a more explicit demonstration of the method's impact on exploration, would strengthen the paper's empirical argument and provide a more convincing case for the approach's effectiveness.