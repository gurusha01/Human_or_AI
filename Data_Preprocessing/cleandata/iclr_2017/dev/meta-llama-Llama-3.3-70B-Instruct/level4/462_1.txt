This manuscript presents a novel approach to mitigating the impact of adversarial examples by employing a complementary classifier to identify them. The experimental results demonstrate that adversarial examples can indeed be readily detected, and the proposed detector exhibits robust generalization capabilities to similar or weaker adversarial examples. Although the concept is straightforward, its implementation is non-trivial. While the paper does not provide a comprehensive framework for integrating this idea into defensive systems, it offers a promising new direction for future research. Based on its innovative contribution, I recommend acceptance.
My primary concern regarding this paper lies in its comprehensiveness, as it fails to report an effective method for defending against dynamic adversaries. It is acknowledged that addressing this challenge may be difficult; however, the paper appears to have devoted insufficient effort to exploring this aspect. Investigating the feasibility of defending against dynamic adversaries is a crucial and intriguing question that arises from the paper's conclusions, and such an inquiry could substantially enhance our understanding of adversarial examples. Despite this limitation, the paper's novelty remains substantial.
Minor comment:
The manuscript would benefit from improved clarity, as certain important details are omitted. For instance, the paper should provide a more detailed explanation of dynamic adversaries and the methodology employed for dynamic adversary training.