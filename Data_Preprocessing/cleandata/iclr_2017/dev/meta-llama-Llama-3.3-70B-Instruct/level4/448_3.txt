This manuscript provides an in-depth mathematical examination of information propagation within deep feed-forward neural networks, offering innovative insights into mitigating the issues of vanishing and exploding gradients during the backward pass of backpropagation, as well as the application of the dropout technique. The presentation is lucid and well-organized, the analytical approach is comprehensive, and the empirical findings, which validate the proposed model, are particularly noteworthy.