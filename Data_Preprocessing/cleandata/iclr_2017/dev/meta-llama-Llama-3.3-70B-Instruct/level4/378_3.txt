This paper presents a novel algorithm, building upon the REINFORCE framework, designed to investigate under-explored action sequences. The core concept involves comparing the probability of action sequences under the current policy to their estimated rewards, with actions that are undervalued by the current policy receiving higher feedback, thereby promoting the exploration of specific action sequences. The proposed UREX model is evaluated on six algorithmic reinforcement learning problems, demonstrating intriguing characteristics when compared to both the standard regularized REINFORCE (MENT) model and Q-Learning.
The model itself is compelling, rigorously defined, and clearly explained. To the best of my knowledge, the UREX model constitutes an original contribution that is likely to be of significant value to the reinforcement learning community. However, a limitation of the paper is its restriction of the algorithm's evaluation to specific algorithmic problems, whereas extending the evaluation to include other standard reinforcement learning tasks would be straightforward and beneficial. Incorporating additional tasks would substantially strengthen the manuscript, and I strongly recommend that the authors consider expanding their evaluation to enhance the paper's impact.