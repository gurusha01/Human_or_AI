This manuscript presents intriguing empirical results demonstrating the capability of state-of-the-art deep reinforcement learning techniques to enable agents to learn latent physical properties within their environment. The authors formulate the problem of an agent assigning labels to environmental properties following interaction with the environment, based on its actions, and apply a deep reinforcement learning model to assess the feasibility of such learning. The approach employs a joint learning mechanism for convolutional layers, facilitating pixel-based perception, and subsequent layers, which learn actions based on reinforcement signals.
Our assessment of this paper is ambivalent. On one hand, the manuscript is well-written, presents interesting experimental findings, and introduces a potentially significant problem with broad implications for robotics applications. On the other hand, the paper is limited by the lack of novel algorithmic contributions and the absence of crucial experiments necessary to validate its purported benefits.
The strengths of this paper include:
+ The introduction of a novel problem related to learning latent properties in the agent's environment, which contributes to the field by exploring new avenues for research.
+ The presentation of a framework that effectively combines existing tools to address the formulated problem, demonstrating a clear understanding of the underlying methodologies.
+ The application of reinforcement learning to image inputs and fist-like actuator actions, which has direct implications for robotic applications and highlights the potential for real-world implementation.
However, the paper also has several weaknesses:
- The lack of algorithmic innovation is a significant concern, as the manuscript relies on existing methods rather than developing new ones or extending current approaches. The approach essentially involves training LSTMs with convolutional layers using the previously established Asynchronous Advantage Actor-Critic method, without introducing substantial modifications or improvements.
- A critical omission is the absence of results for the "Fist Pixels" setting in the Towers experiment, which is the most relevant configuration for real-world robotic applications. This setting, involving pixel inputs and the use of the Fist actuator in a continuous space, is essential for confirming the applicability of the proposed approach to real-world scenarios. The absence of these results in Figure 5 is notable, and it is unclear whether there is a specific reason for this omission.
- The paper lacks a comparison to baseline methods, making it challenging to evaluate the true benefits of the proposed approach. Without explicit baselines, it is difficult to determine what aspects of the approach are contributing to the task's performance. For instance, in the Towers task, a comparison to an agent using a fixed action policy (e.g., randomly pushing or hitting the tower and then passively observing the consequences) would provide valuable insights into the effectiveness of the full deep reinforcement learning version. Such a comparison would help to clarify the advantages of the proposed approach and identify areas for further improvement.