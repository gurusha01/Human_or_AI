Summary: The authors propose a straightforward RNN architecture with linear dynamics for language modeling, which enhances model interpretability and offers potential performance improvements through caching common sub-sequence dynamics. Although the quantitative benchmark comparison is underwhelming, with unclear motivations for the chosen dataset and a limited single-dataset evaluation, the authors demonstrate well-executed techniques for analyzing model behavior, leveraging the linear dynamics to facilitate insights unattainable with non-linear RNNs.
Overall, I recommend accepting the paper due to its engaging narrative and significant contribution to the research discourse, despite the underwhelming results.
Feedback:
To enhance the paper, consider condensing the analysis experiments and expanding the discussion on related sequence models. Certain experiments, such as 4.6, appear to merely demonstrate the model's ability to fit the data, rather than revealing important properties. Given the reasonable perplexity results, it is assumed that the model fits the data well.
A comparison with non-linear methods, like LSTMs and GRUs, on data with rigid combinatorial structures (e.g., nested parentheses) would be valuable. Even negative results could provide interesting insights, highlighting potential advantages of non-linear methods on such tasks.
The related work section should include a discussion of Belanger and Kakade (2015), who also employ linear dynamics but focus on fast, scalable learning algorithms. Their analysis of the transition matrix's singular vectors could be compared to the authors' approach.
A more direct discussion of Linear Dynamical Systems (LDS) would benefit readers, addressing comparisons that arose in the open review discussion. For instance, emphasizing the relationship between bias vectors and the Kalman gain matrix columns could provide clarity. Additionally, exploring the possibility of incorporating Kalman smoothing, which infers state vectors using both past and future observations, could be an interesting extension.
Consider introducing parameter sharing by representing each matrix as a sparse/convex combination of dictionary matrices. This could enhance interpretability by representing characters as low-dimensional weights combining dictionary elements, potentially improving scalability for word-level problems.