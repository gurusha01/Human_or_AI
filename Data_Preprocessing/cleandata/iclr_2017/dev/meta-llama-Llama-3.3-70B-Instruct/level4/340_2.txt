I appreciate the authors' efforts in conducting additional experiments and providing further explanations in the manuscript, which has led me to revise my evaluation accordingly.
This study focuses on learning a generative function G that maps inputs from a source domain to a target domain, ensuring that a given representation function f remains unchanged regardless of the input domain, a concept referred to as f-constancy. The proposed approach is assessed through two visual domain adaptation tasks.
The manuscript is generally well-structured and easy to comprehend, with the authors presenting extensive experimental results on two datasets. The introduction of f-constancy is a significant novelty of this work.
However, it appears counterintuitive to constrain the function G to be a composition of g and f, particularly since f may have already lost crucial information due to its restricted nature. For instance, in the face dataset, f is optimized for a specific task on an external dataset, raising questions about the recoverability of inputs from either domain after applying G, as shown in equations (5) and (6). Moreover, the function f is learned with a particular task in mind, such as identifying digits in the SVHN dataset or faces in another dataset. Consequently, if the task were to change, such as recognizing expressions instead of identities, the entire procedure would need to be repeated.
I would like the authors to provide insight into why the baseline method outlined in equations (1) and (2) yields poor performance.
Figure 5 presents a visual comparison between style transfer and the proposed method, although it is unclear which approach is superior. I wonder if it would be feasible to apply style transfer to generate emojis from photos and replicate the experiments presented in table 4 to further compare the methods.