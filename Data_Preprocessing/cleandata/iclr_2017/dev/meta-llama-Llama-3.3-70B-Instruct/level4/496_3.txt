This paper presents a novel variant of recurrent networks capable of learning hierarchical information in sequential data, such as character-to-word relationships, without requiring boundary information to segment sequences into meaningful groups, differing from approaches like Chung et al., 2016.
The proposed model is structured as a series of layers designed to capture information at various levels of abstraction. The lowest level triggers the upper one and determines when to update it based on a controller or state cell (c), a key feature being that c is a discrete variable, potentially allowing for fast inference times. However, this discreteness makes the model more challenging to learn, necessitating the use of the straight-through estimator as introduced by Hinton in 2012.
The experimental section is comprehensive, with the model achieving competitive performance on several challenging tasks. Qualitative results also demonstrate the model's ability to capture natural boundaries, showcasing its potential.
Overall, this paper introduces a strong and novel model with promising experimental results, contributing significantly to the field.
On a minor note, several suggestions for improvement in writing and related work are noted:
- In the introduction, claims about learning principles in the human brain should be supported with evidence. The reference to Mikolov et al., 2010, seems to be missing in the context of recent advances in recurrent neural networks for modeling temporal data. Additionally, a reference to Lin et al., 1996, would strengthen the argument about the natural existence of hierarchical multiscale structures in temporal data.
- In the related work section, the description of the Clockwork RNN (CW-RNN) by Koutn√≠k et al., 2014, should accurately reflect its extension of the NARX model by Lin et al., 1996, rather than the hierarchical RNN by El Hihi & Bengio, 1995. The discussion on online prediction problems could be enhanced with references to Socher's work and older recursive networks. The mention of gradient clipping should acknowledge earlier works, such as Mikolov et al., 2010, rather than solely attributing it to Pascanu et al., 2012.
- Several key references are missing, including "Recurrent neural network based language model" by Mikolov et al., 2010, "Learning long-term dependencies in NARX recurrent neural networks" by Lin et al., 1996, "Sequence labelling in structured domains with hierarchical recurrent neural networks" by Fernandez et al., 2007, and "Learning sequential tasks by incrementally adding higher orders" by Ring, 1993.