This manuscript reports a noteworthy finding that sparse operations can be employed during the training of Long Short-Term Memory (LSTM) networks without compromising accuracy. Although this discovery may not be entirely unexpected, it appears to be a novel contribution, at least to my limited knowledge in the area of efficient Recurrent Neural Network (RNN) implementations.
A minor clarification is warranted: the LSTM language model utilizes a linear embedding layer, rather than a 'word2vec' layer, as the term 'word2vec' specifically refers to a distinct model that is not directly applicable to character-level language models.
The central observation of the paper is clearly presented, but the argument would be more persuasive if it were supported by experiments using well-established datasets and setups, such as those described by Graves (2013) or Sutskever et al. (2014), including comprehensive reports of training, validation, and test performance metrics.
While the primary observation is undoubtedly intriguing, it may not be sufficient to warrant a full conference paper in its current form, lacking as it does concrete implementation, simulation, and benchmarking of the proposed speed enhancements across multiple tasks. For instance, it would be beneficial to investigate how different architectural choices might influence the observed gains.
In its present form, this work constitutes an interesting technical report, and I look forward to seeing more detailed and comprehensive results in future submissions.