This paper presents a novel approach for generating synthetic training data for deep networks by leveraging 3D model rendering and adversarial training to learn additional transformations. The methodology is applied to the generation of barcode-like markers for honeybee identification, demonstrating that a classifier trained on synthetic data outperforms those trained on limited real data or data with manually designed augmentations.
The subject matter, focusing on the use of machine learning and adversarial training for creating realistic synthetic training data, is both intriguing and significant. The proposed method appears sound, and the paper is well-written. However, the experiments are confined to a relatively simple and specialized domain of honeybee marker classification. To establish the broader applicability of the method and facilitate comparison with existing techniques, it would be beneficial to conduct experiments on standard and realistic datasets. Overall, I recommend acceptance, with the suggestion that the authors expand their experimental scope to additional datasets.
The inclusion of a baseline with manually designed transformations strengthens the paper, as acknowledged. It would also be insightful to investigate whether restricting the Generative Adversarial Network (GAN) to a fixed set of transformations is necessary and to identify the most critical transformations. This analysis could provide valuable guidelines for designing transformation sets in more complex scenarios, as suggested by Reviewer3.
The authors should moderate their claims, such as asserting their method surpasses previous work by training a Deep Convolutional Neural Network (DCNN) from scratch with generated data that performs well on real data, unlike previous studies that relied on real data or mixed real and generated data for training. This comparison is unfair due to the significant difference in complexity between the domain studied in this work and those in previous studies, making such a direct comparison inappropriate.