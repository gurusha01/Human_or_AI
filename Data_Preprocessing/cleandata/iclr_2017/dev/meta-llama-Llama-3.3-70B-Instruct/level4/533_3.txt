This manuscript investigates intrinsic motivation within the framework of deep reinforcement learning (RL), presenting several variants that stem from an auxiliary model-learning process, including prediction error, surprise, and learning progress. The authors demonstrate the effectiveness of these variants in enhancing exploration across various continuous control tasks and the Atari game "Venture".
In terms of novelty, the proposed intrinsic motivation types are not new, and the application of these concepts to deep RL may also not be entirely original, as evidenced by earlier works such as Kompella et al. (2012).
The potential of the idea to seek out states where the transition model is uncertain is reasonable but has its limitations. The authors should consider discussing these limitations more thoroughly. For instance, in complex domains like Go, where the transition model can be learned with ease, this approach may default to random exploration, suggesting that alternative measures of learning progress or surprise, derived from the agent's competence, could be more fruitful in the long term. Relevant discussions can also be found in Srivastava et al. (2012).
Regarding computational efficiency, the claimed superiority of the proposed method over VIME seems exaggerated. The observed gain appears to primarily result from faster initialization, with similar per-step costs. Given that VIME performs competitively, the authors should provide stronger arguments to justify the advantages of their method over existing approaches.