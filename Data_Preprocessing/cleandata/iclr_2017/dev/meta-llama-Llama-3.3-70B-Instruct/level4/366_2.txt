This manuscript presents a novel application of neural variational inference to topic models, introducing a clever approximation of the Dirichlet prior using a softmax basis with a Gaussian distribution, which is then optimized by maximizing the variational lower bound. The authors also investigate a solution to mitigate the component collapsing issue, a longstanding problem in models with continuous latent variables following a Gaussian distribution. The obtained results appear promising, and the experimental methodology seems sound.
Minor comments:
The authors should consider citing references [1] or [2] to provide context for neural variational inference and [2] for Variational Autoencoders (VAEs).
A minor typo is present in the sentence describing the approximation of the Dirichlet prior, where "results in the distribution" should be preceded by "is".
In Table 2, it is mentioned that DMFVI was trained for over 24 hours without yielding results; however, it would be beneficial to report the outcome after allowing the training process to complete.
Table 3 reveals that LDA-Collapsed Gibbs and NVDM exhibit lower perplexities, whereas the proposed ProdLDA model generates more coherent topics, prompting the question of what intuition the authors have regarding this discrepancy.
The training speed until convergence, as influenced by different learning rate and momentum scheduling approaches (as depicted in Figure 1), would be an interesting aspect to explore.
Further analysis of the latent variables, including the issue of component collapsing, could provide additional insights, although the results suggest that the proposed learning rate and momentum scheduling strategy effectively addresses this concern.
Overall, the paper effectively conveys its central idea, justifies the use of neural variational inference, and supports its claims with experimental results. It clearly articulates the challenges and demonstrates potential solutions, making a compelling contribution to the field.
References:
[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML'14
[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML'14