This paper introduces a methodology for visualizing and analyzing policies derived from observed trajectories, enabling the inference of higher-level skills and state clustering. The outcome is a simplified, discrete higher-order state and action transition matrix, which can be utilized for analysis, modeling, and interpretation. The authors propose constructing semi-aggregated MDPs by combining concepts from semi-MDPs and aggregated-MDPs, involving feature selection, state clustering, skill inference, reward and skill length inference, and model selection. The methodology is demonstrated using a small grid-world problem and a DQN-trained agent for playing Atari games.
The importance of tools for interpreting RL methods is correctly identified by the authors, particularly for real-world applications in fields like robotics and high-consequence systems. The presented method yields a high-level transition matrix, similar to hierarchical RL methods that combine lower-level skills with higher-level policies. However, the novelty of this approach lies in its ability to analyze already trained agents and derive a structure, which is a notable advantage.
The methodology builds upon existing ideas, combining semi-MDPs and aggregated-MDPs with modified k-means for state clustering. While the approach is interesting, its novelty appears limited, and the paper would benefit from a clearer emphasis on the contributions beyond combining existing methods. Additionally, highlighting the practical utility of the method would strengthen the paper.
The evaluation section could be improved with more analytical results and precise evaluations to demonstrate the full potential of the method. To enhance readability, the Semi-Aggregated MDP section should provide a more detailed description of the methods, including algorithms and formulas where applicable. The paper should also be self-contained, with additional background information on relevant principles, such as Occam's Razor. Reducing the number of acronyms and defining them before use would also improve clarity.
The approach of reverse-engineering the hierarchy and learning high-level transition matrices is promising, with potential applications in outperforming single-network approaches by using the model as input to specialized hierarchical trainers. However, the paper falls short in terms of novelty, precision, and clarity, which detracts from its overall impact. With revisions to address these concerns, the paper could more effectively convey the value and potential of the proposed methodology.