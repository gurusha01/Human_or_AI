This manuscript explores a neglected aspect of reinforcement learning and locomotion studies, shedding light on a crucial research gap.
A primary concern is the limited scope of the experimental findings, which restricts the ability to extrapolate or generalize the results. The authors' exclusive focus on a single neural network architecture and reward function raises questions about the potential limitations of the policy network and torque controller. To strengthen the conclusions, I recommend exploring the effects of varying neural network parameters, such as the number of neurons, or demonstrating the robustness of the results using alternative state representations, like pixel data-based training. As it stands, the reference to "DeepRL" appears somewhat unjustified.
On a positive note, the paper exhibits a clear and well-organized structure, making it easily accessible to readers. The experimental design is rigorous, and the results are clearly presented and interpretable. The research direction is intriguing, and I believe that extending the study to more realistic scenarios, such as incorporating physical constraints like actuator limitations, communication delays, or real-robot experiments, could significantly enhance the impact of this work, particularly beyond the current 3D extension.