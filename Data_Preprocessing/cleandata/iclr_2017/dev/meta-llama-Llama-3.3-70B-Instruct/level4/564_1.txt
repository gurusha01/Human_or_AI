The authors of this paper investigate the concept of introducing skip connections over time in Recurrent Neural Networks (RNNs), although the fundamental idea is not especially novel. They do, however, assess various methods for integrating this information into the current hidden state using different pooling functions. The performance of these models is evaluated on two widely-used text benchmarks.
Several concerns arise:
1) The experimental scope is limited to NLP tasks, specifically prediction tasks. Expanding the evaluation to other domains, such as modeling conditional distributions p(y|x) beyond just p(x), and incorporating sensory input data like audio or video, would provide more comprehensive insights.
2) As noted by other reviewers, the comparisons to existing models seem unfair. The state-of-the-art in NLP is rapidly evolving, making it challenging to contextualize the experiments within the broader landscape.
3) The paper claims that the proposed approach enhances long-term prediction capabilities, yet it lacks a thorough analysis to support this assertion, as I previously pointed out.
4) The authors assert that LSTM training is slow and difficult to scale, which contradicts my personal experience and is also belied by the widespread adoption of LSTM systems in production environments at major companies like Google, Baidu, and Microsoft.
While the paper's core idea is intriguing, the aforementioned issues lead me to believe that it requires further development before being considered for publication.