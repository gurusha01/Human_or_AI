This manuscript presents a novel intrinsic reward approach based on surprise for reinforcement learning, accompanied by two algorithms designed to estimate these rewards. The underlying concepts bear resemblance to existing research in intrinsic motivation, including VIME and other related studies. 
A notable advantage of the proposed methods is their ease of implementation, which yields benefits across various tasks. 
However, their performance is generally surpassed by VIME, and none of the proposed methods consistently emerge as the top performer among those presented (with surprisal being the most consistent, albeit not asymptotically equivalent to the true reward). The authors assert a significant speed improvement, yet the numerical results indicate that while VIME may be slower in initialization, its per-iteration speed is not substantially different (a big O analysis could potentially provide clarity on these claims).
Overall, the technique is reasonable and straightforward, potentially offering a minor incremental advancement over the current state of the art.