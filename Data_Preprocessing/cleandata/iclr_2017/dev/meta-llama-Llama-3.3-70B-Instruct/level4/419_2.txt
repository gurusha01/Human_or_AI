This manuscript introduces TopicRNN, a novel approach that integrates Latent Dirichlet Allocation (LDA) and Recurrent Neural Networks (RNN) by incorporating a switching variable. This variable selectively includes or excludes additive effects from latent topics during word generation, thus enhancing traditional RNN capabilities.
The authors conduct experiments on two primary tasks: language modeling using the Penn Treebank (PTB) dataset and sentiment analysis on the IMDB dataset. The results demonstrate that TopicRNN surpasses the performance of standard RNN on PTB and achieves state-of-the-art (SOTA) results on IMDB.
Several aspects of the paper warrant further clarification and consideration:
- The methodology behind utilizing LDA features for RNN, as presented in Table 2, requires additional explanation.
- Incorporating results from Long Short-Term Memory (LSTM) networks would provide a more comprehensive comparison, even if LSTM exhibits lower perplexity than TopicRNN. This would help assess how the addition of latent topics narrows the performance gap between RNN and LSTM.
- The generated text samples in Table 3 are unclear in their significance. It would be beneficial to understand what specific aspect these samples are intended to illustrate, particularly in relation to the topic "trading" and the IMDB dataset.
- The scalability of the proposed method for large vocabulary sizes (exceeding 10,000) is a crucial consideration that needs to be addressed.
- To ensure a fair comparison with baseline methods such as Bag-of-Words, LDA, and Support Vector Machines (SVM), it would be insightful to report the accuracy on IMDB when the extracted features are directly used for classification, bypassing the neural network with a single hidden state.