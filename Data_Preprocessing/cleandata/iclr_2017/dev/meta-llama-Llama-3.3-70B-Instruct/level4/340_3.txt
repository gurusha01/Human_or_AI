Following the revised submission and rebuttal, my evaluation remains unchanged.
This paper introduces an unsupervised domain transfer algorithm, which translates samples between related domains under the condition that a predefined function f yields the same output for both the input and the translated result.
Strengths:
1. The paper explores a novel concept of comparing cross-domain samples using a fixed perceptual function f, offering an interesting perspective.
2. The proposed approach generates aesthetically pleasing results across multiple datasets, demonstrating its potential.
3. The authors successfully apply their method to domain adaptation, achieving improved performance on the SVHN to MNIST task, which highlights the method's applicability.
4. The paper's clarity and readability make it accessible to a broad audience.
Weaknesses:
1. The method's novelty is somewhat limited, with the f-constancy term being the primary innovative aspect.
2. The approach's robustness across significantly dissimilar domains is questionable. Relying on a fixed function f, trained on the source domain, may lead to information loss crucial for achieving better target domain reconstructions and a tighter relationship between x and g(f(x)). To address this, the authors could consider end-to-end training of all model components or incorporate target domain samples into f's training process.
3. With only a single domain adaptation experiment, it's premature to consider the proposed method a universal alternative to existing domain adaptation approaches, warranting further experimentation.
Additionally, presenting super-resolved outputs instead of the model's actual outputs may create an overly favorable impression of the transferred samples' visual quality. It would be beneficial to relocate the original outputs from the appendix to the main body of the paper for a more accurate representation.