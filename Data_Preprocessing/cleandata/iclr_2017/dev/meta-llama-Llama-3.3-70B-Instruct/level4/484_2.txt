This paper presents a sophisticated algebraic framework for analyzing the functionality of convolutional networks, leveraging a complex machinery to deconstruct the underlying mathematical structures. However, similar to other attempts in the literature, the idealized networks described herein, which are interpretable as polynomials over tensors, diverge from the convolutional neural networks (CNNs) commonly used in practice. For instance, the ReLU non-linearity is substituted with a product of linear functions or a sum of logarithms.
Despite the high technicality of the paper, which makes it challenging to read, each concept is meticulously defined, and mathematical terminology is properly introduced. Nevertheless, the authors could enhance the accessibility of key concepts and provide a more intuitive understanding of the separation rank before delving into various mathematical interpretations. Given my rusty familiarity with Support Vector Machine (SVM)-era algebra and limited knowledge of the separation rank framework, I found it difficult to grasp the general deep case without first fully comprehending a simpler, more gentle case, such as the shallow network example in section 5.3.
To summarize my understanding of the key theorem 1 result: 
- In the shallow case, the upper bound of the separation rank is utilized to demonstrate that this rank grows at most linearly with the network size, as measured by the single hidden layer. Consequently, exponential network sizes are necessitated by the requirement for this rank to grow exponentially, as dictated by the partition.
- In the deep case, the upper bound is also linear in the size of the network, measured by the last hidden layer, but this scenario is contingent upon the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is inherently linear, thereby allowing the network size to remain linear.
My attempt to summarize the key point of this paper likely falls short, underscoring the complexity of the partition rank notion and its linear growth with network size, which can be either beneficial or detrimental depending on the context. Hopefully, a future explanation will be able to distill this concept into a single, comprehensible slide.
While this paper is worthy of publication in its current form, I propose two suggestions to enhance its significance:
- On the theoretical side, the current work remains far from the completeness achieved by the Probably Approximately Correct (PAC) bound papers of the "shallow era." Specifically, the non-probabilistic lower and upper bounds in theorem 1 are likely loose, and a PAC-like theory is needed to guide the selection of bounds and predict their impact on performance. Furthermore, the prediction of inductive bias is incomplete, as this paper only addresses the maximal representation capacity of a Deep Neural Network (DNN) under bounded network size constraints without considering overfitting, which is typically justified by PAC or VC-dim like bounds.
- On the practical side, the experiments primarily confirm intuitive expectations or simpler forms of reasoning. For instance, using convolutions that join symmetrical pixels in images to detect symmetry can be achieved through basic, hand-crafted pattern detectors, as has been done in computer vision for decades. A more compelling motivation for using this framework would be its ability to answer questions that defy simple human intuition and remain unclear, such as the recent application of gated convolutions 'a trous' for 1D speech signals, popularized in Google WaveNet.