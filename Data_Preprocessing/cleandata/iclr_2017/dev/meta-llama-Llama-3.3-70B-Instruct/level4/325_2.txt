This paper proposes a novel method for training generative models through an iterative denoising process, which initializes with a random sample from a crude data distribution approximation and refines it into a high-quality sample via multiple denoising steps. The training process involves setting up a Markov chain that gradually blends the current denoising model's propositions with a real data example, updating the model to reproduce the improved samples from this blending process.
The paper is well-written and presents an intriguing approach to training generative models. The simplicity of the method is notable, and the author's enthusiasm for the paper is evident. The proposed method is novel, yet it draws clear inspiration from recent works on denoising models for distribution sampling, such as those by Sohl-Dickstein and the use of Denoising Autoencoders (DAEs) as generative models.
This direction of research is important, as it has the potential to sidestep issues associated with current directed generative model training procedures, including convergence and mode coverage problems in Generative Adversarial Networks (GANs) and difficulties in modeling multi-modal distributions. However, another promising method, combining Hamiltonian Monte Carlo inference with variational inference, as seen in [1], may also address these issues and could potentially outperform the presented method.
A direct comparison to Sohl-Dickstein's procedure in terms of sampling steps and generation quality would be beneficial, given the direct relation between the two methods. Additionally, a more comprehensive comparison between different inference methods using Markov Chain Monte Carlo (MCMC) chain-like procedures would be valuable, although it may be outside the scope of this paper.
Key points to note include:
- The method is generally well-explained, but some training details are lacking, such as the setting of alpha and omega, and how alpha affects the generator's capabilities.
- The absence of infusion chains or generating chains for more complex data distributions is unfortunate, as they would be interesting to examine.
- The paper effectively evaluates the model using various metrics and provides a bound on the log-likelihood.
- The current approach lacks theoretical guarantees, and it is unclear how the choice of alpha impacts the procedure's success or its connection to MCMC sampling or energy-based models.
Minor issues include:
- A broken reference
- Figure 3's limited information due to starting at 100 epochs
- Unclear explanation of the convolutional network structure
- Unusual choice of parametrizing variance via a sigmoid output unit
- Errors in footnote 1 and pages 1 and 4
Despite these points, the author's response has clarified open issues, and the paper's acceptance to the conference is strongly supported. The sub-optimal generator architecture may impact performance but does not detract from the paper's main contributions. Therefore, the score remains a clear accept, emphasizing the paper's publication value.