This paper proposes several techniques to compress a broad and shallow text classification model that relies on n-gram features, including the utilization of optimized product quantization for embedding weight compression, the pruning of certain vocabulary elements, and the application of hashing to reduce vocabulary storage, albeit the latter being a minor aspect. The primary focus is on models with exceptionally large vocabularies, demonstrating a significant reduction in model size with a relatively minor compromise on accuracy.
The issue of compressing neural models is both pertinent and intriguing. The methods section is well-structured, complete with high-level commentary and relevant references. However, from my perspective, the paper's contributions to the field of machine learning are somewhat limited. The experimental results, which mainly draw on less commonly used benchmarks, are not entirely convincing, and the implications for state-of-the-art RNN text classification models are unclear.
The application of optimized product quantization for approximating inner products is not particularly groundbreaking, as similar approaches have been explored in previous research. The majority of the model size reduction can be attributed to the pruning of vocabulary elements, a method based on the assumption that embeddings with larger L2 norms hold greater importance, also considering a coverage heuristic. From a machine learning standpoint, a more appropriate baseline for addressing this issue would involve using a set of relaxed binary coefficients for each embedding vector, learning these coefficients in conjunction with the weights, and potentially employing an L1 regularizer on the coefficients to encourage sparsity. Additionally, from a practical perspective, a significant baseline appears to be missing: an examination of the outcomes when simply utilizing fewer vocabulary elements, such as those based on subword units.