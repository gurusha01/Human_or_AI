UPDATE: Having reviewed the responses to this thread, my assessment remains unchanged.
The authors present deep VCCA, a probabilistic CCA model extension that utilizes neural networks to parameterize likelihoods, thereby introducing a deep variant. This approach employs variational inference, incorporating an inference network and reparameterization gradients. Additionally, a variant termed VCCA-private is proposed, which introduces local latent variables for each data point or view, and a connection to the multi-view autoencoder is established.
In the context of recent advancements in black box variational inference and variational autoencoders, the methodology outlined in this paper may be seen as somewhat unremarkable. The model represents a straightforward extension of probabilistic CCA, leveraging neural networks to parameterize likelihoods, with inference proceeding mechanically as in standard black box approaches using reparameterization gradients and inference networks. Furthermore, the mean-field approximation employed is relatively outdated, given the development of more expressive approximations in recent years, as exemplified by the work of Rezende and Mohamed (2015) and Tran et al. (2016).
The initial insight into the connection between deep VCCA and multi-view autoencoders is noteworthy but ultimately reduces to the distinction between MAP and variational inference, a well-established concept. The key difference lies in the KL regularizer, and even with noisy samples, the variances of a normal variational approximation would converge to zero, effectively rendering it equivalent to optimizing a point estimate from the MVAE objective. Although the authors seem to be aware of this to some extent, as evident from their remarks in the paper, the implications are not fully explored.
Despite these reservations, the paper demonstrates significant strengths in its applications. The experimental results are robust, comparing favorably to alternative multi-view approaches across a range of interesting datasets. The use of "private variables," although simple, successfully disentangles the per-view latent representation from the shared view. A comparison to methods utilizing probabilistic inference, such as full Bayes for linear CCA, would have been a valuable addition.
The paper also relies on several approximations that have become standard, including the use of a mean-field family and an inference network, which may not be strictly necessary. To better understand the influence of approximate inference on the model's fit, I strongly recommend incorporating MCMC and non-amortized variational inference into at least one experiment.
References:
- Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.
- Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.