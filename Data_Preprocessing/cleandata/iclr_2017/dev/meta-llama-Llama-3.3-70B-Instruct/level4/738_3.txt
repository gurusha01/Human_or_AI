CONTRIBUTIONS
The training of LSTMs often yields intermediate gradients that are nearly zero, a consequence of the tanh and sigmoid nonlinearities' flat shape far from the origin. This study demonstrates that rounding these negligible gradients to zero leads to matrices with a sparsity of up to 80% during training. Furthermore, it is shown that applying this sparsification technique to character-level LSTM language models does not substantially impact the model's final performance. The authors suggest that this inherent sparsity could be leveraged by specialized hardware to enhance the energy efficiency and training speed of recurrent networks.
NOVELTY
To the best of my knowledge, the concept of thresholding gradients to induce sparsity and improve efficiency in RNN training represents a novel contribution.
MISSING CITATIONS
Previous research has investigated the use of low-precision arithmetic in recurrent neural network language models, including the work of Hubara et al, "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations".