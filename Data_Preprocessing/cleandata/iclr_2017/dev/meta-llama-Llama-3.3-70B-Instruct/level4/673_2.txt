This paper presents an algorithm for training memory networks with exceptionally large memories, addressing the limitations of traditional training methods that utilize soft-attention mechanisms over all memory slots, which are not only computationally expensive but also challenging to train due to gradient dispersion. The proposed approach involves employing the k-mips algorithm to select a subset of memory slots for attention application, mitigating the costs associated with exact k-mips, which are equivalent to those of full attention, by using approximate k-mips, albeit at the expense of performance. A notable consequence of utilizing k-mips is the inability to learn memory slots, necessitating their pre-training and fixation throughout the training process. The experimental evaluation, conducted on the SimpleQuestions dataset, demonstrates the efficacy of k-mips, with exact k-mips yielding performance comparable to full attention, while approximate k-mips results in performance degradation. The paper's clarity and readability are notable.
However, the proposed ideas are not entirely convincing, and several concerns arise. Firstly, the k-mips algorithm's requirement for fixed memories imposes a significant constraint, particularly for problems or datasets that demand multiple training hops for compounded reasoning, which raises questions about the technique's usefulness. Secondly, exact k-mips has the same sample complexity as full attention, and the only means of achieving a speedup is through approximate k-mips, which, as expected, leads to a substantial drop in performance. Thirdly, the paper's motivation for eliminating heuristics used to prune memories is undermined by the authors' own use of multiple heuristics in Section 3.1 to facilitate training, despite these heuristics being data-independent, which seems to merely defer the issue of heuristics rather than resolving it. Lastly, the experimental results are unconvincing due to the absence of speed comparisons and the lack of evaluation against alternative fast nearest neighbor search methods, such as FLANN, beyond k-mips.