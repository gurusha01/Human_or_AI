This submission presents a novel letter-level decoder that utilizes a modified version of the Connectionist Temporal Classification (CTC) approach, referred to as ASG. The ASG method eliminates the blank symbol and replaces it with letter repetition symbols, while also omitting explicit normalization. The concept of a letter-level model, although not entirely new, and the proposed CTC variant are both noteworthy.
The approach is evaluated on the LibriSpeech task, with the authors claiming that their method is competitive. A comparison is made between the ASG modeling variant and the traditional CTC approach. However, a comparison between the letter-level approach and existing word-level results is lacking. When compared to the results obtained by Panayotov et al. in 2015, the performance achieved in this submission appears to be comparable only to word-level GMM/HMM models, but inferior to word-level hybrid DNN/HMM models. It is worth noting that Panayotov et al. also employed speaker adaptation, which is not used in this submission. To provide a more comprehensive understanding, it is suggested that the authors include a comparison with Panayotov's results, in addition to mentioning Baidu's results on Librispeech, which are not directly comparable due to the significantly larger amount of training data used. The authors' comment that Baidu's GPU implementation for CTC is more suited for larger vocabularies is well-taken, and therefore, the comparison to GPU in Tables 1a-c may not be entirely relevant without further discussion on the implementations.
The use of a relatively large analysis window, nearly 2 seconds, is notable. Although other authors have used windows of up to 0.5s to 1s (e.g., MRASTA features), it would be beneficial to provide some insight into the reasoning behind the choice of such a large window and the advantages it offers.
The submission is well-written, but additional details on the experiences with using non-normalized (transition) scores and beam pruning would be valuable. The readability of Table 1 could be improved by including the units of the numbers within the tables themselves, rather than only in the caption.
It is essential to clearly mention and reference any prior (partial) publications of this work, such as the NIPS end-to-end workshop paper. Furthermore, clarification is needed on the term "transition scalars."
Minor comments include:
- In Section 2.3, the sentence "train properly the model" should be revised to "train the model properly," and the word "boostrap" should be corrected to "bootstrap." An automatic spell check should be performed to avoid such errors.
- In Section 2.3, "Bayse" should be corrected to "Bayes."
- The definition of logadd is incorrect and should be revised.
- The line before Equation (3) should be revised to "all possible sequences of letters" (plural).
- In Section 2.4, the word "threholding" should be corrected to "thresholding."
- Figure 4 should mention the corpus used, which appears to be the dev corpus.
A condensed version of this submission will be presented at the NIPS end-to-end workshop on December 10, 2016. The NIPS submission appears to be a subset of this submission and should be acknowledged in the paper. When dropping the normalization of acoustic model scores, the range of scores obtained may vary, affecting beam pruning and its relation to the normalized LM scores. It would be beneficial to analyze this aspect. In Section 2.3, digits are used to label character repetitions, but it is unclear how numbers are handled. Additionally, the notation used is inconsistent, with the variable 't' representing different time scales in Equation (1) and elsewhere in the text.