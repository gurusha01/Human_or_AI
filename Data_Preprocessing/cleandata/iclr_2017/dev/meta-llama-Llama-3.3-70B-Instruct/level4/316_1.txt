This manuscript tackles the challenge of differential privacy in a broadly applicable setting, where multiple teachers are trained on non-overlapping subsets of sensitive data and a student model makes predictions based on publicly available data labeled by teachers through a noisy voting mechanism. The authors' approach is well-founded and clearly articulated, making it easy to follow. However, further examination of the bound established in Theorem 1, including its tightness, would enhance the discussion. The authors effectively repurpose the concept of introducing perturbation error to counts, a familiar technique in differential privacy research, and skillfully adapt it to a more complex, non-convex environment with practical implications, surpassing the scope of similar studies in the field. The methodology's broad applicability, significant improvement over existing work, and the clarity of the presentation collectively make a strong case for publication.