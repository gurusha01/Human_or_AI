This paper presents a novel end-to-end speech recognition system that combines a convolutional neural network (CNN) with a graph decoding approach, trained using a new automatic segmentation criterion called AutoSegCriterion (ASG). The authors claim that their system is simpler and faster than existing approaches, such as Connectionist Temporal Classification (CTC), while achieving competitive results on the LibriSpeech corpus.
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper presents a well-motivated approach that addresses a significant problem in speech recognition, namely the need for force alignment of phonemes. The authors provide a clear and concise overview of the existing literature and demonstrate a good understanding of the field. Secondly, the experimental results show that the proposed ASG criterion is faster and as accurate as CTC, which is a significant contribution to the field.
The paper provides a thorough evaluation of the proposed approach, including a comparison with CTC and an analysis of the impact of training size and data augmentation on the results. The authors also provide a detailed description of the architecture and the decoding process, which makes it easy to follow and understand the paper. The results are promising, with a word error rate (WER) of 7.2% on the LibriSpeech corpus with MFCC features, which is competitive with state-of-the-art systems.
To further improve the paper, I would suggest that the authors provide more details on the implementation of the ASG criterion and the decoding process, including any optimization techniques used to improve the speed and accuracy of the system. Additionally, it would be interesting to see a more detailed analysis of the results, including an error analysis and a comparison with other state-of-the-art systems.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the ASG criterion handle out-of-vocabulary words and rare events? Can the authors provide more details on the language model used in the decoding process and how it is integrated with the acoustic model? How does the system perform on other datasets and in different acoustic environments? 
Overall, the paper presents a significant contribution to the field of speech recognition, and with some minor revisions, it has the potential to be a strong publication.