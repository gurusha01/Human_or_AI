This paper introduces two new models, Generative Paragraph Vector (GPV) and Supervised Generative Paragraph Vector (SGPV), which are designed to learn distributed representations of texts. The GPV model is a probabilistic extension of the Paragraph Vector model, and it can infer distributed representations for unseen texts. The SGPV model builds upon GPV and incorporates text labels into the model, allowing it to be used for supervised learning tasks.
The paper claims that GPV and SGPV can outperform existing state-of-the-art models on several text classification tasks. The authors provide experimental results on five benchmark datasets, which show that GPV and SGPV can achieve superior performance compared to other unsupervised and supervised models.
I decide to accept this paper with minor revisions. The main reasons for this decision are:
1. The paper introduces a novel and well-motivated approach to learning distributed representations of texts. The idea of using a generative process to infer representations for unseen texts is innovative and has the potential to improve the performance of text classification models.
2. The experimental results provided in the paper are thorough and demonstrate the effectiveness of GPV and SGPV on several benchmark datasets.
However, there are some minor issues that need to be addressed:
* The paper could benefit from a more detailed discussion of the limitations of the proposed models. For example, the authors mention that GPV and SGPV rely on uni-grams, which may not capture compositional semantics. A more detailed analysis of the limitations of the models would strengthen the paper.
* The authors could provide more insight into the hyperparameter tuning process for GPV and SGPV. The paper mentions that the hyperparameters were tuned using a grid search, but it would be helpful to know more about the specific hyperparameters that were used and how they were selected.
* The paper could benefit from a more detailed comparison with other state-of-the-art models. While the authors provide a thorough comparison with several baseline models, it would be helpful to know how GPV and SGPV compare to other recent models that have achieved state-of-the-art performance on text classification tasks.
To improve the paper, I suggest that the authors address these minor issues and provide more detail on the limitations and hyperparameter tuning process of the proposed models. Additionally, I would like the authors to answer the following questions:
* Can you provide more insight into the choice of prior distribution for the paragraph vectors in GPV? Why was a multivariate normal distribution chosen, and how does this affect the performance of the model?
* How do GPV and SGPV handle out-of-vocabulary words? Are there any plans to extend the models to handle this case?
* Can you provide more detail on the computational resources required to train GPV and SGPV? How do the models scale to large datasets, and are there any plans to parallelize the training process?