This paper proposes a novel neural network structure, called Higher Order Recurrent Neural Networks (HORNNs), to better model long-term dependency in sequential data. The authors extend the standard Recurrent Neural Network (RNN) structure by using more memory units to keep track of more preceding RNN states, which are fed to the hidden layers as feedback through different weighted paths. The paper also explores various pooling functions to calibrate the signals from different feedback paths.
I decide to accept this paper, with two key reasons for this choice. Firstly, the approach is well-motivated and placed in the literature, addressing the notorious problem of vanishing gradients in RNNs. Secondly, the paper provides strong experimental results, demonstrating the state-of-the-art performance of HORNNs on two popular language modeling tasks, namely the Penn Treebank (PTB) and English text8 sets.
The supporting arguments for these reasons are as follows. The authors provide a clear and concise introduction to the problem of modeling long-term dependency in sequential data and the limitations of traditional RNNs. They also provide a thorough review of related work, including the use of long short-term memory (LSTM) models and gated recurrent units (GRUs). The proposed HORNN structure is well-explained, and the use of pooling functions is justified. The experimental results are impressive, with HORNNs outperforming regular RNNs and LSTMs on both PTB and text8 sets.
Additional feedback to improve the paper includes providing more details on the computational complexity of HORNNs and comparing it with other models. The authors could also explore the use of HORNNs for other sequential modeling tasks, such as speech recognition and sequence-to-sequence modeling. Furthermore, it would be interesting to see a more detailed analysis of the effect of different pooling functions on the performance of HORNNs.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend the HORNN structure to other sequential modeling tasks? (2) Can the authors provide more insights into the effect of different pooling functions on the performance of HORNNs? (3) How do the authors plan to address the potential issue of overfitting in HORNNs, especially when using large models and datasets?