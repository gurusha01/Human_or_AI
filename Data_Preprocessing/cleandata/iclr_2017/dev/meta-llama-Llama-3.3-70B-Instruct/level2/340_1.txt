The paper "Domain Transfer Network" presents a novel approach to transferring samples from one domain to another, while preserving the representation of a given function. The authors propose a compound loss function that combines a multiclass GAN loss, an f-preserving component, and a regularizing component to encourage the generator to map samples from the target domain to themselves. The method is applied to visual domains, including digits and face images, and demonstrates its ability to generate convincing novel images of previously unseen entities while preserving their identity.
I decide to accept this paper, with the main reason being that it presents a well-motivated and novel approach to the problem of domain transfer. The authors provide a clear and concise explanation of the problem, and the proposed solution is well-supported by theoretical analysis and experimental results.
The paper supports its claims through a series of experiments, including the transfer of images from the Street View House Number (SVHN) dataset to the MNIST dataset, and the transfer of face images to emoji. The results demonstrate the effectiveness of the proposed method in generating high-quality images that preserve the representation of the given function. The authors also provide a thorough analysis of the contributions of each component of the loss function, and demonstrate the importance of each term in achieving good performance.
One potential limitation of the paper is that the method is not thoroughly compared to other state-of-the-art methods for domain transfer and style transfer. While the authors provide some comparisons to existing methods, a more comprehensive evaluation would be beneficial to fully understand the strengths and weaknesses of the proposed approach.
To improve the paper, I suggest that the authors provide more details on the implementation of the method, including the architecture of the generator and discriminator networks, and the hyperparameters used in the experiments. Additionally, the authors could provide more analysis on the robustness of the method to different types of noise and perturbations, and explore the application of the method to other domains and tasks.
Some questions I would like the authors to answer include: How does the method handle cases where the representation function f is not accurate on the target domain? Can the method be extended to handle multiple source and target domains? How does the method compare to other state-of-the-art methods for domain transfer and style transfer in terms of computational efficiency and scalability?