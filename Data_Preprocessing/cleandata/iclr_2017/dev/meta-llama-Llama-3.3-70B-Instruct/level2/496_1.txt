This paper proposes a novel multiscale approach, called the Hierarchical Multiscale Recurrent Neural Network (HM-RNN), which can capture the latent hierarchical structure of sequences without using explicit boundary information. The authors introduce a set of binary variables, called boundary detectors, to learn segments at each level, and a novel update rule dependent on the states of these binary variables. The HM-RNN is evaluated on two tasks: character-level language modeling and handwriting sequence generation, achieving state-of-the-art results on the Text8 dataset and comparable results to the state-of-the-art on the Penn Treebank and Hutter Prize Wikipedia datasets.
I decide to accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and well-motivated problem in the field of recurrent neural networks, and (2) the approach is well-supported by empirical evidence, including experiments on multiple benchmark datasets.
The paper provides a clear and well-structured presentation of the proposed model, including a detailed description of the update mechanism and the boundary detectors. The experiments demonstrate the effectiveness of the HM-RNN in discovering the latent hierarchical structure of sequences and achieving better generalization performance. The use of visualization techniques, such as heatmaps and segmentations, helps to illustrate the internal process of the RNN and provide insights into the learned hierarchical structure.
To further improve the paper, I suggest the authors provide more analysis on the learned boundary detectors and their relationship to the underlying structure of the data. Additionally, it would be interesting to see more comparisons with other state-of-the-art models, such as the Clockwork RNN and the Biscale RNN, to better understand the strengths and weaknesses of the HM-RNN.
Some questions I would like the authors to answer to clarify my understanding of the paper include: (1) How do the boundary detectors learn to fire at the correct boundaries, and what is the role of the slope annealing trick in this process? (2) Can the authors provide more insights into the learned hierarchical structure of the sequences, and how it relates to the underlying syntax and semantics of the data? (3) How does the HM-RNN handle sequences with variable-length segments, and are there any limitations to the model's ability to learn hierarchical structures in such cases?