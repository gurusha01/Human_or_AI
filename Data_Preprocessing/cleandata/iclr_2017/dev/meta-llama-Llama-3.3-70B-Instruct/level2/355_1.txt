Summary of the Paper
The paper proposes a new framework for training a vision-based agent for the First-Person Shooter (FPS) game Doom, using a combination of deep reinforcement learning and curriculum learning. The framework combines the state-of-the-art Asynchronous Advantage Actor-Critic (A3C) model with curriculum learning, where the agent is trained on a sequence of progressively more difficult environments. The authors demonstrate the effectiveness of their approach by winning the champion of Track 1 in the ViZDoom AI Competition 2016.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and challenging problem in the field of reinforcement learning, and proposes a novel and well-motivated approach to solve it. Secondly, the paper provides strong empirical evidence to support its claims, including winning the champion of Track 1 in the ViZDoom AI Competition 2016.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem of training a vision-based agent for FPS games, and motivates the use of deep reinforcement learning and curriculum learning. The authors also provide a detailed description of their framework, including the architecture of the neural network and the training procedure. The empirical results are impressive, with the agent winning 10 out of 11 attended games and achieving a 35.4% higher score than the second-place agent.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process, as well as the computational resources required to train the agent. Additionally, it would be interesting to see more analysis on the learned tactics and behaviors of the agent, and how they compare to human players. Finally, I would like to see more discussion on the potential applications of this framework to other domains, such as robotics or autonomous driving.
Questions for the Authors
1. Can you provide more details on the hyperparameter tuning process, and how you selected the specific values used in the paper?
2. How do you think the agent's performance would be affected by using a different reinforcement learning algorithm, such as Deep Q-Networks (DQN) or Policy Gradient Methods (PGMs)?
3. Can you provide more analysis on the learned tactics and behaviors of the agent, and how they compare to human players? For example, do the agents learn to use specific strategies, such as camping or flanking, and how do they adapt to different opponents?