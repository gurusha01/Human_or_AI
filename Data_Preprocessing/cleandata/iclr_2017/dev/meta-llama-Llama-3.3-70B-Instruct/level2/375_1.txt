This paper presents a simple yet effective method for generating sentence embeddings, which achieves state-of-the-art results on various textual similarity tasks. The method involves computing a weighted average of word vectors in a sentence, where the weights are determined by a smooth inverse frequency (SIF) scheme, and then removing the common components by subtracting the projection of the sentence vector onto its first principal component. The authors provide a theoretical justification for this approach using a latent variable generative model for sentences, which is an extension of the Random Walk on Discourses model.
I decide to accept this paper for several reasons. Firstly, the approach is well-motivated and grounded in a solid theoretical framework. The authors provide a clear explanation of the intuition behind their method and demonstrate its effectiveness through extensive experiments on multiple datasets. Secondly, the results are impressive, with the proposed method outperforming several supervised and unsupervised baselines, including sophisticated neural network models. Finally, the paper is well-written, with clear and concise language, making it easy to follow and understand.
One potential limitation of the paper is that the method ignores the word order in sentences, which may be important for certain tasks. However, the authors acknowledge this limitation and provide some evidence that their method can still achieve good results despite this simplification. Additionally, the paper could benefit from more analysis on the robustness of the method to different types of noise or adversarial attacks.
To further improve the paper, I would like the authors to address the following questions: (1) How does the choice of word vectors affect the performance of the method? (2) Can the method be extended to handle longer documents or texts? (3) How does the method compare to other unsupervised sentence embedding methods, such as those based on autoencoders or generative adversarial networks? Overall, this is a strong paper that makes a significant contribution to the field of natural language processing, and I believe it deserves to be accepted.