This paper presents a comprehensive study on the behavior of untrained neural networks with randomly distributed weights and biases using mean field theory. The authors demonstrate the existence of depth scales that limit the maximum depth of signal propagation through these random networks. They show that random networks can be trained precisely when information can travel through them, and the depth scales identified provide bounds on how deep a network can be trained for a specific choice of hyperparameters.
The paper is well-structured, and the authors provide a clear and concise introduction to the problem, followed by a detailed analysis of the mean field theory and its application to random neural networks. The experimental results are thorough and well-presented, and the authors provide a good discussion of the implications of their findings.
One of the main strengths of the paper is its ability to provide a theoretical framework for understanding the behavior of random neural networks. The authors' use of mean field theory allows them to derive analytical expressions for the depth scales and the behavior of the gradients, which are then validated through numerical experiments. This provides a solid foundation for understanding the underlying mechanisms that govern the behavior of neural networks.
The paper also raises some interesting questions about the nature of neural network training and the role of hyperparameters in determining the trainability of a network. The authors' finding that networks can be trained precisely when information can travel through them suggests that the choice of hyperparameters is critical in determining the success of training. This has important implications for the design of neural network architectures and the development of training algorithms.
However, there are some limitations to the paper. The authors' analysis is limited to fully connected feed-forward networks, and it would be interesting to see how their results generalize to other types of networks, such as convolutional or recurrent networks. Additionally, the authors' use of mean field theory assumes that the weights and biases are randomly distributed, which may not always be the case in practice.
In terms of suggestions for improvement, it would be helpful to see more discussion of the implications of the authors' findings for the design of neural network architectures and the development of training algorithms. Additionally, it would be interesting to see more experimental results that validate the authors' theoretical predictions, particularly for larger and more complex networks.
Overall, this is a well-written and well-structured paper that provides a significant contribution to our understanding of the behavior of random neural networks. The authors' use of mean field theory provides a powerful framework for analyzing the behavior of these networks, and their experimental results provide strong validation of their theoretical predictions.
Decision: Accept
Reasons:
1. The paper presents a clear and concise introduction to the problem and provides a thorough analysis of the mean field theory and its application to random neural networks.
2. The authors provide a good discussion of the implications of their findings and raise some interesting questions about the nature of neural network training and the role of hyperparameters in determining the trainability of a network.
3. The experimental results are thorough and well-presented, and the authors provide a good validation of their theoretical predictions.
Additional feedback:
* It would be helpful to see more discussion of the implications of the authors' findings for the design of neural network architectures and the development of training algorithms.
* It would be interesting to see more experimental results that validate the authors' theoretical predictions, particularly for larger and more complex networks.
* The authors may want to consider generalizing their results to other types of networks, such as convolutional or recurrent networks.
Questions for the authors:
* Can you provide more discussion of the implications of your findings for the design of neural network architectures and the development of training algorithms?
* How do you think your results will generalize to other types of networks, such as convolutional or recurrent networks?
* Can you provide more experimental results that validate your theoretical predictions, particularly for larger and more complex networks?