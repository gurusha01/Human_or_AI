The paper "Tensorial Mixture Models" introduces a new family of probabilistic models that combine the strengths of mixture models and tensor decompositions. The authors propose a generative model, called Tensorial Mixture Models (TMMs), which represents the data as a sequence of local structures, where each local structure is generated from a small set of simple component distributions. The dependencies between the local structures are represented by a prior tensor, which is decomposed using tensor decomposition techniques, such as CANDECOMP/PARAFAC (CP) or Hierarchical Tucker (HT).
The authors show that TMMs have several attractive properties, including tractable inference, tractable marginalization, and a simple architectural design. They also demonstrate the effectiveness of TMMs in classification tasks with missing data, where the model can leverage its ability to marginalize over missing values to achieve optimal performance.
The paper is well-written, and the authors provide a clear and concise explanation of the technical details. The experiments are thorough, and the results are impressive, showing that TMMs outperform other state-of-the-art methods in classification tasks with missing data.
One of the strengths of the paper is its ability to provide a unified framework for modeling high-dimensional data with missing values. The authors show that TMMs can be used for both discriminative and generative tasks, and that they can be easily extended to other domains, such as image and text classification.
However, one potential limitation of the paper is that the authors do not provide a detailed analysis of the computational complexity of TMMs. While they mention that the model can be trained using standard stochastic gradient descent methods, they do not provide a detailed analysis of the time and space complexity of the algorithm.
Overall, I would recommend accepting this paper for publication. The authors have made a significant contribution to the field of machine learning, and their work has the potential to impact a wide range of applications, from computer vision to natural language processing.
To improve the paper, I would suggest that the authors provide a more detailed analysis of the computational complexity of TMMs, including a discussion of the time and space complexity of the algorithm. Additionally, the authors could provide more experimental results, including comparisons with other state-of-the-art methods on a wider range of datasets.
Some specific questions that I would like the authors to address in their response are:
* Can you provide a more detailed analysis of the computational complexity of TMMs, including a discussion of the time and space complexity of the algorithm?
* How do TMMs compare to other state-of-the-art methods in terms of computational efficiency?
* Can you provide more experimental results, including comparisons with other state-of-the-art methods on a wider range of datasets?
* How do you plan to extend TMMs to other domains, such as image and text classification?