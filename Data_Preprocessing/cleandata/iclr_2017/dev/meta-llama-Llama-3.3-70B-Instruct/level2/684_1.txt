This paper presents a novel model-based approach to deep reinforcement learning, which tackles the problem of multi-task learning in complex environments such as ATARI games. The authors propose a Predictive Reinforcement Learning (PRL) framework, which separates the understanding of the environment from the strategy, allowing the model to learn from different strategies simultaneously. The approach is based on a recurrent neural network architecture, specifically a Residual Recurrent Neural Network (RRNN), which decouples memory from computation.
The paper claims to achieve state-of-the-art results in multi-task learning, surpassing human performance in three different ATARI games simultaneously. The authors also demonstrate that their approach can benefit from learning multiple tasks, with no degradation in performance.
I decide to accept this paper, with the main reason being that it presents a well-motivated and novel approach to deep reinforcement learning, which tackles a significant problem in the field. The paper is well-written, and the authors provide a clear explanation of their methodology and results.
The supporting arguments for this decision are:
* The paper presents a significant improvement over existing approaches, specifically Q-learning, which is widely used in reinforcement learning.
* The authors provide a thorough analysis of the limitations of Q-learning and demonstrate how their approach can overcome these limitations.
* The experimental results are impressive, with the model achieving state-of-the-art results in multi-task learning.
However, I do have some additional feedback to help improve the paper:
* The authors could provide more details on the training process, specifically the learning schedule and the hyperparameters used.
* The paper could benefit from a more detailed analysis of the results, specifically the comparison with other state-of-the-art methods.
* The authors mention that the model can potentially play a very different strategy from the one it has observed, but this could be explored further in future work.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* Can the authors provide more details on the RRNN architecture and how it is implemented?
* How do the authors plan to address the instability during training, which is mentioned as a potential problem?
* Can the authors provide more insights into the transfer learning that occurs when learning multiple tasks simultaneously?