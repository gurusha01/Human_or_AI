This paper proposes a novel approach to memory networks, called Hierarchical Memory Networks (HMNs), which leverages Maximum Inner Product Search (MIPS) to efficiently access large memories. The authors claim that HMNs can scale to larger memories than traditional soft attention-based memory networks, while also being easier to train than hard attention-based models.
I decide to accept this paper, with the main reason being that it presents a well-motivated and novel approach to addressing the scalability issue of memory networks. The authors provide a clear and detailed explanation of their proposed method, including the use of hierarchical memory structures and approximate MIPS techniques.
The paper supports its claims through a series of experiments on the SimpleQuestions dataset, which demonstrate the effectiveness of HMNs in improving accuracy and reducing computational cost. The authors also provide a thorough analysis of the trade-offs between different approximate MIPS techniques and their impact on performance.
One potential limitation of the paper is that the experiments are limited to a single dataset, and it would be beneficial to see results on other datasets to further demonstrate the generalizability of the approach. Additionally, the authors could provide more details on the computational cost of their method, particularly in terms of training time and memory usage.
To improve the paper, I suggest that the authors consider the following:
* Provide more details on the implementation of the hierarchical memory structure and the approximate MIPS techniques used.
* Conduct experiments on additional datasets to demonstrate the robustness of the approach.
* Analyze the computational cost of the method in more detail, including training time and memory usage.
* Consider exploring other applications of HMNs, such as question answering or dialogue systems.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How do the authors plan to address the issue of dynamic memory updates during training, which could potentially reduce the approximation bias of the MIPS algorithm?
* Can the authors provide more details on the choice of hyperparameters for the approximate MIPS techniques, and how they were tuned?
* How do the authors plan to extend their approach to other types of memory networks, such as Neural Turing Machines or Dynamic Memory Networks?