This paper proposes a novel approach to detect adversarial perturbations in deep neural networks by training a small "detector" subnetwork to distinguish between genuine data and data containing adversarial perturbations. The authors demonstrate that their method can detect adversarial examples with high accuracy, even when the perturbations are quasi-imperceptible to humans.
I decide to accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and important problem in the field of deep learning, and (2) the approach is well-motivated and supported by empirical results.
The paper provides a clear and thorough introduction to the problem of adversarial examples, and the proposed solution is well-explained and easy to follow. The authors also provide a comprehensive review of related work, which helps to situate their contribution within the broader context of the field. The experimental results are convincing, and the authors demonstrate that their method can detect adversarial examples with high accuracy on several datasets, including CIFAR10 and ImageNet.
One potential limitation of the paper is that the detector subnetwork is trained to detect specific types of adversarial perturbations, and it is unclear how well the method would generalize to other types of attacks. However, the authors acknowledge this limitation and propose a novel training procedure for the detector that can counteract dynamic adversaries.
To improve the paper, I would suggest that the authors provide more details on the architecture and training procedure of the detector subnetwork, as well as more analysis on the robustness of the method to different types of attacks. Additionally, it would be interesting to see more discussion on the potential applications of this method in safety- and security-critical systems.
Some questions I would like the authors to answer include: (1) How does the performance of the detector subnetwork vary with the size and complexity of the classification network? (2) Can the detector subnetwork be used to detect other types of attacks, such as data poisoning or model inversion attacks? (3) How does the proposed method compare to other approaches for detecting adversarial examples, such as input validation or robust optimization techniques? 
Overall, I believe that this paper makes a significant contribution to the field of deep learning, and I look forward to seeing further developments and applications of this work.