This paper presents a comprehensive study on the impact of different action parameterizations on the learning difficulty and performance of deep reinforcement learning (DeepRL) policies for motion control tasks. The authors compare four action parameterizations: torques, muscle activations, target joint angles, and target joint velocities, and evaluate their effects on learning time, policy robustness, motion quality, and policy query rates.
The paper claims to contribute to the field by introducing a DeepRL framework for motion imitation tasks, evaluating the impact of different actuation models on learned control policies, and proposing an optimization approach that combines policy learning and actuator optimization. The authors demonstrate that action parameterizations that incorporate local feedback, such as target joint angles and muscle activations, can improve policy performance and learning speed across different motions and character morphologies.
I decide to accept this paper with minor revisions. The key reasons for this choice are:
1. The paper tackles a specific and well-defined problem in the field of DeepRL, and provides a thorough evaluation of different action parameterizations.
2. The approach is well-motivated, and the authors provide a clear explanation of the background and related work in the field.
3. The paper supports its claims with extensive experimental results, including learning curves, policy performance, and robustness evaluations.
However, I have some minor suggestions for improvement:
* The paper could benefit from a more detailed discussion of the limitations of the current approach, such as the assumption of planar articulated figures and the use of simplified actuation models.
* The authors could provide more insight into the optimization process for the actuator parameters, and discuss the potential for improving the optimization technique.
* Some of the figures and tables could be improved for clarity and readability.
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the implementation of the MTU actuator optimization technique, and discuss the potential for improving the optimization process?
* How do you plan to extend the current approach to 3D articulated figures, and what challenges do you anticipate in this extension?
* Can you provide more insight into the choice of reward function, and discuss the potential for using alternative reward functions that may be more robust to different action parameterizations?