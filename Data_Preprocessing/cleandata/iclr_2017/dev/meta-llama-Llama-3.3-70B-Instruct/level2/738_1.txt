This paper proposes a novel approach to induce sparsity in the gradients of Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs) during training, which can lead to significant improvements in energy efficiency and training speed. The authors observe that the activations of the sigmoid-based gates and the tanh-based new cell state in LSTM cells exhibit a skewed distribution, resulting in potential sparsity in the linear gate gradients during backward propagation.
The main claim of the paper is that by applying a simple yet effective thresholding technique to the linear gate gradients, the sparsity can be increased to more than 80% without loss of performance. The authors demonstrate the effectiveness of their approach through extensive experiments on various LSTM-based RNN applications, including character-based language models, image captioning, and machine translation.
I decide to accept this paper with minor revisions. The main reasons for this decision are:
1. The paper tackles a specific and important problem in the field of deep learning, namely, improving the energy efficiency and training speed of LSTM-based RNNs.
2. The approach proposed by the authors is well-motivated and supported by theoretical analysis and experimental results.
The supporting arguments for this decision are:
* The paper provides a clear and concise overview of the background and motivation for the research, including a thorough review of prior work on sparsity-centric optimization techniques for neural networks.
* The authors conduct a thorough application characterization study to identify potential sparsity in LSTM-based RNNs, which provides a solid foundation for their proposed approach.
* The experimental results demonstrate the effectiveness of the proposed thresholding technique in inducing sparsity in the linear gate gradients without loss of performance.
To improve the paper, I suggest the following additional feedback:
* The authors could provide more details on the sensitivity of the thresholding technique to different hyperparameters, such as the learning rate and network topology.
* The paper could benefit from a more detailed discussion on the potential limitations and challenges of applying the proposed approach to other types of neural networks or applications.
* The authors may want to consider providing more insights into the theoretical implications of their approach, such as the impact on the convergence of the SGD algorithm.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How do the authors plan to extend their approach to other types of neural networks or applications?
* What are the potential challenges and limitations of applying the proposed thresholding technique to hardware accelerators, such as GPUs or TPUs?
* How do the authors plan to address the potential issue of over-sparsification, where the thresholding technique may remove too many gradients, leading to a loss of performance?