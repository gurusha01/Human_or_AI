This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties, called Under-Appreciated Reward Exploration (UREX). The authors propose a directed exploration strategy that promotes exploration of action sequences that yield rewards larger than what the model expects. The UREX objective is a combination of a mode-seeking objective (standard REINFORCE) and a mean-seeking term, which provides a well-motivated trade-off between exploitation and exploration.
The paper claims to make several contributions, including: (1) introducing a new variant of policy gradient that improves exploration in sparse reward settings, (2) demonstrating significant improvements over baseline methods on a set of algorithmic tasks, and (3) showing that UREX can solve a benchmark multi-digit addition task and generalize to long sequences.
I decide to Accept this paper for several reasons. Firstly, the paper tackles a specific question/problem in RL, namely, improving exploration in sparse reward settings. The approach is well-motivated, and the authors provide a clear explanation of the UREX objective and its relationship to existing methods. Secondly, the paper provides empirical evidence of the effectiveness of UREX on a range of algorithmic tasks, including multi-digit addition and binary search. The results demonstrate that UREX outperforms baseline methods, including REINFORCE with entropy regularization and one-step Q-learning.
The paper also provides additional feedback and suggestions for improvement. One potential limitation of the paper is that the UREX objective is not convex in the policy parameters, which may lead to local optima. The authors may want to consider exploring alternative optimization methods or providing more analysis on the convergence properties of UREX. Additionally, the paper could benefit from more discussion on the relationship between UREX and other exploration strategies, such as intrinsic motivation or curiosity-driven exploration.
Some questions I would like the authors to answer to clarify my understanding of the paper include: (1) How does the UREX objective relate to other exploration strategies, such as entropy regularization or intrinsic motivation? (2) Can the authors provide more analysis on the convergence properties of UREX, including the effect of the temperature parameter Ï„ on the optimization process? (3) How does the UREX method scale to more complex tasks or larger state and action spaces? 
Overall, the paper presents a significant contribution to the field of RL, and the UREX method has the potential to improve exploration in sparse reward settings. With some additional analysis and discussion, the paper could be even stronger.