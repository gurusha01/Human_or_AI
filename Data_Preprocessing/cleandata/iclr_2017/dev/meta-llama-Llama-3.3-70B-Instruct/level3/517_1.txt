Summary
The paper proposes a novel approach to learning distributed representations for pieces of text, introducing the Generative Paragraph Vector (GPV) and its supervised extension, Supervised Generative Paragraph Vector (SGPV). GPV is a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector, allowing for the inference of distributed representations for unseen texts. SGPV incorporates text labels into the model, enabling supervised learning and prediction tasks. The authors demonstrate the effectiveness of their models on five text classification benchmark datasets, outperforming state-of-the-art baselines.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a significant problem in natural language processing, proposing a novel and effective approach to learning distributed text representations; and (2) the experimental results demonstrate the superiority of the proposed models over existing baselines, including complex deep learning models.
Supporting Arguments
The paper is well-motivated, and the authors provide a clear and concise introduction to the problem and related work. The proposed GPV and SGPV models are reasonable and well-explained, with a complete generative process allowing for the inference of distributed representations for unseen texts. The experimental results are thorough and convincing, demonstrating the effectiveness of the proposed models on a range of text classification tasks.
Additional Feedback
To further improve the paper, I suggest the authors consider the following points: (1) provide more detailed analysis of the results, including error analysis and discussion of the limitations of the proposed models; (2) explore the application of the proposed models to other natural language processing tasks, such as language modeling or machine translation; and (3) consider using more advanced optimization techniques, such as stochastic gradient descent with momentum or Adam, to improve the efficiency of the learning process.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the proposed GPV and SGPV models handle out-of-vocabulary words, and what strategies can be employed to mitigate this issue? (2) Can the authors provide more details on the computational complexity of the proposed models, including the time and memory requirements for training and inference? (3) How do the proposed models perform on longer texts, such as documents or articles, and what modifications can be made to improve their performance on such tasks?