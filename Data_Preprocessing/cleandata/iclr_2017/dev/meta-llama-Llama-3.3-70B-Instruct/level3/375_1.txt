This paper proposes a novel approach to the problem of transferring a sample in one domain to an analog sample in another domain, which is a fundamental problem in machine learning and computer vision. The authors introduce the Domain Transfer Network (DTN), a deep neural network architecture that learns to map an input sample from a source domain to a target domain, while preserving the representation of the sample in a given representation function.
The paper claims to contribute a new method for unsupervised domain transfer, which is a significant problem in many applications, including computer vision, natural language processing, and robotics. The authors demonstrate the effectiveness of their approach on several tasks, including transferring images from one domain to another, generating face emojis from photos, and performing style transfer.
I decide to accept this paper with some minor revisions. The main reason for my decision is that the paper presents a well-motivated and well-executed approach to a significant problem in machine learning. The authors provide a clear and concise introduction to the problem, a thorough review of related work, and a detailed description of their method. The experimental results are impressive and demonstrate the effectiveness of the proposed approach.
One of the key strengths of the paper is its ability to handle asymmetric domains, where the source and target domains have different distributions and characteristics. The authors demonstrate that their approach can handle such cases effectively, which is a significant advantage over other methods.
However, I do have some minor concerns and suggestions for improvement. Firstly, the paper could benefit from a more detailed analysis of the limitations of the proposed approach. For example, the authors mention that the method may not work well when the source and target domains are very different, but they do not provide a detailed analysis of this limitation.
Secondly, the paper could benefit from more comparisons with other state-of-the-art methods. While the authors provide some comparisons with other methods, they are limited to a few specific tasks and datasets. A more comprehensive comparison with other methods would help to establish the strengths and weaknesses of the proposed approach.
Finally, I would like to ask the authors to clarify some aspects of their method. For example, how do they choose the hyperparameters of the network, and how do they handle cases where the source and target domains have different sizes and distributions? Additionally, I would like to see more visualizations of the results, particularly for the face emoji generation task, to get a better sense of the quality of the generated images.
Overall, I believe that this paper presents a significant contribution to the field of machine learning and computer vision, and I look forward to seeing the revised version. 
To improve the paper, I suggest the authors address the following questions:
1. Can you provide more details on how you chose the hyperparameters of the network, and how you handled cases where the source and target domains have different sizes and distributions?
2. Can you provide more visualizations of the results, particularly for the face emoji generation task, to get a better sense of the quality of the generated images?
3. Can you provide a more detailed analysis of the limitations of the proposed approach, including cases where the source and target domains are very different?
4. Can you provide more comparisons with other state-of-the-art methods, including a more comprehensive evaluation on multiple tasks and datasets?