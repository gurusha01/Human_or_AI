Summary
The paper proposes a method to reduce the memory footprint of the FastText approach by several orders of magnitude while preserving its classification accuracy through lossy compression techniques. The approach uses Product Quantization to compress embeddings and classifier matrices, and aggressive dictionary pruning to achieve a significant reduction in model size, up to 100-1000 folds. However, the paper does not introduce any novel ideas for text classification, instead adapting existing lossy compression techniques.
Decision
I decide to reject this paper, with the main reason being that it lacks novelty in its approach. The techniques used, such as Product Quantization and dictionary pruning, are generic and could be applied to other works. Additionally, the paper has some minor issues, including a lack of clarity on how the full model size is computed and some unclear explanations of certain methods.
Supporting Arguments
The paper is well-written, with clear goals and well-conducted experiments. However, the approach is not well-motivated, as it does not introduce any new ideas or techniques. The paper also does not provide a clear comparison with other state-of-the-art methods, making it difficult to evaluate its performance. Furthermore, the paper's focus on adapting existing techniques rather than introducing new ideas limits its potential impact.
Additional Feedback
To improve the paper, the authors could provide more clarity on how the full model size is computed and provide more detailed explanations of certain methods, such as the greedy approach to prune the dictionary. Additionally, the authors could provide more comparisons with other state-of-the-art methods to evaluate the performance of their approach. The authors could also consider introducing new ideas or techniques to improve the novelty of their approach.
Questions for the Authors
I would like to ask the authors to clarify how they computed the full model size and to provide more detailed explanations of certain methods, such as the greedy approach to prune the dictionary. I would also like to ask the authors to provide more comparisons with other state-of-the-art methods to evaluate the performance of their approach. Finally, I would like to ask the authors to consider introducing new ideas or techniques to improve the novelty of their approach.