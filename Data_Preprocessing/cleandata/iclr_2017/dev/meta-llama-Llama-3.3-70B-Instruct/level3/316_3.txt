This paper proposes a novel approach to recurrent neural networks (RNNs) called Higher Order RNNs (HORNNs), which aims to better model long-term dependency in sequential data. The authors introduce a new recurrent structure that uses multiple memory units to keep track of preceding RNN states, allowing the model to capture longer-term dependencies. The paper is well-written and provides a thorough discussion of related work.
I decide to reject this paper, with the main reason being the lack of theoretical guarantees about the learning performance of HORNNs. While the authors provide empirical results showing the effectiveness of HORNNs on language modeling tasks, they do not provide any theoretical analysis of the model's capabilities. Additionally, the approach may not be generalizable to other architectures, and future experimentation with different architectures is encouraged to validate the results.
The paper's contributions are significant, as it advances the state of the art on differentially-private deep learning. However, the reported epsilons are not privately releasable, and a technique to resolve this issue is needed for meaningful comparison with related work. Furthermore, the approach may not work on other natural data types, and experiments on other datasets are strongly encouraged to test its generalizability.
To improve the paper, I suggest that the authors provide more clarification on certain sections, including the last paragraph of 3.1, 4.1, and the discussion of figure 3. Additionally, the related work discussion is thorough, but lacks a survey of differentially-private semi-supervised learning, which should be added for completeness. The comparison of approaches in Section C should ensure an apples-to-apples comparison with the GAN results for quantitative accuracy.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) Can you provide more details on the theoretical guarantees of HORNNs, and how they compare to other RNN architectures? (2) How do you plan to address the issue of privately releasable epsilons, and what techniques do you propose to use? (3) Can you provide more experiments on other datasets and architectures to demonstrate the generalizability of HORNNs?