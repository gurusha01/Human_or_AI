Summary
The paper proposes a model-based approach to deep reinforcement learning, called Predictive Reinforcement Learning (PRL), which learns a predictive model of the environment that is independent of the strategy. The approach uses a recurrent neural network architecture based on residual nets, which decouples memory from computation, allowing for more complex environments to be modeled. The paper demonstrates the effectiveness of PRL in learning multiple tasks simultaneously, including three ATARI games, and shows that it can benefit from transfer learning.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's approach, although novel, is not well-motivated in the context of existing literature on reinforcement learning. The authors do not provide a clear explanation of how their approach addresses the limitations of existing methods, such as Q-learning. Secondly, the paper's experimental results, although promising, are not comprehensive enough to demonstrate the superiority of the proposed approach over existing methods.
Supporting Arguments
The paper's approach is not well-motivated because it does not provide a clear explanation of how it addresses the limitations of existing methods. For example, the authors do not discuss how their approach handles the problem of exploration-exploitation trade-off, which is a key challenge in reinforcement learning. Additionally, the paper's experimental results are not comprehensive enough because they do not include comparisons with other state-of-the-art methods, such as Deep Q-Networks (DQN) or Policy Gradient Methods (PGMs).
Additional Feedback
To improve the paper, the authors should provide a more detailed discussion of the limitations of existing methods and how their approach addresses these limitations. Additionally, the authors should include more comprehensive experimental results, including comparisons with other state-of-the-art methods. The authors should also consider using more advanced techniques, such as annealed importance sampling, to improve the efficiency of their approach.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How does the proposed approach handle the problem of exploration-exploitation trade-off?
* How does the authors' approach compare to existing methods, such as DQN or PGMs, in terms of performance and efficiency?
* Can the authors provide more details on the implementation of their approach, including the architecture of the recurrent neural network and the training procedure?