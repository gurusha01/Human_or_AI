Summary
The paper introduces a new approach to analyzing trained policies in deep reinforcement learning using the Semi-Aggregated Markov Decision Process (SAMDP) model. The authors propose a framework that combines spatial and temporal abstractions to simplify the analysis of complex policies. They demonstrate the effectiveness of their approach on various domains, including Atari games, and show that it can be used to monitor and improve the performance of trained policies.
Decision
I decide to accept this paper with the condition that the authors address some of the concerns and suggestions raised in this review. The main reasons for this decision are: (1) the paper introduces a novel and well-motivated approach to analyzing trained policies, and (2) the experimental results demonstrate the effectiveness of the proposed framework.
Supporting Arguments
The paper is well-written, and the authors provide a clear and concise introduction to the SAMDP model and its components. The experimental results are thorough and demonstrate the potential of the proposed approach. The authors also provide a detailed discussion of the limitations and potential extensions of their work.
Additional Feedback
To further improve the paper, I suggest that the authors: (1) provide more comparisons with other recent work on analyzing trained policies, (2) discuss the potential applications of the SAMDP model beyond reinforcement learning, and (3) investigate the consistency of the SAMDP model across different runs and hyperparameters.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the authors plan to address the issue of consistency in re-building the SAMDP model, and (2) Can the authors provide more details on the computational complexity of the SAMDP model and its scalability to larger domains?