Summary
The paper proposes a novel approach to modeling sensory neurons using a multitask recurrent neural network (RNN) framework. The authors demonstrate that this approach outperforms traditional generalized linear models (GLMs) in predicting the spiking responses of primate retinal ganglion cells to natural images. The RNN framework is shown to capture complex temporal and spatial nonlinearities, and the authors investigate the benefits of sharing information across neurons in a multitask framework.
Decision
I decide to reject this paper, primarily due to concerns about the computational cost and scalability of the method. The paper reports that training the RNN model takes 8-10 days on 10 GPUs, which is prohibitively long for most applications, especially for relatively small datasets.
Supporting Arguments
While the paper presents promising results, the computational cost of the method is a major concern. The authors do not provide a clear justification for the choice of 10 GPUs or the training time, and it is unclear whether the model can be trained on a single GPU or with a smaller number of GPUs. Furthermore, the paper does not demonstrate the scalability of the method to larger images or more complex datasets, which is a crucial aspect of any machine learning model.
Additional Feedback
To improve the paper, I suggest that the authors conduct an experiment on the Caltech-101 dataset to demonstrate the method's ability to discover a competitive architecture in a low-data regime. This would help to alleviate concerns about the scalability of the method and provide a more comprehensive evaluation of its performance. Additionally, I recommend mentioning ResNets in the table for completeness, as they are a relevant baseline for image classification tasks.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can the RNN model be trained on a single GPU or with a smaller number of GPUs, and if so, what is the expected training time?
2. How does the method perform on larger images or more complex datasets, such as ImageNet or CIFAR-10?
3. Can the authors provide more details on the experimental setup and the choice of hyperparameters, such as the number of layers, units, and training epochs?