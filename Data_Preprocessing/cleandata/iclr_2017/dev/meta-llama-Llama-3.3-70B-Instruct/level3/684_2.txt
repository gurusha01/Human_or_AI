Summary
The paper presents a model-based approach to deep reinforcement learning, called Predictive Reinforcement Learning (PRL), which aims to learn multiple tasks simultaneously. The authors propose a new kind of recurrent neural network, Residual Recurrent Neural Network (RRNN), and demonstrate its effectiveness in learning predictive models that can generalize across different tasks. The approach is evaluated on three ATARI games, and the results show that PRL can surpass human performance and benefit from learning multiple tasks.
Decision
I decide to reject this paper, primarily due to the lack of clarity in the use of reinforcement learning terms and the ambiguity of the term "strategy". The paper's use of informal language and misuse of common reinforcement learning terms contributes to its overall vagueness, making it difficult to understand the approach and its contributions.
Supporting Arguments
The paper lacks a clear definition of the term "strategy", which is crucial in understanding the approach. The notation "r" is also unclear, and it is not specified whether it represents the discounted return or reward at time t. Furthermore, the paper does not provide a clear comparison with existing methods, such as TD learning, which makes it difficult to evaluate the approach's effectiveness. The experiments are also small-scale, with learning curves only shown for 19 iterations, and the results are not comparable to previous works.
Additional Feedback
To improve the paper, I suggest that the authors provide a clear definition of the term "strategy" and clarify the notation "r". Additionally, the authors should compare their approach with existing methods, such as TD learning and Q-learning, to provide a better understanding of its effectiveness. The authors should also consider providing more detailed experiments, including larger-scale evaluations and comparisons with state-of-the-art methods.
Questions for the Authors
1. Can you provide a clear definition of the term "strategy" and how it is used in the context of PRL?
2. How does the RRNN architecture differ from existing recurrent neural network architectures, and what are its advantages?
3. Can you provide more detailed experiments, including larger-scale evaluations and comparisons with state-of-the-art methods, to demonstrate the effectiveness of PRL?