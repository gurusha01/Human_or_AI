Summary
The paper proposes a novel model-based approach to deep reinforcement learning, called Predictive Reinforcement Learning (PRL), which learns a predictive model of the environment and decouples it from the strategy. The approach is evaluated on three ATARI games, demonstrating its ability to learn multiple tasks simultaneously without degradation in performance. The paper also introduces a new type of recurrent neural network, Residual Recurrent Neural Network (RRNN), which is well-suited for this task.
Decision
I decide to accept this paper, with the main reason being that it presents a well-motivated and novel approach to deep reinforcement learning, which has the potential to open new lines of research. The paper is well-written, and the experiments demonstrate the effectiveness of the approach.
Supporting Arguments
The paper tackles a specific question/problem in reinforcement learning, which is learning multiple tasks simultaneously. The approach is well-motivated, and the authors provide a clear explanation of the limitations of existing methods, such as Q-learning. The use of a model-based approach and the introduction of the RRNN are notable contributions. The experiments are well-designed, and the results demonstrate the potential of the approach.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the training process, such as the learning schedule and the weights assigned to each iteration. Additionally, it would be interesting to see a comparison to other multi-task learning approaches in reinforcement learning. The authors should also address the potential problem of the model "forgetting" which actions lead to a death, as mentioned in the discussion section.
Questions for the Authors
I would like the authors to clarify the following points:
* How did you choose the hyperparameters for the RRNN, and what is the effect of changing these hyperparameters on the performance of the model?
* Can you provide more details on the instability during training, and how you addressed this issue?
* How do you plan to extend this approach to more complex environments, such as those with long-term dependencies?