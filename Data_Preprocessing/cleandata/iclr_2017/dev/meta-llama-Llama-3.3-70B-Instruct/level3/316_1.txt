This paper addresses the problem of achieving differential privacy in machine learning applications where teachers are trained on disjoint subsets of sensitive data and a student model performs prediction based on public data labeled by teachers. The authors propose a plausible approach, Private Aggregation of Teacher Ensembles (PATE), which combines multiple models trained with disjoint datasets in a black-box fashion. The student model learns to predict an output chosen by noisy voting among all teachers, ensuring that no single teacher dictates the student's training, thus protecting the privacy of the training data.
I decide to accept this paper for several reasons. Firstly, the paper tackles a specific and relevant problem in the field of machine learning and differential privacy. The approach is well-motivated, building upon existing literature on knowledge aggregation and transfer, and is clearly explained. The use of a simple yet effective idea of adding perturbation error to the counts is a key strength of the paper. Additionally, the paper provides a thorough evaluation of the approach on MNIST and SVHN datasets, demonstrating state-of-the-art privacy/utility trade-offs.
The paper supports its claims through a combination of theoretical analysis and empirical evaluation. The authors provide a detailed privacy analysis, using the moments accountant technique to bound the privacy cost of the approach. The empirical evaluation demonstrates the effectiveness of the approach in achieving strong privacy guarantees while maintaining high accuracy.
To further improve the paper, I suggest that the authors provide more discussion on the bound and its tightness from Theorem 1. This would help to strengthen the approach and provide a more comprehensive understanding of the privacy guarantees. Additionally, the authors may consider exploring other techniques for reducing the student's dependence on its teachers, such as active learning or transfer learning.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the number of teachers affect the privacy cost, and what is the optimal number of teachers for a given dataset? How does the approach perform on other datasets, such as those with different characteristics or sizes? Are there any potential limitations or challenges in applying the PATE approach to real-world machine learning applications?