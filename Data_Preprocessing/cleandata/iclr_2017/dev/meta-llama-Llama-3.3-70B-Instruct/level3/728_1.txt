Summary of the Paper's Contributions
The paper proposes a novel approach to state and temporal representation learning in Reinforcement Learning (RL), called Semi-Aggregated Markov Decision Process (SAMDP). The authors aim to develop methods that provide insight into what is being learned and enable potential human intervention. The SAMDP model combines the advantages of Semi-Markov Decision Process (SMDP) and Aggregated MDP (AMDP) approaches, allowing for analysis with spatiotemporal abstractions. The paper demonstrates the effectiveness of the SAMDP model in a basic gridworld problem and challenging Atari2600 games solved using Deep Q-Networks (DQNs).
Decision and Key Reasons
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks formal definitions and descriptions of algorithms, including AMDP and SAMDP, and their construction, which would improve clarity and understanding. Secondly, the significance and general ease of use of the proposed approach are unclear due to its complex and specific components.
Supporting Arguments
The paper's approach to achieving interpretability involves restrictive abstraction methods, such as skills starting and ending in single states and using clustering to form higher-level states. While the experiments are well-conducted, with positive results, the paper requires improvements in presentation, including explanations for specific design choices and corrections of errors, such as typos and incorrect notation. Additionally, the paper's evaluation criteria, such as Value Mean Square Error (VMSE) and Inertia, are not thoroughly explained, making it difficult to understand the results.
Additional Feedback
To improve the paper, the authors should provide formal definitions and descriptions of algorithms, including AMDP and SAMDP, and their construction. They should also clarify the significance and general ease of use of the proposed approach, including its potential applications and limitations. Furthermore, the authors should consider providing more detailed explanations of their evaluation criteria and results, as well as addressing potential issues with the paper's presentation, such as typos and incorrect notation.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide a more detailed explanation of the SAMDP model's construction, including the feature selection, state aggregation, skill identification, inference, and model selection stages?
2. How do you plan to address the issue of consistency in re-building an SAMDP, given the randomness in the creation of the t-SNE map and the clustering phase?
3. Can you provide more information on the potential applications and limitations of the SAMDP approach, including its potential use in continuous-action policies and its scalability to more complex problems?