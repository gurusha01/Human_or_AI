Summary
The paper proposes a novel algorithm, Generative Paragraph Vector (GPV), which is a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector (PV-DBOW) with a complete generative process. This allows GPV to infer distributed representations for unseen texts, making it a powerful tool for text classification tasks. The authors also introduce a supervised version of GPV, called Supervised Generative Paragraph Vector (SGPV), which incorporates labels into the model to guide representation learning. The experiments demonstrate that both GPV and SGPV outperform state-of-the-art baselines on several text classification tasks.
Decision
I decide to Accept this paper with minor revisions. The key reasons for this choice are: (1) the paper tackles a specific and important problem in natural language processing, namely learning distributed representations for texts, and (2) the proposed approach is well-motivated and supported by empirical results.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem and the proposed approach. The authors motivate the need for a generative model that can infer representations for unseen texts and demonstrate the effectiveness of their approach through experiments on several benchmark datasets. The results show that GPV and SGPV outperform state-of-the-art baselines, including complex deep models, while being simpler and more efficient to train.
Additional Feedback
To improve the paper, I suggest the authors provide more details on the network architectures, such as the definition and dimension of the paragraph vector and word vector spaces. Additionally, the authors could provide more insights into the hyperparameter tuning process and the sensitivity of the model to different hyperparameters. Finally, the authors could consider providing more qualitative analysis of the learned representations and their interpretability.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the prior distribution used for the paragraph vectors and how it is chosen?
2. How do you handle out-of-vocabulary words in the test set, and what is the impact on the model's performance?
3. Can you provide more insights into the computational cost of training GPV and SGPV compared to other deep models?