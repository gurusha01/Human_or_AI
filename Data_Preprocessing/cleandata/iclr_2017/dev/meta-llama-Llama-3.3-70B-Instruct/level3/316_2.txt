This paper proposes a novel approach to guarantee privacy for training data by using multiple "teacher" models to train a "student" model through noisy voting. The authors demonstrate that their approach, called Private Aggregation of Teacher Ensembles (PATE), provides strong privacy guarantees for training data while maintaining high utility.
The paper tackles the specific question of protecting sensitive training data from being memorized by machine learning models. The approach is well-motivated, as it builds upon existing techniques of knowledge aggregation and transfer, and is placed within the context of differential privacy, a well-established standard for privacy guarantees.
The paper supports its claims through a combination of theoretical analysis and empirical experiments. The authors provide a rigorous analysis of the privacy guarantees of their approach, using the moments accountant technique to bound the privacy loss. They also conduct experiments on the MNIST and SVHN datasets, demonstrating that their approach achieves state-of-the-art privacy/utility trade-offs.
Based on the provided guidelines, I decide to Accept this paper. The two key reasons for this choice are:
1. The paper proposes a novel and well-motivated approach to protecting sensitive training data, which is a critical problem in machine learning.
2. The paper provides a rigorous analysis of the privacy guarantees of the approach, using established techniques from differential privacy.
To further improve the paper, I provide the following feedback:
* The authors could consider additional experiments on sensitive data applications, such as medical histories, to demonstrate the approach's usefulness in real-world scenarios.
* The authors could provide more details on the computational cost of their approach, particularly in terms of the number of teacher models required to achieve strong privacy guarantees.
* The authors could explore the application of their approach to other machine learning tasks, such as natural language processing or recommender systems.
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can the authors provide more intuition on why the noisy voting mechanism is effective in protecting sensitive training data?
* How do the authors plan to extend their approach to more complex machine learning models, such as deep neural networks or recurrent neural networks?
* Can the authors provide more details on the trade-offs between the number of teacher models, the noise level, and the privacy guarantees of the approach?