Summary
The paper proposes a novel recurrent neural network (RNN) architecture, called Input Switched Affine Network (ISAN), which achieves state-of-the-art performance on a character-level language modeling task while being more interpretable and computationally efficient than traditional RNNs. The ISAN model uses affine transformations, without nonlinearity, to update the hidden state, allowing for easier analysis and understanding of the network's behavior.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks evaluation on standard datasets and comparison to previous works, limiting its validity and generalizability. Secondly, the results show only mild improvement compared to the oracle improvement, lacking significance and insight into incorrect decisions.
Supporting Arguments
The paper presents an interesting dataset and application, and the ISAN model is well-motivated and well-placed in the literature. However, the study fails to explore intermediate architectures, such as feature fusion and class probabilities, and does not report feature fusion results. Additionally, the network's policy for CP-1 and its performance compared to max or mean pooling given two input class probabilities are unclear and require further explanation.
Additional Feedback
To improve the paper, I suggest that the authors evaluate their model on standard datasets and compare it to previous works to demonstrate its validity and generalizability. Additionally, they should provide more insight into the incorrect decisions made by the model and explore intermediate architectures to further improve its performance. The authors should also clarify the network's policy for CP-1 and its performance compared to max or mean pooling.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence:
1. How do the authors plan to address the lack of evaluation on standard datasets and comparison to previous works?
2. Can the authors provide more insight into the incorrect decisions made by the model and how they plan to improve its performance?
3. How do the authors plan to explore intermediate architectures, such as feature fusion and class probabilities, to further improve the model's performance?