Summary of the Paper's Contributions
The paper proposes a novel neural network structure, called Higher Order Recurrent Neural Networks (HORNNs), which extends the traditional Recurrent Neural Network (RNN) architecture to better model long-term dependencies in sequential data. The authors introduce multiple memory units to keep track of preceding RNN states, which are fed back to the hidden layer through different weighted paths. This structure is analogous to digital filters in signal processing. The paper also explores various pooling functions to calibrate the signals from different feedback paths, including max-based pooling, FOFE-based pooling, and gated pooling. The authors evaluate HORNNs on language modeling tasks using two popular datasets, Penn Treebank (PTB) and English text8, and report state-of-the-art performance.
Decision and Key Reasons
Based on the review, I decide to Reject the paper, with two key reasons:
1. Lack of clarity and presentation: The paper's writeup and overall presentation need significant improvement for better clarity and impact. The text is dense and difficult to follow, with many sections feeling like a laundry list of related work rather than a cohesive narrative.
2. Insufficient experimentation and analysis: While the authors report promising results on language modeling tasks, the experimentation and analysis are limited, and additional experiments are necessary to fully understand the connections made in the paper and why they are helpful.
Supporting Arguments
The paper's concept of combining signal processing with recurrent networks is interesting and potentially useful. However, the presentation and clarity of the paper hinder its impact. The authors could benefit from reorganizing the text, using clearer headings and sectioning, and providing more concise and focused explanations of their contributions. Additionally, the experimentation and analysis are limited to two datasets, and the authors could explore more tasks, datasets, and comparisons to other state-of-the-art models to strengthen their claims.
Additional Feedback and Questions
To improve the paper, I suggest the authors:
* Reorganize the text for better clarity and flow
* Provide more concise and focused explanations of their contributions
* Explore more tasks, datasets, and comparisons to other state-of-the-art models
* Analyze the computational complexity and training time of HORNNs in more detail
* Investigate the effect of different pooling functions and hyperparameters on the performance of HORNNs
Some questions I would like the authors to answer to clarify my understanding of the paper and provide additional evidence include:
* How do the authors plan to address the vanishing gradient problem in HORNNs, and what are the implications of using higher-order structures on the learning process?
* Can the authors provide more insights into the choice of pooling functions and their impact on the performance of HORNNs?
* How do the authors plan to extend HORNNs to other sequential modeling tasks, such as speech recognition and sequence-to-sequence modeling?