The paper proposes a novel neural network structure, called Higher Order Recurrent Neural Networks (HORNNs), to better model long-term dependency in sequential data. The authors extend the standard RNN structure by using multiple memory units to keep track of more preceding RNN states, which are fed to the hidden layer through different weighted paths. The paper also explores various pooling functions to calibrate the signals from different feedback paths. The authors evaluate their proposed HORNNs on two popular language modeling tasks, namely the Penn Treebank (PTB) and English text8 datasets, and report state-of-the-art performance.
I decide to reject this paper, primarily due to two key reasons. Firstly, the experiments are limited to NLP and prediction tasks, lacking exploration of other domains and sensory input data such as audio or video. This limited scope raises concerns about the generalizability of the proposed HORNNs to other sequential modeling tasks. Secondly, the comparisons to other models, particularly LSTMs, are questionable, as the rapidly changing state-of-the-art in NLP makes it challenging to ensure fair comparisons.
To support my decision, I argue that the paper's claim to improve long-term prediction lacks a corresponding analysis to support this claim. The authors provide some theoretical insights into how HORNNs can capture long-term dependency, but more empirical evidence is needed to substantiate this claim. Additionally, the paper's criticism of LSTM training speed and scalability is disputed, as LSTMs are widely used in production and have been shown to be scalable in many applications.
To improve the paper, I suggest that the authors conduct more extensive experiments on various sequential modeling tasks, including those involving audio and video data. They should also provide a more detailed analysis of the benefits and limitations of HORNNs compared to other RNN variants, such as LSTMs and GRUs. Furthermore, the authors should consider using more advanced evaluation metrics, such as metrics that assess the model's ability to capture long-term dependencies, to provide a more comprehensive understanding of the proposed HORNNs.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims: (1) How do the authors plan to extend their proposed HORNNs to other sequential modeling tasks, such as speech recognition and sequence-to-sequence modeling? (2) Can the authors provide more detailed comparisons between HORNNs and other RNN variants, including LSTMs and GRUs, in terms of training speed, scalability, and performance on various tasks? (3) How do the authors plan to address the potential limitations of HORNNs, such as the increased computational complexity and the need for careful hyperparameter tuning?