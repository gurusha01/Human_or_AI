Summary of the Paper's Contributions
The authors propose a simple Recurrent Neural Network (RNN) with linear dynamics, called the Input Switched Affine Network (ISAN), for language modeling. This architecture achieves near-identical performance to traditional nonlinear RNNs on a character-level language modeling task, while offering improved interpretability and potential computational speedup. The paper presents well-executed techniques for analyzing the model's behavior, allowing for a deeper understanding of how the network makes predictions.
Decision: Accept with Reservations
I decide to accept this paper, but with some reservations. The main reason for this decision is that the paper presents a novel and interesting approach to language modeling, and the analysis techniques used to understand the model's behavior are well-executed and insightful. However, I have some concerns about the quantitative comparison to other models, which is underwhelming, and the lack of discussion about related sequence models, such as LSTMs and GRUs.
Supporting Arguments
The paper's approach to designing an interpretable RNN is well-motivated, and the use of linear dynamics allows for a more straightforward analysis of the model's behavior. The authors demonstrate that the ISAN can achieve competitive performance to nonlinear RNNs on a character-level language modeling task, which is a significant result. Additionally, the paper presents several interesting analysis techniques, such as the decomposition of current predictions into contributions from previous time steps, which provides valuable insights into how the network makes predictions.
Additional Feedback
To improve the paper, I suggest that the authors shorten the analysis experiments and increase the discussion of related sequence models, such as LSTMs and GRUs. It would also be beneficial to compare the ISAN to non-linear methods on specific tasks, such as nested parentheses, and discuss related work like Belanger and Kakade (2015). Furthermore, the authors could explore the possibilities of representing each matrix as a sparse/convex combination of dictionary matrices, which could provide low-dimensional weights for character representation.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence for my assessment, I would like the authors to answer the following questions:
1. Can you provide more details about the computational benefits of the ISAN, particularly in terms of memory and computational efficiency?
2. How do you plan to scale the ISAN to larger vocabularies and continuous-valued inputs?
3. Can you provide more comparisons to other related models, such as LSTMs and GRUs, on a variety of tasks, including nested parentheses and language modeling tasks with larger vocabularies?