Summary
The paper proposes a new method for learning distributed representations of texts, called Generative Paragraph Vector (GPV) and its supervised extension, Supervised Generative Paragraph Vector (SGPV). GPV is a probabilistic extension of the Paragraph Vector model, which allows for inferring distributed representations of unseen texts. SGPV incorporates labels into the model, enabling it to guide representation learning and predict labels for new texts. The authors evaluate their models on five text classification benchmark datasets and demonstrate superior performance over state-of-the-art counterparts.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks clarity in its notation and algorithms, making it difficult to understand the proposed method. For example, the distinction between Σ and Σi in Algorithm 1 is unclear, and the input data X is not clearly used in the algorithm. Secondly, the paper does not provide sufficient evidence to support its claims, particularly with regards to the hyper-parameter tuning in the sparse prior P(G).
Supporting Arguments
The paper proposes a new method for learning graphical models using a neural network architecture and sparse edge structure estimation via sampling methods. However, the method still implicitly includes model selection, and it is unclear how the hyper-parameters in the sparse prior P(G) are tuned. This lack of clarity and transparency in the methodology makes it challenging to evaluate the effectiveness of the proposed approach. Furthermore, the paper's unorganization and unclear notation, such as the distinction between Σ and Σi in Algorithm 1, raise questions about the validity of the results.
Additional Feedback
To improve the paper, I suggest that the authors provide a clearer explanation of their methodology, including the notation and algorithms used. Additionally, they should provide more evidence to support their claims, such as hyper-parameter tuning and model selection. It would also be helpful to include more details about the experimental setup and results, such as the specific datasets used and the performance metrics employed.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide a clearer explanation of the distinction between Σ and Σi in Algorithm 1?
2. How do you tune the hyper-parameters in the sparse prior P(G)?
3. Can you provide more details about the experimental setup and results, such as the specific datasets used and the performance metrics employed?
4. How do you handle the input data X in Algorithm 1, and what is its role in the proposed method?
5. Can you provide a clearer definition of the receptive field in Proposition 2 and Proposition 3?