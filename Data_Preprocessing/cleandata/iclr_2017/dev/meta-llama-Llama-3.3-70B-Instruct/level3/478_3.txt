Summary of the Paper's Contributions
The paper proposes a novel approach to providing strong privacy guarantees for training data in machine learning applications, called Private Aggregation of Teacher Ensembles (PATE). The approach combines multiple models trained on disjoint datasets, using a black-box fashion, to produce a student model that learns to predict an output chosen by noisy voting among all the teachers. The paper demonstrates the effectiveness of PATE in achieving state-of-the-art privacy/utility trade-offs on MNIST and SVHN datasets, while also providing a rigorous guarantee of training data privacy.
Decision and Reasons
Based on the evaluation of the paper, I decide to Accept the paper. The two key reasons for this decision are:
1. The paper proposes a novel and well-motivated approach to providing strong privacy guarantees for training data, which is a critical problem in machine learning applications.
2. The paper provides a rigorous analysis of the approach, including a data-dependent privacy analysis, and demonstrates its effectiveness in achieving state-of-the-art privacy/utility trade-offs on benchmark datasets.
Supporting Arguments
The paper's approach is well-motivated, as it addresses a critical problem in machine learning applications, where sensitive training data may be inadvertently stored or revealed by the model. The use of multiple models trained on disjoint datasets, combined with a noisy voting mechanism, provides a strong guarantee of privacy. The paper's analysis, including the use of the moments accountant technique, provides a rigorous bound on the privacy loss, which is essential for establishing the effectiveness of the approach.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the role of each layer in the representation, as well as the impact of the number of teachers on the privacy cost. Additionally, it would be helpful to explore the applicability of the approach to other model structures and datasets with different characteristics.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more insight into the role of each layer in the representation, and how it contributes to the overall privacy guarantee?
2. How does the number of teachers affect the privacy cost, and what is the optimal number of teachers for a given dataset and task?
3. Can you provide more details on the experimental setup, including the specific models and hyperparameters used, and how they were selected?