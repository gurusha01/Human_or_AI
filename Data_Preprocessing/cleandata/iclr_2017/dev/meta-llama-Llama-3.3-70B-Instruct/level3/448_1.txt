This paper presents a novel model-based approach to deep reinforcement learning, which learns a predictive model of the environment that is independent of any specific strategy. The approach is demonstrated to be effective in learning multiple tasks simultaneously, and even benefits from doing so, with results showing no degradation in performance and even improvement in some cases.
I am inclined to accept this paper, with the primary reason being that the approach is well-motivated and well-placed in the literature. The authors provide a clear and thorough discussion of the limitations of existing Q-learning approaches and demonstrate how their model-based approach can address these limitations.
One potential concern I have is with the soundness of Equation 2, which is based on mean-field techniques that I am not familiar with. However, I am willing to accept this equation as it is presented in the paper.
To improve the presentation, I suggest clarifying the "evolution" of x{i;a} to avoid confusion with the immutable input vector x{*;a}. Additionally, the analysis suggests that a network may be trainable even if information does not initially pass through it, as long as the training steps perturb the weights to allow information to flow. This is an interesting insight and could be further explored in future work.
I also question the need for a clear definition of "training algorithm" to support the central claims made in the paper. The authors compare their approach to previous work on initializing neural networks to promote information flow, such as the work by Glorot & Bengio, but a more explicit definition of the training algorithm used in this paper would help to clarify the contributions.
To further improve the paper, I would like the authors to address the following questions: Can you provide more details on how the Perception network is tailored for vision-based inputs? How does the Residual Recurrent Neural Network (RRNN) architecture compare to other recurrent neural network architectures, such as LSTMs? What are the potential applications of the RRNN architecture beyond reinforcement learning?
Overall, I believe that this paper presents a significant contribution to the field of deep reinforcement learning and has the potential to open up new lines of research. With some minor revisions to address the concerns mentioned above, I would be happy to accept this paper.