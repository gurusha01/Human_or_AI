Summary
The paper presents a novel end-to-end speech recognition system that combines a convolutional neural network (CNN) with a graph decoding approach, trained using an automatic segmentation criterion called AutoSegCriterion (ASG). The system is evaluated on the LibriSpeech corpus and achieves competitive results with MFCC features and promising results with power spectrum and raw speech features.
Decision
I decide to Accept this paper with two key reasons: (1) the paper introduces a novel and simple architecture that achieves competitive results on a benchmark dataset, and (2) the authors provide a thorough evaluation of their approach, including comparisons with other state-of-the-art methods.
Supporting Arguments
The paper is well-written and provides a clear explanation of the proposed architecture and the ASG criterion. The authors also provide a detailed analysis of the results, including the impact of training size and data augmentation on the performance of the system. The comparison with other state-of-the-art methods, such as CTC and Deep Speech, is also thorough and provides a good understanding of the strengths and weaknesses of the proposed approach.
Additional Feedback
One area for improvement is the visualization of the results. Figure 2 has colors that are difficult to distinguish, making it hard to interpret. The authors may consider using different colors or visualization techniques to make the figure more readable. Additionally, the authors may consider providing more details on the computational resources required to train and deploy the system, as this is an important consideration for practical applications.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
* Can you provide more details on the ASG criterion and how it differs from other sequence criteria, such as CTC?
* How do you plan to extend the proposed approach to handle more complex speech recognition tasks, such as multi-speaker recognition or speech recognition in noisy environments?
* Can you provide more information on the computational resources required to train and deploy the system, and how it compares to other state-of-the-art methods?