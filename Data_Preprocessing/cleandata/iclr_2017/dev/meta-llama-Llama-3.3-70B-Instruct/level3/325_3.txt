This paper proposes a novel approach to providing strong privacy guarantees for training data in machine learning applications, known as Private Aggregation of Teacher Ensembles (PATE). The authors demonstrate the effectiveness of their approach on several benchmark datasets, including MNIST and SVHN, and show that it can achieve state-of-the-art privacy/utility trade-offs.
The paper tackles the specific question of protecting sensitive training data from being inadvertently stored or revealed by machine learning models. The authors' approach is well-motivated, building on existing work in knowledge aggregation and transfer, and is placed within the context of differential privacy, a well-established standard for privacy guarantees.
The paper supports its claims with empirical evaluations on several datasets, including MNIST and SVHN, and provides a detailed analysis of the privacy guarantees provided by the PATE approach. The authors also compare their approach to existing methods, such as those proposed by Abadi et al. (2016) and Shokri & Shmatikov (2015), and show that their approach can achieve better privacy/utility trade-offs.
However, I have some concerns regarding the paper. Firstly, the proposed approach is interesting but its potential to generate impact is uncertain due to the existence of more theoretically sound and similarly complex approaches that work well in practice. Secondly, the empirical evaluation of the proposed approach is limited and requires more comparisons with other methods to assess its practicality and effectiveness.
To improve the paper, I would suggest that the authors provide more detailed comparisons with other methods, including those that use different privacy-preserving techniques, such as differential privacy and federated learning. Additionally, the authors could provide more analysis on the trade-offs between privacy and utility, and explore the applicability of their approach to other domains and datasets.
Some specific questions I would like the authors to address include:
* How does the number of teachers affect the privacy cost, and what is the optimal number of teachers for a given dataset and task?
* Can the authors provide more detailed analysis on the moments accountant technique, and how it is used to bound the privacy cost?
* How does the PATE approach compare to other privacy-preserving methods, such as differential privacy and federated learning, in terms of privacy/utility trade-offs?
Overall, I believe that the paper has the potential to make a significant contribution to the field of privacy-preserving machine learning, but requires more detailed analysis and comparisons to existing methods to fully demonstrate its effectiveness.
I decide to reject the paper for now, with the reason being that the empirical evaluation is limited and requires more comparisons with other methods to assess its practicality and effectiveness. However, I believe that the paper has the potential to be accepted with revisions, and I would like to see the authors address the concerns and questions I have raised.