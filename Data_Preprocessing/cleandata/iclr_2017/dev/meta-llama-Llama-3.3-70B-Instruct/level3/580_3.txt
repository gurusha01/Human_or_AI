This paper presents a character language model that balances interpretability and predictivity by allowing credit apportionment for predictions among elements of the past history. The proposed Input Switched Affine Network (ISAN) model is well-written, easy to read, and proposes a simple architecture that performs respectably, making it suitable for classroom assignments and easy to implement. The model offers computational benefits, including sparse parameter access and composition of affine updates, which can lead to significant speedups in input processing.
After careful evaluation, I decide to Reject this paper, with two key reasons for this choice. Firstly, while the ISAN model performs well on the Text8 task, its predictive accuracy has caveats, including near-identical performance to other architectures only when comparing the largest models, and significant performance gaps for smaller parameter sizes. Secondly, the model's competitiveness is uncertain beyond the tested toy situations, with unproven performance on larger datasets and word-based language modeling tasks.
To support these reasons, I note that the paper's results on the Text8 task are impressive, but the comparison to other architectures is limited to a specific dataset and task. The paper also lacks a thorough analysis of the model's performance on more complex tasks, such as word-based language modeling or larger datasets. Furthermore, the paper's claims about the computational benefits of the ISAN model, while intriguing, require more rigorous evaluation and comparison to existing methods.
To improve the paper, I suggest that the authors provide additional experiments and analysis on more complex tasks and datasets, as well as a more thorough comparison to existing methods. Additionally, the authors could provide more insight into the interpretability of the ISAN model, including visualizations and analysis of the learned representations.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to scale the ISAN model to larger datasets and more complex tasks, such as word-based language modeling? (2) Can the authors provide more insight into the learned representations and dynamics of the ISAN model, including visualizations and analysis of the learned transition matrices and bias vectors? (3) How do the authors plan to address the performance gaps between the ISAN model and other architectures for smaller parameter sizes?