Summary
The paper proposes an Input Switched Affine Network (ISAN) model for character-level language modeling, which achieves comparable performance to traditional recurrent neural network (RNN) architectures, including GRUs and LSTMs, on the Text8 dataset. The ISAN model uses affine transformations, without nonlinearity, to update the hidden state, allowing for easier interpretation and analysis of the model's behavior. The authors demonstrate the benefits of the ISAN model, including its ability to decompose current predictions into contributions from previous time steps, and its potential for computational efficiency.
Decision
I decide to Accept this paper, with the primary reason being that it makes a significant contribution to the field of neural networks by introducing a novel, interpretable, and efficient architecture for language modeling. The paper is well-motivated, and the approach is well-placed in the literature, building upon existing work on recurrent neural networks and language modeling.
Supporting Arguments
The paper provides a clear and concise introduction to the ISAN model, and the authors demonstrate its effectiveness on a character-level language modeling task. The analysis of the model's behavior, including the decomposition of current predictions into contributions from previous time steps, is thorough and well-presented. The paper also discusses the potential benefits of the ISAN model, including its computational efficiency and interpretability.
Additional Feedback
To further improve the paper, I suggest that the authors provide more concrete metrics to support their conclusions, particularly in Figure 6, where a high-resolution image or actual metrics would help to determine the clarity of the image. Additionally, the authors may want to consider providing more details on the training process, including the hyperparameter tuning infrastructure used, to facilitate reproducibility.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the training process, including the hyperparameter tuning infrastructure used?
2. How do you plan to address the issue of scaling the ISAN model to larger vocabularies or continuous-valued inputs?
3. Can you provide more insights into the computational benefits of the ISAN model, including the potential speedups and memory improvements, and how these benefits can be realized in practice?