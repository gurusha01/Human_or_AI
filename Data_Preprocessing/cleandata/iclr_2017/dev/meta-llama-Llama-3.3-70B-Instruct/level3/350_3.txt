Summary
The paper proposes a novel approach to analyzing trained policies in Reinforcement Learning (RL) by introducing the Semi-Aggregated Markov Decision Process (SAMDP) model. The SAMDP model combines the advantages of temporal and spatial abstractions, allowing for a more concise and interpretable representation of the policy. The authors demonstrate the effectiveness of the SAMDP model on various tasks, including a gridworld problem and Atari 2600 games, and show that it can be used to identify skills and improve policy robustness.
Decision
I decide to accept this paper with two key reasons: (1) the paper proposes a novel and well-motivated approach to analyzing trained policies in RL, and (2) the empirical results demonstrate the effectiveness of the SAMDP model in various tasks.
Supporting Arguments
The paper is well-written and easy to follow, with a clear introduction to the problem and a detailed explanation of the SAMDP model. The authors provide a thorough analysis of the model's strengths and weaknesses and demonstrate its effectiveness in various tasks. The use of spatiotemporal clustering and the introduction of a novel extension of the K-means algorithm are particularly noteworthy. The paper also provides a detailed evaluation of the model's performance, including a comparison with other modeling approaches and an analysis of the model's robustness.
Additional Feedback
To further improve the paper, I suggest that the authors consider addressing the following points: (1) provide more details on the computational complexity of the SAMDP model and its scalability to larger tasks, (2) investigate the use of other clustering algorithms and their impact on the model's performance, and (3) explore the application of the SAMDP model to continuous-action policies and other RL domains.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence for my assessment, I would like the authors to answer the following questions: (1) Can you provide more details on the choice of hyperparameters for the SAMDP model, such as the number of clusters and the window size for the clustering algorithm? (2) How do you plan to address the issue of consistency in re-building the SAMDP model, and what optimization problem do you propose to solve to account for all the performance criteria introduced in the paper? (3) Can you provide more insights into the interpretability of the SAMDP model and how it can be used to provide a more concise and interpretable representation of the policy?