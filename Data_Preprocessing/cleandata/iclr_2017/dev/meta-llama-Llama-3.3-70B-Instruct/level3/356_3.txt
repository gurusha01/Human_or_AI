The paper proposes a novel approach to analyzing trained policies in reinforcement learning by introducing the Semi-Aggregated Markov Decision Process (SAMDP) model. This model combines the strengths of temporal and spatial abstractions, allowing for a more concise and interpretable representation of the policy. The authors demonstrate the effectiveness of SAMDP in various domains, including gridworld and Atari games, and show that it can be used to identify skills and improve policy robustness.
Based on the provided guidelines, I decide to accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper provides a clear and concise exposition of the SAMDP model and its applications.
The approach is well-motivated because it addresses a significant challenge in reinforcement learning, namely, understanding the behavior of trained policies. The authors provide a thorough review of the literature and clearly explain the limitations of existing approaches, such as MDP and SMDP models. The SAMDP model is a natural extension of these approaches, and the authors demonstrate its effectiveness in various domains.
The paper provides a clear and concise exposition of the SAMDP model and its applications. The authors explain the five stages of building an SAMDP model, including feature selection, state aggregation, skill identification, inference, and model selection. They also provide examples of how to apply the SAMDP model in different domains, including gridworld and Atari games.
To improve the paper, I suggest that the authors provide more details on the clustering algorithm used in the state aggregation phase. Specifically, they could elaborate on the modified K-means algorithm and its advantages over other clustering approaches. Additionally, they could provide more information on the hyperparameters used in the experiments and the sensitivity of the results to these hyperparameters.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to address the issue of consistency in re-building an SAMDP model, given the randomness in the creation of the t-SNE map and the clustering phase? (2) Can the authors provide more details on the optimization problem that they plan to lay out in future work to directly account for all of the performance criteria introduced in the paper? (3) How do the authors plan to extend the SAMDP model to continuous-action policies, and what are the potential challenges and limitations of this extension?