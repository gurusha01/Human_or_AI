Summary
The paper presents a novel model-based approach to deep reinforcement learning, which learns a predictive model of the environment and decouples it from the strategy. This approach is shown to be effective in learning multiple tasks simultaneously, without degradation in performance, and even benefits from transfer learning. The paper also introduces a new type of recurrent neural network, Residual Recurrent Neural Network (RRNN), which is well-suited for this task.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper presents a well-motivated and novel approach to deep reinforcement learning, which addresses the limitations of existing Q-learning methods; and (2) the paper provides thorough mathematical analysis, numerical experiments, and empirical results that support the theoretical claims.
Supporting Arguments
The paper provides a clear and well-structured presentation of the approach, including the prediction problem, the model architecture, and the training procedure. The experiments demonstrate the effectiveness of the approach in learning multiple tasks simultaneously, and the results show that the model can surpass human performance in three different games. The introduction of the RRNN is also a significant contribution, as it provides a new tool for modeling complex environments with limited memory requirements.
Additional Feedback
To further improve the paper, I suggest that the authors provide more discussion on the potential applications of this approach beyond Atari games, and explore the possibilities of combining this approach with other reinforcement learning methods. Additionally, it would be helpful to provide more details on the training procedure, such as the learning schedule and the hyperparameter tuning process.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors to provide more information on the following: (1) How do the authors plan to address the issue of long-term dependencies in the predictive model? (2) Can the authors provide more insights on the stability of the training process, and how to alleviate the problem of the model "forgetting" which actions lead to a death? (3) How do the authors envision the application of this approach in real-world environments, such as robotics, and what are the potential challenges and limitations?