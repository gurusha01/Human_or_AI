The paper proposes a novel approach to model-based reinforcement learning, achieving super-human level performance in both individual and multi-task setups on 3 ATARI games. The approach involves training a model to predict rewards and life loss probabilities, and the authors demonstrate its effectiveness through extensive experiments.
I decide to reject this paper, primarily due to two key reasons. Firstly, the combination of reward and life loss probability predictions is ad-hoc and lacks insight into the importance of different system components. Secondly, the paper lacks exploratory and ablation experiments to fully demonstrate the effectiveness and potential of the proposed approach.
The paper's approach to model-based reinforcement learning is well-motivated, and the authors provide a clear explanation of their methodology. However, the lack of insight into the importance of different system components and the ad-hoc combination of predictions raise concerns about the robustness and generalizability of the approach. Furthermore, the absence of exploratory and ablation experiments makes it difficult to fully understand the contributions of each component and the potential limitations of the approach.
To improve the paper, I suggest that the authors provide more detailed analysis and visualization of the learning process, including predictions on multiple action sequences and measurement of learning progress through training loss plots. Additionally, the authors should conduct more extensive experimentation to determine the necessity of the proposed RRNN architecture and its components, such as residual connections. The paper would also benefit from more thorough citations to earlier work on using models in reinforcement learning and minor notation and formatting issues throughout.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims: (1) Can you provide more insight into the importance of different system components and how they contribute to the overall performance of the approach? (2) How do you plan to address the lack of exploratory and ablation experiments to fully demonstrate the effectiveness and potential of the proposed approach? (3) Can you provide more detailed analysis and visualization of the learning process, including predictions on multiple action sequences and measurement of learning progress through training loss plots?