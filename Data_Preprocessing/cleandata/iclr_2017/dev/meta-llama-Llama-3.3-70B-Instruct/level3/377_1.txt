Summary
The paper proposes a novel neural network structure, called Higher Order Recurrent Neural Networks (HORNNs), to better model long-term dependency in sequential data. HORNNs extend the standard RNN structure by using multiple memory units to keep track of more preceding RNN states, which are fed to the hidden layer through different weighted paths. The authors also propose several pooling functions to calibrate the signals from different feedback paths. Experimental results on language modeling tasks using two popular datasets, Penn Treebank (PTB) and English text8, show that HORNNs achieve state-of-the-art performance, outperforming regular RNNs and LSTMs.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks a deeper analysis of the proposed HORNNs, such as dissecting learned representations or discussing links to human behavior or physical properties/laws. Secondly, the paper could be improved with more thorough testing and analysis, including comparisons with other baseline approaches.
Supporting Arguments
The paper explores an interesting idea of using higher order RNNs to model long-term dependency in sequential data. However, the contribution of the paper is unclear, and the results, although promising, do not bring new insights into the field. The experimental setups are interesting, but the tasks are not significantly different from other typical DRL tasks, such as learning from playing games. The paper also lacks a clear explanation of why interaction is necessary to learn physical properties and how the proposed HORNNs address this problem.
Additional Feedback
To improve the paper, I suggest that the authors provide more detailed explanations of the proposed HORNNs, including the intuition behind the design and the role of each component. Additionally, the authors should provide more thorough comparisons with other baseline approaches, including LSTMs and other RNN variants. The authors should also consider providing more analysis of the learned representations and the links to human behavior or physical properties/laws.
Questions
I would like the authors to answer the following questions to clarify my understanding of the paper:
1. Can you provide more intuition behind the design of HORNNs and the role of each component?
2. How do the proposed HORNNs address the problem of learning physical properties, and why is interaction necessary?
3. Can you provide more comparisons with other baseline approaches, including LSTMs and other RNN variants?
4. How do the learned representations in HORNNs relate to human behavior or physical properties/laws?