Summary
The paper proposes a model-based approach to deep reinforcement learning, called Predictive Reinforcement Learning (PRL), which learns a predictive model of the environment and a valuation network to predict the change in score. The authors introduce a new kind of recurrent neural network, Residual Recurrent Neural Network (RRNN), which decouples memory from computation, allowing for more complex environments to be modeled. The approach is tested on three ATARI games, demonstrating that it can learn multiple tasks simultaneously without degradation in performance and even improve on some tasks.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks a clear comparison to state-of-the-art methods in deep reinforcement learning, making it difficult to assess the significance of the results. Secondly, the training process and experimental setup are not thoroughly explained, which raises concerns about the reproducibility of the results.
Supporting Arguments
The paper presents an interesting approach to deep reinforcement learning, but the results are not convincing due to the lack of comparison to other methods. The authors claim that their approach can learn multiple tasks simultaneously, but it is unclear how this compares to other multi-task learning methods. Additionally, the experimental setup is not well-explained, which makes it difficult to understand how the results were obtained. The use of a hard-coded strategy and the lack of exploration of other strategies also limit the scope of the paper.
Additional Feedback
To improve the paper, I suggest that the authors provide a more thorough comparison to state-of-the-art methods in deep reinforcement learning, including Q-learning and other model-based approaches. Additionally, the authors should provide more details about the training process and experimental setup, including the hyperparameters used and the computational resources required. It would also be beneficial to explore other strategies and to provide more analysis of the results, including ablation studies and visualizations of the learned models.
Questions for the Authors
I would like to ask the authors to provide more information about the following:
* How does the proposed approach compare to other model-based reinforcement learning methods, such as Q-learning and model-based RL?
* Can the authors provide more details about the training process, including the hyperparameters used and the computational resources required?
* How do the authors plan to address the instability during training and the potential problem of the model "forgetting" which actions lead to a death?
* Can the authors provide more analysis of the results, including ablation studies and visualizations of the learned models?