Summary
The paper proposes a novel neural network structure, called Higher Order Recurrent Neural Networks (HORNNs), to better model long-term dependency in sequential data. HORNNs extend the standard RNN structure by using multiple memory units to keep track of more preceding RNN states, which are fed to the hidden layer through different weighted paths. The authors also propose several pooling functions to calibrate the signals from different feedback paths. The experimental results show that HORNNs achieve state-of-the-art performance on two popular language modeling tasks, outperforming regular RNNs and LSTMs.
Decision
I decide to accept this paper with some minor revisions. The main reasons for this decision are: (1) the paper tackles a specific and important problem in sequential modeling, and (2) the proposed approach is well-motivated and achieves impressive empirical results.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem of modeling long-term dependency in sequential data. The authors also provide a thorough review of related work, including RNNs, LSTMs, and other variants. The proposed HORNN structure is well-motivated, and the use of pooling functions to calibrate the signals from different feedback paths is a nice touch. The experimental results are impressive, and the authors provide a detailed analysis of the performance of different HORNN variants.
Additional Feedback
To further improve the paper, I would like to see more discussion on the following points: (1) the effect of the order of HORNNs on the performance, (2) the impact of the pooling functions on the learning process, and (3) a more detailed comparison with other state-of-the-art models, such as attention-based models. Additionally, it would be helpful to provide more implementation details, such as the effect of patch size and overlap between sampled patches, to fully understand the method's effectiveness.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend HORNNs to other sequential modeling tasks, such as speech recognition and sequence-to-sequence modeling? (2) Can the authors provide more insights into the learning process of HORNNs, such as the effect of the pooling functions on the gradient flow? (3) How do the authors plan to address the potential issue of overfitting in HORNNs, especially when dealing with larger datasets?