Summary
The paper presents a novel model-based approach to deep reinforcement learning, called Predictive Reinforcement Learning (PRL), which learns a predictive model of the environment and decouples it from the strategy. The authors propose a new kind of recurrent neural network, Residual Recurrent Neural Network (RRNN), which is well-suited for this task. The approach is tested on three ATARI games, and the results show that it can beat human performance simultaneously on all three games, with no degradation in performance compared to individual models trained on each game.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper presents a well-motivated and novel approach to deep reinforcement learning, which addresses the limitations of existing model-free methods; and (2) the experimental results are thorough and demonstrate the effectiveness of the proposed approach.
Supporting Arguments
The paper provides a clear and well-structured presentation of the proposed approach, including a detailed description of the RRNN architecture and the training procedure. The authors also provide a thorough analysis of the results, including a comparison with individual models trained on each game. The paper also discusses the potential limitations of the approach and suggests future directions for improvement.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the sensitivity of the results to different hyperparameter settings. Additionally, it would be interesting to see a comparison with other state-of-the-art model-based reinforcement learning methods.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors to provide more details on the following: (1) How did you select the specific ATARI games used in the experiments, and do you think the results would generalize to other games? (2) Can you provide more insight into the training process, including the number of iterations and the convergence criteria used? (3) How do you plan to address the potential instability during training, and do you think this could be a major limitation of the approach?