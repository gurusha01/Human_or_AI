Summary of the Paper's Contributions
The paper introduces a novel approach to analyzing trained policies in reinforcement learning, called the Semi-Aggregated Markov Decision Process (SAMDP) model. This model combines the benefits of temporal and spatial abstractions, allowing for a more concise and interpretable representation of complex policies. The authors demonstrate the effectiveness of SAMDP on several tasks, including a gridworld problem and Atari 2600 games, and show that it can be used to identify skills and improve policy robustness.
Decision and Key Reasons
Based on the evaluation criteria, I decide to Reject this paper. The two key reasons for this decision are:
1. Limited novelty and incremental contribution: While the SAMDP model is a useful extension of existing work, it does not significantly advance the state of the art in reinforcement learning. The paper builds on previous research in intrinsic motivation and skill discovery, but the contributions, although decent and simple, are only slightly incremental.
2. Lack of rigorous evaluation and comparison: The paper's claims of improved performance and robustness are not fully supported by rigorous numerical measurements and comparisons to existing methods. For example, the authors claim a massive speedup, but the measurements show that VIME is only slower in initialization, not per iteration.
Supporting Arguments
The paper's technical contributions are sound, and the authors provide a clear and well-structured presentation of their work. However, the evaluation section lacks a thorough comparison to existing methods, such as VIME, which is only briefly mentioned. Additionally, the paper could benefit from more detailed analysis and discussion of the results, particularly in terms of the SAMDP model's limitations and potential applications.
Additional Feedback and Questions
To improve the paper, I suggest that the authors:
* Provide a more detailed comparison to existing methods, including VIME, and discuss the advantages and limitations of SAMDP in relation to these methods.
* Clarify the evaluation criteria and metrics used to assess the performance of SAMDP, and provide more detailed analysis of the results.
* Discuss potential applications and extensions of SAMDP, such as its use in continuous-action policies or multi-agent settings.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How does the SAMDP model handle cases where the policy is not nearly deterministic, and what are the implications for the model's performance and interpretability?
* Can the authors provide more insight into the choice of clustering algorithm and its impact on the quality of the SAMDP model?
* How do the authors plan to address the issue of consistency in re-building an SAMDP, and what are the potential implications for the model's uniqueness and robustness?