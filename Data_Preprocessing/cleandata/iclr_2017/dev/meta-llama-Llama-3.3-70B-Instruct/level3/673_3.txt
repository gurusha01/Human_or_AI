This paper proposes a novel technique called Neuro-Symbolic Program Synthesis (NSPS) for automatically constructing computer programs in a domain-specific language (DSL) that are consistent with a set of input-output examples provided at test time. The approach is based on two novel neural modules: the cross correlation I/O network and the Recursive-Reverse-Recursive Neural Network (R3NN). The R3NN model encodes partial program trees and expands them into full program trees, conditioned on the input-output examples.
The specific question or problem tackled by the paper is how to efficiently search for consistent programs in a DSL given a set of input-output examples. The approach is well-motivated, as it addresses the limitations of existing program induction methods, such as being computationally expensive, hard to train, and lacking interpretability.
However, I decide to reject this paper for two key reasons. Firstly, the idea of using a fast neighbor searcher in a memory-augmented net is not novel, as it was previously discussed by Rae et al. Secondly, the authors' decision to use their own neighbor searcher instead of standardized ones is unclear and lacks benchmarking against existing standards.
To support these reasons, I provide the following arguments. The paper's contribution to the field of program synthesis is significant, but the lack of novelty in the underlying techniques used is a major concern. Furthermore, the evaluation of the approach is limited to a specific domain of regular expression-based string transformations, which may not be representative of other domains.
To improve the paper, I suggest that the authors provide more context on the novelty of their approach and compare their method to existing standardized neighbor searchers. Additionally, they should consider evaluating their approach on a wider range of domains and tasks to demonstrate its generalizability.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the R3NN model handle programs with multiple possible expansions? Can the authors provide more details on the training process and the hyperparameters used? How does the approach scale to larger program sizes and more complex DSLs?