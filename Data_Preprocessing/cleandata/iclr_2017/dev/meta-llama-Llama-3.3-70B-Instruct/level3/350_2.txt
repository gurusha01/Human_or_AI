Summary
The paper proposes a novel approach to analyzing trained policies in Reinforcement Learning (RL) by introducing the Semi-Aggregated Markov Decision Process (SAMDP) model. The SAMDP model combines the advantages of temporal and spatial abstractions, allowing for a more concise and interpretable representation of the policy. The authors demonstrate the effectiveness of the SAMDP model on various Atari 2600 games and a gridworld problem, showing that it can identify skills and provide insights into the policy's behavior.
Decision
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and important problem in RL, namely analyzing trained policies, and proposes a novel and well-motivated approach to address it. Secondly, the paper provides extensive experimental evaluations and demonstrates the effectiveness of the SAMDP model on various tasks.
Supporting Arguments
The paper is well-written, clear, and interesting, with a thorough introduction to the background and related work in RL. The authors provide a detailed explanation of the SAMDP model and its components, including feature selection, state aggregation, skill identification, inference, and model selection. The experimental evaluations are thorough and demonstrate the effectiveness of the SAMDP model in identifying skills and providing insights into the policy's behavior. The paper also discusses potential limitations and future directions, such as modifying the t-SNE algorithm to take into account temporal distances and investigating the consistency of re-building an SAMDP.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational complexity of the SAMDP model and its scalability to larger tasks. Additionally, it would be interesting to see a comparison with other approaches to analyzing trained policies, such as those using graphical models or attention mechanisms. The authors may also consider providing more insights into the interpretability of the SAMDP model and how it can be used to improve the performance of RL agents.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the authors plan to address the issue of consistency in re-building an SAMDP, and what optimization problem can be formulated to directly account for all the performance criteria introduced in the paper?
* Can the SAMDP model be applied to continuous-action policies, and if so, what modifications would be necessary?
* How do the authors plan to modify the t-SNE algorithm to take into account temporal distances, and what benefits can be expected from this modification?