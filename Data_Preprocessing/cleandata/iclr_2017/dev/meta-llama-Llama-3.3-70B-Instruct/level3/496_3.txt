This paper proposes a novel technique called Neuro-Symbolic Program Synthesis (NSPS) that learns to generate a program incrementally without the need for an explicit search. The approach is based on two novel neural modules: the cross correlation I/O network and the Recursive-Reverse-Recursive Neural Network (R3NN). The R3NN model encodes partial trees in a Domain-Specific Language (DSL) and assigns probabilities to different non-terminals in a partial derivation and corresponding expansions to guide the search for complete derivations.
The paper tackles the specific question of program induction, which is a fundamental problem in Machine Learning and Artificial Intelligence. The approach is well-motivated, building on recent progress in deep learning and program synthesis. The paper presents a strong and novel model with promising experimental results, demonstrating the effectiveness of NSPS in constructing programs from new input-output examples and even constructing new programs for tasks that it had never observed before during training.
I decide to accept this paper with two key reasons: (1) the paper proposes a novel and well-motivated approach to program induction, and (2) the experimental results demonstrate the effectiveness of the approach in constructing programs from new input-output examples.
The paper provides supporting arguments for the reasons for the decision, including the thorough experiment section, which shows that the R3NN model obtains competitive performance on several challenging tasks and captures natural boundaries. The paper also presents a strong and novel model, which is a significant contribution to the field of program induction.
To improve the paper, I provide additional feedback, including the need to address minor issues with writing and related work. Specifically, the paper is missing some references, including Mikolov et al. (2010), Lin et al. (1996), and Socher's work, which should be included to provide a more comprehensive overview of the related work. Additionally, there are some errors in the related work section, including incorrect attribution of the clockwork RNN and gradient clipping.
To clarify my understanding of the paper and provide additional evidence, I would like to ask the authors the following questions:
* Can you provide more details on the training process, including the hyperparameter settings and the computational resources used?
* How do you plan to address the scalability issues in training with programs of larger size?
* Can you provide more examples of the programs constructed by the NSPS approach, including programs that require 4 or more Concat operations?
* How do you plan to extend the NSPS approach to other domains, such as program synthesis for numerical computations or data structures?