This review is based on the provided key points and conference guidelines.
The paper explores why deep networks perform well in practice and how modifying pooling geometry can improve separation rank in polynomially sized deep networks. The authors' current work differs from their previous studies, which used ReLu activation and max/mean pooling, by not using an activation function after convolution and using multiplication pooling instead. The paper contributes to the literature by studying convolutional arithmetic circuits and their ability to address inductive biases through pooling adjustments. The research provides insight into how deep networks can capture exponential correlations between input variables with polynomial-sized networks.
To answer the three key questions, I will summarize what the paper claims to do/contribute, clearly state my decision (accept or reject) with one or two key reasons for this choice, provide supporting arguments for the reasons for the decision, and provide additional feedback with the aim to improve the paper.
Summary of the paper's claims and contributions:
The paper claims to provide a theoretical analysis of convolutional arithmetic circuits and their ability to model correlations among regions of their input. The authors contribute to the literature by studying the separation rank of functions realized by convolutional arithmetic circuits and its relation to the network's pooling geometry.
Decision:
I decide to accept this paper with one key reason being that it provides a thorough theoretical analysis of convolutional arithmetic circuits and their ability to model correlations among regions of their input. The paper's contribution to the literature is significant, and the authors' notation and definitions are carefully expressed.
Supporting arguments:
The paper's analysis of the separation rank of functions realized by convolutional arithmetic circuits is well-motivated and well-placed in the literature. The authors provide a clear and concise explanation of their methodology and results, making it easy to follow and understand. The paper's contribution to the literature is significant, as it provides a new perspective on how deep networks can capture exponential correlations between input variables with polynomial-sized networks.
Additional feedback:
One potential area for improvement is the paper's notation and definitions, which could be further elaborated to make them more accessible to readers. Additionally, the paper could benefit from more experimental results to validate the authors' theoretical analysis. However, overall, the paper is well-written and provides a significant contribution to the literature.
Questions for the authors:
To clarify my understanding of the paper and provide additional evidence for my assessment, I would like to ask the authors the following questions:
1. Can you provide more intuition on why the separation rank of functions realized by convolutional arithmetic circuits is related to the network's pooling geometry?
2. How do the authors plan to extend their analysis to other types of convolutional networks, such as those with ReLU activation and max or average pooling?
3. Can you provide more experimental results to validate the authors' theoretical analysis, such as comparisons with other deep learning architectures?