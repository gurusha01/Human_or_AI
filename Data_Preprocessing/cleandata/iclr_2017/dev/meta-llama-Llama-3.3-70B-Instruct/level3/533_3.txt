Summary of the Paper's Contributions
The paper proposes a novel approach to analyzing trained policies in Reinforcement Learning (RL) by introducing the Semi-Aggregated Markov Decision Process (SAMDP) model. The SAMDP model combines the advantages of temporal and spatial abstractions to simplify the analysis of complex policies. The authors demonstrate the effectiveness of the SAMDP model in a gridworld problem and three Atari games, showing that it can identify skills and provide insights into the policy hierarchy.
Decision and Key Reasons
I decide to reject this paper, with two key reasons for this choice. Firstly, the proposed types of intrinsic motivation are not novel, with similar concepts already existing in previous works such as Kompella et al 2012. Secondly, the paper's claimed superiority over VIME is questionable, as the gain attributed to faster initialization rather than per-step cost, making the comparison to VIME less convincing.
Supporting Arguments
The paper explores intrinsic motivation in deep RL, proposing variants derived from auxiliary model-learning processes to aid exploration in continuous control tasks and Atari games. However, the idea of seeking uncertain states has potential, but is limited, and the authors are encouraged to discuss its limitations and explore alternative forms of learning progress or surprise. Furthermore, the paper's evaluation criteria, such as Value Mean Square Error (VMSE) and Inertia, are not thoroughly justified, and the authors should provide more rigorous analysis to support their claims.
Additional Feedback and Questions
To improve the paper, the authors should provide more detailed comparisons with existing works, such as Kompella et al 2012, and discuss the limitations of their approach. Additionally, the authors should consider exploring alternative forms of learning progress or surprise, and provide more rigorous analysis to support their evaluation criteria. I would like to ask the authors to clarify how they plan to address the issue of consistency in re-building an SAMDP, and how they intend to optimize the clustering method to directly account for all of the performance criteria introduced in the paper. Furthermore, I would like to know more about the authors' plans to modify the t-SNE algorithm to take into account temporal distances as well as spatial ones, and how this would impact the quality of the SAMDP model.