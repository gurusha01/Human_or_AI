Summary of the Paper's Contributions
The paper proposes an unsupervised algorithm for transferring samples between related domains using a fixed perceptual function f. The method, called Domain Transfer Network (DTN), employs a compound loss function that includes a multiclass GAN loss, an f-preserving component, and a regularizing component. The approach produces visually appealing results on several datasets, including digits and face images, and demonstrates improved performance on the SVHN->MNIST domain adaptation task.
Decision and Key Reasons
Based on the evaluation, I decide to Reject the paper. The two key reasons for this decision are:
1. Lack of sufficient domain adaptation experiments: The paper lacks sufficient experiments to establish the proposed method as a universal alternative to existing domain adaptation approaches.
2. Limited novelty of the method: The novelty of the method is relatively minor, with the f-constancy term being the main contribution, and may not be effective for more dissimilar domains.
Supporting Arguments
The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f. However, the approach has limitations, including reliance on a fixed function f trained on the source domain, which may drop important information for target domain reconstructions. Additionally, the use of super-resolved outputs may create a false impression of visual quality, and the original model outputs should be moved from the appendix to the main part of the paper.
Additional Feedback and Questions
To improve the paper, I suggest the authors provide more domain adaptation experiments to demonstrate the effectiveness of the proposed method. Additionally, the authors should consider addressing the limitations of the approach, such as the reliance on a fixed function f and the potential mode effect of the generator network.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How do the authors plan to address the limitation of the approach for more dissimilar domains?
* Can the authors provide more insights into the choice of the perceptual function f and its impact on the results?
* How do the authors plan to extend the approach to other domains and tasks beyond visual domains?