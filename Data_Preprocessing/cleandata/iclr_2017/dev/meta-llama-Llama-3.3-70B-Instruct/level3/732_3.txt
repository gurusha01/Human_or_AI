This paper proposes a novel approach to providing strong privacy guarantees for training data in machine learning applications, called Private Aggregation of Teacher Ensembles (PATE). The authors demonstrate that their approach can achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN datasets. The paper is well-written, and the authors provide a clear and concise explanation of their methodology and results.
The specific question tackled by the paper is how to protect the privacy of sensitive training data in machine learning applications. The authors address this question by proposing a black-box approach that combines multiple models trained on disjoint datasets, using a noisy voting mechanism to aggregate the predictions. The approach is well-motivated, and the authors provide a thorough analysis of the privacy guarantees provided by their method.
I decide to accept this paper because it presents a novel and well-motivated approach to providing privacy guarantees for training data. The authors provide a clear and concise explanation of their methodology and results, and their approach has the potential to be widely applicable to various machine learning applications.
The approach is well-supported by the results, which demonstrate that the PATE approach can achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN datasets. The authors also provide a thorough analysis of the privacy guarantees provided by their method, using the moments accountant technique to bound the privacy cost.
To improve the paper, I suggest that the authors provide more details on the experimental setup and the hyperparameters used in their experiments. Additionally, it would be helpful to include more comparisons with other privacy-preserving methods, such as differential privacy and federated learning.
Some questions I would like the authors to answer are: How does the number of teachers affect the privacy cost, and what is the optimal number of teachers for a given dataset? How does the approach perform on other datasets, such as medical datasets or datasets with different characteristics? Can the approach be extended to other machine learning applications, such as natural language processing or recommender systems? 
Overall, the paper presents a novel and well-motivated approach to providing privacy guarantees for training data, and the results demonstrate its potential to be widely applicable to various machine learning applications. With some additional details and comparisons, the paper has the potential to be even stronger.