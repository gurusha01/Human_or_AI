I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that "if it's not worth doing, it's not worth doing well." There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of "why use this representation" with the authors and they said their "main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network." Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.