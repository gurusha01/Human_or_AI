This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.
Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?