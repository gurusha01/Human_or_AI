This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.
My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. 
The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  
My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term "DeepRL" seems arbitrary.
On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. 
It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.