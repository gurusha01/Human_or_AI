The authors present a character language model that gains some interpretability without large losses in predictivity. 
CONTRIBUTION:
I'd characterize the paper as some experimental investigation of a cute insight.  Recall that multi-class logistic regression allows you to apportion credit for a prediction to the input features: some features raised the probability of the correct class, while others lowered it.  This paper points out that a sufficiently simple RNN model architecture is log-linear in the same way, so you can apportion credit for a prediction among elements of the past history.  
PROS:
The paper is quite well-written and was fun to read.  It's nice to see that a simple architecture still does respectably.
It's easy to imagine using this model for a classroom assignment.  
It should be easy to implement, and the students could replicate the authors' investigation of what influences the network's predictions.
The authors present some nice visualizations.
Section 5.2 also describes some computational benefits.
CAVEATS ON PREDICTIVE ACCURACY:
* Figure 1 says that the ISAN has "near identical performance to other architectures."  But this appears true only when comparing the largest models.  
Explanation: It appears that for smaller parameter sizes, a GRU still beats the authors' model by 22% to 39% in the usual metric of perplexity per word (ppw).  (That's how LM people usually report performance, with a 10% reduction in ppw traditionally being considered a good Ph.D. dissertation.  I assumed an average of 7 chars/word when converting cross-entropy/char to perplexity/word.)  
* In addition, it's not known whether this model family will remain competitive beyond the toy situations tested here.
Explanation: The authors tried it only on character-based language modeling, and only on a 10M-char dataset, so their ppw is extremely high: 2135 for the best models in this paper.  By contrast, a word-based RNN LM trained on 44M words gets ppw of 133, and trained on 800M words gets ppw of 51.  [Numbers copied from the paper I cited before: