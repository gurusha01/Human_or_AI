This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.
Minor note:
The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.
The paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.
While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?
At present this is an interesting technical report and I would like to see more detailed results in the future.