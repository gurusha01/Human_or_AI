SUMMARY.
The paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage.
The model first encodes the passage and the query using a recurrent neural network.
With an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question.
The encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN.
Three convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features.
Candidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length.
Each candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated.
The scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question.
The method is tested on the SQUAD dataset and outperforms the proposed baselines.
----------
OVERALL JUDGMENT
The method presented in this paper is interesting but not very motivated in some points.
For example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones.
The contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding.
In fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models.
----------
DETAILED COMMENTS
Equation (13) i should be s, not s^l.
I still do not understand the sentence " the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN". The RNN is over what all the words in the chunk? in the passage? 
The answer the authors gave in the response does not clarify this point.