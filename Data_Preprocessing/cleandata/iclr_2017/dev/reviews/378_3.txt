The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward. Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions. The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning.
The model is interesting, well defined and well explained. As far as I know, the UREX model is an original model which will certainly be useful for the RL community. The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper.