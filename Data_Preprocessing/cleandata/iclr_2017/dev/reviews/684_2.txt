The term strategy is a bit ambiguous. Could you please explain more in formal terms what is strategy?
Is r the discounted Return at time t, or the reward at time t?
Could the author compare the method to TD learning?
The paper is vague and using many RL terms with different meanings without clarifying those diversions.
"So, the output for a given state-actions pair is always same". Q function by definition is the value of (state, action). So as long as the policy is deterministic the output would be always same too. How's this different from Q learning?
The model description doesn't specify what is the policy, and it's only being mentioned in data generation part.
Why is it a model based approach?
The learning curves are only for 19 iterations, which does not give any useful information. The final results are clearly nothing comparable to previous works. The model is only being tested on three games.
The paper is vague and using informal language or sometimes misusing the common RL terms. The experiments are very small scale and even in that scenario performing very bad. It's not clear, why it's a model-based approach.