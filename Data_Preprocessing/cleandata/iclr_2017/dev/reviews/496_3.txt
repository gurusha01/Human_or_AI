This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).
Their model is organized as a set of layers that aim at capturing the information form different "level of abstraction". The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. 
The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.
Overall this paper presents a strong and novel model with promising experimental results.
On a minor note, I have few remarks/complaints about the writing and the related work:
- In the introduction:
"One of the key principles of learning in deep neural networks as well as in the human brain" : please provide evidence for the "human brain" part of this claim.
"For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances" I believe you re missing Mikolov et al. 2010 in the references.
"in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data": missing reference to Lin et al., 1996
- in the related work:
"A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)" : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.
While the above models focus on online prediction problems, where a prediction needs to be made…": I believe there is a lot of missing references, in particular to Socher's work or older recursive networks.
"The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)": this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.
Missing references:
"Recurrent neural network based language model.", Mikolov et al. 2010
"Learning long-term dependencies in NARX recurrent neural networks", Lin et al. 1996
"Sequence labelling in structured domains with hierarchical recurrent neural networks", Fernandez et al. 2007
"Learning sequential tasks by incrementally adding  higher  orders", Ring, 1993