This paper investigates the question of gathering information (answering question)
through direct interaction with the environment. In that sense, it is closely
related to "active learning" in supervised learning, or to the fundamental
problem of exploration-exploitation in RL. The authors consider a specific 
instance of this problem in a physics domain and learn
information-seeking policies using recent deep RL methods.
The paper is mostly empirical and explores the effect of changing the
cost of information (via the discount factor) on the structure of the learned
policies. It also shows that general-purpose deep policy gradient methods are
sufficient powerful to learn such tasks. The proposed environment is, to my knowledge,
novel as well the task formulation in section 2. (And it would be very valuable to the
the community if the environment would be open-sourced)
The expression "latent structure/dynamics" is used throughout the text and the connection
with bandits is mentioned in section 4. It therefore seems that authors aspire
for more generality with their approach but the paper doesn't quite fully ground
the proposed approach formally in any existing framework nor does it provide a
new one completely.
For example: how does your approach formalize the concept of "questions" and "answers" ?
What makes a question "difficult" ? How do you quantify "difficulty" ?
How do you define the "cost of information"? What are its units (bits, scalar reward), its semantics ?
Do you you have an MDP or a POMDP ? What kind of MDP do you consider ?
How do you define your discounted MDP ? What is the state and action spaces ?
Some important problem structure under the "interaction/labeling/reward"
paragraph of section 2 would be worth expressing directly in your definition
of the MDP: labeling actions can only occur during the "labeling phase" and that the transition
and reward functions have a specific structure (positive/negative, lead to absorbing state).
The notion of "phase" could perhaps be implemented by considering an augmented state space : $\tilde s = (s, phase)$