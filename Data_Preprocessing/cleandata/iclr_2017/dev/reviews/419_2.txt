This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. 
Experiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. 
The authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.
Some questions and comments:
- In Table 2, how do you use LDA features for RNN (RNN LDA features)? 
- I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM.
- The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic "trading"? What about the IMDB one?
- How scalable is the proposed method for large vocabulary size (>10K)?
- What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines.