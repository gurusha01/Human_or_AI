This paper proposes a new method for learning graphical models. Combined with a neural network architecture, some sparse edge structure is estimated via sampling methods. In introduction, the authors say that a problem in graphical lasso is model selection. However, the proposed method still implicitly includes model selection. In the proposed method, $P(G)$ is a sparse prior, and should include some hyper-parameters. How do you tune the hyper-parameters? Is this tuning an equivalent problem to model section? Therefore, I do not understand real advantage of this method over previous methods. What is the advantage of the proposed method?
Another concern is that this paper is unorganized. In Algorithm 1, first, Gi and \Sigmai are sampled, and then xj is sampled from N(0, \Sigma). Here, what is \Sigma? Is it different from \Sigmai? Furthermore, how do you construct (Yi, \hat{\Sigma}i) from (Gi, Xi )? Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1?
What is the definition of the receptive field in Proposition 2 and Proposition 3?