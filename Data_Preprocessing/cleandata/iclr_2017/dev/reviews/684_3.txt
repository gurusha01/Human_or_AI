This paper proposes a new approach to model based reinforcement learning and
evaluates it on 3 ATARI games. The approach involves training a model that
predicts a sequence of rewards and probabilities of losing a life given a
context of frames and a sequence of actions. The controller samples random
sequences of actions and executes the one that balances the probabilities of
earning a point and losing a life given some thresholds. The proposed system
learns to play 3 Atari games both individually and when trained on all 3 in a
multi-task setup at super-human level.
The results presented in the paper are very encouraging but there are many
ad-hoc design choices in the design of the system. The paper also provides
little insight into the importance of the different components of the system.
Main concerns:
- The way predicted rewards and life loss probabilities are combined is very ad-hoc.
  The natural way to do this would be by learning a Q-value, instead different
  rules are devised for different games.
- Is a model actually being learned and improved? It would be good to see
  predictions for several actions sequences from some carefully chosen start
  states. This would be good to see both on a game where the approach works and
  on a game where it fails. The learning progress could also be measured by
  plotting the training loss on a fixed holdout set of sequences.
- How important is the proposed RRNN architecture? Would it still work without
  the residual connections? Would a standard LSTM also work?
Minor points:
- Intro, paragraph 2 - There is a lot of much earlier work on using models in
  RL. For example, see Dyna and "Memory approaches to reinforcement learning in
  non-Markovian domains" by Lin and Mitchell to name just two.
- Section 3.1 - Minor point, but using a_i to represent the observation is
  unusual.  Why not use oi for observations and ai for actions?
- Section 3.2.2 - Notation again, r_i was used earlier to represent the
  reward at time i but it is being used again for something else.
- Observation 1 seems somewhat out of place. Citing the layer normalization
  paper for the motivation is enough.
- Section 3.2.2, second last paragraph - How is memory decoupled from
  computation here? Models like neural turning machines accomplish this by using
  an external memory, but this looks like an RNN with skip connections.
- Section 3.3, second paragraph - Whether the model overfits or not depends on
  the data. The approach doesn't work with demonstrations precisely because it
  would overfit.
- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy
  instead of Morimoto et al.
Overall I think the paper has some really promising ideas and encouraging
results but is missing a few exploratory/ablation experiments and some polish.