This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach:
- It uses only a small number of denoising steps, and is thus far more computationally efficient.
- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.)
- There is no tractable variational bound on the log likelihood.
I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.
Detailed comments follow:
Sec. 2:
"theta(0) the" -> "theta(0) be the"
"theta(t) the" -> "theta(t) be the"
"what we will be using" -> "which we will be doing"
I like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks.
"q. Having learned" -> "q. [paragraph break] Having learned"
Sec 3.3:
"learn to inverse" -> "learn to reverse"
Sec. 4:
"For each experiments" -> "For each experiment"
How sensitive are your results to infusion rate?
Sec. 5: "appears to provide more accurate models" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.
Fig 4. -- neat!