This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called "amortized inference" can be much faster than normal inference where inference must be run iteratively for every document. Some comments:
Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?
The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?
The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: