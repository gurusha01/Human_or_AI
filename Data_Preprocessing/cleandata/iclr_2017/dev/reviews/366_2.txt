This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.
Minor comments:
Please add citation to [1] or [2] for neural variational inference, and [2] for VAE. 
A typo in "This approximation to the Dirichlet prior p(θ|α) is results in the distribution", it should be "This approximation to the Dirichlet prior p(θ|α) results in the distribution"
In table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?
In table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?
How does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?
It may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).
Overall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. 
[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML'14
[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML'14