Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias based on the input character. This is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_s^t terms, and use of basic linear algebra to probe the network.
Regarding myself as a reviewer, I am quite sure I understood the main ideas and arguments of this paper, but am not an expert on RNN language models or intelligibility/interpretability in ML.
I did not read any papers with a similar premise - closest related work I'm familiar with would be deconvnet for insight into vision-CNNs.
PRO:
I think this is original and novel work. This work is high quality, well written, and clearly is the result of a lot of work.
I found section 4.5 about projecting into readout subspace vs "computational" subspace most interesting and meaningful.
CON:
+ The main hesitation I have is that the results on both parts (ISAN model, and analysis of it) are not entirely convincing:
   (1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks,
   (2) nor do the analysis sections provide all that much real insight in the learned network.
(1b) Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocab discrete input nor continuous input.
(2a) For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights.
(2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs.
(2c) Re sec 4.2 - 4.3: It seems that the quantity \kappa_s^t on which analysis rests, isn't all that meaningful. Elaborating a bit on what I wrote in the question:
For example: Fig 2, for input letter "u" in revenue, there's a red spot where '' character massively positively impacts the logit of 'e'. This seems quite meaningless, what would be the meaning of influence of '' character? So it looks ot me that the switching matrix Wu (and prior Wn We etc) are using previous state in an interesting way to produce that following e. So that metric \kappas^t just doesn't seem very meaningful.
This remark relates to the last paragraph of Sec4.2.
Even though the list of cons here is longer than pro's, I recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques. This work is well-motivated, very well-executed, and can inspire many more interesting investigations along these lines.