This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. 
+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. 
+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. 
+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. 
-It will be more interesting to show results in other domains such as texts and images. 
-In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.