I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it.
Minor point on presentation: Speaking of the "evolution" of x{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?
In interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of "training algorithm"?
Comments on central claims:
Previous work on initializing neural networks to promote information flow (e.g. Glorot & Bengio,