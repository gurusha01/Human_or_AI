Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. 
> Significance & Originality:
The explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. 
> Clarity:
The paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. 
> Experiments:
Experimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.