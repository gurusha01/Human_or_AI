This paper considers different methods of producing adversarial examples for generative models such as VAE and VAEGAN. Specifically, three methods are considered: classification-based adversaries which uses a classifier on top of the hidden code, VAE loss which directly uses the VAE loss and the "latent attack" which finds adversarial perturbation in the input so as to match the latent representation of a target input.
I think the problem that this paper considers is potentially useful and interesting to the community. To the best of my knowledge this is the first paper that considers adversarial examples for generative models. As I pointed out in my pre-review comments, there is also a concurrent work of "Adversarial Images for Variational Autoencoders" that essentially proposes the same "latent attack" idea of this paper with both L2 distance and KL divergence.
Novelty/originality: I didn't find the ideas of this paper very original. All the proposed three attacks are well-known and standard methods that here are applied to a new problem and this paper does not develop novel algorithms for attacking specifically generative models. However I still find it interesting to see how these standard methods compare in this new problem domain.
The clarity and presentation of the paper is very unsatisfying. The first version of the paper proposes the "classification-based adversaries" and reports only negative results. In the second set of revisions, the core idea of the paper changes and almost an entirely new paper with a new co-author is submitted and the idea of "latent attack" is proposed which works much better than the "classification-based adversaries". However, the authors try to keep around the materials of the first version, which results in a 13 page long paper, with different claims and unrelated set of experiments. "in our attempts to be thorough, we have had a hard time keeping the length down" is not a valid excuse.
In short, the paper is investigating an interesting problem and apply and compare standard adversarial methods to this domain, but the novelty and the presentation of the paper is limited.