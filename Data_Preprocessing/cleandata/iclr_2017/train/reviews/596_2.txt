This paper proposes a method for link prediction on Knowledge Bases. The method contains 2 main innovations: (1) an iterative inference process that allows the model to refine its predictions and (2) a shared memory component. Thanks to these 2 elements, the model introduced in the paper achieved remarkable results on two benchmarks.
The paper is fairly written. The model is interesting and the experimental results are strikingly good. Still, I only rate for a weak accept for the following reasons.
* The main problem with this paper is that there is little explanation of how and why the two new elements aforementioned are leading to such better results. For instance:
  - What are the performance without the shared memory? And when its size is grown? 
  - How does the performance is impacted when one varies Tmax from 1 to 5 (which the chosen value for the experiments I assume)? This gives an indications of how often the termination gate works.
  - It would also be interesting to give the proportion of examples for which the inference is terminated before hitting Tmax.
  - What is the proportion of examples for which the prediction changed along several inference iterations?
* A value of \lambda set to 10 (Section 2) seems to indicate a low temperature for the softmax. Is the attention finally attending mostly at a single cell? How do the softmax activations change with the type of relationships? the entity type?
* FB15k and WN18 are quite old overused benchmarks now. It would be interesting to test on larger conditions.