The paper reports that "[a]fter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity."
Is it possible for you to include (or provide here) the hyperparameters and type of dropout (i.e. recurrent dropout, embedding dropout, ...) used? Without them, replication would at best require a great deal of trial and error. As with "Recurrent Neural Network Regularization" (Zaremba et al., 2014), releasing a base set of hyper parameters greatly assists in the future work of the field.
This will likely also be desired for the other experiments, such as character LM.
This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.
The paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.
This is a well written paper on an interesting topic with strong results. I recommend it be accepted.