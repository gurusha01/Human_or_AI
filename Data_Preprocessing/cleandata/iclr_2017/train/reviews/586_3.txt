The paper investigates on better training strategies for the Neural GPU models as well as studies the limitations of the model.
Pros:
* Well written.
* Many investigations.
* Available source code.
Cons:
* Misleading title, there is no extension to the Neural GPU model, just to its training strategies.
* No comparisons to similar architectures (e.g. Grid LSTM, NTM, Adaptive Computation Time).
* More experiments on other tasks would be nice, it is only tested on some toy tasks.
* No positive results, only negative results. To really understand the negative results, it would be good to know what is missing to make it work. This has not been studied further.
* Some details remain unclear or missing, e.g. if gradient noise was used in all experiments, or the length of sequences e.g. in Figure 3.
* Misleading number of NTM computation steps. You write O(n) but it is actually variable.
After the results from the paper, the limitations still remain unclear because it is not clear exactly why the model fails. Despite showing some examples which make it fail, it was not studied in more detail why it failed for those examples, and how you could fix the problem.
I guess you should write ICRL 2017 there.