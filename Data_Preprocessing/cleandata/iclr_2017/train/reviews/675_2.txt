The submitted paper proposes a new way of learning sequence predictors. In the lines of incremental learning and curriculum learning, easier samples are presented first and the complexity is increased during training. The particularity here is that the complexity is defined as the length of the sequences given for training, the premise being is that longer sequences are harder to learn, since they need a more complex internal representation.
The targeted application is sequence prediction from primed prefixes, tested on a single dataset, which the authors extract themselves from MNIST.
The idea in the paper is interesting and worth reading. There are also many interesting aspects of evaluation part, as the authors perform several ablation studies to rule out side-effects of the tests. The proposed learning strategy is compared to other strategies.
However, my biggest concern is still with evaluation. The authors tested the method on a single dataset, which is non standard and derived from MNIST. Given the general nature of the claim, in order to confirm the interest of the proposed algorithm, it need to be tested on other datasets, public datasets, and on a different application.
The paper is too long and should be trimmed significantly.
The transfer learning part (from prediction to classification) is a different story and I do not see a clear connection to the main contribution of the paper.
The presentation and organization of the paper could be improved. It is quite sequentially written and sometimes reads like a student's report.
The loss given in the long unnumbered equation on page 6 should be better explained: provide explanations for each term, and make clearer what the different symbols mean. Learning is supervised, so which variables are predictions, and which are observations from the data (ground truth).
Names in table 2 do not correspond to the descriptions in section 4.
You mention the "Best value for the average over 10 runs".
Can you explain what you calculated here?