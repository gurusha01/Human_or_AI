The authors did solid work in collecting all the reported data. However, most findings don't seem to be too surprising to me:
- Finding 1 mainly shows that all architectures and batch sizes manage to utilize the GPU fully (or to the same percentage).
- Regarding Finding 2, I agree that from a linear relationship in Figure 9 you could conclude said hyperbolic relationship.
However, for this finding to be relevant, it has to hold especially for the latest generations of models. These cluster in the upper left corner of Figure 9 and on their own do not seem to show too much of a linear behaviour. Therefore I think there is not enough evidence to conclude asymptotic hyperbolic behaviour: For this the linear behaviour would have to be the stronger, the more models approach the upper left corner.
- Finding 3 seems to be a simple conclusion from finding 1: As long as slower models are better and faster models do draw the same power, finding 3 holds.
- Finding 4 is again similar to finding 1: If all architectures manage to fully utilize the GPU, inference time should be proportional to the number of operations.
Maybe the most interesting finding would be that all tested models seem to use the same percentage of computational resources available on the GPU, while one might expect that more complex models don't manage to utilize as much computational resources due to inter-dependencies. However actual GPU utilization was not evaluated and as the authors choose to use an older GPU, one would expect that all models manage to make use of all available computational power.
Additionally, I think these findings would have to be put in relation with compressing techniques or tested on actual production networks to be of more interest.