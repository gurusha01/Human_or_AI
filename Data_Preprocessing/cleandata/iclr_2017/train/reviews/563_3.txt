This submission introduces a formulation of Generative Adversarial Networks (GANs) under the lens of density ratio estimation, when using Bregman divergences. Even thought GANs already perform density estimation, the motivation of using Bregman divergences is to obtain an objective function with stronger gradients. I have three concerns with this submission.
First, the exposition of the paper must be significantly improved. The current version of the manuscript is at some points unreadable, and does a poor job at motivating, describing, and justifying the contributions.
Second, the authors scatter a variety of alternatives and heuristics throughout the description of the proposed b-GAN. This introduces a great amount of complexity when it comes to understanding, implementing, and using b-GAN. Further work is necessary to rule out (in a principled manner!) many of the proposed variants of the algorithm.
Third, it is next to impossible to interpret the experimental results, in particular Figures 2, 3, 4. The authors claim that these figures show that "learning does not stop", but such behavior can also be attributed to the typical chaotic dynamics of GANs. Even after reading Appendix A, I am left unconvinced on whether the proposed approach provides with any practical advantage (even no comparison is offered to other GAN approaches with similar architectures).
Overall, I believe this submission calls for significant improvements before being considered for publication.