This paper presents an algorithm for approximating the solution of certain time-evolution PDEs. The paper presents an interesting learning-based approach to solve such PDEs. The idea is to alternate between:
1. sampling points in space-time
2. generating solution to PDE at "those" sampled points
3. regressing a space-time function to satisfy the latter solutions at the sampled points (and hopefully generalize beyond those points).
I actually find the proposed algorithm interesting, and potentially useful in practice. The classic grid-based simulation of PDEs is often too expensive to be practical, due to the curse of dimensionality. Hence, learning the solution of PDEs makes a lot of sense for practical settings. On the other hand, as the authors point out, simply running gradient descent on the regression loss function does not work, because of the non-differentiablity of the "min" that shows up in the studied PDEs.
Therefore, I think the proposed idea is actually very interesting approach to learning the PDE solution in presence of non-differentability, which is indeed a "challenging" setup for numerically solving PDEs.
The paper motivates the problem (time-evolution PDE with "min" operator applied to the spatial derivatives) by applications in control thery, but I think there is more direct interest in such problems for the machine learning community, and even deep learning community. For example