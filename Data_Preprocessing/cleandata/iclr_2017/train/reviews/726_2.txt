An interesting connection is made between dropout, Tishby et al's "information bottleneck" and VAEs. Specifically, classification of 'y' from 'x' is split in two faces: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z). By optimizing the objective E{(x,y)~data} [ E{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is formed, where lambda controls an upper bound on the number of bits traveling through 'z'.
The objective is equivalent to a VAE objective with downweighted KL(posterior|prior), an encoder that takes as input 'x', and a decoder that only predicts 'x'.
- Related work (section 2) is discussed sufficiently. 
- In section 3, would be better to remind us the definition of mutual information.
- Connection to VAEs in section 5 is interesting.
- Unfortunately, the MNIST/CIFAR-10 results are not great. Since the method is potentially more flexible than other forms of dropout, this is slightly disappointing.
- It's unclear why the CIFAR-10 results seem to be substantially worse than the results originally reported for that architecture.
- It's unclear which version of 'beta' was used in figure 3a.
Overall I think the theory presented in the paper is promising. However, the paper lacks sufficiently convincing experimental results, and I encourage the authors to do further experiments that prove significant improvements, at least on CIFAR-10, perhaps on larger problems.