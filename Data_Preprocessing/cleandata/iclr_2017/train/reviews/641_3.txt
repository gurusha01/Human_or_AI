The authors propose methods for wild variational inference, in which the
variational approximating distribution may not have a directly accessible
density function. Their approach is based on the Stain's operator, which acts
on a given function and returns a zero mean function with respect to a given
density function which may not be normalized.
Quality:
The derviations seem to be technically sound. However, my impression is that
the authors are not very careful and honest at evaluating both the strengths
and weaknesses of the proposed work. How does the method perform in cases in
which the distribution to be approximated is high dimensional? The logistic
regression problem considered only has 54 dimensions. How would this method
perform in a neural network in which the number of weights is goint to be way
much larger? The logistic regression model is rather simple and its posterior
will be likely to be close to Gaussian. How would the method perform in more
complicated posteriors such as the ones of Bayesia neural networks?
Clarity:
The paper is not clearly written. I found it very really hard to follow and not
focused. The authors describe way too many methods: 1) Stein's variational
gradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy
(KSD), 4) Lavengin inference network, not to mention the introduction to
Stein's discrepancy. I found very difficult to indentify the clear
contributions of the paper with so many different techniques.
Originality:
It is not clear how original the proposed contributions are. The first of the
proposed methods is also discussed in
Wang, Dilin and Liu, Qiang. Learning to draw samples: With application to
amortized mle for generative adversarial learning. Submitted to ICLR 2017, 2016
How does this work differ from that one?
Significance:
It is very hard to evaluate the importance of proposed methods. The authors
only report results on a 1d toy problem with a mixture of Gaussians and on a
logistic regression model with dimension 54. In both cases the distributions to
be approximated are very simple and of low dimension. In the regression case
the posterior is also likely to be close to Gaussian and therefore not clear
what advances the proposed method would provide with respect to other more
simple approaches. The authors do not compare with simple variational
approaches based on Gaussian approximations.