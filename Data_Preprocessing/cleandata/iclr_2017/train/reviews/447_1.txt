The paper introduces a simulator and a set of synthetic question answering tasks where interaction with the "teacher" via asking questions is desired. The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users. The paper studies this problem in an offline supervised and an online reinforcement learning settings. The results show that the models improve by asking questions.  
-- The idea is novel, and is relatively unexplored in the research community. The paper serves as a good first step in that direction.
-- The paper studies three different types of tasks where the agent can benefit from user feedback.
-- The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings.
Other comments/questions: 
-- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper's contributions?
-- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems.
-- The first point on Page 10 "The performance of TestModelAQ is worse than TestAQ but better than TestQA." is not true for Task 2 from the numbers in Tables 2 and 4.
-- What happens if the conversational history is smaller or none? 
-- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student. 
-- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this?
-- Figure 1, Task 2 AQ, last sentence should have a negative response "(-)" instead of positive as currently shown. 
Preliminary Evaluation: 
A good first step in the research direction of learning dialogue agents from unstructured user interaction.