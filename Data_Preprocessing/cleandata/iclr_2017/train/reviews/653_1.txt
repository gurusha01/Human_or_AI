This paper theoretically justified a faster convergence (in terms of average gradient norm attained after processing a fixed number of samples) of using small mini-batches for SGD or ASGD with smaller number of learners. This indicates that there is an inherent inefficiency in the speed-up obtained with parallelizing gradient descent methods by taking advantage of hardware. This paper looks good overall and makes some connection between algorithm design and hardware properties.
My main concern is that Lemma 1 looks incorrect to me. The factor Df / S should be Df/ (S*M) for me. Please clarify this and check the subsequent theorem.