This paper introduces a novel RNN architecture named QRNN.
QNNs are similar to gated RNN , however their gate and state update  functions depend only on the recent input values, it does not depend on the previous hidden state. The gate and state update functions are computed through a temporal convolution applied on the input.
Consequently, QRNN allows for more parallel computation since they have less  operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM. However, they possibly loose in expressiveness relatively to those models. For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers.
Various extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed.
Authors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation). 
Overall the paper is an enjoyable read and the proposed approach is interesting,
Pros:
- Address an important problem
- Nice empirical evaluation showing the benefit of their approach
- Demonstrate up to 16x speed-up relatively to a LSTM
Cons:
- Somewhat incremental novelty compared to (Balduzizi et al., 2016)
Few specific questions:
- Is densely layer necessary to obtain good result on the IMDB task. How does a simple 2-layer QRNN compare with 2-layer LSTM?  
- How does the i-fo-ifo pooling perform comparatively? 
- How does QRNN deal with long-term time depency? Did you try on it on simple toy task such as the copy or the adding task?