This paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - ht = f(xt, x{t-1}, ...x{t-T}) - instead of the standard - ht = f(xt, h_{t-1}) -, and that if you do so the networks can run faster and work better. You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.
- I would encourage the authors to improve the explanation of the model. 
- Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from. 
- Otherwise the experiments seem adequate and I enjoyed this paper.
This could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.