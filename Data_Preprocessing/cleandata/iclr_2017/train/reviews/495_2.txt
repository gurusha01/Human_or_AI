The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps)) layers and O(poly log(1/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them. The paper is well written and clear. The arguments and proofs are easy to follow. I only have two questions:
1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof?
2- Is there an example of piecewise smooth function that requires at least poly(1/eps) hidden units with a shallow network?