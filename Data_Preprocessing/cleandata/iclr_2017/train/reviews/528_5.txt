This paper presents small but important modifications which can be made to differentiable programs to improve learning on them. Overall these modifications seem to substantially improve convergence of the optimization problems involved in learning programs by gradient descent. That said, the set of programs which can be learned is still small, and unlikely to be directly useful.