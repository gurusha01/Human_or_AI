The method overall seems to be a very interesting structural approach to variational autoencoders, however it seems to lack motivation as well as the application areas sufficient to prove its effectiveness.
I see the attractiveness of using structural information in this context and I find it more intuitive than using a flat sequence representation, especially when there is a clear structure in the data. However experimental results seem to fail to be convincing in that regard.
One issue is the lack of a variety of applications in general, the experiments seem to be very limited in that regard, considering that the paper itself speaks about natural language applications. It would be interesting to use the latent representations learned with the model for some other end task and see how much it impacts the success of that end task compared to various baselines.
In my opinion, the paper has a potentially strong idea however in needs stronger results (and possibly in a wider variety of applications) as a proof of concept.