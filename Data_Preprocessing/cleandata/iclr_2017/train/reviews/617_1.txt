This paper describe an implementation of delayed synchronize SGD method for multi-GPU deep ne training.
Comments
1) The described manual implementation of delayed synchronization and state protection is helpful. However, such dependency been implemented by a dependency scheduler, without doing threading manually.
2) The overlap of computation and communication is a known technique implemented in existing solutions such as TensorFlow(as described in Chen et.al) and MXNet. The claimed contribution of this point is somewhat limited.
3) The convergence accuracy is only reported for the beginning iterations and only on AlexNet. It would be more helpful to include convergence curve till the end for all compared networks.
In summary, this is paper implements a variant of delayed SyncSGD approach. I find the novelty of the system somewhat limited (due to comment (2)). The experiments should have been improved to demonstrate the advantage of proposed approach.