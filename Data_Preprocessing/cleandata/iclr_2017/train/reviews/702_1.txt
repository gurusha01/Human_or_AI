This paper presents two models for extractive document summarization: the classifier architecture and the selector architecture. These two models basically use either classification or ranking in a sequential order to pick the candidate sentences for summarization. Experiments in this paper show the results are either better or close to the SOTA.
Technical comments:
- In equation (1), there is a position-relevant component call "positional importance". I am wondering how important this component is? Is it possible to show the performance without this component? Especially, for the discussion on impact of document structure, when the model is trained on the shuffled order but tested on the original order.
- A similar question about equation (1), is the content-richness component really necessary? Since the score function already has salience part, which could measure how important of $h_j$ with respect to the whole document.
- For the dynamic summary representation in equation (3), why not use the same updating equation for both training and test procedures? During test time, the model actually knows the decisions that have been made so far by the decoder. In this way, the model will be more consistent during training and test. 
- I think section 5 is the most interesting part of this paper, and it is also convincing on the difference between the two architectures.
- It is a little disappointing that the decoding algorithm used in this paper is too simple. In a minimal case, both of them could use beam search and the results could be better.