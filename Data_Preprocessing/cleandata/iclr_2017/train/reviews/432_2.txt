Nice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG.  
Presentation: 
Although that seems to be the flow in the first part of the paper, I think it could be cast as a extension/ generalization of the dueling Q-network â€“ for me that would be a more intuitive exposition of the new algorithm and findings. 
Small concern in general case derivation: 
Section 3.2: Eq. (7) the expectation (s,a) is wrt to \pi, which is a function of \theta -- that dependency seems to be ignored, although it is key to the PG update derivation. If these policies(the sampling policy for the expectation and \pi) are close enough it's usually okay -- but except for particular cases (trust-region methods & co), that's generally not true. Thus, you might end up solving a very different problem than the one you actually care solving.
Results:
A comparison with the dueling architecture could be added as that would be the closest method (it would be nice to see if and in which game you get an improvement)
Overall: strong paper, good theoretical insights.