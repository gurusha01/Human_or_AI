This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard "fine-tuning" approach.
Pros:
-The method is simple and clearly explained.
-Standard fine-tuning is used widely, so improvements to and analysis of it should be of general interest.
-Experiments are performed in multiple domains -- vision and NLP.
Cons:
-The additional modules incur a rather large cost, resulting in 2x the parameters and roughly 3x the computation of the original network (for the "stiched" network).  These costs are not addressed in the paper text, and make the method significantly less practical for real-world use where performance is very often important.
-Given these large additional costs, the core of the idea is not sufficiently validated, to me.  In order to verify that the improved performance is actually coming from some unique aspects of the proposed technique, rather than simply the fact that a higher-capacity network is being used, some additional baselines are needed:
(1) Allowing the original network weights to be learned for the target task, as well as the additional module.  Outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network.
(2) Training the full module/stitched network from scratch on the source task, then fine-tuning it for the target task.  Outperforming this baseline would verify that having a set of weights which never "sees" the source dataset is useful.
-The method is not evaluated on ImageNet, which is far and away the most common domain in which pre-trained networks are used and fine-tuned for other tasks.  I've never seen networks pre-trained on CIFAR deployed anywhere, and it's hard to know whether the method will be practically useful for computer vision applications based on CIFAR results -- often improved performance on CIFAR does not translate to ImageNet.  (In other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network fine-tuning is far enough on the "practical" end of the spectrum that claiming an improvement to it should necessitate an ImageNet evaluation.)
Overall I think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don't simply come from the use of a larger network, and the lack of ImageNet evaluation calls into question its real-world application.
===============
Edit (1/23/17): I had indeed missed the fact that the Stanford Cars does do transfer learning from ImageNet -- thanks for the correction.  However, the experiment in this case is only showing late fusion ensembling, which is a conventional approach compared with the "stitched network" idea which is the real novelty of the paper.  Furthermore the results in this case are particularly weak, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is completely expected given that ResNet alone is a stronger base network than VGG ("ResNet+VGG > ResNet" would be a stronger result, but still not surprising). Demonstrating the stitched network idea on ImageNet, comparing with the corresponding VGG-only or ResNet-only finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don't sufficiently validate the stitched network idea, in my opinion.