This paper introduce a variant of A3C model where while agents run on multiple cores on CPU the model computations which is the computationally intensive part is passed to the GPU. And they perform various analysis to show the gained speedup.
Thanks the authors for the replying to the questions and adjusting the paper to make it more clear.
It's an interesting modification the the original algorithm. And section 5 does a through analysis of gpu utilization on different configurations.
The main weakness of the paper is lack of more extensive experiments in more atari domains and non atari domains, also multiple plots for multiple runs for observing the instabilities. Stability is a very important issue in RL, and also the most successful algorithms should be able to achieve good results in various domains. I do understand the computational resource limitation, especially in academia if in fact this work was done outside Nvidia.