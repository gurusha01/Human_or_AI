SUMMARY 
This paper presents a study of the number of hidden units and training examples needed to learn functions from a particular class. 
This class is defined as those Boolean functions with an upper bound on the variability of the outputs. 
PROS
The paper promotes interesting results from the theoretical computer science community to investigate the efficiency of representation of functions with limited variability in terms of shallow feedforward networks with linear threshold units. 
CONS 
The analysis is limited to shallow networks. The analysis is based on piecing together interesting results, however without contributing significant innovations. 
The presentation of the main results and conclusions is somewhat obscure, as the therein appearing terms/constants do not express a clear relation between increased robustness and decreasing number of required hidden units. 
COMMENTS 
- In the abstract one reads ``The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights.'' 
In page 1 the paper points the reader to a review article. It could be a good idea to include also more recent references. 
Given the motivation presented in the abstract of the paper it would be a good idea to also comment of works discussing the classes of Boolean functions representable by linear threshold networks. 
For instance the paper [Hyperplane Arrangements Separating Arbitrary Vertex Classes in n-Cubes. Wenzel, Ay, Paseman] discusses various classes of functions that can be represented by shallow linear threshold networks and provides upper and lower bounds on the number of hidden units needed for representing various types of Boolean functions. In particular that paper also provides lower bounds on the number of hidden units needed to define a universal approximator. 
- It certainly would be a good idea to discuss the results on the learning complexity in terms of measures such as the VC-dimension. 
- Thank you for the explanations regarding the constants.  
So if the noise sensitivity is kept constant, larger values of epsilon are associated with a smaller value of delta and of 1/epsilon. 
Nonetheless, the description in Theorem 2 is in terms of poly(1/epsilon, 1/delta), which still could increase? 
Also, in Lemma 1 reducing the sensitivity at a constant noise increases the bound on k? 
- The fact that the descriptions are independent of n seems to be related to the definition of the noise sensitivity as an expectation over all inputs. This certainly deserves more discussion. One good start could be to discuss examples of functions with an upper bound on the noise sensitivity (aside from the linear threshold functions discussed in Lemma 2). 
Also, reverse statements to Lemma 1 would be interesting, describing the noise sensitivity of juntas specifically, even if only as simple examples. 
- On page 3 ``...variables is polynomial in the noise-sensitivity parameters'' should be inverse of?
MINOR COMMENTS
On page 5 Proposition 1 should be Lemma 1?