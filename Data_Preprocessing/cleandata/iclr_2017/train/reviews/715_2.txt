This paper proposes two pruning methods to reduce the computation of deep neural network. In particular, whole feature maps and the kernel connections can be removed with not much decrease of classification accuracy. 
However, this paper also has the following problems. 
1)	The method is somehow trivial, since the pruning masks are mainly chosen by simple random sampling. The novelty and scalability are both limited. 
2)	Experiment results are mainly focused on the classification rate and the ideal complexity. As a paper on improving computation efficiency, it should include results on practical time consumption. It is very common that reducing numbers of operations may not lead to reduced computational time on a highly parallel platform (e.g., GPU). 
3)	It is more important to improve the computational efficiency on large-scale models (e.g., ImageNet classification network) than on small models (e.g., MNIST, CIFAR network). However, results on large-scale network is missing.
4)	(Logical validity of the proposed method) For feature map pruning, what if just to train reduced-size network is trained from scratch without transfer any knowledge from the pretrained large network? Is it possible to get the same accuracy? If so, it will simply indicate the hyper-parameter is not optimal for the original network. Experimental results are necessary to clarify the necessity of feature map pruning. 
Note that I agree with that a smaller network may be more generalizable than a larger network. 
----------------------------------------------
Comments to the authors's response:
Thanks for replying to my comments. 
1) I still believe that the proposed methods are trivial.
2) It is nice to show GPU implementation. Compared to existing toolboxes (e.g., Torch, Caffe, TensorFlow), is the implementation of convolution efficient enough?
3) Experiments on Cifar-100 are helpful (better than cifar-10), but it is not really large-scale, where speed-up is not so critical. ImageNet and Places datasets are examples of large-scale datasets.
4) The author did not reply to the question wrt the validity of the proposed methods. This question is critical.