This paper proposes a model to learn across different views of objects.  The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.  The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance.  Furthermore, a comparison against human perception on the "Tenenbaum objects" is shown.
Positives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below).  The paper is reasonably written.
Negatives: The paper is missing relevant references of related work in this space and should compare against an existing approach.
More details:
The "image purification" paper is very related to this work:
[A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su, Yangyan Li, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015.
There they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval.  If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]).  It appears that code and data is available online (