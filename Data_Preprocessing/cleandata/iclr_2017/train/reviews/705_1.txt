This paper is a follow-up on the NIPS 2016 paper "Unsupervised learning of spoken language with visual context", and does exactly what that paper proposes in its future work section: "to perform acoustic segmentation and clustering, effectively learning a lexicon of word-like units" using the embeddings that their system learns. The analysis is very interesting and I really like where the authors are going with this.
My main concern is novelty. It feels like this work is a rather trivial follow-up on an existing model, which is fine, but then the analysis should be more satisfying: currently, it feels like the authors are just illustrating some of the things that the NIPS model (with some minor improvements) learns. For a more interesting analysis, I would have liked things like a comparison of different segmentation approaches (both in audio and in images), i.e., suppose we have access to the perfect segmentation in both modalities, what happens? It would also be interesting to look at what is learned with the grounded representation, and evaluate e.g. on multi-modal semantics tasks.
Apart from that, the paper is well written and I really like this research direction. It is very important to analyze what models learn, and this is a good example of the types of questions one should ask. I am afraid, however, that the model is not novel enough, nor the questions deep enough, to make this paper better than borderline for ICLR.