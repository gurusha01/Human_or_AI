* Brief Summary: 
This paper explores an extension of multiplicative RNNs to the LSTM type of models. The resulting proposal is very similar to [1]. Authors show experimental results on character-level language modeling tasks. In general, I think the paper is well-written and the explanations are quite clear.
* Criticisms:
- In terms of contributions, the paper is weak. The motivation makes sense, however, very similar work has been done in [1] and already an extension over [2]. Because of that this paper mainly stands as an application paper.
- The results are encouraging. On the other hand, they are still behind the state of art without using dynamic evaluation. 
- There are some non-standard choices on modifications on the standard algorithms, such as "l" parameter of RMSProp and multiplying output gate before the nonlinearity.
- The experimental results are only limited to character-level language modeling only. 
* An Overview of the Review:
Pros:
- A simple modification that seems to reasonably well in practice.
- Well-written.
Cons:
- Lack of good enough experimental results.
- Not enough contributions (almost trivial extension over existing algorithms).
- Non-standard modifications over the existing algorithms.
[1] Wu Y, Zhang S, Zhang Y, Bengio Y, Salakhutdinov RR. On multiplicative integration with recurrent neural networks. InAdvances in Neural Information Processing Systems 2016 (pp. 2856-2864).
[2] Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024).