Overall, this is a nice paper. Developing a unifying framework for these newer
neural models is a worthwhile endeavor.
However, it's unclear if the DRAGNN framework (in its current form) is a
significant standalone contribution. The main idea is straightforward: use a
transition system to unroll a computation graph. When you implement models in
this way you can reuse code because modules can be mixed and matched. This is
nice, but (in my opinion) is just good software engineering, not machine 
learning research.
Moreover, there appears to be little incentive to use DRAGNN, as there are no
'free things' (benefits) that you get by using the framework. For example:
- If you write your neuralnet in an automatic differentiation library (e.g.,
  tensorflow or dynet) you get gradients for 'free'.
- In the VW framework, there are efficiency tricks that 'the credit assignment
  compiler' provides for you, which would be tedious to implement on your
  own. There is also a variety of algorithms for training the model in a
  principled way (i.e., without exposure bias).
I don't feel that my question about the limitations of the framework has been
satisfactorily addressed. Let me ask it in a different way: Can you give me
examples of a few models that I can't (nicely) express in the DRAGNN framework?
What if I wanted to implement