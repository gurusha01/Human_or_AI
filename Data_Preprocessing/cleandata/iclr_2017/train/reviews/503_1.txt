Paper proposes Gated Muiltimodal Unit, a building block for connectionist models capable of handling multiple modalities.
(Figure 2) The bimodal case returns weighted activation by gains of gating units, do you do anything special to keep multi-modal case weighted as well? I.e. how the equation for h in section 3.1 would look like for multi-modal case. Also what's the rationale for using tanh nonlinearity (over, say RELU), is it somehow experimentally optimised choice?
I would find interesting a discussion on a possibility of handling missing data in case one or more modalities are unavailable at test time. Is this possible in the current model to back-off to fewer modalities? Synthetic example may suggest that's in fact possible. Those numbers, perhaps, could be added to table 2.
In the synthetic experiment, you should compare MGU with the fully-connected MLP model really, with similar complexity - that is - at least two hidden units (as GMU has two such for each modality) followed by logistic regression. At least in terms of capability of drawing decision boundary, those should be comparable.
I think, broader discussion shall be written on the related work associated with mixture of experts models (which is fact are very similar conceptually) as well as multiplicative RNN models [1]. Also, gating unit in LSTM can, in principle, play very similar role when multiple modalities are spliced in the input.
Overall, the paper is interesting, so is the associated (and to be released) dataset.
Minor comments/typos:
Sec. 3.3:  layers and a MLP (see Section 3.4) -> layers and an MLP
Apologies for unacceptably late review.
[1] Multiplicative LSTM for sequence modelling B Krause, L Lu, I Murray, S Renals