The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.
Exploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in "Improving Distributional Similarity with Lessons Learned from Word Embeddings" by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will be even more important.
While this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality). For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding's inequality).
The word-order experiments were interesting. A relevant citation for this sort of conditional ordering procedure is "Generating Text with Recurrent Neural Networks" by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as "debagging."
Although this is just a first step in better understanding of sentence embeddings, it is an important one and I recommend this paper for publication.