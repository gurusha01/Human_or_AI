This is mainly a (well-written) toy application paper. It explains SGVB can be applied to state-space models. The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables. The prior over the innovation variables is not a function of time. Approximate inference is performed over these innovation variables, rather the states. This is a solution to a fairly specific problem (e.g. it doesn't discuss how priors over the beta's can depend on the past), but an interesting application nonetheless. The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity.
My compliments for the amount of detail put in the paper and appendix.
The experiments are on toy examples, but show promise.
- Section 2.1: "In our notation, one would typically set betat = wt, though other variants are possible" -> It's probably better to clarify that if Ft and Bt and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized).
- Section 2.2 last paragraph: "A key contribution is [â€¦] forcing the latent space to fit the transition". This seems rather trivial to achieve.
- Eq 9: "This interpretation implies the factorization of the recognition model:.."
The factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)