This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks.
One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. 
Minors: one line before Eq. (3.1), U \in R ? \times k