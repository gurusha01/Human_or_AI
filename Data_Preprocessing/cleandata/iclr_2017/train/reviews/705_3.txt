CONTRIBUTIONS 
This paper introduces a method for learning semantic "word-like" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.
NOVELTY+SIGNIFICANCE
As correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).
However, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.
The methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.
MISSING CITATION
There is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:
Ngiam, et al. "Multimodal deep learning." ICML 2011
POSITIVE POINTS
- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval
- The presented method performs efficient acoustic pattern discovery
- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs
NEGATIVE POINTS
- Limited novelty, especially compared with Harwath et al, NIPS 2016
- Although it gives good results, the clustering method has limited novelty and feels heuristic
- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices