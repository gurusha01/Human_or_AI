This paper develops Submodular Sum Product Networks (SSPNs) and
an efficient inference algorithm for approximately computing the
most probable labeling of variables in the model. The main
application in the paper is on scene parsing. In this context,
SSPNs define an energy function with a grammar component for
representing a hierarchy of labels and an MRF for encoding
smoothness of labels over space. To perform inference, the
authors develop a move-making algorithm, somewhat in the spirit
of fusion moves (Lempitsky et al., 2010) that repeatedly improves
a solution by considering a large neighborhood of alternative segmentations
and solving an optimization problem to choose the best neighbor.
Empirical results show that the proposed algorithm achieves better
energy that belief propagation of alpha expansion and is much faster.
This is generally a well-executed paper. The model is interesting
and clearly defined, the algorithm is well presented with proper
analysis of the relevant runtimes and guarantees on the
behavior. Overall, the algorithm seems effective at minimizing
the energy of SSPN models.
Having said that, I don't think this paper is a great fit for
ICLR. The model is even somewhat to the antithesis of the idea of
learning representations, in that a highly structured form of
energy function is asserted by the human modeller, and then
inference is performed. I don't see the connection to learning
representations. One additional issue is that while the proposed
algorithm is faster than alternatives, the times are still on the
order of 1-287 seconds per image, which means that the
applicability of this method (as is) to something like training
ConvNets is limited.
Finally, there is no attempt to argue that the model produces
better segmentations than alternative models. The only
evaluations in the paper are on energy values achieved and on
training data.
So overall I think this is a good paper that should be published
at a good machine learning conference, but I don't think ICLR is
the right fit.