This paper presents an multi-view learning algorithm which projects the inputs of different views (linearly) such that the neighborhood relationship (transition probabilities) agree across views.
This paper has good motivation--to study multi-view learning from a more information retrieval perspective. Some concerns:
-- The time complexity of the algorithm in its current form is high (see last paragraph of page 4). This might be the reason why the authors have conducted experiments on small datasets, and using linear projections.
-- The proposed method does have some nice properties, e.g., it does not require the projections to have the same dimension across views (I like this). While it more directly models neighborhood relationship than CCA based approaches, it is still not directly optimizing typical retrieval (e.g., ranking-based) criteria. On the other hand, the contrastive loss in 
Hermann and Blunsom. Multilingual Distributed Representations without Word Alignment. ICLR 2014. 
is certainly a relevant "information retrieval" approach, and shall be discussed and compared with.
My major concern about this paper is the experiments. As I mentioned in my previous comments, there are limited cases where linear mapping is more desirable than nonlinear mappings for dimension reduction. While the authors have argued that linear projection may provide better interpretability, I have not found empirical justification in this paper. Moreover, one could achieve interpretability by visualizing the projections and see what variations of the input is reflected along certain dimensions; this is commonly done for nonlinear dimension reduction methods. 
I agree that the general approach here generalizes to nonlinear projections easily, but the fact that the authors have not conducted experiments with nonlinear projections and comparisons with nonlinear variants of CCA and other multi-view learning algorithms limits the significance of the current paper.