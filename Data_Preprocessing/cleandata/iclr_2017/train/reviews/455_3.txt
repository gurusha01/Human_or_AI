This paper proposes a novel extension of generative adversarial networks that replaces the traditional binary classifier discriminator with one that assigns a scalar energy to each point in the generator's output domain. The discriminator minimizes a hinge loss while the generator attempts to generate samples with low energy under the discriminator. The authors show that a Nash equilibrium under these conditions yields a generator that matches the data distribution (assuming infinite capacity). Experiments are conducted with the discriminator taking the form of an autoencoder, optionally including a regularizer that penalizes generated samples having a high cosine similarity to other samples in the minibatch.
Pros:
* The paper is well-written.
* The topic will be of interest to many because it sets the stage for the exploration of a wider variety of discriminators than currently used for training GANs.
* The theorems regarding optimality of the Nash equilibrium appear to be correct.
* Thorough exploration of hyperparameters in the MNIST experiments.
* Semi-supervised results show that contrastive samples from the generator improve classification performance.
Cons:
* The relationship to other works that broaden the scope of the discriminator (e.g. [1]) or use a generative network to provide contrastive samples to an energy-based model ([2]) is not made clear in the paper.
* From visual inspection alone it is difficult to conclude whether EB-GANs produce better samples than DC-GANs on the LSUN and CelebA datasets.
* It is difficult to assess the effect of the PT regularizer beyond visual inspection as the Inception score results are computed with the vanilla EB-GAN.
Specific Comments
* Sec 2.3: It is unclear to me why a reconstruction loss will necessarily produce very different gradient directions.
* Sec 2.4: It is confusing that "pulling-away" is abbreviated as "PT".
* Sec 4.1: It seems strange that the Inception model (trained on natural images) is being used to compute KL scores for MNIST. Using an MNIST-trained CNN to compute Inception-style scores seems to be more appropriate here.
* Figure 3: There is little variation across the histograms, so this figure is not very enlightening.
* Appendix A: In the proof of theorem 2, it is unclear to me why a Nash equilibrium of the system exists.
Typos / Minor Comments
* Abstract: "probabilistic GANs" should probably be "traditional" or "classical" GANs.
* Theorem 2: "A Nash equilibrium ... exists"
* Sec 3: Should be "Several papers were presented"
Overall, I have some concerns with the related work and experimental evaluation sections, but I feel the model is novel enough and is well-justified by the optimality proofs and the quality of the generated samples.
[1] Springenberg, Jost Tobias. "Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks." arXiv preprint arXiv:1511.06390 (2015).
[2] Kim, Taesup, and Yoshua Bengio. "Deep Directed Generative Models with Energy-Based Probability Estimation." arXiv preprint arXiv:1606.03439 (2016).