This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted.
The inverted Softmax idea is very nice.
A few minor issues that ought to be addressed in a published version of this paper:
1) There is no mention of Haghighi et al (2008) "Learning Bilingual Lexicons from Monolingual Corpora.", which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed.
2) Likewise, Hermann & Blunsom (2013) "Multilingual distributed representations without word alignment." is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data.
3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages
4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion
5) I don't have a better suggestion, but is there an alternative to using the term "translation (performance/etc.)" when discussing word alignment across languages? Translation implies something more complex than this in my mind.
6) The Mikolov citation in the abstract is messed up