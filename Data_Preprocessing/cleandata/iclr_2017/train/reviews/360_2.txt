This paper formalizes the problem setting of having only a subset of available MDPs for which one has access to a reward. The authors name this setting "semi-supervised reinforcement learning" (SSRL), as a reference to semi-supervised learning (where one only has access to labels for a subset of the dataset). They provide an approach for solving SSRL named semi-supervised skill generalization (S3G), which builds on the framework of maximum entropy control. The whole approach is straightforward and amounts to an EM algorithm with partial labels (: they alternate iteratively between estimating a reward function (parametrized) and fitting a control policy using this reward function. They provide experiments on 4 tasks (obstacle, 2-link reacher, 2-link reacher with vision, half-cheetah) in MuJoCo.
The paper is well-written, and is overall clear. The appendix provides some more context, I think a few implementation details are missing to be able to fully reproduce the experiments from the paper, but they will provide the code.
The link to inverse reinforcement learning seems to be done correctly. However, there is no reference to off-policy policy learning, and, for instance, it seems to me that the \tau \in D_{samp} term of equation (3) could benefit from variance reduction as in e.g. TB(\lambda) [Precup et al. 2000] or Retrace(\lambda) [Munos et al. 2016].
The experimental section is convincing, but I would appreciate a precision (and small discussion) of this sentence "To extensively test the generalization capabilities of the policies learned with each method, we measure performance on a wide range of settings that is a superset of the unlabeled and labeled MDPs" with numbers for the different scenarios (or the replacement of superset by "union" if this is the case). It may explain better the poor results of "oracle" on "obstacle" and "2-link reacher", and reinforce* the further sentences "In the obstacle task, the true reward function is not sufficiently shaped for learning in the unlabeled MDPs. Hence, the reward regression and oracle methods perform poorly".
Correction on page 4: "5-tuple M_i = (S, A, T, R)" is a 4-tuple.
Overall, I think that this is a good and sound paper. I am personally unsure as to if all the parallels and/or references to previous work are complete, thus my confidence score of 3.
(* pun intended)