The authors explore whether the halting time distributions for various algorithms in various settings exhibit "universality", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.
The idea of the described universality is very interesting. However I see several shortcomings in the paper:
In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view.
Especially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a sensible initialization to cut of part of the right tail of the distribution.
Additionally, I found the paper quite hard to read. Here are some clarity issues:
- abstract: "even when the input is changed drastically": From the abstract I'm not sure what "input" refers to, here
- I. Introduction: "where the stopping condition is, essentially, the time to find the minimum": this doesn't seem to make sense, a condition is not a time. I guess the authors wanted to say that the stopping condition is that the minimum has been reached?
- I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are. From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful)
- I.3 "We use x^\ell for \ell \in Z=\{1, \dots, S\} where Z is a random sample from of training samples" This formulation doesn't make sense. Either Z is a random sample, or Z={1, ..., S}.
- II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.