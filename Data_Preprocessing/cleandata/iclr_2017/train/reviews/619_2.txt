This paper presents a simple method of adding gradient noise to improve the training of deep neural networks. This paper first appeared on arXiv over a year ago and while there have been many innovations in the area of improving the training of deep neural networks in tha time (batch normalization for RNNs, layer normalization, normalization propagation, etc.) this paper does not mention or compare to these methods. 
In particular, the authors state "However, recent work on applying batch normalization to recurrent networks (Laurent et al., 2015) has not shown promise in improving generalization ability for recur- rent architectures, which are the focus of this work." This statement is simply incorrect and was thoroughly explored in, e.g. Cooijmans et al. (2016) that establish that batch normalization is effective for RNNs.
The proposed method itself is extremely simple and is similar to numerous training strategies that have previously been advocated in the literature. As a result the contribution would be incremental at best and could be significant with sufficiently strong empirical results supporting this particular variant. However, as discussed above there are now multiple training strategies and algorithms in the literature that are not empirically compared.
Unfortunately, this paper is now fairly seriously out of date. It would not be appropriate to publish this at ICLR 2017.