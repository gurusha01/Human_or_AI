This paper describes a simple but clever method for allowing variable amounts of computation at each time step in RNNs. The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks. Visualizations of the assignment of computational resources over time support the hypothesis that the model is able to learn to assign more computations whenever longer longer term dependencies need to be taken into account.
The proposed model is evaluated on a multitude of tasks and its ability to outperform similar architectures seems consistent. Some of the tasks allow for an interesting analysis of the amount of computation the model requests at each time step. It's very interesting to see how the model seems to use more resources at the start of each word or ASCII character. I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task. The superior performance of the architecture is impressive but I'm not yet convinced that the baseline models had an equal number of hyperparameters to tune. I'll come back to this point in the next paragraph because it's mainly a clarity issue.
The abstract claims that the model is computationally more efficient than regular RNNs. There are no wall time measurements supporting this claim. While the model is theoretically able to save computations, the points made by the paper are clearly more conceptual and about the ability of the model to choose how to allocate its resources. This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up. I also find it unfortunate that it's not clear from the text how the hyperparameter \bar{m} was chosen. Whether it was chosen randomly or set using a hyperparameter search on held-out data influences the fairness of a comparison with RNNs which did not have a similar type of hyperparameter for controlling regularization like for example dropout or weight noise (even if regularization of RNNs is a bit tricky). I don't consider this a very serious flaw because I'm impressed enough by the fact that the new architecture achieves roughly similar performance while learning to allocate resources but I do think that details of this type are too important to be absent from the text. Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value.
To my knowledge, the proposed architecture is novel. The way the amount of computation is determined is unlike other methods for variable computation I have seen and quite inventive. Originality is one of this paper's strongest points. 
It's currently hard to predict whether this method for variable computation will be used a lot in practice given that this also depends on how feasible it is to obtain actual computational gains at the hardware level. That said, the architecture may turn out to be useful for learning long-term dependencies. I also think that the interpretability of the value m_t is a nice property of the method and that it's visualizations are very interesting. It might shed some more light into what makes certain tasks difficult for RNNs. 
Pros:
Original clever idea.
Nice interesting visualizations.
Interesting experiments.
Cons:
Some experimental details are not clear.
I'm not convinced of the strength of the baseline.
The paper shouldn't claim actual computational savings without reporting wall-clock times.
Edit:
I'm very positively impressed by the way the authors ended up addressing the biggest concerns I had about the paper and raised my score. Adding an LSTM baseline and results with a GRU version of the model significantly improves the empirical quality of the paper. On top of that, the authors addressed my question about some experimental detail I found important and promised to change the wording of the paper to remove confusion about whether the computational savings are conceptual or in actual wall time. I think it's fine that they are conceptual only as long as this is clear from the paper and abstract. I want to make clear to the AC that since the changes to the paper are currently still promises, my new score should be assumed to apply to an updated version of the paper in which the aforementioned concerns have indeed been addressed. 
Edit: 
Since I didn't know that the difference with the SOTA for some of these tasks was so large, I had to lower my score again after learning about this. I still think it's a good paper but with these results I cannot say that it stands out.