The paper tackles important problems in multi-task reinforcement learning: avoid negative transfer and allow finer selective transfer. The method is based on soft attention mechanism, very general, and demonstrated to be applicable in both policy gradient and value iteration methods. The introduction of base network allows learning new policy if the prior policies aren't directly applicable. State-dependent sub policy selection allows finer control and can be thought of assigning state space to different sub policies/experts. The tasks are relatively simplistic but sufficient to demonstrate the benefits. One limitation is that the method is simple and the results/claims are mostly empirical. It would be interesting to see extensions to option-based framework, stochastic hard attention mechanism, sub-policy pruning, progressive networks. 
In figure 6, the read curve seems to perform worse than the rest in terms of final performance. Perhaps alternative information to put with figures is the attention mask activation statistics during learning, so that we may observe that it learns to turn off adversarial sub-policies and rely on newly learned base policy mostly. This is also generally good to check to see if any weird co-adaptation is happening.