This work is basically a combined pointer network applied on language modelling. 
The smart point is that this paper aims at language modelling with longer context, where a memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences. 
Hence, a combination of a pointer network and a standard language model would balance the copying seen words and predicting unseen words. 
Generally, such as the combined pointer networks applied in sentence compression, a vector representation of the source sequence would be used to compute the gate. 
This paper, instead, introduces a sentinel vector to carry out the mixture model, which is suitable in the case of language modelling. 
I would be interested in the variations of sentinel mixture implementation, though the current version has achieved very good results. 
In addition, the new WikiText language modelling dataset is very interesting. 
It probably can be a more standard dataset for evaluating the continuously-updated language model benchmarks than ptb dataset. 
Overall, this is a well-written paper. I recommend it to be accepted.