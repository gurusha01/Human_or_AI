Updated review: 18 Jan. 2017
Thanks to the authors for including a comparison to the previously published sparsity method of Yu et al., 2012.  The comparison is plausible, though it would be clearer if the authors were to state that the best comparison for the results in Table 4 is the "RNN Sparse 1760" result in Table 3.
I have updated my review to reflect my evaluation of the revised paper, although I am also leaving the original review in place to preserve the history of the paper.
This paper has three main contributions.  (1) It proposes an approach to training sparse RNNs in which weights falling below a given threshold are masked to zero, and a schedule is used for the threshold in which pruning is only applied after a certain number of iterations have been performed and the threshold increases over the course of training.  (2) It provides experimental results on a Baidu-internal task with the Deep Speech 2 network architecture showing that applying the sparsification to a large model can lead to a final, trained model which has better performance and fewer non-zero parameters than a dense baseline model.  (3) It provides results from timing experiments with the cuSPARSE library showing that there is some potential for faster model evaluation with sufficiently sparse models, but that the current cuSPARSE implementation may not be optimal.
Pros
+ The paper is mostly clear and easy to understand.
+ The paper tackles an important, practical problem in deep learning:  how to successfully deploy models at the lowest possible computational and memory cost.
Cons
- As a second baseline, this paper should compare to "distillation" approaches (e.g.,