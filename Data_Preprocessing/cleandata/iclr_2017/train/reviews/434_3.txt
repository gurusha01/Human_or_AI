Contributions
The paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering.
Novelty+Significance
Batch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs.  The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics. 
Adding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison.
As RNNs are used across many tasks, this work is of interest to many.  However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn't address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization? 
Clarity
The paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly.
Summary
Interesting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead.
Pros
- Shows batch normalization to work for RNNs where previous works have not succeeded
- Good empirical analysis of hyper-parameter choices and of the activations
- Experiments on multiple tasks
- Clarity
Cons
- Relatively incremental
- Several 'hacks' for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization)
- No mention of computational overhead
- Only character or pixel-level tasks, what about word-level?