The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights.
This density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer.
Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity.
As weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient.
The training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights.
The experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance.
The paper is presented very clearly,  presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data.
The result tables are a bit confusing unfortunately.
minor issues:
p1
english mistake: "while networks that consist of convolutional layers".
p6-p7
Table 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse:
 In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better.
I assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.