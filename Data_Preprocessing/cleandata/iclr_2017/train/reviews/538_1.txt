The authors propose a new model to learn symbolic expression representations. They do a reasonably extensive evaluation with similar approaches and motivate their approach well.
As expressed in the preliminary questions, I think the authors could improve the motivation for their subexpforce loss.
At the top of page 6 the authors mention that they compare to two-layer MLP w/o residual connections. I think having a direct comparison between a model with and w/o the subexpforce loss would be helpful too and should be included (i.e. keep the residual connections and normalization).
My main concern is the evaluation "score". It appears to be precision on a per query basis. I believe a more standard metric, precision-recall or roc would be more informative. In particular the chosen metric is expected to perform better when the equivalence classes are larger, since this isn't taken into account in the denominator, but the likelihood of a random expression matching the query increases.