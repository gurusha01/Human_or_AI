This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but the writing can be improved. 
Comments:
- The authors compare their proposed approach with several alternative approaches and demonstrate strong performance of the proposed approaches. But it is unclear if the improvement is from the proposed approach or from the implementation.  
- The paper is not easy to follow and the writing can be improved in many place (aside from typos and missing references). Specifically, the authors should provide more intuitions of the proposed approach in the introduction and in Section 3. 
- The proposition and the analysis in Section 3.2 do not suggest the communication cost of linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as claimed in many places in the paper. Instead, it suggests LP cannot be faster than these methods by 2x and log p  times. More specifically, Eq (2) shows TbroadcaseBE/ TbroadcaseLP < 2. This does not provide an upper-bound of TbroadcaseLP and it can be arbitrary worse when comparing with TbroadcaseBE from this inequality. Therefore, instead of showing TbroadcaseBE/ TbroadcaseLP < 2, the authors should state TbroadcaseBE/ TbroadcaseLP > 1 when n approaches infinity. 
- It would be interesting to emphasize more on the differences between designing parallel algorithms on CPU v.s. on GPU to motivate the paper.