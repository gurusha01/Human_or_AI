Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations.
Contributions:
The main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document.
Writing:
The text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. 
Pros:
- The proposed model is a simple extension to the model to the model proposed in [2] for summarization.
- The results are better than the baselines.
Cons:
- The improvements are not that large.
- Justifications are not strong enough.
- The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal.
- The paper is very application oriented.
Question:
- How does the training speed when compared to the regular LSTM?
Some Criticisms:
A similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn't consider the application of that on the summarization task a significant contribution.  The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage.
As authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from "read-again" mechanism or the use of "pointing". 
The paper is very application focused, the contributions of the paper in terms of ML point of view is very weak.
It is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are 
The writing of this paper needs more work. In general, it is not very well-written. 
Minor comments:
Some of the corrections that I would recommend fixing,
On page 4: "… better than a single value … " —> "… scalar gating …"
On page 4: "… single value lacks the ability to model the variances among these dimensions." —> "… scalar gating couldn't capture the …."
On page 6: " … where h0^2 and h0^'2 are initial zero vectors … " —> "… h0^2 and h0^'2 are initialized to a zero vector in the beginning of each sequence …"
There are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2.
Better naming of the models in Table 1 is needed.
The location of Table 1 is a bit off.
[1] Zaremba, Wojciech, and Ilya Sutskever. "Reinforcement learning neural Turing machines." arXiv preprint arXiv:1505.00521 362 (2015). 
[2] Gulcehre, Caglar, et al. "Pointing the Unknown Words." arXiv preprint arXiv:1603.08148 (2016).