This paper discusses recurrent networks with an update rule of the form h{t+1} = Rx R h{t}, where Rx is an embedding of the input x into the space of orthogonal or unitary matrices, and R is a shared orthogonal or unitary matrix.    While this is an interesting model, it is by no means a new model:  the idea of using matrices to represent input objects (and multiplication to update state) is often used in the embedding-knowledge-bases or embedding-logic literature (e.g. Using matrices to model symbolic relationships by Ilya Sutskever and Geoffrey Hinton, or Holographic Embeddings of Knowledge Graphs by Maximillian Nickel et al.).  I don't think the experiments or analysis in this work add much to our understanding of it.    In particular, the experiments are especially weak, consisting only of a very simplified version of the copy task (which is already very much a toy).  I know several people who have played with this model in the setting of language modeling, and as the other reviewer notes, the inability of the model to forget is an actual annoyance.   
I think it is incumbent on the authors to show how this model can be really useful on a nontrivial task; as it is we should not accept this paper.
Some questions:  is there any reason to use the shared R instead of absorbing it into all the Rx?  Can you find any nice ways of using the fact that the model is linear in h or linear in Rx ?