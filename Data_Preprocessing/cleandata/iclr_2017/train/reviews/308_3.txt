SUMMARY 
This paper addresses important questions about the difficulties in training generative adversarial networks. It discusses consequences of using an asymmetric divergence function and sources of instability in training GANs. Then it proposes an alternative using a smoothening approach. 
PROS 
Theory, good questions, nice answers. 
Makes an interesting use of concepts form analysis and differential topology. 
Proposes avenues to avoid instability in GANs. 
CONS 
A bit too long, technical. Some parts and consequences still need to be further developed (which is perfectly fine for future work). 
MINOR COMMENTS
- Section 2.1 Maybe shorten this section a bit. E.g., move all proofs to the appendix. 
- Section 3 provides a nice, intuitive, simple solution. 
- On page 2 second bullet. This also means that P_g is smaller than the data distribution in some other x, which in turn will make the KL divergence non zero. 
- On page 2, ``for not generating plausibly looking pictures'' should be ``for generating not plausibly looking pictures''.  
- Lemma 1 would also hold in more generality. 
- Theorem 2.1 seems to be basic analysis. (In other words, a reference could spare the proof). 
- In Theorem 2.4, it would be good to remind the reader about p(z). 
- Lemma 2 seems to be basic analysis. (In other words, a reference could spare the proof). 
Specify the domain of the random variables. 
- relly - > rely 
- Theorem 2.2 the closed manifolds have boundary or not? (already in the questions)
- Corollary 2.1, ``assumptions of Theorem 1.3''. I could not find Theorem 1.3. 
- Theorem 2.5 ``Therefore'' -> `Then'? 
- Theorem 2.6 ``Is a... '' -> `is a' ? 
- The number of the theorems is confusing.