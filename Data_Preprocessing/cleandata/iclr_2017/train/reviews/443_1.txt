This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation.
PROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated. Furthermore, the empirical evaluation is very well done, and the authors obtain good results.
CONs: One might be concerned about whether the additional training and decoding complexity is warranted. For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y). (It's worth noting that Li et al. (NAACL 2016) do something similar for conversation modeling). At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this.
Other Comments: 
 - Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper. 
 - It initially seems strange to suggest a noisy-channel model as a way of addressing the "explaining away" problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever.