This paper focusses on attention for neural language modeling and has two major contributions:
1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.
2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.
The paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.
I am convinced with authors' responses for my pre-review questions.
Minor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.