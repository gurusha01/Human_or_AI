The paper discusses a range of modelling choices for designing differentiable programming languages. Authors propose 4 recommendations that are then tested on a set of 13 algorithmic tasks for lists, such as "length of the list", "return k-th element from the list", etc. The solutions are learnt from input/output example pairs (5 for training, 25 for test).
The main difference between this work and differentiable architectures, like NTM, Neural GPU, NRAM, etc. is the fact that here the authors aim at automatically producing code that solves the given task.
My main concern are experiments - it would be nice to see a comparison to some of the neural networks mentioned in related work. Also, it would be useful to see how this model is doing on typical problems used by mentioned neural architectures (problems such as "sorting", "merging", "adding"). I'm wondering how this is going to generalize to other types of programs that can't be solved with prefix-loop-suffix structure.
It is also concerning that although  1) the tasks are simple, 2) the structure of the solution is very restricted and 3) model is using extensions doing most of the work, the proposed model still fails to find solutions (example: A+L model that has "loop" fails to solve "list length" task in 84% of the runs).
Pro:
- generates code rather than black-box neural architecture
- nice that it can learn from very few examples
Cons:
- weak results, works only for very simple tasks, missing comparison to neural architectures