The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost. This paper is a generalization of HashedNets by Chen et al. (ICML'15) where parameters of a neural network are mapped into smaller memory arrays using some hash functions with possible collisions. Instead of training the original parameters, given a hash function, the elements of the compressed memory arrays are trained using back-propagation. In this paper, some new tricks are proposed including: (1) the compression space is shared among the layers of the neural network (2) multiple hash functions are used to reduce the effects of collisions (3) a small network is used to combine the elements retrieved from multiple hash tables into a single parameter. Fig 1 of the paper describes the gist of the approach vs. HashedNets.
On the positive side,
+ The proposed ideas are novel and seem useful.
+ Some theoretical justification is presented to describe why using multiple hash functions is a good idea.
+ All of the experiments suggest that the proposed MFH approach outperforms HashedNets.
On the negative side,
- The computation cost seems worse than HashedNets and is not discussed.
- Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.
That said, I believe this paper benefits the deep learning community as it sheds light into ways to share parameters across layers of a neural network potentially leading to more interesting follow-ups. I recommend accept, while asking the authors to address the comments below.
More comments:
- Please discuss the computation cost for both HashedNets and MFH for both fully connected and convolutional layers.
- Are the experiments only run once for each configuration? Please run multiple times and report average / standard error.
- For completeness, please add U1 results to Table 1.
- In Table 1, U4-G3 is listed twice with two different numbers.
- Some sentences are not grammatically correct. Please improve the writing.