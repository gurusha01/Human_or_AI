This paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation. The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible. 
The paper nicely presents this core idea and a way to achieve this - by choosing special "routings" between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example) the resulting Jacobian has a tractable structure. This routing can be cascaded to achieve even more complex transformation. 
On the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures. 
I would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration.
In summary - the paper is nicely written, results are quite good and the model is interesting - I'm happy to recommend acceptance.