Summary:
The paper introduces a parametric class for non linearities used in neural networks. The paper suggests two stage optimization to learn the weights of the network, and the non linearity weights.
significance:
The paper introduces a nice idea, and present nice experimental results. however  I find the theoretical analysis not very informative, and  distractive from the main central idea of the paper. 
 
A more thorough experimentation with the idea using different basis and comparing it to wider networks (equivalent to the number of cosine basis used in the leaned one ) would help more supporting results in the paper. 
Comments: 
- Are the weights of the non -linearity learned shared across all units in all layers ? or each unit has it is own non linearity?
- If all weights are tied across units and layers. One question that would be interesting to study , if there is an optimal non linearity. 
- How different is the non linearity learned if the hidden units are normalized or un-normalized.  In other words how does the non linearity change if you use or don't use batch normalization? 
- Does normalization affect  the conclusion that polynomial basis fail?