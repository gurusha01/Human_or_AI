The paper proposed a tensor factorization approach for MTL to learn cross task structures for better generalization. The presentation is clean and clear and experimental justification is convincing. 
As mentioned, including discussions on the effect of model size vs. performance would be useful in the final version and also work in other fields related to this. 
One question on Sec. 3.3, to build the DMTRL, one DNN per-task is trained with the same architecture. How important is this pretraining? Would random initialization also work here? If the data is unbalanced, namely, some classes have very few examples, how would that affect the model?