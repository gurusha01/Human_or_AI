This is a very interesting and timely paper, with multiple contributions. 
- it proposes a setup for dealing with combinatorial perception and action-spaces that generalizes to an arbitrary number of units and opponent units,
- it establishes some deep RL baseline results on a collection of Starcraft subdomains,
- it proposes a new algorithm that is a hybrid between black-box optimization REINFORCE, and which facilitates consistent exploration.
As mentioned in an earlier comment, I don't see why the "gradient of the average cumulative reward" is a reasonable choice, as compared to just the average reward? This over-weights late rewards at the expense of early ones, so the updates are not matching the measured objective. The authors state that they "did not observe a large difference in preliminary experiments" -- so if that is the case, then why not choose the correct objective?
DPQ is characterized incorrectly: despite its name, it does not "collect traces by following deterministic policies", instead it follows a stochastic behavior policy and learns off-policy about the deterministic policy. Please revise this. 
Gradient-free optimization is also characterized incorrectly ("it only scales to few parameters"), recent work has shown that this can be overcome (e.g. the TORCS paper by Koutnik et al, 2013). This also suggests that your "preliminary experiments with direct exploration in the parameter space" may not have followed best practices in neuroevolution? Did you try out some of the recent variants of NEAT for example, which have been applied to similar domains in the past?
On the specific results, I'm wondering about the DQN transfer from m15v16 to m5v5, obtaining the best win rate of 96% in transfer, despite only reaching 13% (the worst) on the training domain? Is this a typo, or how can you explain that?