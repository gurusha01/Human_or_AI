This paper proposes a deep extension of generalized CCA. The main contribution of the paper is deriving the gradient update for the GCCA objective.
I disagree with the claim that "this is the first Multiview representation learning technique that combines the flexibility of nonlinear representation learning with the statistical power of incorporating information from many independent resources or views".  [R1] proposes a Multiview representation learning method which is both non-linear and capable of handling more than 2 views. This is very much relevant to what authors are proposing. The objective function proposed in [R1] maximizes the correlation between views and minimizes the self and cross reconstruction errors. This is intuitively similar to nonlinear version of PCA+CCA for multiple views. Comparing these 2 methods is crucial to prove the usefulness of DGCCA and the paper is incomplete without this comparison. Authors should also change their strong claim.
Related work section is minimal. There are significant advances in 2-view non-linear representation learning which are worth mentioning. 
References:
[R1] Janarthanan Rajendran, Mitesh M. Khapra, Sarath Chandar, Balaraman Ravindran: Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning. HLT-NAACL 2016: 171-181