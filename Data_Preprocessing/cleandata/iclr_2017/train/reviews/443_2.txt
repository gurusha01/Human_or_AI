The paper proposes an online variant of segment to segment transducers, which allows to circumvent the necessity of observing whole sentence, before making target predictions. Authors mostly build on their previous work, allowing additionally to leverage independent priors on the target hypotheses, like the language grammar or sentence length.
Strong points:
- well written, interesting idea of combining various sources of information in a Bayesian framework for seq2seq models
Handling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here - which is definitely of interest to the community
- strong experimental section, with some strong results (though not complete: see weak points)
Weak points:
- Authors do not improve on computational complexity (w.r.t Tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables)
- What about the baseline where you only combine direct, LM and bias contributions (no channel)? Was there any (non-obvious) algorithmic constraint why - this has not been included?
Some other (minor) comments:
- Related to the first weak point: can you elaborate more on how the clue of your work is conceptually different from the work of Tillmann et al. (1997) (except, of course, the fact you use connectionist discriminative models to derive particular conditional probabilities). 
- How sensitive is the model to different choices of hyper-parameters in eq (3). Do you naively search through the search space of those, or do something more clever?
- Some more comments on details of the auxiliary direct model would be definitely of interest.
- How crucial is the correct choice of the pruning variables (K1 and K2)? 
- Sec. 2: makes no Markovian assumptions -> no first-order Markovian assumption?
Typos:
Table 1: chanel -> channel (one before last row)
Apologies for late review.