Thanks a lot for your detailed response and clarifications.
The paper proposes to use a scattering transform as the lower layers of a deep network. This fixed representation enjoys good geometric properties (local invariance to deformations) and can be thought as a form of regularization or prior. The top layers of the network are trained to perform a given supervised task. This is the final model can be thought as plugging a standard deep convolutional network on top of the scattering transform. Evaluation on CIFAR 10 and 100 shows that the proposed approach achieves performance competitive with high performing baselines.
I find the paper very interesting. The idea of cascading these representations seems very natural thing to try. To the best of my knowledge this is the first work that combines predefined and generic representations with modern CNN architectures achieving competitive performance to high performing approaches. While the state of the art (Resnets and variants) achieves significantly higher performances, I believe that this work strongly delivers it's point.
The paper convincingly shows that lower level invariances can be obtained from analytic representations (scattering transform), simplifying the training process (using less parameters) and allowing for faster evaluation. The of the hybrid approach become crucial in the low data regime. 
The author argues that with the scattering initialisation instabilities cannot occur in the first layers contrary as the operator is non-expansive. This naturally suggests that the model is more robust to adversarial examples. It would be extremely interesting to present an empirical evaluation of this task. What's the practical impact? Can this hybrid network be fooled with adversarial? If this is the case, it would render the use of scattering initialization very attractive.