Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks.
This change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice. 
The experiments show slight improvement (98.0 -> 98.1) for some MNIST configurations. They show strong improvement (almost 100% higher accuracy after 1500 iterations) on a toy algorithmic task. It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present. Hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks.