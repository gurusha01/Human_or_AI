This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.
A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.
The experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.
A major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid, the lack of originality lets it down.
Minor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?