Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.
Paper summary: this work proposes to use RNNs inside a convolutional network architecture as a complementary mechanism to propagate spatial information across the image. Promising results on classification and semantic labeling are reported.
Review summary:
The text is clear, the idea well describe, the experiments seem well constructed and do not overclaim. Overall it is not a earth shattering paper, but a good piece of incremental science.
Pros:
* Clear description
* Well built experiments
* Simple yet effective idea
* No overclaiming
* Detailed comparison with related work architectures
Cons:
* Idea somewhat incremental (e.g. can be seen as derivative from Bell 2016)
* Results are good, but do not improve over state of the art
Quality: the ideas are sound, experiments well built and analysed.
Clarity: easy to read, and mostly clear (but some relevant details left out, see comments below)
Originality: minor, this is a different combination of ideas well known.
Significance: seems like a good step forward in our quest to learn good practices to build neural networks for task X (here semantic labelling and classification).
Specific comments:
* Section 2.2 "we introduction more nonlinearities (through the convolutional layers and ...". Convolutional layers are linear operators.
* Section 2.2, why exactly RNN cannot have pooling operators ? I do not see what would impede it.
* Section 3 "into the computational block", which block ? Seems like a typo, please rephrase.
* Figure 2b and 2c not present ? Please fix figure or references to it.
* Maybe add a short description of GRU in the appendix, for completeness ?
* Section 5.1, last sentence. Not sure what is meant. The convolutions + relu and pooling in ResNet do provide non-linearities "between layers" too. Please clarify
* Section 5.2.1 (and appendix A), how is the learning rate increased and decreased ? Manually ? This is an important detail that should be made explicit. Is the learning rate schedule the same in all experiments of each table ? If there is a human in the loop, what is the variance in results between "two human schedulers" ?
* Section 5.2.1, last sentence; "we certainly have  a strong baseline"; the Pascal VOC12 for competition 6 reports 85.4 mIoU as best known results. So no, 64.4 is not "certainly strong". Please tune down the statement.
* Section 5.2.3 Modules -> modules
* The results ignore any mention of increased memory usage or computation cost. This is not a small detail. Please add a discussion on the topic.
* Section 6 "adding multi-scale spatial" -> "adding spatial" (there is nothing inherently "multi" in the RNN)
* Section 6 Furthermoe -> Furthermore
* Appendix C, redundant with Figure 5 ?