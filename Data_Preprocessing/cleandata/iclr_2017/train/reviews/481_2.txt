This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.
The paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.
Pros:
For the first time in the literature, this paper proposed the concept of 'label leaking'. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.
Using the ratio of 'clean accuracy' over 'adversarial accuracy' as the measure of robust is more reasonable compared to the existing works in the literature. 
Cons:
Although the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main contribution of the paper. Note that Section 4.3 and Section 4.4 are not specific to large-scale dataset, thus emphasizing the 'large-scale' in the title and in the introduction seems improper. 
Basically all the conclusions of the paper are made based on observing the experimental results. Further tests should have been performed to verify these hypotheses. Without that, the conclusions of the paper seems rushy. For example, one dataset of imageNet can not infer the conclusions for all large-scale datasets.