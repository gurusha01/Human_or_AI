This paper proposes relation networks in order to model the pairwise interactions between objects in a visual scene. 
The model is very straight forward, first an MLP (with shared weights) is applied to each pair of objects. Finally a prediction is created by an MLP which operates by summing non-linear functions of these pairs of objects. 
Experimental evaluation is done in a synthetic dataset that is generated to fit the architecture hand-crafted in this paper. 
The title of the paper claims much more than the paper delivers. Discovering objects and their relations is a very important task.
However, this paper does not discover objects or their relations, instead, each objects is represented with hand coded ground truth attributes, and only a small set of trivial relationships are "discovered", e.g., relative position. 
Discovering objects and their relationships has been tackled for several decades in computer vision (CV). The paper does not cite or compare to any technique in this body of literature. This is typically refer to as "contextual models".
Can the proposed architecture help object detection and/or scene classification? would it work in the presence of noise (e.g, missing detections, non accurate detection estimates, complex texture)? would it work when the attributes of objects are estimated from real images? 
 
I'll be more convinced if experiments where done in real scenes. In the case of indoor scenes, datasets such as NYUv2, Sun-RGB-D, SceneNN, Chen et al CVPR 14 (text-to-image-correference) could be used. In outdoor scenes, KITTI and the relationships between cars, pedestrians and cyclist could also serve as benchmark. 
Without showing real scenes, this paper tackles a too toy problem with a very simple model which does not go much further than current context models, which model pairwise relationships between objects (with MRFs, with deep nets, etc).