1) Summary
This paper proposes an end-to-end hybrid architecture to predict the local linear trends of time series. A temporal convnet on raw data extracts short-term features. In parallel, long term representations are learned via a LSTM on piecewise linear approximations of the time series. Both representations are combined using a MLP with one hidden layer (in two parts, one for each stream), and the entire architecture is trained end-to-end by minimizing (using Adam) the (l2-regularized) euclidean loss w.r.t. ground truth local trend durations and slopes.
 
2) Contributions
+ Interesting end-to-end architecture decoupling short-term and long-term representation learning in two separate streams in the first part of the architecture.
+ Comparison to deep and shallow baselines.
3) Suggestions for improvement
Add a LRCN baseline and discussion:
The benefits of decoupling short-term and long-term representation learning need to be assessed by comparing to the popular "long-term recurrent convolutional network" (LRCN) of Donahue et al (