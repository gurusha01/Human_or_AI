This paper empirically studies the invariance, equivariance and equivalence properties of representations learned by convolutional networks under various kinds of data augmentation. Additional loss terms are presented which can make a representation more invariant or equivariant.
The idea of measuring invariance, equivariance and equivalence of representations is not new (Lenc & Vedaldi). The authors are the first to systematically study the effect of data augmentation on these properties, but it is unclear in what way the results are surprising, interesting, or useful. It is not really surprising that data augmentation increases invariance, or that training with the same augmentation leads to more similar representations than training with different augmentations.
Regarding the presented method to increase invariance and equivariance: while it could be that a representation will generalize better if it is invariant or equivariant, it is not clear why one would want to increase in/equivariance if it does not indeed lead to improvements in performance. The paper presents no evidence that training for increased invariance / equivariance leads to substantial improvements in performance. Combined with the fact that the loss (eq. 6) would substantially increase the computational burden, I don't think this technique will be very useful.
Minor comments:
-R^{nxn} should be R^{n \times n}
-In eq. 2: 'equivaraince'
-In 3.3, argmax is not properly formatted
-I think data augmentation was already considered essential before Krizhevsky et al. Not really correct to attribute this to them.
- About the claim "This is related to the idea of whether CNNs collapse (invariance) or linearize (equivariance) view manifolds of 3D objects". The idea that equivariance means that the manifold (orbit) is linearized, is incorrect. A linear representation M_g can create nonlinear manifolds. A simple example is given by a rotation matrix in 2D (clearly linear), generating a nonlinear manifold (the circle). 
- Equivariance in eq. 2 should be called "non-equivariance". If the value is low, the representation is equivariant, while if it is high it is non-equivariant.
- "Eq. 2  also uses the paradigm that", uses the word paradigm in a strange manner
- In the definition of x'ij, should one of the gj be inverted? Otherwise it seems like the transformation is applied twice, instead of being undone.