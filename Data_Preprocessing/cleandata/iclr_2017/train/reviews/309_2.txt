This paper is about improving feature learning in deep reinforcement learning, by augmenting the main policy's optimization problem with terms corresponding to (domain-independent) auxiliary tasks. These tasks are about control (learning other policies that attempt to maximally modify the state space, i.e. here the pixels), immediate reward prediction, and value function replay. Except for the latter, these auxiliary tasks are only used to help shape the features (by sharing the CNN+LSTM feature extraction network). Experiments show the benefits of this approach on Atari and Labyrinth problems, with in particular much better data efficiency than A3C.
The paper is well written, ideas are sound, and results pretty convincing, so to me this is a clear acceptance. At high level I only have few things to say, none being of major concern:
- I believe you should say something about the extra computational cost of optimizing these auxiliary tasks. How much do you lose in terms of training speed? Which are the most costly components?
- If possible, please try to make it clearer in the abstract / intro that the agent is learning different policies for each task. When I read in the abstract that the agent "also maximises many other pseudo-reward functions simultaneously by reinforcement learning", my first understanding was that it learned a single policy to optimize all rewards together, and I realized my mistake only when reaching eq. 1.
- The "feature control" idea is not validated empirically (the preliminary experiment in Fig. 5 is far from convincing as it only seems to help slightly initially). I like that idea but I am worried by the fact the task is changing during learning, since the extracted features are being modified. There might be stability / convergence issues at play here.
- Since as you mentioned, "the performance of our agents is still steadily improving", why not keep them going to see how far they go? (at least the best ones)
- Why aren't the auxiliary tasks weight parameters (the lambda_*) hyperparameters to optimize? Were there any experiments to validate that using 1 was a good choice?
- Please mention the fact that auxiliary tasks are not trained with "true" Q-Learning since they are trained off-policy with more than one step of empirical rewards (as discussed in the OpenReview comments)
Minor stuff:
- "Policy gradient algorithms adjust the policy to maximise the expected reward, L_pi = -..." => that's actually a loss to be minimized
- In eq. 1 lambda_c should be within the sum
- Just below eq. 1 rt^(c) should be rt+k^(c)
- Figure 2 does not seem to be referenced in the text, also Figure 1(d) should be referenced in 3.3
- "the features discovered in this manner is shared" => are shared
- The text around eq. 2 refers to the loss L_PC but that term is not defined and is not (explicitly) in eq. 2
- Please explain what "Clip" means for dueling networks in the legend of Figure 3
- I would have liked to see more ablated versions on Atari, to see in particular if the same patterns of individual contribution as on Labyrinth were observed
- In the legend of Figure 3 the % mentioned are for Labyrinth, which is not clear from the text.
- In 4.1.2: "Figure 3 (right) shows..." => it is actually the top left plot of the figure. Also later "This is shown in Figure 3 Top" should be Figure 3 Top Right.
- "Figure 5 shows the learning curves for the top 5 hyperparameter settings on three Labyrinth navigation levels" => I think it is referring to the left and middle plots of the figure, so only on two levels (the text above might also need fixing)
- In 4.2: "The left side shows the average performance curves of the top 5 agents for all three methods the right half shows..." => missing a comma or something after "methods"
- Appendix: "Further details are included in the supplementary materials." => where are they?
- What is the value of lambda_PC? (=1 I guess?)
[Edit] I know some of my questions were already answered in Comments, no need to re-answer them