This paper investigates the  modeling of graph sequences . Authors propose Graph Convolutional Recurrent Networks (GRCN)  that extends convLSTM  (Shi et al. 2015) for data having an unregular graph structure at each timestep. They replace the 2D convolution with a graph convolutional operator from (Defferrad et al., 2016).
Authors propose two variations of the GRCN model. In Model 1,  the graph convolution is only applied on the input data. In Model 2, the graph convolution  is applied on both  input data and the previous hidden states. They evaluate their approaches on two different tasks, video generation using the movingMNIST dataset and world-level language modelling using Penntreebank.
On movingMNIST authors show that their GRCN 2 improves upon convLSTM. However, they evaluate only with one-layer convLSTM, while Shi et al. report better results with 3 layers (also not as good as  GRCN) . It would be nice to evaluate GCRCN in that setting as well.
While the authors show an improvement of GRCN relatively to convLSTM, GRCN on this task seems relatively weak compared to recent works such as the Video Pixel Networks (Kalchbrenner et al., 2016). It contradicts the claim that "Model 2 has shown good performance in the case of video prediction" in the conclusion.
For the Penntreebank experiments, author compares  their model 1 with FC-LSTM, with or without dropout. However, the results in (Zaremba et al., 2014) still seems different than the one reported here. In (Zaremba et al., 2014), they  reports a test perplexity of 78.4 for the large regularized LSTM in their table 1 which outperforms the score of the GRCN. Also, following works such as variational dropout or zoneout have since improve upon Zaremba results. Is there some differences in the experimental setting?  It would be nice to have results that are directly comparable to previous work.
Pros:
- Interesting model, 
Cons:
- Overall, the proposed contribution is relatively incremental compared to (Shi et al. 2015) and (Defferrad et al., 2016). 
- Weak results of GRCN relatively to previous works in the experiments, that do not convince of the GRCN advantages.