This paper compares the performance, in terms of sensitivity to perturbations, of multilayer neural networks to human vision.  In many of the tasks tested, multilayer neural networks exhibit similar sensitivities as human vision.  
From the tasks used in this paper one may conclude that multilayer neural networks capture many properties of the human visual system.  But of course there are well known adversarial examples in which small, perceptually invisible perturbations cause catastrophic errors in categorization, so against that backdrop it is difficult to know what to make of these results.  That the two systems exhibit similar phenomenologies in some cases could mean any number of things, and so it would have been nice to see a more in depth analysis of why this is happening in some cases and not others.  For example, for the noise perturbations described in the the first section, one sees already that conv2 is correlated with human sensitivity.  So why not examine how the first layer filters are being combined to produce this contextual effect?  From that we might actually learn something about neural mechanisms.
Although I like and am sympathetic to the direction the author is taking here, I feel it just scratches the surface in terms of analyzing perceptual correlates in multilayer neural nets.