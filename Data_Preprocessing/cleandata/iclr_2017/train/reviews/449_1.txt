Authors claim that the reviewers reflect "narrow view" of on the number of parameters and fractions of accuracy and  "complete focus on engineering" etc . Authors claims about focusing on engineering are open to debate as only the technicality in the paper is the fractal network architecture with intuitive claims. One can either formulate the system with rigorous analysis with clear assumptions (which would require satisfactory theoretical analysis and relatively small scale empirical sanity check without heavy experiments) or propose a comprehensive empirical analysis by designing careful experiments to support intuitive claims. Therefore, as the approach clearly does not belong to the first category,  such an approach needs strong experimental evidence to support intuitive claims without a doubt.  The rebuttal lists unsatisfactory answers with somehow manipulative arguments about narrow reviewing/ dates of the baselines( which will be discussed below).. 
Authors state in the paper that " In experiments, fractal networks match the state-of-the-art performance held by residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks" and "Fractal architectures are arguably the simplest means of satisfying this requirement, and match or exceed residual networks in experimental performance". Therefore the main support behind all the claims is that fractal network match (or exceed) performance of state of art residual networks, indeed the empirical study only compares accuracy to some baselines without designing empirical analysis for the claims about differences to resnets (or ensemble explanation of resnet by Veit et al(NIPS 2016) ). 
However even this comparison lacks many results as explained in my review.   Therefore the expectation of the fair comparison is completely reasonable and it is not about only focusing on fractions of accuracy or number of parameters. As a good example, Veit et al demonstrates systematical empirical support for their claims about analyzing residual networks (which seems to be on arxiv since May but still not cited on paper ). Table 4 and Figure 3 provide good preliminary sanity check but compares only to plain networks, do not support claims about differences between residual and fractal networks.
The paper claims that the fractal network scales to "ultra" deep networks, however authors can not report results on dozens of layers. Authors also claim that extra depth may slow training (not clear how much) but does not decrease accuracy but this is not clear as there are no results for dozens of layers, also in Table 3, error increases as depth increase to 160 layers.  Number of parameters of the proposed architecture is significantly greater than the state of art resnet variants as I explained in the review but the authors argue that it is slightly more parameters. 
As an answer to lack of comparison to DenseNet, authors argue in rebuttal that DenseNet cites FractalNet but this can not be a reason for the lack of comparison as DenseNet was published in August and well-known as holding state of art results as a resnet variant. 
Authors state in rebuttal that "Many of the variants only reported results on CIFAR/SVHNâ€¦ it is a bit difficult to have already compared to results that did not exist at submission time." .  However there were clearly published ones, for example two results by Huang et al, 2016b on july (arxiv) ..
Regarding the "simplifying power" claim of authors.  It is not very convincing that it is simplifying. how can it simplify with a harder training procedure with many parameters that can not scale as good as baselines?
In essence, my evaluation did not change as rebuttal did not provide satisfactory clarification or improvement.
This paper proposes a new architecture that does not explicitly use residuals but constructs an architecture that is composed of networks with fractal structure by using expand and join operations. Using the fractal architecture,  authors argue and try to demonstrate that the large nominal network depth with many short paths is the key for 'training 'ultra-deep" networks while residuals are incidental.
The main bottleneck of this paper is that number of parameters needed for the FractalNet is significantly higher than the baselines which makes it hard to scale to ''ultra-deep" networks.  Authors replied that Wide ResNets also require many parameters but this is not the case for ResNet and other ResNet variants. ResNet and ResNet with Stochastic depth scales to depth of 110 with 1.7M parameters and to depth of 1202 with 10.2M parameters which is much less than the number of parameters for depths of 20 and 40 in Table 1(Huang et al, 2016a).   It is not clear whether FractalNet can perform better than these depths with a reasonable computation. Authors report less parameters for 40 layers but this scaling trick is not validated for other depths including depth 20 in Table 1. On the other hand, the number of parameters for 40 layers with scaling trick is clearly still large compared to most of the baselines. Unsatisfactory comparison to these baselines makes the claims of authors unconvincing.
Authors also claim that drop-path to provide improvement compared to layer dropping procedure in Huang et al, 2016b however the results show that the empirical gain of this specific regularization disappears when well-known data augmentation techniques applied. Therefore the empirical effectiveness of drop-path is not convincing too.
DenseNets (Huang et al, 2016a) should be also included in the comparison since it outperforms most of the state of art Res Nets on both CIFAR10 and ImageNet and more importantly outperforms the proposed FractalNet significantly and it requires significantly less computation. 
Table 1 has Res-Net variants as baselines however Table 2 has only ResNet.  Therefore ImageNet comparison only shows that one can run FractalNet on ImageNet and can perform comparably well to ResNet which is not a satisfactory result given the improvements of other baselines over ResNet.  In addition, there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.
Also, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis. 
Although the paper attempts to explore many interesting intuitive directions using the proposed architecture, the empirical results are not support the given claims and the large number of parameters makes the model restrictive in practice hence the contribution does not seem to be significant. 
Pros:
Provides an interesting architecture compared to ResNet and its variants and investigates the differences to residual networks which can stimulate some other promising analysis
cons:
     -    Number of parameters are very large compared to baselines that can have even much higher depths with smaller number of parameters
The claims are intuitive but not supported well with empirical evidence
Path regularization does not yield improvement when the data augmentation is used
     -     The empirical results do not show whether the method is promising for "ultra-deep" networks 
- Number of parameters needed is dramatically increasing with respect to competitors for state of art results. Can you also give comparison on depth, parameters,training times to Resnet and its variants in Table 1 and Table 2 and elaborate on this?
 Did you also try to decrease the number of parameters of 20 layers ?
-In Table 1, why didn't you report any result for no augmentation for 40 layers?
- Why did you set B values and corresponding layers and C values to given numbers, what else have you tried and what did you get in terms of training efficiency and misclassification error for getting the results on Table 1 and Table 2? Are the results stable or very sensitive according to these parameters? 
- Can you elaborate on the cases where you could not getter better results than best res-net variant in Table 1?