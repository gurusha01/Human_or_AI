This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.
Reducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:
1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.
2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. 
3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.
BTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., 
Xue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369).