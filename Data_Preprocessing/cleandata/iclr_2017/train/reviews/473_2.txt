This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax.
Experiments on PTB shows significant improvement.
The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting.
I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted.
Have you considered using character or sub-word units in that context?