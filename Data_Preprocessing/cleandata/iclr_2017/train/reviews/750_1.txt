Summary
===
This paper extends and analyzes the gradient regularizer of Hariharan and
Girshick 2016. In that paper a regularizer was proposed which penalizes
gradient magnitudes and it was shown to aid low-shot learning performance.
This work shows that the previous regularizer is equivalent to a direct penalty
on the magnitude of feature values weighted differently per example.
The analysis goes to to provide two examples where a feature penalty
favors a better representation. The first example addresses the XOR
problem, constructing a network where a feature penalty encourages
a representation where XOR is linearly separable.
The second example analyzes a 2 layer linear network, showing improved stability
of a 2nd order optimizer when the feature penalty is added.
One last bit of analysis shows how this regularizer can be interpreted as
a Gaussian prior on both features and weights. Since the prior can be
interpreted as having a soft whitening effect, the feature regularizer
is like a soft version of Batch Normalization.
Experiments show small improvements on a synthetic XOR test set.
On the Omniglot dataset feature regularization is better than most baselines,
but is worse than Moment Matching Networks. An experiment on ImageNet similar
to Hariharan and Girshick 2016 also shows effective low-shot learning.
Strengths
===
* The core proposal is a simple modification of Hariharan and Girshick 2016.
* The idea of feature regularization is analyzed from multiple angles
both theoretically and empirically.
* The connection with Batch Normalization could have broader impact.
Weaknesses
===
* In section 2 the gradient regularizer of Hariharan and Girshick is introduced.
While introducing the concept, some concern is expressed about the motivation:
"And it is not very clear why small gradients on every sample produces
good generalization experimentally." This seems to be the central issue to me.
The paper details some related analysis, it does not offer a clear answer to
this problem.
* The purpose and generality of section 2.1 is not clear.
The analysis provides a specific case (XOR with a non-standard architecture)
where feature regularization intuitively helps learn a better representation.
However, the intended take-away is not clear.
The take-away may be that since a feature penalty helps in this case it
should help in other cases. I am hesitant to buy that argument because of the
specific architecture used in this section. The result seems to rely on the
choice of an x^2 non-linearity, which is not often encountered in recent neural
net literature.
The point might also be to highlight the difference between a weight
penalty and a feature penalty because the two seem to encourage
different values of b in this case. However, there is no comparison to
a weight penalty on b in section 2.1.
* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy
loss. A more general class of losses for which eq. 3 holds is not provided. This
should be made clear before eq. 3 is presented.
* The Omniglot and ImageNet experiments are performed with Batch Normalization,
yet the paper points out that feature regularization may be similar in effect
to Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are
clear improvements over that baseline, the proposed regularizer has a clear
additional positive effect. However, results should be provided without
Batch Norm so a 1-1 comparison between the two methods can be performed.
* The ImageNet experiment should be more like Hariharan and Girshick.
In particular, the same split of classes should be used (provided in
the appendix) and performance should be measured using n > 1 novel examples
per class (using k nearest neighbors).
Minor:
* A brief comparison to Matching Networks is provided in section 3.2, but the
performance of Matching Networks should also be reported in Table 1.
* From the approach section: "Intuitively when close to convergence, about half
of the data-cases recommend to update a parameter to go left, while
the other half recommend to go right."
Could the intuition be clarified? There are many directions in high
dimensional space and many ways to divide them into two groups.
* Is the SGM penalty of Hariharan and Girshick implemented for this paper
or using their code? Either is acceptable, but clarification would be appreciated.
* Should the first equal sign in eq. 13 be proportional to, not equal to?
* The work is dense in nature, but I think the presentation could be improved.
In particular, more detailed derivations could be provided in an appendix
and some details could be removed from the main version in order to increase
focus on the results (e.g., the derviation in section 2.2.1).
Overall Evaluation
===
This paper provides an interesting set of analyses, but their value is not clear.
There is no clear reason why a gradient or feature regularizer should improve
low-shot learning performance. Despite that, experiments support that conclusion,
the analysis is interesting by itself, and the analysis may help lead to a
clearer explanation.
The work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.
Some points are not completely clear, as mentioned above.