The paper proposes a methodology for morphing a trained network to different architecture without having to retrain from scratch.
The manuscript reads well and the description is easy to follow.
However, the results are not very convincing as the selected baselines are considerably far from the state of the art.
The paper should include comparisons with state of the art, for example wide residual networks.
Tables should also report number of parameters for each architecture, this would help fair comparison.