This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7.
It is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done.
The experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1.
Minor comments:
Third line in abstract: where model -> where the model
Second line in section 7: into space -> into the space
Shouldn't the RHS in Eq 3.5 be \sum \tilde{y{t,i}}(\frac{\hat{y}t}{\tilde{y{t,i}}} - ei) ?