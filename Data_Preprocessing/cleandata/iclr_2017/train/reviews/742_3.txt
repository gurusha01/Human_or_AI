Summary of the paper:
Authors study in this paper quantities related to the expressivity of neural networks.The analysis is done for a random network. authors define the 'trajectory length' of a one dimensional trajectory as the length of the trajectory as the points (in a m- dimensional space) are embedded by layers of the network. They provide growth factors as function of hidden units k, and number of layers d.  the growth factor is exponential in the number of layers. Authors relates this trajectory length to authors quantities : 'transitions','activation patterns ' and 'Dichotomies'. 
As a consequence of this study authors suggest that training only  earlier layers in the network  leads higher accuracy then just training later layers. Experiments are presented on MNIST and CIFAR10.
Clarity:
The  paper is a little hard to follow, since  the motivations are not clear in the introduction and the definitions across the paper are not clear. 
Novelty:
Studying the trajectory length as function of transforming the data by a multilayer network is   new and interesting idea. The relation to transition numbers is in term of the growth factor, and not as a quantity to quantity relationship. Hence it is hard to understand what are the implications.
Significance:
The geometry of the input set (of dimension m)  shows up only weakly in the activation patterns analysis.  The trajectory study should tell us how the network organizes the input set. As observed in the experiments the network becomes contractive/selective as we train the network. It would be interesting to study those phenomenas using this trajectory length , as a measure for disentangling nuisance factors ( such as invariances etc.). In the supervised setting the network need not to be contractive every where , so it needs to be selective to the class label, a  theoretical study of the selectivity and contraction using the trajectory length would be more appealing.
Detailed comments:
Theorem 1:
- As raised by reviewer one the definition of a one dimensional input trajectory is missing. 
- What does theorem 1 tells us about the design and the architecture to use in neural networks as promised in the introduction is not clear. The connection to transitions in Theorem 2 is rather weak. 
Theorem 2:
- in the proof of theorem 2 it not clear what is meant by T and t. Notations are confusing, the expectation is taken with respect to which weight: is it W{d+1} or (W{d+1} and W{d})? I understand you don't want to overload notation but maybe E{d+1} can help keeping track. I don't see how the recursion is applied if T and t in it, have different definitions. seems T{d+1} for you is a random variable and t{d} is fixed. Are you fixing Wd and then looking at W{d+1} as  random?
- In the same proof:  the recursion  is for d>1  ? your analysis is for W \in R^{k\times k}, you don't not study the W \in \mathbb{R}^{k\times m}. In this case you can not assume assume that |z^(0)|=1.
- should d=1, be analyzed alone to know how it scales with m?
Theorem 4 in main text:
- Is the proof missing? or Theorem 4 in the main text is Theorem 6 in the appendix?
Figures 8 and 9:
- the trajectory length reduction in the training isn't that just the network becoming contractive to enable mapping the training points to the labels? See for instance  on contraction in deep networks