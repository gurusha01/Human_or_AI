SUMMARY.
This paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.
The proposed model combines two well-know neural network architectures match-lstm and pointer nets.
First the passage and the questions are encoded with a unidirectional LSTM.
Then the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.
For each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.
The same process is done in the opposite direction with a backward lstm.
The final representation is a concatenation of the two lstms.
As a decoded a pointer network is used.
The authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.
The proposed model is tested on the Stanford Question Answering Dataset.
An ensemble of the proposed model achieves performance close to state-of-the-art models.
----------
OVERALL JUDGMENT
I think the model is interesting mainly because of the use of pointer networks as a decoder.
One thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.
The analysis of the model is interesting and insightful.
The sharing of the code is good.