Paper summary
This paper develops a generalization of dropout using information theoretic
principles. The basic idea is that when learning a representation z of input x
with the aim of predicting y, we must choose a z such that it carries the least
amount of information about x, as long as it can predict y. This idea can be
formalized using the Information Bottleneck Lagrangian. This leads to an
optimization problem which is similar to the one derived for variational
dropout, the difference being that Information dropout allows for a scaling
factor associated with the KL divergence term that encourages noise. The amount
of noise being added is made a parameterized function of the data and this
function is optimized along with the rest of the model. Experimental results on
CIFAR-10 and MNIST show (small) improvements over binary dropout.
Strengths
- The paper highlights an important conceptual link between probabilistic
  variational methods and information theoretic methods, showing that dropout
can be generalized using both formalisms to arrive at very similar models.
- The presentation of the model is excellent.
- The experimental results on cluttered MNIST are impressive.
Weaknesses
- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless
  the axis label is a typo). It is not clear why the test set was not used. This
makes it hard to compare to results reported in Springenberg et al, as well as
other results in literature.
Quality
The theoretical exposition is high quality. Figure 2 gives a nice qualitative
assessment of what the model is doing. However, the experimental results
section can be made better, for example, by matching the results on CIFAR-10 as
reported in Springenberg et al. and trying to improve on those using information
dropout.
Clarity
The paper is well written and easy to follow.
Originality
The derivation of the information dropout optimization problem using IB
Lagrangian is novel. However, the final model is quite close to variational
dropout.
Significance
This paper will be of general interest to researchers in representation learning
because it highlights an alternative way to think about latent variables (as
information bottlenecks). However, unless the model can be shown to achieve
significant improvements over simple dropout, its wider impact is likely to be
limited.
Overall
The paper presents an insightful theoretical derivation and good preliminary
results. The experimental section can be improved.
Minor comments and suggestions -
- expecially -> especially
- trough -> through
- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).
- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those
for Figure 3(b) as well.
- Please consider comparing Figure 2 with the activity map of a standard CNN
  trained with binary dropout, so we can see if similar filtering out is
happening there already.