he authors provide an interesting, computational-complexity-driven approach for efficient softmax computation for language modeling based on GPUs. An adaptive softmax approach is proposed based on a hierarchical model. Dynamic programming is applied to optimize the structure of the hierarchical approach chosen here w.r.t. computational complexity based on GPUs. 
However, it remains unclear, how robust the specific configuration obtained from dynamic programming is w.r.t. performance/perplexity. Corresponding comparative results with perplexity-based clustering would be desirable. Especially, in Sec. 5, Paragraph Baselines, and Table 1, respectively, it would be interesting to see a result on HSM(PPL) (cf. Zweig et al. 2013).
AFAIK, the first successful application of an LSTM-based language model for large vocabulary was published by Sundermeyer et al. 2012 (see below), which is missing in the sumary of prior work on the bottom of p. 3.
Mainly, the paper is well written and accessible, though notation in some cases should be improved, see detailed comments below.
Prior work on LSTM language modeling: 
 - Sundermeyer et al.: LSTM Neural Networks for Language Modeling, Interspeech, pp. 194-197, 2012.
Notation:
 - use of g(k) vs. g(k,B,d): g(k) should be clearly defined (constant B and d?)
 - notation should not be reused (B is matrix in Eq. (3), and batch size in Sec. 4.1).
 - notation p{i+j} (Eq. (10) and before) is kind of misleading, as p{i+j} is not the same as p_{(i+j)}
Minor comments:
 - p. 1, item list at bottom, first item: take -> takes
 - p. 5, second paragraph: will then contained -> will then contain
 - p. 5, third paragaph: to associated -> to associate
 - Sec. 4.3, first paragraph: At the time being -> For the time being
 - below Eq. (9): most-right -> right-most
 - below Eq. (10): the second term of this equation -> the second term of the right-hand side of this equation
 - p. 6, second to last line: smaller that the -> smaller than the
 - p. 7, Sec. 5, itemize, first item: 100 millions -> 100 million
 - p. 8, last sentence: we are the -> ours is the
g() is used in a number of different configurations with different variables. Please provide corresponding definitions (e.g. g(k,B,d) vs. g(k) same apart from keeping B and d constant in g(k)?).
In many places, references/citations seem to be erroneous/missing (cf. "(?)" in the text). Please provide these.