This paper proposes the Neural Graph Machine that adds in graph regularization on neural network hidden representations to improve network learning and take the graph structure into account.  The proposed model, however, is almost identical to that of Weston et al. 2012.
As the authors have clarified in the answers to the questions, there are a few new things that previous work did not do:
1. they showed that graph augmented training for a range of different types of networks, including FF, CNN, RNNs etc. and works on a range of problems.
2. graphs help to train better networks, e.g. 3 layer CNN with graphs does as well as than 9 layer CNNs
3. graph augmented training works on a variety of different kinds of graphs.
However, all these points mentioned above seems to simply be different applications of the graph augmented training idea, and observations made during the applications.  I think it is therefore not proper to call the proposed model a novel model with a new name Neural Graph Machine, but rather making it clear in the paper that this is an empirical study of the model proposed by Weston et al. 2012 to different problems would be more acceptable.