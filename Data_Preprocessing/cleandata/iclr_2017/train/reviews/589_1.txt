The paper proposes to combine graph convolution with RNNs to solve problems in which inputs are graphs. The two key ideas are: (i) a graph convolutional layer is used to extract features which are then fed in an RNN, and (ii) matrix multiplications are replaced by graph convolution operations. (i) is applied to language modelling, yielding lower perplexity on Penn Treebank (PTB) compared with LSTM. (ii) outperformed LSTM + CNN on the moving-MNIST.
Both two models/ideas are actually trivial and in line with the current trend of combining different architectures. For instance, the idea of replacing matrix multiplications by graph convolution is a small extension for Shi et al.
Regarding to the experiment on PTB (section 5.2), I'm skeptical about the way the experiment carried out. The reason is that, instead of using the given development set to tune the models, the authors blindly used an available configuration which is for a different model.
Pros: 
- good experimental results
Cons:
- ideas are quite trivial 
- the experiment on PTB was carried out improperly