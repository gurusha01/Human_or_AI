SUMMARY: This paper describes a set of experiments evaluating techniques for
training a dialogue agent via reinforcement learning. A
standard memory network architecture is trained on both bAbI and a version of
the WikiMovies dataset (as in Weston 2016, which this work extends). Numerous
experiments are performed comparing the behavior of different training
algorithms under various experimental conditions.
STRENGTHS: The experimentation is comprehensive. I agree with the authors that
these results provide additional useful insight into the performance of the
model in the 2016 paper (henceforth W16).
WEAKNESSES: This is essentially an appendix to the earlier paper. There is no
new machine learning content. Secondarily, the paper seems to confuse the
distinction between "training with an adaptive sampling procedure" and "training
in interactive environments" more generally. In particular, no comparisons are
presented to the to the experiments with a static exploration policy presented
in W16, when the two training can & should be evaluated side-by-side.
The only meaningful changes between this work and W16 involve simple
(and already well-studied) changes to the form of this exploration policy.
My primary concern remains about novelty: the extra data introduced here is
welcome enough, but probably belongs in a *ACL short paper or a technical
report. This work does not stand on its own, and an ICLR submission is not an
appropriate vehicle for presenting it.
"REINFORCEMENT LEARNING"
[Update: concerns in this section have been addressed by the authors.]
This paper attempts to make a hard distinction between the reinforcement
learning condition considered here and the ("non-RL") condition considered in
W16. I don't think this distinction is nearly as sharp as it's
made out to be. 
As already noted in Weston 2016, the RBI objective is a special case of vanilla
policy gradient with a zero baseline and off-policy samples. In this sense the
version of RBI considered in this paper is the same as in W16, but with a
different exploration policy; REINFORCE is the same objective with a nontrivial
baseline. Similarly, the change in FP is only a change to the sampling policy.
The fixed dataset / online learning distinction is not especially meaningful
when the fixed dataset consists of endless synthetic data.
It should be noted that some variants of the exploration policy in W16 provide a
stronger training signal than is available in the RL "from scratch" setting
here: in particular, when $\pi_acc = 0.5$ the training samples will feature much
denser reward. However, if I correctly understand Figures 3 and 4 in this paper,
the completely random initial policy achieves an average reward of ~0.3 on bAbI
and ~0.1 on movies---as good or better than the other exploration policies in
W16!
I think this paper would be a lot clearer if the delta from W16 were expressed
directly in terms of their different exploration policies, rather than trying to
cast all of the previous work as "not RL" when it can be straightforwardly
accommodated in the RL framework.
I was quite confused by the fact that no direct comparisons are made to the
training conditions in the earlier work. I think this is a symptom of the
problem discussed above: once this paper adopts the position that this work is
about RL and the previous work is not, it becomes possible to declare that the
two training scenarios are incomparable. I really think this is a mistake---to
the extent that the off-policy sample generators used in the previous paper are
worse than chance, it is always possible to compare to them fairly here.
Evaluating everything in the "online" setting and presenting side-by-side
experiments would provide a much more informative picture of the comparative
behavior of the various training objectives.
ON-POLICY VS OFF-POLICY
Vanilla policy gradient methods like the ones here typically can't use
off-policy samples without a little extra hand-holding (importance sampling,
trust region methods, etc.). They seem to work out of the box for a few of the
experiments in this paper, which is an interesting result on its own. It would
be nice to have some discussion of why that might be the case.
OTHER NOTES
- The claim that "batch size is related to off-policy learning" is a little
odd. There are lots of on-policy algorithms that require the agent to collect a
large batch of transitions from the current policy before performing an 
(on-policy) update.
- I think the experiments on fine-tuning to human workers are the most exciting
part of this work, and I would have preferred to see these discussed (and
explored with) in much more detail rather than being relegated to the
penultimate paragraphs.