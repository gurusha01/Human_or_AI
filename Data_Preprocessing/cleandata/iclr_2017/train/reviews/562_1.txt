This paper proposes an extension of the GAN framework known as GAP whereby multiple generators and discriminators are trained in parallel. The generator/discriminator pairing is shuffled according to a periodic schedule.
Pros:
+ The proposed approach is simple and easy to replicate.
Cons:
- The paper is confusing to read.
- The results are suggestive but do not conclusively show a performance win for GAP.
The main argument of the paper is that GAP leads to improved convergence and improved coverage of modes. The coverage visualizations are suggestive but there still is not enough evidence to conclude that GAP is in fact improving coverage. And for convergence it is difficult to assess the effect of GAP on the basis of learning curves. The proposed GAM-II metric is circular in that model performance depends on the collection of baselines the model is being compared with. Estimating likelihood via AIS seems to be a promising way to evaluate, as does using the Inception score.
Perhaps a more systematic way to determine GAP's effect would be to set up a grid search of hyperparameters and train an equal number of GANs and GAP-GANs for each setting. Then a histogram over final Inception scores or likelihood estimates of the trained models would help to show whether GAP tended to produce better models. Overall the approach seems promising but there are too many open questions regarding the paper in its current form.
* Section 2: "Remark that when..." => seems like a to-do.
* Section A.1: The proposed metric is not described in adequate detail.