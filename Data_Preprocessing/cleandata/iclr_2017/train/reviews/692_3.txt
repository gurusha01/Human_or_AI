This paper presents a hierarchical attention-based method for document classification. 
The main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). 
A simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). 
Experiments on three datasets are presented, however the results are mostly not state-of-the-art.
I think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. 
Why is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? 
I noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?
You should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)
I also suggest removing phrases such as "Learning to Understand" when presenting their model.
Overall, I think that this submission is a better fit for the workshop.
Minor comments:
- gloal -> global
- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings.