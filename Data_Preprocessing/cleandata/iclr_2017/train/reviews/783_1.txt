This paper was easy to read, the main idea was presented very clearly.
The main points of the paper (and my concerns are below) can be summarized as follows:
1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a "MPI_Reduce" would be used (even if we wait for the slowest guy)?
2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.
3.they propose to take gradient from the first "N" workers out of "N+b" 
workers available. My concern here is that they focused only on the 
workers, but what if the "parameter server" will became to slow? What 
if the parameter server would be the bottleneck? How would you address 
this situation? But still if the number of nodes (N) is not large, and 
the deep DNN is used, I can imagine that the communciation will not 
take more than 30% of the run-time.
My largest concern is with the experiments. Different batch size 
implies that different learning rate should be chosen, right? How did 
you tune the learning rates and other parameters for e.g. Figure 5 you 
provide some formulas in (A2) but clearly this can bias your Figures, 
right? meaning, that if you tune "\gamma, \beta" for each N, it could 
be somehow more representative? also it would be nicer if you run the 
experiment many times and then report average, best and worst case 
behaviour. because now it can be just coinsidence, right?