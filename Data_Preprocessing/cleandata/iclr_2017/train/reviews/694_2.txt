- Proven again that end to end training with deep networks gives large
gains over traditional hybrid systems with hand crafted features. The results 
are very nice for the small vocabulary grammar task defined by the GRID corpus. The engineering here is clearly very good, will be interesting to see the performance on large vocabulary LM tasks. Comparison to human lip reading performance for conversational speech will be very interesting here.
- Traditional AV-ASR systems which apply weighted audio/visual posterior fusion reduce to pure lip reading when all the weight is on the visual, there are many curves showing performance of this channel in low audio SNR conditions for both grammar and LM tasks.
- Traditional hybrid approaches to AV-ASR are also sentence level sequence trained with fMPE/MPE/MMI etc. objectives (see old references), so we cannot say here that this is the first sentence-level objective for lipreading model (analogous to saying there was no sequence training in hybrid LVCSR ASR systems before CTC).