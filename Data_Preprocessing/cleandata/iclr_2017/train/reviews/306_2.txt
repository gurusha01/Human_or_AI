This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters. The idea is original and important for research related to transfer learning. The paper has a clear structure, but clarity could be improved at some points.
Pros:
- An interesting and feasible approach to meta-learning
- Competitive results and proper comparison to state-of-the-art
- Good recommendations for practical systems
Cons:
- The analogy would be closer to GRUs than LSTMs
- The description of the data separation in meta sets is hard to follow and could be visualized
- The experimental evaluation is only partly satisfying, especially the effect of the parameters of it and ft would be of interest
- Fig 2 doesn't have much value
Remarks:
- Small typo in 3.2: "This means each coordinate has it" -> its
> We plan on releasing the code used in our evaluation experiments.
This would certainly be a major plus.