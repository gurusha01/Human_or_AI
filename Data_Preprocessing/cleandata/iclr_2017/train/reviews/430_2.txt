Interesting paper which proposes jointly learning automatic segmentation of words to sub words and their acoustic models.
Although the training handles the word segmentation as hidden variable which depends also on the acoustic representations, during the decoding only maximum approximation is used.
The authors present nice improvements over character based results, however they did not compare results with word segmentation which does not assume the dependency on acoustic.
Obviously, only text based segmentation would result in two (but simpler) independent tasks. In order to extract such segmentation several publicly open tools are available and should be cited. Some of those tools can also exploit the unigram probabilities of the words to perform their segmentations.
It looks that the improvements come from the longer acoustical units - longer acoustical constraints which could lead to less confused search -, pointing towards full word models. In another way, less tokens are more probable due to less multiplication of probabilities. As a thought experiment for an extreme case: if all the possible segmentations would be possible (mixture of all word fragments, characters, and full-words), would the proposed model use word fragments at all? (WSJ is a closed vocabulary task). It would be good to show that the sub word model could outperform even a full-word model (no segmentation).
Your model estimates p(z_t|x,z