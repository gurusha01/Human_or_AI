This paper proposes to initialize the weights of a deep neural network layer-wise with a marginal Fisher analysis model, making use of potentially the similarity metric.
 
Pros: 
There are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.
Cons:
lacking baseline such as discriminatively trained convolutional network on standard dataset such as CIFAR-10.
It is also unclear how costly in computation to compute the association matrix A in equation 4.
This is an OK paper, where a new idea is proposed, and combined with other existing ideas such as greedy-layerwise stacking, dropout, and denoising auto-encoders.
However, there have been many papers with similar ideas perhaps 3-5 years ago, e.g. SPCANet. 
Therefore, the main novelty is the use of marginal Fisher Analysis as a new layer. This would be ok, but the baselines to demonstrate that this approach works better is missing. In particular, I'd like to see a conv net or fully connected net trained from scratch with good initialization would do at these problems.
To improve the paper, the authors should try to demonstrate without doubt that initializing layers with MFA is better than just random weight matrices.