This paper proposes a simple but effective extension to reinforcement learning algorithms, by adding a temporal repetition component as part of the action space, enabling the policy to select how long to repeat the chosen action for. The extension applies to all reinforcement learning algorithms, including both discrete and continuous domains, as it is primarily changing the action parametrization. The paper is well-written, and the experiments extensively evaluate the approach with 3 different RL algorithms in 3 different domains (Atari, MuJoCo, and TORCS).
Here are some comments and questions, for improving the paper:
The introduction states that "all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k". This statement is too strong, and is actually disproved in the experiments — repeating an action is helpful in many tasks, but not in all tasks. The sentence should be rephrased to be more precise.
In the related work, a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the pre-review questions)
Experiments:
Can you provide error bars on the experimental results? (from running multiple random seeds)
It would be useful to see experiments with parameter sharing in the TRPO experiments, to be more consistent with the other domains, especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains. Right now, it is hard to tell if the smaller improvement is because of the nature of the task, because of the lack of parameter sharing, or something else.
The TRPO evaluation is different from the results reported in Duan et al. ICML '16. Why not use the same benchmark?
Videos only show the policies learned with FiGAR, which are uninformative without also seeing the policies learned without FiGAR. Can you also include videos of the policies learned without FiGAR, as a comparison point?
How many laps does DDPG complete without FiGAR? The difference in reward achieved seems quite substantial (557K vs. 59K).
Can the tables be visualized as histograms? This seems like it would more effectively and efficiently communicate the results.
Minor comments:
-- On the plot in Figure 2, the label for the first bar should be changed from 1000 to 3500.
-- "idea of deciding when necessary" - seems like it would be better to say "idea of only deciding when necessary"
-- "spaces.Durugkar et al." — missing a space.
-- "R={4}" — why 4? Could you use a letter to indicate a constant instead? (or a different notation)