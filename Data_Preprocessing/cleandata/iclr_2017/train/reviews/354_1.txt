This work develops a method to quickly produce an ensemble of deep networks that outperform a single network trained for an equivalent amount of time. The basis of this approach is to use a cyclic learning rate to quickly settle the model into a local minima and saving a model snapshot at this time before quickly raising the learning rate to escape towards a different minima's well of attraction. The resulting snapshots can be collected throughout a single training run and achieve reasonable performance compared to baselines and have some of the gains of traditional ensembles (at a much lower cost). 
This paper is well written, has clear and informative figures/tables, and provides convincing results across a broad range of models and datasets. I especially liked the analysis in Section 4.4.  The publicly available code to ensure reproducibility is also greatly appreciated.
I would like to see more discussion of the accuracy and variability of each snapshot and further comparison with true ensembles.
Preliminary rating:
This is an interesting work with convincing experiments and clear writing. 
Minor note:
Why is the axis for lambda from -1 to 2 in Figure 5 where lambda is naturally between 0 and 1.