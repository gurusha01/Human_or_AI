The paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks. This criterion is not clearly defined (the term \phi^t{i,j} is never defined, only \phi^ti is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context).
The paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations. The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR)
The study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions
 - "the overall sample importance is different under different epochs" => yes the norm of the gradient is expected to vary
 - "Output layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops" => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables
The question of Figure 4 is absurd "Is Sample Importance the same as Negative log-likelihood of a sample?". Of course not.
The results are very bad on CIFAR which discredits the applicability of those results.
On Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20%
Despite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples. Also the experiments are very well presented.