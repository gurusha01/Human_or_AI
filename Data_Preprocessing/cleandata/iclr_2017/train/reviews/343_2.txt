The authors propose a method for language modeling by first generating a program from a DSL, then learning the count-based parameters of that program. Pros include: The proposed method is innovative and highly different from standard LSTM-based approaches of late. The model should also be much quicker to apply at query time. Strong empirical results are obtained on modeling code, though there is some gap between the synthesis method and neural methods on the Hutter task. A detailed description of the language syntax is provided.
Cons/suggestions:
- The synthesis procedure using MCMC is left very vague, even though being able to make this procedure efficient is one of the key questions.
- The work builds on work from the PL literature; surely the related work could also be expanded and this work better put in context.
- More compact/convincing examples of human interpretability would be helpful.
Other comments
- Training time evaluation in Table 1 should give basic information such as whether training was done on GPU/CPU, CPU specs, etc.