This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.
On the surface, the proposed method seems extremely close to momentum. It would be very valuable to think of a clear diagram illustrating how it differs from momentum and why it might be better near a saddle point. The illustration of better convergence on the toy saddle example is not what I mean here—optimization speed comparisons are always difficult due to the many details and hyper parameters involved, so seeing it work faster in one specific application is not as useful as a conceptual diagram which shows a critical case where CPN will behave differently from—and clearly qualitatively better than—momentum.
Another way of getting at the relationship to momentum would be to try to find a form for Rt(f) that yields the exact momentum update. You could then compare this with the Rt(f) used in CPN.
The overly general notation $\phi(W,W)$ etc should be dropped—Eqn 8 is enough.
The theoretical results (Eqn 1 and Thm 1) should be removed, they are irrelevant until the joint density can be specified.
Experimentally, it would be valuable to standardize the results to allow comparison to other methods. For instance, recreating Figure 4 of Dauphin et al, but engaging the CPN method rather than SFN, would clearly demonstrate that CPN can escape something that momentum cannot.
I think the idea here is potentially very valuable, but needs more rigorous comparison and a clear relation to momentum and other work.