Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way.
Unfortunately, as the results show, the authors could not get better results with less parameters.
However, the proposed structure with even more number of parameters shows significant gain e.g. in LM.
The paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent.
E.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.
Could the authors provide the num. of trainable parameters for Table 6?
Probably presenting less results could also improve the readability.
Only marginal accept due to the writing style.