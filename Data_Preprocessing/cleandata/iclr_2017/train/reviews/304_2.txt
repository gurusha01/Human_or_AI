This is a very interesting and fairly easy to read paper. 
The authors present a small, yet nifty approach to make Neural Programming Interpreters significantly more powerful. By allowing recursion, NPI generalizes better from fewer execution traces.
It's an interesting example of how a small but non-trivial extension can make a machine learning method significantly more practical.
I also appreciate that the same notation was used in this paper and the original Deepmind paper. As a non-expert on this topic, it was easy to read the original paper in tandem. 
My one point of critique is that the generalization proves are a bit vague. For the numerical examples in the paper, you can iterate over all possible execution paths until the next recursive call. However, how would this approach generalize a continuous input space (e.g. the 3D car example in the original paper). It seems that a prove of generalization will still be intractable in the continuous case? 
Are you planning on releasing the source code?