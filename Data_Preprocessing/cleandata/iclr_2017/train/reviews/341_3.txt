The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).
The basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.
I would have expected to see comparison to the following methods added to Figure 3:
1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.
2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.
3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don't see the performance results from just using this procedure on the learner (to compare to Fig.3 results).
Including these results would in my view significantly enhance the impact of the paper.