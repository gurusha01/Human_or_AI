This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns,
thus helping to prune irrelevant information from the video and improve action recognition.
The work is interesting and has shown state-of-the-art results on predicting human attention on action videos.
It has also shown promise for helping action clip classification.
The paper would benefit from a discussion on the role of context in attention.
For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model?
One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair.
The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features.
Is there a way to combine the context and attention without concatenation?
The rational for concatenating the features extracted from the original clip,
and the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important' will improve performance.
The authors should also mention the current state-of-the-art results in Table 4, for comparison.
Other comments:
Abstract
- Typo: `mixed with irrelevant ...'
``Time consistency in videos ... expands the temporal domain from few frames to seconds'' - These two points are not clear, probably need a re-write.
Contributions
- 1) `The model can be trained without having to engineer spatiotemporal features' - you would need to collect training data from humans though.. 
Section 3.1
The number of fixation points is controlled to be fixed for each frame - how is this done?
In practice we freeze the layers of the C3D network to values pretrained by Tran etal.
What happens when you allow gradients to flow back to the C3D layers?
Is it not better to allow the features to be best tuned for the final task?
The precise way in which the features are concatenated needs to be clarified in section 3.4.
Minor typo:
`we added them trained central bias'