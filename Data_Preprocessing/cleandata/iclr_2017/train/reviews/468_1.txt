This paper proposes a network quantization method for compressing the parameters of neural networks, therefore, compressing the amount of storage needed for the parameters.  The authors assume that the network is already pruned and aim for compressing the non-pruned parameters. The problem of network compression is a well-motivated problem and of interest to the ICLR community. 
The main drawback of the paper is its novelty. The paper is heavily built on the results of Han 2015 and only marginally extends Han 2015 to overcome its drawbacks. It should be noted that the proposed method in this paper has not been proposed before. 
The paper is well-structured and easy to follow. Although it heavily builds on Han 2015, it is still much longer than Han 2015. I believe that there is still some redundancy in the paper. The experiments section starts on Page 12 whereas for Han 2015 the experiments start on page 5. Therefore, I believe much of the introductory text is redundant and can be efficiently cut. 
Experimental results in the paper show good compression performance compared to Han 2015 while losing very little accuracy. Can the authors mention why there is no comparison with Hang 2015 on ResNet in Table 1?
Some comments:
1) It is not clear whether the procedure depicted in figure 1 is the authors' contribution or has been in the literature.
2) In section 4.1 the authors approximate the hessian matrix with a diagonal matrix. Can the authors please explain how this approximation affects the final compression? Also how much does one lose by making such an approximation?
minor typos (These are for the revised version of the paper):
1) Page 2, Parag 3, 3rd line from the end: fined-tuned -> fine-tuned
2) Page 2, one para to the end, last line: assigned for -> assigned to
3) Page 5, line 2, same as above
4) Page 8, Section 5, Line 3: explore -> explored