This paper looks at how to train if there are significant label noise present.
This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label.
The second directly integrates out the true label and simply optimizes the p(z|x).
Pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated.
Cons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets.
- comments:
Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes?
It would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. 
Overall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.