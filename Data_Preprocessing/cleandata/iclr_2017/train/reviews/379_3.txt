The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.
Pros:
- significant speed improvements through dynamic batching
- source code provided
Cons:
- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context
- presentation/vizualisation can be improved