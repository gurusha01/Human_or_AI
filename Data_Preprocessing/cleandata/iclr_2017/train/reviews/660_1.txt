The paper introduced an extension of Adam optimizer that automatically adjust learning rate by comparing the subsequent values of the cost function during training. The authors empirically demonstrated the benefit of the Eve optimizer on CIFAR convnets, logistic regression and RNN problems.
I have the following concerns about the paper
- The proposed method is VARIANT to arbitrary shifts and scaling to the cost function.  
- A more fair comparison with other baseline methods would be using additional exponential decay learning scheduling between the lower and upper threshold of dt. I suspect 1/dt just shrinks as an exponential decay from Figure 2.
- Three additional hyper-parameters: k, K, \beta_3.
Overall, I think the method has its fundamental flew and the paper offers very limited novelty. There is no theoretical justification on the modification, and it would be good for the authors to discuss the potential failure mode of the proposed method. Furthermore, it is hard for me to follow Section 3.2. The writing quality and clarity of the method section can be further improved.