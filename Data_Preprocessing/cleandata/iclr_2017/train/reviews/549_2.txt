This paper proposes sparse coding problem with cosine-loss and integrated it as a feed-forward layer in a neural network as an energy based learning approach. The bi-directional extension makes the proximal operator equivalent to a certain non-linearity (CReLu, although unnecessary). The experiments do not show significant improvement against baselines. 
Pros: 
- Minimizing the cosine-distance seems useful in many settings where compute inner-product between features are required. 
- The findings that the bidirectional sparse coding is corresponding to a feed-forward net with CReLu non-linearity. 
Cons:
- Unrolling sparse coding inference as a feed-foward network is not new. 
- The class-wise encoding makes the algorithm unpractical in multi-class cases, due to the requirement of sparse coding net for each class. 
- It does not show the proposed method could outperform baseslines in real-world tasks.