In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the approach is not new, cf. (Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016). Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs/LSTMs. Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work. THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here. That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: "making the notion of vocabulary obsolete", whereas the authors e.g. express doubts concerning the interpretation of perplexity w/o an explicit output vocabulary. For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144
Sec. 2.3: You claim that the objective requires a finite vocabulary. This statement only is correct if the units considered are limited to full word forms. However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion. Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.
The way character embeddings are used for the output should be clarified. The description in Sec. 2.4 is not explicit enough in my view.
Concerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.
Sec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t. the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.
Sec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf. (Shaik et al. IWSLT 2013).
Sec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.
Minor comments:
p. 2, bottom: three different input layer -> three different input layers (plural)
Fig. 1: fonts within the figure are way too small
p. 3, first item below Fig. 1: that we will note WE -> that we will denote WE
Sec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)
p. 5, first paragraph: in factored way -> in a factored way
p. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list
Sec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing
In Sec. 4.2 you mention that perplexity is hard to interpret for models not using an explicit output vocabulary. When analysing open vocabulary approaches, perplexity can also be renormalized to character level, cf. e.g. Shaik et al. IWLST 2013. Did you consider this?
Also Kim et al. AAAI 2015 got the similar conclusions w.r.t. the performance of character-level embeddings and also provided a discussion with suggestions for improvements. Did you consider these?
Can you provide more details on the configuration of the NCE training?
From your notation I get that you used a feed-forward NN, can you confirm?
Can you confirm that the character-level word embedding used here is the same as in the google paper by Kim et al. AAAI 2015? It is not cited in Sec. 2.1.
Pls. define the use of the colon in the first equation of Sec. 2.1
P^H((w:H)\in D) is not defined before the second equation in Sec. 2.3. Also, in the sentence introducing this equation to refer to "this probability" - please provide an explicit reference to what probability is meant here.
Pls. define e^{out} and e^{char-out} in Sec. 2.4 - are they the same as e^{out}_w in Sec. 2.2 and e^{char} in Sec. 2.1?