This paper proposes a simple method for pruning filters in two types of architecture to decrease the time for execution.
Pros:
- Impressively retains accuracy on popular models on ImageNet and Cifar10
Cons:
- There is no justification for for low L1 or L2 norm being a good selection criteria. There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set.
- There is no direct comparison to the multitude of other pruning and speedup methods.
- While FLOPs are reported, it is not clear what empirical speedup this method gives, which is what people interested in these methods care about. Wall-clock speedup is trivial to report, so the lack of wall-clock speedup is suspect.