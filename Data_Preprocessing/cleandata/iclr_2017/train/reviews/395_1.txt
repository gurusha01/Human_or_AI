The authors propose a new software package for probabilistic programming, taking advantage of recent successful tools used in the deep learning community. The software looks very promising and has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly. The composability principles are used insightfully, and the extension of inference to HMC for example, going beyond VI inference (which is simple to implement using existing deep learning tools), makes the software even more compelling. 
However, the most important factor of any PPL is whether it is practical for real-world use cases. This was not demonstrated sufficiently in the submission. There are many example code snippets given in the paper, but most are not evaluated. The Dirichlet process mixture model example (Figure 12) is an important one: do the proposed black-box inference tools really work for this snippet? and will the GAN example (Figure 7) converge when optimised with real data? To convince the community of the practicality of the package it will be necessary to demonstrate these empirically. Currently the only evaluated model is a VAE with various inference techniques, which are not difficult to implement using pure TF.
Presentation:
* Paper presentation could be improved. For example the authors could use more signalling for what is about to be explained. On page 5 qbeta and qz are used without explanation - the authors could mention that an example will be given thereafter.
* I would also suggest to the authors to explain in the preface how the layers are implemented, and how the KL is handled in VI for example.
It will be useful to discuss what values are optimised and what values change as inference is performed (even before section 4.4). This was not clear for the majority of the paper. 
Experiments:
* Why is the run time not reported in table 1?
* What are the "difficulties around convergence" encountered with the analytical entropies? inference issues become more difficult to diagnose as inference is automated. Are there tools to diagnose these with the provided toolbox? 
* Did HMC give sensible results in the experiment at the bottom of page 8? only run time is reported. 
* How difficult is it to get the inference to work (eg HMC) when we don't have full control over the computational graph structure and sampler?
* It would be extremely insightful to give a table comparing the performance (run time, predictive log likelihood, etc) of the various inference tools on more models.
* What benchmarks do you intend to use in the Model Zoo? the difficulty with probabilistic modelling is that there are no set benchmarks over which we can evaluate and compare many models. Model zoo is sensible for the Caffe ecosystem because there exist few benchmarks a large portion of the community was working on (ImageNet for example). What datasets would you use to compare the DPMM on for example?
Minor comments:
* Table 1: I would suggest to compare to Li & Turner with alpha=0.5 (the equivalent of Hellinger distance) as they concluded this value performs best. I'm not sure why alpha=-1 was chosen here. 
* How do you handle discrete distributions (eg Figure 5)?
* x_real is not defined in Figure 7.
* I would suggest highlighting M in Figure 8.
* Comma instead of period after "rized), In" on page 8.
In conclusion I would say that the software developments presented here are quite exciting, and I'm glad the authors are pushing towards practical and accessible "inference for all". In its current form though I am forced to give the submission itself a score of 5.