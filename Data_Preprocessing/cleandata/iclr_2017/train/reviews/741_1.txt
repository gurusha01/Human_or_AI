Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. 
Authors claim that this:
1. is a very interesting finding. 
2. CNN has figured out game rules. 
3. Cross modal supervision is applicable to higher-level semantics. 
I don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only "one" stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. 
For (3), author's definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It's unclear what points authors are trying to make. 
For (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of "what do do" and "what will happen". They claim by supervising for "what will happen", the CNN can automatically learn about "what to do". This is extensively studied in the model predictive control literature. Where model is "what will happen next", and the model is used to infer a control law - "what to do". However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. 
For further analysis of what the CNN has learnt I would recommend:
(a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning).
(b) Split the data into train/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of "generalizable" features the CNN pays attention to. 
In summary, understanding why CNN's make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above).