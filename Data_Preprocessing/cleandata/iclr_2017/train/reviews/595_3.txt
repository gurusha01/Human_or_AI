This paper introduces three tricks for training deep latent variable models on sparse discrete data:
1) tf-idf weighting
2) Iteratively optimizing variational parameters after initializing them with an inference network
3) A technique for improving the interpretability of the deep model
The first idea is sensible but rather trivial as a contribution. The second idea is also sensible, but is conceptually not novel. What is new is the finding that it works well for the dataset used in this paper.
The third idea is interesting, and seems to give qualitatively reasonable results. The quantitative semantic similarity results don't seem that convincing, but I am not very familiar with the relevant literature and therefore cannot make a confident judgement on this issue.