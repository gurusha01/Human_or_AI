Summary:
This paper looks at the structure of the preimage of a particular activity at a hidden layer of a network. It proves that any particular activity has a preimage of a piecewise linear set of subspaces.
Pros:
Formalizing the geometry of the preimages of a particular activity vector would increase our understanding of networks
Cons:
Analysis seems quite preliminary, and no novel theoretical results or clear practical conclusions.
The main theoretical conclusion seems to be the preimage being this stitch of lower dimensional subspaces? Would a direct inductive approach have worked? (e.g. working backwards from the penultimate layer say?) This is definitely an interesting direction, and it would be great to see more results on it (e.g. how does the depth/width, etc affect the division of space, or what happens during training) but it doesn't seem ready yet.