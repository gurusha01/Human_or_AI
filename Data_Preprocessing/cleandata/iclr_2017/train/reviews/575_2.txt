After a second look of the paper, I am still confused what the authors are trying to achieve.
The CCA objective is not differentiable in the sense that the sum of singular values (trace norm) of T is not differentiable. It appears to me (from the title, and section 3), the authors are trying to solve this problem. However,
-- Did the authors simply reformulate the CCA objective or change the objective? The authors need to be explicit here.
-- What is the relationship between the retrieval objective and the "CCA layer"? I could imagine different ways of combining them, such as combination or bi-level optimization. And I could not find discussion about this in section 3. For this, equations would be helpful.
-- Even though the CCA objective is not differentiable in the above sense, it has not caused major problem for training (e.g., in principle we need batch training, but empirically using large minibatches works fine). The authors need to justify why the original gradient computation is problematic for what the authors are trying to achieve. From the authors' response to my question 2, it seems they still use SVD of T, so I am not sure if the proposed method has advantage in computational efficiency.
In terms of paper organization, it is better to describe the retrieval objective earlier than in the experiments. And I still encourage the authors to conduct the comparison with contrastive loss that I mentioned in my previous comments.