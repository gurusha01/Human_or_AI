This paper presents a relatively novel way to visualize the features / hidden units of a neural network and generate adversarial examples. The idea is to do gradient descent in the pixel space, from a given hidden unit in any layer. This can either be done by choosing a pair of images and using the difference in activations of the unit as the thing to do gradient descent over or just the activation itself of the unit for a given image. In general this method seems intriguing, here are some comments:
It's not clear that some of the statements at the beginning of Sec 4.1 are actually true, re: positive/negative signs and how that changes (or does not change) the class. Mathematically, I don't see why that would be the case? Moreover the contradictory evidence from MNIST vs. faces supports my intuition.
The authors use the PASS score through the paper, but only given an intuition + citation for it. I think it's worth explaining what it actually does, in a sentence or two.
The PASS score seems to have some, but not complete, correlation with L2, L\{infty} or visual estimation of how "good" the adversarial examples are. I am not sure what the take-home message from all these numbers is.
"In general, LOTS cannot produce high quality adversarial examples at the lower layers" (sec 5.2) seems false for MNIST, no?
I would have liked this work to include more quantitative results (e.g., extract adversarial examples at different layers, add them to the training set, train networks, compare on test set), in addition to the visualizations present. That to me is the main drawback of the paper, in addition to basically no comparisons with other methods (it's hard to judge the merits of this work in vacuum).
-----
EDIT after rebuttal: thanks to the authors for addressing the experimental validation concerns. I think this makes the paper more interesting, so revising my score accordingly.