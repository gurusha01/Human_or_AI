I have not read the revised version in detail yet. 
SUMMARY 
This paper studies the preimages of outputs of a feedforward neural network with ReLUs. 
PROS 
The paper presents a neat idea for changes of coordinates at the individual layers. 
CONS 
Quite unpolished / not enough contributions for a finished paper. 
COMMENTS 
- In the first version the paper contains many typos and appears to be still quite unpolished. 
- The paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a Conference paper. 
I would be happy to recommend for the Workshop track. 
- Irreversibly mixed and several other notions from the present paper are closely related to the concepts discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014]. I feel that that paper should be cited here and the connections should be discussed. In particular, that paper also contains a discussion on the local linear maps of ReLU networks. 
- I am curious about the practical considerations when computing the pre-images. The definition should be rather straight forward really, but the implementation / computation could be troublesome. 
DETAILED COMMENTS 
- On page 1 ``can easily be shown to be many to one'' in general. 
- On page 2 ``For each point x^{l+1}'' The parentheses in the superscript are missing. 
- After eq. 6 ``the mapping is unique''  is missing `when w1 and w2 are linearly independent' 
- Eq. 1 should be a vector. 
- Above eq. 3. ``collected the weights a_i into the vector w'' and bias b. Period is missing. 
- On page 2 ``... illustrate the preimage for the case of points on the lines ... respectively'' 
Please indicate which is which.  
- In Figure 1. Is this a sketch, or the actual illustration of a network. In the latter case, please state the specific value of x and the weights that are depicted. Also define and explain the arrows precisely. 
What are the arrows in the gray part? 
- On page 3 `` This means that the preimage is just the point x^{(l)}''  the points that W maps to x^{(l+1)}. 
- On page 3 the first display equation. There is an index i on the left but not on the right hand side. 
The quantifier in the right hand side is not clear. 
- ``generated by the mapping ... w^i '' subscript
- ``get mapped to this hyperplane'' to zero 
- ``remaining'' remaining from what? 
- ``using e.g. Grassmann-Cayley algebra'' 
How about using elementary linear algebra?!
- ``gives rise to a linear manifold with dimension one lower at each intersection'' 
This holds if the hyperplanes are in general position. 
- ``is complete in the input space'' forms a basis 
- ``remaining kernel'' remaining from what? 
- ``kernel'' Here kernel is referring to nullspace or to a matrix of orthonormal basis vectors of the nullspace, or to what specifically? 
- Figure 3. Nullspaces of linear maps should pass through the origin. 
- `` from pairwise intersections'' \cap 
- ``indicated as arrows or the shaded area'' this description is far from clear. 
- typos: peieces, diminsions, netork, me,