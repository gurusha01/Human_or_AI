This paper proposes the RIMs that unrolls variational inference procedure. 
The author claims that the novelty lies in the separation of the model and inference procedure, making the MAP inference as an end-to-end approach. The effectiveness is shown in image restoration experiments.
While unrolling the inference is not new, the author does raise an interesting perspective towards the `model-free' configuration, where model and inference are not separable and can be learnt jointly. 
However I do not quite agree the authors' argument regarding [1] and [2]. Although both [1] and [2] have pre-defined MAP inference problem. It is not necessarily that a separate step is required. In fact, both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig. 1(a). I believe that the implementation of both follows the same procedure as the proposed, that could be explained through Fig. 1(c). That is to say, the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters.  
Moreover, the RNN block architecture (GRU) and non-linearity (tanh) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm. This is also similar with [1] and [2].
Based on that fact, I have the similar feeling with R1 that the novelty is somewhat limited. Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen.