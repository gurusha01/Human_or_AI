SUMMARY 
This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. 
PROS 
The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. 
CONS
The main result appears to address only strongly convex univariate functions. 
SPECIFIC COMMENTS 
- Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. 
I still have not read the revision and maybe you have already addressed some of these points there. 
- The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle. I would suggest to include that paper it in the overview. 
- In Lemma 3, there is an i that should be x
- In Theorem 4, ``\tilde f'' is missing the (x). 
- Theorem 11, the lower bound always increases with L ? 
- In Theorem 11, \bf x\in [0,1]^d?