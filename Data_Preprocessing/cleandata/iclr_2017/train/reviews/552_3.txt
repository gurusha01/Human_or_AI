This is a nice proposal, and could lead to more efficient training of
recurrent nets. I would really love to see a bit more experimental evidence.
I asked a few questions already but didn't get any answer so far.
Here are a few other questions/concerns I have:
- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)
- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?
- The experiments are a bit disappointing as the number of distinct input/output
sequences were in fact very small and as noted by the authr, training
becomes unstable (I didn't understand what "success" meant in this case).
The authors point that the experiment section need to be expanded, but as
far as I can tell they still haven't unfortunately.