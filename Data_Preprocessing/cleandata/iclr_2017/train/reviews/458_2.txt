The paper introduces a new regularization term which encourages the optimizer 
to search for a flat local minimum of reasonably low loss instead of seeking a 
sharp region of a low loss. This is motivated by some empirical observations that
local minima of good generalization performance tend to have flat shape. 
To achieve this, a regularization term based on the free local energy is proposed
and the gradient of this term, which do not have tractable closed-form solution, 
is obtained by performing Monte Carlo estimation using SGLD sampler. In the 
experiments, the authors show some evidence of the flatness of good local 
minima, and also the performance of the proposed method in comparison to the
Adam optimizer. 
The paper is well and clearly written. I enjoyed reading the paper. The connection
to the concept of free energy in optimization framework seems interesting. The 
motivation of pursuing flatness is also well analyzed with a few experiments. I'm
wondering if the first term in eqn. (8) is correct. I guess it should be f(x') not f(x)?
Also, I'm wondering why the authors did not add the experiment results on RNN in
the evaluation of the performance because char-lstm for text generation was 
already used for the flatness experiments. I think adding more experiments on 
various models and applications of deep architectures (e.g., RNN, seq2seq, etc.) 
will make the author's claim more persuasive. I also found the mixed usage of the
terminology, e.g., free energy and free entropy, a bit confusing.