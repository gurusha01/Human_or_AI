Update: raised the score, because I think the arguments about adversarial examples are compelling.  I think that the paper convincingly proves that this method acts as a decent regularizer, but I'm not convinced that it's a competitive regularizer.  For example, I don't believe that there is sufficient evidence that it gives a better regularizer than dropout/normalization/etc.  I also think that it will be much harder to tune than these other methods (discussed in my rebuttal reply).  
----
Summary: If I understand correctly, this paper proposes to take the "bottleneck" term from variational autoencoders which pulls the latent variable towards a noise prior (like N(0,1)) and apply it in a supervised learning context where the reconstruction term log(p(x|z)) is replaced with the usual supervised cross-entropy objective.  
The argument is that this is an effective regularizer and increases robustness to adversarial attacks.  
Pros: 
-The presentation is quite good and the paper is easy to follow.  
-The idea is reasonable and the relationship to previous work is well described.  
-The robustness to adversarial examples experiment seems convincing, though I'm not an expert in this area.  Is there any way to compare to an external quantitative baseline on robustness to adversarial examples?  This would help a lot, since I'm not sure how the method here compares with other regularizers in terms of combatting adversarial examples.  For example, if one uses a very high dropout rate, does this confer a comparable robustness to adversarial examples (perhaps at the expense of accuracy)?  
Cons: 
-MNIST accuracy results don't seem very strong, unless I'm missing something.  The Maxout paper from ICML 2013 listed many permutation invariant MNIST results with error rates below 1%.  So the 1.13% error rate listed here doesn't necessarily prove that the method is a competitive regularizer.  I also suspect that tuning this method to make it work well is harder than other regularizers like dropout.  
-There are many distinct architectural choices with this method, particularly in how many hidden layers come before and after z.  For example, the output could directly follow z, or there could be several layers between z and the output.  As far as I can tell the paper says that p(y | z) is a simple logistic regression (i.e. one weight matrix followed by softmax), but it's not obvious why this choice was made.  Did it work best empirically?  
Other: 
-I wonder what would happen if you "trained against" the discovered adversarial examples while also using the method from this paper.  Would it learn to have a higher variance p(z | x) when presented with an adversarial example?