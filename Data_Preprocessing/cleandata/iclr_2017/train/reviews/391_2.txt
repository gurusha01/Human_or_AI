Summary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model with less storage requirement and higher inference rate. 
Pros:
Proposes a pruning method that doesn't need re-training and doesn't affect the training phase of RNN. The method achieves 90% sparsity, and hence less number of parameters.
Cons & Questions:
Judiciously choosing hyper parameters for different models and different applications wouldn't be cumbersome? In equation 1, is q the sparsity of final model? Is there a formula to know what is sparsity, number of parameters and accuracy of final model given a set of hyper parameters, before going through training? (Questions answered)
In table3, we see a trade-off between number of units and sparsity to achieve better number of parameters or accuracy, or in table5 better speed. Good, but where are the results for GRU sparse big? I mean, accuracy must be similar and still get decent compression rate and speed up. Just like RNN Sparse medium compared with RNN Dense. I can't see much advantage of pruning and getting high speed-up if you are sacrificing so much accuracy. (Issue fixed with updated data)
Why sparsity for table3 and table5 are different? In text: "average sparsity of 88%" but in table5 is 95%? Are the models used in table3 different from table5? (Issue fixed)
In introduction: "... unlike previous approaches
such as in Han et al. (2015). State of the art results in speech recognition generally require between
days and weeks of training time, so a further 3-4Ã— increase in training time is undesirable."
But, according to Han et al. (2015), "Huffman coding doesn't require training and is implemented
offline after all the fine-tuning is finished."
Both yours and Han et al. (2015) use a weight pruning technique. Intuitively, they should have similar training time for LSTM models.
Where does 3-4x extra training time comes from Han et al. (2015) but doesn't have in your approach?