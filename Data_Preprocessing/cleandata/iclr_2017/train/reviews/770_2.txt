The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.
The writing could be improved. The beginning of Section 3 in particular is hard to parse.
There have been similar efforts recently (like "Pointer Sentinel Mixture Models" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is nice.
The architecture appears sound, but the writing makes it hard to fully understand completely so I can not give a higher rating. 
Other comments:
* How to cope with the dependency on the KB? Freebase is not updated anymore so it is likely that a lot of the new unseen words in the making are not going to be in Freebase.
* What is the performance on standard benchmarks like Penn Tree Bank?
* How long is it to train compare to a standard RNNLM?
* What is the importance of the knowledge context $e$?
* How is initialized the fact embedding $a_{t-1}$ for the first word?
* When a word from a fact description has been chosen as prediction (copied), how is it encoded in the generation history for following predictions if it has no embedding (unknown word)? In other words, what happens if "Michelle" in the example of Section 3.1 is not in the embedding dictionary, when one wants to predict the next word?