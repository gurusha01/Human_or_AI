Pros:
* The general idea behind the paper seems pretty novel and potentially quite cool.
* The specific technical implementation seems pretty reasonable and well-thought through.
* The general types of the tasks that they try out their approach on spans a wide and interesting spectrum of cognition abilities. 
* The writing is pretty clear.  I basically felt like I could replicate much of what they did from their paper descriptions. 
Cons:
* The evaluation of the success of these ideas, as compared to other possible approaches, or as compared to human performance on similar tasks, is extremely cursory. 
* The specific tasks that they try are quite simple.   I really don't know whether their approach is better than a bunch of simpler things on these tasks.   
Taking these two cons together, it feels like the authors basically get the implementation done and working somewhat, and then just wrote up the paper.  (I know how it feels to be under a deadline without a complete set of results.)   If the authors had used their approach to solve an obviously hard problem that previously was completely unsolved, even the type of cursory evaluation level chosen here would have been fine.  Or if they had done a very thorough evaluation of a bunch of standard models on each task (and humans too, ideally), and compared their model to those results, that would have been great.  But given the complexity of their methods and the fact that the tasks are either not well-known benchmarks or very challenging as such, it's really hard to tell how much of an advance is made here.     But it does seem like a potentially fruitful research direction.