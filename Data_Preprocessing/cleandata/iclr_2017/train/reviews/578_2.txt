This work presents an empirical study of the influence of different types of data augmentation on the performance of CNNs. It also proposes to incorporate additional loss functions to encourage approximate invariance or equivariance, and shows there are some benefits.
The paper reads well and the objectives are clear. The study of invariances in CNNs is a very important topic, and advances in this area are greatly appreciated. The paper splits itself in two very different parts -- the empirical study of equivariances in existing CNNs, and the proposal of equivariance objectives. However, taken separately each of these two parts could be better executed.
On the empirical study, its breath is relatively limited, and it's hard to draw any far-reaching conclusions from it:
- Only one network is studied; at least one other architecture would have made for better generalization.
- Only one layer (fc7) is studied; this presents issues as the top layer is the most invariant. At least one convolutional layer (possibly more) should have been considered.
- The reliance on the scanned text dataset does not help; however the ImageNet results are definitely very encouraging.
It is nice to see how performance degrades with the degree of transformations, and the authors do interpret the results, but it would be better to see more analysis. There is only a limited set of conclusions that can be drawn from evaluating networks with jittered data. If the authors could propose some other interesting ways to assess the invariance and equivariance, they would potentially draw more insightful conclusions from it.
On the proposed loss function, only a very quick treatment of it is given (Section 4, half a page). It does not differ too much from known invariance/equivariance objectives studied in the literature previously, e.g. Decoste and Scholkopf, "Training Invariant Support Vector Machines", Machine Learning, 2002.
I'm not sure that dividing the paper into these two different contributions is the best approach; they both feel a bit incomplete, and a full treatment of only one of them would make for an overall better paper.