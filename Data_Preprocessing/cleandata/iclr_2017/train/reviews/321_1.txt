Interesting work on hierarchical control, similar to the work of Heess et al. 
Experiments are strong and manage to complete benchmarks that previous work could not. Analysis of the experiments is a bit on the weaker side.
(1) Like other reviewers, I find the use of the term 'intrinsic' motivation somewhat inappropriate (mostly because of its current meaning in RL).  Pre-training robots with locomotion by rewarding speed (or rewarding grasping for a manipulating arm) is very geared towards the tasks they will later accomplish. The pre-training tasks from Heess et al., while not identical, are similar. 
(2) The Mutual Information regularization is elegant and works generally well, but does not seem to help in the more complex mazes 1,2 and 3. The authors note this - is there any interpretation or analysis for this result?
(3) The factorization between Sagent and Srest should be clearly detailed in the paper. Duan et al specify Sagent, but for replicability, Srest should be clearly specified as well - did I miss it?
(4) It would be interesting to provide some analysis of the switching behavior of the agent. More generally, some further analysis of the policies (failure modes, effects of switching time on performance) would have been welcome.