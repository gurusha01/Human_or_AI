As you noted for Figure 5 Left, sometimes it seems sufficient to tune learning rates. I see your argument for Figure 6 Right, 
but 
1) not for all good learning rates make Adam fail, I guess you selected the one where it did (note that Adam was several times faster than Eve in the beginning)
2) I don't buy "Eve always converges" because you show it only for 0.1 and since Eve is not Adam, 0.1 of Adam is not 0.1 of Eve because of d_t. 
To my understanding, you define dt over time with 3 hyperparameters. Similarly, one can define dt directly. The behaviour of dt that you show is not extraordinary and can be parameterized. If Eve is better than Adam, then looking at dt we can directly see whether we underestimated or overestimated learning rates. You could argue that Eve does it automatically but you do tune learning rates for each problem individually anyway.