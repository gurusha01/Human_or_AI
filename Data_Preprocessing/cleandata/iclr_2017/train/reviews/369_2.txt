This paper presents new way for compressing CNN weights. In particular this paper uses a new neural network quantization method that compresses network weights to ternary values.
The group has recently published multiple paper on this topic, and this one offers possibly the lowest returns I have seen. Only a fraction of percentage in ImageNet. Results on AlexNet are of very little interest now, given the group already showed this kind of older style-network can be compressed by large amounts. 
I also would have liked to see this group release code for the compression, and also report data on the amount of effort required to compress: flops, time, number of passes, required original dataset, etc. This data is important to decide if a compression is worth the effort.