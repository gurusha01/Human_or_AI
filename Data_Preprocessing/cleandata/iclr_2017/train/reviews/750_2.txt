This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.
First, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about "results can be derived for Ï†(x) with convex differentiable non-linear activation functions such as ReLU", both via analysis and experimentation to measure numerical stability.
Second, the authors again show an interesting correspondance to batch normalization, but IMO fail to experimentally show its relevance.
Finally, I understand the appeal of the proposed method from a numerical stability point of view, but am not convinced that it has any effect on low-shot learning in the high dimensional spaces that deep networks are used for.
I commend the authors for contributing to the mathematical understanding of our field, but I think they have yet to demonstrate the large scale effectiveness of what they propose. At the same time, I feel like this paper does not have a clear and strong message. It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning.
notes:
- "an expectation taken with respect to the empirical distribution generated by the training set", generally the training set is viewed as a "montecarlo" sample of the underlying, unknown data distribution \mathcal{D}.
- "we can see that our model learns meaningful representations", it gets a 6.5% improvement on the baseline, but there is no analysis of the meaningfulness of the representations.
- "Table 13.2" should be "Table 2".
- please be mindful of formatting, some citations should be parenthesized and there are numerous extraneous and missing spacings between words and sentences.