This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. 
In particular, a hierarchical model is learned by combining AND, OR and POOL operations. Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary. 
Learning hierarchical representations that are interpretable is a very interesting topic, and this paper brings some good intuitions in light of modern convolutional neural nets. 
I have however, some concerns about the paper:
1) the paper fails to cite and discuss relevant literature and claims to be the first one that is able to learn interpretable parts. 
I would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:
- Compositional hierarchies of Sanja Fidler
- AND-OR graphs used by Leo Zhu and Alan Yuille to model objects
- AND-OR templates of Song-Chun Zhu's group at UCLA 
The claim that this paper is the first to discover such parts should be removed. 
2) The experimental evaluation is limited to very toy datasets. The papers I mentioned have been applied to real images (e.g., by using contours to binarize the images). 
I'll also like to see how good/bad the proposed approach is for classification in more well known benchmarks. 
A comparison to other generative models such as VAE, GANS, etc will also be useful.
3) I'll also like to see a discussion of the relation/differences/advantages of the proposed approach wrt to sum product networks and grammars.
Other comments:
- the paper claims that after learning inference is feed-forward, but since message passing is used, it should be a recurrent network. 
- the algorithm and tech discussion should be moved from the appendix to the main paper
- the introduction claims that compression is a prove for understanding. I disagree with this statement, and should be removed. 
- I'll also like to see a discussion relating the proposed approach to the Deep Rendering model. 
- It is not obvious how some of the constraints are satisfied during message passing. Also constraints are well known to be difficult to optimize with max product. How do you handle this?
- The learning and inference algorithms seems to be very heuristic (e.g., clipping to 1, heuristics on which messages are run). Could you analyze the choices you make?
- doing multiple steps of 5) 2) is not a single backward pass 
I'll reconsider my score in light of the answers