Summary:
The paper "Deep Variational Information Bottleneck" explores the optimization of neural networks for variational approximations of the information bottleneck (IB; Tishby et al., 1999). On the example of MNIST, the authors show that this may be used for regularization or to improve robustness against adversarial attacks.
Review:
The IB is potentially very useful for important applications (regularization, adversarial robustness, and privacy are mentioned in the paper). Combining the IB with recent advances in deep learning to make it more widely applicable is an excellent idea. But given that the theoretical contribution is a fairly straight-forward application of well-known ideas, I would have liked to see a stronger experimental section.
Since the proposed approach allows us to scale IB, a better demonstration of this would have been on a larger problem than MNIST. It is also not clear whether the proposed approach will still work well to regularize more interesting networks with many layers.
Why is dropout not included in the quantitative comparison of robustness to adversarial examples (Figure 4)?
How was the number of samples (12) chosen?
What are the error bars in Figure 1 (a)?
On page 7 the authors claim "the posterior covariance becomes larger" as beta "decreases" (increases?). Is this really the case? It's hard to judge based on Figure 1, since the figures are differently scaled.
It might be worth comparing to variational fair autoencoders (Louizos et al., 2016), which also try to learn representations minimizing the information shared with an aspect of the input.
The paper is well written and easy to follow.