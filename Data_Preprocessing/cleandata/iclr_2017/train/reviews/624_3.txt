The paper studies some special cases of neural networks and datasets where optimization fails. Most of the considered models and datasets are however highly constructed and do not follow the basic hyperparameters selection and parameter initialization heuristics. This reduces the practical relevance of the analysis.
The experiment "bad initialization on MNIST" shows that for very negative biases or weights drawn from a non-centered distribution, all ReLU activations are "off" for all data points, and thus, optimization is prevented. This never occurs in practice, because using proper initialization heuristics avoid these cases.
The "jellyfish" dataset constructed by the authors is demonstrated to be difficult to fit by a small model. However, the size/depth of the considered model is unsuitable for this problem.
Proposition 4 assumes that we can choose the mean from which the weight parameters are initialized. This is typically not the case in practice as most initialization heuristics draw weight parameters from a distribution with mean 0.
Proposition 5 considers infinitely deep ReLU networks. Very deep networks would however preferably be of type ResNet.