The method in this paper introduces a binary encoding level in the PV-DBOW and PV-DM document embedding methods (from Le & Mikolov'14). The binary encoding consists in a sigmoid with trained parameters that is inserted after the standard training stage of the embedding.
 
For a document to encode, the binary vector is obtained by forcing the sigmoid to output a binary output for each of the embedding vector components. The binary vector can then be used for compact storage and fast comparison of documents.
 
Pros:
 
- the binary representation outperforms the Semantic hashing method from Salakhutdinov & Hinton '09
 
- the experimental approach sound: they compare on the same experimental setup as Salakhutdinov & Hinton '09, but since in the meantime document representations improved (Le & Mikolov'14), they also combine this new representation with an RBM to show the benefit of their binary PV-DBOW/PV-DM
 
Cons:
 
- the insertion of the sigmoid to produce binary codes (from Lin & al. '15) in the training process is incremental
 
- the explanation is too abstract and difficult to follow for a non-expert (see details below)
 
- a comparison with efficient indexing methods used in image retrieval is missing. For large-scale indexing of embedding vectors, derivations of the Inverted multi-index are probably more interesting than binary codes. See eg. Babenko & Lempitsky, Efficient Indexing of Billion-Scale Datasets of Deep Descriptors, CVPR'16
 
Detailed comments:
 
Section 1: the motivation for producing binary codes is not given. Also, the experimental section could give some timings and mem usage numbers to show the benefit of binary embeddings
 
figure 1, 2, 3: there is enough space to include more information on the representation of the model: model parameters + training objective + characteristic sizes + dropout. In particular, in fig 2, it is not clear why "embedding lookup" and "linear projection" cannot be merged in a single smaller lookup table (presumably because there is an intermediate training objective that prevents this).
 
p2: "This way, the length of binary codes is not tied to the dimensionality of word embeddings." -> why not?
 
section 3: This is the experimental setup of  Salakhutdinov & Hinton 2009. Specify this and whether there is any difference between the setups.
 
"similarity of the inferred codes": say here that codes are compared using Hamming distances.
 
"binary codes perform very well, despite their far lower capacity" -> do you mean smaller size than real vectors?
 
fig 5: these plots could be dropped if space is needed.
 
section 3.1: one could argue that "transferring" from Wikipedia to anything else cannot be called transferring, since Wikipedia's purpose is to include all topics and lexical domains
 
section 3.2: specify how the 300D real vectors are compared. L2 distance? inner product?
 
fig4: specify what the raw performance of the large embedding vectors is (without pre-filtering with binary codes), or equivalently, the perf of (code-size, Hamming dis) = (28, 28), (24, 24), etc.