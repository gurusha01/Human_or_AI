The paper addresses the problem of predicting learning curves. The key difference from prior work is that (1) the authors learn a neural network that generalizes across hyperparameter settings and (2) the authors use a Bayesian neural network with SGHMC. 
The authors demonstrate that the proposed approach is effective on extrapolating partially observed curves as well as predicting unobserved learning curves on various architectures (FC, CNN, LR and VAE). This seems very promising for Bayesian optimization, I'd love to see an experiment that evaluates the relative advantage of this proposed method :)
Have you thought about ways to handle learning rate decays? Perhaps you could run the algorithm on a random subset of data and extrapolate from that?
I was thinking of other evaluation measures in addition to MSE and LL. In practice, we care about the most promising run. Would it make sense to evaluate how accurately each method identified the best run?
Minor comments:
Fonts are too small and almost illegible on my hard copy. Please increase the font size for legends and axes in the figures.
Fig 6: not all figures seem to have six lines. Are the lines overlapping in some cases?