This work analyzes the continuous-time dynamics of gradient descent when training two-layer ReLU networks (one input, one output, thus only one layer of ReLU units). The work is interesting in the sense that it does not involve some unrealistic assumptions used by previous works with similar goal. Most importantly, this work does not assume independence between input and activations, and it does not rely on noise injection (which can simplify the analysis). Nonetheless, removing these simplifying assumptions comes at the expense of limiting the analysis to:
1. Only one layer of nonlinear units
2. Discarding the bias term in ReLU while keeping the input Gaussian (thus constant input trick cannot be used to simulate the bias term).
3. Imposing strong assumption on the representation on the input/output via (bias-less) ReLU networks: existence of orthonormal bases to represent this relationships.
Having that said, as far as I can tell, the paper presents original analysis in this new setting, which is interesting and valuable. For example, by exploiting the symmetry in the problem under the assumption 3 I listed above, the authors are able to reduce the high-dimensional dynamics of the gradient descent to a bivariate dynamics (instead of dealing with original size of the parameters). Such reduction to 2D allows the author to rigorously analyze the behavior of the dynamics (e.g. convergence to a saddle point in symmetric case, or to the optimum in non-symmetric case).
Clarification Needed: first paragraph of page 2. Near the end of the paragraph you say "Initialization can be arbitrarily close to origin", but at the beginning of the same paragraph you state "initialized randomly with standard deviation of order 1/sqrt(d)". Aren't these inconsistent?
Some minor comments about the draft:
1. In section 1, 2nd paragraph: "We assume x is Gaussian and thus the network is bias free". Do you mean "zero-mean" Gaussian then?
2. "standard deviation" is spelled "standard derivation" multiple times in the paper.
3. Page 6, last paragraph, first line: Corollary 4.1 should be Corollary 4.2