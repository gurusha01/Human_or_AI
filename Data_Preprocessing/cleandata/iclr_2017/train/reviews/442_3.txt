Thank you for an interesting read. I personally like the information bottleneck principle and am very happy to see its application to deep neural networks. To my knowledge, this is the first paper that applies IB to train deep networks (the original papers only presented the concept), but see below for the note of independent work claim. 
The derivation of the variational lowerbound is very clear, even for those who are not very familiar with variational inference. Also the explanation of the IB principle is clear. Experimental results seem to be very promising.
I found the presentation for the model a bit confusing. In variational inference/information maximisation, p usually denotes the model and q represents the "inference engine". This means the choice of inference method is independent to the modelling procedure. However the presented VIB assumed p(x, y) as the underlying data distribution (and approximated by the empirical distribution), thus here the model is actually q(y|z)p(z|x). Then the authors presented p(y|x) as the predictive distribution in page 8, paragraph 2 of section 4.2.3. Predictive in what sense? I guess you meant p(y|x) = \int q(y|z) p(z|x) dz in this case, but this makes the two definitions contradict to each other!
The authors have made an interesting connection to variational auto-encoder and the warm-up training (by tuning beta). However, even when the loss function formula is the same to the variational lowerbound used in VAE (in this case beta = 1), the underlying model is different! For example, r(z) in VIB is the variational approximation to p(z) (which means r(z) is not a component in the model), while in VAE it is the prior distribution which is actually defined in the modelling procedure. Similaly p(z|x) in VIB is included in the model, while in VAE that is the approximate posterior and can be independently chosen (e.g. you can use p(x|z) as a deep NN but p(z|x) as a deep NN or a Gaussian process).
In summary, I think the presentation for the modelling procedure is unclear. I hope these point would be made clearer in revision since the current presentation makes me uncomfortable as a Bayesian person. In the VAE part, it's better to clearly mention the difference between VIB and VAE, and provide some intuitions if the VIB interpretation is preferred.
Typos:
Eq. 9-11: did you mean q(y|z) instead of q(z|y)?
Fig 2 "as beta becomes smaller": did you mean "larger"?
claim for independent work
The authors claimed that the manuscript presented an independent work to Chalk et al. 2016 which is online since May 2016. It seems to me that nowadays deep learning research is very competitve that many people publish the same idea at the same time. So I would trust this claim and commend the authors' honesty, but in case this is not true, I would not recommend the manuscript for acceptance.