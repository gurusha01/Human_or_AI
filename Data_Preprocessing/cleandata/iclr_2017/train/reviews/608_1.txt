Summary:
In this paper, the authors explore the advantages/disadvantages of using a sin activation function.
They first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions.
They then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well.
They then try different algorithmic tasks, where the periodicity of the functions is helpful.
Pros:
The closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages and disadvantages was educational.
Cons: 
Seems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from.