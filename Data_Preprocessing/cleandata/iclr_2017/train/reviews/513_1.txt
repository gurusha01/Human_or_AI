Paper Summary
This paper evaluates the ability of two unsupervised learning models to learn a
generalizable physical intuition governing the stability of a tower of blocks.
The two models are (1) A model that predicts the final state of the tower given
the initial state, and (2) A model that predicts the sequence of states of this
tower over time given the initial state. Generalizability is evaluated by
training a model on towers made of a certain number of blocks but testing on
towers made of a different number of blocks.
Strengths
- This paper explores an interesting way to evaluate representations in terms of
  their generalizability to out-of-domain data, as opposed to more standard
methods which use train and test data drawn from the same distribution.
- Experiments show that the predictions of deep unsupervised learning models on
  such out-of-domain data do seem to help, even though the models were not
trained explicitly to help in this way.
Weaknesses
- Based on Fig 4, it seems that the models trained on 3 blocks (3CD, 3CLD)
  ``generalize" to 4 and 5 blocks.  However, it is plausible that these models
only pay attention to the bottom 3 blocks of the 4 or 5 block towers in order to
determine their stability. This would work correctly a significant fraction of
the time. Therefore, the models might actually be overfitting to 3 block towers
and not really generalizing the physics of these blocks. Is this a possibility ?
I think more careful controls are needed to make the claim that the features
actually generalize. For example, test the 3 block model on a 5 block test set
but only make the 4th or 5th block unstable. If the model still works well, then
we could argue that it is actually generalizing.
- The experimental analysis seems somewhat preliminary and can be improved. In
  particular, it would help to see visualizations of what the final state looks
like for models trained on 3 blocks but test on 5 (and vice-versa). That would
help understand if the generalization is really working. The discriminative
objective gives some indication of this, but might obfuscate some aspects of
physical realism that we would really want to test.  In Figure 1 and 2, it is
not mentioned whether these models are being tested on the same number of blocks
they were trained for.
- It seems that the task of the predicting the final state is really a binary
  task - whether or not to remove the blocks and replace them with gray
background. The places where the blocks land in case of a fall is probably quite
hard to predict, even for a human, because small perturbations can have a big
impact on the final state. It seems that in order to get a generalizable
physics model, it could help to have a high frame rate sequence prediction task.
Currently, the video is subsampled to only 5 time steps.
Quality
A more detailed analysis and careful choices of testing conditions can increase
the quality of this paper and strengthen the conclusions that can be drawn from
this work.
Clarity
The paper is well written and easy to follow.
Originality
The particular setting explored in this paper is novel.
Significance
This paper provides a valuable addition to the growing work on
transferability/generalizability as an evaluation method for unsupervised
learning. However, more detailed experiments and analysis are needed to make
this paper significant enough for an ICLR paper.
Minor comments and suggestions
- The acronym IPE is used without mentioning its expansion anywhere in the text.
- There seems to be a strong dependence on data augmentation. But given that
  this is a synthetic dataset, it is not clear why more data was not generated
in the first place.
- Table 3 : It might be better to draw this as a 9 x 3 grid : 9 rows corresponding to the
models and 3 columns corresponding to the test sets. Mentioning the train set is
redundant since it is already captured in the model name. That might make it
easier to read.
Overall
This is an excellent direction to work and preliminary results look great.
However, more controls and detailed analysis are needed to make strong
conclusions from these experiments.