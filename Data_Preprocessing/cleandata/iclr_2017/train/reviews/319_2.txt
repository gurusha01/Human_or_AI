The paper presented a modified knowledge distillation framework that minimizes the difference of the sum of statistics across the a feature map between the teacher and the student network. The authors empirically demonstrated the proposed methods outperform the fitnet style distillation baseline. 
Pros:
+ The author evaluated the proposed methods on various computer vision dataset 
+ The paper is in general well-written
Cons:  
- The method seems to be limited to the convolutional architecture
- The attention terminology is misleading in the paper. The proposed method really just try to distill the summed squared(or other statistics e.g. summed lp norm) of  activations in a hidden feature map.
- The gradient-based attention transfer seems out-of-place. The proposed gradient-based methods are never compared directly to nor are used jointly with the "attention-based" transfer. It seems like a parallel idea added to the paper that does not seem to add much value.
- It is also not clear how the induced 2-norms in eq.(2) is computed. Q is a matrix \in \mathbb{R}^{H \times W}  whose induced 2-norm is its largest singular value. It seems computationally expensive to compute such cost function. Is it possible the authors really mean the Frobenius norm?
Overall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation.