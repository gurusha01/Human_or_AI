This paper studies in depth the idea of quantizing down convolutional layers to 3 bits, with a different positive and negative per-layer scale. It goes on to provide an exhaustive analysis of performance (essentially no loss) on real benchmarks (this paper is remarkably MNIST-free).
The relevance of this paper is that it likely provides a lower bound on quantization approaches that don't sacrifice any performance, and hence can plausibly become the approach of choice for resource-constrained inference, and might suggest new hardware designs to take advantage of the proposed structure.
Furthermore, the paper provides power measurements, which is really the main metric that anyone working seriously in that space cares about. (Nit: I don't see measurements for the full-precision baseline).
I would have loved to see a SOTA result on ImageNet and a result on a strong LSTM baseline to be fully convinced.
I would have also liked to see discussion of the wall time to result using this training procedure.