The paper presented an extension to the current visual attention model that learns a deformable sampling lattice.  Comparing to the fixed sampling lattice from previous works, the proposed method shows different sampling strategy can emerge depending on the visual classification tasks. The authors empirically demonstrated the learnt sampling lattice outperforms the fixed strategies. More interestingly, when the attention mechanism is constrained  to be translation only, the proposed model learns a sampling lattice resembles the retina found in the primate retina.  
Pros:
+ The paper is generally well organized and written 
+ The qualitative analysis in the experimental section is very comprehensive.
Cons:
-  The paper could benefit substantially from additional experiments on different datasets.
-  It is not clear from the tables the proposed learnt sampling  lattice offer any computation benefit when comparing to  a fixed sampling strategy with zooming capability, e.g. the one used in DRAW model.
Overall, I really like the paper. I think the experimental section can be improved by additional experiments and more quantitative analysis with other baselines. Because the current revision of the paper only shows experiments on digit dataset with black background, it is hard to generalize the finding or even to verify the claims in the paper, e.g.  linear relationship
between eccentricity and sampling interval leads to the primate retina, from the results on a single dataset.