In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.  
Pros:
+ The organization is generally very clear
+ Novel meta-learning approach that is different than the previous learning to learn approach
Cons: 
- The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.  
- Neither MNIST nor CIFAR experimental section explained the architectural details
- Mini-batch size for the experiments were not included in the paper
- Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. 
Overall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method.