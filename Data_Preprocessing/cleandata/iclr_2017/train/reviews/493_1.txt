The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. 
Consequently, the optimized upper bound (log-loss) gets looser. 
As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. 
I would like to draw the author's attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. 
There is a large body of work on directly optimizing task losses[2][3] and the references therein. These should also be discussed and related particularly to section 3 (optimizing the ROC curve).
[1] Training Highly Multiclass Classifiers, Gupta et al. 2014.
[2] Direct Loss Minimization for Structured Prediction, McAllester et al. 
[3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet.
Final comment:
I believe the material presented in this paper is of interest to a wide audience at ICLR.
The problem studied is interesting and the proposed approach is sound. 
I recommend to accept the paper and increase my score (from 7 to 8).