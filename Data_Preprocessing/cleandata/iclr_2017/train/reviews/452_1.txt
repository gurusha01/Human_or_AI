This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces.
A few comments/questions:
- Table 1 could be easier to interpret as a figure of histograms.
- Figure 3 could be easier to interpret as a table.
- How was the subset of Atari games selected?
- The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores.
- It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR.
- FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought.
Overall, this is a nice simple addition to deep RL algorithms that many people will probably start using.
--------------------
I'm increasing my score to 8 based on the rebuttal and the revised paper.