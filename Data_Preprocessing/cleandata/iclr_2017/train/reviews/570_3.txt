This paper proposes a neural architecture for answering non-factoid questions. The author's model improves over previous neural models for answer sentence selection. Experiments are conducted on a Japanese love advice corpus; the coolest part of the paper for me was that the model was actually rolled out to the public and its answers were rated twice as good as actual human contributors! 
It was hard for me to determine the novelty of the contribution. The authors mention that their model "fills the gap
between answer selection and generation"; however, no generation is actually performed by the model! Instead, the model appears to be very similar to the QA-LSTM of Tan et al., 2015 except that there are additional terms in the objective to handle conclusion and supplementary sentences. The structure of the answer is fixed to a predefined template (e.g., conclusion --> supplementary), so the model is not really learning how to order the sentences. The other contribution is the "word embedding with semantics" portion described in sec 4.1, which is essentially just the paragraph vector model except with "titles" and "categories" instead of paragraphs. 
While the result of the paper is a model that has actually demonstrated real-life usefulness, the technical contributions do not strike me as novel enough for publication at ICLR.
Other comments:
- One major issue with the reliance of the model on the template is that you can't evaluate on commonly-used non-factoid QA datasets such as InsuranceQA. If the template were not fixed beforehand (but possibly learned by the model), you could conceivably evaluate on different datasets. 
- The examples in Table 4 don't show a clear edge in answer quality to your model; QA-LSTM seems to choose good answers as well.
- Doesn't the construction model have an advantage over the vanilla QA-LSTM in that it knows which sentences are conclusions and which are supplementary? Or does QA-LSTM also get this distinction?