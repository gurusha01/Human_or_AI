Summary:
The authors propose a multi-hop "gated attention" model, which models the interactions between query and document representations, for answering cloze-style questions. The document representation is attended to sequentially over multiple-hops using similarity with the query representation (using a dot-product) as the scoring/attention function. 
The proposed method improves upon (CNN, Daily Mail, Who-Did-What datasets) or is comparable to (CBT dataset) the state-of-the-art results.
Pros:
1. Nice idea on heirarchical attention for modulating the context (document) representation by the task-specific (query) representation.
2. The presentation is clear with thorough experimental comparison with the latest results.
Comments:
1. The overall system presents a number of architectural elements: (1) attention at multiple layers (multi-hop), (2) query based attention for the context (or gated attention), (3) encoding the query vector at each layer independently.
It is important to breakdown the gain in performance due to the above factors: the ablation study presented in section 4.4 helps establish the importance of Gated Attention (2 above). However, it is not clear:
  (1) how much multiple-hops of gated-attention contribute to the performance.
  (2) how important is it to have a specialized query encoder for each layer.
Understanding the above better, will help simplify the architecture.
2. The tokens are represented using L(w) and C(w). It is not clear if C(w) is crucial for the performance of the proposed method.
There is a significant performance drop when C(w) is absent (e.g. in "GA Reader--"; although there are other changes in "GA Reader--" which could affect the performance). Hence, it is not clear how much does the main idea, i.e., gated attention contributes towards the superior performance of the proposed method.