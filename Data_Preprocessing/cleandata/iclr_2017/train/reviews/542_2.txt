This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. 
Sketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. 
Experiments are provided through a standard game like domain (maze, minecraft etc.). 
The paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the context of the specific approach proposed here. Second, the experiments are not thorough enough in terms of comparing with all the related work. For example, Ghavamzadeh et al. explored the use of MAXQ like abstractions in the context of mulitagent RL. It would be great to get a more detailed comparison to MAXQ based multiagent RL approaches, where the value function is explicitly decomposed.