The goal of this paper is to learn " a collection of experts that are individually
meaningful and that have disjoint responsibilities." Unlike a standard mixture model, they "use a different mixture for each dimension d." While the results seem promising, the paper exposition needs significant improvement.
Comments:
The paper jumps in with no motivation at all. What is the application, or even the algorithm, or architecture that this is used for? This should be addressed at the beginning.
The subsequent exposition is not very clear. There are assertions made with no justification, e.g. "the experts only have a small variance for some subset of the variables while the variance of the other variables is large." 
Since you're learning both the experts and the weights, can this be rephrased in terms of dictionary learning? Please discuss the relevant related literature.
The horse data set is quite small with respect to the feature dimension, and so the conclusions may not necessarily generalize.