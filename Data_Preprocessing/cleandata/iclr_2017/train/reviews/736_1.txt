This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.
Pros:
The method is very simple and easy to understand and apply.
The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.
The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.
Cons:
There is little novelty -- the method is arguably too simple to be called a "method." Rather, it's the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation.  The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what's done in the Inception BN results in Table 1-2?)
The analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted/scaled to N(0,1) by BN.
Section 3.3: it's not clear to me what point is being made here.
Overall, there's not much novelty here, but it's hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with "Frustratingly Easy Domain Adaptation").  If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.