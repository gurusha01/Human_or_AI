Comments: 
"This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied,
which often have telltale noise"
Is this really true?  If it were the case, wouldn't it imply that training "against" adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)?  
Pros: 
  -The question of whether adversarial examples exist in generative models, and indeed how the definition of "adversarial example" carries over is an interesting one.  
  -Finding that a certain type of generative model doesn't have adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result.  
  -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014).  Is this because it's actually harder to find adversarial examples in these types of generative models?  
Issues: 
  -Paper is significantly over length at 13 pages.  
  -The beginning of the paper should more clearly motivate its purpose.  
  -Paper has "generative models" in the title but as far as I can tell the whole paper is concerned with autoencoder-type models.  This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they're distinct from a paper called "adversarial examples for generative models".  
   -I think that the introduction contains too much background information - it could be tightened.