The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances. The method's gradient behaviour is analysed. Experimental results seem to indicate that the method compares well with similar approaches.
Points
1) The writing is sloppy in parts. See at the end of the review for a non-exhaustive list.
2) The experimental results show marginal improvements, of which the the statistical significance is impossible to asses. (Not completely the author's fault for PTB, as they partially rely on results published by others.) Weight normalisation seems to be a viable alternative in the: the performance and runtime are similar. The implementation complexity of weight norm is, however, arguably much lower. More effort could have been put in by the authors to clear that up. In the current state, practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate.
3) Section 4 is nice, and I applaud the authors for doing such an analysis.
List of typos etc.
- maintain -> maintain
- requisits -> requisites
- a LSTM -> an LSTM
- "The gradients of ot and ft are equivalent to equation 25." Gradients cannot be equivalent to an equation.
- "beacause"-> because
- One of the γx > γh at the end of page 5 is wrong.