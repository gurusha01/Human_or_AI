The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.
Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.
The proposed approach relates to Batch Norm and weight decay.
Experiments are given on "low-shot" settting.
There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?
Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?
Overall, the idea is simple but feels like preliminary: while it is supposed to be a "soft BN", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?
-- edits after revised version:
Thank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:
- on Omniglot, the paper is still significantly far from the current state of the art.
- the new experiments do not really confirm/infirm the relationship with BN.
- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.
I'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.