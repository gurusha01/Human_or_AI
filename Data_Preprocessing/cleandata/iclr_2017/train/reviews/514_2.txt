This paper discusses ways to enforce invariance in neural networks using weight sharing.  The authors formalize a way for feature functions to be invariant to a collection of relations and the main invariance studied is a "set-invariant" function, which is used in an anomaly detection setting and a point cloud classification problem.  
"Invariance" is, at a high level, an important issue of course, since we don't want to spend parameters to model spurious ordering relationships, which may potentially be quite wasteful and I like the formalization of invariance presented in this paper.  However, there are a few weaknesses that I feel prevent this from being a strong submission.  First, the exposition is too abstract and this paper could really use a running and concrete example starting from the very beginning.
Second, "set invariance", which is the main type of invariance studied in the paper is defined via the author's formalization of invariance, but is never explicitly related to what I might think of as "set invariance" — e.g. to permutations of input or output dimensions.  Explicitly defining set invariance in some other way, then relating it to the  "structural invariance" formulation may be a better way to explain things.  It is never made clear, for example, why Figure 1(b) is the set data-structure.
I like the discussion of compositionality of structures (one question I have here is: are the resulting compositional structures are still valid as structures?).  But the authors have ignored the other kind of compositionality that is important to neural networks — specifically that relating the proposed notion of invariance to function composition seems important — i.e. under what conditions do compositions of invariant functions remain invariant?  And  It is clear to me that just by having one layer of invariance in a network doesn't make the entire network invariant, for example.  So if we look at the anomaly detection network at the end for example, is it clear that the final predictor is "set invariant" in some sense?  
Regarding experiments, there are no baselines presented for anomaly detection.  Baselines are presented in the point cloud classification problem, but the results of the proposed model are not the best, and this should be addressed.  (I should say that I don't know enough about the dataset to say whether these are exactly fair comparisons or not).  It is also never really made clear why set invariance is a desirable property for a point cloud classification setting.  As a suggestion: try a network that uses a fully connected layer at the end, but uses data augmentation to enforce set invariance.  Also, what about classical set kernels?
Other random things:
* Example 2.2: Shouldn't |S|=5 in the case of left-right and up-down symmetry?
* "Parameters shared within a relation" is vague and undefined.
* Why is "set convolution" called "set convolution" in the appendix?  What is convolutional about it?
* Is there a relationship to symmetric function theory?