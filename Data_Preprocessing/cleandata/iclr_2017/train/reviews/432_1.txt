This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done.
One minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. 
Overall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.