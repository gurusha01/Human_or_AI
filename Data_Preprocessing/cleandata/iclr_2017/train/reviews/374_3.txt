SUMMARY.
The paper proposes a gating mechanism to combine word embeddings with character-level word representations.
The gating mechanism uses features associated to a word to decided which word representation is the most useful.
The fine-grain gating is applied as part of systems which seek to solve the task of cloze-style reading comprehension question answering, and Twitter hashtag prediction.
For the question answering task, a fine-grained reformulation of gated attention for combining document words and questions is proposed.
In both tasks the fine-grain gating helps to get better accuracy, outperforming state-of-the-art methods on the CBT dataset and performing on-par with state-of-the-art approach on the SQuAD dataset.
----------
OVERALL JUDGMENT
This paper proposes a clever fine-grained extension of a scalar gate for combining word representation.
It is clear and well written. It covers all the necessary prior work and compares the proposed method with previous similar models.
I liked the ablation study that shows quite clearly the impact of individual contributions.
And I also liked the fact that some (shallow) linguistic prior knowledge e.g., pos tags ner tags, frequency etc. has been used in a clever way. 
It would be interesting to see if syntactic features can be helpful.