This papers adds to the literature on learning optimizers/algorithms that has gained popularity recently. The authors choose to use the framework of guided policy search at the meta-level to train the optimizers. They also opt to train on random objectives and assess transfer to a few simple tasks.
As pointed below, this is a useful addition.
However, the argument of using RL vs gradients at the meta-level that appears below is not clear or convincing. I urge the authors to run an experiment comparing the two approaches and to present comparative results. This is a very important question, and the scalability of this approach could very well hinge on this fact. Indeed, demonstrating both scaling to large domains and transfer to those domains is the key challenge in this domain.
In summary, the idea is a good one, but the experiments are weak.
This work appeared on arxiv just before we were able to release our learning to learn by gradient descent by gradient descent paper. 
In our case, we were motivated by neural art and a desire to replace the lBFGS optimizer with a neural Turing machine (both have the same equation forms - see appendix of our arxiv version). In the end, we settled on an LSTM optimizer that is also learned by SGD. 
This paper, on the other hand, uses guided policy search to choose the parameter updates (policy). Importantly, this paper also emphasizes the importance of transfer of optimizers to novel tasks.
This is undoubtedly a timely, good contribution to the learning to learn literature, which has gained great popularity in recent months.