- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. 
- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. "Reinforcement learning with unsupervised auxiliary tasks.", Blundell, Charles, et al. "Model-free episodic control." , Narasimhan et al. "Language understanding for text-based games using deep reinforcement learning"), which could perhaps reinforce the agent to avoid revisiting catastrophic states. 
- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model?