The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.
Overall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they don't perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers.