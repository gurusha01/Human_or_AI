The authors present a method for training probabilistic models by maximizing a stochastic variational-lower-bound-type objective. Training involves sampling and then learning a transition-based inference to "walk back" samples to the data. Because of its focus on transitions, it can be used to learn a raw transition operator rather than purely learning an energy-based model. The objective is intuitively appealing because of its similarity to previous successful but less principled training methods for MRFs like Contrastive Divergence.
The idea for the algorithm is appealing, and it looks like it could find a nice place in the literature. However, the submission in its current form is not yet ready for publication. Experiments are qualitative and the generated samples are not obviously indicative of a high model quality. As pointed out elsewhere, the mathematical analysis does not currently demonstrate tightness of the variational bound in the case of a learned transition operator. More evaluation using e.g. annealed importance sampling to estimate held-out likelihoods is necessary. Assuming that the analysis can be repaired, the ability to directly parametrize a transition operator, an interesting strength of this method, should be explored in further experiments and contrasted with the more standard energy-based modeling.
This looks like a promising idea, and other reviews and questions have already raised some important technical points which should help strengthen this paper for future submission.