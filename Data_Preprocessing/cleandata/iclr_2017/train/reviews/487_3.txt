From my original comments:
The results looks good but the baselines proposed are quite bad.
For instance in the table 2 "Misclassification rate for a 784-1024-1024-1024-10 " the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see "significant" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...
In CIFAR-10 experiments, i do not understand  why "Sparsely-Connected 90% + Single-Precision Floating-Point" is worse than "Sparsely-Connected 90% + BinaryConnect". So it is better to use binary than float. 
Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. 
In fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.
----
The authors reply still does not convince me.
I still think that the same technique should be applied on more challenging scenarios.