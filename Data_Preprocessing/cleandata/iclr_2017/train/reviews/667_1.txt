This paper addresses the problem of data sparsity in the healthcare domain by leveraging hierarchies of medical concepts organized in ontologies. The paper focuses on sequential prediction given a patient's medical record (a sequence of medical codes, some of which might occur very rarely). Instead of simply assigning each medical code an independent embedding before feeding it to an RNN, the proposed approach assigns each node in the medical ontology a "basic" embedding, and composes a "final" embedding for each medical code by taking a learned weighted average (via an attention mechanism) of the medical code's ancestors in the ontology. Notably, the paper is well written and the approach is quite intuitive.
I have the following comments: 
- Why is the patient's visit taken as just the sum of medical codes found in the visit, and not say the average or a learned weighted average? Wouldn't this bias for/against the number of codes in the visit?
- I don't see why basic embeddings are not fine tuned as well. Did you find that to hurt performance? Do you have an explanation for that?
- Looking at Figure 2, the results seem very close and the figures are not very clear (figure (b) top is missing). Also, I am wondering how significant the differences are so it would be nice to comment on that.
Finally, I think this is an interesting application paper applying well-established deep learning techniques. The paper deals with an important issue that arises when applying deep learning models in domains with scarce data resources. However, I would like the authors to comment on what there paper offers as new insights to the ICLR community and why they think ICLR is a good avenue for their work.