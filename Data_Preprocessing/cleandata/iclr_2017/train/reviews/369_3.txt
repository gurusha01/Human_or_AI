This work presents a novel ternary weight quantization approach which quantizes weights to either 0 or one of two layer specific learned values. Unlike past work, these quantized values are separate and learned stochastically alongside all other network parameters. This approach achieves impressive quantization results while retaining or surpassing corresponding full-precision networks on CIFAR10 and ImageNet.
Strengths:
- Overall well written and algorithm is presented clearly.
- Approach appears to work well in the experiments, resulting in good compression without loss (and sometimes gain!) of performance.
- I enjoyed the analysis of sparsity (and how it changes) over the course of training, though it is uncertain if any useful conclusion can be drawn from it.
Some points:
- The energy analysis in Table 3 assumes dense activations due to the unpredictability of sparse activations. Can the authors provide average activation sparsity for each network to help verify this assumption. Even if the assumption does not hold, relatively close values for average activation between the networks would make the comparison more convincing.
- In section 5.1.1, the authors suggest having a fixed t (threshold parameter set at 0.05) for all layers allows for varying sparsity (owed to the relative magnitude of different layer weights with respect to the maximum). In Section 5.1.2 paragraph 2, this is further developed by suggesting additional sparsity can be achieved by allowing each layer a different values of t. How are these values set? Does this multiple threshold style network appear in any of the tables or figures? Can it be added?
- The authors claim "ii) Quantized weights play the role of "learning rate multipliers" during back propagation." as a benefit of using trained quantization factors. Why is this a benefit? 
- Figure and table captions are not very descriptive.
Preliminary Rating:
I think this is an interesting paper with convincing results but is somewhat lacking in novelty. 
Minor notes:
- Table 3 lists FLOPS rather than Energy for the full precision model. Why?
- Section 5 'speeding up'
- 5.1.1 figure reference error last line