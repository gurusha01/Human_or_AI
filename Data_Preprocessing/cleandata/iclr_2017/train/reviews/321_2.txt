*Edited the score 6->7.
The paper presents a method for hierarchical RL using stochastic neural networks. The paper has introduced using information-theoretic measure of option identifiability as an additional reward for learning a diverse mixture of sub-policies. One nice result in the paper is the comparison with strong baseline which directly combines the intrinsic rewards with sparse rewards and shows that this supposedly smooth reward can't solve tasks. Besides the argument made from the authors on difficulty on long-term credit assignment/benefits from hierarchical abstraction, one possible explanation for this might be the diversity requirement imposed in sub-policy training, which is assumed to be off in the baseline case. Wonder if this can shed insights into improving the baseline and proposing new end-to-end hierarchical policy learning as hierarchical REPS/option-critic etc. papers do. Nice visualizations.
The paper presents a promising direction, and it may be strengthened further by possibly addressing some of the following points. 
1) Limited diversification of sub-policies: Both concatenation and bilinear integration allow only minimal differentiations in sub-policies through first hidden weight, which is not a problem in the tested tasks because they essentially require same locomotion policies with minimal diversification, but such limitation can be more obvious in other tasks where ideal sub-policies are more diverse. Thus it is interesting to see it apply on harder, non-locomotion domains, where ideal sub-policies are not that similar, e.g. for manipulation, solving some task from one state can be very different from solving it from another state. 
2) Limitation on hierarchical policies: Manager network is trained while the sub-policies are fixed. Furthermore, the time steps for sub-policies are fixed. This requires "intrinsic" rewards and their learned sub-policies to be very good for solving down-stream tasks. It would be nice to see some more discussions/results on handling such cases, ideally connecting to end-to-end hierarchical policy learning.
3) Intrinsic/unsupervised rewards seem domain-specific/supervised rewards: Because of (2), this seems unavoidable. 
Sorry for some last minute questions. What is the intuition for why the strong baseline in section 7.3 perform very poorly and can't solve the task? Assuming "CoM maze reward"is this baseline. Does the baseline also take into account the extra experience pre-trained policies have got? What is multi-policy, as compared to snn?
In figure 1, is it correct that (1) concatenation means based on categorical value, you have different bias to the first hidden layer, and (2) bilinear integration means you have different weight matrix connecting observation to the first hidden layer?