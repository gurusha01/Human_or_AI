This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.
Pros:
- The paper is clear and easy to follow
- The experimental results seem to show some benefit from the proposed approach
Cons:
(1) The paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation
(2) No comparison with an ensemble
(3) Full experiments on ImageNet under the "partial privileged information" setting would be more impactful
This paper is promising and I would be willing to accept an improved version. However, the current version lacks focus and clean experiments.
First, the abstract and intro focus on the need to replace ensembles with a single model that has diverse (ensemble like) features. The hope is that such a model will have the same boost in accuracy, while requiring fewer FLOPs and less memory. Based on this introduction, I expect the rest of the paper to focus on this point. But it does not; there are no experimental results on ensembles and no experimental evidence that the proposed approach in able to avoid the speed and memory cost of ensembles while also retaining the accuracy benefit.
Second, the technical contribution of the paper is presented as group orthogonality (GO). However, in Sec 4.1 the idea of background feature suppression is introduced. While some motivation for it is given, the motivation does not tie into GO. GO does not require bg suppression and the introduction of it seems ad hoc. Moreover, the experiments never decouple GO and bg suppression, so we are unable to understand how GO works on its own. This is a critical experimental flaw in my reading.
Minor suggestions / comments:
- The equation in definition 2 has an incorrect normalizing factor (1/c^(k)^2)
- Figure 1 seems to have incorrect mask placements. The top mask is one that will mask out the background and only allow the fg to pass