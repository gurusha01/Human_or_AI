The paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a Bi-LSTM. The proposed models outperform many existing models in the literature on 3 sentiment analysis datasets. 
The key idea of using Bi-LSTM to compute global context for attention is actually not novel, as proposed several times in the literature, e.g., Luong et al (2015) and Shen & Lee (2016). Especially, Luong et al (2015) already proposed to combine global context with local context for attention.
Regarding to the experiments, of course it would be nice if the model can work well without the need of tricks like dropout or pre-trained word embeddings. However, it would be even better if the model can work well using those tricks. The authors should show results of the models using those tricks and compare them to the results in the literature.  
Ref:
Luong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015