The paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for CNNs.
The first contribution is more about engineering, but the authors make the source code available which is greatly appreciated.
The second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains. Imbuing knowledge of running speed into a pruning algorithm seems like the proper way to tackle this problem. The authors are very methodical in how they build the model and evaluate it very thoroughly.
It seems that the same idea could be used not just for pruning existing models, but also when building new architectures: selecting layers and their parameters as to achieve an optimal throughput rate. This could make for a nice direction for future work.
One point that is missing is some discussion of how transferable the performance model is to GPUs. This would make the technique easier to adopt broadly.
Other areas for improvement: The points in Figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the "base learning rate" in Section 3 is the start or end rate of the annealing schedule; typos: "punning" (p.4), "spares" (p.5).