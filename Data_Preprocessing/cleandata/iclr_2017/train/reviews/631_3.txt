This paper proposes a cascade of paired (left/right, up/down) 1D RNNs as a module in CNNs in order to quickly add global context information to features without the need for stacking many convolutional layers. Experimental results are presented on image classification and semantic segmentation tasks.
Pros:
- The paper is very clear and easy to read.
- Enough details are given that the paper can likely be reproduced with or without source code.
- Using 1D RNNs inside CNNs is a topic that deserves more experimental exploration than what exists in the literature.
Cons (elaborated on below):
(1) Contributions relative to, e.g. Bell et al., are minor.
(2) Disappointed in the actual use of the proposed L-RNN module versus how it's sold in the intro.
(3) Classification experiments are not convincing.
(1,2): The introduction states w.r.t. Bell et al. "more substantial differences are two fold: first, we treat the L-RNN module as a general block, that can be inserted into any layer of a modern architecture, such as into a residual module. Second, we show (section 4) that the
L-RNN can be formulated to be inserted into a pre-trained FCN (by initializing with zero recurrence
matrices), and that the entire network can then be fine-tuned end-to-end."
I felt positive about these contributions after reading the intro, but then much less so after reading the experimental sections. Based on the first contribution ("general block that can be inserted into any layer"), I strongly expected to see the L-RNN block integrated throughout the CNN starting from near the input. However, the architectures for classification and segmentation only place the module towards the very end of the network. While not exactly the same as Bell et al. (there are many technical details that differ), it is close. The paper does not compare to the design from Bell et al. Is there any advantage to the proposed design? Or is it a variation that performs similarly? What happens if L-RNN is integrated earlier in the network, as suggested by the introduction?
The second difference is a bit more solid, but still does not rise to a 'substantive difference' in my view. Note that Bell et al. also integrate 1D RNNs into an ImageNet pretrained VGG-16 model. I do, however, think that the method of integration proposed in this paper (zero initialization) may be more elegant and does not require two-stage training by first freezing the lower layers and then later unfreezing them.
(3) I am generally skeptical of the utility of classification experiments on CIFAR-10 when presented in isolation (e.g., no results on ImageNet too). The issue is that CIFAR-10 is not interesting as a task unto itself and methods that work well on CIFAR-10 do not necessarily generalize to other tasks. ImageNet has been useful because, thus far, it produces features that generalize well to other tasks. Showing good results on ImageNet is much more likely to demonstrate a model that learns generalizable features. However, that is not even necessarily true, and ideally I would like to see that that a model that does well on ImageNet in fact transfers its benefit to at least one other ask (e.g., detection).
One additional issue with the CIFAR experiments is that I expect to see a direct comparison of models A-F with and without L-RNN. It is hard to understand from the presented results if L-RNN actually adds much. In sum, I have a hard time taking away any valuable information from the CIFAR experiments.
Minor suggestion:
- Figure 4 is hard to read. The pixelated rounded corners on the yellow boxes are distracting.