This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating "causal factors", to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems. The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.  
- Not sure what the authors mean by "causal factors". There is a reference to it in Abstract and in Problem formulation on page 3 without any definition/discussion.
- In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each
- Authors outline two key disadvantages of the existing KPCA approach. The first one, that of low-dimensional manifold assumption not holding exactly, has received lots of attention in the machine learning literature. It is common to assume that the data lies near a low-dimensional manifold rather than on a low-dimensional manifold. Second disadvantage is somewhat unclear as finding "a data point (pre-image) corresponding to each projection in the input space" is not a standard step in KPCA. 
- On page 3, you never define $\mathcal{X} \times N$, $\mathcal{Y} \times N$, $\mathcal{H} \times N$. Clearly, they cannot be cartesian products. I have to assume that notation somehow implies N-tuples. 
- On page 3, Section 2, $\mathcal{X}$ and $\mathcal{Y}$ are sets. What do you mean by $\mathcal{Y} \ll \mathcal{X}$
- On page 5, $\mathcal{S}^n$ is never defined. 
- Experiments: None of the standard algorithms for matrix completion such as OptSpace or SVT were considered 
- Experiments: There is no comparison with alternate existing approaches for Non-rigid structure from motion.  
- Proof of the main result Theorem 3.1: To get from (16) to (17) using the Holder inequality (as stated) one would end up with a term that involves sum of fourth powers of weights w_{ij}. Why would they equal to one using the orthonormal constraints? It would be useful to give more details here, as I don't see how the argument goes through at this point.