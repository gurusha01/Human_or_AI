Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written.
In experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood. The fact that they do report this 'negative' result suggests good scientific taste. In a semi-supervised setting, the results are better.
Comments:
- sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc.
- sec 2.2: two comma's
- text flow eq 6: please refer to appendix with the closed-form KL divergence
- "The v's are sampled via" => "In the posterior, the v's are sampled via". It's not clear you're talking about the posterior here, instead of the prior.
- The last paragraph of section 4 is great.
- Sec 7.1: "Density estimation" => Technically you're also doing mass estimation.
- Sec 7.1: 100 IS samples is a bit on the low side. 
- Figure 3(f). Interesting that k-NN works so well on raw pixels.