This paper investigates the benefits of visual servoing using a learned
visual representation. The authors  propose to first learn an action-conditional
bilinear model of the visual features (obtained from a pre-trained VGG net) from
which a policy can be derived using a linearization of the dynamics. A multi-scale,
multi-channel and locally-connected variant of the bilinear model is presented.
Since the bilinear model only predicts the dynamics one step ahead, the paper
proposes a weighted objective which incorporates the long-term values of the
current policy. The evaluation problem is addressed using a fitted-value approach.
The paper is well written, mathematically solid, and conceptually exhaustive.
The experiments also demonstrate the benefits of using a value-weighted objective
and is an important contribution of this paper. This paper also seems to be the
first to outline a trust-region fitted-q iteration algorithm. The use of
pre-trained visual features is also shown to help, empirically, for generalization.
Overall, I recommend this paper as it would benefit many researchers in robotics.
However, in the context of this conference, I find the contribution specifically on
the "representation" problem to be limited. It shows that a pre-trained VGG
representation is useful, but does not consider learning it end-to-end. This is not
to say that it should be end-to-end, but proportionally speaking, the paper
spends more time on the control problem than the representation learning one.
Also, the policy representation is fixed and the values are approximated
in linear form using problem-specific features. This doesn't make the paper
less valuable, but perhaps less aligned with what I think ICLR should be about.