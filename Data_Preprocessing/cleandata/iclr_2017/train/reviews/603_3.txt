This paper studies the problem of source code completion using neural network models. A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY). Another minor tweak is the option to "deny prediction," which makes sense in the context of code completion in an IDE, as it's probably better to not make a prediction if the model is very unsure about what comes next.
Empirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals. However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is. Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code. Why not compute this and report a single number to summarize the performance?
Overall the paper is OK, but it has a flavor of "we ran LSTMs on an existing dataset". The results are OK but not amazing. There are also some issues with the writing that could be improved (see below). In total, I don't think there is a big enough contribution to warrant publication at ICLR.
Detailed comments:
* I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state.
* The discussion of related work needs reworking. For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong