This paper presents an architecture and corresponding algorithms for
learning to act across multiple tasks, described in natural language.
The proposed system is hierarchical and is closely related to the options
framework. However, rather than learning a discrete set of options, it learns
a mapping from natural instructions to an embedding which implicitly (dynamically)
defines an option. This is a novel and interesting new perspective on options
which had only slightly been explored in the linear setting (see comments below).
I find the use of policy distillation particularly relevant for this setting.
This, on its own, could be a takeaway for many RL readers who might not necessarily
be interested about NLP applications.
In general, the paper does not describe a single, simple, end-to-end,
recipe for learning with this architecture. It rather relies on many recent
advances skillfully combined: generalized advantage estimation, analogy-making
regularizers, L1 regularization, memory addressing, matrix factorization,
policy distillation. I would have liked to see some analysis but
understand that it would have certainly been no easy task.
For example, when you say "while the parameters of the subtask controller are
frozen", this sounds to me like you're having some kind of two-timescale stochastic gradient
descent. I'm also unsure how you deal with the SMDP structure in your gradient
updates when you move to the "temporal abstractions" setting.
I am inclined to believe that this approach has the potential to scale up to
very large domains, but paper currently does not demonstrate this
empirically. Like any typical reviewer, I would be tempted to say that
you should perform larger experiments. However, I'm also glad that you have
shown that your system also performs well in a "toy" domain. The characterization
in figure 3 is insightful and makes a good point for the analogy regularizer
and need for hierarchy.
Overall, I think that the proposed architecture would inspire other researchers
and would be worth being presented at ICLR. It also contains novel elements
(subtask embeddings) which could be useful outside the deep and NLP communities
into the more "traditional" RL communities.
Parameterized Options
Sutton et. al (1999) did not explore the concept
of parameterized options originally. It only came later, perhaps first with
["Optimal policy switching algorithms for reinforcement
learning, Comanici & Precup, 2010"] or
["Unified Inter and Intra Options Learning Using Policy Gradient Methods", Levy & Shimkin, 2011].
Konidaris also has a line of work  on "parametrized skills":
["Learning Parameterized Skills". da Silva, Konidaris, Barto, 2012)]
or ["Reinforcement Learning with Parameterized Actions". Masson, Ranchod, Konidaris, 2015].
Also, I feel that there is a very important distinction to be made with
the expression "parametrized options". In your work, "parametrized" comes in
two flavors. In the spirit of policy gradient methods,
we can have options whose policies and termination functions are represented
by function approximators (in the same way that we have function approximation
for value functions). Those options have parameters and we might call them
"parameterized" because of that. This is the setting of Comanicy & Precup (2010),
Levy & Shimkin (2011) Bacon & Precup (2015), Mankowitz, Mann, and
Mannor (2016) for example.
Now, there a second case where options/policies/skills take parameters as inputs
and act accordingly. This is what Konidaris & al. means by "parameterized", whose
meaning differs from the "function approximation" case above.
In your work, the embedding of subtasks arguments is the "input" to your options
and therefore behave as "parameters" in the sense of Konidaris.
Related Work
I CTRL-F through the PDF but couldn't find references to any of S.R.K. Branavan's
work. Branavan's PhD thesis had to do with using control techniques from RL
in order to interpret natural instructions so as to achieve a goal. For example,
in "Reinforcement Learning for Mapping Instructions to Actions", an RL agent
learns from "Windows troubleshooting articles" to interact with UI elements
(environment) through a Softmax policy (over linear features) learned by policy
gradient methods.
As you mention under "Instruction execution" the focus of your work in
on generalization, which is not treated explicitely (afaik) in Branavan's work.
Still, it shares some important algorithmic and architectural similarities which
should be discussed explicitly or perhaps even compared to in your experiments
(as a baseline).
Zero-shot and UVFA
It might also want to consider
"Learning Shared Representations for Value Functions in Multi-task
Reinforcement Learning", Borsa, Graepel, Shawe-Taylor]
under the section "zero-shot tasks generalization". 
Minor Issues
I first read the abstract without knowing what the paper would be about
and got confused in the second sentence. You talk about "longer sequences of
previously seen instructions", but I didn't know what clearly
meant by "instructions" until the second to last sentence where you specify
"instructions described by natural language." You could perhaps
re-order the sentences to make it clear in the second sentence that you are
interested in NLP problems.
Zero-generalization: I was familiar with the term "one-shot" but not "zero-shot".
The way that the second sentence "[...] to have similar zero-shot [...]" follows
from the first sentence might as well hold for the "one-shot" setting. You
could perhaps add a citation to "zero-shot", or define it more
explicitly from the beginning and compare it to the one-shot setting. It could
also be useful if you explain how zero-shot relates to just the notion of
learning with "priors".
Under section 3, you say "cooperate with each other" which sounds to me very much
like a multi-agent setting, which your work does not explore in this way.
You might want to choose a different terminology or explain more precisely if there
is any connection with the multi-agent setting.
The second sentence of section 6 is way to long and difficult to parse. You could
probably split it in two or three sentences.