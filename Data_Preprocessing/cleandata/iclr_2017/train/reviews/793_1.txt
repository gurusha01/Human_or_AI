Summary:
This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks.
Contributions:
The introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps.
Questions:
A point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input.
Criticisms:
The paper is really badly written, authors should rethink the organization of the paper.
Most of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. 
The justification is not convincing enough.
Experimental results are lacking, only results on a single dataset are provided.
Although the authors claim that they got SOTA on enwiki8, there are other papers such as the HyperNetworks that got better results (1.34) than the result they achieve. This claim is wrong.
The model requires the ground-truth labels for the test-set, however, this assumption really limits the application of this technique to a very limited set of applications(more or less rules out most conditional language modeling tasks).
High-level Review:
    Pros: 
        - A simple modification of the model that seems to improve the results and it is an interesting modification.
    Cons:
       - The authors need to use test-set labels.
       - Writing of the paper is bad.
       - The authors assume that they have access to the ground-truth labels during the test-set.
       - Experimental results are lacking