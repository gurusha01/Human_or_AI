This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. 
The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. 
The authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. 
The experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.
The authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.
The paper contains errors:
- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!
- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. 
- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. 
- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. 
It is not clear to me why the author say for LVMs such as GPLVM that "the latent space is learned a priority with clean training data". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  
It is not clear what the authors mean in the paper by "pre-training" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.