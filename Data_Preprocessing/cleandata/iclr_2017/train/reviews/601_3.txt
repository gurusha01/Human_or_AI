Summary: The paper proposes a novel machine comprehension dataset called NEWSQA. The dataset consists of over 100,000 question answer pairs based on over 10,000 news articles from CNN. The paper analyzes the different types of answers and the different types of reasoning required to answer questions in the dataset. The paper evaluates human performance and the performance of two baselines on the dataset and compares them with the performance on SQuAD dataset. 
Strengths:
1. The paper presents a large scale dataset for machine comprehension. 
2. The question collection method seems reasonable to collect exploratory questions. Having an answer validation step is desirable.
3. The paper proposes a novel (computationally more efficient) implementation of the match-LSTM model.
Weaknesses:
1. The human evaluation presented in the paper is not satisfactory because the human performance is reported on a very small subset (200 questions). So, it seems unlikely that these 200 questions will provide a reliable measure of the human performance on the entire dataset (which consists of thousands of questions).
2. NEWSQA dataset is very similar to SQuAD dataset in terms of the size of the dataset, the type of dataset -- natural language questions posed by crowdworkers, answers comprising of spans of text from related paragraphs. The paper presents two empirical ways to show that NEWSQA is more challenging than SQuAD -- 1) the gap between human and machine performance in NEWSQA is larger than that in SQuAD. However, since the human performance numbers are reported on very small subset, these trends might not carry over when human performance is computed on all of the dataset.
2) the sentence-level accuracy on SQuAD is higher than that in NEWSQA. However, as the paper mentions, the differences in accuracies could likely be due to different lengths of documents in the two datasets. So, even this measure does not truly reflect that SQuAD is less challenging than NEWSQA.
So, it is not clear if NEWSQA is truly more challenging than SQuAD.
3. Authors mention that BARB is computationally more efficient and faster compared to match-LSTM. However, the paper does not report how much faster BARB is compared to match-LSTM.
4. On page 7, under "Boundary pointing" paragraph, the paper should clarify what "s" in "n_s" refers to.
Review summary: While the dataset collection method seems interesting and promising, I would be more convinced after I see the following --
1. Human performance on all (or significant percentage of the dataset).
2. An empirical study that fairly shows that NEWSQA is more challenging (or better in some other way) than SQuAD.