This is a very nice paper. The writing of the paper is clear. It starts from the traditional attention mechanism case. By interpreting the attention variable z as a distribution conditioned on the input x and query q, the proposed method naturally treat them as latent variables in graphical models. The potentials are computed using the neural network.
Under this view, the paper shows traditional dependencies between variables (i.e. structures) can be modeled explicitly into attentions. This enables the use of classical graphical models such as CRF and semi-markov CRF in the attention mechanism to capture the dependencies naturally inherit in the linguistic structures.
The experiments of the paper prove the usefulness of the model in various level â€” seq2seq and tree structure etc. I think it's solid and the experiments are carefully done. It also includes careful engineering such as normalizing the marginals in the model.
In sum, I think this is a solid contribution and the approach will benefit the research in other problems.