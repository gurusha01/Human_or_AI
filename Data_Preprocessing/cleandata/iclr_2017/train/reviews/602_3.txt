SUMMARY.
The paper proposes a machine reading approach for cloze-style question answering.
The proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).
GA calculates the compatibility of each word in the document and the query as a probability distribution.
For each word in the document a gate is calculated weighting the query representation according to the word compatibility.
Ultimately, the gate is applied to the gru-encoded document word.
The resulting word vectors are re-encoded with a bidirectional GRU.
This process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.
The probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.
The proposed model is tested on 4 different dataset. 
The authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.
----------
OVERALL JUDGMENT
The main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.
The paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.
The GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.
I would have liked to see some discussion on why the model works less well on the CBT dataset, though.
----------
DETAILED COMMENTS
minor. In the introduction, Weston et al., 2014 do not use any attention mechanism.