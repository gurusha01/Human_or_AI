This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular.   
Pros:
* This paper addresses an important question I and many others would have liked to know the answer to but didn't have the computational resources to thoroughly attack it.   This is a nice use of Google's resources to help the community. 
* The work appears to have been done carefully so that the results can be believed.
* The basic answer arrived at (that, in the "typical training environment" LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful.   Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper.
* The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work.  In sum, they're much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity.
* The point about the near-equivalence of capacity at equal numbers of parameters is very useful.   
* The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures.
* The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it's a practical problem that everyone working with these networks has but often isn't addressed. 
* The paper text is very clearly written.
Cons:
* The work on the UGRNNs and the +RNNs seems a bit preliminary.  I don't think that the authors have clearly shown that the +RNN should be "recommended" with the same generality as the GRU.   I'd at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel).   In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important.   I don't really mind having them in the paper though.   I guess the point of this paper is not really to be novel in the first place -- which is totally fine with me, though I don't know what the ICLR area chairs will think. 
* The paper gives short shrift to the details of the HP algorithm itself.  They do say: 
     "Our setting of the tuner's internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a   
     Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs"  
and give some good references, but I expect that actually trying to replicate this involves a lot of missing details.   
* I found some of the figures a bit hard to read at first, esp. Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness.   
* The neuroscience reference ("4.7 bits per synapse") seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained.  I guess it's just in the discussion, but it seems gratuitous.   Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be "in agreement" here between computational architectures and neuroscience, but perhaps they could say something like -- "We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience.")