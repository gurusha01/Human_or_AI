The paper extends the NTM by a trainable memory addressing scheme.
The paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.
Pros:
* Extension to NTM with trainable addressing.
* Experiments with discrete addressing.
* Experiments on bAbI QA tasks.
Cons:
* Big gap to MemN2N and DMN+ in performance.
* Code not available.
* There could be more experiments on other real-world tasks.
I think the footnote is missing.