This paper examines the so called "Sample Importance" of each sample of a training data set, and its effect to the overall learning process.
The paper shows empirical results that shows different training cases induces bigger gradients at different stages of learning and different layers.
The paper shows some interesting results contrary to the common curriculum learning ideas of using easy training samples first. However, it is unclear how one should define "Easy" training cases.
In addition, the experiments demonstrating ordering either NLL or SI is worse than mixed or random batch construction to be insightful.
Possible Improvements:
It would be nice to factor out the magnitudes of the gradients to the contribution of "sample importance". Higher gradient (as a function of a particular weight vector) can be affected weight/initialization, thereby introducing noise to the model.
It would also be interesting if improvements based on "Sample importance" could be made to the batch selection algorithm to beat the baseline of random batch selection.
Overall this paper is a good paper with various experiments examining how various samples in SGD influences the various aspect of training.