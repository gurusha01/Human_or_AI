The paper proposes a method to combine arbitrary content into recommender systems, such as images, text, etc. These various features have been previously used to improve recommender systems, though what's novel here is the contribution of a general-purpose framework to combine arbitrary feature types.
Positively, the idea of combining many heterogeneous feature types into RS is ambitious and fairly novel. Previous works have certainly sought to include various feature types to improve RSs, though combining different features types successfully is difficult.
Negatively, there are a few aspects of the paper that are a bit ad-hoc. In particular:
-- There are a lot of pieces here being "glued together" to build the system. Different parts are trained separately and then combined together using another learning stage. There's nothing wrong with doing things in this way (and indeed it's the most straightforward and likely to work approach), but it pushes the contribution more toward the "system building" direction as opposed to the "end-to-end learning" direction which is more the focus of this conference.
-- Further to the above, this makes it hard to say how easily the model would generalize to arbitrary feature types, say e.g. if I had audio or video features describing the item. To incorporate such features into the system would require a lot of implementation work, as opposed to being a system where I can just throw more features in and expect it to work.
The pre-review comments address some of these issues. Some of the responses aren't entirely convincing, e.g. it'd be better to have the same baselines across tables, rather than dropping some because "the case had already been made elsewhere".
Other than that, I like the effort to combine several different feature types in real recommender systems datasets. I'm not entirely sure how strong the baselines are, they seem more like ablation-style experiments rather than comparison against any state-of-the-art RS.