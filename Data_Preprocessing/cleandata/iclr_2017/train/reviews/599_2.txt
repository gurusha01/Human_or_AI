This paper proposed a way to deal with supervised multivariate time series tasks involving missing values. The high level idea is still using the recurrent neural network (specifically, GRU in this paper) to do sequence supervised learning, e.g., classification, but modifications have been made to the input and hidden layers of RNNs to tackle the missing value problem. 
pros: 
1) the insight of utilizing missing value is critical. the observation of decaying effect in the healthcare application is also interesting;
2) the experiment seems to be solid; the baseline algorithms and analysis of results are also done properly. 
cons:
1) the novelty of this work is not enough. Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture. 
2) the datasets used in this paper are small. 
3) the decaying effect might not be able to generalize to other domains.