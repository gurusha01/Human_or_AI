The paper introduces Gated Multimodal Units GMUs, which use multiplicative weights to select the degree to which a hidden unit will consider different modalities in determining its activation.  The paper also introduces a new dataset, "Multimodal IMDb," consisting of over 25k movie summaries, with their posters, and labeled genres.
GMUs are related to "mixture of experts" in that different examples will be classified by different parts of the model, (but rather than routing/gating entire examples, individual hidden units are gated separately).  They are related to attention models in that different parts of the input are weighted differently; there the emphasis is on gating modalities of input.
The dataset is a very nice contribution, and there are many experiments varying text representation and single-modality vs two-modality.  What the paper is lacking is a careful discussion, experimentation and analysis in comparison to other multiplicative gate models---which is the core intellectual contribution of the paper.  For example, I could imagine that a mixture of experts or attention models or other gated models might perform very well, and at the very least provide interesting scientific comparative analysis.  I encourage the authors to continue the work, and submit a revised paper when ready.
As is, I consider the paper to be a good workshop paper, but not ready for a major conference.