This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN's(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training.
The method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position pt (starts at a selected position p0) and merges layer accordingly hence slightly adopts DIB. 
The empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation.  Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary.
In addition, it is not clear how sensitive the boosting to the selection of injection point.
This paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant.
Pros
-provides some preliminary results for boosting of Res Nets
Cons
-not sufficiently novel: an incremental approach 
-empirical analysis is not satisfactory
- Can you give the details of the experiment setup e.g. parameters to be tuned, algorithm to train at each step of boosting etc? Also can you give the details of networks architecture and references? 
- Can you elaborate on comparison to state of resNet variants, dense convolutional network?
- Can you give also comparison on training time?
- Do you have any result on Imagenet?