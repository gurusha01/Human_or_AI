The submission considers the setting of 2-sample testing from the perspective of evaluating a classifier.  For a classifier between two samples from the same distribution, the distribution of the classification accuracy follows a simple form under the null hypothesis.  As such, a straightforward threshold can be derived for any classifier.  Finding a more powerful test then amounts to training a better classifier.  One may then focus efforts, e.g. on deep neural networks, for which statistics such as the MMD may be very difficult to characterize.
+ The approach is sound and very general
+ The paper is timely in that deep learning has had huge impacts in classification and other prediction settings, but has not had as big an impact on statistical hypothesis testing as kernel methods have
- The discussion of the relationship to kernel-MMD has not always been as realistic as it could have been.  For example, the kernel-MMD can also be seen as a classifier based approach, so a more fair discussion could be provided.  Also, the form of kernel-MMD used in the comparisons is a bit contradictory to the discussion as well
 * The linear kernel-MMD is used which is less powerful than the quadradic kernel-MMD (the authors have justified this from the perspective of computation time)
 * The kernel-MMD is argued against due to its unwieldy distribution under the null, but the linear time kernel-MMD (see also Zaremba et al., NIPS 2013) has a Gaussian distribution under the null.
Arthur Gretton's comment from Dec 14 during the discussion period was very insightful and helpful.  If these insights and additional experiments comparing the kernel-MMD to the classifier threshold on the blobs dataset could be included, that would be very helpful for understanding the paper.  The open review format gives an excellent opportunity to assign proper credit for these experiments and insights by citing the comment.