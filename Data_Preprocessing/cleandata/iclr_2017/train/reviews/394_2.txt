Paper Summary
This paper proposes a variant of dropout, applicable to RNNs, in which the state
of a unit is randomly retained, as opposed to being set to zero. This provides
noise which gives the regularization effect, but also prevents loss of
information over time, in fact making it easier to send gradients back because
they can flow right through the identity connections without attenuation.
Experiments show that this model works quite well. It is still worse that
variational dropout on Penn Tree bank language modeling task, but given the
simplicity of the idea it is likely to become widely useful.
Strengths
- Simple idea that works well.
- Detailed experiments help understand the effects of the zoneout probabilities
  and validate its applicability to different tasks/domains.
Weaknesses
- Does not beat variational dropout (but maybe better hyper-parameter tuning
  will help).
Quality
The experimental design and writeup is high quality.
Clarity
The paper clear and well written, experimental details seem adequate.
Originality
The proposed idea is novel.
Significance
This paper will be of interest to anyone working with RNNs (which is a large
group of people!).
Minor suggestion-
- As the authors mention - Zoneout has two things working for it - the noise and
  the ability to pass gradients back without decay. It might help to tease apart
the contribution from these two factors. For example, if we use a fixed
mask over the unrolled network (different at each time step) instead of resampling
it again for every training case, it would tell us how much help comes from the
identity connections alone.