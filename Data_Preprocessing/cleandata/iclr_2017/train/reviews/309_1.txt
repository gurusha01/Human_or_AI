This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL agent like A3C. Authors propose a bunch of auxiliary control tasks and auxiliary reward tasks and evaluate the agent in Labyrinth and Atari. Proposed UNREAL agent performs significantly better than A3C and also learns faster. This is definitely a good contribution to the conference. However, this is not a surprising result since adding additional auxiliary tasks that are relevant to the goal should always help in better and faster feature shaping. This paper is a proof of concept for this idea.
The paper is well written and easy to follow by any reader with deep RL expertise.
Can authors comment about the computational resources needed to train the UNREAL agent?
The overall architecture is quite complicated. Are the authors willing to release the source code for their model?
--------------------------------------------------------
After rebuttal:
No change in the review.