This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.
The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context). The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application.
While the paper describes the differences between the proposed approach and Gulcehre et al.'s approach, I find some of the claims either wrong or not that significant. For example, quoting from Section 1:
"Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel."
As far as I can tell, your model also uses the recent hidden state to form a query vector,  which is matched by the pointer network to previous words. Can you please clarify what you mean here?
In addition, quoting from section 3 which describes the model of Gulcehre et al.:
"Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use"
This is not correct. The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text.
Finally, in the following quote, also from section 3: 
"The pointer network is not used as a source of information for the switching network as in our model." 
It is not clear what the authors mean by "source of information" here. Is it the fact that the switching probability is part of the pointer softmax? I am wondering how significant this difference is.
With regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009). Can you please comment on the differences between your dataset and those as well?
I would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part.