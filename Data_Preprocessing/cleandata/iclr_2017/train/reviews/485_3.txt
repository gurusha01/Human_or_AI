SUMMARY 
This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. 
PROS 
Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. 
CONS 
The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). 
COMMENTS 
It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. 
Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. 
MINOR COMMENTS 
- Figure 1 could be referenced first in the text.  
- ``Color coded'' where the color codes what? 
- Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. 
- On page 5, mention how the orthogonal projection on S_k is realized in the network. 
- On page 6 ``divided into segments'' here `segments' is maybe not the best word. 
- On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean?