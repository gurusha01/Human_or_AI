This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.
The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.  The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.
At a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this.
However, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. 
- I cannot find where  f{symbkey} / f{voca} / f_{copy} are defined
- w^v, w^s are confusing.
- e_k seems to be the average of all previous fact embeddings? It is necessary to make it clear enough.
- (ht, ct) = fLSTM(x{t−1}, h{t−1})  ct is not used?
- The notion of "fact embeddings" is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end).  For the anchor / "topic-itself" facts, do you learn the embedding for the special relations and use the entity embeddings from TransE?
On generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first.  Most entities are multiple words, and it is necessary to keep that order. Also it might be helpful to incorporate some prior information, for example, it is common to only mention "Obama" for the entity "Barack Obama"?