The paper proposes an attention-based approach for video description. The approach uses three LSTMs and two attention mechanisms to sequentially predict words from a sequence of frames.
In the LSTM-encoder of the frames (TEM), the first attention approach predicts a spatial attention per frame, and computes the weighted average. The second LSTM (HAM) predicts an attention over the hidden states of the encoder LSTM.
The third LSTM which run temporally in parallel to the second LSTM generates the sentence, one word at a time.
Strength:
===============
-	The paper works on a relevant and interesting problem.
-	Using 2 layers of attention in the proposed way have to my knowledge not been used before for video description. The exact architecture is thus novel (but the work claims much more without sufficient attribution, see blow)
-	The experiments are evaluated on two datasets, MSVD and Charades, showing performance on the level of related work for MSVD and improvements for Charades.
Weaknesses:
===============
1.	Claims about the contribution/novelty of the model seem not to hold: 
1.1.	One of the main contributions is the Hierarchical Attention/Memory (HAM):
1.1.1.	It is not clear to me how the presented model (Eq 6-8), are significantly different from the presented model in Xu et al / Yao et al. While Xu et al. attends over spatial image locations and Yao et al. attend over frames, this model attends over encoded video representations h_v^i. A slight difference might be that Xu et al. use the same LSTM to generate, while this model uses an additional LSTM for the decoding.
1.1.2.	The paper states in section 3.2 "we propose fm to memorize the previous attention", however Hm^{t'-1} only consist of the last hidden state. Furthermore, the model f_m does not have access to the "attention" \alpha. This was also discussed in comments by others, but remains unclear.
1.1.3.	In the discussion of comments the authors claim that "attention not only is a function a current time step but also a function of all previous attentions and network states.": While it is true that there is a dependency but that is true also for any LSTM, however the model does not have access to the previous network states as Hg^{t'-1} only consist of the last hidden state, as well as Hm^{t'-1} [at least that is what the formulas say and what Figure 1 suggests]. 
1.1.4.	The authors claim to have multi-layer attention in HAM, however it remains unclear where the multi-layer comes from.
1.2.	The paper states that in section 3.1. "[CNN] features tend to discard the low level information useful in modeling the motion in the video (Ballas et al., 2016)." This suggests that the approach which follows attacks this problem. However, it cannot model motion as attention \rho between frames is not available when predicting the next frame. Also, it is not clear how the model can capture anything "low level" as it operates on rather high level VGG conv 5 features.
2.	Related work: The difference of HAM to Yao et al. and Xu et al. should be made more clear / or these papers should be cited in the HAM section.
3.	Conceptual Limitation of the model: The model has two independent attention mechanisms, a spatial one, and a temporal one. The spatial (within a frame) is independent of the sentence generation. It thus cannot attend to different aspects of the frames for different words which would make sense. E.g. if the sentence is "the dog jumps on the trampoline", the model should focus on the dog when saying "dog" and on the trampoline when saying "trampoline", however, as the spatial attention is fixed this is difficult. Also, the encoder model does not have an explicitly way to look at different aspects in the frame during the encoding so might likely get stuck and always predict the same spatial attention for all frames (or it might e.g. always attend to the dog which moves around, but never on the scene).
4.	Eq 11 contradicts Fig 1: How is the model exactly receiving the previous word as input. Eq. 11 suggests it is the softmax. If this is the case, the authors should emphasize this in the text as this is unusual. More common would be to use the ground truth previous word during training (which Fig 11 suggests) and the "hardmax", i.e. the highest predicted previous word encoded as one-hot vector at test time.
5.	Clarity:
5.1.	It would be helpful if the same notation would be used in Eq 2-5 and 6-9. Why is a different notation required?
5.2.	It would be helpful if Fig 1 could contain more details or additional figures for the corresponding parts would be added. If space is a problem, e.g. the well-known equations for LSTM, softmax (Eq 2), and log likelihood loss (Eq 12) could be omitted or inlined.
6.	Evaluation:
6.1.	The paper claims that the "the proposed architecture outperforms all previously proposed methods and leads to a new state of the art results".
6.1.1.	For the MSVD dataset this clearly is wrong, even given the same feature representation. Pan et al. (2016 a) in Table 2 achieve higher METEOR (33.10).
6.1.2.	For this strong claim, I would also expect that it outperforms all previous results independent of the features used, which is also wrong again, Yu et al achieve higher performance in all compared metrics.
6.1.3.	For Charades dataset, this claim is also too bold as hardly any methods have been evaluated on this dataset, so at least all the ablations reported in Table 1 should also be reported for the Charades dataset, to make for this dataset any stronger claims.
6.2.	Missing qualitative results of attention: The authors should show qualitative results of the attention, for both attention mechanisms to understand if anything sensible is happening there. How diverse is the spatial and the temporal attention? Is it peaky or rather uniform?
6.3.	Performance improvement is not significant over model ablations: The improvements over Att+No TEM is only 0.5 Meteor, 0.7 Blue@4 and the performance drops for CIDEr by 1.7.
6.4.	Missing human evaluation: I disagree with the authors that a human evaluation is not feasible. 1. An evaluation on a subset of the test data is not so difficult. 2. Even if other authors do not provide their code/model [and some do], they are typically happy to share the predicted sentences which is sufficient and even better for human evaluation [if not I would explicitly mention that some authors did not share sentences, as this seems clearly wrong]. 3. For model ablations the sentences are available to the authors.
7.	Several of the comments raised by reviewers/others have not yet been incorporated in a revised version of the paper and/or are still not clear from the explanations given. E.g. including SPICE evaluation and making fixes seems trivial.	
8.	Hyperparameters are inconsistent: Why are the hyperparemters inconsistent between the ablation analysis (40 frames are sampled) and the performance comparison (8 frames)? Should this not be selected on the validation set? What is the performance of all the ablations with 8 frames?
Other (minor/discussion points)
-	Equation 10: what happens with hm, and hg, the LSTM formulas provided only handle two inputs. Are hm and hg concatenated.
-	There is a section 4.1 but no 4.2.
-	The paper states in section 4.1 "our proposed architecture can alone not only learn a representation for video that can model the temporal structure of a video sequence, but also a representation that can effectively map visual space to the language space." However, this seems to be true also for many/most other approaches, e.g. [Venugopalan et al. 2015 ICCV]
Summary:
===============
While the paper makes strong claims w.r.t. to the approach and results, the approach lacks novelty and the results are not convincing over related work and ablations. Furthermore, improved clarity and visualizations of the model and attention results would benefit the paper.