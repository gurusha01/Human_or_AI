This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory.
The model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik).
The model is evaluated on a set of toy problems (the "babi task") and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years.  
In terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., "softplus"), overloading notations (w_t, b…), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?…).
Overall it is very hard to put together all the pieces of this model(s), there is no code available and I'm afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models.
The model described in his paper is quite complicated and reproducing the method from its description may be challenging: Are you planing to release the code and what is your estimate release date?
Different cost functions and regularization are introduce in the paper. Would it be possible to summarize the overall cost function minimized by this model? 
Some variables seems to have different definition. In particular w_t and b, would it be possible to clarify this?
"gammat is a shallow MLP": What does it mean? Is gammat a function or a variable? It seems from eq. (10) that it is a vector (or a scalar?).
"curriculum learning for the discrete attention": Can you compare this to simpler schemes? Like rounding the continuous attention?
In your introduction, you state that "it is possible to use the discrete non-differentiable attention mechanism", referencing Zaremba & Sutskever, 2016 that use REINFORCE and a simple controller. However, in your sec. 4, you are stating that  "Training discrete attention with feed-forward controller and REINFORCE is challenging". It seems that these two statements contradict each other, could you comment on it? 
In the introduction, you state that "Memory network (Weston et al. 2015b) [..] uses an attention-based mechanism to index them". To the best of my knowledge, it is actually Sukhbaatar et al., 2015 that has introduce the attention mechanism to memory networks. 
In the introduction, the authors state that "memory networks […] [are] used in real tasks (Bordes et al. 2015, Dodge et al. 2015)". However in both papers, they use memory network systems different from Weston et al. The current statement is quite misleading, would it be possible to clarify it?