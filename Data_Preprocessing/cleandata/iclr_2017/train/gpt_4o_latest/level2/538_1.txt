Review of the Paper: "Neural Equivalence Networks for Learning Continuous Semantic Representations of Symbolic Expressions"
This paper introduces Neural Equivalence Networks (EQNETs), a novel architecture aimed at learning continuous semantic representations (SEMVECs) of symbolic expressions, such as mathematical and logical expressions. The authors argue that EQNETs address key challenges in compositionality and semantic equivalence, outperforming existing architectures like recursive neural networks (TREENNs). The paper's contributions include: (a) formalizing the problem of learning SEMVECs, (b) proposing EQNETs with innovations like subexpression forcing and output normalization, and (c) demonstrating superior performance on diverse datasets of boolean and polynomial expressions.
Decision: Accept  
Key Reasons:  
1. Novelty and Impact: The paper tackles the underexplored problem of learning continuous representations for symbolic equivalence, presenting a significant innovation over existing methods. The introduction of subexpression forcing and multi-layer residual networks is compelling and well-motivated.  
2. Strong Empirical Evidence: The exhaustive experimental evaluation demonstrates that EQNETs outperform state-of-the-art baselines across multiple datasets and metrics. The results are robust, covering both seen and unseen equivalence classes, and the compositionality transfer experiments further validate the model's generalization capabilities.
Supporting Arguments:  
1. Claims and Support: The paper's claims are well-supported by rigorous experiments. The authors provide quantitative evidence (e.g., k-NN metrics) and qualitative analyses (e.g., PCA visualizations) to demonstrate EQNET's ability to cluster semantically equivalent expressions while distinguishing non-equivalent ones. The ablation studies convincingly show the importance of subexpression forcing and output normalization.  
2. Positioning in Literature: The paper situates itself well within the context of related work, highlighting the limitations of prior approaches (e.g., TREENNs) and clearly articulating how EQNETs overcome these challenges.  
3. Usefulness: The proposed approach has practical implications for fields like program synthesis, theorem proving, and symbolic reasoning, making it highly relevant to the AI and ML communities.
Suggestions for Improvement:  
1. Scalability: While the authors acknowledge the limitations of fixed-size SEMVECs, it would be helpful to discuss potential directions for scaling EQNETs to handle more complex symbolic expressions. Could variable-sized representations or hierarchical embeddings address this limitation?  
2. Comparison with Alternative Objectives: The paper briefly mentions that a Siamese objective was unstable, but more details on why it failed and how EQNET's max-margin objective resolves these issues would strengthen the argument.  
3. Dataset Diversity: The datasets used are synthetic and limited to boolean and polynomial expressions. Expanding the evaluation to real-world symbolic tasks (e.g., programming languages or theorem proving benchmarks) would enhance the paper's practical relevance.  
4. Interpretability: While the visualizations are insightful, further analysis of why EQNET confuses certain expressions (e.g., XOR operations) could provide deeper insights into its limitations.
Questions for the Authors:  
1. How does EQNET handle noise or errors in the training data? Would it generalize well to noisy equivalence classes?  
2. Could EQNETs be extended to handle symbolic expressions with probabilistic or fuzzy semantics?  
3. How sensitive is the model to hyperparameter choices, particularly the weight of the subexpression forcing term (Âµ)?  
Overall, this paper presents a significant and well-executed contribution to the field of representation learning for symbolic reasoning. While there are areas for further exploration, the work is robust, novel, and impactful, warranting acceptance at the conference.