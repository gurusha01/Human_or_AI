Review of the Paper
This paper introduces a novel approach to training probabilistic models with discrete latent variables using the variational autoencoder (VAE) framework. The authors propose a discrete VAE architecture that incorporates an undirected discrete component (a Restricted Boltzmann Machine, RBM) and a directed hierarchical continuous component. This combination allows the model to capture both discrete classes and their continuous variations. The key contribution is the development of a method to enable backpropagation through discrete latent variables by augmenting them with continuous smoothing variables. The proposed model achieves state-of-the-art performance on several benchmark datasets, including permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes.
Decision: Accept
The paper is well-motivated and addresses a significant challenge in unsupervised learning: the difficulty of training models with discrete latent variables. The proposed method is novel, scientifically rigorous, and demonstrates strong empirical results. However, there are areas where the clarity and presentation of the paper could be improved.
Supporting Arguments
1. Novelty and Contribution: The paper makes a significant contribution by bridging the gap between discrete latent variable models and the VAE framework. The introduction of continuous smoothing variables to enable backpropagation through discrete latent variables is a novel and impactful idea. The hierarchical structure of the approximating posterior and the addition of continuous latent variables further enhance the model's representational power.
2. Empirical Validation: The authors provide extensive experimental results, demonstrating that the proposed discrete VAE outperforms existing methods on multiple datasets. The inclusion of ablation studies and comparisons with simplified architectures strengthens the validity of the claims.
3. Scientific Rigor: The paper provides a detailed theoretical foundation, including derivations of the evidence lower bound (ELBO) and gradient approximations. The use of bridge sampling to estimate the log partition function of the RBM is a thoughtful addition that ensures accurate evaluation of the model's performance.
Additional Feedback
1. Clarity: The paper is dense and highly technical, which may make it challenging for readers unfamiliar with the topic. Simplifying the presentation of key ideas, particularly in Sections 2 and 3, would improve accessibility. For example, the explanation of the spike-and-exponential smoothing transformation could benefit from additional visual aids or intuitive examples.
2. Limitations: While the paper acknowledges the high variance of gradient estimates in certain cases, it does not provide a detailed discussion of the computational cost of the proposed method, particularly the use of Gibbs sampling and hierarchical approximating posteriors. Including a comparison of training times with baseline methods would be helpful.
3. Broader Impact: The paper focuses primarily on technical details and empirical results. A discussion of potential applications and broader implications of discrete VAEs in real-world scenarios would enhance the paper's impact.
Questions for the Authors
1. How does the computational cost of the proposed method compare to baseline models, particularly in terms of training time and memory requirements?
2. The RBM component appears to struggle with datasets like Omniglot, which have a large number of classes. Could the authors comment on how the model might be scaled or adapted to handle such cases more effectively?
3. Have the authors explored alternative smoothing transformations beyond the spike-and-exponential approach, and how do they compare in terms of performance and stability?
In summary, this paper presents a novel and impactful contribution to the field of unsupervised learning with discrete latent variables. While there are areas for improvement in clarity and discussion, the scientific rigor and empirical results justify acceptance.