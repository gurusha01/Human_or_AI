Review
This paper addresses the critical problem of defending deep neural networks (DNNs) against adversarial examples by empirically comparing various defensive strategies and introducing an improved framework. The authors propose an enhanced adversarial retraining framework (RAD) and an improved AutoEncoder stacked with a Classifier (IAEC) to assess robustness, cross-model generalization, and resilience against additional attacks. The paper also evaluates vulnerabilities introduced by these defensive strategies and provides comprehensive experimental results on MNIST and CIFAR-10 datasets. The contributions are well-structured, with a focus on both theoretical and empirical evaluations, making the work relevant to the field of adversarial machine learning.
Decision: Accept
Key reasons for acceptance:
1. Comprehensive Evaluation: The paper provides a thorough empirical analysis of defensive strategies, including RAD, AEC, IAEC, and Distillation, across various adversarial models. The cross-model generalization and resilience against repeated attacks are particularly well-analyzed.
2. Novel Contributions: The improved IAEC with a cross-entropy regularizer and the robust RAD framework demonstrate significant advancements over existing methods. The RAD framework's ability to defend against black-box attacks without prior knowledge of the adversary model is a notable innovation.
Supporting Arguments:
1. Robustness and Generalization: The RAD framework consistently outperforms other defensive strategies in terms of classification error and resilience against diverse adversarial attacks. The cross-model evaluation highlights its potential as a universal defensive strategy.
2. Experimental Rigor: The experiments are well-designed, with clear comparisons across multiple datasets, adversarial models, and defensive strategies. The inclusion of distortion analysis to measure vulnerabilities adds depth to the evaluation.
3. Practical Utility: The RAD framework's ability to defend against both one-step and iterative attacks, while maintaining low vulnerability penalties, makes it highly applicable in real-world scenarios.
Additional Feedback:
1. Clarity of Presentation: While the paper is comprehensive, some sections, particularly those describing adversarial models and experimental setups, are overly dense. Simplifying the descriptions or adding visual summaries (e.g., diagrams) could improve readability.
2. Limitations and Future Work: Although the paper acknowledges limitations, such as RAD's reduced effectiveness against iterative attacks like coordinate greedy (cg) and Adam, a more detailed discussion on addressing these limitations would strengthen the work. For instance, exploring hybrid defensive strategies or adaptive retraining could be promising directions.
3. Broader Applicability: While the focus on MNIST and CIFAR-10 is appropriate for benchmarking, extending the analysis to larger and more complex datasets (e.g., ImageNet) would enhance the generalizability of the findings.
Questions for Authors:
1. How does the RAD framework perform in terms of computational efficiency compared to other methods, especially when scaling to larger datasets or more complex architectures?
2. Could the RAD framework be combined with other defensive strategies, such as Distillation, to further enhance robustness against iterative attacks like cg and Adam?
3. How sensitive is the RAD framework to the choice of adversarial models used for retraining? Would including a broader range of adversarial examples improve its generalization further?
In conclusion, this paper makes a significant contribution to the field by advancing the understanding of adversarial defenses and proposing a robust framework with strong empirical support. While there is room for improvement in presentation and broader applicability, the work is well-motivated, scientifically rigorous, and practically useful, warranting acceptance.