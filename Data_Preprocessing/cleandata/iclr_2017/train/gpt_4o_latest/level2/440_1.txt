Review of the Paper
The paper presents a novel algorithm for policy search in stochastic dynamical systems using Bayesian neural networks (BNNs) with stochastic input variables, trained via α-divergence minimization (α = 0.5). The authors claim that their approach captures complex stochastic patterns, such as multi-modality and heteroskedasticity, which are often missed by alternative methods. The paper demonstrates the effectiveness of the proposed method through experiments on challenging benchmarks, including the Wet-Chicken problem, gas turbine control, and an industrial benchmark.
Decision: Accept
The paper is well-motivated, presents a significant contribution to model-based reinforcement learning (RL), and demonstrates strong empirical results. The key reasons for this decision are: (1) the novelty of using BNNs with stochastic inputs to model complex dynamics, and (2) the demonstrated success of the method in solving challenging benchmarks where existing approaches struggle.
Supporting Arguments
1. Novelty and Contribution: The use of BNNs with stochastic inputs to model transition dynamics is innovative, addressing limitations in prior methods that assume deterministic or Gaussian noise. The α-divergence minimization with α = 0.5 is a notable improvement over variational Bayes (VB), as it balances fitting local modes and global posterior coverage.
2. Experimental Validation: The paper provides rigorous empirical evidence. The proposed method outperforms baselines (e.g., Gaussian Processes, VB-trained BNNs) in terms of predictive accuracy and policy performance across multiple benchmarks. The Wet-Chicken problem, in particular, highlights the method's ability to handle bi-modal and heteroskedastic dynamics.
3. Practical Usefulness: The method is well-suited for industrial applications, as demonstrated by its success in controlling gas turbines and the industrial benchmark. Its ability to work with batch data and avoid unsafe exploration makes it highly relevant for real-world scenarios.
Additional Feedback
1. Clarity: While the technical details are thorough, the presentation could be improved for accessibility. For example, the explanation of α-divergence minimization and its advantages over VB could be simplified for readers less familiar with Bayesian inference.
2. Limitations: The paper does not explicitly discuss the limitations of the approach, such as computational cost. Training BNNs with α-divergence minimization is computationally intensive, as noted in Appendix D. Including a discussion on scalability and potential trade-offs would strengthen the paper.
3. Comparison with Model-Free RL: While the focus is on model-based RL, a brief comparison with state-of-the-art model-free methods (e.g., PPO or SAC) would provide additional context for the method's performance.
4. Reproducibility: The inclusion of a public implementation is commendable. However, the paper could benefit from a more detailed discussion on hyperparameter sensitivity and guidelines for practitioners.
Questions for the Authors
1. How sensitive is the performance to the choice of α? Would values other than 0.5 yield similar results?
2. Can the method handle high-dimensional state-action spaces, or are there scalability concerns for larger problems?
3. How does the method perform in scenarios with sparse rewards or delayed feedback, which are common in RL tasks?
Overall, the paper makes a strong contribution to model-based RL and demonstrates the potential of BNNs with stochastic inputs for policy search in complex, real-world systems. With minor improvements in clarity and discussion of limitations, the paper is a valuable addition to the conference.