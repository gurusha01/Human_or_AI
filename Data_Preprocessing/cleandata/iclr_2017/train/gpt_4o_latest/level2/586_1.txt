The paper investigates the Neural GPU, a model designed to learn algorithms that generalize to inputs of arbitrary length, and proposes two key improvements: curriculum learning and increasing model size. These enhancements enable the Neural GPU to solve a broader range of algorithmic tasks, including arithmetic operations in decimal representation and evaluating complex expressions. The authors also explore failure modes, revealing that the Neural GPU struggles with highly symmetric or structured inputs, akin to adversarial examples. The paper provides a detailed empirical analysis of these findings and suggests that further research is needed to achieve perfect generalization.
Decision: Accept (with minor revisions)  
The paper makes a meaningful contribution to the field of program induction and algorithm learning by improving the Neural GPU's performance and expanding its capabilities. The proposed methods are well-motivated, and the experiments are thorough, providing valuable insights into the model's strengths and limitations. However, some areas require clarification and additional discussion to strengthen the paper.
Supporting Arguments:  
1. Novelty and Contribution: The paper addresses a critical gap in the literature by enabling the Neural GPU to handle decimal arithmetic and complex expressions, which were previously unsolved. The curriculum learning approach and memory-efficient implementation are practical and innovative contributions.  
2. Empirical Rigor: The experiments are extensive, covering multiple tasks, model sizes, and curricula. The use of multiple random seeds to evaluate generalization is commendable and adds robustness to the findings.  
3. Relevance: The work is well-situated within the literature, referencing key prior works and clearly articulating its advancements. The discussion of failure modes is particularly valuable for understanding the limitations of neural algorithm learners.
Additional Feedback:  
1. Clarity of Results: While the experiments are comprehensive, the presentation of results could be improved. For example, the figures and tables (e.g., Table 2, Fig. 3) should be more explicitly discussed in the text to ensure readers can easily interpret their significance.  
2. Failure Modes: The discussion of failure on highly structured inputs is intriguing but underexplored. Could the authors elaborate on why these cases are particularly challenging and propose potential architectural modifications to address them?  
3. Curriculum Design: The curriculum learning approach is effective but somewhat ad hoc. A more systematic exploration of curriculum design principles would enhance the paper's impact.  
4. Memory Efficiency: The implementation details for handling larger models are briefly mentioned but lack depth. A more detailed explanation of the memory optimization techniques would benefit readers seeking to replicate or extend this work.
Questions for the Authors:  
1. How does the choice of curriculum (e.g., base transitions in decimal multiplication) affect generalization across different tasks? Could a more generalized curriculum design framework be proposed?  
2. The paper mentions that larger models generalize better, but is there a trade-off in terms of training time or computational resources? How does this impact the practical usability of the Neural GPU?  
3. Could the authors provide more insight into the observed bimodal distribution of generalization performance across random seeds? Are there specific initialization strategies that consistently lead to better outcomes?
In conclusion, the paper presents significant advancements in algorithm learning with Neural GPUs and offers valuable insights into their generalization capabilities. Addressing the minor concerns raised above would further enhance its clarity and impact.