The paper proposes a novel activation function, Parametric Exponential Linear Unit (PELU), as an enhancement to the widely used Exponential Linear Unit (ELU). The authors claim that PELU improves performance by learning a parameterization of ELU, which provides greater flexibility in controlling vanishing gradients and bias shift. The paper demonstrates this through theoretical analysis and empirical results on CIFAR-10/100 and ImageNet datasets, showing consistent performance improvements across various architectures such as ResNet, NiN, and Vgg. Notably, PELU achieves up to 7.28% relative error reduction on ImageNet with a negligible parameter increase of 0.0003%.
Decision: Accept
The primary reasons for acceptance are the paper's clear contributions to the field and its strong empirical validation. The proposed PELU function is both novel and practically useful, offering a significant improvement over ELU with minimal computational overhead. The authors provide rigorous theoretical analysis to support their claims and demonstrate the utility of PELU across a range of architectures and datasets. The results are scientifically rigorous, with statistically significant improvements and detailed experimental setups.
Supporting Arguments:
1. Novelty and Contribution: The paper introduces a novel parameterization of ELU, which is a meaningful extension of existing activation functions. The theoretical analysis of vanishing gradients and bias shift is well-grounded and provides valuable insights into the behavior of PELU.
2. Empirical Validation: The experiments are thorough, covering multiple datasets (CIFAR-10/100, ImageNet) and architectures (ResNet, NiN, Vgg). The consistent performance improvements, particularly the 7.28% error reduction on ImageNet, highlight the practical utility of PELU.
3. Minimal Overhead: The addition of only 2L parameters (where L is the number of layers) ensures that PELU is computationally efficient. The negligible parameter increase makes it attractive for real-world applications.
Additional Feedback:
1. Limitations and Overfitting: While the paper acknowledges potential overfitting in deeper networks with PELU, this aspect could be explored further. For example, additional experiments with regularization techniques or alternative training regimes could provide more clarity.
2. Comparison with Other Parametric Functions: The paper briefly discusses related work, such as PReLU and Maxout, but a more detailed comparison of computational efficiency and performance trade-offs would strengthen the argument for PELU.
3. Broader Applicability: The authors suggest extending PELU to other architectures (e.g., RNNs) and tasks (e.g., object detection). Preliminary experiments in these areas would enhance the paper's impact and demonstrate the generalizability of PELU.
Questions for the Authors:
1. How does PELU perform in scenarios with limited data or under heavy regularization? Does its flexibility lead to overfitting in such cases?
2. Can the parameterization framework be extended to other activation functions, such as Softplus or Tanh, as suggested in the conclusion? If so, what challenges might arise?
3. How sensitive is PELU to hyperparameter choices, such as the initial values of a and b or the learning rate? Would adaptive learning rates improve performance further?
In conclusion, the paper makes a strong case for the adoption of PELU in CNNs, offering both theoretical insights and practical benefits. With minor refinements and additional experiments, it has the potential to make a significant impact on the field.