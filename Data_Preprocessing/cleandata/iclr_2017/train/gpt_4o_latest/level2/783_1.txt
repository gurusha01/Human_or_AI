Review of the Paper
Summary of Contributions
This paper revisits the trade-offs between synchronous and asynchronous distributed stochastic optimization for deep learning and proposes a novel approach: synchronous optimization with backup workers. The authors argue that this method mitigates the straggler problem in synchronous optimization while avoiding the gradient staleness inherent in asynchronous methods. The paper makes several contributions, including:  
1. Demonstrating how gradient staleness in asynchronous training degrades test accuracy, particularly in deep models.  
2. Empirically analyzing straggler effects in synchronous optimization using large-scale GPU deployments.  
3. Proposing and implementing a backup worker mechanism to alleviate straggler effects without compromising gradient quality.  
4. Validating the proposed method through experiments on Inception and PixelCNN models, showing faster convergence and better test accuracies compared to asynchronous methods.  
Decision: Accept  
The paper addresses an important problem in distributed deep learning and provides a well-motivated, empirically validated solution. The proposed method demonstrates clear improvements over existing approaches, both in terms of convergence speed and final test accuracy, making it a valuable contribution to the field.
Supporting Arguments
1. Well-Motivated Problem: The paper identifies and addresses key limitations of both synchronous and asynchronous optimization methods. The analysis of gradient staleness and straggler effects is thorough and supported by empirical evidence.  
2. Novelty: The use of backup workers in synchronous optimization is a novel and practical idea. The authors provide a clear explanation of how this approach balances the trade-off between iteration time and gradient quality.  
3. Empirical Rigor: The experiments are comprehensive, spanning multiple models (Inception, PixelCNN) and datasets (ImageNet, CIFAR-10). The results consistently demonstrate the superiority of the proposed method over asynchronous optimization in terms of both convergence speed and test accuracy.  
4. Practical Relevance: The proposed method is highly relevant for large-scale distributed training, where the straggler problem and gradient staleness are significant bottlenecks. The use of TensorFlow for implementation further underscores its practicality.  
Suggestions for Improvement
1. Theoretical Analysis: While the empirical results are compelling, a more detailed theoretical analysis of the trade-offs between backup workers and gradient quality would strengthen the paper. For instance, a formal exploration of the relationship between the number of backup workers and convergence rates could provide deeper insights.  
2. Broader Evaluation: The experiments focus on specific models and datasets. Evaluating the method on additional tasks, such as natural language processing or reinforcement learning, would demonstrate its generalizability.  
3. Communication Overhead: The paper briefly mentions communication overhead but does not provide a detailed analysis. Quantifying the impact of backup workers on communication costs would be valuable, especially for large-scale deployments.  
4. Comparison with SoftSync: The related work section mentions SoftSync as a similar approach but does not provide a direct empirical comparison. Including such a comparison would clarify the advantages of the proposed method.  
Questions for the Authors
1. How does the choice of the number of backup workers (b) scale with the number of total workers (N)? Are there diminishing returns beyond a certain value of b?  
2. Did you observe any scenarios where the proposed method underperformed compared to asynchronous optimization? For example, in cases with highly imbalanced workloads or extreme hardware variability?  
3. Could the backup worker approach be combined with gradient clipping or other stabilization techniques to further improve performance?  
Overall, this paper presents a significant and well-executed contribution to distributed deep learning optimization. While there is room for further exploration, the proposed method is both novel and impactful, warranting acceptance at the conference.