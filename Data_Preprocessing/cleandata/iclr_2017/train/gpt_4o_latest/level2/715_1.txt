Review of the Paper
The paper proposes a novel approach to reduce the computational complexity of deep convolutional neural networks (CNNs) through feature map and kernel pruning. The authors introduce a generic and efficient strategy for selecting pruning masks, which involves generating random masks and selecting the least adversarial one based on validation set performance. The proposed method is evaluated on multiple datasets, including CIFAR-10, CIFAR-100, SVHN, and MNIST, demonstrating that significant sparsity (60-70%) can be achieved with minimal degradation in classification accuracy. The approach is computationally efficient due to its one-shot pruning and retraining strategy, making it suitable for real-time inference and resource-constrained environments.
Decision: Accept
Key reasons for this decision are:  
1. Novelty and Practicality: The paper introduces a simple yet effective pruning strategy that balances coarse and fine-grained pruning granularities. The method is generic, scalable, and computationally efficient, addressing a critical challenge in deploying deep learning models on resource-limited devices.  
2. Experimental Rigor: The authors provide extensive experimental results across multiple datasets and architectures, demonstrating the robustness and scalability of their approach. The results convincingly show that the proposed method outperforms existing techniques in terms of pruning ratios and computational efficiency.
Supporting Arguments:  
- Claims and Support: The paper claims that its pruning strategy achieves high sparsity with minimal accuracy loss and is computationally efficient. These claims are well-supported by empirical results, including comparisons with existing methods (e.g., weight sum criterion). The experiments are statistically significant and demonstrate consistent trends across datasets.  
- Usefulness: The proposed method is highly relevant for real-world applications, particularly in scenarios requiring real-time inference or deployment on hardware-constrained devices. The focus on coarse-grained pruning granularities ensures practical implementation benefits, such as compatibility with GPUs and VLSI-based systems.  
- Field Knowledge and Novelty: The paper builds on existing pruning literature but introduces a novel mask selection strategy and demonstrates the advantages of combining feature map and kernel pruning. The related work section is thorough, and the authors clearly position their contributions within the broader context.  
Suggestions for Improvement:  
1. Reproducibility: While the paper provides detailed experimental results, it would benefit from including more implementation details, such as hyperparameter settings and code availability, to enhance reproducibility.  
2. Theoretical Insights: The paper could strengthen its theoretical foundation by providing a deeper analysis of why the proposed mask selection strategy outperforms alternatives, beyond empirical observations.  
3. Broader Evaluation: While the method is evaluated on standard benchmarks, testing on larger-scale datasets like ImageNet would further validate its scalability and generalizability.  
Questions for the Authors:  
1. How does the method scale with extremely deep networks, such as ResNet-50 or Transformers, where pruning granularities might interact differently?  
2. Can the proposed approach be extended to other types of neural networks, such as recurrent or graph neural networks?  
3. How does the computational overhead of generating and evaluating random pruning masks compare to iterative pruning methods in practice?  
Overall, the paper presents a significant contribution to the field of network pruning and offers a practical solution to a pressing problem in deep learning deployment. With minor improvements, it has the potential to make an even greater impact.