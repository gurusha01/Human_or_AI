Review
The paper proposes a novel multi-view Bayesian non-parametric algorithm for learning multi-sense word embeddings using multilingual corpora. The key contributions are twofold: (1) leveraging multilingual distributional information to improve sense disambiguation beyond bilingual approaches, and (2) employing a principled Bayesian non-parametric framework to infer a variable number of senses per word in a data-driven manner. The authors demonstrate that their approach achieves competitive performance on word sense induction (WSI) tasks with significantly less training data compared to state-of-the-art monolingual models. Additionally, they provide insights into the effects of language family distance and crosslingual window size on performance.
Decision: Accept
The paper introduces a significant improvement in multi-sense word embedding learning by extending the scope from bilingual to multilingual corpora and addressing key limitations of existing approaches. The novelty, practical utility, and rigorous evaluation make this work a valuable contribution to the field.
Supporting Arguments:
1. Novelty and Motivation: The paper addresses a critical gap in the literature by extending multi-sense embedding learning to multilingual corpora. Unlike prior bilingual approaches, the proposed algorithm efficiently combines multiple parallel corpora, demonstrating the potential of multilingual signals for better sense disambiguation. The use of a Bayesian non-parametric framework to infer variable senses per word is another innovative aspect.
2. Experimental Rigor: The experiments are thorough and well-designed, covering multiple datasets (Semeval-2007, Semeval-2010, and WWSI) and metrics (Adjusted Rand Index and contextual word similarity). The results convincingly show that the proposed model outperforms monolingual and bilingual baselines, even when trained on smaller datasets.
3. Practical Utility: The approach is practically useful for NLP tasks requiring sense disambiguation, especially in resource-constrained settings. The ability to achieve competitive performance with less data and parameters is a significant advantage.
Additional Feedback:
1. Clarity of Presentation: While the technical details are comprehensive, the paper could benefit from a clearer explanation of the Bayesian non-parametric framework for readers unfamiliar with the concept. Simplifying the mathematical notation and providing more intuitive descriptions of key equations (e.g., Eq. 1 and Eq. 3) would improve accessibility.
2. Comparison with Related Work: The paper provides a strong comparison with prior work, but it would be helpful to include a more detailed discussion of how the proposed method compares to retrofitting approaches (e.g., Faruqui et al., 2015) in terms of computational efficiency and scalability.
3. Limitations and Future Work: While the authors briefly acknowledge limitations (e.g., not modeling polysemy in foreign languages), a more explicit discussion of the challenges and potential extensions (e.g., multilingual WordNet generation) would strengthen the paper.
4. Parameter Sensitivity: The paper mentions tuning parameters like the crosslingual window size and hyperparameter Î± but does not provide sufficient detail on how sensitive the results are to these choices. A more detailed analysis would be valuable.
Questions for Authors:
1. How does the model handle cases where the multilingual corpora are imbalanced in size or quality? Does this affect the embeddings learned for specific languages?
2. Could the proposed approach be extended to jointly model polysemy in foreign languages, and what challenges might arise in doing so?
3. How does the computational complexity of the proposed method compare to monolingual and bilingual baselines, especially for large-scale multilingual corpora?
Overall, the paper makes a significant contribution to the field of multi-sense word embeddings and is well-suited for acceptance at the conference. With minor improvements to clarity and discussion of limitations, it could have an even greater impact.