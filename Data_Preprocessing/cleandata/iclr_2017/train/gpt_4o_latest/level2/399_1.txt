The paper introduces a novel approach to conditional computation in neural networks through the development of a Sparsely-Gated Mixture-of-Experts (MoE) layer. The authors claim to achieve over 1000x improvements in model capacity with minimal computational overhead, demonstrating significant advancements in language modeling and machine translation tasks. By leveraging sparsity in gating mechanisms, the MoE layer activates only a subset of experts per input example, enabling scalability to models with up to 137 billion parameters. The paper addresses key challenges in conditional computation, such as load balancing, shrinking batch sizes, and network bandwidth, and provides empirical evidence of state-of-the-art performance on large-scale benchmarks.
Decision: Accept.  
Key reasons for this decision are the paper's significant contributions to scaling neural networks efficiently and its rigorous experimental validation. The proposed MoE layer represents a meaningful innovation in model design, and the results convincingly demonstrate its practical utility and superiority over existing methods.
Supporting Arguments:  
1. Novelty and Contributions: The paper introduces a scalable and efficient implementation of conditional computation, addressing long-standing challenges in the field. The hierarchical MoE design and the use of sparse gating are innovative and well-motivated.  
2. Empirical Validation: The authors provide extensive experimental results on large-scale datasets, including a 1-billion-word language modeling benchmark and machine translation tasks. The models consistently outperform state-of-the-art baselines in terms of perplexity and BLEU scores while maintaining computational efficiency.  
3. Practical Usefulness: The proposed approach is highly relevant for tasks requiring large model capacities, such as language modeling and translation, and demonstrates scalability to massive datasets and parameter counts.  
Additional Feedback:  
1. Reproducibility: While the paper provides detailed descriptions of the MoE architecture and training strategies, some implementation details, such as hyperparameter tuning and specific hardware configurations, could be clarified further for reproducibility.  
2. Limitations: The paper briefly mentions diminishing returns in perplexity improvements for extremely large models (e.g., 131072 experts). A more in-depth discussion of this phenomenon and potential remedies would strengthen the work.  
3. Generalization: While the focus on text tasks is justified, exploring the applicability of the MoE layer to other domains, such as vision or audio, would broaden the impact of the work.  
Questions for the Authors:  
1. How does the proposed MoE layer handle scenarios with imbalanced data distributions or rare input patterns?  
2. Can the gating network's sparsity mechanism introduce biases in expert utilization over time, and how is this mitigated?  
3. What are the primary bottlenecks preventing scaling beyond the reported 137 billion parameters, and how might these be addressed in future work?  
Overall, this paper makes a strong case for acceptance due to its innovative contributions, rigorous experimentation, and practical relevance to the field of deep learning. The proposed MoE layer has the potential to inspire further research and applications in scalable neural network design.