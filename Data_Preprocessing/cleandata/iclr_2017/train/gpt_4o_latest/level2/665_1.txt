The paper introduces MS MARCO, a large-scale, real-world dataset for machine reading comprehension (RC) and question answering (QA). The authors claim that MS MARCO addresses key limitations of existing datasets by using real anonymized user queries, extracting context passages from real web documents, and providing human-generated answers. The dataset includes 100,000 queries in its initial release, with plans to expand to one million. The authors argue that MS MARCO is unique in its realism, diversity, and scale, making it a valuable resource for advancing RC and QA research.
Decision: Accept
The paper should be accepted for its significant contribution to the field of RC and QA. The creation of a large-scale dataset based on real-world queries is a meaningful step forward, addressing the artificiality and limitations of existing datasets. The dataset's potential to inspire new research and improve model performance on real-world tasks justifies its acceptance.
Supporting Arguments:
1. Novelty and Contribution: MS MARCO is the first large-scale dataset to use real user queries and web passages, distinguishing it from synthetic or crowdworker-generated datasets like SQuAD. Its inclusion of human-generated answers and multiple-answer queries further enhances its realism and utility.
2. Practical Usefulness: The dataset has clear applications in developing and benchmarking RC and QA systems, particularly for real-world scenarios like personal assistants and customer service bots. Its scale and diversity make it a valuable resource for training data-intensive models.
3. Scientific Rigor: The authors provide detailed descriptions of the dataset creation process, including filtering, human annotation, and quality control. They also present experimental results demonstrating the dataset's utility in evaluating generative and cloze-style models.
Additional Feedback:
1. Limitations and Future Work: While the paper acknowledges limitations, such as the current lack of multiple answers for all queries, it would benefit from a more detailed discussion of how these limitations might impact research. For example, how does the dataset handle ambiguous or incomplete queries?
2. Evaluation Metrics: The paper introduces metrics like ROUGE-L and phrasing-aware BLEU but could elaborate on how these metrics compare to standard evaluation methods in terms of reliability and interpretability.
3. Dataset Accessibility: The authors should clarify the dataset's licensing and accessibility to ensure it can be widely adopted by the research community.
4. Scalability: While the authors plan to expand the dataset to one million queries, it would be helpful to outline a timeline and specific steps for achieving this goal.
Questions for the Authors:
1. How do you ensure the dataset remains representative of real-world queries as it scales to one million entries? Are there plans to include queries from diverse languages or regions?
2. Can you provide more details on the feedback loop and auditing processes used to ensure annotation quality? How do you handle disagreements among human annotators?
3. Have you considered including metadata, such as query timestamps or user demographics, to enable richer analyses?
In conclusion, MS MARCO is a well-motivated and rigorously developed dataset that addresses critical gaps in RC and QA research. While there are areas for improvement, the paper's contributions are substantial, and its acceptance would benefit the AI community.