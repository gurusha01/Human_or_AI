Review of the Paper
Summary of Contributions
This paper introduces StarCraft micromanagement scenarios as benchmarks for reinforcement learning (RL) algorithms, focusing on the short-term, low-level control of units during battles. The authors propose several challenging scenarios characterized by large state-action spaces and the absence of obvious feature representations for value functions. The paper presents a novel heuristic RL algorithm, referred to as zero-order (ZO) optimization, which combines direct exploration in policy space with backpropagation. The authors demonstrate that ZO outperforms traditional RL algorithms such as Q-learning and REINFORCE in these scenarios, achieving superior performance in learning non-trivial strategies for armies of up to 15 agents. The paper also highlights the challenges of exploration and parameterization in multi-agent RL tasks and proposes a neural network architecture for joint state-action encoding. Experimental results validate the efficacy of the proposed approach, with ZO demonstrating robust training and better generalization across different scenarios.
Decision: Accept
The paper makes a compelling case for the use of StarCraft micromanagement as a benchmark for RL and introduces a novel algorithm that addresses key challenges in multi-agent RL. The decision to accept is based on two primary reasons: (1) the introduction of a well-motivated and practically useful benchmark that can drive progress in RL research, and (2) the development of a novel RL algorithm that demonstrates significant improvements over existing methods in terms of performance and robustness.
Supporting Arguments
1. Novelty and Significance: The paper introduces a new RL algorithm (ZO) that explores directly in policy space, a departure from traditional action-space exploration methods. This approach is particularly well-suited for the complex, high-dimensional action spaces of StarCraft micromanagement tasks. The novelty of the algorithm and its demonstrated superiority over Q-learning and REINFORCE make it a valuable contribution to the field.
2. Experimental Rigor: The authors conduct extensive experiments across multiple scenarios, comparing ZO with baseline heuristics and traditional RL algorithms. The results are robust and demonstrate clear advantages of ZO in terms of win rates, generalization, and training stability. The inclusion of ablation studies and detailed hyperparameter tuning further strengthens the experimental rigor.
3. Practical Utility: The proposed benchmarks and algorithm have practical implications for advancing RL research in multi-agent and real-time strategy environments. The scenarios are well-designed to test the scalability and adaptability of RL algorithms, making them a valuable resource for the community.
Suggestions for Improvement
1. Clarity of Algorithm Description: While the ZO algorithm is well-motivated, its presentation could be streamlined for clarity. For instance, the heuristic rule for updating the embedding network parameters (line ) could benefit from a more formal derivation or justification.
2. Comparison with State-of-the-Art: The paper primarily compares ZO with traditional RL algorithms (Q-learning, REINFORCE). Including comparisons with more recent advancements in multi-agent RL, such as actor-critic methods or hierarchical RL, would strengthen the evaluation.
3. Generalization Across Domains: While the paper focuses on StarCraft, it would be beneficial to test ZO on other domains, such as Atari or robotics, to demonstrate its broader applicability.
4. Acknowledgment of Limitations: The paper could more explicitly discuss the limitations of ZO, such as its reliance on specific parameterizations or its scalability to larger armies and more complex scenarios.
Questions for the Authors
1. How does ZO perform in scenarios with more diverse unit types and actions beyond "move" and "attack"? Could the algorithm handle more complex tasks involving hierarchical planning?
2. Could you elaborate on the choice of Adagrad over other optimizers like RMSProp for ZO? Did you observe any specific advantages in terms of convergence or stability?
3. How sensitive is ZO to the choice of the exploration hyperparameter (δ)? Would adaptive tuning of δ improve performance further?
Conclusion
This paper makes a strong contribution to the field of reinforcement learning by introducing a novel algorithm and a challenging benchmark. While there is room for improvement in terms of clarity, broader comparisons, and acknowledgment of limitations, the paper's strengths outweigh its weaknesses. I recommend acceptance, as the contributions are both novel and impactful, with the potential to inspire further research in multi-agent RL and real-time strategy games.