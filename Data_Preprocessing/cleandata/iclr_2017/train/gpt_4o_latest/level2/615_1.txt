The paper introduces L-SR1, a novel second-order optimization method for training deep neural networks, addressing key challenges such as saddle points and Hessian conditioning. The authors propose a limited-memory symmetric rank-one (SR1) update combined with a trust-region framework and batch normalization, aiming to make second-order methods more practical for large-scale non-convex optimization. Experimental results demonstrate that L-SR1 performs competitively with first-order methods like Nesterov's Accelerated Gradient (NAG) and outperforms L-BFGS, a widely used second-order method, on MNIST and CIFAR10 datasets. Furthermore, the paper highlights L-SR1's potential for distributed training due to its robustness to larger mini-batch sizes.
Decision: Accept.  
Key reasons: (1) The paper presents a novel and well-motivated approach that addresses significant limitations of second-order methods in deep learning. (2) The experimental results are thorough and demonstrate the practical utility of L-SR1, particularly in distributed training scenarios.
Supporting Arguments:  
1. Novelty and Contribution: The paper introduces a new second-order method, L-SR1, which leverages SR1 updates and a trust-region approach to overcome saddle points and ill-conditioned Hessians. This is a meaningful advancement over existing methods like L-BFGS. The integration of batch normalization to improve Hessian conditioning is also a novel and practical insight.  
2. Experimental Validation: The experiments are comprehensive, covering both shallow (LeNet5) and deep (residual networks) architectures on standard datasets. The results convincingly show that L-SR1 is competitive with first-order methods and superior to L-BFGS. The analysis of hyperparameters and mini-batch size further strengthens the case for L-SR1's robustness and scalability.  
3. Practical Implications: The method's insensitivity to larger mini-batch sizes and minimal hyperparameter tuning requirements make it a promising candidate for distributed training, a critical area in deep learning.  
Additional Feedback for Improvement:  
1. Comparison with Recent Methods: While the paper compares L-SR1 with L-BFGS and first-order methods, it would benefit from a direct comparison with other recent stochastic quasi-Newton methods, such as those by Keskar & Berahas (2015) or Curtis (2016). This would provide a more comprehensive evaluation of L-SR1's relative performance.  
2. Skipped Updates: The paper notes a high proportion of skipped updates in certain scenarios (e.g., MNIST with batch normalization) but does not delve deeply into the implications. A more detailed analysis or hypothesis on why this occurs and its potential impact on performance would be valuable.  
3. Residual Network Results: While L-SR1 achieves competitive final test loss, its slower convergence and higher test loss variability compared to SGD with momentum warrant further investigation. Suggestions for mitigating these issues, such as adaptive trust-region adjustments, could strengthen the method's applicability to deeper networks.  
4. Clarity of Algorithm Description: The pseudocode for L-SR1 is detailed but could be simplified for readability. Highlighting key steps and their significance would help readers unfamiliar with trust-region methods or SR1 updates.
Questions for Authors:  
1. How does L-SR1 compare with other recent stochastic second-order methods (e.g., Hessian-free approaches or stochastic BFGS variants) in terms of convergence speed and scalability?  
2. What specific insights can be drawn from the high proportion of skipped updates in certain cases? Could this be a limitation of the SR1 update or an artifact of batch normalization?  
3. Have you tested L-SR1 on larger datasets (e.g., ImageNet) or more complex architectures? If not, what challenges do you foresee in scaling the method to such settings?  
4. Could the slower convergence of L-SR1 on residual networks be mitigated by tuning the trust-region parameters or using adaptive strategies?  
Overall, this paper makes a valuable contribution to the field of optimization for deep learning, and its insights into the practical application of second-order methods are both timely and impactful. With minor clarifications and additional comparisons, it could be further strengthened.