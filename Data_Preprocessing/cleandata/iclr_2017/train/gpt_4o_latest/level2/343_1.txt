The paper introduces a novel approach to character-level language modeling using a domain-specific language (DSL) called TChar. The authors propose a two-phase learning process: first, synthesizing a program from the DSL to represent the data, and second, estimating probabilities through counting, akin to n-gram models. The key contributions include defining the TChar DSL, which allows for human-readable, interpretable models, and demonstrating its competitive performance against state-of-the-art neural networks on structured datasets like the Linux Kernel and Hutter Prize Wikipedia datasets. The model combines the precision of neural networks with the efficiency and flexibility of n-gram models, offering advantages such as fast query times, modularity, and the ability to incorporate domain-specific knowledge.
Decision: Accept  
Key reasons:  
1. Novelty and Practicality: The paper introduces an innovative approach to language modeling by synthesizing interpretable programs from a DSL, which is a significant departure from traditional neural network methods. The ability to manually inspect and modify the model enhances its practical utility.  
2. Strong Experimental Results: The proposed model outperforms n-gram and LSTM models in structured datasets, achieving lower bits-per-character (BPC) and error rates, while maintaining competitive performance on less structured datasets.  
Supporting Arguments:  
- Claims and Support: The authors convincingly demonstrate the applicability of their DSL-based approach through extensive experiments. The results are statistically significant, showing clear advantages in structured data modeling. The inclusion of metrics like BPC and error rate strengthens the evaluation.  
- Field Knowledge and Completeness: The paper reflects a solid understanding of the field, citing relevant literature on program synthesis and neural networks. The methodology is detailed, and the experiments are reproducible, with clear descriptions of the datasets, evaluation metrics, and training procedures.  
- Usefulness: The model's interpretability, modularity, and efficiency make it a valuable tool for practitioners working with structured data, such as source code.  
Additional Feedback:  
1. Limitations: While the paper acknowledges that the DSL's restricted expressiveness may limit its applicability to unstructured datasets, this aspect could be further explored. For instance, how might the DSL be extended to better handle natural language text?  
2. Training Time: The DSL model's training time (~8 hours) is significantly longer than n-gram models, which may limit its scalability. Suggestions for optimizing the synthesis process would be beneficial.  
3. Comparison with Advanced Neural Models: The paper compares its model to LSTMs but does not include newer transformer-based models, which are now dominant in language modeling. Adding such comparisons would strengthen the claims.  
4. Interactive Visualization: The interactive visualization of the synthesized program is a valuable resource. However, the paper could provide more details on how this tool aids in debugging and interpretability.  
Questions for the Authors:  
1. How does the DSL model perform when integrated with modern neural architectures, such as transformers? Could it serve as a complementary component?  
2. What are the trade-offs between increasing the expressiveness of the DSL and maintaining efficient synthesis?  
3. Could the synthesis process be parallelized or optimized further to reduce training time?  
Overall, the paper presents a compelling and innovative approach to language modeling with strong experimental validation and practical implications. It is a valuable contribution to the intersection of program synthesis and machine learning.