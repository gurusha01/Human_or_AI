Review of "Rotation Plane Doubly Orthogonal Recurrent Neural Networks"
Summary of Contributions:
This paper introduces the Rotation Plane Doubly Orthogonal Recurrent Neural Network (RP-DORNN), a novel RNN architecture designed to address the vanishing and exploding gradient problems in training recurrent neural networks on long sequences. The authors propose an architecture where the hidden state is updated multiplicatively using a time-invariant orthogonal transformation followed by an input-modulated orthogonal transformation. By preserving both forward hidden state activation norms and backward gradient norms, the model is theoretically immune to gradient vanishing or explosion. The authors parameterize orthogonal matrices using a rotation plane representation, which allows for efficient computation and expressivity. The approach is validated on a simplified memory copy task, demonstrating the ability to learn dependencies up to 5,000 timesteps, outperforming prior methods.
Decision: Reject
While the paper presents an interesting and theoretically sound approach, it lacks sufficient experimental validation on diverse and practical tasks to justify its claims. The limited scope of the experiments, combined with the absence of comparisons to state-of-the-art models on real-world benchmarks, makes it difficult to assess the practical utility and generalizability of the proposed method.
Supporting Arguments for Decision:
1. Novelty and Theoretical Contributions:  
   The proposed RP-DORNN architecture is novel and well-motivated. The use of rotation plane parameterization for orthogonal matrices is an innovative approach that enables efficient computation while preserving gradient norms. The theoretical guarantees of norm preservation are rigorously derived and are a significant contribution to the field.
2. Experimental Validation:  
   The primary weakness of the paper lies in its experimental evaluation. The memory copy task, while a standard benchmark for testing long-term dependencies, is highly simplified and does not reflect the complexity of real-world tasks. The authors acknowledge this limitation but fail to provide additional experiments on more challenging datasets or tasks (e.g., language modeling, speech recognition, or time-series forecasting). Furthermore, the lack of comparisons with state-of-the-art models like LSTMs, GRUs, and uRNNs on these tasks makes it difficult to assess the practical impact of the proposed architecture.
3. Clarity and Completeness:  
   The paper is well-written and provides sufficient mathematical detail to understand the architecture and its theoretical properties. However, the discussion section highlights several open questions and limitations (e.g., the linearity of the transition, sensitivity to random initialization of rotation planes) that remain unresolved. These issues suggest that the work is still in an exploratory stage.
4. Usefulness and Practicality:  
   While the architecture shows promise for learning extremely long-term dependencies, its practical utility is unclear. Most real-world tasks involve a mix of short- and long-term dependencies, and the paper does not address how RP-DORNN would handle such scenarios. Additionally, the scalability of the approach to larger datasets and more complex tasks remains untested.
Suggestions for Improvement:
1. Expand Experimental Validation:  
   Evaluate the RP-DORNN on a broader range of tasks, including real-world benchmarks like Penn Treebank, WikiText, or sequential MNIST. Include comparisons with state-of-the-art models to demonstrate the advantages of the proposed architecture.
2. Address Practical Limitations:  
   Explore methods to combine RP-DORNN with nonlinear transitions or hybrid architectures (e.g., integrating LSTM-like mechanisms) to handle both short- and long-term dependencies. Investigate ways to optimize the rotation planes rather than relying on random initialization.
3. Analyze Training Stability:  
   The paper mentions instability during training for longer sequences but does not provide a detailed analysis or solutions. Addressing this issue would strengthen the practical applicability of the model.
4. Provide Computational Analysis:  
   Include a comparison of computational efficiency (e.g., training time, memory usage) between RP-DORNN and other orthogonal/unitary RNNs to highlight the benefits of the rotation plane parameterization.
Questions for the Authors:
1. How does RP-DORNN perform on tasks that involve both short- and long-term dependencies? Can it effectively balance these requirements?
2. Have you considered applying RP-DORNN to real-world sequence modeling tasks, such as language modeling or speech recognition? If so, what were the results?
3. How sensitive is the model to the random initialization of rotation planes? Would optimizing these planes improve performance?
4. What are the computational trade-offs of using the rotation plane parameterization compared to other orthogonal/unitary representations?
In summary, while the paper introduces a promising architecture with strong theoretical foundations, its limited experimental validation and unresolved practical challenges suggest that it is not yet ready for acceptance. I encourage the authors to address these issues and resubmit after further development.