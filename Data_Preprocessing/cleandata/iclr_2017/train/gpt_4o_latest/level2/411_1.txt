Review
This paper presents a novel learning-based approach to code super-optimization, addressing limitations in existing stochastic search methods by leveraging reinforcement learning to learn the proposal distribution. The authors argue that current state-of-the-art methods, such as Stoke, fail to exploit the semantics of the program or learn from past behavior. By introducing a reinforcement learning framework that optimizes the proposal distribution using unbiased gradient estimators, the proposed method demonstrates significant improvements over Stoke on two datasets: the Hacker's Delight benchmark and a set of automatically generated programs. The results show that the learned proposal distribution achieves faster and higher-quality optimizations compared to uniform sampling.
Decision: Accept
The paper should be accepted for its significant contribution to the field of code optimization. The primary reasons for this decision are: (1) the novelty of introducing a reinforcement learning framework to learn proposal distributions for stochastic search, and (2) the strong empirical results demonstrating superior performance over state-of-the-art methods on diverse datasets.
Supporting Arguments:
1. Novelty and Contribution: The paper introduces a novel approach to code super-optimization by framing the problem as a learning task. Unlike Stoke, which uses a fixed, uniform proposal distribution, the proposed method adapts the proposal distribution based on program features, leading to more efficient search. This represents a meaningful advancement in the field.
2. Empirical Validation: The experiments are well-designed and demonstrate clear improvements over Stoke. On the Hacker's Delight benchmark, the proposed method achieves a 60% improvement in efficiency compared to Stoke's 40%. On the more challenging dataset of automatically generated programs, the learned model outperforms the uniform baseline by a significant margin, showcasing its generalizability.
3. Scientific Rigor: The paper provides a detailed explanation of the methodology, including the use of the REINFORCE algorithm for gradient estimation and the parameterization of the proposal distribution. The experimental setup is robust, with clear comparisons to baselines and ablation studies.
Additional Feedback for Improvement:
1. Clarity of Presentation: While the technical details are thorough, the paper could benefit from a more concise and structured explanation of the reinforcement learning framework. For instance, the derivation of the REINFORCE gradient estimator could be summarized more succinctly.
2. Dataset Limitations: The Hacker's Delight benchmark, while a standard dataset, is relatively small and may not fully capture the diversity of real-world programs. The authors acknowledge this limitation and suggest collecting a larger dataset of frequently used programs. This should be prioritized in future work.
3. Interpretability of Results: The paper could include more qualitative examples of optimized programs to illustrate the practical impact of the proposed method. For instance, showing side-by-side comparisons of input, intermediate, and optimized programs would enhance the reader's understanding.
4. Broader Applicability: While the method is promising, its applicability to larger and more complex programs remains unclear. Future work could explore scaling the approach to handle longer programs or more diverse instruction sets.
Questions for the Authors:
1. How does the proposed method scale with the length and complexity of input programs? Are there any computational bottlenecks that might limit its applicability to larger programs?
2. Could the learned proposal distribution be integrated into existing compilers to improve real-world code optimization workflows? If so, what challenges might arise in such integration?
3. The paper mentions that the method operates directly on actual programs. How does this compare to approaches that use differentiable representations of programs in terms of scalability and generalization?
Conclusion:
This paper makes a strong contribution to the field of code optimization by introducing a reinforcement learning-based approach to improve stochastic super-optimization. The results are compelling, and the methodology is scientifically rigorous. While there are areas for improvement, particularly in terms of scalability and dataset diversity, the paper's contributions are significant enough to warrant acceptance.