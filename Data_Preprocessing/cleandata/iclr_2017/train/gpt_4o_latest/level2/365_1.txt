Review of the Paper
Summary of Contributions:
This paper introduces a novel "density-diversity penalty" regularizer for fully-connected layers in deep neural networks to achieve high sparsity and low diversity in weight matrices. The authors propose an efficient optimization technique, the "sorting trick," to make the regularizer computationally feasible. The regularizer simultaneously enforces sparsity and reduces the diversity of weight values during training, enabling significant compression of neural networks without compromising performance. The method is evaluated on the MNIST and TIMIT datasets, demonstrating compression rates of up to 200X for fully-connected layers while maintaining comparable accuracy. The proposed approach is positioned as an improvement over existing methods like "deep compression," offering a unified framework for sparsity and diversity reduction.
Decision: Accept
Key Reasons:
1. The paper presents a novel, well-motivated regularization technique that addresses a critical challenge in deploying deep neural networks on resource-constrained devices.
2. The proposed method achieves competitive or superior compression rates compared to state-of-the-art methods while maintaining model performance.
3. The work is supported by rigorous experiments and provides a clear theoretical foundation for the proposed regularizer.
Supporting Arguments:
1. Novelty and Contribution: The density-diversity penalty is a unique contribution that combines sparsity and diversity reduction into a single regularizer. Unlike prior methods that decouple these objectives, this approach offers a streamlined solution with fewer hyperparameters to tune. The "sorting trick" is a clever optimization that significantly reduces computational overhead, making the method practical for large-scale networks.
   
2. Experimental Validation: The paper provides comprehensive experiments on MNIST and TIMIT datasets, demonstrating the effectiveness of the regularizer in achieving high compression rates without performance degradation. The results are benchmarked against the "deep compression" method, showing competitive or superior performance, especially for fully-connected layers.
3. Practical Relevance: The proposed method directly addresses the challenge of deploying deep learning models on lightweight devices, such as IoT platforms, where memory and computational resources are limited. The ability to achieve compression rates of up to 200X while preserving accuracy makes this work highly relevant to the community.
4. Clarity and Completeness: The paper is well-written, with clear explanations of the methodology, optimization techniques, and experimental results. The inclusion of detailed mathematical formulations and pseudocode enhances reproducibility.
Suggestions for Improvement:
1. Broader Evaluation: While the results on MNIST and TIMIT are promising, it would strengthen the paper to include experiments on larger, more complex datasets (e.g., ImageNet) or tasks (e.g., natural language processing) to demonstrate scalability and generalizability.
2. Comparison with Other Regularizers: The paper could benefit from a more detailed comparison with other sparsity-inducing regularizers (e.g., L1 regularization) to highlight the unique advantages of the density-diversity penalty.
3. Ablation Studies: An ablation study to isolate the contributions of the sparse initialization, density-diversity penalty, and weight tying would clarify the relative importance of each component.
4. Limitations: The paper does not explicitly discuss potential limitations, such as the computational cost of the sorting trick for very large weight matrices or the impact of hyperparameter tuning on performance.
Questions for the Authors:
1. How does the density-diversity penalty perform on convolutional layers or recurrent architectures? Are there specific challenges in extending the method to these settings?
2. Have you explored the impact of different values of the p-norm in the regularizer? How sensitive is the method to this choice?
3. Could the sorting trick be parallelized further to reduce computational cost for extremely large networks?
Conclusion:
This paper makes a significant contribution to the field of model compression by introducing a novel regularizer that achieves high compression rates while maintaining performance. The method is well-motivated, rigorously evaluated, and practically relevant. With additional experiments on larger datasets and a discussion of limitations, this work could have an even broader impact. I recommend acceptance.