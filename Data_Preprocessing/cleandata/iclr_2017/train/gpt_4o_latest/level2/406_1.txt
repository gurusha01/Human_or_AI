Review of the Paper
The paper introduces the Ensemble Policy Optimization (EPOpt) algorithm, a novel approach to training robust reinforcement learning (RL) policies for real-world tasks. The key contributions include (1) leveraging an ensemble of simulated source domains and adversarial training to improve policy robustness against model discrepancies, and (2) adapting the source domain distribution using approximate Bayesian methods to better approximate the target domain. The authors demonstrate the effectiveness of EPOpt on challenging robotic control tasks (hopper and half-cheetah) in the MuJoCo simulator, showcasing improved robustness and generalization compared to standard policy optimization methods.
Decision: Accept.  
Key Reasons:  
1. The paper addresses a critical challenge in RL—robustness to model discrepancies—by proposing a well-motivated and innovative algorithm that combines adversarial training and Bayesian adaptation.  
2. The experimental results are thorough and convincingly demonstrate the advantages of EPOpt over baseline methods in terms of robustness and transferability.
Supporting Arguments:  
1. Motivation and Placement in Literature: The paper is well-positioned in the context of prior work on robust control, Bayesian RL, and model-based RL. It highlights the limitations of existing approaches, such as their inability to handle high-dimensional continuous control tasks or unmodeled effects, and proposes EPOpt as a solution.  
2. Support for Claims: The claims are supported by rigorous experiments on two benchmark tasks. The results show that EPOpt-trained policies outperform standard TRPO in robustness to parameter discrepancies and generalization to unmodeled effects. The inclusion of ablation studies (e.g., varying the adversarial training parameter, analyzing the importance of baselines) further strengthens the claims.  
3. Novelty: The combination of adversarial training on model ensembles and Bayesian adaptation is novel and represents a significant improvement over existing methods. The paper also provides a detailed theoretical formulation of the algorithm.  
4. Practical Usefulness: The proposed method is highly relevant for real-world applications where safety and robustness are critical, such as robotics. The ability to adapt the source domain distribution with limited target domain data is particularly valuable.  
Additional Feedback:  
1. Clarity: While the paper is generally well-written, the explanation of the Bayesian adaptation step (Section 3.2) could benefit from additional clarity, especially for readers unfamiliar with importance sampling. A visual illustration of the adaptation process might help.  
2. Computational Efficiency: The paper mentions that sampling-based Bayesian adaptation can be computationally intensive as the number of parameters increases. It would be helpful to include a discussion on potential strategies to mitigate this limitation, such as using non-parametric methods or dimensionality reduction techniques.  
3. Real-World Validation: While the experiments in MuJoCo are compelling, a discussion on the challenges of transferring EPOpt to real-world robotic systems (e.g., sensor noise, hardware constraints) would enhance the paper's practical impact.  
Questions for Authors:  
1. How sensitive is the performance of EPOpt to the choice of the initial source domain distribution? Would a poor initial distribution significantly degrade performance, and how can this be mitigated?  
2. Can EPOpt handle dynamic target domains where the parameters change over time? If so, how does the algorithm adapt to such scenarios?  
3. Have you considered extending EPOpt to tasks with high-dimensional observation spaces (e.g., vision-based RL)? If not, what are the main challenges?  
Conclusion:  
The paper makes a significant contribution to the field of robust RL by addressing key challenges in transferring policies from simulation to the real world. The proposed EPOpt algorithm is well-motivated, novel, and supported by strong experimental evidence. With minor improvements in clarity and additional discussion on real-world applicability, the paper has the potential to make a lasting impact. I recommend acceptance.