Review of the Paper: "Nonparametric Learning of Activation Functions in Deep Neural Networks"
This paper introduces a novel framework for nonparametrically learning activation functions in deep neural networks, addressing a relatively unexplored area in deep learning research. The authors propose a method to estimate activation functions as part of the training process, using a Fourier basis expansion. They provide theoretical justifications for their approach, including provable generalization bounds, and demonstrate its effectiveness through experiments on benchmark datasets such as MNIST and CIFAR-10. The paper reports up to a 15% relative improvement in test performance compared to baseline methods, particularly when using a two-stage training process.
Decision: Accept
The paper presents a significant and well-supported contribution to the field of deep learning. The novelty of learning activation functions nonparametrically, combined with strong theoretical underpinnings and empirical validation, makes this work a valuable addition to the conference.
Supporting Arguments:
1. Novelty and Contribution: The paper addresses a gap in the literature by proposing a method to learn activation functions dynamically during training, rather than treating them as fixed hyperparameters. This is a meaningful innovation, as it expands the functional capacity of neural networks.
2. Theoretical Rigor: The authors provide a thorough theoretical analysis, including generalization bounds using algorithmic stability, which strengthens the scientific validity of their claims.
3. Empirical Validation: The experimental results are compelling, showing consistent improvements across multiple datasets and architectures. The two-stage training process is particularly noteworthy for improving the stability and performance of convolutional networks.
4. Practical Relevance: The proposed method integrates seamlessly with existing backpropagation frameworks, making it accessible for practitioners.
Suggestions for Improvement:
1. Clarity of Presentation: While the theoretical sections are rigorous, they are dense and could benefit from clearer explanations or visual aids to improve accessibility for a broader audience.
2. Comparison with Alternatives: The paper briefly mentions related work (e.g., piecewise linear activation functions and Network-in-Network), but a more detailed comparison with these methods, including additional empirical benchmarks, would strengthen the argument for the proposed approach.
3. Two-Stage Training: The two-stage training process is effective but introduces additional complexity. Exploring ways to eliminate this step or provide more insights into why it is necessary would enhance the practicality of the method.
4. Limitations: While the paper acknowledges some limitations (e.g., the need for Fourier basis expansion), a more explicit discussion of potential drawbacks, such as computational overhead or scalability to very large datasets, would be helpful.
Questions for the Authors:
1. How does the computational cost of training networks with nonparametric activation functions compare to standard networks, especially for large-scale datasets?
2. Have you explored other basis functions (e.g., polynomial or wavelet) for activation function estimation? If so, how do they compare to Fourier basis expansion?
3. Can the two-stage training process be generalized or simplified for broader applicability?
In conclusion, this paper makes a strong theoretical and practical contribution to the field of deep learning. With minor revisions to improve clarity and address the above questions, it has the potential to significantly influence future research on neural network architectures.