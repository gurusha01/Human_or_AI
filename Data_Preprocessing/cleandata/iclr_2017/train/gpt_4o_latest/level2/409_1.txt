The paper introduces MusicNet, a large-scale, freely available dataset for supervised learning and evaluation in music research, addressing the lack of substantial labeled datasets for classical music note prediction. MusicNet comprises 34 hours of classical music recordings, annotated with over 1.2 million temporal labels across 11 instruments and 10 composers. The authors define a multi-label classification task for note prediction and benchmark several machine learning models, including spectrogram-based and end-to-end neural networks. A key finding is that end-to-end models learn frequency-selective filters as low-level features, which modestly outperform spectrogram features for note prediction. The paper also provides an evaluation protocol and discusses the dataset's construction, alignment methodology, and experimental results.
Decision: Accept
The paper makes a significant contribution by introducing a large, publicly available dataset for music research, which fills a critical gap in the field. The dataset's scale, diversity, and detailed annotations make it a valuable resource for the machine learning and music informatics communities. Additionally, the experiments demonstrate the utility of MusicNet for benchmarking machine learning models, and the findings on learned features are insightful. However, there are areas for improvement, particularly in the analysis of limitations and broader applicability.
Supporting Arguments:
1. Novelty and Contribution: MusicNet is a novel and impactful resource, addressing the long-standing challenge of limited labeled data in music research. The dataset's scale and diversity are unparalleled compared to existing datasets like MIREX and MAPS.
2. Experimental Rigor: The paper benchmarks multiple machine learning models, providing a clear evaluation protocol and demonstrating the utility of the dataset. The results are scientifically rigorous, with detailed comparisons and insights into model performance.
3. Relevance and Practicality: The dataset and tasks are well-motivated, aligning with the needs of the music informatics community. The findings on learned features have implications for future research on audio representation learning.
Suggestions for Improvement:
1. Limitations and Bias: The paper briefly mentions the dataset's skew towards Beethoven and solo piano recordings but does not adequately discuss the implications of this bias on generalizability. A more thorough analysis of potential limitations and suggestions for mitigating them (e.g., augmenting underrepresented instruments) would strengthen the paper.
2. Reproducibility: While the dataset and code are publicly available, the paper could provide more details on hyperparameter settings, training protocols, and computational requirements to enhance reproducibility.
3. Broader Impact: The paper could explore potential applications of MusicNet beyond note prediction, such as music generation or style transfer, to highlight its broader utility.
Questions for the Authors:
1. How does the dataset handle variations in recording quality and microphone conditions? Were any preprocessing steps applied to standardize the audio data?
2. Could the authors elaborate on the choice of evaluation metrics and their implications for real-world music applications?
3. Are there plans to expand MusicNet to include other genres or instruments, and how might this affect the dataset's utility?
In conclusion, the paper presents a valuable dataset and a well-structured evaluation framework, making it a strong candidate for acceptance. Addressing the suggested improvements would further enhance its impact and usability.