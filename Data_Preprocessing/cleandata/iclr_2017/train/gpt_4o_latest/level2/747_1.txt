Review of the Paper: "Interior Gradients for Feature Importance in Deep Networks"
The paper introduces Interior Gradients, a novel method for quantifying feature importance in deep neural networks. The authors address a critical limitation of gradient-based feature attribution methods: the issue of saturation in nonlinear deep networks, where important features can have near-zero gradients. By examining gradients of counterfactual inputs (scaled versions of the original input), the proposed method provides a more faithful representation of feature importance. The paper demonstrates the applicability of Interior Gradients across diverse architectures, including GoogleNet for image recognition, ligand-based virtual screening networks, and LSTM-based language models. The method is computationally efficient, easy to implement, and satisfies key axioms for attribution methods, such as Sensitivity and Implementation Invariance.
Decision: Accept
The paper makes a significant contribution to the field of explainable AI by addressing a well-known limitation of gradient-based attribution methods and proposing a practical, theoretically grounded solution. The method is simple to implement, widely applicable, and empirically validated across multiple domains. The authors also provide a thorough theoretical analysis, demonstrating that Interior Gradients satisfy desirable axioms for feature attribution. These strengths outweigh the limitations discussed in the paper, making it a valuable contribution to the conference.
Supporting Arguments:
1. Novelty and Significance: The paper identifies a critical issue (saturation) in existing gradient-based methods and proposes a novel approach that is both theoretically sound and practically useful. The integration of counterfactual inputs to mitigate saturation is an innovative idea that has not been explored in prior work.
2. Empirical Validation: The authors present extensive experiments across multiple domains (vision, molecular screening, and language modeling) to demonstrate the effectiveness of Interior Gradients. The results consistently show that the method outperforms standard gradients in capturing feature importance.
3. Theoretical Rigor: The paper provides a solid theoretical foundation for Interior Gradients, including proofs of key properties like Sensitivity and Implementation Invariance. The method's connection to Aumann-Shapley cost-sharing further strengthens its theoretical grounding.
4. Practical Utility: The method is computationally efficient, requiring only standard gradient computations, and is easy to implement in existing deep learning frameworks. This lowers the barrier to adoption for practitioners.
Suggestions for Improvement:
1. Evaluation Metrics: While the paper uses ablation studies, bounding box localization, and qualitative visualizations to evaluate the method, the metrics could be expanded. For example, a comparison with other state-of-the-art attribution methods (e.g., DeepLift, LRP) would strengthen the empirical claims.
2. Broader Applicability: The paper focuses on specific types of counterfactuals (e.g., scaling inputs). It would be helpful to discuss how the method could be adapted to other types of inputs, such as categorical data or graph-structured data, beyond the ligand-based example.
3. Limitations: While the paper acknowledges limitations like the inability to capture feature interactions and challenges with feature correlations, more concrete suggestions for addressing these issues would be valuable.
4. Visualization Clarity: The visualizations of feature importance (e.g., Figure 3) are compelling but could benefit from additional annotations or explanations to make them more accessible to readers unfamiliar with the specific datasets.
Questions for the Authors:
1. How does the method perform compared to other white-box attribution methods like DeepLift or LRP? Are there specific scenarios where Interior Gradients might underperform compared to these methods?
2. Can the proposed method be extended to handle feature interactions or correlations explicitly? If so, how?
3. Have you explored the use of Interior Gradients during the training process to mitigate saturation and improve model performance, as speculated in the conclusion?
Conclusion:
The paper presents a compelling and well-executed contribution to the field of explainable AI. The proposed method is theoretically sound, practically useful, and empirically validated. While there is room for further evaluation and broader applicability, the strengths of the paper make it a strong candidate for acceptance.