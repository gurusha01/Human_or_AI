The paper presents a novel approach to handwritten word recognition inspired by cognitive neuroscience research on reading, specifically leveraging open-bigram (OB) representations rather than traditional sequential character recognition. The authors propose a decoder based on cosine similarity in the OB space and train Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict bigrams. The method is evaluated on two public datasets (Rimes and IAM), demonstrating competitive performance compared to conventional techniques. The work highlights the potential of OB representations to encode global letter order and redundancy, enabling robust recognition even with noisy predictions.
Decision: Accept.  
The paper introduces an innovative and biologically inspired approach to word recognition that is both conceptually intriguing and practically competitive. The key reasons for acceptance are the novelty of the OB-based method and the rigor of the experimental evaluation, which demonstrates comparable performance to state-of-the-art systems while offering a fresh perspective on word recognition.
Supporting Arguments:  
1. Novelty and Contribution: The use of OB representations, inspired by cognitive neuroscience, is a significant departure from conventional character-sequence-based methods. This approach is well-motivated by prior research in psychology and neuroscience, which lends credibility to its theoretical foundation.  
2. Experimental Rigor: The authors conduct extensive experiments on two well-known datasets, comparing their method against baseline systems (HMMs and RNNs with Viterbi decoding). The results demonstrate that the OB-based decoder achieves competitive error rates, particularly when incorporating boundary bigrams and single-character predictions.  
3. Practical Implications: The OB representation's robustness to noisy predictions and its ability to encode global letter order suggest potential applications in noisy or low-resource settings. The decoder's simplicity (cosine similarity) is another practical advantage.  
Additional Feedback for Improvement:  
1. Clarity of Presentation: While the paper is thorough, some sections (e.g., the mathematical formulation of bigrams) are dense and could benefit from clearer explanations or illustrative examples. Simplifying these sections would make the work more accessible to a broader audience.  
2. Error Analysis: The paper briefly mentions error analysis but does not delve deeply into the types of errors made by the OB decoder compared to sequential models. A more detailed discussion could provide insights into the strengths and weaknesses of the proposed approach.  
3. Scalability: The paper focuses on bigrams of order up to three. It would be helpful to discuss the scalability of the method to higher-order bigrams or larger vocabularies, as well as its computational efficiency compared to traditional methods.  
Questions for the Authors:  
1. How does the OB decoder handle out-of-vocabulary (OOV) words, and could it be extended to accommodate them?  
2. The paper mentions that higher-order bigrams are more challenging to predict. Could you elaborate on how this impacts the overall system performance and whether alternative architectures might improve higher-order bigram prediction?  
3. Have you considered integrating a language model or other contextual information into the OB decoder to further improve recognition accuracy?  
Overall, the paper provides a compelling case for the use of OB representations in handwritten word recognition and opens up new avenues for research at the intersection of cognitive neuroscience and machine learning. With minor improvements in clarity and additional analysis, this work could make a significant impact on the field.