The paper proposes a novel method for reducing the computational cost of convolutional neural networks (CNNs) by pruning filters with low importance, as measured by their `1-norm. Unlike weight pruning, which often leads to irregular sparsity and requires specialized sparse libraries, the proposed filter pruning approach maintains dense connectivity and is compatible with existing efficient BLAS libraries. The authors demonstrate that their method achieves significant reductions in FLOPs (up to 34% for VGG-16 and 38% for ResNet-110 on CIFAR-10) while maintaining accuracy through retraining. They also provide a sensitivity analysis of layers to pruning and propose strategies for pruning across multiple layers, including for complex architectures like ResNets.
Decision: Accept
Key Reasons for Decision:
1. Practical Utility and Novelty: The paper addresses a critical challenge in deploying CNNs on resource-constrained devices by proposing a structured pruning method that avoids the pitfalls of irregular sparsity. The approach is novel in its simplicity and compatibility with existing dense computation libraries.
2. Strong Empirical Support: The authors provide extensive experimental results on multiple architectures (VGG-16, ResNet-56/110/34) and datasets (CIFAR-10, ImageNet), demonstrating the effectiveness of their method in reducing computational costs without significant accuracy loss.
Supporting Arguments:
- The paper is well-motivated, highlighting the limitations of existing pruning methods, such as sparse weight pruning and the need for specialized hardware or libraries. The proposed filter pruning method effectively addresses these issues.
- The use of `1-norm as a criterion for filter importance is justified with empirical evidence, and the results are benchmarked against alternative pruning strategies (e.g., random pruning, activation-based pruning).
- The authors provide a detailed analysis of layer sensitivity to pruning and propose practical strategies for pruning across multiple layers, including holistic and greedy approaches. This adds depth to the work and makes it applicable to a wide range of architectures.
- The experiments are rigorous, with comparisons to baseline models and alternative pruning methods. The results are reproducible, as the methodology and hyperparameters are clearly described.
Suggestions for Improvement:
1. Clarify Sensitivity Analysis: While the sensitivity analysis is insightful, it would be helpful to provide more details on how the pruning ratios for sensitive layers are determined and whether these ratios generalize across datasets.
2. Comparison with State-of-the-Art: The paper could benefit from a direct comparison with more recent state-of-the-art pruning techniques, particularly those that also focus on structured pruning.
3. Real-World Applications: Including results on real-world deployment scenarios (e.g., mobile devices or embedded systems) would strengthen the practical relevance of the work.
4. Limitations: The paper does not explicitly discuss potential limitations, such as the impact of pruning on transfer learning tasks or the scalability of the method to extremely large-scale datasets.
Questions for Authors:
1. How does the method perform on larger datasets like ImageNet when applied to architectures with higher capacity, such as ResNet-50 or EfficientNet?
2. Can the proposed method be combined with other compression techniques, such as quantization or knowledge distillation, to achieve further reductions in computational cost?
3. How sensitive is the method to the choice of the pruning ratio, and what guidelines can be provided for selecting this ratio in practice?
Overall, the paper presents a significant contribution to the field of model compression and is well-suited for acceptance at the conference. The proposed method is practical, effective, and supported by strong empirical evidence.