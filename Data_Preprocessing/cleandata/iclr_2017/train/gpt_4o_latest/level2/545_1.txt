Review of the Paper: "Compositional Kernel Machines (CKMs)"
Summary of Claims
This paper introduces Compositional Kernel Machines (CKMs), a novel instance-based learning method designed to address the limitations of convolutional neural networks (convnets) and traditional kernel methods in computer vision tasks. The authors claim that CKMs:  
1. Leverage compositionality and symmetry to create an exponential number of virtual training instances, mitigating the curse of dimensionality.  
2. Efficiently compute discriminant functions using a sum-product function (SPF), enabling tractable summation over virtual instances.  
3. Achieve competitive or superior performance compared to convnets and support vector machines (SVMs) in tasks requiring compositional and symmetry-based generalization, particularly with limited data or computational resources.  
4. Offer scalability advantages, including faster training and easier hyperparameter tuning compared to deep learning methods.  
The paper supports these claims through experiments on the NORB datasets, demonstrating CKMs' ability to generalize to new compositions, symmetries, and distortions.
Decision: Accept  
The paper makes a compelling case for CKMs as a novel and practically useful alternative to convnets and SVMs, particularly in scenarios with limited data or computational resources. The key reasons for this decision are:  
1. Novelty and Innovation: CKMs represent a significant conceptual advancement by combining kernel methods with compositionality and symmetry, offering a unique approach to mitigating the curse of dimensionality.  
2. Experimental Validation: The results on the NORB datasets are promising, showing that CKMs outperform SVMs and are competitive with convnets in tasks requiring compositional reasoning. The learning curves and scalability analysis further strengthen the paper's claims.  
Supporting Arguments
1. Support for Claims: The paper provides strong experimental evidence for CKMs' efficacy, including comparisons with state-of-the-art methods. The use of virtual instances and the SPF framework is well-motivated and rigorously analyzed.  
2. Practical Usefulness: CKMs' ability to train quickly on CPUs and generalize from fewer samples makes them highly relevant for real-world applications, especially in resource-constrained environments.  
3. Positioning in Literature: The paper demonstrates a solid understanding of related work, situating CKMs as a natural evolution of instance-based learning methods while addressing the limitations of convnets and SVMs.  
Suggestions for Improvement
1. Architectural Details: While the SPF framework is well-explained, the paper could provide more clarity on how CKM architectures could be extended to richer image structures or other domains.  
2. Comparison with Modern Deep Learning: The experiments primarily compare CKMs to convnets trained from scratch. Including results with transfer learning or fine-tuning on pre-trained deep models would provide a more comprehensive evaluation.  
3. Limitations: The paper briefly mentions that CKMs result in large model sizes. A more detailed discussion of this trade-off and potential solutions (e.g., sparsity-inducing regularization) would strengthen the paper.  
4. Broader Applications: While the focus on object recognition is appropriate, exploring CKMs' potential in other domains (e.g., structured prediction or reinforcement learning) could broaden their appeal.  
Questions for the Authors
1. How do CKMs perform on larger, more diverse datasets (e.g., ImageNet)? Are there scalability challenges as the dataset size increases?  
2. Can CKMs incorporate learned feature representations (e.g., from pre-trained convnets) to improve performance on tasks requiring hierarchical feature extraction?  
3. How sensitive are CKMs to the choice of kernel functions and hyperparameters? Could automated kernel selection methods be integrated into the framework?  
Conclusion
This paper presents a novel and well-supported contribution to the field of computer vision and kernel methods. While there is room for improvement in terms of broader evaluation and discussion of limitations, the core ideas and results are strong enough to warrant acceptance. CKMs have the potential to inspire future research in both machine learning theory and practical applications.