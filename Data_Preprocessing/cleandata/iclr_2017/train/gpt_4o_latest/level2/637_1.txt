The paper proposes a novel approach for mining logical theories directly from relational embeddings, addressing limitations of prior methods in rule mining and inductive logic programming (ILP). The key contribution is the formulation of theory learning as a sparse recovery problem in the space of embeddings, enabling the discovery of expressive logical rules that include conjunctions, disjunctions, and negations. The authors leverage compressed sensing techniques, such as Orthogonal Matching Pursuit (OMP), to extract diverse and interpretable rules while avoiding redundancy. Empirical evaluations demonstrate the superiority of the proposed Feature Rule Miner (FRM) over existing kNN-based methods on multiple knowledge bases, achieving higher F-scores and per-rule recall.
Decision: Accept
The paper is well-motivated, presents a significant improvement over existing methods, and demonstrates strong empirical results. The key reasons for acceptance are: (1) the novel formulation of rule mining as a sparse recovery problem, which enables the use of efficient algorithms and supports richer logical constructs, and (2) the empirical evidence showing FRM's superior performance in terms of rule diversity, interpretability, and reconstruction accuracy.
Supporting Arguments:
1. Novelty and Contribution: The paper builds on the limitations of prior methods, such as Yang et al. (2015), by introducing a more expressive and scalable approach. The ability to handle negations and avoid redundant rules is a notable advancement.
   
2. Empirical Validation: The experiments are comprehensive, covering four diverse knowledge bases and comparing FRM against strong baselines. The results consistently favor FRM, highlighting its practical utility and robustness.
3. Theoretical Soundness: The use of compressed sensing techniques is well-justified, and the derivation of the optimization problem is rigorous. The authors also provide a clear explanation of how rule diversity is enforced through orthogonality.
4. Practical Relevance: The method is applicable to large-scale knowledge bases, addressing scalability issues of traditional ILP methods. The use of embeddings aligns with current trends in representation learning.
Additional Feedback:
1. Clarity: While the paper is technically sound, the presentation of the mathematical derivations could be streamlined for better readability. For instance, the transition from logical operations to matrix operations might benefit from additional illustrative examples.
2. Limitations: The paper briefly mentions the computational cost of enumerating candidate rule bodies but does not provide a detailed analysis. A discussion of runtime performance and scalability for larger knowledge bases would strengthen the paper.
3. Future Work: The authors suggest using online recovery algorithms to address the exponential growth of candidate paths. Expanding on this direction with preliminary results or insights would be valuable.
4. Evaluation Metrics: While F-score and per-rule recall are appropriate, additional metrics such as interpretability scores or runtime comparisons could provide a more holistic evaluation.
Questions for the Authors:
1. How does the method scale with increasing rule length (`l`) and the size of the knowledge base? Are there practical limits to the approach?
2. Can the proposed method handle noisy or incomplete embeddings, and how does this affect rule quality?
3. How sensitive is the performance to the choice of hyperparameters, such as the coefficient threshold (`Ï„`) and embedding size (`d`)?
Overall, this paper makes a strong contribution to the field of rule mining and relational learning, and I recommend its acceptance with minor revisions to improve clarity and address scalability concerns.