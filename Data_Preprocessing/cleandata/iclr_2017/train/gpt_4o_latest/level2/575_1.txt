Review of "Differentiable Canonical Correlation Analysis for Multi-Modality Neural Networks"
Summary of Contributions
This paper introduces Differentiable Canonical Correlation Analysis (CCA), a novel formulation of CCA that can be integrated as a layer within multi-view neural networks. Unlike Deep CCA (DCCA), which optimizes for maximally correlated projections as a final objective, Differentiable CCA enables gradient flow through the CCA projection matrices, allowing for task-specific optimization objectives. The authors demonstrate the utility of this approach in cross-modality retrieval tasks on two public datasets (Flickr30k and IAPR TC-12), achieving superior performance compared to DCCA and freely-learned projection layers. The paper argues that Differentiable CCA could serve as a versatile building block for multi-modality tasks.
Decision: Accept
The paper is accepted due to its significant methodological contribution, strong empirical results, and potential for broader applicability in multi-modality tasks. The key reasons for this decision are:
1. Novelty and Innovation: The proposed differentiable CCA layer extends DCCA by enabling backpropagation through the CCA computation, which is a meaningful advancement in the field.
2. Empirical Validation: The paper provides robust experimental results demonstrating the superiority of Differentiable CCA over existing methods in cross-modality retrieval tasks.
3. Practical Utility: The approach is well-motivated and has clear potential for application in other multi-modality tasks, making it relevant for the target audience.
Supporting Arguments
1. Claims and Support: The paper claims that Differentiable CCA improves cross-modality retrieval performance by enabling task-specific optimization objectives. This claim is well-supported by experiments on two datasets, where the proposed method outperforms DCCA and other baselines in terms of Recall@k, Median Rank, and MAP.
2. Positioning in Literature: The paper demonstrates a solid understanding of prior work, including classical CCA, DCCA, and related methods. The authors clearly articulate how their approach builds on and extends these methods.
3. Scientific Rigor: The methodology is presented in detail, with clear mathematical derivations and practical considerations (e.g., handling stochastic optimization and mini-batch training). The experiments are thorough, and the results are statistically significant.
4. Acknowledgment of Limitations: The authors acknowledge potential limitations, such as the sensitivity of covariance estimation to batch size, and provide practical recommendations to mitigate these issues.
Suggestions for Improvement
1. Broader Evaluation: While the experiments on Flickr30k and IAPR TC-12 are compelling, additional datasets or tasks (e.g., video-text retrieval or speech-vision tasks) would strengthen the generalizability of the results.
2. Ablation Studies: The paper could benefit from more detailed ablation studies, particularly on the choice of optimization objectives (e.g., cosine distance vs. squared cosine distance) and the impact of hyperparameters like batch size and regularization.
3. Reproducibility: While the authors mention that the experimental code will be released, including a link to the code repository in the paper would enhance reproducibility.
Questions for the Authors
1. How does the performance of Differentiable CCA compare to more recent methods beyond DCCA (e.g., transformer-based multi-modality models)?
2. Can the proposed method be extended to handle more than two modalities (e.g., image, text, and audio simultaneously)?
3. How sensitive is the method to the choice of dimensionality for the CCA projection space (e.g., k=128)? Would adaptive dimensionality selection improve performance?
Conclusion
The paper presents a well-motivated and innovative contribution to multi-modality learning. Its strong empirical results and potential for broader applicability make it a valuable addition to the conference. Addressing the suggested improvements and questions could further enhance the impact of this work.