The paper introduces adaptive softmax, a novel approximation technique for efficiently training neural network-based language models over large vocabularies. The authors address the computational bottleneck of the softmax layer in language modeling by leveraging the unbalanced word distribution to form clusters that minimize computational complexity. The proposed method is designed to exploit modern GPU architectures, achieving significant speed-ups (2× to 10×) compared to the full softmax, while maintaining competitive accuracy. Experimental results on benchmarks such as EuroParl, Text8, and One Billion Word demonstrate the method's efficiency and scalability. The authors also provide publicly available code, enhancing reproducibility.
Decision: Accept  
The paper is well-motivated, demonstrates strong empirical results, and provides a significant contribution to the field of efficient language modeling. The adaptive softmax achieves a compelling trade-off between computational efficiency and accuracy, making it a practical solution for large-scale language modeling tasks. The decision to accept is based on the novelty of the approach, its empirical rigor, and its practical utility.
Supporting Arguments:
1. Novelty and Contribution: The adaptive softmax introduces a new hierarchical clustering strategy explicitly optimized for GPU architectures, which is a notable improvement over existing methods like hierarchical softmax and differentiated softmax. The use of a short-list for frequent words and reduced capacity for rare words is both innovative and effective.
   
2. Empirical Validation: The paper provides extensive experiments on diverse datasets, demonstrating consistent speed-ups and competitive perplexity scores. Notably, the method achieves a perplexity of 43.9 on the One Billion Word benchmark using a single GPU, outperforming comparable methods in efficiency.
3. Practical Relevance: The method addresses a critical bottleneck in language modeling, making it highly relevant for real-world applications where training time and computational resources are constrained. The availability of code further enhances its impact.
Suggestions for Improvement:
1. Clarity of Presentation: While the technical details are thorough, the paper could benefit from a more concise explanation of the adaptive softmax's core algorithm. Simplifying the mathematical derivations and providing a high-level overview would improve accessibility for a broader audience.
2. Comparison with More Recent Baselines: The paper compares its method to several established baselines but does not include comparisons with more recent advancements in efficient softmax approximations. Including such comparisons would strengthen the evaluation.
3. Ablation Studies: While the paper discusses the impact of certain design choices (e.g., short-listing frequent words), more detailed ablation studies quantifying the contribution of each component would provide deeper insights into the method's effectiveness.
4. Limitations: The paper does not explicitly discuss potential limitations, such as the trade-offs in accuracy for very small datasets or the scalability of the approach to non-GPU architectures. Addressing these would provide a more balanced perspective.
Questions for the Authors:
1. How does the adaptive softmax perform on tasks beyond language modeling, such as machine translation or speech recognition? Could the method generalize to these domains?
2. What are the implications of using the adaptive softmax on inference speed, particularly in real-time applications?
3. How sensitive is the method to the choice of hyperparameters, such as the size of the short-list or the projection dimensions for rare words?
In conclusion, the paper makes a significant contribution to efficient language modeling and is well-suited for acceptance at the conference. Addressing the above suggestions would further enhance its impact and clarity.