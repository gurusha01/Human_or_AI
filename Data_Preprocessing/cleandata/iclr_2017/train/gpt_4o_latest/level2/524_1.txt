Review
The paper presents a novel, domain-agnostic dataset augmentation technique that operates in a learned feature space rather than the input space. The authors propose using sequence autoencoders (SA) to construct feature spaces and apply transformations such as noise addition, interpolation, and extrapolation to augment datasets. The primary claim is that extrapolation in feature space generates more diverse and useful synthetic data, leading to improved performance of supervised learning models across various domains. The paper demonstrates the effectiveness of this approach on multiple datasets, including time-series data (e.g., Arabic Digits, AUSLAN) and image data (e.g., MNIST, CIFAR-10), achieving competitive or near state-of-the-art results.
Decision: Accept  
Key reasons: (1) The paper introduces a simple yet effective and domain-independent approach to dataset augmentation, which is a significant contribution to the field. (2) The experimental results are comprehensive and demonstrate the practical utility of the proposed method across diverse datasets.
Supporting Arguments:  
1. Novelty and Contribution: The idea of performing dataset augmentation in feature space rather than input space is innovative and well-motivated. The paper highlights the limitations of traditional domain-specific augmentation techniques and provides a general-purpose alternative. The use of extrapolation to generate diverse synthetic samples is particularly compelling.  
2. Experimental Validation: The authors conduct rigorous experiments on five datasets spanning different domains (speech, motion capture, images, etc.), showing consistent improvements in model performance. The quantitative results, such as the significant error reduction in Arabic Digits and AUSLAN datasets, validate the effectiveness of the approach.  
3. Relevance and Timeliness: The method leverages recent advances in unsupervised representation learning (e.g., sequence autoencoders) and aligns with the growing interest in domain-agnostic machine learning techniques.  
Additional Feedback:  
- While the paper provides strong evidence for the effectiveness of extrapolation, the results for noise addition and interpolation are less convincing. It would be helpful to include a more detailed analysis of when these techniques might be beneficial (e.g., simpler decision boundaries).  
- The paper could benefit from a more explicit discussion of computational overhead introduced by the sequence autoencoder and nearest neighbor search, particularly for large-scale datasets.  
- The visualization results (e.g., sinusoids, handwritten characters) are insightful but could be expanded to include failure cases or edge scenarios where the method performs poorly.  
- The authors mention that extrapolation complements domain-specific augmentation (e.g., in CIFAR-10), but this interaction is not explored in depth. A more detailed analysis of how feature space augmentation interacts with traditional methods would strengthen the paper.  
Questions for Authors:  
1. How does the computational cost of training the sequence autoencoder and performing nearest neighbor search scale with dataset size? Could this limit the applicability of the method to very large datasets?  
2. Have you tested the method on datasets with highly imbalanced classes? If so, how does extrapolation perform in such scenarios?  
3. Could the feature space learned by other unsupervised models (e.g., variational autoencoders or contrastive learning models) yield similar or better results compared to sequence autoencoders?  
Overall, the paper presents a promising and well-supported contribution to dataset augmentation techniques. Addressing the above feedback and questions could further strengthen its impact.