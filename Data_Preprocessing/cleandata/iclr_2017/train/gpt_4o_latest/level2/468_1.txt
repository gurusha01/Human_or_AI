The paper presents a novel approach to network quantization, a critical technique for compressing deep neural networks to enable deployment on resource-constrained devices. The authors propose Hessian-weighted k-means clustering to minimize performance loss during quantization, and they connect the network quantization problem to entropy-constrained scalar quantization (ECSQ) in information theory. Two practical solutions, uniform quantization and an iterative ECSQ algorithm, are introduced. The paper demonstrates significant compression ratios (e.g., 51.25 for LeNet, 22.17 for ResNet, and 40.65 for AlexNet) with minimal performance degradation, supported by experiments on various neural networks.
Decision: Accept  
The paper should be accepted due to its strong contributions to the field of network compression, particularly the innovative use of Hessian-weighted distortion measures and the connection to ECSQ, which provides a theoretical foundation for optimizing compression ratios. The experimental results are robust and demonstrate practical utility.
Supporting Arguments:  
1. Novelty and Contribution: The paper introduces Hessian-weighted k-means clustering, which accounts for the varying importance of network parameters based on their impact on the loss function. This is a significant improvement over conventional k-means clustering, which treats all parameters equally. The connection to ECSQ is also novel and provides a theoretical framework for optimizing compression ratios.  
2. Experimental Validation: The experiments are comprehensive, covering three different neural networks (LeNet, ResNet, and AlexNet) and demonstrating the effectiveness of the proposed methods. The results show that the proposed techniques outperform conventional methods in terms of compression ratios and accuracy retention.  
3. Practical Relevance: The methods are computationally efficient and applicable to real-world scenarios, such as deploying deep neural networks on mobile devices. The use of alternatives to Hessian (e.g., second moment estimates from Adam optimizer) further enhances the practicality of the approach.  
Additional Feedback for Improvement:  
1. Clarity of Presentation: While the paper is technically sound, some sections, particularly those involving mathematical derivations (e.g., Hessian-weighted distortion and ECSQ), could benefit from additional explanations or visual aids to improve accessibility for a broader audience.  
2. Comparison with Other Methods: The paper could include more detailed comparisons with other state-of-the-art compression techniques, such as pruning and low-rank approximations, to contextualize its contributions further.  
3. Limitations: The authors briefly mention the diagonal approximation of the Hessian but do not quantify its impact on performance. A more detailed discussion of this limitation and potential future work to address it would strengthen the paper.  
Questions for the Authors:  
1. How does the proposed Hessian-weighted k-means clustering perform when applied to extremely large-scale models, such as GPT or Vision Transformers? Are there scalability concerns?  
2. Could the authors provide more insights into the trade-offs between using Hessian and its alternative (e.g., second moment estimates) in terms of computational cost and accuracy?  
3. How sensitive are the results to the choice of the compression ratio constraint? Could this method be extended to dynamically adjust the compression ratio based on hardware constraints?  
Overall, the paper makes a strong contribution to the field of network compression, and its methods are both theoretically grounded and practically relevant. With minor improvements to clarity and additional comparisons, the paper could have an even greater impact.