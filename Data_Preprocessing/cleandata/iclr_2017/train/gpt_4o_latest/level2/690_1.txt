The paper presents a comprehensive analysis of state-of-the-art Deep Neural Networks (DNNs) submitted to the ImageNet challenge, focusing on practical metrics such as accuracy, memory footprint, parameter count, operations count, inference time, and power consumption. The authors identify key findings, including the hyperbolic relationship between accuracy and inference time, the utility of operations count as a proxy for inference time, and the energy constraints that limit model complexity and accuracy. The paper also introduces ENet, a highly efficient architecture that achieves state-of-the-art results with significantly fewer parameters. The authors aim to provide actionable insights for designing efficient DNNs for real-world applications.
Decision: Accept  
The paper is well-motivated, provides valuable insights into practical considerations for DNN deployment, and offers a novel contribution in the form of ENet. The analysis is thorough, and the findings are supported by rigorous experiments. The work addresses an important gap in the literature by emphasizing resource efficiency, which is critical for real-world applications. However, there are areas where the paper could be improved for clarity and impact.
Supporting Arguments:
1. Novelty and Practical Relevance: The paper addresses a significant and underexplored problem in the fieldâ€”optimizing DNNs for resource-constrained environments. The introduction of ENet as an efficient architecture adds a novel contribution.
2. Thorough Experimental Support: The claims are supported by extensive experiments conducted on a resource-limited device, ensuring real-world applicability. The use of multiple metrics (accuracy, power, memory, etc.) provides a holistic view of model performance.
3. Field Knowledge and Placement in Literature: The paper demonstrates a strong understanding of the field, referencing key works and situating its contributions within the broader context of DNN optimization.
Suggestions for Improvement:
1. Clarity in Results Presentation: While the results are comprehensive, some figures (e.g., Figures 2, 4, and 7) are not adequately explained in the text. Clearer descriptions of how these figures support the claims would enhance readability.
2. Limitations and Generalizability: The paper does not sufficiently discuss the limitations of its findings. For example, how generalizable are the results to other datasets or hardware platforms beyond the NVIDIA Jetson TX1? Addressing this would strengthen the paper.
3. Comparison with Related Work: While the paper references prior work, a more explicit comparison of ENet with other efficiency-focused architectures (e.g., MobileNet, SqueezeNet) would contextualize its contributions better.
4. Reproducibility: The methods section provides some implementation details, but additional information (e.g., hyperparameters, training protocols) would improve reproducibility.
Questions for the Authors:
1. How does ENet compare to other lightweight architectures like MobileNet or SqueezeNet in terms of accuracy and resource efficiency?
2. Could the findings about the hyperbolic relationship between accuracy and inference time be generalized to other datasets or tasks beyond ImageNet?
3. How does the choice of the NVIDIA Jetson TX1 influence the results? Would similar trends hold on other hardware platforms?
Overall, this paper makes a valuable contribution to the field by addressing the practical challenges of deploying DNNs in resource-constrained environments. With minor revisions to improve clarity and address limitations, it has the potential to make a significant impact.