The paper presents a novel approach to visual servoing by combining learned visual features, predictive dynamics models, and reinforcement learning (RL) to address the limitations of traditional methods that rely on manually designed features and analytical models. The authors focus on target following and propose a method that uses pre-trained deep features (from VGG networks) and a bilinear predictive model, coupled with a sample-efficient fitted Q-iteration algorithm, to achieve robust and adaptive visual servoing with minimal training data. The approach demonstrates significant improvements in robustness to visual variations, occlusions, and sample efficiency compared to conventional methods and standard model-free RL algorithms.
Decision: Accept.  
Key reasons: (1) The paper introduces a method that is both novel and practically useful, achieving state-of-the-art results in sample efficiency and robustness. (2) The claims are well-supported by extensive experiments, including comparisons to prior methods, ablation studies, and generalization tests.
Supporting Arguments:  
1. Novelty and Contribution: The integration of pre-trained deep features with bilinear dynamics and fitted Q-iteration is innovative and addresses key challenges in visual servoing, such as robustness to distractors and efficient adaptation to new targets. The work significantly improves upon traditional and learning-based approaches, as evidenced by the results on a synthetic car-following benchmark.  
2. Experimental Rigor: The paper provides thorough experimental validation, including comparisons to classical image-based and position-based visual servoing methods, end-to-end RL policies, and alternative feature representations. The results convincingly demonstrate the superiority of the proposed method in terms of robustness, generalization, and sample efficiency.  
3. Practical Utility: The proposed method is highly practical, requiring only 20 training trajectories to achieve effective servoing. The use of pre-trained features and a simple servoing policy makes the approach computationally efficient and accessible for real-world applications.
Suggestions for Improvement:  
1. Clarify Limitations: While the paper briefly mentions that the method relies on pre-trained features, it would benefit from a more explicit discussion of its limitations, such as potential challenges in real-world deployment (e.g., domain gaps between synthetic and real environments).  
2. Ablation on Feature Choice: Although the paper evaluates different VGG feature layers, it would be helpful to include a discussion on why certain layers (e.g., conv4_3) perform better and how this choice might generalize to other pre-trained networks.  
3. Real-World Validation: While the synthetic benchmark is comprehensive, demonstrating the method on a real-world dataset or physical robot would strengthen the paper's impact and applicability.  
4. Comparison to Other RL Algorithms: The paper could include a more detailed analysis of why standard RL algorithms (e.g., TRPO) perform poorly in this context and how the proposed fitted Q-iteration addresses these shortcomings.
Questions for Authors:  
1. How does the proposed method handle domain shifts between synthetic and real-world environments? Have you considered fine-tuning the pre-trained features for the specific task?  
2. Could the bilinear dynamics model be extended to handle more complex, non-linear dynamics, or would this compromise the computational efficiency of the approach?  
3. How sensitive is the method to the choice of hyperparameters, such as the regularization coefficient in the Q-iteration algorithm?  
Overall, the paper makes a strong contribution to the field of visual servoing and reinforcement learning, and its findings are likely to inspire further research in data-efficient and robust robotic control.