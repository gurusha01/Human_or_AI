The paper proposes a novel approach to multimodal learning using Joint Multimodal Variational Autoencoders (JMVAE), addressing the challenge of bi-directional modality generation. The authors introduce JMVAE, which models the joint distribution of modalities by conditioning them independently on a shared latent representation. Furthermore, they propose JMVAE-kl, a method to mitigate the issue of collapsed samples when modalities are missing by reducing the divergence between the multimodal encoder and single-modality encoders. The paper demonstrates the effectiveness of JMVAE and JMVAE-kl through experiments on MNIST and CelebA datasets, showing improvements in log-likelihoods and bi-directional generation capabilities.
Decision: Accept.  
Key reasons: (1) The paper presents a significant innovation in multimodal learning by enabling bi-directional generation of modalities, addressing a key limitation of existing VAE-based models. (2) The experimental results convincingly demonstrate the superiority of JMVAE and JMVAE-kl over baseline methods, both qualitatively and quantitatively.
Supporting Arguments:  
1. Novelty and Contributions: The paper introduces the first VAE-based model to train joint distributions of multiple modalities, enabling bi-directional generation. The addition of JMVAE-kl to address missing modality issues is a valuable enhancement. These contributions are well-motivated and represent a meaningful advancement over prior work like CVAEs and CMMAs.  
2. Experimental Validation: The experiments on MNIST and CelebA datasets are thorough, with both quantitative (log-likelihood comparisons) and qualitative (visualizations of generated samples) evaluations. The results clearly demonstrate the advantages of JMVAE and JMVAE-kl in terms of generating high-quality samples and extracting meaningful joint representations.  
3. Relevance and Practicality: The ability to generate modalities bi-directionally has practical applications in areas like image-captioning and attribute-based image editing. The use of GANs in conjunction with JMVAE for high-dimensional data further enhances its applicability.  
Additional Feedback:  
1. Clarity of Presentation: While the paper is well-structured, some sections, such as the derivation of JMVAE-kl's objective function, could benefit from clearer explanations or additional diagrams to aid understanding.  
2. Parameter Sensitivity: The paper notes a trade-off in JMVAE-kl's performance based on the α parameter. A more detailed analysis of how to choose α in practice would be helpful for reproducibility and broader adoption.  
3. Scalability: While the paper demonstrates strong results on two datasets, it would be valuable to discuss the scalability of JMVAE to datasets with more than two modalities or significantly larger datasets.  
Questions for Authors:  
1. How does the performance of JMVAE change when scaling to datasets with three or more modalities?  
2. Can you elaborate on the computational overhead introduced by JMVAE-kl compared to JMVAE-zero?  
3. How robust is the model to noise or incomplete data during training, particularly for high-dimensional modalities like images?  
Overall, the paper makes a strong contribution to the field of multimodal learning and is well-suited for acceptance at the conference. With minor clarifications and additional analysis, it could have even broader impact.