The paper presents a novel unsupervised pretraining method to enhance the performance of sequence-to-sequence (seq2seq) models, particularly in machine translation and abstractive summarization tasks. The proposed approach involves initializing the encoder and decoder of a seq2seq model with pretrained language model weights, followed by fine-tuning on labeled data. The authors demonstrate that this method improves generalization and optimization, achieving state-of-the-art results on the WMT Englishâ†’German translation tasks (WMT'14 and WMT'15) with a 1.3 BLEU improvement over previous best models. Additionally, the method surpasses supervised baselines in abstractive summarization tasks. The paper also includes an ablation study to analyze the contributions of various components, such as pretraining the encoder and decoder, residual connections, and monolingual language modeling objectives.
Decision: Accept.  
The key reasons for this decision are the significant empirical improvements demonstrated on challenging benchmarks and the clear novelty of the proposed pretraining method. The paper is well-motivated, scientifically rigorous, and provides valuable insights into the benefits of unsupervised pretraining for seq2seq models.
Supporting Arguments:  
1. Strong Results: The method achieves state-of-the-art performance on machine translation tasks, outperforming both single-model and ensemble baselines. The results are statistically significant and demonstrate the robustness of the approach across different datasets.  
2. Novelty and Practicality: The use of pretrained language models to initialize seq2seq components is a novel and practical idea, particularly for tasks with limited labeled data. The paper also highlights the flexibility of the method, making it applicable to various seq2seq tasks.  
3. Thorough Analysis: The ablation study provides a detailed understanding of the contributions of different components, such as pretraining the encoder versus the decoder, and the importance of the language modeling objective. This adds depth to the paper and strengthens its claims.
Additional Feedback:  
1. While the results on abstractive summarization are promising, the paper acknowledges that the gains are less pronounced compared to machine translation. It would be helpful to explore whether additional techniques (e.g., bidirectional LSTMs or longer context windows) could further improve summarization performance.  
2. The paper could benefit from a more detailed discussion of the computational costs associated with pretraining and fine-tuning, as this is an important consideration for practical deployment.  
3. The human evaluation of summarization outputs is a valuable addition, but the sample size (200 documents) is relatively small. Expanding this evaluation could provide more robust insights into the quality of the generated summaries.  
Questions for Authors:  
1. How does the proposed method compare to other semi-supervised techniques, such as backtranslation, in terms of computational efficiency and performance gains?  
2. Could the pretraining approach be extended to other seq2seq tasks, such as dialogue generation or question answering? If so, what modifications would be necessary?  
3. Did you observe any limitations or failure cases where pretraining did not lead to significant improvements? If so, could you elaborate on these scenarios?  
Overall, this paper makes a strong contribution to the field of seq2seq learning and is likely to inspire further research in unsupervised pretraining techniques.