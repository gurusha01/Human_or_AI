The paper explores adversarial attacks on deep generative models, specifically Variational Autoencoders (VAEs) and VAE-GANs, which have not been extensively studied in adversarial contexts. The authors propose three novel attack methodologies: a classifier-based attack, an attack using the VAE loss function (LVAE), and a latent space attack. These attacks are demonstrated on MNIST, SVHN, and CelebA datasets, showcasing the vulnerabilities of generative models to adversarial perturbations. The paper also provides a motivating attack scenario and evaluates the effectiveness of the proposed methods using metrics like attack success rates and reconstruction quality.
Decision: Reject
While the paper addresses an important and underexplored problem, the decision to reject is based on two key reasons: (1) insufficient novelty in the attack methodologies, and (2) lack of rigorous evaluation of real-world implications and defenses. Below, I provide detailed arguments and constructive feedback.
---
Supporting Arguments for Decision:
1. Novelty:  
   While the paper introduces three attack methods, the classifier-based attack is a straightforward application of existing adversarial techniques for classifiers. The LVAE and latent space attacks, though more tailored to generative models, lack significant innovation beyond adapting existing optimization techniques. The work does not sufficiently differentiate itself from prior adversarial research, such as attacks on latent spaces (e.g., Sabour et al., 2015).
2. Evaluation Gaps:  
   The evaluation focuses heavily on synthetic datasets (MNIST, SVHN) and a relatively simple dataset (CelebA). While these are standard benchmarks, the paper does not explore more complex or real-world datasets like CIFAR-10 or ImageNet, as mentioned in future work. Additionally, the metrics used (e.g., ASignore-target and AStarget) are limited to measuring attack success but do not assess the broader impact of these attacks on downstream tasks or real-world systems.
3. Practical Implications:  
   The motivating attack scenario is compelling but lacks empirical validation. For example, the paper does not demonstrate how these attacks could compromise real-world applications of generative models, such as image compression or data augmentation pipelines. Furthermore, the paper does not discuss potential defenses or robustness strategies, leaving the work incomplete.
---
Suggestions for Improvement:
1. Expand Evaluation Scope:  
   Include experiments on more complex datasets (e.g., CIFAR-10, ImageNet) to demonstrate the generalizability of the attacks. Additionally, evaluate the impact of these attacks on practical applications, such as anomaly detection or generative data augmentation.
2. Strengthen Novelty:  
   Explore more innovative attack methodologies that leverage unique properties of generative models, such as their ability to interpolate in latent space. For example, attacks that exploit disentangled latent representations or adversarial training dynamics in VAE-GANs could provide more significant contributions.
3. Address Defenses:  
   Discuss potential defense mechanisms, such as adversarial training for generative models or regularization techniques to improve robustness. Including even preliminary results on defenses would make the work more balanced and impactful.
4. Clarify Limitations:  
   While the paper acknowledges some limitations, such as the reliance on synthetic datasets, a more detailed discussion of the constraints of the proposed attacks (e.g., computational cost, transferability to unseen models) would strengthen the paper.
---
Questions for the Authors:
1. How do the proposed attacks perform on more complex datasets like CIFAR-10 or ImageNet? Are the methods scalable to such datasets?
2. Have you considered evaluating the impact of these attacks on downstream tasks, such as compressed image retrieval or generative data augmentation?
3. Can you provide insights into potential defenses against these attacks? For example, would adversarial training or latent space regularization mitigate the vulnerabilities?
---
In summary, the paper addresses an important gap in adversarial research for generative models but falls short in terms of novelty, evaluation rigor, and practical implications. With revisions addressing these points, the work has the potential to make a meaningful contribution to the field.