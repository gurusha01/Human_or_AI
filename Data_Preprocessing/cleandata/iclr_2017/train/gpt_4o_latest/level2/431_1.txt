The paper introduces PALEO, an analytical performance model for predicting the scalability and performance of deep learning systems. The authors claim that PALEO can accurately model the computational and communication requirements of neural network architectures across a wide variety of hardware, software, and parallelization strategies. The key contribution is the ability to estimate execution times without requiring extensive empirical benchmarking, making it a valuable tool for practitioners designing scalable deep learning systems. The paper demonstrates the robustness of PALEO through empirical validation on popular architectures like AlexNet, VGG, and Inception, as well as hypothetical setups, showing close alignment with real-world results.
Decision: Accept  
Key reasons:  
1. Novelty and Practical Utility: PALEO addresses a significant gap in the field by providing a scalable, analytical alternative to expensive benchmarking. Its ability to model diverse architectures and configurations makes it highly relevant for both researchers and practitioners.  
2. Strong Empirical Support: The paper provides thorough validation of PALEO's predictions against real-world results, demonstrating its accuracy and robustness.
Supporting Arguments:  
The paper is well-motivated, addressing critical questions about the scalability of deep learning systems. The authors situate their work effectively within the existing literature, referencing key frameworks like TensorFlow, FireCaffe, and cuDNN. The methodology is clearly described, with detailed explanations of how PALEO models computation and communication. The experiments are comprehensive, covering both real-world and hypothetical scenarios, and the results convincingly support the claims. For instance, PALEO's predictions for AlexNet and Inception align closely with empirical results, showcasing its practical reliability. Additionally, the inclusion of case studies and hypothetical setups highlights the model's versatility.
Suggestions for Improvement:  
1. Clarify Limitations: While the authors briefly mention that PALEO assumes ideal hardware utilization (e.g., peak FLOPS), a more detailed discussion of its limitations would strengthen the paper. For example, how does PALEO handle scenarios with highly heterogeneous hardware or non-standard architectures?  
2. Expand on PPP Parameter: The "Platform Percent of Peak" (PPP) parameter is an important aspect of the model, but its derivation and implications could be explained more thoroughly. How sensitive are PALEO's predictions to variations in PPP?  
3. Reproducibility: While the authors mention that PALEO is open-sourced, including a link to detailed documentation or examples would enhance reproducibility and encourage adoption by the community.  
4. Broader Applicability: The paper focuses primarily on CNNs and GANs. It would be helpful to discuss how PALEO might generalize to other architectures, such as transformers or graph neural networks, which are increasingly prevalent.
Questions for the Authors:  
1. How does PALEO handle scenarios where the underlying hardware or software stack evolves rapidly (e.g., new GPU architectures or communication protocols)?  
2. Can PALEO be extended to model energy consumption or cost efficiency, which are critical considerations for large-scale training?  
3. How does the model account for non-deterministic factors like job scheduling overhead or asynchronous communication in distributed systems?
In conclusion, PALEO is a significant contribution to the field of scalable deep learning, offering a novel and practical tool for performance modeling. With minor clarifications and expansions, the paper could have even greater impact. I recommend acceptance.