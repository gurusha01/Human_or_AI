The paper presents a novel approach to question classification by leveraging answer data to improve question representation. The main contributions are the introduction of Group Sparse Autoencoders (GSA) and their integration into Convolutional Neural Networks (CNNs) to form Group Sparse Convolutional Neural Networks (GSCNNs). The authors argue that traditional question classification methods overlook the hierarchical and overlapping nature of question categories and fail to utilize the rich semantic information in answer sets. The proposed GSCNN framework addresses these limitations and demonstrates significant performance improvements over strong baselines on four datasets, including real-world applications like insurance and DMV FAQs.
Decision: Accept
Key Reasons for Decision:
1. Novelty and Innovation: The introduction of GSA and its integration into CNNs is a novel contribution that addresses a clear gap in question classification research. The work extends sparse coding techniques into a fully neural framework, enabling end-to-end training and the ability to capture both inter- and intra-group sparsity.
2. Empirical Validation: The proposed method demonstrates significant improvements over baselines on multiple datasets, including challenging real-world datasets like Insurance and DMV FAQs. The experiments are thorough and provide evidence of the model's effectiveness.
Supporting Arguments:
- The paper is well-motivated, highlighting the unique challenges of question classification compared to other sentence classification tasks. The use of answer data to refine question representation is a compelling idea with practical implications.
- The experiments are comprehensive, covering both single-label and multi-label classification tasks, as well as unseen-label scenarios. The results consistently show that GSCNN outperforms traditional CNNs and other baseline methods.
- The visualization of GSA's projection matrix and activations provides intuitive insights into how the model learns grouped representations, further validating the proposed approach.
Additional Feedback for Improvement:
1. Clarity and Accessibility: While the technical depth is commendable, the paper could benefit from clearer explanations of key concepts, especially for readers unfamiliar with sparse coding or autoencoders. For example, the mathematical formulations in Sections 2 and 3 could be accompanied by more intuitive descriptions or diagrams.
2. Dataset Availability: The authors mention that the DMV dataset will be made publicly available in the future. Providing access to this dataset at the time of publication would enhance reproducibility and encourage further research.
3. Ablation Studies: While the paper compares initialization methods for the projection matrix, additional ablation studies could clarify the individual contributions of GSA and CNN components to the overall performance.
4. Limitations: The paper does not explicitly discuss potential limitations, such as the computational overhead introduced by GSA or the scalability of the approach to larger datasets. Acknowledging these would strengthen the paper.
Questions for the Authors:
1. How does the computational complexity of GSCNN compare to traditional CNNs, particularly for large-scale datasets?
2. Could the proposed framework be extended to other NLP tasks, such as sentiment analysis or paraphrase detection? If so, what modifications would be required?
3. How sensitive is the model to hyperparameters like the sparsity parameters (ρ and η) and the number of groups (G)? Have you explored automated tuning methods?
Overall, the paper makes a significant contribution to the field of question classification by introducing a novel framework that effectively leverages answer data. With minor improvements in clarity and additional experiments, it has the potential to be a strong addition to the conference.