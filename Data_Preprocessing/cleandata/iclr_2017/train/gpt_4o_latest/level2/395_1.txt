The paper introduces Edward, a Turing-complete probabilistic programming language (PPL) that integrates probabilistic modeling and inference into computational graph frameworks like TensorFlow. The authors claim that Edward achieves flexibility by enabling compositional inference methods (e.g., variational inference, MCMC, GANs) and efficiency by leveraging TensorFlow's computational graph optimizations, including GPU acceleration. They demonstrate Edward's capabilities through experiments, showing significant speedups compared to existing PPLs (e.g., 35x faster than Stan and 6x faster than PyMC3 on a logistic regression task) and its adaptability to complex inference tasks like variational autoencoders and Bayesian RNNs.
Decision: Accept
The paper should be accepted due to its significant contributions to the field of probabilistic programming, particularly its novel integration of compositional inference with computational graphs, which bridges the gap between expressiveness and efficiency. The claims are well-supported by experiments and comparisons with existing systems.
Supporting Arguments:
1. Novelty and Contribution: Edward introduces a unique design where inference is treated as a first-class citizen, enabling flexible combinations of modeling and inference. This is a significant improvement over traditional PPLs, which often treat inference as a black box. The integration with TensorFlow is particularly innovative, allowing Edward to leverage GPU acceleration and scale to massive datasets.
   
2. Experimental Validation: The paper provides robust experimental evidence to support its claims. For instance, Edward's Hamiltonian Monte Carlo implementation demonstrates substantial speedups over Stan and PyMC3, showcasing its computational efficiency. Additionally, the flexibility of Edward is highlighted through its ability to implement diverse inference methods and novel combinations, such as hierarchical variational models and GAN-based optimization.
3. Practical Usefulness: Edward is positioned as a research platform for developing new probabilistic models and inference algorithms. Its flexibility and efficiency make it highly relevant to both researchers and practitioners in machine learning.
Suggestions for Improvement:
1. Limitations: While the paper acknowledges challenges with complex control flow and recursion, it does not provide concrete solutions or a roadmap for addressing these issues. Including a discussion on potential approaches would strengthen the paper.
   
2. Reproducibility: The paper provides examples and references a companion webpage, but it would benefit from a clearer description of how to reproduce the experiments, particularly the benchmarks against Stan and PyMC3.
3. Comparison with Dynamic Computational Graphs: The paper briefly mentions the trade-offs of static vs. dynamic computational graphs but does not explore this in depth. A more detailed discussion of how Edward compares to dynamic frameworks like PyTorch would be valuable.
Questions for the Authors:
1. How does Edward handle models with complex control flow and recursion, and are there plans to improve support for these use cases?
2. Can you provide more details on the experimental setup for the benchmarks, particularly the hardware configurations and dataset preprocessing?
3. Have you considered integrating Edward with dynamic computational graph frameworks like PyTorch, and what challenges might arise in doing so?
In conclusion, Edward represents a significant advancement in probabilistic programming, combining flexibility, efficiency, and scalability. While there are areas for improvement, the paper's contributions and experimental validation make it a strong candidate for acceptance.