Review of the Paper
Summary of Contributions
This paper introduces a novel hybrid deep learning architecture that combines scattering transforms with convolutional networks (convnets). The scattering transform serves as a fixed, unsupervised initialization for the first layers, while the remaining layers are learned in a supervised manner. The authors demonstrate that this hybrid approach achieves competitive performance on standard benchmarks (CIFAR10, CIFAR100, and STL10), particularly excelling in scenarios with limited data. The paper also highlights the stability and computational efficiency of scattering layers, which are robust to geometric transformations and noise. Additionally, the authors provide a GPU-accelerated implementation of the scattering transform, ScatWave, and analyze the learned invariances in the hybrid architecture. This work contributes to the growing interest in integrating mathematical structure into deep learning to improve generalization and stability.
Decision: Accept
The paper is well-motivated, presents a novel and practically useful contribution, and provides strong empirical evidence to support its claims. The hybrid architecture addresses a relevant problem—training deep networks on small datasets—and offers a computationally efficient solution. The release of ScatWave further enhances the paper's impact.
Supporting Arguments
1. Novelty and Motivation: The hybrid architecture is a significant innovation, combining the theoretical guarantees of scattering transforms with the flexibility of supervised deep learning. This approach is particularly valuable for small datasets, where fully supervised deep networks often overfit.
   
2. Empirical Validation: The authors provide extensive experimental results on CIFAR10, CIFAR100, and STL10, demonstrating competitive performance with state-of-the-art architectures. The hybrid model outperforms purely supervised networks in low-data regimes, showcasing its practical utility.
3. Theoretical Insights: The paper provides a rigorous mathematical foundation for the scattering transform and its stability properties. The analysis of learned invariances in the hybrid architecture adds depth to the work.
4. Reproducibility: The release of ScatWave and detailed experimental setups enhance the reproducibility of the results, a critical aspect of impactful research.
Suggestions for Improvement
1. Comparison with Semi-Supervised Methods: While the paper briefly mentions semi-supervised approaches like GANs, a more detailed comparison would strengthen the argument for the hybrid architecture, especially in low-data scenarios.
2. Scalability to Larger Datasets: The authors acknowledge that the work is preliminary and does not extend to larger datasets like ImageNet. Including a discussion on potential challenges and future directions for scaling the method would be valuable.
3. Ablation Studies: While the paper provides some analysis of the scattering layers, additional ablation studies (e.g., varying the number of scattering layers or comparing different wavelet configurations) could offer deeper insights into the architecture's design choices.
4. Clarity in Presentation: Certain sections, particularly those describing the scattering transform's mathematical properties, are dense and may be difficult for readers unfamiliar with the topic. Simplifying these explanations or providing visual aids could improve accessibility.
Questions for the Authors
1. How does the hybrid architecture perform when fine-tuned on larger datasets like ImageNet? Are there specific challenges in scaling the scattering transform to higher-dimensional data?
2. Could the scattering transform be adapted or learned to better suit specific datasets, rather than relying on fixed wavelets?
3. How does the computational efficiency of the hybrid model compare to state-of-the-art architectures like Wide ResNets in real-world applications?
Conclusion
This paper makes a strong case for hybrid architectures that combine fixed mathematical representations with learned components. Its contributions are novel, well-supported by experiments, and practically relevant, particularly for low-data regimes. With minor improvements in presentation and additional comparisons, this work has the potential to significantly impact the field of deep learning. I recommend acceptance.