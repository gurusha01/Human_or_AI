The paper presents a novel framework for addressing challenges in deep reinforcement learning (RL), particularly for tasks with sparse rewards and long horizons. The authors propose a two-stage approach: first, pre-training a diverse set of skills using Stochastic Neural Networks (SNNs) with an information-theoretic regularizer, and second, leveraging these learned skills in a hierarchical policy to solve downstream tasks. The framework combines intrinsic motivation and hierarchical RL, enabling efficient exploration and skill reuse across tasks. Experimental results demonstrate the effectiveness of the approach in improving sample efficiency and task performance across a range of sparse-reward environments.
Decision: Accept
Key Reasons for Acceptance:
1. Novelty and Contribution: The paper introduces a compelling combination of SNNs, mutual information regularization, and hierarchical RL to address sparse reward challenges. The use of a single proxy reward for skill pre-training with minimal domain knowledge is particularly innovative.
2. Empirical Validation: The experiments convincingly demonstrate the framework's ability to learn diverse, interpretable skills and improve exploration and performance in challenging tasks like mazes and object gathering. The results outperform baselines and prior intrinsic motivation methods.
Supporting Arguments:
- The proposed use of SNNs for skill learning is well-motivated, leveraging their ability to represent multi-modal policies. The inclusion of bilinear integration and mutual information regularization ensures a diverse and interpretable skill set.
- The hierarchical policy structure effectively reduces the sample complexity of downstream tasks by reusing pre-trained skills, as evidenced by improved performance in sparse-reward environments.
- The experiments are thorough, covering multiple tasks and robot morphologies, and include ablation studies to validate the importance of key components like the MI bonus and bilinear integration.
Additional Feedback for Improvement:
1. Clarity: While the methodology is well-detailed, the paper could benefit from a more concise explanation of the experimental setup, particularly in Section 7.1. Simplifying the visitation plot descriptions and focusing on key insights would improve readability.
2. Limitations and Future Work: The paper acknowledges limitations, such as fixed sub-policies and switching times, but could elaborate on potential solutions, especially for unstable agents like the Ant robot. Including preliminary results or insights into end-to-end training or adaptive switching policies would strengthen the discussion.
3. Generalization: While the framework shows promise, its applicability to more dynamic or real-world tasks remains unclear. Future work could explore tasks requiring rapid skill switching or environments with non-stationary dynamics.
Questions for Authors:
1. How sensitive is the framework to the choice of proxy reward in the pre-training phase? Could poorly chosen proxy rewards lead to suboptimal skill sets?
2. Have you considered incorporating recurrent architectures in the Manager Network to enable memory-based decision-making for skill selection?
3. For unstable agents like Ant, would incorporating a recovery mechanism during pre-training improve robustness in downstream tasks?
Overall, the paper makes a significant contribution to hierarchical RL and skill learning, with strong empirical results and well-motivated methodology. Addressing the outlined limitations and questions would further enhance its impact.