The paper proposes a novel generalization of count-based exploration methods for reinforcement learning (RL) in high-dimensional and continuous state spaces by leveraging hashing techniques. The authors demonstrate that this approach, which discretizes states via hash functions and assigns exploration bonuses based on state-visitation counts, achieves near state-of-the-art performance on challenging RL benchmarks, including Atari 2600 games and continuous control tasks. The paper also explores the use of learned hash functions, such as those derived from autoencoders, to improve performance further. Key findings include the importance of hash function granularity and the relevance of encoded information for solving Markov decision processes (MDPs).
Decision: Accept
Key Reasons for Decision:
1. Novelty and Practicality: The paper presents a simple yet effective extension of classical count-based exploration to high-dimensional RL tasks, addressing a significant gap in the field. The use of hashing for generalization is both innovative and computationally efficient.
2. Empirical Validation: The approach is rigorously evaluated on diverse benchmarks, showing competitive or superior performance compared to state-of-the-art methods like VIME and pseudo-count-based exploration. The experiments are thorough and address key questions about the method's effectiveness and design choices.
Supporting Arguments:
- The method is well-motivated, building on classical exploration techniques while adapting them to modern deep RL challenges. The authors provide a clear theoretical foundation and empirical evidence to support their claims.
- The experiments are comprehensive, covering both continuous control and discrete action-space tasks. The results demonstrate the method's robustness across different domains and its ability to handle sparse rewards effectively.
- The analysis of hash function granularity and learned representations is insightful, offering practical guidance for implementing the approach in various settings.
Additional Feedback:
1. Clarity of Presentation: While the methodology is well-explained, the paper could benefit from a more concise description of the experimental setup, particularly in the appendices. A clearer summary of key hyperparameter settings and their impact on performance would improve accessibility.
2. Limitations and Future Work: The paper briefly acknowledges limitations, such as the reliance on uninformed exploration during the initial phase and the challenges of designing domain-specific hash functions. Expanding this discussion and outlining potential solutions (e.g., adaptive hash function learning) would strengthen the paper.
3. Comparison with Related Work: While the paper positions itself well within the literature, a more detailed comparison with pseudo-count-based methods and curiosity-driven exploration strategies would provide additional context for the contributions.
Questions for Authors:
1. How sensitive is the proposed method to the choice of hash function beyond the granularity parameter? For instance, how do different types of locality-sensitive hashing (LSH) functions affect performance?
2. Could the learned hash function approach be extended to incorporate temporal dynamics, such as encoding sequences of states, to improve exploration in tasks with long-term dependencies?
3. How does the method scale with increasing state-space dimensionality, particularly in environments with highly complex observations (e.g., 3D visual inputs)?
Overall, this paper makes a strong contribution to the field of RL exploration, offering a simple yet powerful baseline for high-dimensional tasks. With minor improvements in presentation and a deeper discussion of limitations, it has the potential to inspire further research in this area.