The paper presents a novel approach to reward function design for reinforcement learning (RL) by leveraging intermediate visual representations learned by pre-trained deep models. The authors propose a method to infer dense and smooth perceptual reward functions from a small number of human demonstrations, without requiring explicit sub-goal supervision. The paper claims three main contributions: (1) a method for perceptual reward learning from limited demonstrations, (2) the first vision-based reward learning approach for complex robotic manipulation tasks using human demonstrations, and (3) empirical evidence showing that pre-trained visual features can generalize to new scenes without retraining. The approach is validated through qualitative and quantitative experiments on two real-world tasks (door opening and liquid pouring) and a robotic door-opening task.
Decision: Accept
The paper makes significant contributions to the field of RL and imitation learning by addressing the challenges of reward function design and exploration in real-world tasks. The method is well-motivated, novel, and demonstrates practical utility, particularly in robotic manipulation tasks. The empirical results provide strong support for the claims, and the paper opens up promising avenues for future research.
Supporting Arguments:
1. Novelty and Contribution: The paper introduces a scalable and efficient method for reward learning that does not rely on explicit sub-goal supervision or kinesthetic demonstrations, which is a significant advancement over prior work. The use of pre-trained visual features for reward learning is innovative and demonstrates strong generalization capabilities.
2. Empirical Validation: The experiments are well-designed and demonstrate the effectiveness of the proposed method. The quantitative results, such as the Jaccard similarity measure for step discovery and the success rate of the robotic door-opening task, provide compelling evidence of the method's robustness and practical utility.
3. Practical Relevance: The approach addresses real-world challenges in RL, such as the difficulty of designing reward functions and the need for dense feedback in multi-step tasks. The ability to learn from human demonstrations using only video data is particularly impactful for robotic applications.
Additional Feedback:
1. Clarity and Completeness: While the paper is comprehensive, some sections, such as the description of the segmentation algorithm and feature selection, could benefit from additional clarity. Providing more intuitive explanations or visual aids for these methods would enhance accessibility.
2. Failure Cases: The paper briefly discusses failure cases (e.g., noisy intermediate rewards for door opening and issues with transparent liquid in pouring tasks). A deeper analysis of these cases, along with potential solutions, would strengthen the paper.
3. Scalability: The method relies on pre-trained visual features from a specific deep model (Inception). It would be helpful to discuss the scalability of the approach to other architectures or domains, such as tactile or audio data, as hinted in the paper.
4. Generalization: While the experiments demonstrate generalization to new scenes, additional results on more diverse tasks or environments would further validate the method's robustness.
Questions for the Authors:
1. How sensitive is the method to the choice of the pre-trained model (e.g., Inception vs. other architectures)? Have you tested the approach with different feature extractors?
2. The segmentation algorithm requires the number of steps to be manually specified. Could this be automated, and if so, how would it impact the results?
3. How does the method perform in tasks with higher visual complexity or occlusions? Are there plans to address these challenges in future work?
Overall, the paper makes a strong case for acceptance, offering a novel and practical solution to a critical problem in RL and robotics. With minor improvements in clarity and additional analysis of limitations, the work has the potential to significantly impact the field.