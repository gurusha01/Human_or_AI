Review
This paper investigates the capacity and trainability of various recurrent neural network (RNN) architectures, including vanilla RNNs, LSTMs, GRUs, and two novel architectures (UGRNN and +RNN). The authors present a comprehensive experimental study to disentangle the effects of two capacity bottlenecks: the ability to store task information in parameters and the ability to remember input history in hidden units. They find that all architectures achieve similar per-parameter and per-unit capacity bounds, with approximately 5 bits of task information per parameter and the ability to store one real number per hidden unit. The study also highlights that differences in performance across RNN architectures are primarily due to trainability rather than capacity. The authors propose two new architectures, UGRNN and +RNN, which demonstrate improved trainability in specific scenarios.
Decision: Accept with minor revisions.
Key Reasons:
1. Significance and Novelty: The paper provides valuable insights into the capacity and trainability of RNNs, challenging common assumptions about the superiority of gated architectures. The introduction of two novel architectures (UGRNN and +RNN) adds to the field's understanding of RNN design.
2. Thorough Experimental Validation: The authors employ extensive hyperparameter optimization and a variety of tasks to support their claims, lending credibility to their conclusions.
Supporting Arguments:
- The experimental results are robust and well-supported, with thousands of hyperparameter evaluations performed across multiple architectures, tasks, and depths. The use of mutual information to measure task capacity is particularly compelling.
- The finding that trainability, rather than capacity, drives performance differences between architectures is significant and has practical implications for model selection.
- The introduction of the UGRNN and +RNN architectures is well-motivated, and their performance on deep architectures and hard-to-train tasks demonstrates their potential utility.
Additional Feedback:
1. Clarity of Presentation: While the experiments are thorough, the paper could benefit from clearer organization and more concise explanations, particularly in the experimental setup and results sections. For example, the descriptions of the capacity experiments and their implications could be streamlined.
2. Discussion of Limitations: The paper acknowledges some limitations, such as the potential for suboptimal hyperparameter tuning and the focus on specific tasks. However, a more explicit discussion of how these limitations might affect the generalizability of the findings would strengthen the paper.
3. Comparison with Prior Work: While the related work section is comprehensive, the paper could better contextualize its contributions relative to recent advances in RNN architectures and training techniques.
Questions for the Authors:
1. How do the proposed UGRNN and +RNN architectures perform on real-world tasks beyond the experimental setup, such as language modeling or time-series forecasting?
2. Can the authors provide more details on the computational efficiency of the UGRNN and +RNN architectures compared to LSTMs and GRUs, particularly during inference?
3. Given the finding that capacity differences are minimal, how do the authors envision practitioners balancing trainability and computational cost when selecting RNN architectures?
Overall, this paper makes a strong contribution to the understanding of RNN capacity and trainability, and its findings are likely to be of interest to both researchers and practitioners in the field of deep learning. With minor revisions to improve clarity and address limitations, this paper is well-suited for acceptance.