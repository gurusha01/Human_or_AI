The paper introduces two novel Recurrent Neural Network (RNN)-based architectures—Classifier and Selector—for extractive document summarization. The Classifier processes sentences sequentially in their original order, deciding their inclusion in the summary, while the Selector picks sentences in an arbitrary order. Both architectures jointly model salience, redundancy, and other abstract features, offering interpretability through feature visualization. The models achieve state-of-the-art performance on the Daily Mail dataset and competitive results on the DUC 2002 dataset. The authors also recommend scenarios where each architecture excels, such as Selector's potential for unstructured data. The paper further highlights the interpretability of its models and provides insights into the impact of document structure on summarization performance.
Decision: Accept
Key Reasons for Decision:
1. Novelty and Contribution: The paper proposes two distinct architectures for extractive summarization, offering a significant conceptual and methodological contribution to the field. The Selector framework, in particular, is a novel deep learning approach for extractive summarization.
2. Empirical Validation: The models achieve state-of-the-art results on the Daily Mail dataset and provide a thorough comparative analysis of the two architectures, including their performance under different conditions (e.g., shuffled sentence order).
Supporting Arguments:
- The paper is well-motivated and grounded in the literature, with clear connections to prior work such as Cheng & Lapata (2016). It highlights the advantages and limitations of existing methods and positions its contributions effectively.
- The experimental setup is robust, with evaluations on both in-domain (Daily Mail) and out-of-domain (DUC 2002) datasets. The results are statistically significant and provide actionable insights into the strengths and weaknesses of each architecture.
- The interpretability of the models is a notable strength, as it allows for feature-level analysis and visualization, which is valuable for both researchers and practitioners.
- The discussion on the impact of document structure and the potential of the Selector architecture for less structured tasks (e.g., multi-document summarization) adds depth to the analysis and opens avenues for future research.
Suggestions for Improvement:
1. Ground Truth Generation: The paper relies on a greedy algorithm to convert abstractive summaries into extractive labels, which may introduce noise. Exploring more sophisticated methods, such as Integer Linear Programming (ILP), could improve the quality of training data.
2. Domain Adaptation: The models perform less effectively on the DUC 2002 dataset compared to graph-based approaches. Investigating domain adaptation techniques or fine-tuning on out-of-domain data could enhance generalizability.
3. Ablation Studies: While the paper provides some ablation experiments, a deeper analysis of feature interactions and their relative importance could strengthen the interpretability claims.
4. Inference Optimization: Incorporating beam search or Viterbi decoding during inference could improve performance further, particularly for the Classifier architecture.
Questions for Authors:
1. How does the performance of the Selector architecture compare to the Classifier when applied to multi-document summarization or tweet clustering tasks, as suggested in the discussion?
2. Could the greedy pseudo ground-truth generation method be replaced with a more optimal approach, and what impact would this have on model performance?
3. Have you considered integrating pre-trained language models (e.g., BERT or GPT) into your architectures, and if so, how might this affect the results?
Overall, this paper presents a strong contribution to the field of extractive summarization, with clear novelty, rigorous evaluation, and practical insights. It is well-suited for acceptance at the conference.