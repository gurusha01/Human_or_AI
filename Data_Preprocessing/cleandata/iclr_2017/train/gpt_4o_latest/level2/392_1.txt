The paper proposes a novel approach to optimizing autoencoders for lossy image compression, addressing the challenges posed by the non-differentiability of compression loss. The authors introduce a simple yet effective method for approximating the gradients of non-differentiable operations like rounding and entropy coding, enabling end-to-end training of compressive autoencoders. The resulting model achieves performance comparable to or better than JPEG 2000 in terms of perceptual quality (SSIM and MOS scores) while being computationally efficient and adaptable to diverse compression requirements. The paper also highlights its advantages over existing methods, such as RNN-based approaches, by demonstrating superior performance on high-resolution images and reduced computational complexity.
Decision: Accept
Key Reasons for Decision:
1. Significant Contribution: The paper addresses a critical limitation in training autoencoders for lossy compression and demonstrates a practical solution that achieves state-of-the-art results.
2. Strong Empirical Support: The claims are backed by rigorous experiments, including comparisons with established codecs (JPEG, JPEG 2000) and recent neural network-based methods. The results are statistically significant and scientifically rigorous.
Supporting Arguments:
- The proposed method for handling non-differentiability is both elegant and effective, as demonstrated by its ability to outperform JPEG 2000 in subjective quality evaluations (MOS scores) at medium and high bit rates.
- The use of convolutional neural networks with sub-pixel architectures ensures computational efficiency, making the approach suitable for real-world applications, such as high-resolution image compression on low-powered devices.
- The incremental training strategy and fine-tuning for variable bit rates are practical innovations that enhance the flexibility and usability of the proposed framework.
Additional Feedback for Improvement:
1. Clarity in Comparisons: While the paper provides detailed comparisons with JPEG 2000 and RNN-based methods, it would benefit from a more explicit discussion of the computational trade-offs (e.g., encoding/decoding speed) relative to traditional codecs.
2. Broader Evaluation Metrics: The paper primarily focuses on SSIM, MS-SSIM, and MOS scores. Including additional perceptual metrics or task-specific evaluations (e.g., compression for medical imaging or video) could strengthen the generalizability of the results.
3. Reproducibility: While the implementation details are thorough, providing a public code repository or pretrained models would significantly enhance the reproducibility of the work.
4. Limitations: The paper briefly mentions the dependency on perceptual metrics but does not fully explore the limitations of the proposed approach, such as potential performance degradation for non-natural images or extremely low bit rates.
Questions for the Authors:
1. How does the computational efficiency of the proposed method compare to JPEG 2000 and other neural network-based approaches in terms of encoding and decoding times?
2. Have you evaluated the robustness of the proposed method on diverse datasets, such as medical or satellite images, which may have different statistical properties from natural images?
3. Could the proposed gradient approximation method be extended to other non-differentiable tasks in image processing, such as super-resolution or denoising?
Overall, the paper presents a compelling contribution to the field of neural network-based image compression. It is well-motivated, scientifically rigorous, and practically relevant, making it a strong candidate for acceptance at the conference.