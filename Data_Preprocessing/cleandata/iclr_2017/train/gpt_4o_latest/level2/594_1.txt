The paper proposes novel low-dimensional parameterizations for "Passthrough Networks" using low-rank and low-rank plus diagonal matrix decompositions. These parameterizations aim to reduce the number of trainable parameters while maintaining the memory capacity of the network. The authors demonstrate the effectiveness of their approach through competitive experimental results on tasks such as sequential randomly-permuted MNIST classification, memory tasks, and language modeling. The work highlights the potential of these parameterizations to address the vanishing gradient problem and improve computational efficiency.
Decision: Accept
The paper presents a meaningful contribution to the field of deep learning, particularly in the design of efficient neural network architectures. The key reasons for acceptance are the novelty of the proposed parameterizations and the competitive experimental results that validate their utility.
Supporting Arguments:
1. Novelty and Innovation: The paper introduces low-rank and low-rank plus diagonal parameterizations for Passthrough Networks, which represent a significant improvement over traditional fully connected architectures. The approach is well-motivated and addresses the trade-off between representation power and parameter efficiency, a critical challenge in deep learning.
   
2. Experimental Validation: The authors provide extensive empirical evidence to support their claims. For example, the low-rank plus diagonal GRU achieves near state-of-the-art performance on the sequential permuted MNIST task and competitive results on memory and addition tasks. These results demonstrate the practical utility of the proposed methods.
3. Scientific Rigor: The experiments are thorough, with comparisons to relevant baselines such as uRNNs and standard GRUs. The authors also address numerical stability issues and explore the effects of hyperparameters, showing a deep understanding of their approach.
Suggestions for Improvement:
1. Clarity and Accessibility: While the technical content is strong, the paper could benefit from clearer explanations in some sections. For example, the mathematical formulations of the low-rank and low-rank plus diagonal parameterizations could be simplified or accompanied by intuitive diagrams to aid understanding.
2. Broader Comparisons: The paper primarily compares its methods to uRNNs and standard GRUs. Including comparisons to other recent architectures, such as recurrent networks with attention mechanisms or architectures with recurrent batch normalization, would provide a more comprehensive evaluation.
3. Ablation Studies: While the authors demonstrate the utility of the diagonal component in low-rank plus diagonal parameterizations, a more detailed ablation study isolating its impact across different tasks would strengthen the claims.
4. Reproducibility: Although the authors provide code, the paper could include more details on hyperparameter settings and training procedures for all experiments to ensure reproducibility.
Questions for the Authors:
1. How does the proposed low-rank plus diagonal parameterization generalize to tasks with higher-dimensional inputs, such as video or 3D data? Are there scalability concerns?
2. Could the proposed parameterizations be combined with attention mechanisms or other architectural innovations to further enhance performance?
3. What are the implications of using these parameterizations in real-world applications with strict memory and computational constraints, such as edge devices?
Conclusion:
The paper makes a valuable contribution to the field by proposing efficient parameterizations for Passthrough Networks and validating their effectiveness through rigorous experiments. While there is room for improvement in clarity and broader comparisons, the novelty and practical utility of the work justify its acceptance.