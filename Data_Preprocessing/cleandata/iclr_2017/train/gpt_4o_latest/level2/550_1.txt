The paper proposes a novel modification to the objective function of Denoising Auto-Encoders (DAEs) to ensure that the denoising process primarily occurs during the encoding phase, rather than being deferred to the decoding phase. The authors argue that this modification leads to more robust representations and demonstrate its effectiveness through theoretical analysis and empirical results on synthetic datasets and MNIST. The modified objective introduces an additional term penalizing the difference between the encoded representations of clean and corrupted inputs, thereby explicitly enforcing robustness in the encoding phase. The paper also provides insights into the relationship between the proposed approach and other methods, such as contractive auto-encoders, and discusses architectural considerations for avoiding trivial solutions.
Decision: Accept
The key reasons for this decision are the novelty of the proposed modification and its potential impact on improving the robustness of DAEs. The paper provides a clear motivation for the work, a well-defined objective, and experimental results that support the claims. Additionally, the modification is simple yet effective, making it a practical contribution to the field of unsupervised learning.
Supporting Arguments:
1. Novelty and Motivation: The proposed modification addresses a known limitation of DAEs, where the denoising process may occur predominantly in the decoding phase. By explicitly enforcing robustness in the encoding phase, the paper introduces a meaningful improvement over the standard DAE objective.
2. Experimental Validation: The experiments on synthetic datasets and MNIST demonstrate the effectiveness of the modified objective in learning robust representations. The results, particularly the energy landscapes and test error trends, provide strong evidence that the proposed approach achieves its intended goals.
3. Theoretical Insights: The authors provide a solid theoretical foundation for the modified objective, including its interpretation as minimizing an upper bound on the conditional entropy of the code given the inputs. This adds depth to the contribution and situates it within the broader context of representation learning.
Additional Feedback:
1. Clarity of Experiments: While the experiments are compelling, the paper could benefit from additional quantitative comparisons with other approaches, such as contractive auto-encoders or variational auto-encoders, to further highlight the advantages of the proposed method.
2. Architectural Considerations: The discussion on avoiding trivial solutions (e.g., weight shrinkage) is insightful but could be expanded with more concrete examples or guidelines for practitioners.
3. Scalability: The paper mentions the potential scalability of the approach but does not provide experiments on larger datasets or deeper architectures. Including such results would strengthen the claim of scalability.
Questions for the Authors:
1. How does the proposed method perform when applied to deeper architectures or more complex datasets beyond MNIST? Are there any challenges in scaling the approach?
2. Could the modified objective be combined with other regularization techniques, such as sparsity constraints or Jacobian penalties, to further enhance robustness?
3. How sensitive is the performance to the choice of the tradeoff parameter λ? Would an adaptive or learned λ improve the results?
Overall, the paper presents a well-motivated and impactful contribution to the field of unsupervised learning, and I recommend its acceptance with minor revisions to address the above feedback.