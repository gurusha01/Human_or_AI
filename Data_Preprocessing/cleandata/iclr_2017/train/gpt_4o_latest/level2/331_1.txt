The paper presents a novel approach for transferring skills between morphologically different agents using invariant feature spaces, with a focus on reinforcement learning in robotics. The authors propose a method where two agents, such as robots with differing morphologies, can learn a shared feature space from a proxy task they both perform. This shared space enables the transfer of new skills from one agent to another, even when their state and action spaces differ significantly. The approach is validated through experiments involving simulated robotic arms with varying numbers of links and actuation mechanisms, demonstrating significant improvements in learning efficiency and task success rates.
Decision: Accept
Key reasons for this decision are the paper's strong novelty in addressing transfer learning across morphologically distinct agents and its rigorous experimental validation. The proposed method introduces a meaningful innovation in the field of reinforcement learning and robotics, with practical implications for multi-agent systems and lifelong learning scenarios.
Supporting Arguments:
1. Novelty and Contribution: The paper addresses a challenging and underexplored problem of transferring skills between agents with different morphologies. The introduction of invariant feature spaces as a mechanism for transfer is a significant advancement over prior methods, which often rely on direct state-space mappings or hand-designed features.
2. Experimental Validation: The authors provide thorough experimental results, demonstrating the effectiveness of their method in transferring skills between robots with different numbers of links and actuation mechanisms. The comparison with baseline methods, including CCA, UMA, and direct mapping, highlights the superiority of the proposed approach.
3. Practical Usefulness: The method has clear practical applications, particularly in robotics, where agents with varying designs must share knowledge. The ability to transfer skills from torque-driven to tendon-driven robots, and even using image-based inputs, underscores the method's versatility.
Additional Feedback:
1. Clarity of Presentation: While the paper is well-written overall, the explanation of the EM-style alignment procedure (Section 3.3.2) could be expanded for better clarity. A more detailed discussion of how dynamic time warping interacts with the learned feature space would strengthen this section.
2. Limitations and Future Work: The paper briefly mentions potential limitations, such as imperfect pairwise mappings and the need for proxy tasks. A more explicit discussion of these constraints and their impact on real-world applications would be beneficial. Additionally, the authors could elaborate on how their method scales to scenarios with more than two agents or tasks.
3. Broader Impact: The paper could benefit from a discussion on the broader implications of the proposed method, such as its potential use in human-robot interaction or transfer learning in non-robotic domains.
Questions for the Authors:
1. How sensitive is the method to the choice of proxy tasks? Would the performance degrade significantly if the proxy task is only loosely related to the target task?
2. Have you considered extending the method to handle transfer between more than two agents simultaneously? If so, what challenges do you anticipate?
3. Could the proposed approach be adapted to handle real-world noise and uncertainty in sensor data, particularly in the image-based transfer experiments?
In conclusion, the paper makes a significant contribution to the field of transfer learning and robotics, with a novel approach that is both theoretically sound and empirically validated. While there are areas for further exploration and refinement, the work is well-suited for acceptance at the conference.