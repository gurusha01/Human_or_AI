Review of the Paper
The paper introduces "wild variational inference," a novel approach to variational inference that removes the requirement for tractable density functions in inference networks. This innovation allows for more flexible and adaptive designs of variational inference methods, particularly in challenging cases where traditional methods struggle. The authors propose two methods for wild variational inference: one based on amortized Stein Variational Gradient Descent (SVGD) and another on minimizing Kernelized Stein Discrepancy (KSD). The paper demonstrates the application of these methods to adaptively adjust step sizes in stochastic gradient Langevin dynamics (SGLD), showing significant improvements over hand-designed step size schemes. Empirical results on Gaussian mixture models and Bayesian logistic regression highlight the effectiveness of the proposed methods.
Decision: Accept
Key reasons for acceptance:
1. Novelty and Contribution: The concept of wild variational inference is a significant departure from traditional variational inference methods, addressing a key limitation of requiring tractable density functions. This innovation has the potential to broaden the applicability of variational inference in machine learning.
2. Empirical Validation: The paper provides thorough experimental results, demonstrating the practical utility of the proposed methods. The adaptive step size adjustment for SGLD is a clear and impactful application, with results showing substantial improvements over existing baselines.
Supporting Arguments
1. Well-Motivated Approach: The paper is well-placed in the literature, building on recent advancements in variational inference, Stein's method, and kernel techniques. The authors clearly articulate the limitations of existing methods and how their approach addresses these challenges.
2. Scientific Rigor: The theoretical foundations of the proposed methods are solid, leveraging Stein's identity and kernelized Stein discrepancy. The empirical results are statistically significant, with comparisons against multiple baselines and detailed analysis of performance.
3. Practical Usefulness: The adaptive step size adjustment for SGLD is a practical contribution that can be directly applied to real-world problems. The general Langevin inference network further demonstrates the flexibility of the proposed methods.
Additional Feedback
1. Clarity and Accessibility: While the paper is technically sound, it could benefit from improved clarity in certain sections. For instance, the derivation of the KSD-based method and its connection to contrastive divergence could be explained more intuitively for a broader audience.
2. Limitations: The paper does not explicitly discuss the computational overhead of the proposed methods compared to traditional approaches. Including a discussion on scalability and runtime performance would strengthen the paper.
3. Broader Applications: While the focus on SGLD is compelling, the paper could explore additional applications of wild variational inference to showcase its versatility further. For example, applying the methods to other MCMC algorithms or deep generative models would be valuable.
Questions for the Authors
1. How does the computational cost of wild variational inference compare to traditional variational inference methods, particularly in high-dimensional settings?
2. Can the proposed methods handle non-smooth target distributions, and if so, how does their performance compare to existing techniques?
3. Have you explored the sensitivity of the proposed methods to the choice of kernel in the KSD-based approach? How robust are the results to different kernel choices?
Overall, this paper presents a novel and impactful contribution to the field of variational inference, with strong theoretical foundations and promising empirical results. Addressing the feedback and questions raised could further enhance its clarity and impact.