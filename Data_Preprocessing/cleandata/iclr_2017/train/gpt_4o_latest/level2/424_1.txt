Review of the Paper: "Mollifying Objective Functions for Neural Network Optimization"
Summary of Contributions
This paper addresses the challenge of optimizing highly non-convex neural network objectives by introducing a novel approach inspired by continuation methods and curriculum learning. The proposed method begins with a smoothed (mollified) objective function that evolves in complexity during training, enabling better optimization of deep neural networks. The authors introduce a single hyperparameter to control the degree of smoothing and demonstrate its effectiveness through experiments on various tasks, including MNIST, CIFAR-10, and language modeling. The paper also establishes connections between mollification, noise injection, and recent advances like residual connections and batch normalization. The experimental results show that the proposed method improves optimization and generalization, particularly in difficult-to-train models.
Decision: Accept
The paper presents a novel and well-motivated approach to a significant problem in deep learning. The method is theoretically grounded, experimentally validated, and demonstrates improvements over existing techniques. However, some areas could benefit from additional clarity and further exploration.
Supporting Arguments
1. Novelty and Contribution: The idea of mollifying objective functions to simplify optimization is novel and well-explained. The connection to continuation methods and curriculum learning is insightful, and the paper extends these ideas in a meaningful way.
2. Experimental Validation: The authors provide comprehensive experiments across diverse tasks, including MNIST, CIFAR-10, and language modeling. The results consistently show that the mollification approach improves optimization and generalization, particularly in challenging settings like deep MLPs and LSTMs.
3. Theoretical Rigor: The paper includes a detailed theoretical framework for mollification, including derivations of gradients and the use of Gaussian mollifiers. This adds credibility to the proposed method and its underlying principles.
4. Practical Usefulness: The method is practically applicable and could be adopted by researchers and practitioners to improve the training of deep neural networks. The authors also provide pseudo-code, making it easier to implement.
Areas for Improvement and Questions for the Authors
1. Clarity in Derivations: While the theoretical sections are thorough, some derivations (e.g., in Section 2.4 and Appendix B) are dense and may be challenging for readers unfamiliar with mollifiers. Simplifying these explanations or providing more intuitive interpretations would improve accessibility.
2. Annealing Schedules: The paper explores different annealing schedules but does not provide a detailed analysis of why exponential decay outperforms others. Could the authors elaborate on the intuition behind this behavior?
3. Comparison with Related Work: While the paper draws connections to curriculum learning and noise injection, a more detailed comparison with these methods (e.g., quantitative results) would strengthen the claims.
4. Limitations: The paper does not explicitly discuss the limitations of the proposed method. For example, does the mollification approach introduce significant computational overhead? Are there specific scenarios where it might fail or be less effective?
5. Code Availability: The authors mention plans to release the code but do not provide a timeline. Making the code available would enhance reproducibility and encourage adoption.
Additional Feedback
- The experiments on CIFAR-10 and language modeling are promising, but it would be interesting to see results on larger-scale datasets like ImageNet or more complex NLP tasks.
- The connection between mollification and generalization (e.g., flatter minima) is intriguing. Could the authors provide more empirical evidence or theoretical insights into this relationship?
Questions for the Authors
1. How sensitive is the method to the choice of the initial smoothing parameter and its annealing schedule? Is there a systematic way to tune these hyperparameters?
2. Could the mollification approach be combined with other optimization techniques, such as adaptive optimizers (e.g., Adam) or second-order methods?
3. How does the method perform in scenarios with limited data or high noise levels in the training data?
In conclusion, this paper makes a significant contribution to the field of neural network optimization. While there are areas for improvement, the novelty, theoretical grounding, and experimental results justify its acceptance. The proposed mollification approach has the potential to inspire further research and practical applications.