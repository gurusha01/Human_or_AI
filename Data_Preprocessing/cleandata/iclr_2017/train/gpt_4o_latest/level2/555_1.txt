Review of the Paper
Summary of Contributions
This paper addresses the challenge of designing effective deep neural network (DNN) architectures for new domains, particularly when prior knowledge is limited. The authors propose a meta-learning approach to rank DNN architectures based on their predicted performance using features derived from topology and early training dynamics. The paper makes three primary contributions: (1) a systematic evaluation of DNN architectures across multiple tabular datasets, (2) an exploration of parallel-layer architectures and their effectiveness compared to linear architectures, and (3) a novel meta-learning-based ranking method that leverages topological and training-based features. The study provides preliminary yet promising results and highlights the limitations of the current approach, such as the use of fixed hyperparameters and the exclusion of CNNs and RNNs.
Decision: Accept
The paper is well-motivated, presents a novel approach to a significant problem, and provides promising preliminary results. However, further experimentation and refinement are needed to strengthen the claims. The decision to accept is based on the novelty of the meta-learning-based ranking method and the systematic exploration of parallel-layer architectures, which could inspire further research in this area.
Supporting Arguments
1. Novelty and Contribution: The meta-learning approach to rank DNN architectures based on early training dynamics is innovative and addresses a critical bottleneck in neural architecture search. The use of parallel-layer architectures and their systematic evaluation is another notable contribution.
2. Practical Usefulness: The proposed method has the potential to save computational resources by identifying high-performing architectures early in the training process. This is particularly valuable for practitioners working with limited resources.
3. Experimental Rigor: The authors evaluate 11,170 architectures across 13 diverse tabular datasets, providing a robust experimental setup. The leave-one-out cross-validation approach for the meta-learning model further strengthens the validity of the results.
Suggestions for Improvement
1. Hyperparameter Exploration: The use of fixed hyperparameters limits the generalizability of the results. Future work should incorporate automatic hyperparameter tuning to explore a broader architecture space.
2. Broader Dataset Evaluation: While the focus on tabular datasets is appreciated, extending the evaluation to other domains (e.g., image or text data) would strengthen the paper's claims about the generalizability of the proposed method.
3. Component Analysis: The paper notes that biases-based meta-features are rarely used by the ranking model. Further analysis of why these features are less informative could provide insights for refining the meta-feature set.
4. Reproducibility: While the paper provides a detailed description of the experimental setup, releasing the code and datasets would enhance reproducibility and encourage follow-up work.
Questions for the Authors
1. How sensitive is the meta-learning model to the choice of training-based meta-features? Could the inclusion of additional features (e.g., gradient-based metrics) improve the ranking performance?
2. Have you considered the impact of different optimization algorithms (e.g., Adam vs. SGD) on the training-based meta-features and their effectiveness in the ranking model?
3. Could the proposed method be extended to incorporate CNNs and RNNs, given their widespread use in domains like image and sequence data?
Conclusion
This paper presents a promising first step toward efficient DNN architecture search using meta-learning. While there are limitations in the current approach, the novelty and potential impact of the proposed method make it a valuable contribution to the field. Addressing the suggested improvements in future work could significantly enhance the paper's impact.