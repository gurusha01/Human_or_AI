The paper, Steerable Convolutional Neural Networks, introduces a novel framework for designing convolutional neural networks (CNNs) that are equivariant to transformations, enabling efficient parameter sharing and improved performance on vision tasks. The authors claim three primary contributions: (1) a mathematical theory of steerable representations that decomposes features into elementary types associated with specific symmetries, (2) a method to construct steerable CNNs that decouple computational cost from the size of the transformation group, and (3) empirical evidence that steerable CNNs achieve state-of-the-art results on CIFAR-10 and CIFAR-100 benchmarks, particularly in low-data regimes.
Decision: Accept.  
The paper makes a significant theoretical and practical contribution to the field of equivariant deep learning. Its mathematical rigor, combined with empirical validation, demonstrates both novelty and utility. The primary reasons for acceptance are the innovative theoretical framework and its demonstrated ability to outperform existing methods in challenging scenarios.
Supporting Arguments:  
1. Novelty and Theoretical Rigor: The paper extends the concept of equivariant CNNs by introducing a type system for steerable representations, grounded in group representation theory. This is a meaningful advancement over prior work, such as group-equivariant CNNs (G-CNNs), by generalizing beyond regular representations and enabling the design of more efficient architectures. The mathematical foundation is well-articulated and supported by references to established representation theory.  
2. Empirical Validation: The experiments convincingly demonstrate the utility of steerable CNNs. The model achieves state-of-the-art results on CIFAR-10 and CIFAR-100, particularly excelling in low-data regimes where statistical efficiency is critical. The comparison with baseline ResNets and other equivariant architectures is thorough and fair.  
3. Practical Impact: The proposed framework is not only theoretically sound but also practical. The authors provide guidelines for using steerable CNNs, including capsule selection and parameterization, making the approach accessible to practitioners.
Additional Feedback for Improvement:  
1. Clarity of Mathematical Exposition: While the theoretical framework is robust, some sections (e.g., induction and character theory) are dense and may be challenging for readers unfamiliar with representation theory. Simplifying or providing more intuitive explanations would improve accessibility.  
2. Comparison with Other Methods: While the paper references related work, a more detailed comparison of computational efficiency and parameter utilization with state-of-the-art methods (e.g., Wide ResNets, DenseNets) would strengthen the empirical claims.  
3. Broader Applicability: The paper focuses on discrete groups (e.g., rotations and reflections). Extending the discussion to continuous groups and potential applications in other domains (e.g., robotics, medical imaging) would highlight the broader impact of the work.
Questions for the Authors:  
1. How does the computational overhead of steerable CNNs compare to standard CNNs and G-CNNs in terms of training time and memory usage?  
2. Could the proposed framework be extended to other types of transformations, such as scaling or non-rigid deformations?  
3. What are the limitations of steerable CNNs in tasks beyond image classification, such as segmentation or object detection?  
Overall, this paper represents a significant advancement in the field of equivariant deep learning, combining theoretical insights with practical utility. The suggested improvements are minor and do not detract from the paper's core contributions.