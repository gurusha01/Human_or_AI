The paper presents a novel model architecture, RASOR (Recurrent Span Representations), for extractive question answering on the SQuAD dataset. The authors aim to improve upon existing methods by explicitly representing and scoring all possible answer spans in a passage, rather than relying on greedy decoding or independent predictions of start and end markers. The paper claims that this approach leads to significant performance gains, achieving a 5% improvement in exact match and a 3.6% improvement in F1 over the previous best model by Wang & Jiang (2016). The authors also highlight the efficiency of their architecture, which reuses recurrent computations to reduce the computational complexity of span enumeration.
Decision: Accept
The primary reasons for this decision are the paper's strong empirical results and its novel contribution to the field of extractive question answering. The proposed RASOR model demonstrates a clear improvement over state-of-the-art methods, and the authors provide a thorough analysis of its components and performance. Additionally, the paper is well-motivated, addressing a key limitation of existing models (greedy decoding) and proposing a principled solution.
Supporting Arguments:
1. Claims and Support: The paper's main claims are well-supported by experiments on the SQuAD dataset. The authors provide quantitative comparisons to prior work, ablation studies to evaluate the importance of different components, and qualitative analyses of common error cases. The results are statistically significant and demonstrate the robustness of the proposed approach.
2. Novelty and Usefulness: The idea of explicitly representing all answer spans and using global normalization during training is a significant innovation. This approach addresses a key challenge in extractive question answering and has the potential to influence future research in the field.
3. Field Knowledge and Completeness: The paper demonstrates a strong understanding of the relevant literature, situating its contributions within the context of prior work. The methodology is described in sufficient detail to allow for reproducibility, and the authors provide insights into the architectural choices and hyperparameter tuning.
Additional Feedback:
1. Limitations: While the paper acknowledges some limitations, such as the inability to evaluate on the SQuAD test set due to copyright restrictions, it would benefit from a more explicit discussion of potential drawbacks, such as scalability to longer passages or datasets with different characteristics.
2. Clarity: The paper is generally well-written, but some sections, such as the detailed equations in the model description, could be simplified or supplemented with more intuitive explanations for readers less familiar with the technical details.
3. Future Work: The authors mention plans to explore alternate architectures for input encoding. It would be helpful to elaborate on specific directions, such as adapting the model to handle multi-document contexts or incorporating external knowledge sources.
Questions for the Authors:
1. How does the model perform on longer passages or datasets with more complex answer types (e.g., multi-sentence answers)?
2. Can the proposed architecture be adapted to handle non-extractive question answering tasks, where the answer is not a span from the passage?
3. What are the computational trade-offs of the RASOR model compared to other state-of-the-art methods, particularly in terms of training time and memory usage?
Overall, this paper makes a strong contribution to the field of natural language understanding and is a valuable addition to the conference.