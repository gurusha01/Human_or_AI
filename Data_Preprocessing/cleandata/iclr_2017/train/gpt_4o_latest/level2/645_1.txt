Review of "Deep Generalized Canonical Correlation Analysis (DGCCA)"
This paper introduces Deep Generalized Canonical Correlation Analysis (DGCCA), a novel method for multiview representation learning that combines the flexibility of nonlinear transformations with the ability to incorporate information from arbitrarily many data views. The authors present a rigorous derivation of the DGCCA optimization problem, propose an efficient stochastic gradient descent algorithm, and demonstrate the utility of DGCCA on synthetic data, phoneme classification, and Twitter hashtag/friend recommendation tasks. The results indicate that DGCCA outperforms existing methods like linear GCCA and two-view DCCA in several scenarios, particularly when leveraging multiple data views.
Decision: Accept
The paper is well-motivated, demonstrates strong empirical results, and presents a significant methodological contribution to the field of multiview representation learning. The key reasons for this decision are:
1. Novelty: DGCCA is the first method to extend nonlinear CCA to more than two views, addressing a critical limitation of prior work.
2. Empirical Validation: The paper provides compelling evidence of DGCCA's superiority over existing methods across diverse datasets and tasks, including up to 4% accuracy improvement in phoneme classification and significant gains in hashtag recommendation recall.
Supporting Arguments:
1. Claims and Support: The authors clearly articulate their contributions, including the derivation of the DGCCA gradient and the development of a scalable optimization algorithm. These claims are supported by theoretical analysis and extensive experiments. The synthetic data experiment effectively illustrates DGCCA's ability to preserve generative structure, while the real-world tasks demonstrate its practical utility.
2. Usefulness: DGCCA has broad applicability in domains where multiview data is prevalent, such as speech processing and social media analysis. The release of the implementation further enhances its accessibility to the research community.
3. Positioning in Literature: The paper situates DGCCA well within the context of prior work, including CCA, DCCA, and GCCA, and highlights its advantages over both linear and nonlinear alternatives.
4. Reproducibility: The authors provide sufficient implementation details, including pseudocode and hyperparameter settings, to enable reproducibility.
Suggestions for Improvement:
1. Clarity of Presentation: While the paper is technically sound, the mathematical derivations (e.g., gradient computation) are dense and could benefit from additional explanation or illustrative diagrams to improve accessibility for a broader audience.
2. Limitations: The paper does not explicitly discuss the limitations of DGCCA, such as potential sensitivity to hyperparameter tuning or computational overhead compared to linear methods. Including such a discussion would provide a more balanced perspective.
3. Ablation Studies: While the experiments are comprehensive, an ablation study isolating the impact of different components (e.g., network architecture, number of views) would strengthen the empirical analysis.
4. Downstream Task Generalization: The paper focuses on specific tasks (e.g., phoneme classification, hashtag recommendation). It would be helpful to explore DGCCA's performance on additional domains or tasks to further validate its generalizability.
Questions for the Authors:
1. How does DGCCA handle cases where some views are noisy or less informative? Is there a mechanism to downweight such views during training?
2. Can the authors provide more insights into the computational complexity of DGCCA compared to linear GCCA and DCCA, particularly for large-scale datasets?
3. How sensitive is DGCCA to the choice of network architecture and hyperparameters? Are there guidelines for selecting these parameters for new datasets?
In conclusion, this paper makes a significant contribution to multiview representation learning and is well-suited for acceptance at the conference. Addressing the suggested improvements would further enhance the clarity and impact of the work.