The paper introduces dynamic batching, a novel technique to enable efficient batching of operations for neural networks with dynamic computation graphs (DCGs), and presents a high-level library, TensorFlow Fold, to simplify the implementation of such models. The authors claim that dynamic batching allows for significant speedups in training and inference, even for graphs of varying shapes and sizes, by leveraging existing static data-flow graph libraries like TensorFlow. Additionally, the TensorFlow Fold library provides compositional blocks that facilitate the rapid prototyping of DCG-based models. The paper demonstrates the utility of these contributions through empirical benchmarks and re-implementation of models from the literature.
Decision: Accept
Key reasons: (1) The paper addresses a critical bottleneck in the adoption of DCGs by proposing a well-motivated and practical solution, and (2) the experimental results convincingly support the claims of improved efficiency and usability.
Supporting Arguments:
1. Problem Tackled and Motivation: The paper identifies the inefficiency of batching operations in DCGs as a major limitation in their adoption, despite their applicability in domains like NLP and cheminformatics. The motivation is well-placed in the literature, with references to prior work that highlights the challenges of DCGs and the limitations of existing libraries.
   
2. Support for Claims: The authors provide rigorous experimental results comparing dynamic batching to manual batching in TensorFlow. The benchmarks demonstrate substantial speedups (up to 120x on GPUs) and validate the claim that dynamic batching incurs minimal overhead even for graphs of varying shapes. Additionally, the re-implementation of Tree-LSTMs and graph convolution models in TensorFlow Fold showcases the library's practicality and ease of use.
3. Novelty and Usefulness: Dynamic batching is a significant innovation that generalizes batching for DCGs, a problem not fully addressed by prior work like SPINN. The TensorFlow Fold library further enhances the paper's impact by providing a user-friendly tool for practitioners, enabling faster experimentation and iteration.
4. Completeness: The paper provides sufficient details about the dynamic batching algorithm, its implementation, and the library's design. The inclusion of code examples and comparisons to baseline implementations adds clarity and reproducibility.
Additional Feedback:
1. Limitations: While the paper briefly mentions the overhead of concat and gather operations, a more detailed discussion of scenarios where dynamic batching might underperform (e.g., extremely large batch sizes) would strengthen the paper.
   
2. Broader Impact: The paper could benefit from a discussion on the potential limitations of TensorFlow Fold's adoption, such as compatibility with other frameworks or scalability to very large datasets.
3. Clarity: The combinator library section is dense and could be made more accessible by including a simplified example or a visual diagram to illustrate the workflow.
Questions for Authors:
1. How does dynamic batching compare to other batching techniques, such as SPINN, in terms of memory usage and scalability for very large graphs?
2. Are there any plans to extend TensorFlow Fold to support other deep learning frameworks like PyTorch or JAX?
3. Could you provide more details on the trade-offs between the added overhead of concat/gather operations and the benefits of within-tree batching for larger batch sizes?
In conclusion, the paper presents a well-motivated and impactful contribution to the field, addressing a key limitation in DCGs and providing a practical tool for researchers and practitioners. With minor clarifications and additional discussion of limitations, the paper will be a strong addition to the conference.