The paper presents a compelling exploration of customized numeric precision for deep neural network (DNN) inference, focusing on its implications for computational efficiency and accuracy. The authors propose a novel methodology to navigate the design space of customized precision hardware, demonstrating significant performance improvements with minimal accuracy loss. The key contributions include (1) a comprehensive evaluation of precision trade-offs on production-grade DNNs, (2) evidence that floating-point representations outperform fixed-point representations for large-scale networks, and (3) a novel search technique to efficiently identify optimal precision configurations. The results show an average speedup of 7.6× with less than 1% accuracy degradation, a significant improvement over the baseline.
Decision: Accept.  
The paper makes a strong case for acceptance due to its well-motivated problem, rigorous methodology, and impactful findings. The novelty of exploring customized precision on large-scale DNNs and the practical utility of the proposed search technique are particularly noteworthy.
Supporting Arguments:  
1. Well-Motivated Problem and Novelty: The paper addresses a critical challenge in DNN hardware design—balancing computational efficiency and accuracy through customized numeric precision. Unlike prior work, which focused on small-scale networks, this study evaluates production-grade DNNs, providing insights that are directly applicable to real-world scenarios. The novelty of the search technique, which leverages last-layer activations for efficient precision selection, is a valuable contribution to the field.  
2. Scientific Rigor: The authors employ a robust experimental methodology, combining theoretical analysis, hardware simulations, and empirical evaluations. The results are statistically significant and demonstrate a clear trade-off between precision and efficiency. The use of industry-standard tools for hardware design adds credibility to the findings.  
3. Practical Utility: The proposed approach is highly relevant for hardware designers, offering actionable insights and a practical method to optimize DNN accelerators. The reported speedups and energy savings are substantial, making this work a valuable resource for the AI hardware community.
Additional Feedback:  
1. Clarity and Accessibility: While the technical depth is commendable, certain sections, such as the mathematical formulations of numeric representations, could benefit from additional explanations or visual aids to improve accessibility for a broader audience.  
2. Limitations and Future Work: The paper could more explicitly discuss the limitations of the proposed approach, such as potential challenges in adapting the search technique to other DNN architectures or tasks beyond image classification. Additionally, exploring multi-precision configurations, despite the noted challenges, could be an interesting avenue for future work.  
3. Reproducibility: While the methodology is detailed, providing open-source implementations of the modified Caffe framework and hardware design scripts would enhance reproducibility and encourage adoption by the community.
Questions for Authors:  
1. How does the proposed search technique generalize to other types of neural networks, such as recurrent or transformer-based models?  
2. Have you considered the impact of customized precision on training, or is the approach strictly limited to inference?  
3. Could the search technique be extended to dynamically adjust precision during runtime based on workload characteristics?
Overall, this paper makes a significant contribution to the field of AI hardware design and is well-suited for presentation at the conference. With minor clarifications and additional discussion of limitations, the work will be even stronger.