Review of the Paper
The paper explores the use of annealed Gaussian noise injection into gradients as a simple yet effective optimization technique for training complex neural network architectures. The authors claim that this method improves optimization robustness, helps escape poor initializations, and enhances generalization across a variety of deep learning tasks, including question answering and algorithm learning. The paper demonstrates these claims through extensive experiments on models such as deep fully-connected networks, End-To-End Memory Networks, Neural Programmer, Neural Random Access Machines, and Neural GPUs.
Decision: Accept  
The paper presents a novel and practically useful contribution to the field of neural network optimization. The key reasons for this decision are:  
1. Novelty and Practicality: The proposed technique of gradient noise injection is simple to implement (a single line of code) and shows consistent improvements across a variety of challenging tasks and architectures.  
2. Strong Empirical Support: The paper provides rigorous experimental evidence, including comparisons with baseline methods, ablation studies, and robustness analyses, demonstrating the effectiveness of the technique.  
Supporting Arguments  
1. Claims and Evidence: The main claim that annealed Gaussian gradient noise improves optimization and generalization is well-supported by experiments. For example, the method enables successful training of a 20-layer fully-connected network on MNIST with poor initialization and improves performance on complex tasks like binary multiplication with Neural GPUs.  
2. Relevance and Usefulness: The technique is highly relevant to practitioners facing optimization challenges in deep learning. Its compatibility with existing optimizers like Adam and AdaGrad further enhances its practical utility.  
3. Novelty: While noise injection has been explored in other contexts (e.g., weight noise, dropout), the specific use of annealed gradient noise with adaptive optimizers is novel and addresses a gap in the literature.  
Suggestions for Improvement  
1. Theoretical Analysis: While the empirical results are compelling, the paper would benefit from a deeper theoretical analysis of why gradient noise works so effectively, particularly in escaping saddle points and local minima.  
2. Broader Applicability: The paper focuses primarily on complex architectures. It would be helpful to explore whether the technique is equally effective on simpler models or other domains like reinforcement learning.  
3. Failure Cases: While the paper briefly mentions a negative result in language modeling, a more detailed analysis of when and why the method fails would provide valuable insights for practitioners.  
Questions for the Authors  
1. How sensitive is the technique to the choice of noise decay parameters (e.g., η and γ)? Could you provide more guidance on how to tune these parameters for new tasks?  
2. Have you considered combining gradient noise with other regularization techniques like weight decay or label smoothing?  
3. Could the method be extended to reinforcement learning or unsupervised learning tasks, where optimization challenges are also prevalent?  
Overall, the paper makes a significant contribution to the field of neural network optimization and provides a practical tool for improving the training of complex models. With minor improvements in theoretical grounding and broader applicability, this work has the potential to become a widely adopted technique in the deep learning community.