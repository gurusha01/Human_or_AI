The paper introduces a novel method for finding dependent subspaces across multiple views of data, optimizing for neighborhood preservation rather than traditional coordinate correlation. The authors propose a framework that directly maximizes cross-view neighborhood similarity, leveraging probabilistic neighborhoods and information retrieval-inspired metrics. The method is claimed to detect nonlinear and local dependencies, provide invariance to transformations, and outperform existing approaches like Canonical Correlation Analysis (CCA) and its variants in preserving cross-view neighborhood relationships. The paper demonstrates the method's effectiveness through experiments on artificial and real-world datasets, including MNIST, video data, stock prices, and cell-cycle data.
Decision: Accept.  
Key reasons: (1) The paper presents a novel and well-motivated approach that addresses limitations of traditional CCA by focusing on neighborhood relationships rather than coordinate correlations. (2) The experimental results convincingly demonstrate the method's superiority over existing approaches in preserving cross-view neighborhood similarities across diverse datasets.
Supporting Arguments:  
1. Novelty and Contribution: The proposed method introduces a fresh perspective by optimizing for neighborhood similarity, which is more relevant for many real-world tasks than coordinate correlation. The use of probabilistic neighborhoods and information retrieval-inspired metrics is innovative and well-justified.  
2. Experimental Validation: The experiments are comprehensive, covering both synthetic and real-world datasets. The method consistently outperforms CCA and LPCCA in preserving neighborhood relationships, as evidenced by precision-recall curves and correspondence scores. The inclusion of multi-view and different-dimensional subspaces further highlights the method's flexibility.  
3. Theoretical Rigor: The paper provides a detailed mathematical formulation of the method, including the objective function, optimization strategy, and invariance properties. This rigor enhances confidence in the validity of the approach.
Additional Feedback for Improvement:  
1. Scalability: While the authors acknowledge the computational complexity of their method (O(NÂ²) for naive implementation), further discussion on scalability and potential acceleration techniques would strengthen the paper. Including experiments on larger datasets could also provide more practical insights.  
2. Comparison with Nonlinear Methods: The paper focuses on linear transformations but briefly mentions the potential for nonlinear extensions. A comparison with existing nonlinear methods, even in a limited capacity, would provide a more comprehensive evaluation.  
3. Ablation Studies: It would be helpful to include ablation studies to assess the impact of individual components, such as the choice of similarity measures (KL divergence vs. cosine similarity) or the penalty term in the optimization.  
4. Clarity of Presentation: While the paper is mathematically rigorous, some sections (e.g., optimization details) are dense and could benefit from clearer explanations or visual aids to improve accessibility for a broader audience.
Questions for the Authors:  
1. How sensitive is the method to the choice of hyperparameters, such as the neighborhood size or the penalty term in the optimization?  
2. Can the method handle missing data or views with significantly different scales or noise levels?  
3. Have you considered extending the method to nonlinear transformations (e.g., neural networks), and what challenges do you anticipate in doing so?  
4. How does the method perform in terms of runtime and memory usage compared to CCA and LPCCA on larger datasets?
Overall, the paper makes a significant contribution to multi-view learning by introducing a novel and practically useful approach for finding dependent subspaces. The method's strong theoretical foundation and promising experimental results justify its acceptance, with minor revisions to address scalability and clarity.