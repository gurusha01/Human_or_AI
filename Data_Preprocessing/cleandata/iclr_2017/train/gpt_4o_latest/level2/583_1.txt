The paper presents a novel framework for studying computational creativity through the lens of out-of-distribution (OOD) novelty generation. The authors propose an actionable definition of creativity as the ability to generate novel objects outside the training distribution and introduce a new experimental setup using hold-out classes to evaluate generative models. The work is supported by extensive experiments on autoencoders and GANs, demonstrating the potential of these architectures to generate OOD novelty. The paper also introduces and evaluates several metrics, such as out-of-class objectness and out-of-class count, to assess the quality of generated samples.
Decision: Accept
The paper makes a significant contribution to the field of computational creativity by addressing the underexplored problem of OOD novelty generation. The proposed experimental framework and metrics provide a systematic way to evaluate generative models for this task. The work is well-motivated, grounded in relevant literature, and supported by rigorous experimentation, making it a valuable addition to the conference.
Supporting Arguments:
1. Novelty and Contribution: The paper introduces a paradigm shift by focusing on OOD novelty generation, a task that traditional generative modeling frameworks are not designed to handle. The proposed hold-out class setup and evaluation metrics are innovative and address a critical gap in the field.
2. Experimental Rigor: The authors conduct a large-scale study of generative models, exploring a wide range of architectures and hyperparameters. The results are analyzed using both quantitative metrics and human evaluation, providing a comprehensive assessment of the models' creative capacity.
3. Practical Usefulness: The framework and metrics have the potential to accelerate research in computational creativity by enabling systematic exploration and evaluation of generative models. This could lead to applications in fields like design, art, and innovation.
Additional Feedback:
1. Clarity of Metrics: While the paper introduces several metrics for evaluating OOD novelty, their definitions and interpretations could be more clearly explained. For instance, the distinction between out-of-class objectness and out-of-class count might confuse readers unfamiliar with the domain.
2. Broader Applicability: The experiments focus primarily on MNIST and letter datasets. Expanding the evaluation to more complex datasets could strengthen the generalizability of the proposed framework.
3. Human Evaluation: The paper acknowledges the importance of human evaluation but does not delve deeply into its design or results. A more detailed discussion of how human feedback aligns with the proposed metrics would enhance the paper's impact.
4. Limitations: While the paper briefly mentions the challenges of generating meaningful OOD samples, a more explicit discussion of the limitations of the proposed approach (e.g., scalability, dependence on pre-trained discriminators) would provide a balanced perspective.
Questions for the Authors:
1. How does the proposed framework handle the trade-off between generating meaningful novelty and avoiding trivial noise? Are there specific architectural features or hyperparameters that consistently lead to better results?
2. Could the proposed metrics be extended to other domains, such as text or audio generation? If so, what modifications would be required?
3. How does the framework perform on more complex datasets, such as ImageNet or CIFAR-10, where the notion of "classes" is less discrete?
Overall, the paper is a well-executed and timely contribution to the field of computational creativity. While there is room for improvement in clarity and generalization, the proposed framework and metrics provide a strong foundation for future research.