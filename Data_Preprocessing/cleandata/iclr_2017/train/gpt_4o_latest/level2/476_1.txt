Review of the Paper
This paper investigates whether deep convolutional neural networks (CNNs) truly need to be both deep and convolutional to achieve high accuracy, particularly when trained using distillation techniques. The authors revisit prior work by Ba and Caruana (2014) and provide a more rigorous empirical analysis on CIFAR-10, demonstrating that shallow models, even when trained to mimic deep models, cannot achieve comparable accuracy unless they include multiple convolutional layers. The paper's primary claim is that depth and convolution are essential for high accuracy on vision tasks, even when leveraging advanced training techniques like distillation and Bayesian hyperparameter optimization.
Decision: Accept  
Key Reasons:  
1. Strong Empirical Evidence: The paper provides comprehensive experiments demonstrating that shallow models with limited convolutional layers fail to match the performance of deep convolutional models, even under the same parameter budget. The results are statistically significant and well-supported.  
2. Novelty and Contribution: The work builds on prior research but advances the field by rigorously exploring the interplay between depth, convolution, and distillation. The findings clarify the limitations of shallow architectures and provide valuable insights for the community.  
Supporting Arguments:  
1. Thorough Experimental Design: The authors employ Bayesian hyperparameter optimization to ensure that shallow models are trained as effectively as possible. They also use a state-of-the-art ensemble of deep CNNs as teacher models, ensuring a robust baseline for comparison.  
2. Clear Results: The paper convincingly shows that shallow models without convolution perform poorly, and even shallow convolutional models require multiple layers to achieve reasonable accuracy. The "convolutional gap" and "compression gap" are well-documented, providing strong evidence for the necessity of depth and convolution.  
3. Relevance and Practicality: The findings are highly relevant for the design of efficient neural networks, particularly in domains where model size and computational cost are critical.  
Additional Feedback for Improvement:  
1. Clarify Theoretical Implications: While the empirical results are strong, the paper could benefit from a deeper discussion of the theoretical reasons why convolution and depth are critical. For example, the authors could elaborate on representational efficiency or the role of hierarchical feature extraction.  
2. Expand Beyond CIFAR-10: The paper mentions ongoing experiments on ImageNet but does not include results. Including preliminary findings on a more complex dataset would strengthen the generalizability of the conclusions.  
3. Comparison with Other Regularization Techniques: The authors note that dropout reduces accuracy in mimic models but do not explore why. A deeper analysis of how distillation interacts with other regularization methods would be valuable.  
Questions for the Authors:  
1. How do you explain the diminishing returns of adding more parameters to shallow models, particularly MLPs? Could this be related to overfitting or the lack of hierarchical feature extraction?  
2. Have you considered exploring hybrid architectures (e.g., shallow models with partially convolutional layers) to further analyze the trade-offs between depth and convolution?  
3. How do you anticipate your findings scaling to tasks beyond image classification, such as natural language processing or reinforcement learning?  
In conclusion, this paper provides a well-executed empirical study that addresses an important question in the field of deep learning. The results are robust, the methodology is sound, and the findings have practical implications for both researchers and practitioners. I recommend acceptance with minor revisions to enhance the theoretical discussion and broaden the scope of the experiments.