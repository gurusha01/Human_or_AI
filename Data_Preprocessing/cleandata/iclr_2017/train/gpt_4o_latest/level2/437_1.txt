The paper introduces GA3C, a hybrid CPU/GPU implementation of the Asynchronous Advantage Actor-Critic (A3C) algorithm, and claims significant computational improvements over the original CPU-based A3C implementation. The authors focus on optimizing GPU utilization for reinforcement learning (RL) tasks, proposing a system of queues and dynamic scheduling strategies to address inefficiencies in GPU usage. The paper further claims that GA3C achieves up to 6× speedup for small neural networks and up to 45× for larger networks, while maintaining or improving learning stability and convergence speed. The implementation is made publicly available, encouraging further research.
Decision: Accept.  
Key reasons: (1) The paper presents a novel and well-motivated improvement to the widely-used A3C algorithm, addressing a critical bottleneck in RL computation. (2) The claims are supported by rigorous experiments, demonstrating both computational efficiency and learning performance gains.
Supporting Arguments:  
1. Novelty and Contribution: The hybrid CPU/GPU architecture is a significant innovation, addressing a key limitation of A3C in leveraging GPU resources. The introduction of dynamic scheduling and batching strategies is particularly noteworthy, as these methods could generalize to other RL algorithms.  
2. Experimental Rigor: The authors provide extensive empirical evidence, including detailed performance metrics (e.g., Trainings Per Second, GPU utilization) and learning curves across multiple Atari games. The experiments are conducted on various hardware configurations, showcasing the robustness and scalability of GA3C.  
3. Practical Utility: The open-source release of GA3C enhances its impact, enabling other researchers to replicate and build upon this work. The demonstrated speedup and scalability make it highly relevant for real-world RL applications, such as robotics and autonomous systems.  
4. Literature Context: The paper situates its contributions well within the RL literature, comparing GA3C to both the original A3C and other distributed RL systems like Gorilla DQN and AlphaGo. The discussion of trade-offs (e.g., batch size vs. stability) reflects a strong understanding of the field.
Additional Feedback:  
1. Clarity of Presentation: While the technical details are thorough, the paper could benefit from a clearer explanation of the policy lag issue and its mitigation strategies. A visual representation of the lag dynamics might help readers better understand its impact on learning stability.  
2. Limitations: The paper acknowledges potential overheads in Python-based implementation and the need for further optimization (e.g., using C++). Explicitly discussing how these limitations might affect adoption in large-scale systems would strengthen the work.  
3. Future Directions: While the paper mentions potential extensions to other RL algorithms, a more detailed discussion of how GA3C could generalize to non-A3C methods would be valuable. For example, could the dynamic scheduling approach be adapted to algorithms with replay buffers like DQN?  
Questions for Authors:  
1. How does GA3C perform on RL tasks beyond Atari games, such as continuous control benchmarks or real-world robotics tasks?  
2. Could the dynamic configuration approach be extended to automatically tune hyperparameters like learning rate or entropy regularization?  
3. How does the policy lag issue scale with larger DNN architectures or more complex environments?  
Overall, the paper makes a strong contribution to the field of RL by addressing a critical computational bottleneck in A3C. The proposed methods are well-supported by experiments and have significant potential for broader impact.