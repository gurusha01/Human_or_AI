The paper investigates adversarial training for large-scale models and datasets, specifically applying it to Inception v3 on ImageNet. The authors make several key contributions: (1) they provide recommendations for scaling adversarial training to large models and datasets, (2) demonstrate that adversarial training improves robustness against single-step attack methods, (3) find that multi-step attacks are less transferable than single-step attacks, and (4) identify and address the "label leaking" effect, where adversarially trained models perform better on adversarial examples than on clean examples due to the adversarial example generation process. The paper also explores the influence of model capacity on adversarial robustness and the transferability of adversarial examples between models.
Decision: Accept  
Key Reasons:  
1. Significant Contributions to Adversarial Training: The paper provides practical insights into scaling adversarial training to large datasets and models, a notable advancement over prior work limited to smaller datasets like MNIST and CIFAR-10.  
2. Novel Findings: The identification of the "label leaking" effect and the nuanced exploration of adversarial example transferability are valuable contributions to the field.  
Supporting Arguments:  
The paper is well-motivated, addressing the critical problem of adversarial robustness in machine learning models. The authors demonstrate a strong understanding of the literature, referencing foundational works (e.g., Goodfellow et al., 2014) and situating their contributions within the broader context of adversarial training and attacks. The experimental results are robust, leveraging large-scale distributed training and providing detailed evaluations of adversarial robustness under various conditions (e.g., model size, attack methods). The findings, particularly regarding the transferability of adversarial examples and the impact of model capacity, are both novel and practically useful for researchers and practitioners designing robust models.
Additional Feedback:  
1. Clarity and Completeness: While the paper is thorough, some sections could benefit from additional clarity. For example, the discussion of the "label leaking" effect could include more intuitive explanations for readers unfamiliar with the phenomenon.  
2. Iterative Methods: The paper notes that adversarial training with iterative methods is computationally expensive and less effective but does not explore potential solutions. Suggestions for addressing this limitation would strengthen the paper.  
3. Broader Implications: The authors could expand on the implications of their findings for real-world applications, particularly in security-critical domains where adversarial robustness is paramount.  
Questions for the Authors:  
1. Could you elaborate on why iterative adversarial examples are less transferable between models compared to single-step examples?  
2. Did you explore any hybrid approaches combining single-step and iterative methods for adversarial training? If not, do you anticipate such approaches might address the limitations of iterative methods?  
3. How do you envision the trade-off between robustness and clean accuracy evolving as model capacity continues to increase?  
Overall, this paper makes significant contributions to the field of adversarial robustness and provides actionable insights for scaling adversarial training to large models and datasets. With minor revisions to improve clarity and address limitations, the paper will be a valuable addition to the conference.