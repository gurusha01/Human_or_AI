Review of the Paper
This paper investigates the expressive power of deep neural networks, focusing on their behavior before and after training. The authors propose three measures of expressivity—neuron transitions, activation patterns, and dichotomies—and demonstrate that these measures exhibit exponential growth with network depth. They identify a unifying factor, trajectory length, which underpins this exponential dependence. The paper further explores how expressivity evolves during training and highlights the influence of earlier network layers on overall expressivity. The claims are supported through theoretical proofs and experiments on MNIST and CIFAR-10 datasets.
Decision: Accept
The paper is recommended for acceptance due to its novel theoretical insights into the expressivity of deep neural networks and its rigorous combination of theoretical and empirical analysis. The work addresses a fundamental question in deep learning and provides a solid foundation for future research on network architecture and training dynamics.
Supporting Arguments
1. Novelty and Contribution: The paper introduces trajectory length as a key factor driving the exponential growth of expressivity measures with depth, a novel and significant contribution to the field. The theoretical results are complemented by empirical validation, bridging the gap between theory and practice.
2. Theoretical Rigor: The authors provide detailed proofs for their claims, including bounds on trajectory length and its relationship to expressivity measures. The mathematical analysis is robust and well-supported by prior literature.
3. Practical Implications: The findings have practical relevance, suggesting that earlier layers in a network have a disproportionately large impact on expressivity. This insight could inspire new pre-training methods, initialization schemes, and training strategies.
4. Empirical Validation: The experiments on MNIST and CIFAR-10 datasets effectively validate the theoretical claims. The results are consistent and well-presented, enhancing the credibility of the work.
Suggestions for Improvement
1. Clarity of Presentation: While the theoretical sections are comprehensive, they are dense and may be challenging for readers unfamiliar with the mathematical tools used. Simplifying some of the derivations or providing intuitive explanations alongside the formal proofs would improve accessibility.
2. Experimental Scope: The experiments are limited to MNIST and CIFAR-10. Including additional datasets or tasks, such as natural language processing or reinforcement learning, would strengthen the generalizability of the findings.
3. Impact of Width: The paper focuses primarily on depth, but the role of network width is less explored. A more detailed discussion or additional experiments on how width interacts with depth and trajectory length would be valuable.
4. Limitations and Future Work: While the paper briefly mentions limitations, a more explicit discussion would be beneficial. For example, how do the results generalize to architectures with skip connections or attention mechanisms? Addressing these questions would provide a clearer roadmap for future research.
Questions for the Authors
1. How do the results extend to architectures with non-standard components, such as residual connections or transformers? 
2. Have you considered the impact of different activation functions beyond ReLU and hard-tanh on trajectory length and expressivity measures?
3. Can the observed trade-off between stability and expressivity during training be quantified more precisely? How does this trade-off vary across datasets or tasks?
Overall, this paper makes a significant contribution to understanding the expressivity of deep neural networks. While there is room for improvement in presentation and scope, the theoretical and empirical results are compelling and warrant acceptance.