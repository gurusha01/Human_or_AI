Review of the Paper
Summary of Contributions
The paper investigates the principle of identity parameterization in deep learning, particularly in residual networks, and provides both theoretical and empirical contributions. The authors present a simple proof that deep linear residual networks have no spurious local optima, a result that contrasts with standard parameterizations. They also demonstrate that residual networks with ReLU activations possess universal finite-sample expressivity, meaning they can represent any function of a sample given sufficient parameters. Inspired by these theoretical insights, the authors propose an all-convolutional residual architecture without batch normalization, dropout, or max pooling, achieving competitive results on CIFAR10, CIFAR100, and ImageNet benchmarks.
Decision: Accept
The paper makes significant theoretical and practical contributions to understanding and improving residual networks. The simplicity and rigor of the theoretical results, combined with competitive empirical performance, justify acceptance. However, some areas could benefit from further clarification and additional experiments.
Supporting Arguments
1. Theoretical Contributions: The paper provides a clear and elegant proof that linear residual networks have no spurious local optima, a stronger result than prior work on standard parameterizations. The universal finite-sample expressivity result for ReLU-based residual networks is also compelling and well-supported by theoretical construction.
2. Practical Impact: The proposed all-convolutional architecture simplifies deep learning models by removing commonly used components like batch normalization and dropout. Despite its simplicity, the architecture achieves competitive performance on standard benchmarks, demonstrating the practical utility of the theoretical insights.
3. Novelty: The work is novel in both its theoretical contributions and its practical implications. The idea of identity parameterization is explored in greater depth than in prior work, and the proposed architecture represents a significant departure from standard designs.
Additional Feedback
1. Empirical Comparisons: While the proposed architecture performs well on CIFAR10 and CIFAR100, its performance on ImageNet lags behind state-of-the-art models. The authors should explore whether larger models or alternative hyperparameter settings could address the observed underfitting on ImageNet.
2. Ablation Studies: It would be helpful to include ablation studies to isolate the impact of specific design choices, such as the absence of batch normalization and the use of fixed random projections in the final layer.
3. Reproducibility: While the paper provides sufficient details for theoretical reproducibility, the empirical section could benefit from more comprehensive hyperparameter settings and training details. Sharing the code would further enhance reproducibility.
4. Limitations: The paper briefly mentions that the optimization results do not extend to non-linear networks. A more detailed discussion of this limitation and potential avenues for addressing it would strengthen the paper.
Questions for the Authors
1. Can the theoretical results for linear residual networks be extended to non-linear networks with ReLU activations? If so, what are the main challenges?
2. How does the proposed architecture perform when scaled to larger datasets or tasks beyond image classification?
3. Did the authors explore alternative initialization schemes or regularization techniques to address the underfitting observed on ImageNet?
Overall, the paper is a strong contribution to the field, combining theoretical rigor with practical relevance. Addressing the above feedback could further enhance its impact.