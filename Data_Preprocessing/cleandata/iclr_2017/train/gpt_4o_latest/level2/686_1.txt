Review
This paper introduces Homologically Functional Hashing (HFH), a novel method for compressing deep neural networks (DNNs) by leveraging multiple low-cost hash functions and a small reconstruction network. The key contribution is the introduction of a homological compression space shared across all layers, which simplifies the determination of compression ratios and improves performance compared to existing methods like HashedNets. HFH is shown to achieve high compression ratios with minimal loss in prediction accuracy across multiple datasets, including MNIST, CIFAR-10, and ImageNet, as well as in industrial applications like semantic ranking.
Decision: Accept
The paper makes a compelling case for acceptance due to its significant contribution to DNN compression. The method is both novel and practical, addressing key challenges in model compression, such as hash collisions and reconstruction accuracy. The experimental results demonstrate the superiority of HFH over HashedNets and its competitive performance compared to state-of-the-art pruning methods, making it a valuable addition to the field.
---
Supporting Arguments
1. Novelty and Innovation:  
   HFH introduces a homological compression space shared across all layers, which is a novel concept not present in prior work like HashedNets. The use of multiple hash functions and a reconstruction network addresses limitations in HashedNets, such as high collision risk and limited reconstruction capability.
2. Experimental Rigor:  
   The paper provides extensive experiments on benchmark datasets (e.g., MNIST, CIFAR-10, ImageNet) and a large-scale industrial dataset. Results consistently show that HFH outperforms HashedNets in terms of compression ratio and accuracy. The method is also shown to be competitive with state-of-the-art pruning techniques, despite requiring less manual tuning.
3. Practical Utility:  
   HFH is lightweight, easy to integrate into existing DNNs, and applicable to both fully connected and convolutional layers. Its ability to maintain accuracy with significantly reduced memory usage makes it highly relevant for resource-constrained environments like mobile devices.
---
Suggestions for Improvement
1. Theoretical Analysis:  
   While the paper provides empirical evidence of HFH's effectiveness, a deeper theoretical analysis of its properties, such as bounds on reconstruction error or collision probabilities, would strengthen the contribution.
2. Comparison with Pruning Methods:  
   Although HFH is compared to HashedNets and baseline models, a more detailed comparison with pruning techniques (e.g., dynamic network surgery) would provide a clearer picture of its relative strengths and weaknesses.
3. Ablation Studies:  
   The paper explores some configurations of HFH (e.g., number of hash functions, layers in the reconstruction network), but a more systematic ablation study could help clarify the trade-offs between memory usage, computational cost, and accuracy.
4. Reproducibility:  
   The authors mention plans to release code but do not provide sufficient implementation details in the current version. Including pseudo-code or a more detailed description of the training pipeline would enhance reproducibility.
---
Questions for the Authors
1. How does HFH perform when combined with other compression techniques, such as pruning or quantization? Could it be used synergistically to achieve even higher compression ratios?
2. What are the computational overheads of HFH during training and inference compared to HashedNets and pruning methods? Are there scenarios where this overhead might become prohibitive?
3. Can HFH handle dynamic architectures, such as those generated by neural architecture search (NAS), or is it limited to static architectures?
---
Conclusion
HFH is a well-motivated and innovative approach to DNN compression that addresses key limitations in existing methods. Its strong empirical results and practical relevance make it a valuable contribution to the field. With minor improvements in theoretical analysis and reproducibility, this work has the potential to become a standard tool for DNN compression. I recommend acceptance.