Review of the Paper
This paper investigates the approximation capabilities of deep versus shallow neural networks for a large class of piecewise smooth functions. The authors claim that deep networks require exponentially fewer neurons than shallow networks to achieve the same level of approximation accuracy. The main contributions include (1) establishing upper and lower bounds on the size and depth of neural networks required for ε-approximation, (2) demonstrating that shallow networks require Ω(poly(1/ε)) neurons while deep networks require only O(polylog(1/ε)) neurons, and (3) extending these results to multivariate functions. The analysis is conducted for networks using ReLU and binary step activation functions, and the results are supported by theoretical proofs.
Decision: Accept
Key reasons for this decision are the paper's strong theoretical contributions and its relevance to the ongoing discussion about the advantages of deep neural networks. The results are novel, well-motivated, and rigorously derived, providing significant insights into the efficiency of deep architectures for function approximation.
Supporting Arguments:
1. Novelty and Significance: The paper provides a rigorous theoretical foundation for understanding why deep networks outperform shallow ones in terms of neuron efficiency. The results extend prior work by offering generalizations to a broader class of functions and activation types.
2. Theoretical Rigor: The proofs are detailed and mathematically sound, with clear derivations of both upper and lower bounds. The authors also compare their results with existing literature, such as works by Telgarsky (2016) and Barron (1993), highlighting the improvements and differences.
3. Practical Implications: While the paper is theoretical, its findings have practical implications for designing efficient neural network architectures, particularly in resource-constrained environments.
Additional Feedback for Improvement:
1. Clarity of Presentation: While the theoretical results are robust, the paper is dense and challenging to follow in some sections. Including more intuitive explanations, diagrams, or examples to illustrate key concepts (e.g., the difference between shallow and deep networks in specific cases) would improve accessibility for a broader audience.
2. Experimental Validation: Although the paper focuses on theoretical analysis, providing empirical results to validate the theoretical claims would strengthen the paper. For instance, experiments showing the actual neuron counts required for shallow and deep networks to approximate specific functions could provide additional evidence.
3. Discussion of Limitations: The paper briefly mentions that the results apply to piecewise smooth functions but does not discuss potential limitations in real-world scenarios where functions may not meet these smoothness criteria. Acknowledging and addressing these limitations would enhance the paper's completeness.
4. Comparison with Yarotsky (2016): The authors mention a concurrent work by Yarotsky but do not provide a detailed comparison of the results. Clarifying how their approach and findings differ would help contextualize the contribution.
Questions for the Authors:
1. How do the theoretical results extend to functions that are not piecewise smooth or have discontinuities? Are there practical scenarios where the assumptions of the paper might not hold?
2. Can the results be generalized to other activation functions beyond ReLU and binary step units? If not, what are the challenges in doing so?
3. Did you consider any empirical validation of the theoretical bounds? If so, what were the results, and if not, why was this omitted?
In conclusion, this paper makes a significant theoretical contribution to understanding the efficiency of deep neural networks. While some improvements in presentation and additional empirical support would enhance the work, the core results are compelling and merit acceptance at the conference.