Review
The paper addresses a critical issue in deep reinforcement learning (DRL): the tendency of deep Q-networks (DQNs) to periodically revisit catastrophic states due to distributional shift and catastrophic forgetting, termed the "Sisyphean curse." The authors propose a novel solution, Intrinsic Fear, which incorporates a supervised danger model to penalize the Q-learning objective and shape the reward function away from catastrophic states. Through experiments on toy problems (Adventure Seeker and Cart-Pole) and preliminary results on the Atari game Seaquest, the paper demonstrates that the proposed method significantly reduces the frequency of catastrophic failures while improving learning efficiency.
Decision: Accept
The paper makes a strong case for acceptance due to its clear identification of a critical problem in DRL, its innovative solution, and promising experimental results. The key reasons for this decision are:  
1. Novelty and Relevance: The intrinsic fear mechanism is a novel contribution to safe reinforcement learning, addressing a gap in the literature by mitigating catastrophic forgetting in function approximation-based RL.  
2. Scientific Rigor: The experiments are well-designed, with clear comparisons between baseline DQNs and the proposed method. The results convincingly demonstrate the effectiveness of intrinsic fear in reducing catastrophic failures.  
3. Practical Implications: The work has significant implications for deploying DRL agents in real-world safety-critical applications, such as self-driving cars and robotics.
Supporting Arguments  
The paper is well-motivated, as it highlights the limitations of existing DQNs in safety-critical scenarios. The authors provide a thorough review of related work, situating their contribution within the broader context of safe RL, function approximation, and intrinsic motivation. The experiments are comprehensive, covering both toy problems and a more complex domain (Seaquest). The use of a danger model to predict proximity to catastrophic states is an elegant and practical solution, and the results demonstrate its potential to improve both safety and learning efficiency. Importantly, the authors acknowledge the limitations of their approach, such as its reliance on a catastrophe detector and its potential ineffectiveness in environments without a notion of proximity.
Suggestions for Improvement  
1. Theoretical Analysis: While the empirical results are compelling, a more formal theoretical analysis of the intrinsic fear mechanism would strengthen the paper. For example, a proof of convergence or bounds on the penalty's impact on the optimal policy would be valuable.  
2. Scalability: The paper would benefit from a deeper discussion of how the proposed method scales to high-dimensional state spaces and more complex environments.  
3. Hyperparameter Sensitivity: The choice of hyperparameters, such as the fear radius and fear factor, appears critical to the method's success. A sensitivity analysis would help clarify how robust the approach is to these settings.  
4. Broader Evaluation: While the toy problems and Seaquest experiments are illustrative, additional benchmarks on more diverse environments (e.g., robotics or continuous control tasks) would provide stronger evidence of generalizability.
Questions for the Authors  
1. How does the intrinsic fear mechanism handle environments where catastrophic states are sparse or difficult to detect?  
2. Can the danger model be extended to incorporate temporal dependencies, such as predicting catastrophes over longer horizons?  
3. How does the method compare to other safety-focused RL approaches, such as those using hard constraints or risk-sensitive objectives?  
Overall, this paper represents a valuable contribution to the field of safe reinforcement learning and has the potential to inspire further research in mitigating catastrophic failures in DRL.