The paper presents an iterative decoding scheme for machine translation that revisits and refines initial translation guesses, addressing the limitations of traditional monotonic decoding algorithms. The authors propose a convolutional neural network model with attention mechanisms that iteratively improves translations by suggesting discrete word substitutions. The dual-attention model, which attends to both the source sentence and the guess translation, achieves up to a 0.4 BLEU improvement on WMT15 German-English translation with an average of only 0.6 substitutions per sentence. The approach is novel in its ability to refine translations without requiring post-edited text, making it a practical alternative to automatic post-editing.
Decision: Accept.  
Key reasons: (1) The paper introduces a novel, well-motivated iterative refinement approach that addresses a critical limitation in existing machine translation systems. (2) The experimental results, while modest, are statistically significant and demonstrate the feasibility of the proposed method.
Supporting Arguments:  
1. Novelty and Contribution: The iterative refinement approach is an innovative departure from traditional left-to-right or bottom-up decoding schemes. The dual-attention model, in particular, is a significant contribution, as it leverages both the source and guess translations to improve accuracy. The work is well-placed in the literature, with appropriate comparisons to prior methods like beam search and automatic post-editing.  
2. Experimental Rigor: The authors conduct thorough experiments on the WMT15 German-English dataset, using a phrase-based translation system as the baseline. The results are supported by detailed ablation studies, comparisons of single and dual-attention models, and oracle analyses. The reported BLEU improvements, though small, are consistent and achieved with minimal modifications per sentence, highlighting the efficiency of the approach.  
3. Practical Usefulness: The method is practical, requiring only parallel text for training and no reliance on scarce post-edited data. This makes it accessible for real-world applications where high-quality post-edited corpora are unavailable.
Additional Feedback:  
1. Limitations and Future Work: While the paper acknowledges the modest BLEU improvements, it could better contextualize these gains relative to the state-of-the-art neural machine translation systems. Future work could explore the integration of this method with neural translation outputs or extend the refinement process to include insertions, deletions, or multi-word edits.  
2. Clarity of Presentation: The paper is dense, with some sections (e.g., architecture details) requiring more accessible explanations for broader comprehension. Simplifying the mathematical notation and providing more intuitive diagrams could enhance readability.  
3. Evaluation Metrics: While BLEU is a standard metric, additional qualitative analysis or human evaluation of the refined translations would strengthen the claims about fluency and adequacy.  
Questions for Authors:  
1. How does the method perform when applied to neural machine translation outputs instead of phrase-based systems?  
2. Could the refinement process be extended to include other types of edits, such as insertions or deletions, and how might this impact BLEU?  
3. How sensitive is the model's performance to the choice of initial guess translations?  
Overall, the paper introduces a promising approach to iterative translation refinement, with clear potential for further development and application.