Review of the Paper: "Divide and Conquer with Neural Networks"
Summary of Contributions
This paper introduces a novel framework for learning algorithmic tasks using neural networks by leveraging the principle of divide and conquer. The authors propose a recursive architecture that learns two key operations: splitting input data into disjoint subsets and merging partial solutions into a final output. The model is trained using weak supervision, requiring only input-output pairs, and incorporates computational complexity as an optimization objective alongside accuracy. The paper demonstrates the approach on two tasks—sorting and finding planar convex hulls—showing promising empirical results. The authors also highlight the model's ability to generalize to larger input sizes due to its scale-invariant design.
Decision: Reject
While the paper presents an interesting idea with potential, it falls short in several critical areas, including experimental rigor, clarity of presentation, and novelty relative to existing work. Below, I outline the key reasons for this decision.
Supporting Arguments
1. Insufficient Experimental Validation: 
   - The empirical results are limited to two relatively simple tasks (sorting and planar convex hulls). While these tasks are useful as proofs of concept, they are insufficient to demonstrate the broader applicability or scalability of the proposed approach to more complex algorithmic problems.
   - The experiments lack comparisons with strong baselines, such as existing neural architectures designed for algorithmic tasks (e.g., Pointer Networks or Neural GPUs). Without such comparisons, it is difficult to assess the true performance and novelty of the proposed method.
2. Incomplete Implementation:
   - The authors acknowledge that the split and merge operations are trained separately and that joint training is left for future work. This significantly limits the scope of the current study and undermines the claim of a fully integrated divide-and-conquer framework.
   - The merge operation is only partially implemented, as it does not fully learn the concatenation step. This omission further weakens the paper's contribution.
3. Clarity and Reproducibility:
   - The paper is dense and difficult to follow, particularly in the technical sections describing the split and merge architectures. Key details, such as the exact training procedure and hyperparameter settings, are scattered and not presented in a cohesive manner.
   - While the authors promise to release code, the lack of available code at submission time makes it challenging to verify the claims or reproduce the results.
4. Limited Novelty:
   - The idea of using neural networks for algorithmic tasks is not new, and the paper does not sufficiently differentiate itself from prior work. For example, the use of recursive architectures and weak supervision has been explored in related models like Neural GPUs and Pointer Networks. The paper does not provide a clear comparison or justification for how its approach improves upon these methods.
Suggestions for Improvement
1. Expand Experimental Scope: Test the proposed method on a broader range of algorithmic tasks, including more complex problems (e.g., graph-based tasks like shortest paths or spanning trees). Include comparisons with state-of-the-art baselines to contextualize the results.
2. Integrate Split and Merge Training: Jointly train the split and merge operations to demonstrate the full potential of the framework. This would address the current limitation of separate training and provide a more cohesive contribution.
3. Improve Clarity: Simplify and streamline the presentation of the architecture and training procedure. Include a clear diagram of the full pipeline and provide detailed hyperparameter settings and training protocols.
4. Release Code: Make the code publicly available to facilitate reproducibility and allow the community to build upon the work.
5. Acknowledge Limitations: Provide a more thorough discussion of the limitations of the current approach, such as its reliance on specific inductive biases (e.g., scale invariance) and its applicability to tasks with different complexity profiles.
Questions for the Authors
1. How does the proposed method compare quantitatively to existing models like Pointer Networks or Neural GPUs on the same tasks? Can you provide baseline results for comparison?
2. What specific challenges prevented the joint training of the split and merge operations, and how do you plan to address them in future work?
3. How sensitive is the model's performance to the choice of hyperparameters, such as the regularization terms (e.g., βS, βM)?
4. Can the framework handle tasks that do not exhibit strong scale invariance? If not, how generalizable is the approach to other algorithmic problems?
In conclusion, while the paper introduces an intriguing idea, it requires significant refinement and additional validation to meet the standards of the conference. I encourage the authors to address the outlined shortcomings and resubmit after further development.