The paper presents a comprehensive study on the transferability of adversarial examples across large-scale datasets and state-of-the-art deep learning models, with a particular focus on both non-targeted and targeted adversarial examples. The authors introduce novel ensemble-based approaches to generate transferable targeted adversarial examples, achieving a significant breakthrough in this domain. The work also includes geometric analyses to better understand the behavior of adversarial examples and demonstrates the real-world implications of these findings by successfully attacking the black-box classification system Clarifai.com.
Decision: Accept
Key Reasons for Decision:
1. Novelty and Contribution: The paper is the first to extensively study the transferability of adversarial examples on large-scale datasets (ImageNet) and large models, addressing a critical gap in the literature. The proposed ensemble-based approach for generating transferable targeted adversarial examples is a significant innovation.
2. Scientific Rigor: The claims are well-supported by extensive experiments, including comparisons with existing methods, geometric analyses, and real-world validations on a commercial system. The results are statistically significant and reproducible.
Supporting Arguments:
- The paper clearly identifies the limitations of existing methods in generating transferable targeted adversarial examples and addresses these challenges with a novel ensemble-based approach. The method is rigorously evaluated, showing superior performance in both targeted and non-targeted transferability.
- The geometric analysis provides valuable insights into why adversarial examples transfer, such as the alignment of decision boundaries and the orthogonality of gradient directions. These findings contribute to the theoretical understanding of adversarial attacks.
- The real-world demonstration of attacking Clarifai.com highlights the practical implications of the research, making it highly relevant to the AI security community.
Additional Feedback for Improvement:
1. Clarity of Presentation: While the paper is thorough, some sections, particularly those discussing geometric properties, could benefit from clearer explanations and visual aids to make the findings more accessible to a broader audience.
2. Broader Implications: The paper could explore the ethical implications of generating adversarial examples and discuss potential defenses against such attacks in greater detail.
3. Dataset Diversity: While the focus on ImageNet is justified, evaluating the methods on additional datasets could strengthen the generalizability of the findings.
Questions for Authors:
1. How sensitive is the ensemble-based approach to the choice of models in the ensemble? Would including more diverse architectures improve transferability further?
2. Could the proposed methods be extended to other domains, such as natural language processing or audio classification? If so, what challenges might arise?
3. The paper mentions that the ensemble-based approach achieves a high success rate for targeted adversarial examples. Could you provide more details on the failure cases and potential ways to address them?
Overall, this paper makes a significant contribution to the field of adversarial machine learning and provides a solid foundation for future research. The novel methods, rigorous experiments, and practical demonstrations make it a strong candidate for acceptance.