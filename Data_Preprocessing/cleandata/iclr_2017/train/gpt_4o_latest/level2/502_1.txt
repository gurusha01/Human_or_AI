Review of the Paper: "An Automatic Dialogue Evaluation Model (ADEM)"
This paper addresses the critical challenge of automatically evaluating dialogue responses in unstructured domains, proposing ADEM, a novel evaluation model that predicts human-like scores for dialogue responses. The authors claim that ADEM significantly outperforms traditional word-overlap metrics such as BLEU in correlating with human judgments at both the utterance and system levels. Additionally, they demonstrate that ADEM generalizes well to unseen dialogue models, making it a promising step toward scalable and accurate evaluation of dialogue systems.
Decision: Accept  
Key Reasons:  
1. Significant Contribution to the Field: The paper tackles a well-recognized bottleneck in dialogue system research—reliable automatic evaluation—and provides a compelling alternative to BLEU and similar metrics, which are shown to correlate poorly with human judgments.  
2. Strong Empirical Results: The experiments convincingly demonstrate that ADEM achieves higher correlations with human evaluations compared to existing metrics, both at the utterance and system levels. The leave-one-out evaluation further validates its robustness and generalizability to new models.  
Supporting Arguments:  
1. Well-Motivated Approach: The authors clearly articulate the limitations of current word-overlap metrics and motivate the need for a model like ADEM that incorporates semantic similarity and conversational context. The use of a hierarchical RNN encoder and pre-training with VHRED is well-justified and aligns with the goal of capturing nuanced dialogue features.  
2. Rigorous Evaluation: The paper provides comprehensive experimental results, including correlation analysis, qualitative examples, and robustness tests. The inclusion of a failure analysis and data efficiency experiments further strengthens the credibility of the findings.  
3. Practical Utility: ADEM's ability to generalize to unseen dialogue models and its relatively fast evaluation time make it a practical tool for researchers and practitioners aiming to prototype and test dialogue systems efficiently.  
Additional Feedback and Suggestions for Improvement:  
1. Addressing Limitations: While the authors acknowledge ADEM's tendency to predict conservative scores and its susceptibility to human biases (e.g., favoring shorter responses), they could explore potential mitigation strategies, such as alternative loss functions or bias correction mechanisms.  
2. Broader Applicability: The paper primarily focuses on non-task-oriented dialogue systems. It would be valuable to discuss how ADEM might be adapted or extended to task-oriented systems or other domains.  
3. Human Evaluation Subjectivity: The authors note variability in human judgments as a limitation. Future work could explore ways to incorporate multiple annotators' perspectives or use more robust aggregation methods to reduce noise in the training data.  
4. Engagement and Diversity: ADEM currently focuses on appropriateness and relevance. Incorporating metrics for engagement or diversity could make it even more comprehensive, especially for evaluating conversational agents designed for long-term interactions.  
Questions for the Authors:  
1. How does ADEM handle scenarios where the reference response itself is suboptimal or ambiguous?  
2. Could ADEM be extended to evaluate multi-turn dialogues rather than single responses? If so, what modifications would be required?  
3. Have you considered testing ADEM on datasets beyond Twitter, such as Reddit or customer service dialogues, to assess its domain transferability?  
In conclusion, this paper makes a significant and timely contribution to the field of dialogue evaluation. While there are areas for further exploration, the proposed ADEM model is a strong step forward, and its practical utility and empirical rigor justify its acceptance.