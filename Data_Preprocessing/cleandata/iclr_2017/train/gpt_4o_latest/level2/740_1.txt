Review of the Paper
This paper introduces ParMAC, a distributed computation model for the Method of Auxiliary Coordinates (MAC) to optimize nested, nonconvex machine learning models. The authors claim that ParMAC achieves high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance, and streaming data processing. The paper demonstrates ParMAC's effectiveness using MPI to train binary autoencoders for large-scale image retrieval tasks, achieving nearly perfect speedups on a 128-processor cluster with datasets as large as 100 million high-dimensional points.
Decision: Accept
Key reasons for acceptance include:  
1. Novelty and Practicality: The paper presents a novel distributed framework for training nested models, which are notoriously difficult to optimize due to their nonconvexity. ParMAC's simplicity, scalability, and independence from hardware-specific configurations make it highly practical for large-scale machine learning tasks.  
2. Strong Empirical Support: The authors provide extensive experimental results demonstrating ParMAC's scalability and efficiency, including nearly perfect speedups for up to 128 processors and significant improvements in runtime for large datasets like SIFT-1B.  
Supporting Arguments  
- Claims and Their Support: The authors' claims are well-supported by theoretical analysis and empirical results. The theoretical model for parallel speedup aligns closely with experimental observations, and the implementation demonstrates practical feasibility on real-world datasets.  
- Relevance and Usefulness: ParMAC addresses a critical challenge in distributed machine learning—minimizing communication overhead while maximizing parallelism. Its application to binary autoencoders for image retrieval showcases its utility in a relevant and impactful domain.  
- Field Knowledge and Completeness: The paper demonstrates a strong understanding of related work, including distributed optimization frameworks like TensorFlow and Spark, and positions ParMAC as a complementary approach. The implementation details, including MPI usage and experiments on distributed clusters, are sufficiently detailed for reproducibility.  
Suggestions for Improvement  
1. Clarify Limitations: While the paper briefly mentions that MAC's convergence properties are not well understood for nondifferentiable layers, a more explicit discussion of ParMAC's limitations (e.g., potential challenges with very deep or highly irregular nested models) would strengthen the work.  
2. Comparison with Alternatives: While the authors discuss related work, a direct empirical comparison of ParMAC with other distributed frameworks (e.g., TensorFlow or parameter-server-based approaches) would provide additional context for its advantages.  
3. Scalability Beyond 128 Processors: Although the theoretical speedup model predicts scalability for larger clusters, experimental validation with more than 128 processors would solidify the claims.  
Questions for the Authors  
1. How does ParMAC handle scenarios where the number of submodels (M) is significantly smaller than the number of machines (P)? Does this limit scalability?  
2. Can ParMAC be extended to support heterogeneous clusters where machines have varying computational capacities?  
3. How sensitive is ParMAC's performance to the choice of the penalty parameter schedule (µ)? Are there guidelines for selecting it in practice?  
Conclusion  
This paper makes a significant contribution to distributed optimization for nested models, with strong theoretical and empirical support. While there is room for further exploration, the proposed ParMAC framework is a valuable addition to the field and merits acceptance.