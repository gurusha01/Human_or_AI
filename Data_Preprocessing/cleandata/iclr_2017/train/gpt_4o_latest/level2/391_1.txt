Review
The paper proposes a novel technique for pruning Recurrent Neural Networks (RNNs) during training to achieve significant model compression and computational efficiency without substantial loss in accuracy. The authors claim that their method reduces model size by up to 90% and achieves inference speed-ups of 2× to 7×. Additionally, they demonstrate that pruning larger dense models can outperform baseline dense models while still reducing parameter counts. The paper presents experimental results on speech recognition tasks, showing the effectiveness of the proposed approach on both vanilla RNNs and Gated Recurrent Units (GRUs).
Decision: Accept
Key Reasons for Decision:
1. Significant Contribution: The paper addresses a critical challenge in deploying large RNNs on resource-constrained devices by proposing a practical and effective pruning method. The results show substantial improvements in model compression and inference speed, which are highly relevant to the field.
2. Empirical Validation: The claims are well-supported by extensive experiments on real-world datasets. The authors compare their gradual pruning approach to hard pruning and demonstrate its superiority in maintaining accuracy while achieving high sparsity.
3. Practical Utility: The proposed method is straightforward to implement, compatible with existing training frameworks, and does not increase training time, making it highly practical for deployment.
Supporting Arguments:
1. Motivation and Placement in Literature: The paper is well-motivated, with a clear focus on the challenges of deploying large RNNs. The authors provide a thorough review of related work, situating their approach within the context of existing pruning and compression techniques. The comparison to prior methods, such as hard pruning and Hessian-based approaches, highlights the novelty and advantages of the proposed method.
2. Scientific Rigor: The experiments are detailed and scientifically rigorous. The authors evaluate their method on multiple architectures (vanilla RNNs and GRUs) and provide comprehensive benchmarks for model size, accuracy, and inference speed. The use of real-world speech recognition datasets strengthens the validity of the results.
3. Acknowledgment of Limitations: The paper acknowledges limitations, such as the suboptimal performance of existing sparse matrix libraries and the need for further optimization of sparse matrix-vector routines. This transparency adds credibility to the work.
Additional Feedback:
1. Reproducibility: While the paper provides some implementation details, including hyperparameters and pruning schedules, it would benefit from a more detailed discussion on the choice of pruning thresholds and their impact on different tasks. Providing code or pseudocode for key components could further enhance reproducibility.
2. Sparse Matrix Libraries: The authors note that current sparse matrix libraries underperform relative to theoretical expectations. It would be helpful to discuss potential solutions or collaborations with library developers to address this bottleneck.
3. Generalization to Other Tasks: While the focus on speech recognition is compelling, it would be valuable to explore the generalizability of the method to other domains, such as language modeling or computer vision. Preliminary experiments in these areas could strengthen the paper's impact.
4. Comparison with Quantization: The authors mention that their pruning technique is orthogonal to quantization. Including experimental results that combine pruning with quantization could provide a more complete picture of the potential benefits.
Questions for the Authors:
1. How sensitive is the pruning method to the choice of hyperparameters (e.g., start iteration, ramp iteration, and end iteration)? Could these parameters be automatically tuned?
2. Have you evaluated the method on tasks beyond speech recognition, such as natural language processing or time-series forecasting? If not, do you anticipate any challenges in applying it to these domains?
3. Can the proposed pruning technique be extended to other types of layers, such as convolutional or embedding layers, which are also memory-intensive?
In conclusion, the paper makes a significant contribution to the field of model compression for RNNs, with strong empirical results and practical implications. While there is room for further exploration and refinement, the work is well-executed and addresses an important problem, making it a valuable addition to the conference.