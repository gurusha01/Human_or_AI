The paper presents DeepRebirth, a novel framework for accelerating deep neural networks on mobile devices by optimizing non-tensor layers and merging them with neighboring tensor layers. The authors claim that DeepRebirth achieves significant speed-ups (3x-5x) and energy savings on mobile CPUs, with minimal accuracy loss (0.4% on GoogLeNet for ImageNet). The proposed method is compatible with existing compression techniques, further enhancing performance. Extensive experiments on GoogLeNet, AlexNet, and ResNet demonstrate its effectiveness across various devices, including low-end mobile CPUs.
Decision: Accept
Key reasons: (1) The paper addresses a critical problem of deploying deep learning models on mobile devices with a well-motivated and novel approach. (2) The claims are supported by rigorous experiments, showing significant improvements in speed, energy efficiency, and memory usage without substantial accuracy degradation.
Supporting Arguments:
1. Novelty and Contribution: The paper introduces a unique perspective by focusing on optimizing non-tensor layers, which are often overlooked in existing compression methods. The merging of non-tensor layers with tensor layers into "rebirth layers" is a novel idea that significantly reduces computational latency.
2. Experimental Rigor: The authors provide comprehensive evaluations on multiple architectures (GoogLeNet, AlexNet, ResNet) and devices, demonstrating the generality and robustness of the approach. The results are benchmarked against state-of-the-art methods like SqueezeNet and Tucker Decomposition, showing superior performance.
3. Practical Relevance: The framework is highly practical, addressing real-world constraints such as limited computational power, energy efficiency, and runtime memory on mobile devices. The negligible accuracy loss makes it suitable for deployment in real-time applications.
Suggestions for Improvement:
1. Clarity in Methodology: While the paper explains the merging process, some details, such as the criteria for selecting layers for merging and the specific fine-tuning process, could be elaborated for better reproducibility.
2. Comparison with Lightweight Architectures: While the paper compares DeepRebirth with SqueezeNet, additional comparisons with other lightweight architectures like MobileNet or EfficientNet would strengthen the evaluation.
3. Broader Applications: The paper focuses primarily on image classification tasks. It would be helpful to discuss or test the applicability of DeepRebirth in other domains, such as object detection or natural language processing.
4. Limitations: While the paper briefly mentions that storage savings are less significant, a more detailed discussion of the trade-offs (e.g., increased complexity in retraining rebirth layers) would provide a balanced perspective.
Questions for the Authors:
1. How does the merging process handle potential conflicts in functionality when combining layers with different purposes (e.g., pooling vs. normalization)?
2. Is there a theoretical limit to the number of layers that can be merged without compromising accuracy? If so, how is this determined?
3. Can DeepRebirth be extended to GPU-accelerated mobile devices, or is it inherently CPU-focused?
Overall, the paper addresses a critical challenge in deploying deep learning models on resource-constrained devices and provides a compelling solution with strong experimental evidence. The suggestions above aim to enhance clarity and broaden the scope of the work.