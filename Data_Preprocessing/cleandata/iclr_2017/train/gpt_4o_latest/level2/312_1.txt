The paper "Neural Architecture Search" introduces a novel approach for automating the design of neural network architectures using a recurrent neural network (RNN) trained with reinforcement learning. The authors claim that their method can generate architectures that rival or surpass state-of-the-art human-designed models on benchmark datasets such as CIFAR-10 and Penn Treebank. The paper highlights two key contributions: (1) the use of an RNN controller to generate variable-length architectural descriptions, and (2) the application of reinforcement learning to optimize these architectures based on validation set performance. The results demonstrate that the proposed method achieves a test error rate of 3.65% on CIFAR-10 and a test perplexity of 62.4 on Penn Treebank, both of which are competitive with or better than existing methods.
Decision: Accept
Key reasons for acceptance:
1. Novelty and Impact: The paper presents a significant innovation by automating neural architecture design, a traditionally manual and time-intensive process. The use of reinforcement learning to optimize architectures is a compelling and underexplored approach.
2. Strong Empirical Results: The method achieves state-of-the-art or near state-of-the-art performance on multiple benchmarks, demonstrating its practical utility and effectiveness.
Supporting Arguments:
1. The claims are well-supported by extensive experiments on CIFAR-10 and Penn Treebank. The authors provide detailed comparisons with existing methods, showing that their approach is both more accurate and computationally efficient in some cases.
2. The paper demonstrates a clear understanding of the field, referencing relevant literature on hyperparameter optimization, neuro-evolution, and meta-learning. The method is well-situated within the broader context of automated machine learning (AutoML).
3. The authors address key challenges, such as the high variance of policy gradient methods, by employing techniques like baseline functions and distributed training. These technical contributions enhance the robustness and scalability of the approach.
Additional Feedback:
1. Reproducibility: While the paper provides many implementation details, it would benefit from a more explicit discussion of hyperparameter settings and training schedules for reproducibility. The promise to release code is commendable and will further aid reproducibility.
2. Limitations and Generalization: The paper could more explicitly discuss the limitations of the approach, such as the computational cost of training 800 networks in parallel or the reliance on large-scale hardware resources. Additionally, while the method generalizes to character language modeling and machine translation, further experiments on diverse tasks would strengthen the claim of general applicability.
3. Search Space Complexity: The authors mention that the search space is large (e.g., 6 × 10¹⁶ architectures for recurrent cells). It would be helpful to quantify how efficiently the method explores this space compared to alternatives like random search or Bayesian optimization.
Questions for the Authors:
1. How sensitive is the method to the choice of reward function? For example, would using a different metric (e.g., F1 score for classification tasks) significantly impact the results?
2. Can the approach be extended to multi-objective optimization, such as balancing accuracy with model size or inference speed?
3. How does the computational cost of Neural Architecture Search compare to other AutoML methods, particularly for researchers with limited hardware resources?
In conclusion, this paper presents a significant advancement in automated neural architecture design. While there are areas for improvement, the novelty, strong empirical results, and potential impact on the field justify its acceptance.