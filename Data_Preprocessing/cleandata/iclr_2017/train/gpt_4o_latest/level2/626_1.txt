Review of the Paper
The paper presents a method to compute the preimages of activities at arbitrary levels of fully connected multilayer rectifier networks (ReLU networks) and explores their implications for understanding and improving the efficiency of deep learning networks. The authors argue that preimages, which are piecewise linear manifolds in the input space, serve as building blocks for modeling input class manifolds and ensuring efficient classification. They provide a detailed theoretical framework for computing preimages, disregarding max-pooling effects, and discuss the implications of these preimages for convolutional networks, image manifold modeling, and training efficiency.
Decision: Accept with Minor Revisions
The paper makes a significant theoretical contribution by introducing a novel perspective on preimages in deep networks and their role in classification. The key reasons for this decision are:  
1. Novelty and Relevance: The concept of preimages as building blocks for class manifolds is innovative and provides a fresh lens to understand deep learning networks. The work is well-placed in the literature, building on prior studies such as Mahendran & Vedaldi (2015, 2016).  
2. Scientific Rigor: The claims are supported by detailed theoretical derivations and illustrative examples. The authors provide a clear procedure for computing preimages and discuss their implications for both fully connected and convolutional networks.  
3. Potential Impact: The work has practical implications for improving network training efficiency and understanding adversarial examples, making it highly relevant to the deep learning community.
Supporting Arguments  
The paper effectively addresses the problem of class mixing in deep networks by characterizing preimages and their role in preserving class separability. The theoretical analysis is thorough, and the authors provide clear mathematical formulations and visual illustrations to support their claims. The discussion on the relationship between preimages and image manifolds is particularly compelling, as it bridges theoretical insights with practical challenges in deep learning.
Additional Feedback  
1. Clarity: While the theoretical framework is robust, the paper could benefit from a clearer explanation of key concepts, particularly for readers less familiar with preimages and nullspaces. Simplifying some of the mathematical notations and providing more intuitive examples would enhance accessibility.  
2. Empirical Validation: The paper lacks empirical results to validate the theoretical claims. Including experiments that compute preimages for real-world networks and analyze their properties would strengthen the paper.  
3. Pooling Effects: The authors acknowledge disregarding max-pooling effects in their analysis. A brief discussion on how pooling might alter the preimage structure would provide a more complete picture.  
4. Adversarial Examples: The connection between preimages and adversarial examples is intriguing but underexplored. Expanding on this relationship could open new avenues for research.
Questions for the Authors  
1. How do the preimages computed in this work compare to those obtained using numerical methods like those in Mahendran & Vedaldi (2015, 2016)?  
2. Can the proposed framework be extended to networks with max-pooling or other non-linearities beyond ReLU?  
3. Have you considered any specific applications where knowledge of preimages could directly improve training algorithms or adversarial robustness?  
In conclusion, the paper provides a valuable theoretical contribution to understanding deep networks and has the potential to inspire further research. Addressing the above feedback would make the work even more impactful.