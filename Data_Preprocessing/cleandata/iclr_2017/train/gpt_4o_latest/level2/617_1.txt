The paper presents a novel approach to improving distributed gradient descent for deep learning, focusing on asynchronous layer-wise gradient descent. The authors aim to maximize computational efficiency while maintaining equivalence to the default sequential gradient descent algorithm. They propose three main contributions: a baseline asynchronous gradient descent method, a layer-wise gradient descent approach, and a delayed gradient update mechanism. These methods are implemented in a modified version of Caffe and evaluated on both a CPU-based InfiniBand cluster and NVIDIA's DGX-1 system using AlexNet and GoogLeNet on the ImageNet dataset. The results demonstrate up to a 1.7x speedup compared to synchronous gradient descent, with convergence maintained.
Decision: Accept
The paper addresses a significant problem in distributed deep learning and provides a well-motivated, novel solution. The proposed asynchronous methods are rigorously evaluated and demonstrate meaningful improvements in performance without sacrificing model accuracy. The work is relevant to the conference audience and contributes to the growing field of distributed deep learning.
Supporting Arguments:
1. Novelty and Relevance: The paper introduces a new asynchronous layer-wise gradient descent approach that balances computational efficiency and algorithmic equivalence. This is a meaningful contribution to distributed deep learning, addressing the challenges of communication latency and scalability.
2. Experimental Rigor: The authors provide a thorough evaluation of their methods on two distinct hardware platforms and multiple neural network architectures. The use of real-world datasets (ImageNet) and established models (AlexNet, GoogLeNet) enhances the credibility and applicability of the results.
3. Practical Impact: The proposed methods achieve a significant speedup (up to 1.7x) while maintaining convergence, demonstrating their practical utility for large-scale deep learning tasks.
Suggestions for Improvement:
1. Layer-wise Gradient Descent Analysis: While the layer-wise approach is theoretically promising, its underperformance compared to delayed gradient updates warrants further investigation. The authors could provide more detailed insights into why communication latency could not be effectively hidden in this approach.
2. Scalability Beyond 8 Devices: The experiments on the DGX-1 system are limited to 8 GPUs. It would be beneficial to discuss how the proposed methods might scale to larger systems, particularly in scenarios with hundreds or thousands of compute nodes.
3. Convergence Tradeoffs: The paper mentions differences in loss curves during early training but does not delve deeply into the tradeoffs between speed and accuracy at intermediate stages. A more detailed analysis of these tradeoffs would strengthen the claims.
4. Implementation Details: While the implementation using MPI and modifications to Caffe are described, sharing the code or providing more detailed pseudocode would enhance reproducibility.
Questions for the Authors:
1. How does the delayed gradient update approach compare to parameter server methods in terms of scalability and convergence for larger datasets and deeper networks?
2. Can the layer-wise gradient descent method be optimized further to improve its performance, perhaps by overlapping communication and computation more effectively?
3. Did you evaluate the impact of different hyperparameter settings (e.g., learning rate, batch size) on the performance of the proposed methods?
Overall, this paper makes a strong contribution to distributed deep learning and is well-suited for acceptance at the conference. Addressing the suggested improvements would further enhance its impact and clarity.