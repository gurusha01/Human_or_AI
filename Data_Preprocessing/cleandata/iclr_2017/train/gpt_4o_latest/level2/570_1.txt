Review
This paper proposes a novel Neural Answer Construction Model aimed at addressing two key challenges in non-factoid question answering (QA): (1) the inability of existing methods to handle ambiguous word usage across different contexts, and (2) the limitation of current models to only select answers from existing QA site databases, without the ability to generate new, contextually appropriate answers. The authors introduce a biLSTM-based architecture that incorporates semantic biases into word embeddings and employs an attention mechanism to optimize the selection and combination of sentences for answer construction. The model is evaluated on the "Love advice" category of the Japanese QA site, Oshiete goo, and demonstrates a 20% improvement in answer construction accuracy over strong baselines.
Decision: Accept
The paper is well-motivated, presents a significant advancement in the field of non-factoid QA, and is supported by rigorous empirical evaluation. The key reasons for acceptance are:  
1. Novelty and Contribution: The proposed model bridges the gap between answer selection and answer generation, a critical limitation in existing QA systems. The integration of semantic biases and the use of an attention mechanism for sentence combination are innovative contributions.  
2. Strong Empirical Results: The model achieves substantial improvements in both answer selection and answer construction tasks, with a 20% higher accuracy in answer construction compared to the state-of-the-art QA-LSTM baseline. The human evaluation and real-world application further validate its practical utility.  
Supporting Arguments
1. Problem Identification: The paper clearly identifies the limitations of existing methods, particularly their inability to handle contextual word ambiguities and generate new answers. This sets a strong foundation for the proposed solution.
2. Methodological Soundness: The use of biLSTMs for embedding generation and the attention mechanism for sentence combination are well-justified. The approach to incorporate semantic biases into word embeddings is particularly compelling and effectively addresses the contextual ambiguity issue.
3. Comprehensive Evaluation: The evaluation is thorough, including quantitative metrics (e.g., Average Precision) and qualitative human assessments. The use of real-world data from Oshiete goo and the deployment of the model in a live application further strengthen the results.
Suggestions for Improvement
1. Generality of the Model: While the model is evaluated on "Love advice" questions, it would be valuable to test its applicability to other non-factoid QA categories (e.g., "Health Care" or "Traveling") to demonstrate its generalizability.
2. Scalability and Efficiency: The paper mentions that the computation time is under two hours, but more details on the scalability of the model to larger datasets or real-time applications would be helpful.
3. Ablation Studies: While the paper compares the proposed model to several baselines, an ablation study isolating the impact of semantic biases and the attention mechanism would provide deeper insights into the contributions of each component.
4. Error Analysis: A detailed analysis of the failure cases (e.g., when the model fails to construct accurate answers) would help identify areas for further improvement.
Questions for the Authors
1. How does the model handle questions that fall outside the predefined abstract scenarios for answer construction? Can it adapt to entirely new types of questions?  
2. Could the proposed method be extended to generate entirely new sentences rather than relying on pre-extracted sentence candidates?  
3. How does the model perform on shorter non-factoid questions or those with less context, such as those seen in chat-based systems?  
Overall, this paper presents a significant step forward in non-factoid QA and has the potential to inspire further research in answer generation. The proposed model is both innovative and practical, making it a strong candidate for acceptance.