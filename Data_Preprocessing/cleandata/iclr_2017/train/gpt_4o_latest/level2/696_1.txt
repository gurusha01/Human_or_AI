The paper presents two key contributions to the field of reading comprehension: (1) empirical evidence for the emergence of "predication structure" in the hidden state vectors of aggregation-based neural readers, and (2) the integration of linguistic features into neural readers, resulting in state-of-the-art performance on the Who-did-What dataset. The authors provide a logical interpretation of aggregation readers, positing that their hidden states decompose into a direct sum of predicate and entity representations. Additionally, the paper introduces pointer annotation readers, which leverage reference features without anonymization, and demonstrates their competitive performance.
Decision: Accept
The decision to accept is based on two primary reasons: (1) the novelty of the proposed interpretation of aggregation readers as learning emergent logical structures, and (2) the practical impact of integrating linguistic features, which significantly improves performance on a challenging dataset. These contributions are both theoretically insightful and empirically validated, making the paper a valuable addition to the conference.
Supporting Arguments:
1. Novelty and Contribution: The paper's hypothesis that aggregation readers implicitly learn a logical structure is compelling and supported by empirical evidence. This interpretation bridges a conceptual gap between neural architectures and symbolic reasoning, offering a fresh perspective on neural reader behavior. The introduction of pointer annotation readers further demonstrates innovation by addressing limitations of anonymization in non-anonymized datasets.
2. Empirical Rigor: The authors conduct extensive experiments across multiple datasets, including CNN/DailyMail, CBT, and Who-did-What, to validate their claims. The results consistently support the emergence of predication structure and the utility of linguistic features, with clear performance gains over existing models.
3. Usefulness: The proposed methods are practically relevant, as they improve the accuracy of neural readers on real-world datasets. The integration of linguistic features is particularly impactful, as it highlights the importance of combining neural representations with traditional linguistic insights.
Additional Feedback:
1. Clarity of Presentation: While the paper is dense with technical details, certain sections (e.g., the explanation of predication structure) could benefit from more intuitive examples or visualizations to aid understanding.
2. Limitations: The paper acknowledges that aggregation readers rely on reference resolution but could further discuss challenges in automating this process without external annotations. Additionally, the generalizability of the proposed methods to datasets beyond cloze-style tasks remains unclear.
3. Future Directions: The authors could explore extending their analysis to other types of question-answering tasks, such as open-domain or multi-hop reasoning, to assess the broader applicability of their findings.
Questions for Authors:
1. How sensitive are the proposed methods to variations in the quality of linguistic features (e.g., noisy or incomplete reference annotations)?
2. Can the predication structure hypothesis be extended to other neural architectures, such as transformers, which are increasingly popular in reading comprehension tasks?
3. How does the performance of pointer annotation readers compare to aggregation readers on datasets where anonymization is not feasible?
In summary, the paper makes significant contributions to the understanding and improvement of neural readers for reading comprehension. While some areas could benefit from further clarification and exploration, the theoretical insights and practical advancements justify its acceptance.