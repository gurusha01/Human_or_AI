The paper proposes a novel approach to address the high memory and energy consumption challenges of fully-connected layers in deep neural networks (DNNs) by introducing sparsely-connected networks. The authors claim that their method can reduce the number of connections in fully-connected networks by up to 90% while improving accuracy on datasets such as MNIST, CIFAR10, and SVHN. Additionally, they present a hardware architecture leveraging linear-feedback shift registers (LFSRs) to implement these sparsely-connected networks, achieving up to 90% memory savings and 84% energy reduction compared to traditional fully-connected architectures.
Decision: Accept
The paper makes a significant contribution by addressing a critical bottleneck in DNN hardware implementation. The proposed sparsely-connected networks demonstrate both theoretical and practical advancements, supported by rigorous experiments and hardware implementation results. The work is novel, well-motivated, and scientifically rigorous, making it a valuable addition to the conference.
Supporting Arguments:
1. Novelty and Relevance: The paper introduces a sparsity-driven approach to reduce memory and energy requirements in fully-connected layers, which is a pressing issue in DNN hardware implementations. The use of LFSRs for generating sparse connectivity patterns is innovative and practical for hardware design.
   
2. Experimental Validation: The authors validate their approach on three widely-used datasets (MNIST, CIFAR10, SVHN) and show that sparsely-connected networks can achieve better or comparable accuracy with significantly fewer connections. The results are benchmarked against state-of-the-art methods, including binarized and ternarized networks, further strengthening the claims.
3. Hardware Implementation: The proposed VLSI architecture is described in detail, and the ASIC synthesis results demonstrate substantial reductions in energy consumption and silicon area. This practical aspect of the work enhances its impact and applicability.
4. Scientific Rigor: The paper provides a clear training algorithm, detailed experimental setup, and comprehensive comparisons with existing methods, ensuring reproducibility and transparency.
Suggestions for Improvement:
1. Clarity on Limitations: While the paper mentions the potential of the proposed network as a regularizer to prevent overfitting, it does not explicitly discuss limitations, such as potential trade-offs in latency or scalability to larger datasets and architectures. Including these would provide a more balanced perspective.
2. Comparison with More Recent Works: The paper primarily compares its results with older methods (e.g., BinaryConnect, TernaryConnect). It would be beneficial to include comparisons with more recent sparsity-driven techniques or hardware-efficient DNN models.
3. Impact on Training Time: The paper does not discuss the impact of sparsity on training time or convergence behavior. Including insights or metrics on training efficiency would strengthen the practical relevance of the approach.
4. Broader Applicability: While the paper focuses on fully-connected layers, it would be helpful to discuss the potential extension of the sparsity approach to convolutional layers or other architectures.
Questions for the Authors:
1. How does the proposed sparsity approach affect the convergence speed and training time compared to fully-connected networks?
2. Can the LFSR-based sparsity generation method be adapted for convolutional layers or other DNN components?
3. Have you evaluated the robustness of the sparsely-connected networks to adversarial attacks or noisy inputs, given their reduced connectivity?
4. How does the proposed hardware architecture scale with larger DNN models, such as those used in modern NLP or vision tasks?
In conclusion, the paper addresses a critical issue in DNN hardware design with a novel and well-supported approach. While there are areas for improvement, the contributions are significant and merit acceptance.