The paper proposes a novel framework for deep multi-task representation learning (DMTRL) that generalizes matrix factorization-based multi-task learning (MTL) approaches to tensor factorization for deep neural networks (DNNs). The key contribution is an end-to-end method that automatically learns cross-task sharing structures at every layer of a DNN, eliminating the need for user-defined sharing strategies. The framework applies to both homogeneous and heterogeneous MTL settings and demonstrates improved performance over traditional shallow MTL methods and user-defined deep MTL architectures. The authors validate their approach through experiments on MNIST, AdienceFaces, and Omniglot datasets, showing superior accuracy and reduced design complexity.
Decision: Accept
Key Reasons for Decision:
1. Novelty and Contribution: The paper introduces a significant innovation by extending shallow MTL methods to deep networks using tensor factorization. This approach addresses a critical limitation in conventional MTL by automating the sharing structure, reducing the reliance on manual trial-and-error design.
2. Empirical Validation: The experimental results convincingly demonstrate the efficacy of the proposed method across diverse datasets and tasks, outperforming both single-task learning (STL) and user-defined MTL architectures, particularly in low-data regimes.
Supporting Arguments:
- The proposed framework is well-motivated, building on established MTL techniques and addressing their limitations in the context of deep learning. The use of tensor factorization for parameter sharing is a natural and effective extension of matrix-based methods.
- The experiments are thorough, covering both homogeneous and heterogeneous MTL settings. The results consistently show that DMTRL achieves lower error rates compared to STL and user-defined MTL, particularly in scenarios with limited training data.
- The paper provides a detailed methodology for implementing the framework, including initialization strategies and rank selection for tensor factorization, which enhances reproducibility.
- The analysis of learned sharing structures (e.g., cosine similarity of task-specific parameters) provides valuable insights into the model's behavior and its ability to adapt sharing across layers.
Additional Feedback:
1. Clarity: While the methodology is well-explained, the paper could benefit from a more concise presentation of the tensor factorization techniques (e.g., Tucker and Tensor Train decompositions). A visual illustration of these methods would aid understanding.
2. Comparison with Baselines: The comparison with classic shallow MTL methods is appreciated, but the discussion could be expanded to highlight why deep approaches are inherently better suited for tasks like Omniglot and AdienceFaces.
3. Limitations: The paper does not explicitly discuss potential limitations, such as computational overhead introduced by tensor factorization or scalability to very large datasets. Addressing these points would strengthen the contribution.
4. Future Work: The authors could explore extending the framework to unsupervised or semi-supervised MTL settings, which would broaden its applicability.
Questions for Authors:
1. How does the computational cost of DMTRL compare to user-defined MTL architectures, particularly during training?
2. Can the proposed framework handle tasks with highly imbalanced datasets, and if so, how does it adapt the sharing structure in such cases?
3. Have you explored the impact of different activation functions or network architectures on the performance of DMTRL?
Overall, the paper presents a significant advancement in multi-task learning for deep networks, with strong experimental evidence and practical utility. Minor clarifications and additional discussions would further enhance its impact.