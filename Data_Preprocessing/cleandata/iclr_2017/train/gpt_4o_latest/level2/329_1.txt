The paper proposes an augmented training procedure for Generative Adversarial Networks (GANs) that incorporates a denoising auto-encoder to guide the generator towards more probable configurations of discriminator features. The authors claim that this approach improves the stability of GAN training, enhances the diversity and "objectness" of generated samples, and mitigates common GAN issues like mode collapse. The method is evaluated on CIFAR-10, STL-10, and ImageNet datasets, showing qualitative and quantitative improvements, particularly in Inception scores. The paper also highlights the robustness of the proposed method and its potential for integration with other GAN extensions.
Decision: Accept
Key reasons for this decision include:  
1. Novelty and Contribution: The use of a denoising auto-encoder to estimate feature distributions and guide the generator is a novel and well-motivated extension to GAN training. This approach addresses long-standing challenges in GANs, such as mode collapse and poor performance on diverse datasets.  
2. Empirical Validation: The proposed method is rigorously evaluated on multiple datasets, with clear improvements in Inception scores and qualitative results. The experiments are comprehensive and demonstrate the method's effectiveness.  
Supporting Arguments:
1. Claims and Support: The paper's claims are well-supported by empirical results. The improvement in Inception scores, particularly on CIFAR-10 and STL-10, demonstrates the method's efficacy. The qualitative analysis also shows that the generated samples exhibit better "objectness" and diversity.  
2. Usefulness: The proposed method is practically useful for researchers and practitioners working on GANs, as it provides a straightforward way to improve training stability and sample quality without requiring labeled data.  
3. Field Knowledge and Literature: The paper demonstrates a strong understanding of GAN literature, referencing key works like Goodfellow et al. (2014), Salimans et al. (2016), and Radford et al. (2015). The proposed method builds upon and extends these foundational ideas in a meaningful way.  
4. Limitations and Future Work: The authors acknowledge the limitations of their approach, such as the non-stationarity of the feature distribution and the potential for stale gradients. They also outline promising directions for future work, which adds depth to the discussion.  
Additional Feedback:
1. Clarity: While the paper is generally well-written, some sections, particularly the mathematical derivations, could benefit from clearer explanations for readers less familiar with denoising auto-encoders or feature matching.  
2. Comparison with Baselines: Although the paper compares its method to Salimans et al. (2016) and Radford et al. (2015), a more detailed analysis of how the proposed method performs relative to other recent GAN improvements (e.g., Wasserstein GANs) would strengthen the evaluation.  
3. Reproducibility: The authors should provide more details about hyperparameter tuning and architectural choices to ensure reproducibility. While the paper mentions robustness to hyperparameter changes, specific guidelines would be helpful.  
Questions for the Authors:
1. How does the proposed method scale to higher resolutions (e.g., 128x128 or 256x256)? Have you tested its performance in such settings?  
2. Could the denoising auto-encoder introduce additional computational overhead? If so, how does this compare to the benefits in sample quality?  
3. Have you explored the impact of different corruption functions (e.g., structured noise) on the denoising auto-encoder's performance?  
In conclusion, the paper presents a novel and impactful contribution to GAN research. While there are areas for improvement in clarity and additional comparisons, the method's demonstrated effectiveness and potential for broader application justify its acceptance.