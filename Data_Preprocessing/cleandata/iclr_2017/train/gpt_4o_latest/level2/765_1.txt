Review of the Paper
The paper presents a novel approach to model-based reinforcement learning (RL) by jointly learning system dynamics and reward prediction in high-dimensional visual state spaces, specifically in the context of Atari games. The authors propose an extension to the video frame prediction model by Oh et al. (2015), incorporating a reward prediction mechanism into the network architecture. The empirical results demonstrate the feasibility of predicting cumulative rewards up to 200 frames in five Atari games, with varying levels of accuracy. This work addresses a key challenge in RL—data efficiency—by leveraging model-based techniques in environments with initially unknown dynamics and reward structures.
Decision: Accept
The primary reasons for this decision are the paper's significant contribution to model-based RL and its rigorous empirical evaluation. The proposed method demonstrates a clear improvement in cumulative reward prediction over baseline models, and the joint learning of state and reward dynamics represents a meaningful step forward for RL in complex environments.
Supporting Arguments
1. Novelty and Contribution: The paper introduces a novel extension to existing video frame prediction models by enabling joint prediction of states and rewards. This approach is innovative and addresses a critical gap in model-based RL, particularly in environments with unknown reward functions. The results provide evidence that this method can improve data efficiency, a key limitation of model-free RL approaches.
2. Empirical Rigor: The authors evaluate their model on five diverse Atari games, providing both quantitative and qualitative analyses. The results are robust, with detailed error analysis and insights into the limitations of the approach. The comparison to baseline models further strengthens the validity of the claims.
3. Relevance to the Field: The work is well-situated in the literature, building on and extending prior research in video frame prediction and model-based RL. The references are comprehensive and relevant, demonstrating a strong understanding of the field.
Additional Feedback
1. Clarity of Presentation: While the paper is technically sound, the presentation could be improved. The network architecture and training procedure are described in detail, but the explanation is dense and may be challenging for readers unfamiliar with the underlying methods. Simplifying the description or including a high-level diagram could enhance accessibility.
2. Limitations and Future Work: The paper acknowledges its limitations, such as performance variability across games and challenges with stochastic transitions. However, it would benefit from a more detailed discussion of how these limitations could be addressed in future work. For example, the proposed use of dropout or variational autoencoders for handling non-deterministic transitions could be elaborated upon.
3. Broader Applicability: While the results are promising, the paper focuses exclusively on Atari games. It would be valuable to discuss how the approach could generalize to other domains, such as robotics or real-world applications with sparse rewards.
Questions for the Authors
1. How does the proposed method scale to environments with significantly higher-dimensional state spaces or more complex reward structures? 
2. Could the model's performance in games like Seaquest be improved by incorporating additional mechanisms to handle stochastic transitions, such as attention mechanisms or hierarchical representations?
3. How does the choice of the reward weight (λ) impact the trade-off between video frame reconstruction and reward prediction in more complex environments?
Conclusion
This paper presents a meaningful contribution to the field of reinforcement learning by advancing model-based approaches in high-dimensional visual environments. While there are areas for improvement, the novelty, empirical rigor, and potential for future extensions make it a valuable addition to the conference. I recommend acceptance.