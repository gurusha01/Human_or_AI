Review of the Paper: "Exponential Machines (ExM): Modeling All Interactions of Every Order"
The paper introduces Exponential Machines (ExM), a novel machine learning model designed to efficiently model all interactions of every order in high-dimensional data. The authors leverage the Tensor Train (TT) format to represent an exponentially large tensor of parameters compactly, enabling scalability and regularization. The paper claims three key contributions: (1) the development of ExM as a scalable predictor for high-order interactions, (2) a stochastic Riemannian optimization procedure for efficient training, and (3) empirical validation demonstrating state-of-the-art performance on synthetic and real-world datasets.
Decision: Accept
The paper presents a significant methodological innovation with strong theoretical grounding and empirical validation. The use of the TT format to address the computational challenges of modeling high-order interactions is novel and impactful. The proposed Riemannian optimization procedure is well-motivated and outperforms standard baselines in experiments. However, there are areas where the paper could be improved, particularly in terms of clarity and addressing certain limitations.
Supporting Arguments:
1. Novelty and Contribution: The use of the TT format to model exponentially large tensors is a clear innovation over existing high-order factorization methods. The paper also extends the model to handle interactions between functions of features, which broadens its applicability.
2. Empirical Validation: The experiments convincingly demonstrate the superiority of the proposed method on synthetic datasets with high-order interactions and competitive performance on real-world datasets (e.g., MovieLens 100K). The comparison with baselines like Factorization Machines (FM) and neural networks is thorough.
3. Theoretical Rigor: The paper provides detailed proofs for key claims, such as the computational complexity of inference and the initialization strategy. This adds credibility to the proposed approach.
Areas for Improvement:
1. Clarity of Presentation: While the theoretical sections are rigorous, they may be difficult for readers unfamiliar with tensor algebra. Simplifying or providing more intuitive explanations of the TT format and Riemannian optimization could improve accessibility.
2. Limitations and Future Work: The paper acknowledges that the Riemannian optimizer does not support sparse data, which limits scalability for datasets with hundreds of thousands of features. The authors could discuss potential solutions or ongoing work to address this limitation.
3. Initialization Sensitivity: The experiments highlight the sensitivity of the training process to initialization. While the proposed linear model-based initialization is effective, further exploration of initialization strategies could strengthen the robustness of the method.
Questions for the Authors:
1. How does the proposed method perform on datasets with extremely high sparsity, such as those encountered in natural language processing or large-scale recommender systems?
2. Could the Riemannian optimization procedure be adapted to handle sparse data more efficiently? If so, what challenges need to be addressed?
3. The paper mentions that the TT-rank is a hyperparameter. Have you explored automated methods for selecting the TT-rank, such as adaptive rank selection during training?
Additional Feedback:
- The inclusion of a Python implementation is commendable and will facilitate reproducibility. However, providing more detailed documentation or examples for practitioners would enhance usability.
- The comparison with concurrent work (e.g., Stoudenmire & Schwab, 2016) is appreciated, but a deeper discussion of the trade-offs between ExM and these methods would be beneficial.
In conclusion, this paper makes a strong contribution to the field of machine learning by addressing a challenging problem with an innovative and well-supported solution. While there are areas for improvement, the strengths of the work outweigh its limitations, and I recommend acceptance.