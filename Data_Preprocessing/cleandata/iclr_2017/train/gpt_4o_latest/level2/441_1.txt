Review of the Paper
This paper introduces Variable Computation Recurrent Neural Networks (VCRNN) and Variable Computation Gated Recurrent Units (VCGRU), which adaptively adjust the amount of computation performed at each time step in sequential modeling tasks. The authors claim that these models reduce computational costs while improving predictive performance, particularly for tasks with varying temporal dynamics, such as music modeling, bit-level language modeling, and character-level language modeling. The paper also demonstrates that the proposed models can learn meaningful time patterns in the data without explicit supervision.
Decision: Accept
The paper presents a novel and well-motivated approach to adaptive computation in recurrent neural networks, supported by rigorous experimental results. The key reasons for this decision are: (1) the clear improvement in computational efficiency and predictive performance over baseline models, and (2) the ability of the proposed models to learn interpretable time dynamics, which aligns with the paper's claims.
Supporting Arguments:
1. Novelty and Contribution: The paper makes a significant contribution by extending the Elman RNN and GRU architectures to include adaptive computation. This is a meaningful advancement over existing approaches, such as fixed-schedule or multi-layer RNNs, and demonstrates a clear improvement in efficiency and performance.
2. Experimental Validation: The authors provide extensive experimental results across diverse tasks (e.g., music, language modeling) and datasets (e.g., Penn Treebank, Text8, Europarl). The results consistently show that VCRNN and VCGRU outperform their constant-computation counterparts in terms of perplexity and computational efficiency.
3. Interpretability: The scheduler's ability to learn time patterns (e.g., focusing on word boundaries in text or fast passages in music) is a valuable feature that enhances the interpretability of the models. This is particularly interesting for tasks where temporal dynamics are critical.
4. Positioning in Literature: The paper demonstrates a strong understanding of prior work, including multi-scale sequence modeling and adaptive computation paradigms. The proposed approach is well-positioned within this context and addresses a clear gap in the literature.
Suggestions for Improvement:
1. Reproducibility: While the paper provides a detailed description of the models, additional implementation details (e.g., hyperparameters, training schedules) would enhance reproducibility. Including code or pseudocode for the scheduler and soft mask mechanism would be particularly helpful.
2. Comparison with LSTMs: The paper mentions that the proposed approach could be extended to LSTMs, but no experiments are provided. A comparison with LSTMs would strengthen the claims and demonstrate the generalizability of the method.
3. Real-World Applications: While the experiments are diverse, the paper could benefit from showcasing a real-world application (e.g., speech recognition or video analysis) to highlight the practical utility of the proposed models.
4. Limitations: The paper briefly mentions that computational complexity gains do not necessarily translate to training speed-ups on GPUs. A more detailed discussion of this limitation, along with potential solutions, would provide a balanced perspective.
Questions for the Authors:
1. How sensitive are the results to the choice of the sharpness parameter (λ) and the target computation budget (m̄)? Could these parameters be learned dynamically during training?
2. Have you explored the impact of the proposed models on downstream tasks (e.g., classification or generation) beyond language modeling?
3. Could the scheduler be extended to incorporate external supervision signals (e.g., linguistic or musical annotations) to further enhance performance?
In conclusion, this paper presents a compelling and innovative approach to adaptive computation in RNNs. While there are areas for further exploration, the contributions are significant, and the results are well-supported. I recommend acceptance.