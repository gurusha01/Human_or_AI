Review of "Fine Grained Action Repetition (FiGAR) for Deep Reinforcement Learning"
Summary of Contributions:
The paper proposes a novel framework, Fine Grained Action Repetition (FiGAR), which enables reinforcement learning (RL) agents to dynamically decide both the action to execute and the duration for which the action is repeated. This approach introduces temporal abstractions into the action space, potentially improving the efficiency and performance of deep reinforcement learning (DRL) algorithms. The authors demonstrate FiGAR's effectiveness by integrating it with three policy gradient methods—A3C, TRPO, and DDPG—and empirically validate its performance across three domains: Atari 2600 games, MuJoCo physics tasks, and TORCS car racing. The results show significant improvements in performance and smoother policies in many cases. The paper also explores the impact of different action repetition sets and highlights the importance of temporal abstractions.
Decision: Accept
Key Reasons:
1. Novelty and Generality: FiGAR introduces a structured, factored policy representation that decouples action selection from action repetition, avoiding the action-space explosion seen in prior work. The framework is generic and applicable to both discrete and continuous action spaces.
2. Empirical Validation: The authors provide extensive experimental results across diverse domains, demonstrating FiGAR's ability to improve performance and learn meaningful temporal abstractions.
Supporting Arguments:
1. Claims and Evidence: The paper makes two primary claims: (1) FiGAR provides a lightweight extension to DRL algorithms for learning temporal abstractions, and (2) it improves performance across multiple domains. These claims are well-supported by statistically significant results. For instance, FiGAR-A3C outperforms A3C in 26 out of 33 Atari games, with dramatic improvements in some cases (e.g., 900× in Enduro). FiGAR-DDPG achieves a 9× improvement in TORCS, and FiGAR-TRPO shows competitive performance in MuJoCo tasks.
2. Relevance to Literature: The paper situates FiGAR within the context of related work on temporal abstractions, macro-actions, and parameterized policies. It highlights limitations of prior approaches, such as static action repetition or action-space explosion, and demonstrates how FiGAR addresses these issues.
3. Practical Usefulness: The ability to dynamically adjust action repetition is particularly valuable for real-time applications, where computational efficiency and smooth control are critical. The framework's generality makes it applicable to a wide range of RL problems.
Suggestions for Improvement:
1. Limitations and Future Work: While the paper acknowledges that FiGAR is designed for deterministic environments, it would benefit from a more detailed discussion of its limitations in stochastic settings. The authors briefly mention the need for "stop" and "start" actions to handle unexpected changes but do not explore this further. Including preliminary experiments or theoretical insights on this aspect would strengthen the paper.
2. Hyperparameter Sensitivity: The choice of action repetition set \( W \) and other hyperparameters (e.g., entropy regularization) appears somewhat arbitrary. A deeper analysis of how these choices affect performance and generalizability would be helpful.
3. Clarity in Presentation: The paper is dense and could benefit from more concise explanations in sections like Related Work and Experimental Setup. Additionally, the figures and tables could be better integrated into the main text to improve readability.
Questions for the Authors:
1. How does FiGAR perform in highly stochastic environments, and what modifications would be necessary to handle such settings effectively?
2. Did you observe any trade-offs between exploration and exploitation due to the introduction of the action repetition policy? How does this impact training stability?
3. Can FiGAR be extended to hierarchical RL frameworks, where temporal abstractions are learned at multiple levels (e.g., combining FiGAR with STRAW)?
Conclusion:
FiGAR is a well-motivated and innovative contribution to the field of reinforcement learning. Its ability to introduce temporal abstractions while maintaining computational efficiency is a significant step forward. Although there are areas for further exploration, the paper's strong empirical results and theoretical grounding make it a valuable addition to the conference.