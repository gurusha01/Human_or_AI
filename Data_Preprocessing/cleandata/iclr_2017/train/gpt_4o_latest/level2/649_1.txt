Review of the Paper
The paper investigates the critical question of what constitutes the "best" definition of context for learning word embeddings, a topic of significant importance in natural language processing (NLP). The authors claim to provide the first systematic exploration of different context types and representations, evaluating their effectiveness across four tasks using 21 datasets. The paper also promises to offer insights into context selection and provides published code to aid reproducibility and community use. This is a timely and relevant contribution, given the growing number of word embedding models and the lack of consensus on optimal context definitions.
Decision: Reject
While the paper addresses an important and underexplored problem, the decision to reject is based on two primary reasons: (1) insufficient novelty in the experimental setup and (2) lack of rigorous support for the claims made. Below, I provide a detailed rationale for this decision.
Supporting Arguments
1. Novelty: The paper claims to be the first systematic investigation of context types for word embeddings. However, the literature review does not sufficiently differentiate this work from prior studies that have explored context windows, dependency-based contexts, and other variations. The authors should better position their work in relation to existing research and clarify what new insights their study contributes beyond what is already known.
2. Support for Claims: While the authors conduct experiments across four tasks and 21 datasets, the results are presented without sufficient statistical rigor. For example, there is no mention of statistical significance testing to validate the observed differences between context types. Additionally, the paper does not adequately explain why certain context types perform better or worse, leaving the reader with descriptive results rather than actionable insights.
3. Reproducibility: Although the authors provide code, the paper lacks sufficient methodological details for full reproducibility. For instance, the exact hyperparameter settings, preprocessing steps, and evaluation metrics are not described in detail.
Additional Feedback
1. Contextual Motivation: The paper would benefit from a clearer motivation for why specific context types were chosen for evaluation. Are these types inspired by linguistic theory, prior empirical findings, or practical considerations?
2. Analysis of Results: The authors should include a deeper analysis of why certain context types perform better for specific tasks. This would make the insights more actionable for practitioners.
3. Acknowledgment of Limitations: The paper does not discuss its limitations, such as the potential bias introduced by the choice of datasets or tasks. Acknowledging these would strengthen the paper's credibility.
Questions for the Authors
1. How do the proposed context types differ from those explored in prior work? Can you provide a more detailed comparison to existing approaches?
2. Were statistical significance tests conducted to validate the results? If not, could you include them in a revised version?
3. Could you elaborate on the criteria used to select the four tasks and 21 datasets? How representative are they of real-world applications?
Conclusion
While the paper addresses an important problem and has potential, it falls short in terms of novelty, rigor, and depth of analysis. I encourage the authors to revise the paper by addressing the above concerns, as the topic is valuable to the NLP community.