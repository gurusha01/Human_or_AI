Review of the Paper: "Sample Importance in Training Deep Neural Networks"
Summary
This paper investigates the concept of "sample importance" in the context of training deep neural networks using stochastic gradient descent (SGD). The authors define sample importance as the change in model parameters induced by a sample during training and analyze its behavior across training stages and network layers. Key findings include: (1) "Easy" samples primarily influence top-layer parameters during early training, while "hard" samples shape bottom-layer parameters in later stages; (2) mixing easy and hard samples in training batches improves performance compared to homogeneous batches; and (3) the results challenge curriculum learning paradigms, which advocate for training on easy samples first. The authors also propose a quantitative framework for measuring sample importance and conduct empirical evaluations on MNIST and CIFAR-10 datasets. The paper concludes with a discussion on potential extensions, such as applying sample importance to other architectures and using it to identify minimal training subsets.
Decision: Accept
The paper makes a novel and well-supported contribution to understanding sample importance in deep learning, challenging established practices like curriculum learning. The empirical results are thorough, and the proposed framework has the potential to inspire future research in training optimization and interpretability.
Supporting Arguments
1. Novelty and Contribution: The concept of sample importance, as defined and analyzed in this paper, is a novel addition to the field. The finding that mixing easy and hard samples improves training performance contradicts prior work on curriculum learning, providing a fresh perspective on training strategies.
2. Empirical Rigor: The experiments are well-designed and conducted on standard datasets (MNIST and CIFAR-10) using a clear methodology. The authors provide detailed analyses of sample importance across layers, epochs, and batch configurations, supporting their claims with statistically significant results.
3. Practical Implications: The findings have practical relevance for optimizing training strategies in deep learning. For instance, the recommendation to mix hard and easy samples in batches could be directly applied to improve training efficiency and model performance.
Additional Feedback
1. Clarity of Presentation: While the paper is generally well-written, some sections, particularly the derivations in Section 4, are dense and may benefit from additional explanation or visual aids to improve accessibility for a broader audience.
2. Scope of Evaluation: The experiments are limited to fully connected networks on MNIST and CIFAR-10. Extending the analysis to other architectures (e.g., CNNs, RNNs) and datasets would strengthen the generalizability of the findings.
3. Comparison with Curriculum Learning: While the paper challenges curriculum learning, it would benefit from a more detailed discussion of why the observed results diverge from prior work. For example, are the differences due to dataset characteristics, network architecture, or other factors?
4. Reproducibility: The authors provide sufficient details for reproducibility, but sharing the code and data preprocessing scripts would further enhance transparency and encourage adoption of the proposed methods.
Questions for the Authors
1. How does the definition of sample importance generalize to architectures like CNNs or RNNs, where parameters are shared across layers?
2. Did you observe any dataset-specific trends in the behavior of sample importance (e.g., differences between MNIST and CIFAR-10)?
3. Can the proposed framework for measuring sample importance be computationally optimized for larger models or datasets?
4. How sensitive are the findings to hyperparameter choices, such as learning rate or batch size?
Conclusion
This paper provides a novel and well-supported contribution to understanding sample importance in deep learning, with practical implications for training strategies. While some aspects could be expanded or clarified, the work is of high quality and merits acceptance.