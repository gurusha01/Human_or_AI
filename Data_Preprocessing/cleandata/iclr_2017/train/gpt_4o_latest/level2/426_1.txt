Review of the Paper
This paper presents a novel approach to offline bilingual word vector alignment by proving that the optimal transformation between two vector spaces should be orthogonal and can be derived using Singular Value Decomposition (SVD). The authors introduce an "inverted softmax" to address the hubness problem in translation tasks and demonstrate significant improvements in translation precision over existing methods. Additionally, the paper explores the use of pseudo-dictionaries (constructed from identical character strings) and aligned sentence corpora to achieve competitive results without relying on expert bilingual signals. The authors also extend their method to sentence translation, achieving impressive precision in retrieving sentence translations from a large corpus.
Decision: Accept
The paper makes a strong theoretical and practical contribution to the field of bilingual word embeddings. The key reasons for acceptance are: (1) the clear theoretical proof of the orthogonal transformation's optimality, which unifies existing methods, and (2) the substantial empirical improvements in translation precision, particularly when using pseudo-dictionaries and sentence alignment. These contributions are both novel and impactful, addressing significant limitations in prior work.
Supporting Arguments
1. Theoretical Contribution: The paper rigorously proves that the optimal linear transformation between vector spaces should be orthogonal, providing a unifying framework for existing methods. This theoretical insight is a valuable addition to the literature and is supported by a clear derivation and explanation.
2. Empirical Results: The proposed method achieves state-of-the-art results in word translation tasks, improving precision from 34% to 43% compared to Mikolov's method. The use of pseudo-dictionaries and sentence alignment demonstrates the robustness and versatility of the approach, achieving 40% precision without expert bilingual signals and 68% precision in sentence translation tasks.
3. Practical Utility: The ability to align bilingual word vectors without expert bilingual data or extensive computational resources is a significant step forward. This makes the method accessible and applicable to low-resource languages or scenarios where expert data is unavailable.
4. Novelty: The introduction of the inverted softmax to mitigate the hubness problem is a novel and effective contribution, addressing a well-known issue in translation tasks.
Additional Feedback
1. Clarity: While the theoretical sections are well-detailed, the paper could benefit from a more concise explanation of the SVD process and its connection to prior methods like CCA. A diagram or flowchart summarizing the steps of the proposed method would enhance readability.
2. Limitations: The paper does not explicitly discuss the limitations of the approach, such as its reliance on high-quality monolingual word embeddings or its potential challenges with non-European languages. Acknowledging these limitations would strengthen the paper.
3. Sentence Vectors: While the results for sentence translation are impressive, the paper uses simple sentence vectors (summing and normalizing word vectors). It would be interesting to explore how more sophisticated sentence embeddings (e.g., transformer-based methods) might integrate with the proposed framework.
4. Computational Complexity: The authors note that CCA becomes computationally expensive with large vocabularies, but they do not provide a detailed comparison of the computational efficiency of their method relative to others. Including such a comparison would improve the practical evaluation of the method.
Questions for the Authors
1. How does the method perform with languages that have less overlap in vocabulary (e.g., non-European languages)? Would the pseudo-dictionary approach still be viable?
2. The paper mentions that dimensionality reduction improves translation precision. Could you provide more details on how the optimal number of dimensions is determined?
3. Have you considered extending the inverted softmax to sentence translation tasks, and if so, what were the results?
4. How sensitive is the method to the quality of the initial monolingual embeddings? Would embeddings trained on smaller or noisier corpora significantly degrade performance?
Conclusion
This paper makes a significant contribution to the field of bilingual embeddings by providing both theoretical insights and practical advancements. The proposed method is robust, versatile, and achieves state-of-the-art results in multiple tasks. While there are areas for further exploration, the paper is well-positioned to have a meaningful impact on the community. I recommend acceptance.