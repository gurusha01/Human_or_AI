The paper investigates the use of sinusoidal activation functions in neural networks, a topic that has received limited attention in the literature. The authors provide a theoretical analysis of the challenges associated with training such networks, including the emergence of infinitely many shallow local minima and the presence of deep local minima. They also explore the conditions under which sinusoidal activations can be advantageous, particularly for algorithmic tasks. Through experiments on MNIST, Reuters, and algorithmic tasks, the authors demonstrate that while sinusoidal activations are often ignored in classification tasks, they can outperform traditional monotonic activation functions (e.g., tanh) in tasks where periodicity is beneficial.
Decision: Accept with Minor Revisions
The paper makes a compelling case for revisiting sinusoidal activation functions, offering both theoretical insights and empirical evidence. However, there are areas where the paper could be improved for clarity and completeness.
Supporting Arguments:
1. Novelty and Contribution: The paper addresses an underexplored area in neural network research, providing a detailed theoretical characterization of the loss landscape for sinusoidal activations. The experiments on algorithmic tasks highlight a unique advantage of these functions, which could inspire further research.
2. Experimental Rigor: The authors conduct a variety of experiments across different tasks and architectures (DNNs, CNNs, RNNs, LSTMs), providing a well-rounded evaluation of sinusoidal activations. The use of both classification and algorithmic tasks strengthens the paper's claims.
3. Practical Insights: The paper offers actionable insights, such as the importance of weight initialization and the conditions under which sinusoidal activations are likely to succeed. This makes the work useful for practitioners.
Areas for Improvement:
1. Clarity in Theoretical Analysis: While the theoretical analysis is thorough, some sections (e.g., the derivation of the loss landscape) are dense and may be difficult for readers unfamiliar with the mathematical framework. Adding more intuitive explanations or visual aids could improve accessibility.
2. Limited Scope of Tasks: The paper focuses heavily on algorithmic tasks to demonstrate the utility of sinusoidal activations. While this is a strength, additional experiments on real-world tasks where periodicity might play a role (e.g., signal processing or time-series forecasting) would make the results more broadly applicable.
3. Comparison with Other Non-Monotonic Functions: The paper compares sinusoidal activations primarily with tanh. Including comparisons with other non-monotonic or periodic functions (e.g., Gaussian or Fourier-based activations) would provide a more comprehensive evaluation.
4. Acknowledgment of Limitations: While the paper discusses the challenges of training sinusoidal networks, it could benefit from a more explicit acknowledgment of scenarios where these activations are unlikely to be useful, such as tasks with no inherent periodicity.
Questions for the Authors:
1. How do sinusoidal activations perform on tasks with inherent periodicity, such as time-series forecasting or signal processing? Could these be included in future experiments?
2. Have you explored hybrid architectures that combine sinusoidal and monotonic activations? If so, how do they compare to purely sinusoidal or monotonic networks?
3. Could the observed advantages of sinusoidal activations in algorithmic tasks be attributed to specific properties of the tasks (e.g., periodic input-output relationships)? How generalizable are these findings?
Additional Feedback:
- The curriculum learning experiments are a valuable addition but could be better contextualized. For example, how does curriculum learning compare to other regularization techniques in this setting?
- The paper could benefit from a more detailed discussion of the implications of its findings for other domains, such as reinforcement learning or physics-informed neural networks.
In conclusion, the paper offers a novel and well-supported exploration of sinusoidal activation functions. With minor revisions to improve clarity and broaden the scope of experiments, it would make a strong contribution to the field.