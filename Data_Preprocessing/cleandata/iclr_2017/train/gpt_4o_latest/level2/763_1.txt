The paper addresses the problem of modeling relational time series, where observations are correlated both within and across multiple series. The authors propose a novel Relational Dynamic model with Gaussian representations (RDG), which combines latent Gaussian embeddings with a dynamic state space model to capture temporal and relational dependencies. The model explicitly accounts for uncertainty at both the observation and dynamic levels, providing confidence intervals for predictions. The paper claims two main contributions: (i) a new dynamical model for relational time series inspired by representation learning, and (ii) a stochastic component for modeling uncertainties. The authors demonstrate the effectiveness of RDG through experiments on four datasets, comparing it against state-of-the-art baselines.
Decision: Accept
Key Reasons for Decision:
1. Novelty and Contribution: The paper introduces a novel approach to relational time series forecasting by combining latent Gaussian embeddings with a dynamic state space model. The explicit modeling of uncertainty is a significant improvement over existing deterministic models.
2. Empirical Validation: The experimental results demonstrate that RDG outperforms or matches state-of-the-art baselines on multiple datasets. The ability to provide confidence intervals further enhances its practical utility.
Supporting Arguments:
- The paper is well-motivated, addressing a clear gap in the literature where relational dependencies and uncertainty are often overlooked in time series modeling. The related work section is comprehensive, situating the proposed model within the context of spatio-temporal statistics and representation learning.
- The experimental results are robust, with comparisons against five baselines, including classical statistical methods (AR, KF) and modern machine learning approaches (RNN, DFG). The use of multiple datasets from diverse domains (e.g., traffic, flu trends) strengthens the generalizability of the findings.
- The inclusion of structural regularization to incorporate relational dependencies is a thoughtful addition, and the ablation study confirms its positive impact on performance.
Suggestions for Improvement:
1. Clarity of Presentation: While the technical details are thorough, the paper could benefit from a clearer explanation of the loss functions and their role in the learning process. For instance, the distinction between ∆De1 and ∆De2 could be made more intuitive for readers unfamiliar with the nuances of Gaussian embeddings.
2. Scalability Discussion: The paper briefly mentions the use of ADAM and GPU optimization but does not discuss the computational complexity of RDG compared to baselines. A deeper analysis of scalability, especially for large-scale datasets, would be valuable.
3. Broader Applications: While the paper focuses on forecasting, the authors mention imputation tasks as a potential extension. Including preliminary results or a discussion on how RDG could be adapted for imputation would strengthen the paper's impact.
Questions for the Authors:
1. How sensitive is the model to the choice of hyperparameters (e.g., λDy, λR)? Did you observe any significant trade-offs between the different components of the loss function?
2. Can the model handle dynamic (time-varying) relational graphs, and if so, how would this affect the training process?
3. How does RDG perform on datasets with sparse or noisy relational graphs? Would the structural regularization term still provide meaningful improvements in such cases?
Overall, the paper presents a well-executed and innovative approach to relational time series forecasting. With minor improvements in clarity and additional discussion on scalability and broader applications, it has the potential to make a significant contribution to the field.