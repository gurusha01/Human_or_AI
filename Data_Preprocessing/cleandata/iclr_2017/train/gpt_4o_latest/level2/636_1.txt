Review
This paper introduces two key contributions to address the variance introduced by dropout in neural networks: (1) a novel weight initialization technique that adjusts for dropout rates and the compressiveness of nonlinearities, and (2) a method to re-estimate Batch Normalization (BN) variance parameters post-training to account for discrepancies between training and testing conditions. These contributions are validated through experiments on CIFAR-10, CIFAR-100, and MNIST, achieving state-of-the-art performance without data augmentation. The proposed methods are simple, computationally efficient, and show significant improvements over existing techniques like He initialization and standard Batch Normalization.
Decision: Accept
The paper presents a well-motivated and novel solution to a practical problem in training neural networks, supported by rigorous theoretical derivations and strong empirical results. The simplicity and generality of the proposed methods make them highly applicable to the broader machine learning community.
Supporting Arguments:
1. Novelty and Significance: The proposed weight initialization technique is a meaningful improvement over existing methods, such as He and Xavier initializations, particularly for networks with dropout. Similarly, the BN variance re-estimation method addresses a previously overlooked issue, yielding notable accuracy gains.
2. Empirical Validation: The experiments are thorough and demonstrate the effectiveness of the methods across different datasets and architectures. For example, the paper reports significant error reductions on CIFAR-10 and CIFAR-100, surpassing state-of-the-art results without data augmentation.
3. Practical Usefulness: The methods are computationally efficient, requiring no additional forward passes or batch statistics during initialization. The BN re-estimation process is straightforward and can be easily integrated into existing workflows.
4. Theoretical Rigor: The derivations for both forward and backward variance corrections are detailed and grounded in well-established principles, enhancing the credibility of the proposed techniques.
Additional Feedback:
1. Clarity of Presentation: While the technical content is robust, certain sections (e.g., derivations in Section 2.1) could benefit from clearer explanations or visual aids to improve accessibility for a broader audience.
2. Comparison with Alternatives: The paper could further strengthen its claims by including comparisons with other recent variance stabilization methods, such as Layer Normalization or Group Normalization, to highlight the relative advantages of the proposed approach.
3. Ablation Studies: An ablation study to isolate the contributions of the forward and backward variance corrections in the weight initialization would provide deeper insights into their individual impacts.
4. Limitations: While the paper briefly mentions that the methods are less applicable to tasks with small batch sizes or online learning, a more explicit discussion of limitations and potential failure cases would be valuable.
Questions for the Authors:
1. How sensitive are the proposed methods to hyperparameter choices, such as the dropout rate or the corrective factors used in weight initialization?
2. Have you tested the BN variance re-estimation method on tasks beyond image classification, such as natural language processing or reinforcement learning, to assess its generalizability?
3. Could the proposed weight initialization technique be extended to other regularization methods, such as stochastic depth or ShakeDrop?
In conclusion, this paper makes a strong case for acceptance due to its novel contributions, rigorous validation, and practical relevance. Addressing the feedback provided could further enhance its impact and accessibility.