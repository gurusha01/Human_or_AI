Review of the Paper: "Code Completion via Big Code"
Summary of Contributions
This paper addresses the problem of code completion for dynamically typed programming languages, specifically JavaScript, by leveraging deep learning techniques. The authors propose LSTM-based models that incorporate structural information from Abstract Syntax Trees (ASTs) to predict the next non-terminal and terminal nodes in a program. The paper claims to outperform the state-of-the-art decision tree-based approaches by 3.8 percentage points for non-terminal prediction and 0.5 percentage points for terminal prediction on a JavaScript benchmark dataset. The authors also demonstrate that their models perform better on longer programs and achieve high top-5 prediction accuracy, enabling practical usability in IDEs. Additionally, the paper explores the concept of denying predictions for rare tokens and evaluates runtime performance, showing feasibility for real-time applications.
Decision: Accept
The paper makes a significant contribution to the field of code completion by introducing novel LSTM-based models that leverage structural information from ASTs. The results demonstrate clear improvements over prior work, and the paper is well-motivated, comprehensive, and scientifically rigorous. However, there are areas where the paper could be improved, as detailed below.
Supporting Arguments for Decision
1. Novelty and Significance: The paper introduces a novel approach to code completion by integrating AST structural information into LSTM-based models. This is a meaningful advancement over prior token-sequence-based methods and decision tree models.
2. Empirical Validation: The experimental results are robust, with statistically significant improvements over the state-of-the-art. The inclusion of evaluations on programs of varying lengths and top-5 accuracy metrics strengthens the claims.
3. Practical Relevance: The high top-5 accuracy (96.3% for terminal prediction) and runtime performance (16ms per query on GPU) demonstrate the potential for real-world adoption in IDEs.
4. Thoroughness: The paper provides detailed descriptions of the models, training procedures, and evaluation metrics, ensuring reproducibility.
Suggestions for Improvement
1. Comparison with Token-Based Models: While the paper focuses on AST-based approaches, it would be valuable to include a direct comparison with token-sequence-based RNN models to highlight the benefits of leveraging ASTs.
2. Ablation Studies: The paper could benefit from more detailed ablation studies to quantify the impact of specific design choices, such as the use of structural information or the joint prediction approach.
3. Error Analysis: An analysis of failure cases or common errors made by the models would provide insights into their limitations and guide future improvements.
4. Dataset Diversity: The evaluation is limited to a single JavaScript dataset. Testing on additional datasets or languages (e.g., Python) would strengthen the generalizability of the results.
5. User Studies: While the runtime performance is promising, user studies evaluating the practical impact of the proposed models in real-world IDEs would further validate the approach.
Questions for the Authors
1. How does the proposed approach perform when applied to other dynamically typed languages, such as Python? Are there any language-specific challenges?
2. Could you provide more details on how the models handle rare or unseen tokens during prediction? How does this impact the user experience in practice?
3. Have you considered integrating type inference techniques to enhance predictions for dynamically typed languages?
Conclusion
This paper presents a well-executed study on the application of deep learning to code completion, with clear improvements over existing methods. While there are areas for further exploration, the contributions are significant, and the work is likely to inspire future research in this domain. I recommend acceptance with minor revisions to address the suggestions above.