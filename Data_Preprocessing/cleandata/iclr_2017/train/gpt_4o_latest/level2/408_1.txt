The paper presents a novel multi-view approach for learning neural acoustic word embeddings, which jointly embeds acoustic sequences and their corresponding character sequences using deep bidirectional LSTMs and multi-view contrastive losses. The authors claim their method improves upon prior approaches for acoustic word discrimination and enables new tasks, such as cross-view word discrimination and word similarity evaluation. The work is motivated by the need for fixed-dimensional vector representations of speech segments that capture phonetic and orthographic similarities, with applications in speech retrieval and recognition.
Decision: Accept
The paper is well-motivated, demonstrates significant improvements over prior work, and introduces a novel multi-view embedding framework. The authors provide rigorous empirical evidence supporting their claims, including statistically significant results on acoustic word discrimination and novel cross-view tasks. The methodology is scientifically sound, and the experiments are reproducible, with the code made publicly available.
Supporting Arguments:
1. Novelty and Contribution: The multi-view approach is a significant innovation over prior single-view methods. By jointly embedding acoustic and character sequences, the method enables tasks like cross-view word discrimination, which were not addressed in earlier work. The use of cost-sensitive contrastive losses to align embedding distances with orthographic edit distances is another notable contribution.
2. Experimental Rigor: The authors evaluate their embeddings on three tasks: acoustic word discrimination, cross-view word discrimination, and word similarity. The results consistently show improvements over prior methods, with the combined objective (obj0 + obj2) achieving the best performance. The visualization of embeddings further supports the claims of improved clustering and generalization to unseen words.
3. Practical Usefulness: The embeddings are applicable to real-world tasks like spoken term detection and speech recognition. The ability to compare acoustic and orthographic embeddings directly is a valuable feature for multi-modal applications.
Additional Feedback:
1. Limitations and Future Work: While the paper acknowledges limitations, such as the reliance on orthographic labels and the lack of phonetic supervision, a more detailed discussion of potential challenges (e.g., scalability to larger vocabularies or noisy data) would strengthen the paper. Future work could explore phonetic embeddings and non-word segments, as suggested.
2. Evaluation Diversity: The evaluation focuses on intrinsic tasks like word discrimination and similarity. Including downstream tasks, such as query-by-example search or speech recognition, would provide a more comprehensive assessment of the embeddings' practical utility.
3. Negative Sampling Strategy: The paper uses random negative sampling for contrastive losses. Exploring more sophisticated sampling strategies, such as hard negative mining, could further improve performance.
Questions for Authors:
1. How does the method scale to larger vocabularies or more complex datasets with significant noise or variability in acoustic signals?
2. Could the embeddings be extended to handle non-word segments or sub-word units, and how would this impact performance on tasks like speech recognition?
3. Have you considered incorporating phonetic transcriptions as an intermediate representation to improve alignment between acoustic and orthographic embeddings?
Overall, the paper makes a strong contribution to the field of acoustic word embeddings and is a valuable addition to the conference. The proposed multi-view framework and its demonstrated improvements warrant acceptance.