The paper presents a novel approach to automating the design of optimization algorithms by formulating the problem as a reinforcement learning (RL) task. The authors propose representing optimization algorithms as policies and use guided policy search (GPS) to learn these policies. The learned optimizer, referred to as "predicted step descent," is shown to outperform hand-engineered algorithms such as gradient descent, momentum, conjugate gradient, and L-BFGS on several convex and non-convex optimization tasks. The paper claims that the learned optimizer achieves faster convergence and/or better final objective values, particularly on challenging non-convex problems like training neural networks. The authors also demonstrate the generalization of the learned optimizer to unseen objective functions and its robustness to distributional shifts.
Decision: Accept
Key reasons: (1) The paper introduces a novel and well-motivated approach to learning optimization algorithms, which has the potential to significantly impact optimization in machine learning and beyond. (2) The experimental results convincingly demonstrate the superiority of the learned optimizer over traditional methods on a range of tasks.
Supporting Arguments:
1. Novelty and Contribution: The idea of learning optimization algorithms as policies in an RL framework is innovative and represents a significant departure from traditional hand-engineered methods. The use of GPS to learn these policies is well-justified and leverages state-of-the-art techniques in RL.
2. Experimental Validation: The experiments are comprehensive, covering convex and non-convex problems, and include comparisons with widely used optimization algorithms. The learned optimizer consistently outperforms baselines in terms of convergence speed and quality of solutions, particularly on non-convex problems where traditional methods often struggle.
3. Generalization and Robustness: The paper demonstrates that the learned optimizer generalizes well to unseen objective functions and remains robust under distributional shifts, which is critical for practical applicability.
Additional Feedback:
1. Clarity and Accessibility: While the paper is technically sound, the presentation of the reinforcement learning framework and GPS could be simplified for readers less familiar with these concepts. A high-level overview or visual diagram of the learning process would enhance accessibility.
2. Ablation Studies: It would be helpful to include ablation studies to understand the impact of design choices, such as the neural network architecture for the policy or the choice of GPS over other RL methods.
3. Limitations and Future Work: The paper briefly mentions potential limitations, such as the reliance on training data distributions, but does not explore them in depth. A more detailed discussion of scenarios where the learned optimizer might fail or require retraining would be valuable. Additionally, exploring scalability to higher-dimensional problems would strengthen the contribution.
Questions for the Authors:
1. How does the learned optimizer perform on optimization problems with significantly higher dimensionality or more complex error surfaces than those tested in the paper?
2. Could the proposed approach be extended to handle constrained optimization problems, and if so, what modifications would be required?
3. How sensitive is the learned optimizer to the choice of hyperparameters during training, such as the number of trajectories or the neural network architecture?
Overall, this paper presents a promising and impactful direction for optimization research. With some refinements and additional experiments, it could become a foundational contribution to the field.