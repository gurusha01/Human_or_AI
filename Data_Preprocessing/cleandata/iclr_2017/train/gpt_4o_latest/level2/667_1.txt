The paper introduces GRAM, a graph-based attention model designed to address two key challenges in healthcare predictive modeling: data insufficiency and interpretability. By leveraging hierarchical medical ontologies, GRAM represents medical concepts as weighted combinations of their ancestors using an attention mechanism. This approach enables GRAM to generalize effectively in low-data scenarios while aligning learned representations with medical knowledge. The authors evaluate GRAM on three predictive tasks—two sequential diagnoses prediction tasks and one heart failure prediction task—demonstrating superior performance compared to baseline models, particularly for rare diseases and small datasets. GRAM also provides interpretable attention behaviors, offering insights into its decision-making process.
Decision: Accept
The paper presents a novel and well-motivated approach to improving predictive modeling in healthcare, addressing critical challenges in the field. The integration of medical ontologies through an attention mechanism is innovative and demonstrates significant improvements in prediction accuracy, particularly under data insufficiency. The results are supported by rigorous experiments, and the interpretability of GRAM adds practical value for healthcare applications.
Supporting Arguments:
1. Clear Contribution and Novelty: The paper makes a compelling case for the use of hierarchical medical ontologies in deep learning models, an approach that is both novel and impactful. The attention mechanism is well-designed to adaptively balance specificity and generality based on data availability, which is a significant improvement over existing methods.
   
2. Strong Experimental Results: The authors provide extensive empirical evidence across three tasks, showing that GRAM consistently outperforms baseline models, including RNNs and other ontology-based methods. The performance gains, particularly for rare diseases, highlight the practical utility of the approach.
3. Interpretability and Practical Relevance: The attention mechanism not only improves predictive performance but also provides interpretable insights into the model's behavior. This is a critical feature for healthcare applications, where understanding model decisions is essential.
4. Thorough Evaluation: The paper evaluates GRAM against a diverse set of baselines and provides qualitative analyses (e.g., t-SNE visualizations) to demonstrate the interpretability and alignment of learned representations with medical ontologies.
Suggestions for Improvement:
1. Scalability and Computational Overhead: While the paper acknowledges the additional training time required by GRAM, a more detailed analysis of scalability on larger datasets or more complex ontologies would strengthen the practical applicability of the method.
2. Comparison with Alternative Embedding Techniques: The authors use GloVe for initializing embeddings but mention that Skip-gram might offer additional benefits. Including results with Skip-gram initialization would provide a more comprehensive evaluation.
3. Broader Generalization: While the experiments focus on specific datasets and tasks, it would be helpful to discuss the generalizability of GRAM to other domains or types of hierarchical data beyond healthcare.
Questions for the Authors:
1. How does GRAM perform when applied to datasets with different ontology structures (e.g., SNOMED-CT vs. CCS)? Are there limitations in adapting the model to other ontologies?
2. Could the attention mechanism be extended to incorporate relationships beyond parent-child hierarchies, such as causal or treatment relationships in SNOMED-CT?
3. How does the model handle conflicting information when a medical concept has multiple ancestors with differing levels of data availability?
Overall, GRAM is a well-executed and impactful contribution to the field of healthcare predictive modeling. The paper successfully addresses critical challenges and provides a foundation for future research in leveraging hierarchical knowledge for deep learning.