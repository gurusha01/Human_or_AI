The paper proposes a novel sequence-learning framework, RL Tuner, which combines Reinforcement Learning (RL) and supervised learning to refine Recurrent Neural Networks (RNNs) for sequence generation tasks. The approach uses a pre-trained RNN to provide prior probabilities and augments deep Q-learning with music-theory-based rewards, enabling the model to generate sequences that adhere to structural rules while retaining information learned from data. The authors demonstrate the effectiveness of RL Tuner in the domain of music generation, where it significantly reduces common RNN failure modes like excessive repetition and lack of structure. The paper also introduces novel off-policy methods for KL-regularized RL and empirically compares them, showing that the proposed approach produces more pleasing melodies than baseline models.
Decision: Accept.  
Key reasons: (1) The paper presents a significant methodological contribution by combining ML and RL in a novel way, with clear connections to stochastic optimal control and KL control. (2) The empirical results convincingly demonstrate the utility of the approach in improving melody generation, supported by both quantitative metrics and a user study.
Supporting Arguments:  
1. Novelty and Contribution: The integration of RL with pre-trained RNNs to impose structural constraints is innovative. The paper extends existing RL methods (e.g., Q-learning, Ψ-learning, G-learning) to sequence modeling tasks and provides the first empirical comparison of these methods in this context. The application of KL-regularized RL to refine RNN outputs is a noteworthy contribution.  
2. Empirical Validation: The experiments are thorough, with clear metrics for evaluating adherence to music-theory rules and retention of data-driven probabilities. The user study adds credibility to the claim that the generated melodies are subjectively more pleasing.  
3. Practical Usefulness: The proposed method addresses real-world challenges in sequence generation, such as ensuring coherence and avoiding repetitive outputs. The approach is generalizable to other domains like text generation and dialogue systems.  
Additional Feedback for Improvement:  
1. Clarity of Presentation: While the paper is dense with technical details, some sections (e.g., derivations of Ψ-learning and G-learning) could benefit from a more intuitive explanation to improve accessibility for a broader audience.  
2. Ablation Studies: It would be helpful to include ablation studies to isolate the impact of individual components, such as the music-theory reward function or the choice of c parameter.  
3. Generality of Results: While the focus on music generation is compelling, additional experiments in other sequence generation domains (e.g., text or speech) would strengthen the claim of general applicability.  
4. Limitations: The paper could more explicitly discuss the limitations of the approach, such as the reliance on heuristic reward functions and the potential challenges in scaling to more complex tasks.  
Questions for the Authors:  
1. How sensitive is the model's performance to the choice of the c parameter? Could you provide guidance on how to tune this parameter for other domains?  
2. How does the approach handle cases where the reward function is poorly defined or noisy?  
3. Have you explored extending the method to polyphonic music generation or other sequence generation tasks like text generation?  
Overall, the paper makes a strong contribution to the field of sequence modeling and RL, with a well-motivated approach and promising results. Addressing the feedback above could further enhance the paper's impact.