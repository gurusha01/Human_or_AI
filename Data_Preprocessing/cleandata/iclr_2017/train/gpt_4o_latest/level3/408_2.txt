Review
The paper proposes a joint learning approach for word vector representations of character sequences and acoustic spans using a multi-view learning framework. It leverages bidirectional LSTM networks and contrastive loss functions to embed acoustic and orthographic data into a shared space. The authors claim improvements in acoustic word discrimination tasks over previous methods and demonstrate the utility of their embeddings for cross-view tasks like spoken term detection. Additionally, they explore cost-sensitive losses to align embedding distances with orthographic edit distances, providing insights into word similarity.
Decision: Reject
While the paper introduces an interesting multi-view approach and shows promise in improving acoustic word embeddings, several critical limitations undermine its contribution. The synthetic nature of the tasks, reliance on pre-segmented acoustic spans, and limited vocabulary size restrict the practical applicability of the proposed method. Furthermore, the evaluation lacks phoneme-based comparisons and does not address homophones, which are crucial for understanding how the model handles acoustic ambiguities. The absence of experiments on larger, more realistic datasets and tasks like keyword spotting in longer utterances further weakens the paper's impact.
Supporting Arguments
1. Synthetic Motivation and Tasks: The assumption of pre-segmented acoustic spans is unrealistic for real-world applications, where word boundaries are often unknown. This makes the tasks feel synthetic and limits the method's applicability to practical speech processing scenarios.
2. Limited Evaluation: The evaluation focuses primarily on character-based comparisons, neglecting phoneme-level analyses. Including phoneme string edit distance and comparisons between character and phoneme embeddings would provide a more comprehensive assessment of the model's capabilities.
3. Toy Problem Setup: The small vocabulary size and relatively simple distinctions in the dataset make this a toy problem. Stronger conclusions would require experiments on larger vocabularies and more challenging datasets.
Suggestions for Improvement
1. Phoneme-Based Analysis: Incorporate phoneme-level comparisons and evaluate phoneme string edit distances relative to acoustic embedding distances. This would provide a more nuanced understanding of the embeddings' performance.
2. Homophone Handling: Discuss and evaluate how the model handles homophones, as this is a critical challenge in acoustic word embeddings.
3. Realistic Tasks: Extend the approach to address more practical tasks, such as keyword spotting in longer utterances, where word boundaries are not pre-defined.
4. Larger Vocabulary and Dataset: Use a larger vocabulary and more diverse datasets to validate the scalability and robustness of the proposed method.
5. Comparison with Phoneme Embeddings: Comparing character embeddings with phoneme-based embeddings would make the paper more compelling and highlight the advantages or limitations of the multi-view approach.
Questions for the Authors
1. How does the model handle homophones, and how are they represented in the embedding space? Are they clustered together or separated?
2. Why were phoneme-based comparisons not included in the evaluation? Would the model generalize better with phoneme-level supervision instead of orthographic labels?
3. Can the proposed method be extended to handle tasks where word boundaries are not pre-segmented? If so, how would the model's performance change?
4. What are the limitations of the current dataset, and how do you plan to scale the approach to larger, more realistic datasets?
In summary, while the paper presents an innovative approach, the lack of realistic tasks, limited evaluation, and synthetic assumptions significantly limit its impact. Addressing these concerns could make the work more relevant and impactful for the speech processing community.