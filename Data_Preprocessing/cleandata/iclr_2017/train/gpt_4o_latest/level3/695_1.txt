The paper introduces NoiseOut, a novel neuron pruning technique aimed at reducing the number of parameters in neural networks by merging highly correlated neurons. The method innovatively incorporates a "noise output" neuron, which predicts the mean of a noise distribution, to encourage higher correlation between neuron activations, thereby facilitating more efficient pruning. The authors validate their approach through experiments on MNIST and SVHN datasets, demonstrating significant parameter reductions while maintaining accuracy. The paper is well-written, clearly presenting the methodology and encouraging further exploration of the technique.
Decision: Reject
While the paper presents an interesting and promising pruning method, the decision to reject is based on two key reasons:
1. Limited Experimental Validation: The experiments are restricted to small-scale datasets (MNIST and SVHN), which limits the generalizability of the results. Larger and more complex datasets (e.g., CIFAR-10, ImageNet) are necessary to demonstrate the scalability and robustness of NoiseOut.
2. Unaddressed Scalability Concerns: The potential need for additional noise outputs in larger datasets and its impact on accuracy and computational overhead is not thoroughly explored. This raises questions about the method's practicality in real-world scenarios.
Supporting Arguments:
1. Novelty and Contribution: The idea of leveraging noise outputs to increase neuron correlation is novel and well-motivated. The theoretical foundation is sound, and the pruning algorithm is clearly described. The experiments on MNIST and SVHN show impressive pruning rates (e.g., 95% reduction in parameters for Lenet-5) without accuracy degradation, highlighting the method's potential.
2. Experimental Limitations: The reliance on small datasets undermines the broader applicability of the method. While MNIST and SVHN are useful benchmarks, they do not represent the challenges of modern deep learning tasks, such as those involving high-resolution images or large-scale datasets.
3. Scalability and Practicality: The paper does not address whether the addition of noise outputs might introduce computational or memory overhead in larger networks. Furthermore, the impact of noise outputs on training dynamics and convergence is not thoroughly analyzed.
Suggestions for Improvement:
1. Expand Experiments: Test NoiseOut on larger and more diverse datasets, such as CIFAR-10, ImageNet, or NLP benchmarks, to validate its effectiveness and scalability.
2. Analyze Noise Outputs: Investigate the potential need for multiple noise outputs in larger networks and their impact on accuracy, training time, and computational efficiency.
3. Compare with Baselines: Provide a more comprehensive comparison with state-of-the-art pruning methods, such as magnitude-based pruning or Hessian-based approaches, to contextualize the performance of NoiseOut.
4. Clarify Equation (5): While noted as straightforward, Equation (5) could benefit from additional explanation or examples to enhance clarity for readers unfamiliar with the derivation.
Questions for the Authors:
1. How does the addition of noise outputs affect the training time and computational cost, particularly for larger networks?
2. Have you considered testing NoiseOut on convolutional layers or transformer-based architectures? If not, do you anticipate any challenges in extending the method to these architectures?
3. How sensitive is the pruning performance to the choice of noise distribution (e.g., Gaussian, Binomial)? Would a different distribution significantly impact the results?
In conclusion, while NoiseOut is an innovative pruning method with promising results, the paper requires additional experimental validation and a deeper exploration of scalability concerns to justify its acceptance. The suggestions provided aim to strengthen the paper for future submissions.