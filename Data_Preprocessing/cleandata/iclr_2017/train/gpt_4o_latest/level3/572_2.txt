The paper explores adversarial example generation for deep generative models, specifically Variational Autoencoders (VAEs) and VAE-GANs, using three methods: classification-based adversaries, VAE loss-based attacks, and latent space attacks. It claims to be among the first works addressing adversarial vulnerabilities in generative models, a novel and potentially impactful direction given the increasing use of such models in real-world applications. The authors evaluate their methods on MNIST, SVHN, and CelebA datasets, demonstrating the feasibility of adversarial attacks in this domain and motivating potential attack scenarios, such as compromising latent-space-based communication systems.
Decision: Reject. The primary reasons for this decision are the lack of originality in the proposed methods and the poor clarity and organization of the manuscript.
Supporting Arguments:
1. Originality and Contribution: While the problem of adversarial attacks on generative models is interesting and underexplored, the methods presented in the paper are adaptations of existing techniques from adversarial attacks on classifiers. The approaches—classification-based attacks, VAE loss optimization, and latent space perturbations—are not tailored specifically to generative models in a novel or innovative way. The paper does not sufficiently differentiate its contributions from prior work, such as latent space attacks in related literature.
   
2. Clarity and Presentation: The manuscript is poorly organized and overly lengthy, making it difficult to follow. Significant changes between revisions have resulted in a disjointed presentation, with redundant explanations and a lack of focus. Key ideas, such as the motivation for the attacks and the evaluation metrics, are buried under excessive detail, which detracts from the overall readability.
3. Scientific Rigor: While the experiments demonstrate that the proposed attacks can compromise generative models, the evaluation lacks depth. For example, the success metrics (e.g., ASignore-target and AStarget) are not thoroughly analyzed in terms of their implications for real-world scenarios. Additionally, the paper does not explore defenses or robustness strategies, which would strengthen its contribution to the field.
Suggestions for Improvement:
1. Focus on Novelty: The authors should propose methods that are more specifically tailored to generative models, rather than adapting existing techniques. For example, leveraging unique properties of VAE and VAE-GAN architectures could lead to more innovative contributions.
   
2. Improve Clarity and Organization: The manuscript should be significantly condensed and reorganized to improve readability. A clear separation of the problem statement, methodology, experiments, and conclusions is essential. Avoid redundant explanations and focus on the most critical aspects of the work.
3. Expand Evaluation: The evaluation should include a broader discussion of the implications of the attacks, including their practical relevance and potential defenses. Additionally, comparisons with concurrent work (e.g., Tabacof et al.) should be more detailed.
Questions for the Authors:
1. How do the proposed methods compare quantitatively and qualitatively to the latent space attacks in Tabacof et al. (2016)?
2. Could you clarify the practical scenarios where these attacks would be impactful, beyond the motivating example provided?
3. Have you considered evaluating the robustness of the generative models against these attacks or proposing potential defenses?
In summary, while the paper addresses an interesting and relevant problem, its lack of originality, poor presentation, and limited evaluation undermine its contribution. Significant improvements are needed before it can be considered for acceptance.