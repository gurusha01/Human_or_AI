The paper investigates the eigenvalue spectrum of the Hessian matrix in small deep networks near the end of training, revealing a highly singular structure with eigenvalues concentrated around zero (the "bulk") and a few discrete eigenvalues (the "outliers") dependent on the input data. The authors argue that this singularity challenges existing theoretical assumptions in deep learning and propose that the observed bulk/outlier behavior has implications for understanding optimization landscapes and training dynamics.
Decision: Reject.
The primary reasons for rejection are the insufficient contextualization of the work within existing literature and the limited scope of experiments. While the paper addresses an important problem and presents intriguing observations, it fails to adequately position its contributions relative to prior work and does not convincingly demonstrate the generalizability of its findings to larger, practical networks.
Supporting Arguments:
1. Insufficient Literature Contextualization: The paper overlooks key prior work on the Hessian in deep learning, such as Dauphin et al. (2014) and Amari's contributions, which have explored related concepts like saddle points and second-order optimization. While the authors mention Watanabe (2007), they do not sufficiently differentiate their findings from this earlier work. This lack of comprehensive referencing weakens the novelty and impact of the paper.
2. Limited Experimental Scope: The experiments are restricted to small networks, such as two-layer fully connected networks trained on subsets of MNIST or toy datasets. While the results are interesting, the lack of experiments on larger, more practical architectures (e.g., modern deep networks) raises concerns about the applicability of the findings to real-world scenarios. The paper does not provide evidence that the observed bulk/outlier behavior persists in larger-scale settings.
3. Scientific Rigor and Claims: The paper's claims about the implications of Hessian singularity for optimization and theory are intriguing but speculative. The authors do not provide rigorous theoretical analysis or sufficient empirical evidence to substantiate these claims.
Suggestions for Improvement:
1. Expand Literature Review: Include and discuss prior work on the Hessian in deep learning, such as Dauphin et al. (2014) and Amari's contributions. Clearly articulate how this work builds upon or differs from existing studies.
2. Broaden Experiments: Extend the analysis to larger, more complex networks and datasets (e.g., ResNet on CIFAR-10 or ImageNet) to demonstrate the generalizability of the findings.
3. Strengthen Theoretical Contributions: Provide a more rigorous theoretical analysis of the implications of Hessian singularity for optimization and generalization in deep learning.
4. Clarify Practical Implications: Elaborate on how the observed bulk/outlier behavior could be leveraged in practice, such as in designing optimization algorithms or understanding generalization.
Questions for the Authors:
1. How do the observed bulk/outlier behaviors scale with network depth and width in larger architectures?
2. How do the findings relate to prior work on saddle points and flat minima in deep learning landscapes?
3. Can the authors provide more evidence or theoretical justification for the claim that Hessian singularity implies flat regions at a global scale?
While the paper tackles an important problem and presents promising preliminary results, addressing the above concerns would significantly strengthen its contribution and impact.