Review of "Attend, Adapt and Transfer (A2T): An Attentive Deep Architecture for Adaptive Transfer in Reinforcement Learning"
Summary of Contributions
The paper presents a novel framework, A2T (Attend, Adapt, and Transfer), to address two critical challenges in multi-task reinforcement learning: avoiding negative transfer and enabling selective transfer. The proposed architecture employs a soft attention mechanism to dynamically combine knowledge from multiple source tasks and a base network, which learns from scratch when prior knowledge is inapplicable. This state-dependent sub-policy selection allows finer granularity in leveraging source tasks for different parts of the state space. The method is general and compatible with both policy gradient and value iteration methods. Empirical evaluations demonstrate the effectiveness of A2T in avoiding negative transfer and selectively transferring knowledge across tasks in simple domains (e.g., chain world, puddle world, Atari Pong). The paper also identifies potential extensions, such as option-based frameworks and progressive networks, and highlights areas for further exploration, such as monitoring attention mask activation to detect co-adaptation issues.
Decision: Accept
The paper makes a significant contribution to transfer learning in reinforcement learning by proposing a general framework that addresses both negative and selective transfer. The novelty of the soft attention mechanism and its demonstrated ability to dynamically adapt to state-specific requirements make the work impactful. The empirical results, though limited to simple tasks, convincingly support the claims. The paper is well-motivated, well-placed in the literature, and provides a strong foundation for future extensions.
Supporting Arguments
1. Novelty and Generality: The introduction of a soft attention mechanism for state-dependent sub-policy selection is a novel contribution. The framework's compatibility with both policy and value transfer methods enhances its generality and applicability.
2. Empirical Validation: The experiments convincingly demonstrate the framework's ability to avoid negative transfer and perform selective transfer. The visualization of attention weights and the ability to adapt to unfavorable and favorable source tasks further validate the claims.
3. Well-Placed in Literature: The paper builds on prior work in transfer learning, such as policy reuse and multi-task networks, while addressing critical gaps like negative transfer and selective transfer at a finer granularity.
Suggestions for Improvement
1. Task Complexity: While the tasks used (chain world, puddle world, Pong) are sufficient for proof of concept, they are relatively simple. Future work should evaluate A2T on more complex, real-world tasks to demonstrate scalability and robustness.
2. Attention Mask Insights: Figure 6 shows suboptimal performance (red curve), but the paper does not analyze attention mask activation statistics during training. Monitoring these statistics could provide insights into co-adaptation issues and adversarial sub-policy activation.
3. Baseline Comparisons: While the paper compares A2T to learning from scratch and single-source transfer, additional comparisons with state-of-the-art transfer learning methods (e.g., progressive networks, multi-task networks) would strengthen the empirical evaluation.
4. Theoretical Analysis: The claims are primarily supported by empirical results. A theoretical analysis of why A2T avoids negative transfer and achieves selective transfer would enhance the scientific rigor.
5. Visualization of Attention Mechanism: While some visualizations are provided, more detailed and intuitive visualizations of how attention weights evolve during training could improve interpretability.
Questions for Authors
1. How does A2T perform in environments with continuous state and action spaces? Can it scale to high-dimensional tasks such as robotic control?
2. How does the architecture handle scenarios where the source tasks are highly unrelated to the target task? Does the base network always compensate effectively?
3. Could you provide more details on how the attention mechanism avoids co-adaptation between the base network and source tasks?
4. Have you considered using stochastic hard attention instead of soft attention? How would this impact performance and interpretability?
Overall, the paper presents a promising framework with strong empirical results and significant potential for future research. Addressing the above suggestions would further strengthen its impact.