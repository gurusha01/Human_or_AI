Review of the Paper
Summary of Contributions
This paper revisits the challenges of distributed training in deep learning and proposes a novel approach: synchronous stochastic gradient descent (Sync-SGD) with backup workers. The authors argue that this method mitigates the issues of gradient staleness in asynchronous SGD (Async-SGD) while addressing the straggler problem inherent in traditional Sync-SGD. The paper demonstrates the effectiveness of the proposed approach through empirical evaluations on Inception Net and PixelCNN, showing faster convergence and better test accuracies compared to Async-SGD. The method is simple, practical, and particularly relevant for industry settings where adding backup workers is feasible. The authors also provide an in-depth analysis of straggler effects and propose a trade-off between iteration time and gradient quality, supported by experiments on large-scale distributed systems.
Decision: Reject
While the paper presents a practical and straightforward solution to a well-known problem, the decision to reject is based on two primary reasons:
1. Limited Novelty: The proposed approach, while effective, is incremental and does not address a sufficiently challenging or unexplored problem. The use of backup workers in distributed systems is not a novel concept and has been explored in prior work (e.g., Dean & Barroso, 2013). The paper does not sufficiently distinguish its contributions from existing methods like "softsync" or other straggler mitigation strategies.
2. Unfair Comparison: The experiments comparing Sync-SGD and Async-SGD lack fairness. Async-SGD does not include a mechanism to cut off excessively stale updates, which would make the comparison more balanced. This omission weakens the claim that Sync-SGD is categorically superior.
Supporting Arguments
1. Strengths:
   - The paper provides a clear and well-motivated explanation of the challenges with Async-SGD and traditional Sync-SGD.
   - The empirical results convincingly demonstrate the advantages of Sync-SGD with backup workers in terms of convergence speed and test accuracy.
   - The method is practical and scalable, making it relevant for real-world applications in distributed training.
2. Weaknesses:
   - The novelty of the approach is limited. The idea of using backup workers is straightforward and has been discussed in prior literature. The paper does not introduce fundamentally new techniques or insights.
   - The experimental setup for Async-SGD does not include mechanisms to mitigate stale updates, such as cutting off excessively stale gradients. This omission creates an unfair comparison and undermines the validity of the results.
Suggestions for Improvement
1. Addressing Novelty: The authors should clarify how their approach differs from existing methods like "softsync" and other straggler mitigation techniques. Highlighting unique contributions or theoretical insights would strengthen the paper.
2. Fair Comparisons: Async-SGD experiments should include mechanisms to handle stale updates, such as cutting off excessively stale gradients. This would provide a more balanced and rigorous comparison.
3. Broader Applicability: The paper could explore the applicability of Sync-SGD with backup workers to other types of models or datasets, such as sparse models or natural language processing tasks, to demonstrate its generalizability.
4. Theoretical Analysis: While the empirical results are strong, a theoretical analysis of the trade-offs between iteration time, gradient quality, and convergence speed would add depth to the paper.
Questions for the Authors
1. How does the proposed method compare to "softsync" or other existing straggler mitigation techniques in terms of both performance and implementation complexity?
2. Why was Async-SGD not equipped with a mechanism to handle stale updates in the experiments? Would incorporating such a mechanism change the conclusions?
3. Can the proposed approach be extended to settings with sparse gradients or models with highly imbalanced workloads across workers?
In conclusion, while the paper addresses an important problem and provides promising empirical results, it falls short in terms of novelty and rigor in experimental comparisons. Addressing these issues could make the work more impactful.