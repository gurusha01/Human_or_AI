Review of the Paper: Neural Architecture Search Using Reinforcement Learning
Summary of Contributions
This paper introduces an innovative method for automating neural network architecture design using a recurrent neural network (RNN) as a controller and reinforcement learning (RL) to optimize the generated architectures. The approach, termed Neural Architecture Search (NAS), is applied to both convolutional architectures for image classification on CIFAR-10 and recurrent cell architectures for language modeling on Penn Treebank (PTB). The proposed method achieves state-of-the-art results on both tasks, with a test error rate of 3.65% on CIFAR-10 and a test perplexity of 62.4 on PTB. The paper also demonstrates the transferability of the discovered recurrent cell to other tasks, such as character language modeling and machine translation, showcasing its generalization capabilities.
Decision: Accept
The paper presents a novel and promising approach to automating neural architecture design, a critical challenge in deep learning. While there are some limitations, the strengths outweigh the weaknesses, and the method has potential to inspire further research in this area.
Supporting Arguments for Acceptance
1. Innovation and Motivation: The combination of RNNs for generating architectures and RL for optimizing them is an innovative and well-motivated approach. The paper situates its contributions effectively within the existing literature, addressing limitations of prior methods such as fixed-length search spaces and reliance on human expertise.
2. Empirical Results: The results are compelling, with the proposed method achieving competitive or superior performance compared to human-designed architectures on CIFAR-10 and PTB. The transferability of the discovered recurrent cell further strengthens the paper's claims.
3. Scientific Rigor: The methodology is described in detail, including the use of policy gradients, distributed training, and techniques to reduce variance in the reward signal. The experiments are thorough, with comparisons to baselines such as random search and control experiments to test robustness.
Weaknesses and Suggestions for Improvement
1. Training Time and Computational Cost: The training process is extremely resource-intensive, requiring 800 GPUs for the CIFAR-10 experiments and 400 CPUs for PTB. While the authors acknowledge this limitation, further discussion on strategies to reduce computational cost (e.g., pruning the search space or leveraging transfer learning) would be valuable.
2. Generality Across Datasets: The experiments focus on CIFAR-10 and PTB, which are standard benchmarks but may not fully demonstrate the generality of the approach. Additional experiments on diverse datasets, particularly for sequential tasks, would strengthen the claims of generalizability.
3. Comparison with Human-Designed Architectures: While the results are promising, the paper could include a more detailed analysis of how the generated architectures differ from human-designed ones in terms of design principles, efficiency, and interpretability.
Questions for the Authors
1. How does the method perform on larger and more diverse datasets, such as ImageNet or larger-scale language modeling tasks? Are there any limitations in scaling the approach to such datasets?
2. Can the computational cost of the training process be reduced without significantly compromising performance? For example, could transfer learning from previously discovered architectures be used to initialize the search process?
3. Could the authors provide more insights into the interpretability of the discovered architectures? Are there any patterns or design principles that emerge from the generated models?
Additional Feedback
- The inclusion of transfer learning experiments is a strong point, but further exploration of how the discovered architectures perform in real-world applications (e.g., autonomous driving, medical imaging) would enhance the paper's impact.
- The paper could benefit from a more detailed discussion of the limitations of the approach, particularly in terms of scalability and interpretability, and potential future directions to address these challenges.
In conclusion, this paper presents a significant contribution to the field of automated machine learning and neural architecture design. While there are areas for improvement, the novelty, rigor, and empirical results justify its acceptance.