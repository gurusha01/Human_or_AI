Review of the Paper
Summary of Contributions
This paper introduces Prototypical Networks, a method for few-shot learning that simplifies metric learning by representing each class with a single prototype, computed as the mean of the class embeddings in a learned representation space. The approach is computationally efficient, scalable, and conceptually straightforward compared to prior methods like Matching Networks, which rely on attention mechanisms over the entire support set. The paper demonstrates competitive performance on few-shot learning tasks (Omniglot and miniImageNet) and state-of-the-art results in zero-shot learning on the CUB-200 dataset. The authors also adapt the method to zero-shot learning by embedding class metadata (e.g., attributes) into the same space as the image embeddings. The proposed method is appealing due to its simplicity, scalability, and ability to generalize across tasks.
Decision: Reject
While the paper presents an interesting and simplified approach to few-shot learning, it falls short in several critical areas:
1. Insufficient Empirical Comparisons: The results section does not comprehensively compare Prototypical Networks to Matching Networks and other state-of-the-art methods. For instance, fine-tuned and contextual embedding results for Matching Networks on miniImageNet are omitted, which is misleading.
2. Weak Related Work Discussion: The related work section is incomplete and misses key literature on metric learning (e.g., Weinberger et al., 2005), neural network-based approaches (Min et al., 2009), and learning-to-rank methods (e.g., Burges, 2010).
3. Unclear Performance Claims: The paper does not clearly specify which methods Prototypical Networks outperform and which they do not. Additionally, state-of-the-art results on CUB-200 (e.g., 50.1% with GoogLeNet features) are not acknowledged or discussed.
Supporting Arguments
1. Empirical Results: While the paper claims competitive performance, the lack of fine-tuned comparisons on miniImageNet and omission of contextual embeddings for Matching Networks undermines the validity of the results. Without these comparisons, it is unclear whether the proposed method truly advances the state of the art.
2. Related Work: The paper does not adequately situate itself within the broader literature. For example, the connection to Mensink et al. (2013) is noted, but other relevant works in metric learning, nearest neighbor classification, and ranking losses are missing. This weakens the theoretical foundation of the paper.
3. Zero-Shot Learning: While the method achieves strong results on CUB-200, the discussion lacks depth. The authors should comment on why their method outperforms others and how it handles domain differences between image embeddings and class attributes.
Suggestions for Improvement
1. Expand Related Work: Include discussions of metric learning methods (e.g., Weinberger et al., 2005), neural network-based approaches (e.g., Min et al., 2009), and ranking losses (e.g., Burges, 2010). This will better contextualize the contributions of the paper.
2. Comprehensive Empirical Comparisons: Report fine-tuned and contextual embedding results for Matching Networks on miniImageNet. Additionally, acknowledge and comment on state-of-the-art results for CUB-200 (e.g., 50.1% with GoogLeNet features).
3. Clarify Performance Claims: Clearly specify which methods Prototypical Networks outperform and which they do not. Provide a more balanced empirical comparison across all tasks.
4. Discussion of Limitations: Discuss potential limitations of the method, such as its reliance on a single prototype per class, which may not capture intra-class variability effectively.
Questions for the Authors
1. How does the performance of Prototypical Networks compare to Matching Networks when fine-tuned and contextual embeddings are used on miniImageNet?
2. Why were state-of-the-art results on CUB-200 (e.g., 50.1% with GoogLeNet features) not acknowledged or discussed?
3. Have you considered using multiple prototypes per class to capture intra-class variability? If so, how does this affect performance and computational efficiency?
In summary, while the paper introduces a novel and simplified approach to few-shot and zero-shot learning, the lack of comprehensive comparisons, incomplete related work, and unclear performance claims prevent it from meeting the standards for acceptance. Addressing these issues could significantly strengthen the paper.