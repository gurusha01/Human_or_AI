Review of the Paper
The paper introduces a novel memory architecture for neural networks, Hierarchical Attentive Memory (HAM), which leverages a binary tree structure to achieve logarithmic memory access complexity (Θ(log n)) compared to the linear complexity (Θ(n)) of standard attention mechanisms. The authors demonstrate that an LSTM augmented with HAM can learn and generalize algorithmic tasks such as sorting, merging, and binary search, achieving significant improvements in computational efficiency and generalization over baseline models. Additionally, the HAM module is shown to simulate classic data structures like stacks, queues, and priority queues with high accuracy.
Decision: Reject
While the proposed HAM architecture is conceptually interesting and addresses an important limitation of existing memory-augmented neural networks (scalability of memory access), the paper falls short in its experimental evaluation and justification of claims. Specifically, the limited scope of experiments, reliance on small toy datasets, and lack of convincing evidence for scalability and robustness undermine the significance of the contribution.
Supporting Arguments for Rejection
1. Experimental Scope and Scalability: The experiments are restricted to small toy datasets with memory sizes of up to 32 tokens. While the authors claim generalization to larger memory sizes, the results for sequences 2-4x longer are not robust across all tasks (e.g., errors in the Merge and Priority Queue tasks). Furthermore, the scalability of HAM to real-world, large-scale datasets (e.g., books or DNA sequences) is not demonstrated, leaving the practical applicability of the model in question.
2. Training Complexity and Scalability Concerns: The use of REINFORCE for training introduces high variance and complexity, which may limit the scalability of the model to larger datasets. While the authors propose a soft attention variant (DHAM) as an alternative, it sacrifices computational efficiency (Θ(n) complexity) and does not generalize as well as HAM, further complicating the practical utility of the approach.
3. Weak Baseline Comparison: The paper compares HAM primarily against LSTM and LSTM with attention, which are not state-of-the-art for algorithmic tasks. A more rigorous comparison against other memory-augmented architectures (e.g., Neural Turing Machines, Neural Random-Access Machines) is necessary to establish the superiority of HAM.
Suggestions for Improvement
1. Expand Experimental Evaluation: Evaluate HAM on larger, real-world datasets to demonstrate its scalability and practical utility. Include tasks with longer sequences and larger memory sizes to validate the claims of logarithmic complexity and generalization.
2. Improve Baseline Comparisons: Compare HAM against a broader range of memory-augmented architectures, including those specifically designed for algorithmic tasks. This would provide a more comprehensive understanding of HAM's relative strengths and weaknesses.
3. Address Training Complexity: Explore alternative training strategies to mitigate the challenges of using REINFORCE, such as hybrid approaches combining soft and hard attention or curriculum learning tailored for larger datasets.
4. Clarify Model Behavior: Provide more detailed insights into the learned representations and algorithms, especially for tasks like sorting. This would help validate the claim that HAM learns "real" algorithms rather than statistical patterns.
Questions for the Authors
1. How does HAM perform on real-world datasets with significantly larger memory sizes (e.g., n > 1000)? Can you provide evidence for its scalability beyond toy datasets?
2. What are the computational trade-offs between HAM and DHAM, and under what conditions would one be preferred over the other?
3. Can you elaborate on the variance-reduction techniques used in REINFORCE and their impact on training stability and convergence?
4. How does HAM compare to other memory-augmented architectures like Neural Random-Access Machines or Neural Turing Machines on similar tasks?
In conclusion, while the paper presents an innovative idea with potential, the limited experimental validation and unresolved scalability concerns prevent it from making a strong case for acceptance at this time.