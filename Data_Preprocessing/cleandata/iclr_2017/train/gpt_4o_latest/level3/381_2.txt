Review of the Paper
Summary of Contributions
This paper proposes a novel filter pruning method for convolutional neural networks (CNNs) based on the first-order Taylor expansion of loss change. The approach is computationally efficient and integrates seamlessly with traditional fine-tuning procedures, making it practical for real-world applications. Unlike activation magnitude-based pruning, this method considers both the activation and its gradient with respect to the loss, leading to more informed pruning decisions. The authors demonstrate the generalizability of their method across various tasks, including fine-grained image classification and gesture recognition, and provide extensive empirical evaluations against multiple baselines, including an oracle. The results show significant improvements in computational efficiency with minimal accuracy degradation, achieving up to a 12.6Ã— reduction in FLOPs for certain tasks. The paper also discusses the trade-offs between speed and performance, which is valuable for practical deployment scenarios.
Decision: Accept
The paper makes a strong contribution to the field of model compression by introducing a well-motivated and computationally efficient pruning criterion. The method is rigorously evaluated, and the results convincingly support the claims. The paper addresses a relevant problem and provides a practical solution that is likely to have a significant impact on both research and industry.
Supporting Arguments for Acceptance
1. Novelty and Motivation: The proposed Taylor expansion-based pruning criterion is novel and well-motivated. By leveraging both activation and gradient information, the method improves upon existing approaches that rely solely on activation magnitude or weight norms.
2. Empirical Rigor: The authors conduct extensive experiments across diverse datasets and tasks, including Birds-200, Flowers-102, and ImageNet, as well as a gesture recognition task using 3D-CNNs. The results consistently demonstrate the superiority of the proposed method in terms of accuracy retention and computational efficiency.
3. Practicality: The method is computationally efficient and integrates well with standard fine-tuning procedures. The discussion of trade-offs between speed and accuracy is particularly valuable for practitioners.
4. Comparison with Baselines: The paper evaluates the method against multiple baselines, including computationally expensive oracles, and shows that the proposed approach achieves comparable or better performance.
Suggestions for Improvement
1. Additional Baseline Comparisons: While the paper evaluates several baselines, including Optimal Brain Damage (OBD) and activation-based pruning, a comparison with other recent methods, such as [1], would further strengthen the evaluation.
2. Exploration of Partial Filter Pruning: The authors briefly mention the potential for pruning parts of filters (e.g., for 3D convolutions) to achieve further speedups. Exploring this direction, even in a preliminary manner, could add depth to the paper.
3. Clarification on Fine-Tuning Impact: The results suggest that additional fine-tuning can recover much of the accuracy lost during pruning. A more detailed analysis of the relationship between fine-tuning duration and accuracy recovery would be helpful.
4. Real-World Deployment: While the paper measures FLOPs and inference time, a discussion on the method's performance on specific hardware platforms (e.g., embedded GPUs) would provide more practical insights.
Questions for the Authors
1. How does the proposed method perform in scenarios where the dataset for fine-tuning is extremely small? Does the pruning criterion remain reliable in such cases?
2. Can the Taylor criterion be extended to other types of neural network architectures, such as transformers or graph neural networks? If so, what challenges might arise?
3. The paper mentions that combining criteria (e.g., Taylor and activation) yields negligible gains. Could this observation vary for different tasks or datasets?
In conclusion, this paper presents a significant advancement in filter pruning for CNNs. The proposed method is both theoretically sound and empirically validated, making it a valuable contribution to the field. Addressing the suggested improvements would further enhance the paper's impact.