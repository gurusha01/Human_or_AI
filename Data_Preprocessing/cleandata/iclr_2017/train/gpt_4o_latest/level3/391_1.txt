Review
Summary
This paper proposes a method for pruning weights in Recurrent Neural Networks (RNNs) during training to achieve sparse solutions, thereby reducing model size and computational requirements. The approach progressively sets weights to zero using a monotonically increasing threshold, controlled by hyperparameters. The method is demonstrated on state-of-the-art RNN-based speech recognition systems, achieving up to 90% sparsity with minimal performance degradation. In some cases, the pruning even improves evaluation performance by starting with larger dense models and pruning them down. The authors highlight significant computational savings, including speed-ups of up to 7Ã— and memory reductions that make the models more deployable on mobile and embedded devices. The paper also emphasizes the scalability of the approach to large RNN systems and its simplicity compared to other pruning techniques.
Decision: Reject  
While the paper demonstrates promising results, it falls short in several critical areas that limit its overall contribution and reproducibility. The lack of comparisons with existing pruning methods and reliance on private datasets are significant drawbacks. Additionally, the novelty of the approach is limited, as it closely resembles prior work in pruning methods.
Supporting Arguments
1. Strengths:
   - The paper successfully demonstrates that pruning can achieve significant sparsity in RNNs with minimal performance loss, which is a valuable contribution for resource-constrained deployments.
   - The experiments are conducted on a state-of-the-art RNN system, and the methodology is sound, focusing on large networks relevant to real-world applications.
   - The computational gains, including memory compression and inference speed-ups, are substantial and well-documented.
2. Weaknesses:
   - Lack of Novelty: The proposed method is highly similar to existing pruning techniques, such as Han et al.'s work. While the application to RNNs is novel, the core idea of gradual pruning lacks significant innovation.
   - No Baseline Comparisons: The paper does not compare its method with other established pruning approaches, such as those by Han et al. or Yu et al., beyond a brief mention of hard pruning. This omission makes it difficult to assess the relative merits of the proposed method.
   - Reproducibility Issues: The reliance on private datasets limits the ability of other researchers to reproduce the results and validate the claims.
   - Hyperparameter Tuning Overhead: While the method is integrated into training, the need for careful hyperparameter tuning undermines the claim of computational efficiency, especially in comparison to simpler methods.
   - Dense Baseline: The dense baseline could be more convincing if it incorporated model compression techniques like soft target training, which would provide a stronger benchmark.
Additional Feedback
1. Motivation for Design Choices: The paper does not adequately explain the rationale behind specific design choices, such as the threshold ramp-up function. A more detailed discussion would strengthen the theoretical foundation of the method.
2. Sparse RNN Speed-ups: While the paper discusses potential speed-ups from sparse RNNs, these are not specific to the proposed method. A deeper analysis of how the pruning strategy interacts with hardware optimizations would be beneficial.
3. Comparison with Regularization: The authors briefly mention L1 regularization as a potential alternative but do not explore it. A comparison with regularization-based sparsity methods would provide additional context for the effectiveness of the proposed approach.
4. Broader Applicability: The paper could be improved by extending the experiments to other tasks, such as language modeling or machine translation, to demonstrate the generalizability of the method.
Questions for the Authors
1. How does the proposed pruning method compare quantitatively to other pruning techniques, such as Han et al.'s iterative pruning and retraining approach?
2. Can the authors provide more details on the private dataset used, or consider releasing a subset of the data to improve reproducibility?
3. How sensitive is the method to the choice of hyperparameters, and can this sensitivity be mitigated through automated tuning methods?
4. Could the authors clarify the computational trade-offs of their single-stage training approach, particularly in terms of hyperparameter tuning overhead?
In conclusion, while the paper addresses an important problem and demonstrates promising results, the lack of novelty, absence of baseline comparisons, and reproducibility concerns prevent it from meeting the standards for acceptance at this time.