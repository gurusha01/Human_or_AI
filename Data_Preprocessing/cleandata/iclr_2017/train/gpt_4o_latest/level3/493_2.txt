Review of the Paper
Summary of Contributions
This paper proposes a novel approach to supervised classification by introducing tighter bounds on the misclassification error, which are iteratively updated during training. The authors argue that the standard log-loss overemphasizes outliers and poorly classified examples far from the decision boundary, leading to inefficiencies in optimization. By adapting the loss function iteratively, the proposed method improves classification rates, particularly in underfitting scenarios or when outliers are present. The paper also draws connections between supervised learning and reinforcement learning, offering a new perspective on incorporating external constraints and uncertain labels into classification tasks. Experimental results on multiple datasets validate the theoretical claims, showing improvements in classification error in some cases, while confirming that the approach does not outperform log-loss when it suffices.
Decision: Accept with Minor Revisions
The paper is well-written, presents an original idea, and demonstrates its efficacy both theoretically and empirically. However, certain aspects require clarification and additional justification, particularly regarding the use of randomized versus deterministic classifiers and the interpretation of results.
Supporting Arguments for Decision
1. Novelty and Impact: The idea of iteratively updating bounds on misclassification error is innovative and addresses a critical limitation of log-loss. The connection to reinforcement learning and the broader implications for systems with external constraints are compelling.
2. Theoretical and Empirical Support: The theoretical derivations are sound, and the experiments align with the claims, showcasing improvements in specific scenarios (e.g., underfitting). The results are consistent across datasets, reinforcing the validity of the approach.
3. Clarity and Presentation: The paper is well-structured and easy to follow, with clear explanations of the methodology and results.
Additional Feedback for Improvement
1. Randomized vs. Deterministic Classifiers: The paper mixes discussions of randomized and deterministic classifiers, leading to some confusion. It is unclear whether the experiments use randomized classifiers or deterministic ones. This distinction should be explicitly clarified. If randomized classifiers are used, the authors should justify why their error is a good surrogate for deterministic classifier error.
2. Comparison with Deterministic Classifiers: The paper should include a direct comparison between randomized and deterministic classifiers to strengthen its claims. If such a comparison is infeasible, a detailed explanation should be provided.
3. Uncertainty in Decisions: The section on "allowing uncertainty in the decision" is intriguing but underdeveloped. Additional references to related work in this area would enhance the discussion and provide a stronger foundation.
4. Minor Errors: There is a missing "-" sign in Section 3, which should be corrected.
Questions for the Authors
1. Can you clarify whether the experiments were conducted using randomized or deterministic classifiers? If randomized, why is this a valid surrogate for deterministic classifier performance?
2. How does the proposed method compare to other approaches that address outliers or underfitting, such as robust loss functions?
3. In the context of reinforcement learning, how does the proposed method handle scenarios where external constraints are dynamic or unknown during training?
In conclusion, this paper makes a meaningful contribution to the field of supervised classification and opens up interesting avenues for future research. Addressing the above points will further strengthen its impact and clarity.