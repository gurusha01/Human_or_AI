Review of the Paper
Summary of Contributions
This paper extends the Spin Glass analysis of Choromanska et al. (2015a) to Residual Networks (ResNets), offering novel insights into their dynamic ensemble behavior and the role of Batch Normalization (BN). The authors refine the Spin Glass model by removing unrealistic assumptions and provide a rigorous framework for analyzing ResNets' loss surfaces. A key contribution is the demonstration that ResNets behave as dynamic ensembles, transitioning from shallow to deeper effective depths during training, driven by scaling mechanisms such as BN. The paper also connects these findings to the optimization landscape, showing how the dynamic depth behavior influences critical points and optimization efficiency. These contributions encourage a deeper understanding of ResNets, moving beyond heuristic explanations.
Decision: Reject
While the paper makes significant theoretical contributions, it lacks sufficient empirical validation to support its claims. The absence of experiments isolating the effects of BN and the lack of results on standard datasets like CIFAR-10 without BN weaken the paper's impact. Additionally, some assumptions (e.g., Equation 12) remain unclear, and the empirical setup for Figure 1 is insufficiently detailed.
Supporting Arguments
1. Strengths:
   - The paper addresses an important open problem from Choromanska et al. (2015b) by refining unrealistic assumptions in the Spin Glass model.
   - It provides a novel perspective on ResNets' dynamic behavior, linking it to BN and effective depth during training.
   - The theoretical framework is rigorous and well-grounded in prior work, offering valuable insights into the optimization landscape of ResNets.
2. Weaknesses:
   - The claims about the steady increase in the L2 norm and its role in maintaining ensemble features lack empirical support. Results on CIFAR-10 without BN are necessary to validate these claims.
   - The empirical setup for Figure 1 is unclear, making it difficult to assess the validity of the presented results.
   - The paper does not adequately discuss the implications of its findings in the context of related work, such as the Fractal Net paper, which attributes ResNets' success to incidental residuals.
   - The assumption in Equation (12) regarding the realism of the variables should be clarified, as it is central to the analysis.
Suggestions for Improvement
1. Include experiments on CIFAR-10 and CIFAR-100 without BN to isolate its effect on the dynamic ensemble behavior. This would strengthen the empirical validation of the claims.
2. Provide a more detailed description of the empirical setup for Figure 1, including the dataset, training parameters, and evaluation metrics.
3. Clarify the assumption in Equation (12) and discuss its implications for the validity of the theoretical results.
4. Address the claims from the Fractal Net paper about residuals being incidental and analyze how the proposed framework applies to densely connected convolutional networks.
5. Expand the discussion on the practical implications of the findings, particularly how they might inform the design of future architectures.
Questions for the Authors
1. Can you provide empirical results on CIFAR-10 without BN to validate the claim about the steady increase in the L2 norm maintaining ensemble features?
2. How realistic is the assumption in Equation (12), and how does it affect the generalizability of the results?
3. Could you elaborate on the empirical setup for Figure 1? What dataset and training conditions were used?
4. How does your framework account for the claims in the Fractal Net paper about residuals being incidental? Could this perspective challenge your conclusions?
In summary, while the paper makes valuable theoretical contributions, its lack of empirical rigor and clarity in key assumptions limits its overall impact. Addressing these issues would significantly strengthen the work.