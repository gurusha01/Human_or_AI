Review of "Parametric Exponential Linear Unit (PELU)"
Summary of Contributions:
The paper introduces the Parametric Exponential Linear Unit (PELU), a parameterized variant of the ELU activation function, designed to address the vanishing gradient problem more effectively. The authors propose a parameterization that adjusts the activation function's behavior during training, providing CNNs with greater flexibility to manage vanishing gradients and bias shift. Theoretical analysis demonstrates how PELU mitigates vanishing gradients, and empirical results on CIFAR-10/100 and ImageNet validate its effectiveness. The experiments show consistent performance improvements over ELU, with relative error reductions of up to 7.28% on ImageNet, achieved with negligible parameter overhead. Additionally, the paper observes layer-wise and epoch-wise evolution of PELU parameters, suggesting that the network adapts its non-linear behavior dynamically during training.
Decision: Accept
Key reasons:  
1. The paper addresses a well-motivated problem (vanishing gradients and bias shift) and proposes a novel solution that is theoretically sound and practically impactful.  
2. Results demonstrate consistent improvements across multiple datasets and architectures, supporting the claims of the paper.  
Supporting Arguments:
1. Problem Relevance and Motivation: The vanishing gradient problem is a critical challenge in deep learning, and the proposed parameterization of ELU is a logical extension of prior work on parametric activation functions like PReLU. The paper is well-situated in the literature, building on established methods while addressing their limitations.  
2. Theoretical Rigor: The authors provide a detailed mathematical analysis of PELU's behavior, including its impact on vanishing gradients. The derivation of optimal parameter configurations and their effects on gradient flow is a strong theoretical contribution.  
3. Empirical Validation: The experimental results are compelling, showing consistent improvements across diverse architectures (ResNet, NiN, VGG) and datasets (CIFAR-10/100, ImageNet). The negligible parameter overhead (e.g., 0.0003% increase on ImageNet) further strengthens the practical utility of PELU.  
Suggestions for Improvement:
1. Clarify Parameter Evolution: While the paper notes the statistical evolution of PELU parameters, the explanation of why certain layers prefer specific parameter configurations (e.g., saturation near -2 in the last layer) remains speculative. A deeper analysis or visualization of these dynamics would enhance the understanding of PELU's behavior.  
2. Experimental Comparisons: The paper does not normalize the parameter count across different activation functions, making it difficult to isolate the performance gains attributable to PELU. Future work should include experiments where the parameter count is controlled to ensure fair comparisons.  
3. Overfitting and Regularization: The paper mentions potential overfitting in some cases (e.g., All-CNN and Overfeat on ImageNet) but does not explore this issue in depth. Additional experiments with varying regularization strategies could provide insights into mitigating overfitting when using PELU.  
Questions for the Authors:
1. Can you provide more detailed insights into the observed parameter evolution? Specifically, why do certain layers prefer activations saturating near -2 while others converge near zero?  
2. How does PELU perform when the parameter count is normalized across activation functions? Could the observed improvements be partially attributed to the additional parameters?  
3. Have you explored the impact of PELU on tasks beyond image classification, such as object detection or natural language processing?  
Conclusion:
The paper makes a significant contribution to the field by introducing a novel activation function that addresses key challenges in deep learning. While there are areas for further clarification and improvement, the theoretical and empirical results are strong enough to warrant acceptance. The proposed PELU has the potential to inspire further research into parameterized activation functions and their applications in deep learning.