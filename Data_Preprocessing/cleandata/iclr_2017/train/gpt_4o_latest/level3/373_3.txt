Review
Summary of Contributions
The paper presents a hierarchical taxonomy for transfer learning methods in sequence tagging, offering a unified framework that contextualizes and organizes prior work. The taxonomy is complemented by three proposed neural architectures (T-A, T-B, T-C) designed for cross-domain, cross-application, and cross-lingual transfer learning. The authors demonstrate that previously unexplored areas within this taxonomy achieve performance comparable to or exceeding state-of-the-art benchmarks across multiple datasets. The work is notable for its focus on low-resource settings, where transfer learning shows significant gains. Additionally, the paper provides a thorough experimental evaluation, identifying key factors influencing transfer learning performance, such as label abundance, task relatedness, and parameter sharing. The results establish new state-of-the-art performance on several benchmarks, underscoring the practical value of the proposed approach.
Decision: Accept
The paper makes a strong contribution by introducing a clear taxonomy for transfer learning in sequence tagging, unifying prior work, and demonstrating empirical improvements. The novelty, rigorous experimentation, and practical implications justify acceptance.
Supporting Arguments
1. Clear Problem Definition and Motivation: The paper addresses the critical challenge of transfer learning for sequence tagging, particularly in low-resource settings. The motivation is well-grounded in the literature, and the proposed taxonomy provides a structured lens to analyze and extend existing approaches.
   
2. Scientific Rigor: The experiments are comprehensive, covering diverse datasets (cross-domain, cross-application, cross-lingual) and varying resource levels. The results are robust, with clear evidence supporting the claims. The comparison with state-of-the-art methods further validates the contributions.
3. Novelty and Practical Impact: The hierarchical taxonomy and the exploration of parameter-sharing architectures (T-A, T-B, T-C) represent novel contributions. The demonstrated improvements in low-resource scenarios and the establishment of new benchmarks highlight the practical significance of the work.
Suggestions for Improvement
1. Trade-offs Between Restrictive and Less Restrictive Approaches: While the paper mentions this as a potential area for future work, a brief discussion of the observed trade-offs in the experiments (e.g., T-A vs. T-C) would enhance the reader's understanding of the practical implications of parameter sharing.
2. Clarity in Taxonomy Presentation: The taxonomy is a key contribution, but its presentation could be more formalized. A dedicated diagram or table summarizing the taxonomy and its dimensions would make it easier to grasp.
3. Broader Applicability: The paper focuses on transfer between tasks with similar alphabets (e.g., English and Spanish). A discussion on how the approach could be extended to more disparate languages (e.g., English and Chinese) would broaden its relevance.
4. Resource-Based Transfer: While the paper focuses on model-based transfer, exploring hybrid approaches that incorporate resource-based methods (e.g., multilingual embeddings) could be an interesting direction for future work.
Questions for Authors
1. How sensitive are the results to the choice of hyperparameters, particularly in low-resource settings? Did you observe any specific challenges in tuning the models for these scenarios?
2. Can the proposed architectures (T-A, T-B, T-C) be extended to handle transfer between tasks with completely disparate alphabets or label sets? If so, what modifications would be required?
3. Did you explore any methods for automatically determining the optimal level of parameter sharing (e.g., between T-A, T-B, and T-C) for a given task pair?
Overall, this paper makes a significant contribution to the field of transfer learning for sequence tagging and is a strong candidate for acceptance.