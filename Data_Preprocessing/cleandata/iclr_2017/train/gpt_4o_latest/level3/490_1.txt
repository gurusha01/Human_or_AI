Review of the Paper
Summary of Contributions
The paper proposes the Pointer Sentinel Mixture Model (PSMM), an innovative architecture that augments RNN-based language models with a pointer network to handle rare words. By combining the softmax output with a pointer distribution over recent context words, the model effectively addresses the challenge of predicting rare or unseen words, especially in scenarios where the context makes the prediction unambiguous. The authors demonstrate that their approach achieves state-of-the-art perplexity on the Penn Treebank dataset with fewer parameters compared to traditional LSTM models. Additionally, the paper introduces the WikiText dataset, a new benchmark for language modeling that addresses the limitations of existing datasets like Penn Treebank, particularly in terms of vocabulary size, punctuation, and long-range dependencies.
Decision: Reject
While the paper presents an interesting model and a new dataset, the concerns regarding the novelty and significance of the contributions, as well as the lack of clarity in certain claims, outweigh the merits in its current form. Below, I outline the key reasons for this decision.
Supporting Arguments for Rejection
1. Limited Novelty in Model Design:  
   The proposed model appears to be a direct application of Gulcehre et al.'s work, with only minor architectural differences, such as the use of a sentinel-based gating mechanism. Both models rely on mixture models and use RNN hidden states for query vectors. The paper does not convincingly argue why the proposed integration of the pointer network and switching mechanism is significantly different or superior to prior work. The reviewer challenges the claim that the sentinel-based gating mechanism is a substantial improvement over Gulcehre et al.'s switching network.
2. Unclear Significance of the WikiText Dataset:  
   While the WikiText dataset is a welcome addition to the field, the paper does not provide sufficient evidence to demonstrate its advantages over existing datasets like enwik8 or Text8. The authors should clarify how WikiText enables better evaluation of long-range dependencies and rare word prediction beyond what current datasets offer.
3. Empirical Results Lack Depth:  
   Although the model achieves state-of-the-art results on Penn Treebank, the experiments are limited to this dataset and WikiText. The paper would benefit from evaluations on additional benchmarks (e.g., enwik8, Text8) to establish broader applicability. Furthermore, the qualitative analysis of the pointer mechanism, while interesting, is anecdotal and lacks quantitative rigor.
Suggestions for Improvement
1. Clarify Novelty:  
   The authors should explicitly highlight the differences between their model and Gulcehre et al.'s work, both in terms of architecture and performance. A more detailed ablation study isolating the impact of the sentinel-based gating mechanism would strengthen the claims of novelty.
2. Dataset Comparison:  
   Provide a more thorough comparison of WikiText with existing datasets, including quantitative metrics (e.g., vocabulary diversity, long-range dependency statistics) and empirical results on other models to demonstrate its utility.
3. Broader Evaluation:  
   Extend the experimental evaluation to include other language modeling benchmarks and compare against a wider range of baselines, including models that use enwik8 or Text8.
4. Clarify Integration of Components:  
   The integration of the pointer network and sentinel mechanism is not fully explained. The authors should provide more intuition and theoretical justification for why their approach improves performance.
Questions for the Authors
1. How does the sentinel-based gating mechanism fundamentally differ from Gulcehre et al.'s switching network, and why is it better suited for language modeling tasks?
2. Can you provide quantitative evidence to demonstrate the advantages of WikiText over datasets like enwik8 and Text8?
3. How does the pointer sentinel mixture model perform on datasets with larger vocabularies or more complex long-range dependencies, such as enwik8?
Conclusion
The paper introduces a promising approach to language modeling and a new dataset, but the novelty and significance of the contributions are not sufficiently established. Addressing the above concerns could make the paper a strong candidate for acceptance in the future.