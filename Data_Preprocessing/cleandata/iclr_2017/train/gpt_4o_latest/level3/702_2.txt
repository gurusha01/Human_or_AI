The paper introduces two novel RNN-based architectures, Classifier and Selector, for extractive document summarization. The Classifier processes sentences sequentially in their original order, while the Selector allows for arbitrary sentence selection order. Both models compute sentence scores based on content richness, salience, positional importance, and redundancy, using concatenated RNN hidden states. A key contribution is the interpretability of the models, as they explicitly model abstract features influencing sentence selection. The models are trained using "pseudo-ground truth" derived from abstractive summaries via a greedy approximation. Experimental results demonstrate that the Classifier model achieves near state-of-the-art performance on some metrics, outperforming the Selector. However, the performance improvements over the Cheng & Lapata (2016) baseline are marginal and inconsistent, which the authors attribute to noisier ground truth labels generated by their unsupervised approach.
Decision: Reject
The primary reason for rejection is the lack of consistent and significant performance improvements over the Cheng & Lapata baseline. While the paper introduces novel architectures and provides insightful analysis, the marginal improvements fail to substantiate the contribution as a meaningful advance in the field. Additionally, the choice of using a noisy unsupervised method for generating training data, instead of adopting Cheng & Lapata's supervised approach, raises concerns about the validity of the experimental setup.
Supporting Arguments:
1. Marginal Performance Gains: The Classifier model achieves near state-of-the-art results for some metrics, but the improvements are inconsistent across datasets and summary lengths. The Selector model, despite its conceptual novelty, underperforms the Classifier and fails to demonstrate a clear advantage.
2. Training Data Quality: The reliance on a greedy unsupervised approximation for generating pseudo-ground truth introduces noise, which likely impacts model performance. This decision is not well-justified, especially given the availability of a more robust supervised method in prior work.
3. Baseline Comparison: The paper does not convincingly demonstrate that the proposed architectures consistently outperform the Cheng & Lapata baseline, which undermines the significance of the contribution.
Suggestions for Improvement:
1. Adopt Robust Training Data Generation: The authors should consider using Cheng & Lapata's supervised method for generating extractive labels to reduce noise and improve model performance.
2. Expand Evaluation: Conduct additional experiments to demonstrate the Selector model's utility in less structured summarization tasks (e.g., multi-document summarization or tweet clustering) where its flexibility might offer an advantage.
3. Optimize Inference: Incorporating beam search or Viterbi decoding during inference could potentially improve both architectures' performance and provide a fairer comparison with prior work.
4. Ablation Studies: While the paper includes some feature ablation experiments, a deeper analysis of why certain features (e.g., salience or redundancy) perform poorly in specific models would strengthen the interpretability claims.
Questions for the Authors:
1. Why was the unsupervised greedy approximation chosen over Cheng & Lapata's supervised method for generating training labels? Have you evaluated the impact of this choice on model performance?
2. Can the Selector model's flexibility be better demonstrated in tasks where sentence order is less structured? Are there plans to explore such applications?
3. Did you experiment with advanced inference techniques (e.g., beam search) to improve the Selector model's performance? If not, why?
While the paper presents interesting ideas, the lack of consistent performance improvements and the questionable choice of training data generation method limit its impact. Addressing these issues could make the work more compelling in future submissions.