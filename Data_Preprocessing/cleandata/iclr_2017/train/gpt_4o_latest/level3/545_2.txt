Review of the Paper: "Compositional Kernel Machines (CKMs)"
Summary
The paper introduces Compositional Kernel Machines (CKMs), a novel instance-based learning method that integrates compositionality and symmetry, inspired by convolutional neural networks (convnets). CKMs aim to address the curse of dimensionality by generating an exponential number of virtual instances through a sum-product function (SPF), while maintaining computational efficiency. The method is particularly positioned as an alternative to deep learning for small datasets, with experiments demonstrating competitive performance on the NORB dataset. The authors argue that CKMs offer advantages in terms of training speed, scalability, and robustness to adversarial examples, while also leveraging the rich history of kernel methods and instance-based learning.
Decision: Reject
While the paper presents an interesting and thoughtful approach, it falls short in several critical areas, including scalability, experimental rigor, and clarity of presentation. The claims made are overly broad and not sufficiently supported by the results, limiting the paper's impact and generalizability.
Supporting Arguments
1. Strengths:
   - The methodology is creative, combining ideas from sum-product networks and kernel methods to address compositionality and symmetry in visual data.
   - The paper establishes intriguing connections to the sum-product literature and highlights potential advantages of CKMs for small datasets with statistical structure.
   - Preliminary results on the NORB dataset suggest that CKMs can be competitive with convnets in specific scenarios, particularly when data is limited.
2. Weaknesses:
   - Scalability Issues: The claims about CKMs' scalability are unclear and potentially misleading. While CKMs may train faster than convnets on small datasets, the paper does not convincingly demonstrate their feasibility for larger datasets or high-dimensional data, where convnets excel.
   - Incomplete Presentation: The paper lacks critical details about the method's properties, such as its sensitivity to hyperparameters, robustness to noise, and computational trade-offs. The description of the SPF architecture and its implementation is dense and difficult to follow.
   - Limited Experiments: The experiments are narrowly focused on the NORB dataset and its variants, which limits the generalizability of the results. Comparisons with state-of-the-art deep learning models are insufficient, particularly in terms of adversarial robustness and scalability.
   - Broad Claims: The assertion that CKMs could serve as an alternative to convnets is overstated and not adequately supported. The method appears highly tailored to the NORB dataset, and its performance on larger, more diverse datasets remains unexplored.
Additional Feedback
To improve the paper:
1. Clarify Scalability: Provide a more detailed analysis of CKMs' scalability compared to convnets and standard SVMs, including computational complexity and memory requirements for larger datasets.
2. Expand Experiments: Test CKMs on a broader range of datasets, including larger and more diverse benchmarks, to demonstrate generalizability. Include comparisons with adversarially robust deep learning models.
3. Refine Claims: Tone down broad claims about CKMs as an alternative to convnets and focus on their niche advantages, such as fast training on small datasets with compositional structure.
4. Enhance Reproducibility: Release the code and provide more implementation details to enable reproducibility and facilitate future research.
5. Improve Presentation: Simplify the explanation of the SPF architecture and provide intuitive examples to make the method more accessible to readers.
Questions for the Authors
1. How do CKMs perform on larger datasets or datasets without strong compositional structure? Have you tested their scalability in such scenarios?
2. Can you provide more details on the computational trade-offs of CKMs compared to convnets, particularly in terms of memory usage and inference time?
3. How sensitive are CKMs to the choice of hyperparameters, such as the leaf kernel and cost functions? What guidelines can you provide for tuning these parameters?
4. Have you explored the robustness of CKMs to adversarial examples in comparison to deep learning methods? If so, what were the results?
In conclusion, while the paper presents an innovative approach with potential, it requires significant improvements in clarity, experimental rigor, and scope to make a stronger case for CKMs' utility and impact.