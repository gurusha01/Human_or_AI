Review
Summary
The paper proposes Adaptive Batch Normalization (AdaBN), a simple and parameter-free method for domain adaptation in deep neural networks (DNNs). By modulating the Batch Normalization (BN) statistics from the source domain to the target domain, AdaBN achieves domain adaptation without requiring additional components or optimization steps. The method is validated on standard domain adaptation benchmarks (Office and Caltech-Bing datasets) and a practical application in remote sensing for cloud detection. Results show that AdaBN achieves competitive or state-of-the-art performance while being computationally efficient and complementary to other domain adaptation techniques.
Decision: Accept with Minor Revisions
The paper is recommended for acceptance due to its simplicity, competitive performance, and practical applicability. However, revisions are needed to address issues of clarity and redundancy in certain sections.
Supporting Arguments
1. Strengths:
   - The method is simple, easy to understand, and straightforward to implement, making it accessible to a wide audience.
   - Empirical results demonstrate that AdaBN performs competitively or better than state-of-the-art methods on standard benchmarks and practical applications.
   - The analysis shows that AdaBN requires only a small number of target domain samples for effective adaptation, which is valuable in real-world scenarios where labeled data is scarce.
   - The method is parameter-free and computationally efficient, making it suitable for large-scale applications like cloud detection in remote sensing.
2. Weaknesses:
   - The novelty of the approach is limited. While the use of BN statistics for domain adaptation is interesting, the method is arguably too simple to be considered a significant methodological contribution.
   - Section 4.3.1 (sensitivity analysis) is redundant and serves primarily as a sanity check, adding little value to the overall contribution.
   - Section 3.3 lacks clarity in its purpose and contribution. It is unclear how this section advances the understanding or implementation of AdaBN.
Additional Feedback
1. Clarity:
   - Section 3.3 should be revised to clearly articulate its purpose and contribution. If it does not add significant value, it may be better to remove it.
   - The paper would benefit from a more detailed explanation of how AdaBN complements other domain adaptation methods, as this is a key claim.
2. Redundancy:
   - Section 4.3.1 could be shortened or removed, as its findings are intuitive and do not provide new insights.
3. Practical Implications:
   - The practical application to cloud detection is a strong point of the paper. However, more details on the computational efficiency and scalability of AdaBN in this context would strengthen the argument for its real-world utility.
Questions for the Authors
1. How does AdaBN compare to other domain adaptation methods in terms of computational cost, especially for large-scale applications like cloud detection?
2. Could you provide more insights into why combining AdaBN with CORAL improves performance in some cases but not in others (e.g., Caltech-Bing dataset)?
3. Is there a theoretical justification for why modulating BN statistics alone is sufficient for domain adaptation, especially in deeper layers of the network?
In summary, while the paper lacks significant novelty, its simplicity, effectiveness, and practical relevance make it a valuable contribution to the field. Addressing the issues of clarity and redundancy will further strengthen the paper.