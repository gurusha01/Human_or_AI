Review
Summary of the Paper
This paper introduces a reference-aware language modeling framework that explicitly incorporates reference decisions as stochastic latent variables. The proposed model is applied to three tasks: task-oriented dialogue modeling with database support, recipe generation from ingredient lists, and coreference-aware language modeling. The authors claim that their approach outperforms baseline models that rely on deterministic attention mechanisms. The paper also introduces 2-dimensional attention and pointer networks for handling 2-D tables, which are integrated into the proposed framework. Empirical evaluations are conducted on datasets created or adapted by the authors, with results reported in terms of perplexity and BLEU scores.
Decision: Reject
The paper is not ready for acceptance due to the following key reasons:
1. Poor Writing and Clarity: The paper is poorly written, with unclear explanations, inconsistent notation, and grammatical errors. Mathematical formulations contain errors, such as missing marginalization sums and incorrect probability values, which undermine the scientific rigor.
2. Lack of Novelty: The main contribution, 2-dimensional attention and pointer networks for 2-D tables, is a customization of existing methods rather than a novel approach. The work does not sufficiently advance the state of the art.
3. Inconclusive Empirical Results: The experiments are inconclusive due to the small size of the datasets and the lack of strong baselines for newer tasks like recipe generation. The evaluation metrics, such as BLEU and perplexity, are insufficient to validate the claims, especially for dialogue modeling.
Supporting Arguments
- Clarity and Rigor: The mathematical errors and unclear descriptions make it difficult to follow the technical details. For example, the marginalization over latent variables is not properly defined, and some probability values are incorrectly specified. This raises concerns about the correctness of the model and its implementation.
- Novelty: While the paper introduces 2-dimensional attention and pointer networks for table-based tasks, these are incremental extensions of existing techniques like attention mechanisms and pointer networks. The lack of novelty diminishes the paper's contribution to the field.
- Empirical Validation: The datasets used for evaluation are either small (e.g., DSTC2 for dialogue modeling) or lack proper baselines (e.g., recipe generation). The reported improvements in perplexity and BLEU are marginal and do not convincingly demonstrate the effectiveness of the proposed approach.
Suggestions for Improvement
1. Clarity and Writing: The paper needs significant revisions to improve clarity and eliminate mathematical errors. Detailed explanations of the proposed methods and their implementation are necessary to ensure reproducibility.
2. Novelty: The authors should focus on highlighting the unique aspects of their approach and positioning it more clearly within the existing literature. A deeper theoretical or methodological contribution would strengthen the paper.
3. Empirical Evaluation: The authors should use larger and more diverse datasets, especially for dialogue modeling, and compare their model against stronger baselines. Human evaluation for dialogue tasks could provide additional insights into the model's performance.
4. Evaluation Metrics: The paper should include more task-specific evaluation metrics beyond perplexity and BLEU, especially for dialogue and recipe generation tasks.
Questions for the Authors
1. Can you clarify how the marginalization over latent variables is implemented in your model? The mathematical formulation in Section 2 is incomplete.
2. How do you ensure that the datasets used for evaluation are representative and sufficient for benchmarking the proposed model?
3. Why were human evaluations not conducted for the dialogue modeling task, given its importance in assessing real-world applicability?
4. Can you provide more details on the computational complexity of the 2-dimensional attention and pointer networks compared to standard attention mechanisms?
In its current form, the paper is better suited for a workshop where the authors can refine their ideas and address the identified issues.