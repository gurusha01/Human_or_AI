Review
Summary of Contributions
The paper introduces two novel mechanisms for sentence summarization: a "Read-Again" attention model and a copy mechanism. The Read-Again model mimics human behavior by re-reading the input text to refine word representations, while the copy mechanism enables direct copying of words from the source text, addressing out-of-vocabulary (OOV) issues and reducing vocabulary size. The authors claim that these approaches achieve state-of-the-art results on the DUC2004 dataset and demonstrate competitive performance on the Gigaword dataset, with faster decoding times and reduced storage requirements. The paper also highlights the ability of the copy mechanism to handle rare words effectively and provides qualitative visualizations of attention weights and copy operations.
Decision: Reject
While the proposed mechanisms are interesting and potentially impactful, the paper lacks sufficient clarity, rigorous evidence, and comprehensive experimental evaluation to support its claims. The following key issues prevent acceptance:
1. Insufficient Baseline Comparisons: The Read-Again attention mechanism is not compared against simpler alternatives like vanilla attention or stacked LSTMs, making it unclear whether the observed improvements are due to the proposed method or other factors. Similarly, the copy mechanism is not compared with existing hard copy mechanisms, leaving its relative effectiveness unverified.
   
2. Lack of Ablation Studies: The paper does not provide ablation studies to isolate the contributions of the Read-Again model and the copy mechanism. This omission makes it difficult to assess the individual impact of each component.
Supporting Arguments
1. Unclear Necessity for Multi-Sentence Inputs: The paper extends the Read-Again model to handle multiple sentences but does not justify why this is necessary for single-sentence datasets like Gigaword. There is no comparison between single-sentence and multi-sentence inputs, leaving the utility of this extension ambiguous.
2. Weak Baselines on Gigaword: The experiments on the Gigaword dataset use weak baselines, which undermines the credibility of the state-of-the-art claims. Stronger baselines, such as more recent transformer-based models, should be included for a fair comparison.
3. Tangential Discussion on Vocabulary Size: While the paper discusses the benefits of reduced vocabulary size, it does not provide sufficient evidence linking this to the proposed vector-based copy mechanism. The claims about storage and decoding speed improvements are not rigorously quantified.
4. Clarity Issues: The handling of multiple identical words in the copy mechanism is not explained, and the mathematical formulation of the Read-Again model is overly complex without sufficient intuition. Additionally, minor typos (e.g., "Tab. 1" and "Fig. 3.1.2") detract from the paper's polish.
Suggestions for Improvement
1. Conduct thorough baseline comparisons, including vanilla attention, stacked LSTMs, and existing hard copy mechanisms, to establish the relative effectiveness of the proposed methods.
2. Provide ablation studies to isolate the contributions of the Read-Again model and the copy mechanism.
3. Justify the necessity of multi-sentence inputs and compare performance on single-sentence vs. multi-sentence datasets.
4. Use stronger baselines for experiments on Gigaword, such as transformer-based models, to substantiate state-of-the-art claims.
5. Clarify how the copy mechanism handles multiple identical words and simplify the mathematical exposition of the Read-Again model.
6. Address minor typographical errors to improve the paper's presentation.
Questions for the Authors
1. How does the Read-Again model compare to simpler alternatives like vanilla attention or stacked LSTMs?
2. Can you provide an ablation study to quantify the individual contributions of the Read-Again model and the copy mechanism?
3. Why is the multi-sentence extension necessary for single-sentence datasets like Gigaword, and how does it affect performance?
4. How does the copy mechanism handle cases where multiple identical words appear in the source text?
5. Can you provide more rigorous evidence linking reduced vocabulary size to the proposed vector-based copy mechanism?
In summary, while the paper presents promising ideas, it requires significant improvements in clarity, experimental rigor, and baseline comparisons to substantiate its claims.