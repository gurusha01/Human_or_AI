Review
The paper addresses the critical problem of reducing communication overhead in parallel training of neural networks using Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD) on multi-GPU systems. It proposes a novel Linear Pipelining (LP) scheme for collective communication operations (broadcast, reduce, and allreduce) tailored to leverage GPU architectural features such as bi-directional PCI-E and dual DMA engines. The authors claim that LP collectives achieve significant theoretical and empirical improvements over existing Bidirectional Exchange (BE) and Minimal Spanning Tree (MST) approaches, reducing communication costs by a factor of 2 and log(p), respectively. Furthermore, the paper demonstrates that LP collectives improve training convergence times for AlexNet and GoogLeNet without compromising computation costs.
Decision: Accept
The paper makes a strong core contribution by introducing a novel communication scheme that is both theoretically sound and empirically validated. The key reasons for acceptance are: (1) the proposed LP collectives show consistent and significant performance improvements over existing methods, both in theoretical analysis and practical experiments; and (2) the work is well-motivated and addresses a critical bottleneck in large-scale neural network training, making it a valuable addition to the literature.
Supporting Arguments:
1. Core Contribution and Implementation: The LP collectives are well-designed to exploit GPU architectural features, and the theoretical analysis convincingly demonstrates their efficiency. The implementation details, including the use of fine-grained message blocks and overlapping communication with computation, are thorough and innovative.
2. Empirical Results: The experimental results are robust, showing substantial speedups (e.g., 2.3x to 360.55x) over BE and MST collectives across various message sizes and GPU counts. The demonstrated improvements in training times for AlexNet and GoogLeNet further validate the practical utility of the proposed approach.
3. Theoretical Rigor: The cost model and comparative analysis are well-grounded in established MPI literature, and the invariance of LP costs to GPU count is a particularly compelling insight.
Additional Feedback:
1. Clarity Issues: 
   - The sentence about MPI processes sharing the same CPU memory bus is unclear and should be rewritten for better comprehension.
   - The terminology "sub-gradients" should be replaced with "partial gradients" to avoid confusion with the optimization literature's definition of "sub-gradient."
2. Scaling Limitation: The paper notes scaling issues when moving from 4 to 5 GPUs due to QPI traversal. This limitation warrants further investigation, as it may impact the scalability of the proposed approach in larger GPU clusters.
3. Unsupported Claims: The claim that MST collectives are only suitable for high-frequency, short messages is not adequately supported. A more detailed discussion of bandwidth scaling with message size would strengthen the argument.
4. Equation Accuracy: The block size equation appears incorrect and should be revisited for accuracy. Additionally, the claim about parameter inconsistency due to float multiplications should be reconsidered, as it is more likely caused by non-commutative floating-point addition during gradient accumulation.
Questions for Authors:
1. Could you clarify the sentence regarding MPI processes sharing the same CPU memory bus? How does this relate to the proposed LP collectives?
2. Have you tested the LP collectives on larger GPU clusters (e.g., >8 GPUs)? If not, how do you anticipate the QPI traversal issue scaling with GPU count?
3. Can you provide more evidence or experiments to support the claim that MST collectives are only suitable for short messages?
Overall, this paper makes a significant contribution to the field of distributed deep learning and is recommended for acceptance, provided the authors address the minor clarity and technical issues noted above.