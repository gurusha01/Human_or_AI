The paper introduces MusicNet, a large-scale, freely-licensed dataset of classical music recordings aligned with MIDI scores, and demonstrates its utility for polyphonic transcription using deep learning. MusicNet's scale and diversity—34 hours of music, over 1 million temporal labels, and 11 instruments—make it a significant contribution to music informatics. The authors benchmark several machine learning architectures, including spectrogram-based and end-to-end models, showing that learned features outperform traditional spectrograms for note prediction. The paper also highlights the potential of end-to-end models to learn frequency-selective filters, a promising step toward advancing music transcription tasks.
Decision: Accept with Minor Revisions
The key reasons for this decision are the novelty and significance of the dataset and the promising results demonstrated in the experiments. MusicNet addresses a critical gap in the field by providing a large, publicly available dataset for supervised learning in music research, which has historically lacked such resources. The experiments are scientifically rigorous, and the results support the claims made in the paper. However, the paper's organization and clarity need improvement, and some inconsistencies between the abstract/introduction and the stated focus of the paper should be addressed.
Supporting Arguments:
1. Novel Contribution: MusicNet's scale and diversity distinguish it from existing datasets like MIREX and MAPS, which are limited in size and scope. The alignment of recordings with MIDI scores is a valuable feature for supervised learning tasks.
2. Experimental Rigor: The authors benchmark multiple architectures and provide detailed analyses of their performance, including precision-recall curves and learned feature visualizations. The results are compelling and demonstrate the dataset's utility.
3. Relevance: The work is well-placed in the literature, addressing a longstanding call for large-scale labeled datasets in music informatics.
Additional Feedback for Improvement:
1. Organization and Flow: The paper is somewhat disorganized, with overlapping content in sections. Restructuring the paper to separate dataset description, methodology, and experiments more clearly would improve readability.
2. Abstract/Introduction Alignment: The abstract emphasizes the dataset and its applications, but the introduction shifts focus to feature learning. Clarify the primary focus to avoid confusion.
3. Dataset Expansion: While the dataset is impressive, it is skewed toward certain composers (e.g., Beethoven) and instruments (e.g., piano). Future plans to expand the corpus in terms of volume and diversity should be discussed in more detail.
Questions for the Authors:
1. Are there plans to include more diverse composers, instruments, or genres in future versions of MusicNet? How will this be achieved?
2. Can you elaborate on the alignment error rates and their potential impact on the transcription results?
3. Have you considered evaluating the dataset and models on real-world tasks, such as music generation or recommendation, to demonstrate broader applicability?
Overall, the paper makes a valuable contribution to the field, and with minor revisions, it will be a strong addition to the conference.