Review of the Paper
Summary of Contributions
This paper addresses the challenge of zero-shot task generalization in reinforcement learning (RL) by introducing a novel hierarchical deep RL architecture. The proposed system consists of a meta controller that interprets natural language instructions and communicates subtasks to a subtask controller, which executes the subtasks. Key innovations include:
1. Subtask Embeddings: A novel embedding mechanism that captures relationships between subtasks, enabling generalization to unseen instructions.
2. Temporal Abstractions: A differentiable architecture allowing the meta controller to operate at larger time scales, improving stability under delayed rewards.
3. Analogy-Making Regularization: A regularizer that enforces correspondences between subtasks, enhancing generalization.
The architecture is evaluated on a 2D grid world and a 3D visual environment, demonstrating promising results in generalizing to unseen instructions and longer instruction sequences.
Decision: Reject
While the paper introduces an innovative architecture with potential, it falls short in critical areas:
1. Empirical Validation: The experiments are limited to toy domains, and the results lack sufficient evidence of scalability to real-world or more complex environments.
2. Related Work Gap: The omission of Branavan's work on RL for interpreting natural instructions is a significant oversight, as it shares algorithmic similarities with this paper.
3. Clarity Issues: Several sections, including the abstract and Section 6, are difficult to follow, and key terms like "zero-shot" and "cooperate" are not clearly defined.
Supporting Arguments
1. Novelty and Relevance: The hierarchical architecture and subtask embeddings are innovative and could inspire broader applications, particularly in RL and NLP. The use of policy distillation is also a strong contribution that could benefit RL researchers.
2. Empirical Limitations: The paper's reliance on toy domains (e.g., 2D grid world and Minecraft) limits its impact. While the results are promising, they do not convincingly demonstrate scalability or robustness in real-world scenarios.
3. Literature Context: The lack of discussion on Branavan's work and related zero-shot generalization literature weakens the paper's positioning in the field. This omission could mislead readers about the novelty of the contributions.
4. Clarity and Accessibility: The paper's presentation could be significantly improved. Ambiguous terminology and dense explanations hinder comprehension, especially for readers unfamiliar with the specific subfield.
Suggestions for Improvement
1. Expand Empirical Validation: Include experiments in more complex, real-world environments to demonstrate scalability and robustness. For example, consider robotics tasks or real-world navigation problems.
2. Address Related Work: Discuss Branavan's work and other relevant studies on zero-shot generalization, highlighting similarities and differences. This will provide a more comprehensive context for the contributions.
3. Clarify Key Concepts: Rephrase the abstract and Section 6 for better clarity. Define terms like "zero-shot" and "cooperate" precisely, and ensure consistent usage throughout the paper.
4. Simplify Methodology: Provide a more streamlined explanation of the architecture and training process. A high-level overview with clear diagrams could help readers grasp the core ideas without getting lost in technical details.
5. Citations and Terminology: Add citations for related work on zero-shot generalization and refine the terminology to align with standard practices in RL and NLP.
Questions for the Authors
1. How does the proposed architecture perform in real-world or more complex environments beyond toy domains? Can you provide additional empirical evidence?
2. How does your work differ from Branavan et al.'s approach to RL with natural language instructions? Why was this work omitted from the discussion?
3. Can you elaborate on the distinction between zero-shot and one-shot learning in the context of your experiments? How do you ensure that the agent is truly generalizing in a zero-shot manner?
4. How sensitive is the analogy-making regularizer to hyperparameter choices? Have you explored its robustness across different tasks?
In summary, while the paper introduces several promising ideas, its limitations in empirical validation, related work discussion, and clarity prevent it from meeting the standards for acceptance at this time. Addressing these issues could significantly strengthen the paper's contributions and impact.