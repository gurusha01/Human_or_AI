Review
Summary of Contributions
This paper introduces the multiplicative LSTM (mLSTM), a novel recurrent neural network architecture that combines the strengths of LSTM and multiplicative RNN (mRNN) architectures. The authors argue that mLSTM's input-dependent transition functions make it more expressive for autoregressive density estimation tasks. The paper demonstrates mLSTM's performance across a variety of character-level language modeling tasks, including benchmarks like the Penn Treebank, Text8, and the Hutter Prize datasets. The results show that mLSTM outperforms standard LSTM in some tasks, particularly when combined with dynamic evaluation, achieving a state-of-the-art result on the Hutter Prize dataset. The paper also explores mLSTM's robustness to surprising inputs and its ability to model multilingual datasets effectively.
Decision: Reject
While the paper presents an interesting and novel architecture, the decision to reject is based on two key reasons: (1) the mLSTM underperforms compared to other models in most tasks, and (2) the lack of clarity in parameter comparisons across models weakens the empirical claims. Despite its novelty, the paper does not provide sufficient evidence that mLSTM is a consistently superior alternative to existing architectures.
Supporting Arguments
1. Performance Concerns: Although mLSTM achieves state-of-the-art results on the Hutter Prize dataset with dynamic evaluation, it underperforms on other tasks compared to existing models. For example, on the Text8 dataset, mLSTM is outperformed by other architectures that are less prone to overfitting. This inconsistency raises questions about the general applicability of the proposed model.
   
2. Lack of Parameter Comparisons: The paper does not provide detailed information about the number of parameters in mLSTM compared to baseline models. This omission makes it difficult to assess whether the observed performance differences are due to architectural improvements or simply an increase in model capacity.
3. Limited Scope: The paper focuses exclusively on character-level language modeling tasks. While the authors acknowledge this limitation in the discussion, the lack of experiments on word-level tasks or other domains reduces the broader impact of the work.
Suggestions for Improvement
1. Parameter Comparisons: Include a detailed comparison of the number of parameters for mLSTM and baseline models in all experiments. This will help clarify whether the performance gains are due to architectural innovations or increased capacity.
2. Broader Evaluation: Extend the evaluation to include word-level language modeling tasks or other generative modeling domains. This would demonstrate the generality of the proposed architecture.
3. Address Underperformance: Investigate and discuss why mLSTM underperforms on certain tasks. For example, analyze whether the model's increased expressiveness leads to overfitting or other optimization challenges.
4. Ablation Studies: Conduct ablation studies to isolate the contributions of different components of mLSTM (e.g., the factorized hidden weights vs. LSTM gating mechanisms) to better understand their individual impact.
Questions for the Authors
1. How does the number of parameters in mLSTM compare to baseline models in each experiment? Could parameter scaling explain the observed performance differences?
2. Can you provide more insights into why mLSTM underperforms on certain datasets, such as Text8, compared to other architectures?
3. Have you considered applying mLSTM to tasks beyond character-level language modeling, such as word-level tasks or continuous input domains? If so, what were the results?
In summary, while the paper introduces a novel and promising architecture, the inconsistent performance and lack of parameter clarity limit its impact. Addressing these issues in a revised version could make the work more compelling.