Review of the Paper
Summary of Contributions
This paper proposes an extension to the standard attention mechanism by incorporating structured distributions modeled as graphical models with neural network-derived potentials. The authors introduce structured attention layers, which generalize simple attention by embedding graphical models, such as linear-chain CRFs and dependency parsers, into deep networks. These layers are differentiable and allow end-to-end training. The paper demonstrates the utility of structured attention across multiple tasks, including tree transduction, neural machine translation, question answering, and natural language inference. The results show that structured attention models outperform baseline attention mechanisms and learn meaningful latent structures, even in the absence of explicit supervision.
Decision: Accept
The paper is well-written, introduces a novel and technically sound extension to attention mechanisms, and demonstrates its applicability across diverse tasks. While the performance improvements on real-world tasks are modest, the conceptual contribution of structured attention and its potential to model latent structures in a differentiable manner make this work a valuable addition to the field.
Supporting Arguments
1. Novelty and Technical Rigor: The proposed structured attention mechanism is a significant extension of existing attention models. By embedding graphical models into attention layers, the authors provide a principled way to incorporate structural biases, which is a novel contribution.
2. Empirical Validation: The paper demonstrates the effectiveness of structured attention on synthetic and real-world tasks. While the gains are modest in some cases, the results consistently show that structured attention outperforms simpler attention mechanisms.
3. Clarity and Presentation: The paper is well-organized and provides sufficient technical details, including implementation and training specifics, to enable reproducibility. The inclusion of visualizations and analyses of learned structures adds depth to the evaluation.
Additional Feedback for Improvement
1. Analysis of Alignment Quality: In the Japanese-English translation task, the performance gap between Sigmoid and Structured attention models is small. A deeper analysis of alignment quality could provide insights into whether the structured attention mechanism is learning meaningful segmentations or alignments.
2. Pretrained Syntactic Attention: The degradation in performance when using pretrained syntactic attention layers for the natural language inference task is surprising. The authors should investigate and discuss potential reasons for this behavior, such as overfitting or mismatched structural biases.
3. Efficiency Considerations: While the paper acknowledges the increased computational cost of structured attention (e.g., 5× slower training for machine translation), a more detailed discussion of scalability and potential optimizations would be beneficial for practitioners.
4. Typographical Errors: Minor typographical errors were noted in Equation 1 and Section 3.3. These should be corrected for clarity and precision.
Questions for the Authors
1. Can you provide more quantitative or qualitative evidence on the alignment quality learned by structured attention in the machine translation task?
2. Have you explored alternative ways to pretrain the syntactic attention layers (e.g., multi-task learning) to mitigate the observed performance degradation in the NLI task?
3. How does the structured attention mechanism perform when applied to larger datasets or more complex tasks? Are there any scalability concerns beyond the reported 5× slowdown?
In conclusion, this paper makes a strong conceptual and technical contribution to the field of attention mechanisms and structured learning. Addressing the above points would further strengthen the work, but they do not detract from its overall merit.