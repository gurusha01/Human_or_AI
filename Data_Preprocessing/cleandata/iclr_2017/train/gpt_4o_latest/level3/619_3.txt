Review
The paper proposes a simple yet effective optimization technique: adding annealed Gaussian noise to gradients during training. This method aims to address challenges in training deep and complex neural architectures, particularly under poor parameter initialization. The authors demonstrate the method's utility across a variety of tasks and models, including MNIST classification, question answering, and algorithm learning, and highlight its robustness and ease of implementation.
Decision: Accept
The paper should be accepted because it provides a practical and broadly applicable optimization technique that can benefit the deep learning community. While the method lacks novelty and strong theoretical grounding, its simplicity, utility, and demonstrated effectiveness on challenging tasks make it a valuable contribution.
Supporting Arguments
1. Strengths:  
   - The method is easy to implement (a single line of code) and compatible with existing optimizers like Adam and AdaGrad.  
   - It is evaluated on diverse tasks and architectures, showing generalizability. For example, it improves training robustness for Neural GPUs and Neural Programmer models, which are notoriously difficult to optimize.  
   - The experiments provide insightful comparisons, such as the annealed vs. fixed noise schedule and gradient noise vs. weight noise, offering practical guidance to practitioners.  
   - The paper is well-written, with clear explanations and relevant citations situating the work within the literature.  
2. Weaknesses:  
   - The method is not highly original, resembling Langevin dynamics but with a different goal.  
   - The theoretical justification is limited, and the paper does not provide a rigorous analysis of why the method works.  
   - Some results, such as those on MNIST and Section 4.2 (End-to-End Memory Networks), are less convincing, especially under sub-optimal training schemes.  
Despite these weaknesses, the paper's practical contributions and demonstrated utility outweigh its limitations.
Additional Feedback
1. Theoretical Justification: While the paper briefly mentions connections to Langevin dynamics and simulated annealing, a deeper theoretical analysis would strengthen the work. For instance, exploring the method's impact on escaping saddle points or its interaction with adaptive optimizers could provide valuable insights.  
2. Confidence Intervals: The lack of confidence intervals in the experimental results is a notable omission. Including these would improve the scientific rigor and help quantify the variability of the method's performance across runs.  
3. Negative Results: The discussion of failure cases, such as the Penn Treebank language modeling experiment, is appreciated. However, the authors could expand on why the method does not help in these cases and whether specific characteristics of the task or architecture make it unsuitable for gradient noise.  
4. MNIST Results: The results on MNIST, particularly with sub-optimal training schemes, are underwhelming. The authors should clarify whether this is due to the simplicity of the task or limitations of the method.  
Questions for the Authors
1. Can you provide more theoretical insights or empirical evidence on why annealed gradient noise outperforms fixed noise in complex models?  
2. How does the method compare to other regularization techniques like dropout or batch normalization when applied to the same tasks?  
3. Could the method's performance be improved by tuning the noise schedule (e.g., exploring different decay rates or functional forms)?  
Overall, the paper makes a strong case for the utility of gradient noise in training complex neural networks and has the potential to become a widely adopted optimization tool.