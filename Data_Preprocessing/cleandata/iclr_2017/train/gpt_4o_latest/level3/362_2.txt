Review of the Paper
Summary of Contributions
This paper proposes a novel approach to automating the design of optimization algorithms by framing it as a reinforcement learning problem. The authors represent optimization algorithms as policies and employ guided policy search (GPS) to learn these policies. The learned optimizer, referred to as "predicted step descent," is evaluated on various convex and non-convex optimization problems, including logistic regression, robust linear regression, and neural network training. The results demonstrate that the learned optimizer converges faster and/or achieves better optima compared to several hand-engineered algorithms. The paper also highlights the generalization capability of the learned optimizer to unseen objective functions and its robustness to changes in data distributions.
Decision: Reject
While the paper presents an interesting and ambitious idea, it falls short in several critical areas. The key reasons for rejection are: (1) the reliance on "randomly generated tasks" as a training and evaluation framework, which raises concerns about the practical relevance of the results, and (2) insufficient scientific rigor in addressing the limitations of the proposed method, particularly in comparison to state-of-the-art optimizers like L-BFGS and momentum.
Supporting Arguments
1. Problem Relevance and Motivation: The paper tackles an important problemâ€”automating optimization algorithm design. However, the use of "randomly generated tasks" as a training and evaluation mechanism is problematic. While the authors argue that this prevents overfitting to specific tasks, it is unclear how well the learned optimizer would perform on real-world tasks with more structured and meaningful objective functions. The authors' response to this criticism in the revised version remains unsatisfactory, as they do not provide empirical evidence on real-world benchmarks.
2. Scientific Rigor: Although the paper demonstrates improvements over hand-engineered optimizers in some scenarios, the results are not consistently superior. For example, L-BFGS outperforms the learned optimizer on convex problems, and the learned optimizer struggles against momentum in early iterations for non-convex problems. The authors do not provide a detailed analysis of when and why their method fails, which limits the scientific insight gained from the work.
3. Language and Presentation: The revised version of the paper shows improvement in toning down overselling of results, but the use of terms like "mantra" in the introduction is still inappropriate for a scientific paper. Such language detracts from the technical rigor and professionalism of the work.
Suggestions for Improvement
1. Evaluation on Real-World Tasks: To address concerns about the relevance of "randomly generated tasks," the authors should evaluate the learned optimizer on real-world optimization problems, such as training deep neural networks on standard datasets (e.g., CIFAR-10, ImageNet).
2. Comparison with State-of-the-Art: The authors should provide a more thorough comparison with state-of-the-art optimizers, including an analysis of computational overhead, scalability, and robustness to hyperparameter settings.
3. Ablation Studies: Conducting ablation studies to isolate the contributions of different components of the proposed method (e.g., neural network architecture, GPS algorithm) would strengthen the paper's claims.
4. Clarification of Limitations: The authors should explicitly discuss the limitations of their approach, such as its dependence on the choice of training tasks and the scalability of GPS to high-dimensional problems.
Questions for the Authors
1. How does the learned optimizer perform on real-world optimization problems, such as training deep neural networks on standard datasets?
2. Can the authors provide a detailed analysis of the failure cases, particularly when the learned optimizer underperforms compared to L-BFGS or momentum?
3. How does the computational cost of training and using the learned optimizer compare to hand-engineered optimizers?
In summary, while the paper presents an intriguing idea, it requires significant improvements in evaluation, scientific rigor, and presentation to meet the standards of the conference.