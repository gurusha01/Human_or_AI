Review of the Paper: "Learning a Better Optimization Algorithm"
Summary of Contributions
The paper proposes a novel approach to learning optimization algorithms using reinforcement learning (RL) and guided policy search (GPS). The authors frame optimization as a policy search problem within a Markov Decision Process (MDP), where the policy represents the optimization algorithm. By leveraging neural networks to parameterize the policy, the method learns optimization strategies tailored to specific classes of objective functions, including convex and non-convex problems. The learned optimizer, referred to as "predicted step descent," is shown to outperform hand-engineered algorithms such as gradient descent, momentum, and L-BFGS in terms of convergence speed and/or final objective value. The paper also highlights the potential of the approach to generalize across unseen tasks and provides visualizations of optimization trajectories to illustrate its behavior.
Decision: Reject
While the paper presents an interesting and promising methodological contribution, it falls short in demonstrating the robustness and generalization of the proposed approach. The key reasons for this decision are: (1) insufficient evidence supporting the claim that the learned optimizer generalizes beyond the training data distribution, and (2) limited exploration of the scalability of the method to higher-dimensional input spaces or more complex neural network architectures.
Supporting Arguments
1. Generalization Concerns: The authors claim that the learned optimizer captures regularities of an entire class of problems. However, the experiments primarily evaluate the optimizer on test sets drawn from the same distribution as the training data. While the appendix briefly explores transfer to different distributions, the results are not sufficiently detailed to assess robustness. Testing on significantly different distributions or real-world datasets would strengthen the claim of generalization.
2. Scalability and Neural Network Architecture: The choice of a small neural network with a single hidden layer is not well justified, particularly given the increasing complexity of modern optimization tasks. It remains unclear how the approach performs on higher-dimensional input spaces or more complex architectures, such as deep neural networks with millions of parameters.
3. Overstatement of Results: The paper's claims about outperforming hand-engineered algorithms may be oversold, as the experiments are constrained to relatively simple and synthetic problems. The learned optimizer's performance on more challenging, real-world optimization tasks is not demonstrated.
Suggestions for Improvement
1. Generalization Experiments: Conduct experiments where the optimizer is tested on objective functions drawn from distributions significantly different from the training set. For example, train on synthetic data and test on real-world datasets or problems with different structural properties.
2. Scalability Analysis: Evaluate the learned optimizer on higher-dimensional problems and more complex neural network architectures. This would provide insights into the method's applicability to large-scale optimization tasks.
3. Policy Visualization: Include visualizations of the learned policy, such as contour plots on 2D functions, to compare its behavior with hand-engineered optimizers. This could provide valuable intuition about the learned strategies.
4. Clarifications: 
   - Provide a more detailed justification for the choice of neural network architecture.
   - Clarify whether the optimization problems considered are noiseless or deterministic.
   - Standardize notation (e.g., \(\piT^{}\) vs. \(\pit^{}\)) for better readability.
Questions for the Authors
1. How does the learned optimizer perform on real-world datasets or tasks with significantly different distributions from the training set?
2. Why was a single hidden layer neural network chosen, and how does the method scale to higher-dimensional problems or deeper architectures?
3. Can the learned optimizer handle noisy or stochastic objective functions, and if not, how might the approach be extended to do so?
In conclusion, while the paper introduces an innovative approach to learning optimization algorithms, it requires further empirical validation and exploration of its generalization and scalability to justify its claims.