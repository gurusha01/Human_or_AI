Review of "Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning"
Summary of Contributions
This paper introduces the Central Moment Discrepancy (CMD), a novel metric for domain-invariant representation learning in unsupervised domain adaptation tasks. CMD explicitly matches higher-order central moments of probability distributions in the latent activation space of neural networks, offering a computationally efficient alternative to existing methods like Maximum Mean Discrepancy (MMD). The authors provide theoretical guarantees, proving that CMD is a metric and that convergence in CMD implies convergence in distribution for probability distributions on compact intervals. Empirical evaluations on benchmark datasets (Amazon Reviews and Office) demonstrate that CMD achieves state-of-the-art performance on most tasks, outperforming MMD and other domain adaptation methods like Variational Fair Autoencoders (VFAE) and Domain-Adversarial Neural Networks (DANN). The paper also highlights CMD's stability with respect to hyperparameters, reducing the need for extensive tuning.
Decision: Accept
The paper makes a significant contribution to the field of domain adaptation by proposing a theoretically grounded and computationally efficient method that achieves strong empirical results. The key reasons for acceptance are:
1. Novelty and Theoretical Rigor: CMD introduces a new metric for distribution matching, supported by solid theoretical analysis, including proofs of its metric properties and convergence guarantees.
2. Empirical Validation: The method achieves state-of-the-art performance on multiple benchmarks, demonstrating its practical utility.
3. Practical Advantages: CMD is computationally efficient, stable with respect to hyperparameters, and does not require kernel matrix computations, addressing key limitations of MMD.
Supporting Arguments
1. Comparison with MMD: CMD is computationally simpler (linear complexity vs. quadratic for MMD) and avoids the need for kernel parameter tuning, which is a well-known challenge in MMD-based methods. The empirical results confirm CMD's superiority in both performance and robustness.
2. Theoretical Contributions: The paper rigorously establishes CMD as a metric and proves its implications for distribution convergence, providing a strong foundation for its use in domain adaptation.
3. Empirical Robustness: CMD's stability with respect to its key parameter \( K \) (number of moments) is well-demonstrated, with the default choice \( K = 5 \) performing consistently across tasks.
Suggestions for Improvement
While the paper is strong overall, the following points could enhance its clarity and impact:
1. Generative Models: The potential applicability of CMD in training generative models is an exciting direction but remains unexplored. A brief discussion or preliminary experiments could strengthen the paper's broader relevance.
2. Independent Marginals Assumption: The assumption of independent marginals for CMD's empirical estimate is a limitation. The authors should discuss its implications more thoroughly and explore how CMD might handle dependencies between marginals.
3. Sample Complexity: The paper does not analyze CMD's sample complexity, particularly in high-dimensional spaces. A theoretical or empirical discussion would provide additional insights into its scalability.
4. Numerical Stability: While Proposition 1 addresses overall stability, the potential numerical instability of higher-order central moments during backpropagation warrants further investigation.
5. Visualization Improvements: Figure 3 could be decluttered by reducing the number of classes to better illustrate CMD's impact. Additionally, Figure 4 lacks a legend, which should be added for clarity.
6. Geometric Interpretation of \( K=5 \): The claim that \( K=5 \) has a natural geometric interpretation is intriguing but insufficiently explained. Providing references or elaboration would strengthen this argument.
Questions for the Authors
1. Can CMD be extended to handle dependencies between marginals, and how would this impact its theoretical properties and computational efficiency?
2. What is CMD's sample complexity in high-dimensional spaces, and how does it compare to MMD in such settings?
3. Could CMD be adapted for use in training generative models, such as GANs? If so, what challenges might arise?
4. Why does \( K=5 \) have a natural geometric interpretation, and why does this interpretation break down for \( K \geq 6 \)?
In conclusion, this paper makes a compelling case for CMD as a robust and efficient method for domain-invariant representation learning. Addressing the above points would further strengthen its contribution, but they do not detract from the paper's overall quality and significance.