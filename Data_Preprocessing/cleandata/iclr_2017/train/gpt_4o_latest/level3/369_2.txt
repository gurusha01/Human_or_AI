Review of "Trained Ternary Quantization (TTQ)" Paper
Summary of Contributions
This paper introduces Trained Ternary Quantization (TTQ), a novel neural network quantization method that compresses weights to ternary values while maintaining or slightly improving model accuracy. The authors propose a unique approach that learns both ternary values and their assignments using trainable scaling coefficients for positive and negative weights. The method achieves a 16Ã— reduction in model size and demonstrates minimal accuracy degradation on CIFAR-10 and ImageNet datasets. Notably, the paper reports a 0.3% improvement in Top-1 accuracy over full-precision AlexNet on ImageNet and a 3% improvement over prior ternary networks. The authors also highlight the potential of TTQ for energy-efficient inference on custom hardware.
Decision: Reject  
While the paper presents an interesting quantization approach, the contribution is incremental, and the practical utility of the method is undermined by several limitations. Specifically, the performance gains are marginal, the results on AlexNet are not compelling given prior work, and the lack of released code and detailed evaluation metrics (e.g., FLOPs, inference time) limits reproducibility and practical assessment.
Supporting Arguments for Decision
1. Incremental Contribution: The reported performance improvements are minimal, with only a 0.3% gain in Top-1 accuracy on ImageNet for AlexNet and marginal improvements on CIFAR-10. These gains are unlikely to justify the added complexity of the method, especially when compared to existing quantization techniques like TWN or DoReFa-Net. The improvements on AlexNet are particularly underwhelming, as prior work from the same group has already demonstrated significant compression on older architectures.
2. Lack of Practical Evaluation: The paper does not provide sufficient details on key metrics such as computational cost (e.g., FLOPs), inference speed, or energy efficiency. While the authors claim potential benefits for custom hardware, no empirical evidence or benchmarks are provided to support these claims. Without such details, the practical impact of TTQ remains speculative.
3. Reproducibility Concerns: The absence of released code and detailed experimental setups (e.g., hyperparameters, dataset preprocessing) makes it difficult to reproduce the results. This is a significant limitation for a method that aims to be adopted in real-world applications.
Suggestions for Improvement
1. Broader Evaluation: Include detailed benchmarks on computational efficiency, such as FLOPs, inference time, and energy consumption, to substantiate claims of practical benefits. Additionally, evaluate the method on more modern architectures (e.g., ResNet-50, MobileNet) to demonstrate its relevance beyond AlexNet.
2. Release Code: Provide open-source implementations to facilitate reproducibility and adoption by the community.
3. Clarify Novelty: Clearly differentiate TTQ from prior work, such as TWN and DoReFa-Net, by emphasizing the unique aspects of the method and their impact on performance.
4. Expand Results: Report results on larger datasets and more diverse tasks (e.g., object detection, segmentation) to demonstrate the generalizability of the approach.
Questions for the Authors
1. How does TTQ compare to other quantization methods in terms of computational cost (e.g., FLOPs) and inference time on real hardware?
2. Can the method be applied effectively to modern architectures like ResNet-50 or MobileNet, and if so, what are the observed benefits?
3. What is the impact of the hyperparameter choices (e.g., threshold factor `t`) on the model's performance and sparsity?
In conclusion, while TTQ is an interesting approach to ternary quantization, the incremental nature of the contribution, lack of practical evaluation, and reproducibility concerns limit its impact. Addressing these issues could significantly strengthen the paper.