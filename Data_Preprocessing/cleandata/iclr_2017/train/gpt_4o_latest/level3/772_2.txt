Review of the Paper
Summary
This paper investigates the factors influencing the effectiveness of image features extracted from pre-trained convolutional neural networks (CNNs) for instance-level image retrieval. The authors conduct extensive experiments on five key factorsâ€”feature aggregation and normalization, output layer selection, image resizing, multi-scale feature representation, and PCA with whitening. Based on their findings, they propose a new multi-scale image representation method and demonstrate its performance on four benchmark datasets. The paper claims state-of-the-art results, particularly when combined with a layer ensemble approach.
Decision: Reject
The paper is well-written and provides a thorough evaluation of factors affecting CNN-based image retrieval. However, it lacks sufficient novelty and fails to meet the conference's focus on advancing representation learning. The key reasons for rejection are the lack of innovative contributions and concerns about the scientific rigor of the methodology.
Supporting Arguments
1. Lack of Novelty: The paper primarily focuses on tweaking existing methods (e.g., using the last convolutional layer, PCA with whitening, and multi-scale representations) rather than introducing new architectures, training methods, or representation learning techniques. These findings are incremental and already well-known in the community.
2. Misleading Claims: The state-of-the-art results are attributed to the use of the deeper VGG-19 network rather than novel parameter choices. The paper does not convincingly demonstrate that its proposed multi-scale representation is the primary driver of performance improvements.
3. Limited Scope: The conclusions are specific to the VGG-19 architecture and do not generalize to other popular architectures like ResNet or Inception. This limitation makes the title and claims overly broad and somewhat misleading.
4. Parameter Tuning on Test Set: The authors tune parameters on the test set, which raises concerns about the validity of their conclusions. This practice undermines the scientific rigor of the results.
5. Minimal Contribution to Representation Learning: The paper treats CNNs as black-box feature extractors and does not explore new architectures or training paradigms for retrieval tasks. This approach does not align with the conference's focus on advancing representation learning.
Additional Feedback
1. Comparisons with Prior Work: The paper lacks comparisons with relevant prior works, such as Gordo et al. and Radenovic et al. The justification for excluding these comparisons is inconsistent and weakens the paper's claims of superiority.
2. Philosophical Concerns: The field is moving toward understanding and improving representation learning, yet this paper continues the trend of treating CNNs as black-box tools. The authors could explore new architectures or training methods to make a more meaningful contribution.
3. Interesting Insight: Figure 3 appears to provide a novel and interesting insight, but the paper does not elaborate on its significance. The authors should clarify its importance and potential implications.
4. Minor Issues: There are inconsistencies in the references, and the statement "harder than category retrieval" is vague and difficult to compare. The authors should provide clearer definitions and metrics.
Questions for the Authors
1. How do the proposed methods generalize to other architectures like ResNet or Inception? Can the authors provide experimental results on these architectures?
2. Why were certain prior works (e.g., Gordo et al., Radenovic et al.) excluded from comparisons? How do the proposed methods compare to these approaches?
3. Can the authors clarify the significance of Figure 3 and its implications for the broader field of image retrieval?
4. How do the authors address the issue of parameter tuning on the test set? Would the results hold under a more rigorous experimental setup?
In conclusion, while the paper provides a detailed evaluation of existing techniques, it lacks the novelty and rigor required for acceptance at a conference focused on advancing representation learning. The authors are encouraged to address these concerns and explore more innovative directions in future work.