The paper proposes a hierarchical reinforcement learning framework that combines skill pre-training using Stochastic Neural Networks (SNNs) with an information-theoretic regularizer and a high-level policy for downstream tasks with sparse rewards. The authors claim that their approach improves exploration efficiency and task performance by leveraging a diverse set of learned skills. The framework is evaluated on challenging tasks such as navigation mazes and object collection, demonstrating its potential to address sparse reward challenges.
Decision: Reject
While the paper presents a promising framework with strong experimental results, it falls short in several critical areas, including clarity, analysis, and replicability. These shortcomings hinder its overall contribution to the field.
Supporting Arguments for Rejection:
1. Weaker Analysis of Results: Although the experimental results are compelling, the paper lacks sufficient analysis to explain why the proposed method struggles in certain scenarios, such as complex mazes. The authors do not provide a clear interpretation of the limitations of the Mutual Information (MI) regularizer or its interaction with the SNN architecture. This omission weakens the scientific rigor of the claims.
2. Terminological Inaccuracy: The use of the term "intrinsic motivation" is misleading, as the pre-training relies on task-specific proxy rewards. This approach is more akin to task-specific pre-training, similar to Heess et al., rather than true intrinsic motivation. Mislabeling concepts can lead to confusion and misrepresentation in the literature.
3. Insufficient Specification for Reproducibility: The factorization between \( S{agent} \) and \( S{rest} \) is not adequately detailed, making it difficult for others to replicate the work. While \( S{agent} \) is discussed, \( S{rest} \) remains vague, leaving a critical component of the methodology underexplained.
Additional Feedback for Improvement:
1. Analysis of Switching Behavior: The paper would benefit from a deeper exploration of the agent's switching behavior, including failure modes and the impact of switching time \( T \) on performance. This analysis could provide insights into the robustness of the hierarchical approach and inform future improvements.
2. Interpretation of MI Regularizer: The authors should investigate and explain why the MI regularizer fails in complex environments. For instance, does the discretization of the state space limit its effectiveness? Providing such insights would strengthen the paper's contribution.
3. Broader Evaluation: The framework's generalizability could be better demonstrated by testing it on a wider range of tasks and robots. The current focus on specific tasks may limit the perceived applicability of the approach.
4. End-to-End Training: The authors acknowledge the limitation of fixed sub-policies and switching times. Exploring end-to-end training or adaptive switching policies could significantly enhance the framework's flexibility and performance.
Questions for the Authors:
1. How does the choice of discretization for the MI regularizer affect its performance in complex environments? Could alternative methods improve its robustness?
2. Can you provide more details on the factorization of \( S_{rest} \)? How does this factorization generalize across different tasks?
3. Have you considered alternative architectures or regularization techniques to address the limitations of SNNs in learning diverse skills?
In summary, while the paper introduces a novel and promising framework, its lack of rigorous analysis, unclear terminology, and insufficient methodological details prevent it from meeting the standards for acceptance at this time. Addressing these issues could significantly strengthen the work for future submissions.