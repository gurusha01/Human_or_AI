The paper proposes NoiseOut, a pruning method aimed at reducing the number of parameters in neural networks by removing neurons with highly correlated activations. The authors combine this with a novel technique of adding "noise outputs" to the network, which purportedly increases neuron correlation and facilitates more aggressive pruning. The paper demonstrates the method's effectiveness on various architectures and datasets, achieving significant parameter reductions while maintaining the original accuracy.
Decision: Reject
Key reasons: (1) Lack of clarity on the novelty and unique contribution of the proposed methods, particularly in comparison to existing regularization and pruning techniques. (2) Insufficient experimental evidence and missing critical comparisons to validate the claims.
Supporting Arguments:
1. Pruning by Correlated Neurons: The idea of pruning highly correlated neurons and adjusting downstream weights is straightforward and potentially effective. However, the novelty is unclear, as similar correlation-based pruning approaches exist in the literature. The paper does not adequately differentiate its approach from prior work.
   
2. NoiseOut as Regularization: The addition of noise outputs is an interesting idea, but the evidence suggests it functions primarily as a regularization technique, akin to DropOut or L2 regularization. The paper does not convincingly demonstrate how NoiseOut provides additional benefits beyond these established methods.
3. Missing Comparisons: The absence of comparisons to baseline methods (e.g., pruning without NoiseOut, pruning with only L2/DropOut) is a significant limitation. Without these, it is difficult to assess the unique contribution of NoiseOut or its effectiveness over existing techniques.
4. Experimental Gaps: While the paper reports significant parameter reductions, the mechanisms underlying these results are not well-explained. Additionally, the claim of maintaining accuracy lacks clarity—whether it refers to training or test accuracy is ambiguous, and this distinction is critical.
5. Presentation Issues: The use of lowercase rho (ρ) for correlation is not explicitly defined, leading to potential confusion. Furthermore, the lack of numerical comparisons to other pruning methods limits the contextual relevance of the results.
Suggestions for Improvement:
1. Clarify Novelty: Clearly articulate how NoiseOut differs from existing methods, particularly in relation to DropOut, L2 regularization, and other correlation-based pruning techniques.
   
2. Comparative Experiments: Include experiments comparing pruning with NoiseOut to pruning with standard regularization techniques (e.g., DropOut, L2) and without any regularization. This would help isolate the unique contribution of NoiseOut.
3. Accuracy Reporting: Explicitly state whether the reported accuracy refers to training or test performance. Additionally, provide more detailed results on how pruning affects generalization.
4. Broader Comparisons: Compare NoiseOut to state-of-the-art pruning methods quantitatively to establish its relevance and competitiveness.
5. Notation and Clarity: Define all notations explicitly (e.g., ρ for correlation) and ensure the exposition is clear and accessible.
Questions for the Authors:
1. How does NoiseOut differ fundamentally from DropOut or L2 regularization in terms of its impact on network capacity and generalization?
2. Can you provide quantitative comparisons to other pruning methods, such as magnitude-based or Hessian-based pruning, to contextualize the performance of NoiseOut?
3. Does the claim of maintaining accuracy refer to training or test accuracy? If test accuracy, how does NoiseOut affect generalization across datasets?
4. Have you investigated the impact of NoiseOut on training time and computational overhead compared to other pruning methods?
While the ideas in this paper are promising, the lack of clarity, insufficient experimental validation, and missing comparisons hinder its acceptance at this stage. Addressing these issues could significantly strengthen the paper.