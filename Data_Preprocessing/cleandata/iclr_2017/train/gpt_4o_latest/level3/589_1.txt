Review of "Graph Convolutional Recurrent Network (GCRN)"
Summary of Contributions:
This paper introduces the Graph Convolutional Recurrent Network (GCRN), a novel architecture combining graph convolutional networks (GCNs) and recurrent neural networks (RNNs) to model spatio-temporal sequences on graph-structured data. Two architectures are proposed: (i) a stack of GCN and RNN layers, and (ii) a graph-based variant of convolutional LSTM (convLSTM). The paper demonstrates the utility of GCRN on two tasks: video prediction using the moving-MNIST dataset and natural language modeling on the Penn Treebank (PTB) dataset. Experimental results show that GCRN achieves lower perplexity on PTB (with regularization) and outperforms LSTM+CNN on moving-MNIST, highlighting the potential of graph-based spatial representations in sequence modeling.
Decision: Reject
While the paper presents promising experimental results, the novelty of the proposed ideas is limited, and there are concerns regarding the experimental methodology, particularly for the PTB task. These issues undermine the scientific rigor and significance of the work.
Supporting Arguments:
1. Strengths:
   - The experimental results are encouraging, with GCRN outperforming baselines on moving-MNIST and achieving competitive perplexity on PTB under certain configurations.
   - The use of graph convolutions to replace matrix multiplications in RNNs is an interesting direction, particularly for tasks involving graph-structured data.
   - The paper is well-written and provides a comprehensive overview of related work, situating GCRN within the broader context of spatio-temporal sequence modeling.
2. Weaknesses:
   - Triviality of Ideas: The proposed architectures (stacking GCN and RNN, or replacing convolutions in convLSTM with graph convolutions) are straightforward extensions of existing methods. These align closely with current trends in combining GCNs with other neural architectures, limiting the novelty of the contribution.
   - Experimental Concerns on PTB: The PTB experiments use a one-hot word representation instead of the standard word embeddings, which is unconventional and raises questions about the validity of the results. Additionally, the graph construction process (based on word2vec embeddings) is not adequately justified or evaluated.
   - Limited Theoretical Insights: The paper lacks a deeper theoretical analysis of why graph convolutions improve performance in these tasks, particularly for video prediction on a regular grid (moving-MNIST), where standard CNNs are typically sufficient.
Suggestions for Improvement:
1. Novelty and Motivation: Clearly articulate the unique contributions of GCRN beyond combining existing architectures. For example, explore whether graph convolutions introduce specific inductive biases that improve generalization or learning speed.
2. Experimental Rigor: Address the unconventional setup for PTB experiments by justifying the use of one-hot representations and evaluating the impact of different graph construction methods. Consider comparing against stronger baselines, such as Transformer-based architectures, which are state-of-the-art for language modeling.
3. Theoretical Analysis: Provide insights into why graph convolutions outperform traditional convolutions in tasks like moving-MNIST. For example, analyze the isotropic nature of graph filters and its implications for spatio-temporal modeling.
4. Applications to Graph-Structured Data: Extend experiments to datasets where the graph structure is intrinsic (e.g., sensor networks or social graphs) to better demonstrate the utility of GCRN.
Questions for the Authors:
1. How does the choice of graph construction (e.g., k-nearest neighbors for moving-MNIST, cosine similarity for PTB) affect the performance of GCRN? Have alternative graph construction methods been explored?
2. Why was a one-hot word representation used for PTB experiments instead of embeddings, and how does this choice impact the results?
3. Can you provide a more detailed comparison of parameter efficiency between GCRN and baseline models, particularly for moving-MNIST?
In conclusion, while the paper shows promise, the lack of significant novelty, concerns about experimental methodology, and limited theoretical insights prevent it from meeting the standards for acceptance. Addressing these issues could make the work more impactful in future iterations.