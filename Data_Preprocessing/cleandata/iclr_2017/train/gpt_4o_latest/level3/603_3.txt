Review of the Paper
Summary of Contributions
This paper addresses the problem of source code completion in dynamically typed programming languages, specifically JavaScript, using LSTM-based neural network models. The authors propose several model variations, including NT2N, NT2NT, and NTN2T, to predict the next node in an Abstract Syntax Tree (AST). The paper introduces a novel "deny prediction" mechanism, allowing the model to abstain from predictions when uncertain, which is particularly relevant for Integrated Development Environment (IDE) applications. Empirical results demonstrate that the proposed models outperform state-of-the-art decision tree-based approaches for non-terminal prediction by 3.8 percentage points. However, the models underperform in terminal prediction compared to prior work. The authors also highlight the importance of leveraging structural information in ASTs and show that their models achieve higher accuracy on longer programs. The paper concludes with runtime evaluations, suggesting that the proposed methods are efficient enough for real-time code completion.
Decision: Reject
While the paper presents interesting ideas and achieves some improvements over prior work, the overall contribution is insufficient for acceptance at a top-tier conference like ICLR. The results, while promising, are incremental and fail to provide groundbreaking insights. Additionally, there are several methodological and presentation issues that need to be addressed.
Supporting Arguments for Decision
1. Incremental Contribution: The proposed models achieve modest improvements in non-terminal prediction but underperform in terminal prediction compared to prior work. The overall performance gains are not substantial enough to justify acceptance.
2. Unusual Model Design: The NT2NT model independently predicts non-terminals and terminals, which is unconventional and raises questions about the coherence of the approach. The lack of joint modeling for these interdependent tasks is a missed opportunity.
3. Evaluation Metrics: The decision to report separate metrics for terminals and non-terminals is questionable. A single, unified performance metric would provide a clearer picture of the model's effectiveness.
4. Related Work: The related work section is weak, with inaccuracies in citations and overly broad generalizations. This undermines the paper's positioning within the existing literature.
5. Empirical Results: While the "deny prediction" mechanism is novel, its impact on real-world usability is not thoroughly explored. Additionally, the performance drop in terminal prediction is not adequately explained.
Suggestions for Improvement
1. Unified Metric: Report a single, comprehensive metric that combines terminal and non-terminal prediction accuracy to provide a clearer evaluation of the model's overall performance.
2. Model Design: Explore joint modeling of non-terminals and terminals to better capture their interdependencies. This could lead to more coherent and effective predictions.
3. Related Work: Revise the related work section to include accurate and detailed comparisons with prior approaches. Highlight the specific gaps in existing methods that this paper addresses.
4. Deny Prediction Analysis: Provide a more detailed analysis of the "deny prediction" mechanism, including its impact on user experience in real-world IDE scenarios.
5. Error Analysis: Conduct a thorough error analysis to understand why the models underperform in terminal prediction and propose potential solutions.
Questions for the Authors
1. Why was the decision made to predict non-terminals and terminals independently in the NT2NT model? Would a joint modeling approach improve performance?
2. Can you provide more insights into the trade-offs between terminal and non-terminal prediction accuracy? How do these trade-offs impact the practical usability of the model in IDEs?
3. How does the "deny prediction" mechanism compare to alternative uncertainty modeling techniques, such as Bayesian approaches or confidence thresholds?
4. Could you clarify why separate metrics for terminals and non-terminals were chosen instead of a unified performance metric?
5. How does the model perform on other dynamically typed languages, such as Python, and would the results generalize across different programming languages?
In conclusion, while the paper introduces some interesting ideas, it requires significant revisions and stronger contributions to be competitive at ICLR. The suggestions provided aim to help the authors improve the quality and impact of their work.