The paper explores the interpretability of convolutional neural networks (CNNs) by introducing methods to quantify neuron selectivity to specific properties, such as color and class labels. It proposes two primary contributions: the definition of a "Neuron Feature" (NF), which is the pixel average of the top N images that highly activate a neuron, and the development of selectivity indexes for color and class. These indexes enable ranking neurons based on their selectivity, offering insights into how features and classes are represented across different layers of a CNN. The authors apply their framework to the VGG-M network trained on ImageNet, uncovering interesting patterns, such as the persistence of color selectivity in deeper layers and the emergence of highly class-selective neurons earlier than expected.
Decision: Accept
The paper is recommended for acceptance due to its potential to contribute to the long-term understanding of CNN behavior, despite its modest methodological contributions. The insights provided into neural network interpretability, such as the persistence of color selectivity and the alignment of color axes with human vision, are valuable and could inspire future research in both AI and neuroscience.
Supporting Arguments
1. Strengths: The paper provides intriguing observations about CNN behavior, such as the discovery of highly class-selective neurons in convolutional layers and the persistence of color selectivity in deeper layers. These findings challenge common assumptions about feature representation in CNNs and align with known properties of the human visual system, adding interdisciplinary relevance. The proposed framework for neuron ranking is straightforward and could be extended to other properties, making it a useful tool for future research.
2. Weaknesses: The methodological contributions, while clear and well-executed, are not groundbreaking. The definitions of NF and the selectivity indexes are incremental rather than transformative. Additionally, the paper does not present a major result or application that demonstrates the broader utility of its framework.
Suggestions for Improvement
1. Clarify the Color Selectivity Index: The computation and interpretation of the Color Selectivity Index require more explanation, particularly regarding the "two-color units" and their implications for neuron behavior.
2. Address Dataset Bias: The authors should discuss the potential impact of dataset biases (e.g., red mushrooms in ImageNet) on the Color Selectivity Index to ensure the conclusions are not misleading.
3. Correct Citation Errors: The citation for "Learning to generate chairsâ€¦" is incorrect and should be fixed to avoid confusion.
4. Expand on Broader Implications: While the paper speculates on localist versus distributed coding in CNNs, this discussion could be expanded with more rigorous analysis or experiments to strengthen the claims.
Questions for the Authors
1. How does the proposed framework generalize to other architectures beyond VGG-M? Have you tested it on deeper or more modern networks like ResNet or Vision Transformers?
2. Could you provide a more detailed explanation of how the threshold (th) for the Class Selectivity Index was chosen and its impact on the results?
3. Have you considered extending the framework to other properties, such as texture or shape selectivity, as mentioned in the conclusion?
In summary, while the paper's contributions are incremental, its insights into CNN interpretability and its potential for future extensions justify its acceptance. The authors are encouraged to address the suggested improvements to enhance the clarity and impact of their work.