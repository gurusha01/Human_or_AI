Review
Summary of the Paper
The paper proposes a novel approach to improve the trainability of deep neural networks by smoothing highly non-convex loss functions. Inspired by continuation methods and curriculum learning, the authors introduce a "mollification" technique that starts with a simpler, smoothed objective function and gradually transitions to the original, more complex objective. This is achieved through a single hyperparameter that is annealed during training. The method is shown to improve optimization and generalization on challenging tasks, including deep MLPs, parity problems, and CIFAR-10. The paper also draws connections to noise injection methods, residual connections, and batch normalization. Empirical results are provided to demonstrate the efficacy of the approach, with improvements over baseline models in several experiments.
Decision: Accept
The paper is well-motivated, clearly written, and provides a thorough theoretical and empirical analysis of the proposed method. The idea of mollifying loss functions is novel and well-situated in the context of existing literature. The experimental results convincingly demonstrate the benefits of the approach on a variety of tasks, making it a valuable contribution to the field.
Supporting Arguments
1. Problem and Motivation: The paper addresses a critical challenge in deep learningâ€”optimizing highly non-convex loss landscapes. The motivation is strong, as this issue affects the trainability and generalization of deep networks. The connection to continuation methods and curriculum learning is well-articulated and provides a solid theoretical foundation.
   
2. Scientific Rigor: The paper provides both theoretical derivations and empirical validation. The use of mollifiers is rigorously defined, and the experiments are comprehensive, covering diverse tasks and architectures. The results consistently show that the proposed method improves convergence and generalization.
3. Clarity and Writing: The paper is well-structured and easy to follow. Key concepts, such as mollifiers and their implementation, are explained in detail, making the work accessible to a broad audience.
Suggestions for Improvement
While the paper is strong overall, there are areas where additional experiments and analysis could further strengthen the work:
1. Probing Intermediate Layers: The authors could include experiments comparing the proposed method to techniques that insert probes into intermediate network layers. This would help clarify whether the benefits arise primarily from smoothing the loss surface or from other factors.
   
2. Algorithm Learning Tasks: The performance of the smoothing technique on algorithm learning tasks, such as those in the "Neural GPU Learns Algorithms" paper, remains unexplored. Investigating this would provide insights into the method's applicability to tasks requiring precise weight relaxation.
3. Combination with Other Techniques: The paper briefly mentions the potential to combine mollification with stochastic depth or other regularization techniques. Including experiments on such combinations could demonstrate the broader utility of the method.
4. Annealing Schedules: While the paper explores several annealing schedules, the choice of schedule appears to impact stability and performance. A more detailed analysis of why certain schedules work better could provide practical guidance for practitioners.
Questions for the Authors
1. How does the proposed method compare to inserting intermediate loss probes in terms of optimization speed and generalization?
2. Could mollification replace weight relaxation in tasks like those explored in the "Neural GPU Learns Algorithms" paper? If not, what are the limitations?
3. Have you tested the method on large-scale tasks like machine translation or other sequence-to-sequence problems? If so, what were the results?
4. How sensitive is the method to the choice of the annealing schedule and the initial noise level? Could these hyperparameters be learned dynamically during training?
Conclusion
This paper presents a novel and well-executed contribution to improving the trainability of deep neural networks. The proposed mollification technique is both theoretically grounded and empirically validated. While additional experiments could enhance the work, the current results are convincing enough to warrant acceptance.