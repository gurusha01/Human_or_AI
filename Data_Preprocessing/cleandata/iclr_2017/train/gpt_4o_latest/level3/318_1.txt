Review of the Paper
Summary of Contributions
This paper introduces the Gated Graph Transformer Neural Network (GGT-NN), an innovative extension of Gated Graph Sequence Neural Networks (GGS-NNs), to enable graph-to-graph classification tasks via gradient descent. The key contribution lies in the introduction of differentiable graph transformations, which allow the model to construct, modify, and utilize graph-structured intermediate representations. The proposed architecture demonstrates versatility, successfully solving a variety of tasks, including achieving state-of-the-art performance on the bAbI tasks. Additionally, the paper explores the model's ability to learn the rules of cellular automata and Turing machines, showcasing its potential for rule discovery. However, the model struggles with solving Rule 30 in the cellular automaton task, highlighting some limitations. The integration of GGS-NN ideas to generate textual output using graphs as intermediate representations is another noteworthy contribution.
Decision: Accept
The paper is well-motivated, contributes novel ideas to the field, and demonstrates state-of-the-art performance on benchmark tasks. However, there are areas where clarity and additional experimentation could improve the work. The decision to accept is based on the following reasons:
1. Novelty and Impact: The introduction of differentiable graph transformations is a significant advancement, enabling the model to handle graph-structured intermediate states and outputs. This opens up new possibilities for tasks involving structured data.
2. Empirical Validation: The model achieves impressive results on the bAbI tasks, outperforming existing state-of-the-art models in many cases. The ability to generalize to rule discovery tasks further underscores the model's potential.
Supporting Arguments
1. Problem Motivation and Placement in Literature: The paper is well-situated within the existing body of work on graph neural networks (GNNs) and memory-based architectures. It builds upon GGS-NNs and addresses their limitations by introducing graph-structured intermediate representations, which are more interpretable and flexible than vector-based memory states in models like Memory Networks and Differentiable Neural Computers.
2. Scientific Rigor: The results are presented in a scientifically rigorous manner, with detailed experiments on the bAbI tasks and rule discovery tasks. The inclusion of ablation studies (e.g., with and without direct reference) strengthens the claims. However, the failure to solve Rule 30 raises questions about the model's limitations in capturing certain types of complex rules.
3. Clarity and Reproducibility: The paper provides detailed descriptions of the graph transformations and their implementations, making the work reproducible. The inclusion of pseudocode and diagrams further aids understanding.
Suggestions for Improvement
1. Rule 30 Limitations: The paper should provide a deeper analysis of why the model struggles with Rule 30. Is it due to the complexity of the rule, the model architecture, or insufficient training data? Including experiments with alternative configurations or additional supervision could strengthen the discussion.
2. Scalability Concerns: The paper acknowledges the quadratic scaling of time and space complexity with the size of the graph. Exploring optimizations such as sparse edge connections or selective node processing could make the model more practical for larger graphs.
3. Generalization to Longer Sequences: While the model performs well on tasks with limited timesteps, its ability to generalize to significantly longer sequences (e.g., 50+ timesteps) remains unclear. Additional experiments on extended tasks would provide more insights into its scalability and robustness.
4. Comparison with Non-Graph Models: While the paper compares GGT-NN with other graph-based models, a more detailed comparison with non-graph-based architectures (e.g., transformers) on tasks like bAbI would highlight the unique advantages of graph-based representations.
Questions for the Authors
1. Can you provide more insights into the failure of the model on Rule 30? Did you experiment with alternative architectures or training strategies to address this limitation?
2. How does the model perform on tasks with significantly larger graphs or longer sequences? Are there any inherent bottlenecks that limit scalability?
3. Could the proposed graph transformations be extended to handle dynamic graphs with changing node and edge types over time?
4. How sensitive is the model to the choice of hyperparameters, such as the number of propagation steps or the size of the hidden state?
In conclusion, this paper makes a significant contribution to the field of graph neural networks and structured data processing. While there are areas for improvement, the novelty, empirical results, and potential impact justify acceptance.