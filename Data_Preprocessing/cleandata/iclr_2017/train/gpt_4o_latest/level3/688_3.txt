Review of the Paper
Summary of Contributions
This paper introduces a modification to the PoWER algorithm aimed at improving policy optimization with a limited number of updates. The authors propose a theoretically grounded approach that extends existing methods to handle negative rewards, enabling the use of control variates to reduce variance. The paper demonstrates the method's applicability through experiments on the Cartpole benchmark and a real-world online advertising dataset. The authors claim that their iterative approach outperforms the standard PoWER algorithm, particularly in settings where policy updates are constrained, such as industrial production systems or robotics.
Decision: Reject
The paper presents an interesting and well-motivated modification to the PoWER algorithm, but it falls short in providing sufficient empirical evidence to support its claims. The lack of rigorous comparisons with alternative methods and richer problem domains undermines confidence in the generality and scalability of the proposed approach.
Supporting Arguments for the Decision
1. Lack of Comparisons with Other Methods: While the paper positions itself as an improvement over PoWER, it does not compare its performance against other state-of-the-art reinforcement learning algorithms, such as REINFORCE, PPO, or TRPO. This omission makes it difficult to assess whether the proposed method offers a meaningful advancement in the broader context of reinforcement learning.
   
2. Limited Problem Domains: The experiments are restricted to the Cartpole benchmark and a single real-world online advertising task. The Cartpole task is overly simplistic, involving the optimization of only five parameters, and does not provide insights into how the method performs in more complex, high-dimensional, or continuous action spaces. Similarly, the advertising task lacks comparisons with alternative approaches, leaving the claimed benefits unsubstantiated.
3. Insufficient Experimental Evidence: The experiments fail to convincingly demonstrate the claimed benefits of the method. For instance, while the iterative approach shows improvements over PoWER, the results do not explore the scalability of the method to larger or more diverse datasets. Additionally, the paper acknowledges potential issues with high variance in regions of the parameter space but does not provide a robust solution to address this limitation.
Suggestions for Improvement
1. Broader Experimental Validation: Include comparisons with other reinforcement learning algorithms across a variety of benchmarks, including high-dimensional and continuous action spaces. This would provide a clearer picture of the method's strengths and weaknesses relative to existing approaches.
2. Rich Problem Domains: Test the method on more complex tasks, such as robotics or high-dimensional control problems, to better evaluate its scalability and generality.
3. Variance Mitigation: Address the high variance issue more rigorously, either through additional regularization techniques or by exploring alternative formulations of the lower bounds.
4. Detailed Analysis of Control Variates: While the use of control variates is an interesting extension, the paper could provide a more in-depth analysis of their impact on convergence and performance, especially in comparison to other variance reduction techniques.
Questions for the Authors
1. How does the proposed method compare to other state-of-the-art reinforcement learning algorithms, such as PPO or TRPO, in terms of performance and computational efficiency?
2. Have you considered testing the method on more complex benchmarks, such as MuJoCo or Atari, to evaluate its scalability and generality?
3. Can you provide more details on the computational cost of the iterative approach compared to standard PoWER, especially in high-dimensional settings?
4. How does the choice of control variates affect the performance across different tasks, and what are the trade-offs in terms of variance reduction versus computational overhead?
In summary, while the paper introduces a promising modification to the PoWER algorithm, its limited experimental scope and lack of rigorous comparisons with alternative methods prevent it from making a compelling case for acceptance. Addressing these issues in future iterations could significantly strengthen the paper.