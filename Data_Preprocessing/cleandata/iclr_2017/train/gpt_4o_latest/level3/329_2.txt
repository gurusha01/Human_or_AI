Review
Summary of Contributions
This paper proposes an innovative augmentation to the training of Generative Adversarial Networks (GANs) by incorporating a denoising autoencoder (DAE) to reconstruct high-level discriminator features. The authors introduce a novel "denoising feature matching" objective, which guides the generator towards more probable configurations in the discriminator's feature space. This approach is shown to improve the stability of GAN training and mitigate mode collapse, a common pathology in GANs. The method is evaluated on CIFAR-10, STL-10, and ImageNet datasets, demonstrating qualitative and quantitative improvements in sample quality, as measured by the Inception score. The paper also highlights the potential for broader applications of the proposed technique in semi-supervised and conditional GANs.
Decision: Reject
While the paper introduces a promising idea of leveraging discriminator features via DAEs, the overall contribution is not sufficiently compelling to warrant acceptance. The primary concerns are the limited practical significance of the improvements and the added complexity of the proposed method.
Supporting Arguments
1. Significance of Improvement: Although the Inception score is used to demonstrate quantitative gains, the improvement is marginal and difficult to interpret. The reliance on this metric, which has known limitations, does not convincingly establish the practical utility of the method. The qualitative results, while better, are restricted to low-resolution datasets (e.g., 32x32 CIFAR-10), limiting the broader applicability of the approach.
   
2. Complexity vs. Benefit: The proposed method introduces significant conceptual and implementation complexity by requiring an additional denoising autoencoder, which must be trained in parallel with the GAN. The paper does not convincingly argue that the benefits justify this added overhead, especially given the modest improvements observed.
3. Practicality: The method's robustness and ease of implementation are not well demonstrated. Training GANs is already challenging, and adding another component like a DAE may exacerbate these challenges. The paper does not provide sufficient evidence that the method can be reliably reproduced or scaled to higher-resolution tasks.
Suggestions for Improvement
1. Broader Evaluation: Demonstrate the method's effectiveness on higher-resolution datasets and more practical tasks, such as conditional image generation or semi-supervised learning. This would provide stronger evidence of its utility.
   
2. Alternative Metrics: Consider evaluating the method using additional metrics (e.g., Fr√©chet Inception Distance, diversity metrics) to provide a more comprehensive assessment of the improvements.
3. Ablation Studies: Include ablation studies to isolate the contribution of the denoising feature matching objective. This would clarify how much of the improvement is attributable to the proposed method versus other factors.
4. Complexity Analysis: Provide a detailed analysis of the computational and implementation overhead introduced by the DAE. Discuss strategies to mitigate these challenges.
Questions for the Authors
1. How sensitive is the proposed method to the architecture and hyperparameters of the denoising autoencoder? Did you explore alternative denoiser designs?
2. Can the method scale effectively to higher-resolution datasets or more diverse tasks? If not, what are the primary bottlenecks?
3. Have you considered combining this approach with other GAN stabilization techniques, such as Wasserstein GANs or spectral normalization? If so, what were the results?
In summary, while the paper introduces a creative idea with potential, the current results and analysis do not convincingly demonstrate its practical utility or justify the added complexity. Further work addressing these concerns could make this a stronger contribution in the future.