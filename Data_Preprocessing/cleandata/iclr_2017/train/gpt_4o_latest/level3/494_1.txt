Review of the Paper
Summary of Contributions
This paper presents a novel dataset derived from Higher-Order Logic (HOL) proofs, aimed at advancing machine learning (ML) techniques for interactive theorem proving (ITP). The authors propose framing proof statement usefulness as a binary classification task and benchmark several deep learning models, including logistic regression, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). The dataset, which includes over two million training examples, is publicly available and designed to facilitate various ML tasks relevant to theorem proving, such as premise selection and intermediate step generation. The paper highlights the potential of ML to improve proof search efficiency in ITP systems and identifies limitations in current deep learning models for logical reasoning tasks.
Decision: Weak Reject
While the research direction is promising, the paper suffers from several clarity and methodological issues that hinder its accessibility and scientific rigor. Addressing these concerns could elevate the paper to an acceptable standard.
Supporting Arguments
1. Clarity and Accessibility: The paper assumes significant familiarity with theorem proving concepts (e.g., LCF, deBruijn indices, OCaml-top level) without providing adequate explanations. This limits its accessibility to the broader ML audience, which is critical for interdisciplinary research.
   
2. Methodological Concerns: The train-test data split methodology is unclear, particularly whether examples in both sets can originate from the same conjecture. This ambiguity raises concerns about data leakage and the validity of the reported results.
3. Baseline Comparisons: While the authors benchmark deep learning models, the absence of standard NLP baselines (e.g., Bag of Words + SVM) makes it difficult to assess the true difficulty of the problem. Including such baselines would provide a more comprehensive evaluation.
4. Model and Dataset Analysis: The paper lacks an in-depth analysis of success and failure cases, which could provide valuable insights into model limitations and guide future research. Additionally, the global max-pooling layer in the CNN architecture is not sufficiently explained, leaving readers uncertain about its role in the model.
5. Logical Reasoning Limitations: The claim that the models lack logical reasoning is plausible but unsupported by concrete examples of model mistakes. Including such examples would strengthen the argument and clarify the dataset's challenges.
Suggestions for Improvement
1. Explain Theorem Proving Concepts: Provide concise explanations of domain-specific terms to make the paper accessible to an ML audience. A glossary or appendix could be helpful.
   
2. Clarify Data Splits: Clearly describe the train-test split methodology, ensuring that no conjecture contributes examples to both sets. This would address concerns about data leakage.
3. Add Standard Baselines: Include simple NLP baselines like Bag of Words + SVM to contextualize the performance of deep learning models.
4. Analyze Success and Failure Cases: Include qualitative analyses of model predictions, particularly failure cases, to identify patterns and inform future model design.
5. Provide Examples of Model Mistakes: Illustrate the challenges of logical reasoning by showcasing specific errors made by the models on representative examples.
6. Expand Discussion on Model Limitations: Elaborate on why the current models fail to leverage conjecture conditioning or sequence order, and propose concrete steps for improvement.
Questions for the Authors
1. How was the train-test split performed? Were conjectures in the training set disjoint from those in the test set?
2. Why were standard NLP baselines not included for comparison? Would their inclusion alter the interpretation of the results?
3. Can you provide examples of model mistakes to illustrate the claim that the models lack logical reasoning?
4. How does the global max-pooling layer in the CNN architecture contribute to the model's performance? Could alternative pooling strategies improve results?
In conclusion, while the dataset and research direction are valuable, the paper requires significant revisions to improve clarity, methodological rigor, and empirical analysis. Addressing these concerns would make it a stronger contribution to the field.