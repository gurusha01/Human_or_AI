Review
Summary
This paper proposes a novel approach to perceptual similarity judgment by fine-tuning a deep convolutional neural network (DCNN) using object persistence constraints within a Siamese triplet architecture. The authors introduce "Object Persistence Net" (OPnet), which modifies the view-manifold of object representations to better align with human perceptual similarity judgments. The model demonstrates a significant improvement in discriminating between objects within the same category while maintaining inter-category distinctions. The results show strong generalization to novel objects, categories, and synthetic datasets, as well as a closer alignment with human perceptual judgments compared to AlexNet. This work contributes to both computational neuroscience and computer vision by exploring the role of object persistence in shaping perceptual similarity.
Decision: Reject
While the paper presents an interesting idea and achieves promising results, it falls short in several critical areas. The reliance on synthetic datasets without testing on a robust real-world dataset like ALOI limits the practical applicability of the findings. Additionally, the paper does not sufficiently address the broader implications of its results or explore potential limitations of the proposed method.
Supporting Arguments
1. Strengths:
   - The connection to human perception, particularly object persistence and hierarchical groupings, is compelling and aligns with cognitive neuroscience theories.
   - The experimental results demonstrate significant improvements in similarity judgment tasks, particularly for novel objects and categories, suggesting the model's generalization capabilities.
   - The comparison to human perceptual similarity judgments adds an interdisciplinary dimension, which could interest both AI and cognitive science communities.
2. Weaknesses:
   - The experimental suite relies entirely on synthetic datasets, which limits the ecological validity of the findings. The omission of a real-world dataset like ALOI is a missed opportunity to validate the model's performance in practical settings.
   - While the paper claims generalization to novel objects, the results indicate some overfitting to the ShapeNet dataset, as evidenced by reduced performance on synthetic and Pokemon datasets.
   - The paper does not sufficiently discuss the limitations of the approach, such as its reliance on clean, controlled data or its potential challenges in handling real-world variations like lighting and occlusion.
Additional Feedback
1. Suggestions for Improvement:
   - Incorporate a real-world dataset (e.g., ALOI) to evaluate the model's performance in more complex and realistic scenarios.
   - Address the potential overfitting to ShapeNet by introducing additional datasets with diverse characteristics, such as texture, lighting, and background variations.
   - Provide a deeper discussion of the limitations of the proposed method and potential avenues for future work, such as handling real-world noise or extending the model to dynamic scenes.
2. Questions for the Authors:
   - How does OPnet perform on datasets with more complex variations, such as lighting, occlusion, or cluttered backgrounds?
   - Can the authors provide more quantitative comparisons with real-world datasets to validate the model's practical applicability?
   - How does the choice of synthetic datasets impact the generalizability of the findings to real-world perceptual similarity tasks?
Overall, while the paper presents an intriguing approach and achieves promising results, the lack of real-world validation and insufficient discussion of limitations prevent it from meeting the standards for acceptance at this time. Addressing these issues could significantly strengthen the contribution.