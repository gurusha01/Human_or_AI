The paper introduces Edward, a Turing-complete probabilistic programming language (PPL) that emphasizes compositionality in both modeling and inference. By treating inference as a first-class citizen, Edward enables users to flexibly compose and experiment with various inference methods, including point estimation, variational inference, and Markov Chain Monte Carlo (MCMC). The integration with TensorFlow provides significant computational advantages, such as GPU acceleration and efficient computational graph design. The paper claims that Edward bridges the gap between expressiveness and efficiency in PPLs, achieving flexibility comparable to traditional deep learning frameworks while maintaining computational efficiency. It also showcases Edward's ability to handle advanced inference techniques, such as hierarchical variational models and alternative loss functions.
Decision: Accept
The paper should be accepted because it introduces a novel and impactful approach to probabilistic programming through compositional inference, which is underexplored in the literature. Additionally, the integration with TensorFlow demonstrates significant practical benefits, including speedups over existing PPLs like Stan and PyMC3. The paper is well-motivated, scientifically rigorous, and provides both theoretical insights and empirical evidence to support its claims.
Supporting Arguments:
1. Novelty and Contribution: The focus on compositional inference is a significant contribution to the field of PPLs. By allowing inference to be as modular and compositional as modeling, Edward opens new possibilities for designing and experimenting with probabilistic models and algorithms.
2. Practical Impact: Leveraging TensorFlow for computational graph design and GPU acceleration is a practical innovation that makes Edward highly efficient. The reported speedups (e.g., 35x faster than Stan) highlight its potential for large-scale applications.
3. Scientific Rigor: The paper provides clear theoretical foundations for its design choices and validates its claims with empirical benchmarks, such as logistic regression and variational auto-encoders. The experiments demonstrate Edward's flexibility and efficiency convincingly.
Additional Feedback:
1. Clarification on Flexible Objective Functions: While the paper raises the possibility of using alternative loss functions like Rényi divergences, it does not clearly explain how users can implement and test these within Edward. Clarifying whether users need to define a new inference method class or if existing classes can be extended would strengthen the paper.
2. Usability for Non-Experts: The paper could benefit from a discussion on the learning curve for new users, especially those unfamiliar with TensorFlow or advanced inference techniques. Including more user-friendly examples or tutorials might enhance accessibility.
3. Dynamic Computational Graphs: The authors briefly mention challenges with dynamic computational graphs. Expanding on how Edward might address these challenges in future work would provide a more comprehensive outlook.
Questions for the Authors:
1. How does Edward handle the design and implementation of alternative loss functions, such as Rényi divergences? Is it necessary for users to define a new inference method class, or can existing methods be easily adapted?
2. Are there any limitations or trade-offs in Edward's integration with TensorFlow, particularly for models with complex control flow or recursion?
3. How does Edward compare to other PPLs like WebPPL or Venture in terms of ease of use and scalability for non-expert users?
In conclusion, Edward represents a significant advancement in probabilistic programming, and its focus on compositional inference and computational efficiency makes it a valuable contribution to the field. Addressing the feedback and questions raised would further enhance the paper's clarity and impact.