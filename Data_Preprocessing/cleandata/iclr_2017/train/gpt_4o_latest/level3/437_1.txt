Review of the Paper
Summary of Contributions
The paper introduces GA3C, a GPU-based implementation of the Asynchronous Advantage Actor-Critic (A3C) algorithm, which is currently a state-of-the-art reinforcement learning method. The authors propose a hybrid CPU/GPU architecture that leverages a system of queues to batch data, improving GPU utilization and throughput. A notable contribution is the automatic performance tuning strategy, which dynamically adjusts system parameters to maximize training efficiency. The authors demonstrate that GA3C achieves a 20% higher throughput compared to the original A3C implementation, with significant speedups in training, especially for larger deep neural networks. The paper also provides an open-source implementation, enabling further exploration of computational trade-offs in reinforcement learning.
Decision: Reject
While the paper presents a compelling computational optimization for A3C, it fails to adequately support its claims of improved learning speed and data efficiency. The lack of consistent evaluation protocols and the absence of time vs. score or data vs. score plots make it difficult to assess whether GA3C achieves comparable or superior learning performance to A3C. This undermines the scientific rigor of the paper and limits its impact.
Supporting Arguments for Decision
1. Strengths:
   - The proposed architecture is well-motivated and effectively addresses the GPU underutilization issue in A3C.
   - The automatic performance tuning strategy is an innovative feature that simplifies system configuration and adapts to varying computational loads.
   - The open-source release of GA3C is a valuable contribution to the research community.
   - The paper provides a thorough analysis of computational trade-offs, including GPU utilization, training batch sizes, and system configurations.
2. Weaknesses:
   - The evaluation lacks direct comparisons of learning speed and data efficiency between GA3C and A3C. While the authors claim faster convergence, the results are not presented in a standardized format (e.g., time vs. score or data vs. score plots) to substantiate this claim.
   - The evaluation protocols differ significantly from those in the original A3C paper, making it difficult to draw meaningful comparisons.
   - The paper does not adequately address the potential impact of policy lag on learning stability and convergence, particularly for games with high variance in rewards.
   - Some of the results (e.g., faster convergence for certain games) are anecdotal and lack statistical significance or broader generalization.
Suggestions for Improvement
1. Evaluation Metrics: Include time vs. score and data vs. score plots to directly compare the learning speed and data efficiency of GA3C with A3C. This would provide stronger evidence for the claimed improvements.
2. Consistency in Protocols: Align the evaluation protocols with those used in the original A3C paper to enable direct comparisons.
3. Policy Lag Analysis: Provide a more detailed analysis of the impact of policy lag on learning stability and convergence, particularly for challenging environments.
4. Statistical Significance: Report statistical significance for the experimental results to strengthen the claims of faster convergence and improved performance.
5. Broader Applicability: Discuss the applicability of GA3C to real-world reinforcement learning problems beyond gaming tasks, such as robotics or autonomous driving.
Questions for the Authors
1. Can you provide time vs. score or data vs. score plots to validate the claim of faster convergence compared to A3C?
2. How does policy lag affect the stability of GA3C across different environments, and how do you mitigate its impact?
3. Why were the evaluation protocols not aligned with those in the original A3C paper? Would re-evaluating GA3C under the same protocols yield comparable results?
4. Have you tested GA3C on real-world RL tasks beyond Atari games? If so, what were the results?
In conclusion, while the paper offers a valuable computational optimization for A3C, its lack of rigorous evaluation of learning performance limits its impact. Addressing these issues would significantly strengthen the paper.