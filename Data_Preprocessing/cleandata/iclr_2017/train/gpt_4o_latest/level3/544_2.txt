Review of the Paper
Summary of Contributions
This paper introduces a differentiable physics engine designed for optimizing robotic controllers using gradient-based methods. The authors claim that their engine, implemented for both CPU and GPU, can compute analytical gradients efficiently, enabling faster optimization compared to derivative-free methods. The paper demonstrates the engine's utility across various robotic tasks, including optimizing neural network controllers for robotic arms, quadrupeds, and pendulum systems. The authors argue that this approach opens new possibilities for integrating deep learning and robotics, offering an alternative to methods like deep Q-learning. The paper also highlights the potential for extending this engine to optimize hardware parameters and for adversarial robotics training.
Decision: Reject  
Key reasons for rejection:
1. Lack of Comparative Benchmarks: The paper does not provide performance comparisons against existing popular physics engines like MuJoCo, Bullet, or ODE, leaving the claimed speed improvements unsubstantiated.
2. Unclear Performance Metrics: The speed is reported in unconventional units (e.g., "model seconds per day"), making it difficult to evaluate the engine's actual performance relative to other engines.
Supporting Arguments
1. Comparative Benchmarks: While the paper emphasizes the speed of the proposed engine, it fails to benchmark its performance against widely-used engines such as MuJoCo or Bullet. This omission makes it challenging to assess the engine's competitiveness. For instance, MuJoCo is known for its speed and efficiency, and without direct comparisons, the claim of "tremendous speed-up" remains speculative.
2. Collision Limitations: The engine's collision detection is restricted to basic shapes (sphere/sphere and sphere/plane), which significantly limits its applicability to more complex robotic systems. This limitation is not sufficiently addressed in the discussion.
3. Unconventional Metrics: Reporting speed in terms of "model seconds per day" or "model states" is unconventional and makes it difficult to compare the engine's performance with existing tools. Standardized metrics, such as simulation steps per second, would make the results more interpretable.
Suggestions for Improvement
1. Include Benchmarks: The paper would benefit greatly from performance comparisons with established engines like MuJoCo, Bullet, and ODE. These benchmarks should evaluate both forward simulation speed and gradient computation efficiency.
2. Expand Collision Capabilities: Extending the collision detection to handle more complex shapes (e.g., cubes, capsules) would make the engine more versatile and applicable to a broader range of robotic tasks.
3. Clarify Performance Metrics: Use standardized units such as simulation steps per second or wall-clock time for gradient computation to make the results more accessible and comparable.
4. Address Limitations: The paper should explicitly discuss the trade-offs of using rotation matrices instead of quaternions and the implications of limited conditional branching in Theano for GPU implementations.
Questions for the Authors
1. How does the proposed engine compare to MuJoCo, Bullet, or ODE in terms of forward simulation speed and gradient computation time?
2. Why were unconventional units (e.g., "model seconds per day") chosen for reporting speed, and how do these translate to standard metrics like simulation steps per second?
3. Are there plans to extend the collision detection capabilities beyond basic shapes? If so, what is the expected impact on performance?
4. How does the engine handle numerical stability issues, particularly given the use of rotation matrices and renormalization?
In conclusion, while the paper presents an interesting idea with potential for advancing robotics optimization, the lack of comparative benchmarks, unclear performance metrics, and limited collision capabilities make it difficult to assess its practical impact. Addressing these issues would significantly strengthen the paper.