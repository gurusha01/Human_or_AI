The paper introduces RASOR, a novel architecture for extractive question answering, and evaluates its performance on the SQUAD dataset. The model explicitly represents and scores all possible answer spans using recurrent neural networks, enabling efficient computation of fixed-length span representations. By leveraging both passage-aligned and passage-independent question representations, RASOR achieves a significant improvement in exact match and F1 scores over prior baselines, including Wang & Jiang (2016). The paper claims a 5% improvement in exact match and a 3.6% improvement in F1, reducing the error of Rajpurkar et al.'s baseline by over 50%. However, the absence of blind test results due to copyright issues limits broader comparisons, and concerns about potential overfitting on the development set are addressed through cross-validation experiments.
Decision: Reject
The paper demonstrates technical novelty and achieves strong results on the SQUAD dataset. However, the lack of blind test results undermines the generalizability of the findings, and concerns about hyperparameter tuning on the development set remain unresolved. While the methodology is sound and well-motivated, the inability to compare against the full leaderboard and validate the model's performance on unseen data is a critical limitation.
Supporting Arguments:
1. Strengths:
   - The paper tackles a well-defined problem in extractive question answering and introduces a novel architecture that efficiently computes span representations.
   - The use of both passage-aligned and passage-independent question representations is well-motivated and empirically validated, with ablation studies demonstrating their complementary benefits.
   - RASOR achieves state-of-the-art results on the SQUAD development set, outperforming prior baselines and showcasing the advantages of global normalization and explicit span modeling.
2. Weaknesses:
   - The absence of blind test results is a significant drawback. Without these results, it is difficult to assess the model's robustness and generalizability beyond the development set.
   - While the authors conduct cross-validation experiments to mitigate concerns about overfitting, the reliance on the development set for hyperparameter tuning raises questions about the validity of the reported improvements.
   - The paper does not compare RASOR to unpublished systems on the SQUAD leaderboard, which could provide additional context for its performance.
Suggestions for Improvement:
1. Blind Test Results: Resolving the copyright issues and providing results on the SQUAD test set is essential for validating the model's generalizability and enabling meaningful comparisons with other systems.
2. Broader Comparisons: Including comparisons with unpublished systems on the SQUAD leaderboard would strengthen the paper's claims and situate RASOR more effectively within the current state of the art.
3. Error Analysis: While the paper includes some analysis of failure cases, a more detailed discussion of the model's limitations (e.g., semantic dependencies) and potential solutions would enhance its impact.
4. Reproducibility: Providing the code and pretrained models would facilitate independent verification of the results and encourage further research building on this work.
Questions for the Authors:
1. Can you provide an update on the status of the blind test results? How do you plan to address the copyright issues?
2. How does RASOR compare to unpublished systems on the SQUAD leaderboard, such as Co-attention or r-net?
3. Could you elaborate on the potential impact of hyperparameter tuning on the development set? How confident are you that the cross-validation experiments mitigate overfitting concerns?
In conclusion, while the paper presents a promising approach to extractive question answering, the lack of blind test results and broader comparisons limits its contribution. Addressing these issues would significantly strengthen the paper for future submissions.