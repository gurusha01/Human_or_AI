Review of the Paper
Summary of Contributions  
This paper introduces the Graph Convolutional Recurrent Network (GCRN), a novel deep learning architecture that generalizes classical recurrent neural networks (RNNs) to graph-structured data. By combining graph convolutional neural networks (CNNs) with RNNs, the proposed model aims to capture both spatial and temporal patterns in structured sequences. Two architectures are proposed: one stacks graph CNNs and RNNs, while the other generalizes convolutional LSTMs to graphs. The paper evaluates these models on two tasks: video prediction using the Moving MNIST dataset and natural language modeling on the Penn Treebank dataset. The authors claim that GCRN improves learning speed and precision by leveraging graph structures, particularly in video prediction.
Decision: Reject  
The paper is not ready for acceptance due to weak technical contributions, limited experimental evaluation, and insufficient clarity in writing. While the idea of generalizing RNNs to graph-structured data is interesting, the execution lacks rigor and novelty.
Supporting Arguments for Decision  
1. Limited Novelty: The paper builds on existing work by Shi et al. (2015) and Defferrard et al. (2016), but the generalization is incremental. The graph convolution formulation treats all edges equally, losing the directional convolutional kernels' advantage in Shi et al. This limits the model's ability to fully exploit graph structures.  
2. Sparse Experimental Evaluation: The experiments are insufficient to validate the claims. For example, the Penn Treebank results lack comparison with recent state-of-the-art methods, and the interpretation of results is minimal. Key insights into why the proposed model performs better in certain cases are missing.  
3. Writing and Clarity: The paper lacks clarity in explaining the model's mechanisms and its advantages over existing methods. Important questions, such as how the model handles graph neighborhood width and directionality in Moving MNIST, remain unanswered.  
4. Weak Results on Language Modeling: The GCRN underperforms standalone LSTMs in some configurations on the Penn Treebank dataset, raising concerns about its robustness and applicability to language tasks.
Suggestions for Improvement  
1. Enhance Technical Contributions: Address the isotropic nature of the graph convolutional filters and explore ways to incorporate directional or weighted edge information. This could make the model more expressive and better suited for graph-structured data.  
2. Expand Experimental Evaluation: Include more datasets and tasks, such as dynamic graph signals (e.g., fMRI or sensor networks), to demonstrate the model's versatility. Compare results with recent state-of-the-art methods to contextualize the performance.  
3. Improve Clarity and Insights: Provide a detailed analysis of why the proposed model performs better in certain cases, particularly in Moving MNIST. Discuss the implications of isotropic filters and their impact on performance.  
4. Address Open Questions: Clarify how the model handles graph neighborhood width, directionality, and other structural properties. This will help readers understand the model's limitations and potential.  
5. Optimize Model 2: The poor performance of Model 2 on language tasks due to increased dimensionality should be addressed. Consider dimensionality reduction techniques or alternative architectures to improve its applicability.
Questions for the Authors  
1. How does the model handle the directionality of edges in graph-structured data, particularly in tasks like Moving MNIST?  
2. Why does Model 1 outperform Model 2 in language modeling, and how can Model 2 be improved?  
3. Can the proposed GCRN architecture be extended to dynamic graph signals, and if so, how would it perform on such tasks?  
4. Why were recent state-of-the-art methods for Penn Treebank not included in the comparison?  
In conclusion, while the paper presents an interesting idea, it requires significant improvements in technical depth, experimental rigor, and clarity to be considered for acceptance.