Review of the Paper
Summary of Contributions
This paper proposes a novel algorithm, referred to as b-GAN, that extends f-GAN by leveraging Bregman divergences for density ratio matching. The discriminator estimates density ratios, while the generator minimizes the f-divergence. The authors claim that their approach preserves the theoretical motivation of the original GAN objective, which is often altered in practice to address gradient issues. Additionally, the paper provides a unified framework for understanding GANs from a density ratio estimation perspective, incorporating insights from density ratio estimation literature. The experimental results suggest that Pearson divergence and relative density ratio estimation improve GAN training stability, offering practical insights into divergence selection.
Decision: Reject
The primary reasons for rejection are the lack of clarity and rigor in the experimental section and the insufficient motivation and intuition provided for the proposed approach. While the theoretical framework is interesting, the paper fails to convincingly support its claims with clear and scientifically rigorous experiments. Moreover, the lack of comparisons with baseline GAN objectives and the reliance on heuristic tricks undermine the paper's contributions.
Supporting Arguments
1. Experimental Issues: The experimental section is poorly presented, with confusing and uninformative figures. The claims about learning stability are not convincingly demonstrated, as the results lack statistical rigor and fail to provide meaningful comparisons. For example, while the authors argue that Pearson divergence improves stability, the evidence provided is anecdotal and lacks quantitative metrics or baselines for comparison.
2. Lack of Comparison: Despite criticizing the standard GAN objective, the paper does not include experiments comparing b-GAN with the original GAN objective. This omission raises questions about the completeness of the evaluation and whether the proposed method truly outperforms existing approaches.
3. Use of Heuristics: The authors criticize traditional GANs for relying on heuristic objectives but then introduce multiple heuristic tricks to make b-GAN work. This inconsistency weakens the paper's argument and raises concerns about the robustness and generalizability of the proposed method.
4. Clarity and Motivation: The paper lacks clarity in its presentation, particularly in the experimental and methodological sections. The motivation for using Bregman divergences over other divergences is not well-articulated, and the intuition behind the proposed approach is difficult to follow. This makes it challenging for readers to fully grasp the significance of the contributions.
Suggestions for Improvement
1. Experimental Design: The authors should provide clearer and more informative experimental results, including quantitative metrics (e.g., Inception Score, FID) and comparisons with baseline GAN objectives. Statistical significance tests should be included to support claims about stability.
2. Baseline Comparisons: The paper must include experiments comparing b-GAN with standard GAN objectives and other state-of-the-art GAN variants. This would provide a more comprehensive evaluation of the proposed method.
3. Heuristic Justification: The authors should explicitly justify the heuristic tricks used in b-GAN and discuss their impact on the results. If possible, they should explore alternatives to reduce reliance on heuristics.
4. Clarity and Intuition: The paper should be revised to improve clarity, particularly in the methodological and experimental sections. Providing more intuition and motivation for the use of Bregman divergences would make the contributions more accessible to readers.
Questions for the Authors
1. Why were comparisons with the standard GAN objective omitted, despite the paper's criticism of it?
2. How do the heuristic tricks introduced in b-GAN affect its generalizability and robustness across different datasets?
3. Can the authors provide more quantitative evidence to support the claim that Pearson divergence improves stability?
4. What is the theoretical or empirical justification for using Bregman divergences over other divergences in the context of GANs?
In conclusion, while the paper introduces an interesting theoretical framework, the lack of experimental rigor, baseline comparisons, and clarity in presentation significantly undermines its contributions. Addressing these issues would greatly enhance the paper's impact and credibility.