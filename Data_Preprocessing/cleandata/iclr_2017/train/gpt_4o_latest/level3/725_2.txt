Review
Summary of Contributions
This paper addresses the challenge of efficiently training large-scale stochastic feedforward neural networks (SFNNs), which are known for their expressive power and regularization benefits but are notoriously difficult to train. The authors propose a novel intermediate model, Simplified-SFNN, which bridges deterministic deep neural networks (DNNs) and SFNNs. They establish a connection between these three models (DNN → Simplified-SFNN → SFNN), enabling the transfer of pre-trained DNN parameters to Simplified-SFNN and subsequently to SFNN. The paper demonstrates that Simplified-SFNN maintains the stochastic benefits of SFNN while being computationally more efficient to train. The authors also introduce a novel connection between ReLU-based DNNs and Simplified-SFNNs, which is an important contribution. Empirical results show that the proposed approach outperforms baseline DNNs on classification and multi-modal tasks across several datasets, including MNIST, CIFAR-10, CIFAR-100, and SVHN.
Decision: Accept
The paper makes a significant contribution to the field by proposing a novel intermediate model that addresses the computational challenges of training SFNNs. The theoretical connections between DNN, Simplified-SFNN, and SFNN are rigorously established, and the empirical results support the claims. The novelty of the ReLU-based connection and the demonstrated improvements in performance on multiple datasets further strengthen the case for acceptance.
Supporting Arguments
1. Novelty and Theoretical Rigor: The connection between ReLU-based DNNs and Simplified-SFNNs is novel and well-supported by theoretical analysis. The authors provide rigorous proofs for parameter transformations and error bounds, ensuring the scientific validity of their claims.
2. Practical Impact: The proposed Simplified-SFNN model offers a practical solution to the computational inefficiencies of training SFNNs. By leveraging pre-trained DNN parameters, the approach is both scalable and efficient.
3. Empirical Validation: The experimental results on diverse datasets demonstrate that Simplified-SFNN consistently outperforms baseline DNNs, showcasing its regularization benefits and multi-modal learning capabilities. The use of state-of-the-art architectures like Wide Residual Networks (WRNs) further validates the approach's applicability to real-world tasks.
Suggestions for Improvement
1. Scalability and Large-Scale Datasets: While the paper demonstrates positive results on small and medium-scale tasks, it would be beneficial to evaluate the approach on larger datasets (e.g., ImageNet) to address concerns about scalability.
2. Ablation Studies: An ablation study to isolate the contributions of different components (e.g., the stochastic regularization effect of Simplified-SFNN) would provide deeper insights into the model's performance.
3. Comparison to Other SFNN Training Methods: The paper could include a more comprehensive comparison with existing methods for training SFNNs to better contextualize its contributions.
4. Hyperparameter Sensitivity: The choice of hyperparameters (e.g., γ values) plays a critical role in the proposed transformations. A detailed analysis of their sensitivity would help practitioners adopt the method more effectively.
Questions for the Authors
1. How does the proposed approach scale to datasets with significantly larger training sets, such as ImageNet? Are there any computational bottlenecks in training Simplified-SFNNs for such tasks?
2. Can the proposed method be extended to other types of stochastic neural networks beyond SFNNs? If so, what are the potential challenges?
3. How sensitive is the performance of Simplified-SFNN to the choice of the γ hyperparameters in the parameter transformations? Could an adaptive method for selecting these values improve the results?
Overall, this paper makes a substantial contribution to the field of stochastic neural networks and provides a promising direction for future research. The theoretical and empirical results are compelling, and the proposed Simplified-SFNN model addresses a critical bottleneck in training SFNNs.