Review of the Paper
Summary of Contributions
This paper introduces QRAQ (Query, Reason, and Answer Questions), a novel synthetic domain designed to test reasoning and interaction capabilities of learning-based agents in multi-turn conversational settings. The authors present two key contributions: (1) the QRAQ domain, which extends the bAbI problems by requiring reasoning with incomplete information over multiple turns, and (2) reinforcement learning (RL)-based memory network architectures, including a baseline (baseRL) and an improved model (impRL) with soft-attention over memory hops. The paper evaluates these architectures on datasets of varying complexity, demonstrating their ability to solve challenging reasoning tasks with limited feedback. The authors also provide supervised learning baselines to establish upper bounds for performance.
Decision: Reject
While the paper presents an interesting and relatively novel integration of memory networks with reinforcement learning, it falls short in several critical areas. Specifically, the handling of multiple variables, out-of-vocabulary (OOV) answers, and the curriculum learning component are insufficiently addressed. Additionally, the data sample selection strategy during training iterations lacks clarity, which undermines the reproducibility and interpretability of the results.
Supporting Arguments for the Decision
1. Handling of Multiple Variables: The paper does not adequately explain how the model resolves ambiguities when multiple variables are present in a single sentence. This is a critical aspect of the QRAQ domain, and the lack of clarity raises concerns about the robustness of the proposed architectures.
   
2. Out-of-Vocabulary (OOV) Answers: There is no discussion on how the models handle OOV answers, which is a common challenge in natural language processing tasks. This omission limits the generalizability of the proposed approach to real-world scenarios.
3. Curriculum Learning Analysis: While the authors mention a curriculum learning strategy, the analysis of its impact on RL training is minimal. Given its importance in guiding the agent's learning process, a more detailed exploration is necessary to understand its role in achieving the reported results.
4. Data Sample Selection Strategy: The paper does not provide sufficient details on how data samples are selected during training iterations. This lack of transparency makes it difficult to assess whether the reported results are due to the model's capabilities or biases in the training data.
Suggestions for Improvement
1. Clarify Variable Handling: Provide a detailed explanation of how the model handles multiple variables in a single sentence, including examples and failure cases. This would strengthen the paper's claims about its reasoning capabilities.
2. Address OOV Challenges: Include a discussion on how the model deals with OOV answers, and consider evaluating the architectures on datasets with unseen vocabulary to demonstrate robustness.
3. Expand Curriculum Learning Analysis: Conduct ablation studies to quantify the impact of the curriculum learning component on the model's performance. This would provide valuable insights into the training dynamics.
4. Detail Data Sampling Strategy: Clearly describe the data sample selection process during training iterations, including any biases or heuristics used. This would improve reproducibility and allow for a fair comparison with future work.
5. Broaden Evaluation Metrics: While the paper introduces several metrics (e.g., trajectory-completeness, query accuracy), it would benefit from additional qualitative evaluations, such as analyzing error cases or visualizing query graphs for complex problems.
Questions for the Authors
1. How does the model handle cases where multiple variables appear in a single sentence and are interdependent? Are there specific failure modes observed in such scenarios?
2. What mechanisms, if any, are in place to handle out-of-vocabulary answers during testing? How does the model perform when faced with unseen vocabulary?
3. Can you provide more details on the curriculum learning strategy? Specifically, how does the transition between reward functions affect the agent's learning process?
4. How are data samples selected during training iterations? Are there any biases in the sampling process that could influence the results?
In conclusion, while the paper introduces a promising direction for reasoning and interaction in conversational agents, the identified shortcomings need to be addressed to ensure the robustness, generalizability, and reproducibility of the proposed methods.