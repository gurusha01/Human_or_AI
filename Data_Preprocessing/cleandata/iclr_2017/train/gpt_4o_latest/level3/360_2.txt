Review of the Paper: Semi-Supervised Reinforcement Learning (SSRL)
Summary of Contributions:
This paper introduces and formalizes the novel problem of semi-supervised reinforcement learning (SSRL), inspired by semi-supervised learning, where the reward function is only available in a limited set of labeled Markov Decision Processes (MDPs). The authors propose a solution called Semi-Supervised Skill Generalization (S3G), which leverages maximum entropy control and an Expectation-Maximization (EM) algorithm to infer rewards in unlabeled MDPs. The method effectively combines reinforcement learning (RL) and inverse reinforcement learning (IRL) principles, using the agent's prior experience in labeled MDPs as demonstrations for reward inference. Experimental evaluation on four MuJoCo tasks demonstrates that S3G improves policy generalization compared to standard RL and supervised reward regression baselines. The paper is well-written, and the results are promising, showcasing the potential of SSRL for real-world applications like robotics.
Decision: Accept
Key Reasons:
1. Novelty and Problem Formalization: The paper introduces a new and practically relevant problem setting (SSRL) and provides a clear formalization. This is a significant contribution to the RL community, addressing the challenge of limited reward supervision in real-world scenarios.
2. Strong Empirical Results: The experimental results convincingly demonstrate that S3G outperforms baselines in terms of generalization across diverse tasks, including high-dimensional and vision-based settings.
Supporting Arguments:
1. Motivation and Relevance: The motivation for SSRL is well-grounded in real-world challenges, such as robotics and dialogue systems, where continuous learning with limited supervision is critical. The connection to semi-supervised learning and lifelong learning is compelling.
2. Methodological Soundness: The use of maximum entropy control and iterative reward-policy updates via an EM-like approach is well-justified. The connection to IRL is sound, and the method builds on established techniques like guided cost learning.
3. Experimental Rigor: The evaluation spans four diverse MuJoCo tasks, testing generalization under different conditions. The results are robust, with S3G consistently outperforming baselines. The inclusion of an oracle baseline provides a useful upper bound for comparison.
Suggestions for Improvement:
1. Reproducibility: While the authors promise to release the code, the paper currently lacks sufficient implementation details for full reproducibility. For example, hyperparameters, training schedules, and reward function architectures should be described in more detail.
2. Missing References: The paper could benefit from discussing off-policy policy learning methods and variance reduction techniques like TB(λ) or Retrace(λ), which are relevant to improving sample efficiency and stability in RL.
3. Generalization Testing Methodology: The methodology for evaluating generalization could be clarified further. For instance, how are the unlabeled MDPs sampled, and how do they differ from the labeled ones? Providing more numerical details (e.g., success rates per task) would strengthen the results.
4. Typographical Error: The tuple "M_i = (S, A, T, R)" on page 4 should be corrected to a 4-tuple instead of a 5-tuple.
Questions for the Authors:
1. How sensitive is S3G to the choice of the initial policy trained in the labeled MDPs? Would a poorly trained initial policy significantly degrade performance?
2. Could you elaborate on the computational overhead introduced by the iterative reward-policy updates compared to standard RL methods?
3. Have you considered extending S3G to handle cases where the labeled and unlabeled MDPs come from slightly different distributions (i.e., domain adaptation)?
Conclusion:
This paper makes a strong contribution to reinforcement learning by introducing the SSRL problem and providing a practical solution with S3G. While some areas, such as implementation details and references, could be improved, the novelty, methodological rigor, and empirical results justify acceptance. The work opens up exciting avenues for future research in lifelong learning and real-world RL applications.