Review of the Paper
Summary of Contributions
The paper proposes a global-local context attention framework for sentiment analysis, inspired by human reading comprehension behavior. The authors introduce two models: a Two-Scan Approach with Attention (TS-ATT) and a Single-Scan Approach with Attention (SS-ATT). Both models utilize Bi-LSTM networks to extract global and local context representations, with the global context serving as an attention mechanism to refine local context representations. The authors claim that their models outperform existing baselines on benchmark sentiment classification datasets and provide attention visualizations to support their approach.
Decision: Reject  
Key Reasons:
1. Unaddressed Pre-Review Comments: The authors did not address or incorporate feedback from pre-review, which raises concerns about their responsiveness to constructive criticism and the readiness of the paper for publication.
2. Unscientific Claims: The analogy comparing the model's working to human reading behavior is speculative and lacks empirical evidence. Such claims undermine the scientific rigor of the paper.
Supporting Arguments
1. Basic Experimental Setup: The experiments focus on standard sentiment classification tasks using simple baseline models. While the results are competitive, the lack of novelty in the experimental design does not justify the paper's claims of significant contributions. The title, abstract, and introduction exaggerate the impact of the work, creating a mismatch between the claims and the actual contributions.
2. Lack of Novelty in Approach: The attention mechanism described is highly similar to the dynamic memory networks proposed by Kumar et al. The paper does not adequately explore or differentiate its approach from this prior work, nor does it provide a comparative analysis.
3. Missing Related Work: The related work section omits several recent and relevant studies on attention mechanisms and sentiment analysis. This lack of contextualization weakens the paper's placement within the current literature.
4. Unsupported Claims: The claim that the model achieves "state of the art" performance is not substantiated with rigorous comparisons to recent, more advanced models. Additionally, the attention visualization, while interesting, does not provide sufficient evidence to validate the proposed framework's effectiveness.
Additional Feedback
1. Clarify the Novelty: The authors should explicitly state how their approach differs from existing models, particularly dynamic memory networks and other attention-based frameworks. A comparative analysis with these models is essential.
2. Address Exaggerated Claims: The paper should avoid anthropomorphic analogies (e.g., "human-like reading") unless supported by evidence. The title, abstract, and introduction should be revised to reflect the actual scope and contributions of the work.
3. Expand Related Work: The related work section should include recent advancements in attention mechanisms and sentiment analysis to provide a comprehensive context for the proposed approach.
4. Incorporate Pre-Review Feedback: Addressing the concerns raised in pre-review is critical to demonstrate the authors' commitment to improving the quality of their work.
5. Improve Experimental Rigor: The authors should include comparisons with more sophisticated baselines and state-of-the-art methods, as well as ablation studies to isolate the contributions of the global-local attention mechanism.
Questions for the Authors
1. How does your approach differ fundamentally from dynamic memory networks, and why was this not addressed in the paper?
2. Can you provide empirical evidence to support the claim that your model mimics human reading behavior?
3. Why were recent advancements in attention mechanisms and sentiment analysis omitted from the related work section?
4. What steps will you take to address the pre-review feedback in future revisions?
In summary, while the paper presents an interesting idea, it falls short in terms of novelty, scientific rigor, and responsiveness to feedback. Significant revisions are needed before it can be considered for publication.