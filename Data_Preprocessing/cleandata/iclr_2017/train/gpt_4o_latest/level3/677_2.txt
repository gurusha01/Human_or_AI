Review
Summary of Contributions
The paper presents a novel unsupervised method for graph representation learning, specifically targeting graph-structured data such as molecular graphs. The approach adapts the skip-graph architecture, inspired by the skip-thought model in NLP, to generate embeddings for entire graphs. By training an encoder-decoder model on random walk sequences, the method learns to group structurally and functionally similar subgraphs in feature space. The learned graph embeddings are then used for downstream tasks, such as binary classification of molecular graphs for anti-cancer properties. The paper demonstrates competitive performance, outperforming state-of-the-art methods on three out of four datasets. Additionally, the authors explore various aggregation strategies for graph embeddings and provide insights into parameter tuning and visualization of learned embeddings.
Decision: Reject
While the paper introduces an interesting and potentially impactful method, it falls short in critical areas that are necessary for acceptance. The primary reasons for rejection are:  
1. Insufficient Evaluation Details: The evaluation section lacks clarity about the classifier used, making it difficult to assess the robustness and generalizability of the results.  
2. Dataset Familiarity: The datasets used are not well-described, and their significance is unclear to a broader audience, limiting the ability to contextualize the results.  
Supporting Arguments
1. Novelty and Motivation: The paper is well-motivated, and the adaptation of skip-graph architecture to graph-structured data is novel. The unsupervised nature of the method is a strength, as it provides a general-purpose solution for graph representation learning. The analogy to skip-thoughts is well-explained, and the method is positioned effectively within the literature.  
2. Experimental Results: The results demonstrate strong performance on three datasets, suggesting the method's potential. However, the lack of clarity about the classifier and the absence of a discussion on dataset characteristics (e.g., class imbalance, dataset size) weaken the empirical claims.  
3. Scientific Rigor: While the method is technically sound, the evaluation lacks rigor. For example, the paper does not compare against a sufficiently diverse set of baselines, nor does it explore the method's limitations (e.g., scalability to larger graphs or datasets).  
Suggestions for Improvement
1. Evaluation Details: Provide more information about the classifier used, including its architecture, training procedure, and hyperparameter settings. This is critical for reproducibility and for assessing the validity of the results.  
2. Dataset Context: Include a detailed description of the datasets, their significance, and their relevance to the task. Explain why these datasets were chosen and how they reflect real-world challenges.  
3. Baselines: Expand the set of baseline methods to include more recent graph representation learning techniques, such as Graph Neural Networks (e.g., GCN, GIN). This would provide a more comprehensive comparison.  
4. Ablation Studies: Conduct ablation studies to isolate the contributions of different components of the method, such as the random walk generation process or the aggregation strategies.  
5. Scalability: Discuss the scalability of the method to larger graphs or datasets, as this is a key consideration for practical applications.  
6. Visualization: While the visualization of embeddings is a nice addition, it would be helpful to include quantitative metrics (e.g., clustering quality scores) to support the claims about structural and functional similarity.  
Questions for the Authors
1. Can you provide more details about the classifier used in the evaluation? Was it a neural network, and if so, what were its architecture and training parameters?  
2. How were the datasets preprocessed, and why were they chosen for this study? Are they representative of broader real-world challenges?  
3. Did you compare your method against any graph neural network-based techniques? If not, why were these omitted?  
4. How does the method perform on larger graphs or datasets with more complex structures? Have you tested its scalability?  
5. Could you elaborate on the choice of random walk parameters (e.g., length, number of walks) and their impact on performance?  
In conclusion, while the paper introduces a promising method, it requires significant improvements in evaluation, dataset description, and comparison with baselines to meet the standards of the conference.