The paper introduces "interior gradients" as a novel method for analyzing feature importance in deep neural networks. The authors propose that standard gradients often fail to capture feature importance due to saturation in nonlinear networks, and they address this issue by examining gradients of counterfactual inputs, scaled versions of the original input. Integrated gradients, defined as the integral of interior gradients over all scaling factors, are presented as a way to attribute feature importance while satisfying desirable properties like sensitivity and implementation invariance. The paper demonstrates the method's applicability across various architectures, including GoogleNet, molecular graph convolution networks, and LSTMs, and provides visualizations and empirical evaluations to support its claims.
Decision: Reject
The primary reason for rejection is the lack of rigorous comparisons to prior feature attribution methods, such as DeepLift and guided backpropagation. While the paper claims that interior gradients are simpler to implement and satisfy certain axioms, it does not substantiate these claims with direct qualitative or quantitative comparisons to these established methods. Instead, the authors only compare their approach to standard gradients, which is insufficient given the extensive literature on feature attribution. Furthermore, the authors did not address pre-review concerns about the absence of these comparisons, which undermines the completeness of the evaluation.
Supporting Arguments:
1. Novelty and Contribution: The concept of interior gradients and integrated gradients is a meaningful contribution to feature attribution. The method is theoretically grounded, satisfying key axioms like sensitivity and implementation invariance, and it is computationally efficient.
2. Insufficient Evaluation: The empirical results focus on comparisons with standard gradients, which are known to have limitations. Without comparisons to other state-of-the-art methods, it is unclear whether the proposed approach offers significant advantages.
3. Pre-review Concerns: The authors' failure to address prior feedback regarding missing comparisons indicates a lack of responsiveness, which is critical for ensuring a robust and thorough evaluation.
Additional Feedback:
1. Comparative Analysis: The authors should include both qualitative and quantitative comparisons to methods like DeepLift, guided backpropagation, and Layer-wise Relevance Propagation. This would provide a clearer understanding of the advantages and limitations of interior gradients.
2. Broader Evaluation Metrics: The paper could benefit from additional evaluation metrics beyond visualizations and AOPC, such as fidelity to human intuition or robustness to noise.
3. Clarifications on Limitations: While the paper briefly discusses limitations, such as the inability to capture feature interactions, a more detailed exploration of these issues would strengthen the discussion.
Questions for the Authors:
1. How do interior gradients compare to DeepLift and guided backpropagation in terms of computational efficiency and feature attribution quality?
2. Can the authors provide examples where interior gradients outperform these methods in capturing feature importance?
3. How does the method handle scenarios where feature interactions or correlations are critical to the model's predictions?
In summary, while the paper presents a promising method, its lack of comprehensive comparisons to prior work and unaddressed pre-review concerns make it unsuitable for publication in its current form. Addressing these issues could significantly improve the paper's impact and rigor.