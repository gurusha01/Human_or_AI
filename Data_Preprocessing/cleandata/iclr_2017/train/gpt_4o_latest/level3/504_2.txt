Review of the Paper
Summary of Contributions
This paper introduces a novel method for learning vision-based reward functions for real-world robotic tasks using limited human demonstrations. The approach leverages pre-trained deep models to extract visual features, which are then segmented into task-relevant fragments. These fragments are clustered to identify invariant features, and the most discriminative ones are used to define a reward function. The method is simple yet effective, requiring no explicit sub-goal supervision and enabling robots to learn complex manipulation tasks, such as door opening, from just a few human-provided video demonstrations. The authors provide qualitative and quantitative evaluations, demonstrating the method's ability to generalize across different tasks and environments. Notably, the paper claims to be the first to achieve such results without requiring kinesthetic demonstrations or extensive retraining.
Decision: Reject
While the paper presents an interesting and potentially impactful approach, it falls short in several critical areas, including baseline comparisons and clarity of necessity for the proposed complexity. These shortcomings undermine the scientific rigor and practical significance of the work.
Supporting Arguments for Decision
1. Weak Baselines: The paper's baseline comparisons, particularly for vision-based methods, are insufficient. The random reward baseline in Table 2 performs surprisingly well, raising questions about the robustness of the proposed method. Stronger baselines, such as simpler vision-based approaches (e.g., binning features or using off-the-shelf clustering algorithms), should be included to validate the necessity of the proposed multi-step segmentation and feature selection pipeline.
2. Questionable Necessity of Complexity: The proposed method involves multiple steps, including segmentation, clustering, and feature selection. However, the results do not convincingly demonstrate that this complexity is essential. A simpler approach might achieve comparable results, as hinted in the review guidelines. This undermines the claim of the method being both simple and effective.
3. Lack of Error Bars in Key Figures: While Figure 6 provides a useful comparison, the absence of error bars limits the ability to assess the statistical significance of the results. This omission weakens the empirical support for the paper's claims.
Additional Feedback for Improvement
1. Improved Baselines: Incorporate stronger baselines, such as alternative vision-based reward learning methods or simpler segmentation techniques, to better contextualize the performance of the proposed approach.
2. Error Bars and Statistical Analysis: Include error bars in all quantitative results to provide a clearer picture of variability and statistical significance.
3. Ablation Studies: Conduct ablation studies to isolate the contributions of each component (e.g., segmentation, clustering, feature selection) and justify their inclusion.
4. Clarity on Random Baseline: Provide a more detailed explanation of why the random reward baseline performs well in Table 2. This could help clarify the strengths and weaknesses of the proposed method.
5. Real-World Robustness: While the method is demonstrated on a door-opening task, additional experiments on diverse tasks (e.g., pouring liquids, assembling objects) would strengthen the claim of generalizability.
Questions for the Authors
1. Why does the random reward baseline perform surprisingly well in Table 2? Could this indicate that the task is inherently simple or that the proposed method is over-engineered?
2. Have you considered simpler vision-based approaches, such as directly binning features or using raw activations without clustering? How do these compare in terms of performance and computational efficiency?
3. What is the rationale for not including error bars in Figure 6? Can you provide these in a future revision?
4. How robust is the method to noise in the demonstrations, such as occlusions or inconsistent lighting conditions?
In conclusion, while the paper presents a creative and potentially impactful approach, it requires stronger empirical validation and clearer justification for its complexity. Addressing these issues would significantly enhance the paper's contributions and its potential for acceptance in future iterations.