Review
Summary of Contributions
This paper introduces the UNREAL (UNsupervised REinforcement and Auxiliary Learning) agent, which enhances deep reinforcement learning by incorporating domain-independent auxiliary tasks. These tasks include pixel control (maximizing state space changes), reward prediction, and value function replay, all of which share a feature extraction backbone (CNN+LSTM). The authors argue that these auxiliary tasks improve representation learning, particularly in environments with sparse rewards. The proposed method demonstrates significant improvements in data efficiency and performance over the A3C baseline on both Atari and Labyrinth environments. Notably, the UNREAL agent achieves a 10× speedup in learning on Labyrinth tasks and surpasses state-of-the-art results on Atari, averaging 880% human performance.
Decision: Accept
The paper makes a compelling case for the use of auxiliary tasks to improve reinforcement learning agents, supported by strong empirical results. However, there are areas where the clarity and depth of the analysis could be improved, particularly regarding computational costs and hyperparameter choices.
Supporting Arguments
1. Novelty and Motivation: The paper is well-motivated, addressing the challenge of sparse rewards in reinforcement learning. The auxiliary task framework is novel in its integration of multiple pseudo-reward functions with shared representation learning.
2. Empirical Results: The experiments convincingly demonstrate the benefits of the proposed approach. The UNREAL agent achieves substantial improvements in both performance and data efficiency across diverse tasks, including complex 3D environments.
3. Scientific Rigor: The methodology is sound, with clear descriptions of the auxiliary tasks, loss functions, and experimental setups. The ablation studies provide insights into the contributions of individual components.
Suggestions for Improvement
1. Computational Costs: The paper does not adequately discuss the computational overhead introduced by optimizing auxiliary tasks. The authors should quantify the additional costs and identify the most resource-intensive components.
2. Hyperparameter Choices: The weights for auxiliary tasks (e.g., λPC, λRP) are fixed rather than optimized as hyperparameters. The rationale for this choice should be clarified, and experiments validating the selected values should be included.
3. Feature Control Stability: The "feature control" auxiliary task lacks strong empirical validation. The authors should provide additional experiments to address concerns about its stability and convergence.
4. Extended Training: The reviewer recommends extending the training duration for the best-performing agents to explore their full potential, particularly on challenging tasks.
5. Clarity and Presentation: The abstract and introduction should explicitly clarify that the agent learns separate policies for each auxiliary task. Additionally, figure legends, text references, and supplementary material details require improvement for better readability.
Questions for the Authors
1. What is the computational cost of optimizing auxiliary tasks, and how does it scale with the number of tasks?
2. Why were the auxiliary task weights (λ_*) fixed at specific values rather than treated as hyperparameters? Were alternative values tested?
3. Can you provide more ablation studies on Atari to compare the individual contributions of auxiliary tasks, similar to the Labyrinth results?
4. How does the feature control task impact the stability and convergence of the learning process? Are there specific environments where it performs poorly?
Minor Corrections
- Fix grammatical errors and improve text clarity in several sections.
- Ensure all equations and figure references are accurate and consistent.
- Provide missing parameter values (e.g., λ_PC) in the main text or supplementary material.
In conclusion, this paper makes a significant contribution to reinforcement learning by demonstrating the utility of auxiliary tasks for improving data efficiency and performance. While some aspects require further clarification and analysis, the strengths of the work justify its acceptance.