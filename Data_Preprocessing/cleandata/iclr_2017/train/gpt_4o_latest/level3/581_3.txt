Review
Summary of Contributions
The paper proposes a novel approach to handwriting synthesis by combining a physiologically plausible model of handwriting, the Sigma Lognormal model (inspired by Plamondon's work), with Recurrent Mixture Density Networks (RMDNs). Unlike prior work, such as Graves (2013), which operates on raw sequence data, this paper preprocesses data into a concise motor plan representation, enabling the system to learn from fewer examples and perform tasks like handwriting style transfer and one-shot learning. The authors claim that this preprocessing step introduces artificial variability, improves resolution independence, and facilitates modularity in tasks such as virtual target prediction and dynamic parameter prediction. The paper also explores applications like style transfer and interactive handwriting generation.
Decision: Reject  
Key reasons for rejection:
1. Lack of Strong Evidence of Progress: While the paper introduces an interesting combination of models and preprocessing techniques, it does not convincingly demonstrate significant progress over Graves' original work. The results are largely qualitative, with no rigorous quantitative evaluation to substantiate the claims.
2. Incomplete Scope: The paper does not achieve text-conditional handwriting synthesis, a key benchmark established by Graves (2013), limiting its relevance to the broader field of handwriting synthesis.
Supporting Arguments
1. Motivation and Literature Placement: The paper is well-motivated and grounded in the literature. The use of Plamondon's Sigma Lognormal model is innovative and provides a meaningful abstraction of handwriting dynamics. However, the deviation from the deep learning trend of avoiding hand-crafted features is not sufficiently justified, especially given the lack of quantitative evidence showing the superiority of this approach.
2. Claims vs. Evidence: The paper makes ambitious claims about the benefits of its physiologically inspired representation, such as enabling one-shot learning and style transfer. However, these claims are supported only by qualitative examples, which are insufficient to establish scientific rigor. For instance, the one-shot learning results are visually interesting but lack quantitative metrics or comparisons to alternative methods.
3. Bibliographic Issues: The bibliography is inconsistent, with name variants and improper use of "et al.," which detracts from the paper's professionalism and polish.
Suggestions for Improvement
1. Quantitative Evaluation: Include rigorous quantitative metrics to evaluate the proposed approach against existing methods, particularly Graves (2013). Metrics such as handwriting legibility, diversity, and similarity to human handwriting could strengthen the claims.
2. Text-Conditional Synthesis: Extend the work to achieve text-conditional handwriting synthesis, which would make the paper more competitive and relevant to the field.
3. Bibliographic Consistency: Address the inconsistencies in the bibliography to improve the paper's presentation.
4. Clarity on Data Augmentation: Provide more details on the data augmentation process and its impact on model performance. Quantitative comparisons between augmented and non-augmented datasets would be helpful.
Questions for the Authors
1. How does the proposed method compare quantitatively to Graves (2013) in terms of handwriting quality and diversity?
2. Can the authors provide a more detailed explanation of the benefits of the Sigma Lognormal representation over raw sequence data, supported by empirical evidence?
3. What are the limitations of the current implementation, and how do the authors plan to address them in future work?
While the paper introduces an intriguing approach, its lack of quantitative rigor and incomplete scope prevent it from meeting the standards of acceptance at this time.