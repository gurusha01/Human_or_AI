Review
Summary of Contributions
This paper introduces a new weight initialization method that adjusts for dropout-induced variance and proposes a Batch Normalization (BN) variance re-estimation technique to improve test-time accuracy. The authors claim that their initialization method enables faster convergence and more stable training, while the BN correction improves accuracy by aligning variance estimates between training and testing phases. The proposed methods are evaluated on MNIST, CIFAR-10, CIFAR-100, and SVHN, with claims of achieving state-of-the-art results on CIFAR-10 and CIFAR-100 without data augmentation.
Decision: Reject
The primary reasons for rejection are the lack of convincing empirical evidence to support the claims and insufficient experimental rigor. While the proposed ideas are conceptually interesting, the paper falls short in demonstrating their practical significance and generalizability.
Supporting Arguments
1. Limited Experimental Scope: The experiments in Figure 1 are restricted to MNIST, with unclear details about the dataset preprocessing and empirical setup. This limits the reproducibility and generalizability of the results.
2. Convergence Analysis Issues: Figure 2 uses only three dropout values, which is insufficient to draw robust conclusions about the fairness and generalization of the proposed initialization method.
3. Inadequate Baselines: The absence of baselines with Batch Normalization in Figures 1, 2, and 3 weakens the claims. Comparisons to state-of-the-art methods are also missing, making it difficult to assess the significance of the proposed techniques.
4. Parameter Tuning Limitations: Fixed parameter values are used without employing standard tuning methods like random search or Bayesian optimization. This undermines the validity of the comparisons, particularly in Figure 3, where the Adam optimizer shows closer results but Nesterov momentum performs unreasonably poorly.
5. Lack of Data Augmentation: Experiments are conducted without data augmentation, raising concerns about the generalizability of the methods to real-world scenarios where augmentation is standard practice.
6. Absence of State-of-the-Art Architectures: The paper does not evaluate the proposed methods on modern architectures like ResNet or DenseNet, particularly on large-scale datasets like ImageNet. This omission limits the practical relevance of the contributions.
Suggestions for Improvement
1. Expand Experimental Scope: Include experiments on state-of-the-art architectures (e.g., ResNet, DenseNet) and larger datasets like ImageNet to demonstrate the scalability and generalizability of the proposed methods.
2. Provide Detailed Empirical Setup: Clearly describe the dataset preprocessing, hyperparameter tuning, and experimental configurations to improve reproducibility.
3. Incorporate Baselines: Add comparisons with Batch Normalization baselines and other recent variance stabilization techniques to strengthen the claims.
4. Use Standard Parameter Tuning: Adopt standard hyperparameter tuning methods to ensure fair and meaningful comparisons.
5. Include Data Augmentation: Evaluate the methods with and without data augmentation to assess their robustness in practical scenarios.
Questions for the Authors
1. Can you clarify the dataset preprocessing and experimental setup for the MNIST experiments in Figure 1?
2. Why were only three dropout values used in the convergence analysis (Figure 2), and how do you justify the fairness of this evaluation?
3. What is the rationale for the poor performance of Nesterov momentum in Figure 3? Could this be due to insufficient parameter tuning?
4. How do the proposed methods perform on state-of-the-art architectures like ResNet or DenseNet on larger datasets like ImageNet?
5. Can you provide empirical evidence to demonstrate the benefits of the BN variance re-estimation technique on datasets with data augmentation?
In conclusion, while the paper presents interesting ideas, it requires significant improvements in experimental rigor, baseline comparisons, and broader evaluations to establish its contributions convincingly.