Review of "Neural Data Filter (NDF): A Reinforcement Learning Framework for Data Selection in SGD Training"
Summary of Contributions
This paper introduces the Neural Data Filter (NDF), a novel framework for adaptively filtering training data during Stochastic Gradient Descent (SGD) using reinforcement learning. The NDF acts as a "curriculum teacher," dynamically selecting examples to optimize training efficiency and accuracy. The method is framed as a Markov Decision Process (MDP), with two reinforcement learning approaches—REINFORCE and Actor-Critic—used to train the data selection policy. Experiments on IMDB sentiment classification and a corrupted MNIST dataset demonstrate that NDF accelerates convergence while maintaining competitive accuracy. The idea is innovative and aligns with the broader trend of leveraging reinforcement learning for meta-learning tasks.
Decision: Reject
While the paper presents a promising and novel idea, several methodological and experimental flaws prevent it from meeting the standards for acceptance. The primary concerns include insufficient clarity in the methodology, unaddressed issues in experimental reproducibility, and a lack of exploration of critical baseline comparisons.
Supporting Arguments for Decision
1. Methodological Clarity: The process for filtering training instances is underexplained. For example, it is unclear how the forward-pass, feature computation, and backward-pass decisions are integrated into the NDF framework. This lack of clarity makes it difficult to assess the feasibility and generalizability of the approach.
   
2. Reinforcement Learning Design: The choice of using terminal rewards for REINFORCE and stepwise rewards for Actor-Critic is not well justified. Additionally, the paper does not explain why different reward structures are used for the two methods, nor does it analyze the impact of these choices on performance.
3. Experimental Concerns: The results in Figure 2 lack statistical rigor, as it is unclear whether they are based on single or multiple runs. This raises concerns about reproducibility and the reliability of the reported improvements.
4. Baseline Comparisons: The experiments do not include state-of-the-art optimizers like Adam or RMSProp as baselines, which are widely used in SGD-based training. This omission weakens the claim that NDF improves training efficiency.
5. Non-Stationary Environment: The non-stationary nature of the target network's training environment is not addressed. This raises questions about the robustness of the learned policy and its ability to adapt to changing conditions during training.
Suggestions for Improvement
1. Clarify Methodology: Provide a detailed explanation of the data filtering process, including how decisions are made and integrated with the SGD pipeline. A flow diagram or pseudocode could help.
2. Reinforcement Learning Choices: Justify the design decisions for the reward structures and update frequencies in REINFORCE and Actor-Critic. Analyze their impact on the learning process.
3. Experimental Rigor: Ensure that all results are averaged over multiple runs and report standard deviations to demonstrate statistical significance. 
4. Baseline Comparisons: Include experiments with Adam and RMSProp to strengthen the claim that NDF improves training efficiency.
5. Policy Adaptation Analysis: Analyze whether the observed improvements are due to policy adaptation or static policies with changing features. This could be explored by comparing dynamic and static policies.
6. Pseudo-Validation Data: Clearly explain how pseudo-validation data is selected and ensure there is no overlap with the training data. Consistency between the text and algorithms is essential.
Questions for the Authors
1. How does the NDF framework handle the non-stationary nature of the training environment, where the target network's parameters evolve over time?
2. Why were state-of-the-art optimizers like Adam and RMSProp excluded from the experiments?
3. Can you provide more details on the computational overhead introduced by NDF, especially in large-scale datasets?
4. How does the choice of reward structure (terminal vs. stepwise) affect the convergence of the NDF policy?
5. Could you elaborate on how the features used for state representation (e.g., margin, loss) influence the learned policy?
In conclusion, while the paper introduces a novel and promising idea, it requires significant improvements in methodological clarity, experimental rigor, and baseline comparisons to meet the standards for acceptance.