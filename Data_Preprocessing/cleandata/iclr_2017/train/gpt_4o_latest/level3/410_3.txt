Review of the Paper
Summary of Contributions
This paper addresses the critical problem of detecting misclassified and out-of-distribution (OOD) examples in machine learning models. It proposes a simple baseline that leverages softmax probabilities to distinguish between correctly classified, misclassified, and OOD examples. The authors demonstrate the effectiveness of this baseline across diverse tasks, including computer vision, natural language processing (NLP), and automatic speech recognition (ASR). Additionally, the paper introduces an auxiliary abnormality module, which improves detection performance by exploiting learned internal representations of neural networks. The authors also establish standard tasks and evaluation metrics for error and OOD detection, providing a foundation for future research in this underexplored area.
Decision: Reject
While the paper addresses an important problem and provides a strong baseline, it falls short in several key areas that limit its scientific rigor and clarity. The primary reasons for rejection are the lack of detailed analysis on threshold sensitivity and statistical significance, insufficient clarity in the ASR evaluation, and the absence of a comparison with a generative baseline for speech tasks.
Supporting Arguments
1. Threshold Sensitivity: The paper does not adequately analyze the sensitivity of thresholds across different test sets or splits. This omission raises concerns about the reliability of the proposed methods on unseen data. Threshold sensitivity is particularly critical for real-world deployment, where data distributions can vary significantly.
2. Statistical Significance: While the paper mentions that AUROC and AUPR differences are statistically significant, it does not provide detailed statistical tests or confidence intervals to support these claims. Without this, it is difficult to assess the robustness of the reported performance improvements.
3. Speech Recognition Evaluation: The ASR evaluation lacks clarity. It is unclear whether the examples in Table 9 represent entire utterances or single speech frames. Additionally, the rationale for ignoring the blank symbol's logit in Section 3.3 is not well-justified, leaving a gap in understanding the method's design choices.
4. Generative Baseline Comparison: The paper does not compare its approach to a simple generative baseline, such as a Gaussian Mixture Model-Hidden Markov Model (GMM-HMM), for the ASR task. Such a comparison would provide valuable context for evaluating the proposed model's performance.
Suggestions for Improvement
1. Threshold Sensitivity Analysis: Include a detailed analysis of how detection thresholds vary across datasets and splits. This would strengthen the paper's claims about the generalizability of the proposed methods.
2. Statistical Testing: Provide detailed statistical tests, such as p-values or confidence intervals, for the AUROC and AUPR results. This would enhance the scientific rigor of the performance claims.
3. Clarify ASR Evaluation: Clearly specify whether the examples in Table 9 are entire utterances or single frames. Additionally, provide a more thorough explanation for ignoring the blank symbol's logit in Section 3.3.
4. Generative Baseline Comparison: Include a comparison with a GMM-HMM or other simple generative models for the ASR task. This would contextualize the performance of the proposed approach and highlight its advantages or limitations.
5. Broader Discussion: Expand the discussion on the limitations of the proposed methods and potential avenues for future research, particularly in addressing the shortcomings of softmax-based confidence measures.
Questions for the Authors
1. How does the proposed method handle threshold sensitivity across different datasets and splits? Can you provide quantitative results to demonstrate robustness?
2. What statistical tests were used to validate the significance of AUROC and AUPR improvements? Can you include confidence intervals for these metrics?
3. Why was the blank symbol's logit ignored in the ASR task? Would including it affect the detection performance?
4. How does the proposed method compare to a GMM-HMM baseline for the ASR task? Could such a baseline outperform the softmax-based approach in certain scenarios?
By addressing these points, the paper could significantly improve its clarity, rigor, and impact on the field.