The paper introduces MS MARCO, a large-scale dataset for machine reading comprehension (RC) and question answering (QA), aimed at addressing limitations in existing datasets. It features real user queries sourced from Bing, real web document contexts, and human-generated answers, distinguishing it from synthetic or crowdsourced datasets. The dataset currently includes 100,000 queries with plans to scale to 1 million, providing a valuable resource for developing and benchmarking QA models. The authors conduct dataset analysis and initial experiments to evaluate QA model performance, showcasing MS MARCO's potential as a benchmark for real-world RC tasks.
Decision: Reject
While MS MARCO is a promising dataset with unique characteristics, the paper does not sufficiently demonstrate its value or provide rigorous analysis to justify its claims. Key weaknesses in the experimental evaluation and lack of clarity in certain sections prevent acceptance in its current form.
Supporting Arguments:
1. Strengths:
   - The dataset is built on real user queries and web contexts, making it more representative of real-world QA tasks compared to synthetic datasets.
   - Human-generated answers allow for more natural and diverse responses, addressing limitations of span-based answers in other datasets.
   - The scale of the dataset (targeting 1 million queries) has the potential to drive significant advancements in QA research.
2. Weaknesses:
   - The claim that real user queries differ significantly from crowdsourced questions is not substantiated with evidence or analysis.
   - The paper does not clearly articulate what unique insights MS MARCO offers over existing datasets like SQuAD or WikiQA.
   - The experimental results are underwhelming and do not convincingly demonstrate the dataset's superiority. For instance, the performance of Memory Networks suggests limited room for improvement, raising questions about the dataset's challenge level.
   - Important details, such as train/test splits and subset sizes, are missing, making it difficult to assess the rigor of the experiments.
   - The paper lacks a comparison of human performance versus model performance, a critical benchmark for evaluating dataset difficulty.
   - Several textual errors and incomplete analyses suggest the paper was rushed, undermining its credibility.
Suggestions for Improvement:
1. Provide evidence to support the claim that real user queries differ significantly from crowdsourced questions. A comparative analysis would strengthen this argument.
2. Clearly articulate the unique contributions of MS MARCO over existing datasets, both in terms of dataset characteristics and the insights it enables.
3. Include a detailed discussion of the challenges posed by the dataset and how it can drive advancements in QA models.
4. Add a comparison of human versus model performance to establish a baseline and highlight the dataset's difficulty.
5. Clarify experimental details, such as train/test splits, subset sizes, and the role of DSSM in Table 5.
6. Address the limited performance improvements observed in Table 6 and discuss how future models might overcome these limitations.
7. Proofread the paper thoroughly to eliminate textual errors and improve clarity.
Questions for the Authors:
1. What specific insights or advancements do you expect MS MARCO to enable that existing datasets cannot?
2. Can you provide evidence or analysis to support the claim that real user queries differ significantly from crowdsourced questions?
3. How do you plan to address the limited performance improvements observed in the experiments? Does this suggest the dataset is too easy or too hard?
4. Why was DSSM included in Table 5, and what role does it play in the experiments?
5. How do you plan to scale the dataset to 1 million queries, and how will this impact its quality and diversity?
In conclusion, while MS MARCO has the potential to be a valuable resource for QA research, the paper requires significant improvements in analysis, evaluation, and presentation to fully demonstrate its value.