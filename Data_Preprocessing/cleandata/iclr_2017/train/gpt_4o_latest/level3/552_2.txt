Review of "Rotation Plane Doubly Orthogonal RNN"
Summary of Contributions
This paper introduces the Rotation Plane Doubly Orthogonal Recurrent Neural Network (RP-DORNN), a novel recurrent architecture designed to address the vanishing and exploding gradient problems in training RNNs on long sequences. The authors propose a fully orthogonal transition mechanism, parameterized by plane rotations, that preserves both forward activation norms and backpropagated gradient norms. The paper demonstrates the theoretical soundness of the approach and validates it on a simplified 1-bit memory copy task, achieving successful learning for sequences up to 5,000 timesteps. The authors argue that their architecture offers a promising direction for modeling extremely long-term dependencies.
Decision: Reject  
The key reasons for this decision are: (1) the lack of novelty in the core concept, as orthogonal/unitary recurrent architectures have been extensively studied in prior work, and (2) the weak experimental validation, which is limited to a toy task and provides little insight into the model's utility for real-world applications.
Supporting Arguments
1. Novelty and Motivation: While the paper builds on the idea of orthogonal/unitary matrices for recurrent updates, this concept is not new. Prior work, such as uRNN and its extensions, has already explored similar approaches. The proposed rotation plane parameterization is an interesting variation, but its practical advantages over existing methods are not clearly demonstrated. The paper does not sufficiently justify why this specific parameterization is necessary or superior.
2. Experimental Validation: The experiments focus solely on a simplified version of the memory copy task, which is a well-known benchmark for testing long-term dependencies. While the results are promising for this task, they do not provide evidence of the model's effectiveness on more complex or real-world problems, such as language modeling or time-series prediction. Additionally, the task simplifications (e.g., reducing categories and sequence length) limit the generalizability of the findings.
3. Model Limitations: The inability of the model to forget is a significant drawback, particularly for tasks like language modeling, where selective forgetting is crucial. The paper does not address how this limitation might be mitigated or whether it impacts the model's expressiveness.
Suggestions for Improvement
1. Broader Experiments: To strengthen the paper, the authors should evaluate the model on more challenging and diverse tasks, such as language modeling, sequence classification, or real-world time-series prediction. Comparisons with state-of-the-art methods, including LSTMs, GRUs, and uRNNs, are essential to establish the practical utility of the proposed architecture.
2. Addressing Forgetting: The authors should explore mechanisms to introduce selective forgetting into the architecture, potentially by combining orthogonal transitions with gating mechanisms like those in LSTMs or GRUs.
3. Theoretical Insights: The paper raises interesting questions about the necessity of the shared matrix \( R \) and the potential to exploit the model's linearity in \( h \) or \( R_x \). Addressing these questions with theoretical or empirical analysis would add depth to the work.
4. Clarity and Accessibility: While the theoretical proofs are rigorous, the paper could benefit from clearer explanations of the rotation plane parameterization and its advantages. Simplifying the mathematical exposition would make the work more accessible to a broader audience.
Questions for the Authors
1. Why is the rotation plane parameterization particularly advantageous compared to other orthogonal/unitary parameterizations, such as those used in uRNN or Cayley-transform-based methods?
2. How does the model perform on tasks that require selective forgetting or hierarchical dependencies, such as language modeling or sentiment analysis?
3. Could the random initialization of rotation planes introduce instability or limit the model's capacity? Have you considered optimizing the planes jointly during training?
In conclusion, while the paper presents an interesting variation of orthogonal RNNs, it falls short in terms of novelty, experimental rigor, and addressing key limitations. Expanding the scope of experiments and addressing the model's practical limitations would significantly improve the work.