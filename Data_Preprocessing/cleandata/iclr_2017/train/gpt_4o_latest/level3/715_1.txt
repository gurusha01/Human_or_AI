Review of the Paper
Summary of Contributions
This paper proposes a randomized algorithm for pruning convolutional neural network (ConvNet) weights to reduce computational complexity, specifically targeting theoretical FLOPs. It introduces a taxonomy of pruning granularities, ranging from coarse (layer-wise) to fine (intra-kernel), and employs a novel strategy of generating random pruning masks and selecting the least adversarial one using a validation set. The method is claimed to be "one-shot," avoiding iterative retraining, and is evaluated on datasets such as CIFAR-10, CIFAR-100, SVHN, and MNIST. The authors argue that their approach is generic, scalable, and capable of achieving significant sparsity with minimal performance degradation.
Decision: Reject  
The paper is not ready for acceptance due to significant shortcomings in its claims, experimental evaluation, and clarity. While it introduces a useful taxonomy and provides comparisons to recent pruning methods, the lack of rigorous evidence for its claims and incomplete experimental evaluation limit its impact and credibility.
Supporting Arguments for Decision
1. Unsubstantiated Claims: The paper's claims of being "one-shot" and "near optimal" are not supported by theoretical or empirical evidence. The comparison to iterative pruning methods is superficial, and the "near optimal" assertion is misleading without rigorous benchmarks or ablation studies.
   
2. Incomplete Experimental Evaluation: The paper evaluates its approach on relatively small datasets (e.g., CIFAR-10, CIFAR-100, SVHN, MNIST) and older architectures, but does not test on recent models like ResNets or large-scale datasets like ImageNet. This limits the practical relevance and generalizability of the proposed method.
3. Lack of Transfer Learning Analysis: The impact of pruning on transfer learning is not explored, which is a missed opportunity given the increasing importance of transfer learning in real-world applications.
4. Clarity and Rigor: The paper is difficult to follow due to dense writing and a lack of clear organization. The experimental results are noisy and not convincingly presented, particularly regarding the benefits of fine-grained sparsity.
5. Broader Practical Relevance: The paper does not address how the proposed pruning strategy translates into real-world computational speedups on modern hardware (e.g., GPUs, TPUs). The focus on theoretical FLOPs alone is insufficient.
Suggestions for Improvement
1. Strengthen Claims: Provide theoretical analysis or empirical evidence to substantiate the "one-shot" and "near optimal" claims. Include ablation studies to isolate the contributions of the random mask selection strategy.
2. Expand Experimental Scope: Evaluate the proposed method on modern architectures (e.g., ResNets, Transformers) and large-scale datasets like ImageNet. This would demonstrate the scalability and practical relevance of the approach.
3. Transfer Learning Analysis: Investigate how pruning affects transfer learning performance, as this would significantly enhance the paper's impact.
4. Hardware Benchmarks: Include experiments that measure real-world speedups on modern hardware to validate the practical benefits of the proposed pruning granularities.
5. Improve Clarity: Reorganize the paper for better readability and ensure that key results are clearly highlighted. Use visual aids like tables and graphs to summarize findings effectively.
Questions for the Authors
1. How do you justify the claim of "near optimal" pruning without a theoretical framework or rigorous empirical comparison to iterative methods?
2. Why were recent architectures like ResNets and datasets like ImageNet not included in the evaluation? Are there any technical limitations preventing their inclusion?
3. How does the proposed method perform in transfer learning scenarios? Does pruning degrade the ability of the network to generalize to new tasks?
4. Can you provide real-world benchmarks (e.g., inference latency, energy efficiency) to demonstrate the practical benefits of your approach on modern hardware?
In conclusion, while the paper introduces an interesting taxonomy and a novel pruning strategy, its current form lacks the rigor and breadth required for acceptance. Addressing the above concerns would significantly strengthen the paper.