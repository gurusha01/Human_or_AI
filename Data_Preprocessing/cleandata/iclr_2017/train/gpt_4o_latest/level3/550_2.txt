Review
Summary of Contributions
This paper introduces a novel regularization method for denoising autoencoders (DAEs) aimed at enforcing robustness in the encoding phase against input perturbations. The authors propose a modification to the DAE objective function by adding a regularization term that minimizes the conditional entropy of the encoding given the input. This modification ensures that the encoder, rather than the decoder, performs the majority of the denoising. The paper provides theoretical motivation for the proposed objective and evaluates its performance on synthetic datasets and MNIST, demonstrating its potential to learn robust representations, particularly in over-complete scenarios. The authors also draw connections between their method and contractive autoencoders, emphasizing the generality of their approach in handling different forms of input corruption and distance metrics.
Decision: Reject  
While the paper presents an interesting idea with theoretical grounding, it falls short in several critical areas, particularly in experimental rigor and positioning within the existing literature. These shortcomings make it difficult to confidently assess the significance and generalizability of the proposed method.
Supporting Arguments for Decision
1. Experimental Evaluation: 
   - The experiments are insufficiently detailed and lack clarity. For example, figures illustrating results are not well-explained, making it challenging to interpret the findings. 
   - The evaluation on MNIST is limited to a single-layer DAE, which does not fully explore the potential of the proposed method in deeper architectures or more complex datasets.
   - There is no quantitative comparison with closely related methods, such as contractive autoencoders (CAEs). This omission weakens the claims of superiority or distinctiveness of the proposed approach.
2. Positioning in the Literature: 
   - While the paper acknowledges similarities to CAEs, it does not provide a direct empirical comparison, leaving the reader uncertain about the relative advantages of the proposed method.
   - The theoretical contributions, while well-motivated, do not sufficiently differentiate the method from existing approaches in terms of novelty or practical impact.
3. Clarity and Presentation: 
   - The explanation of the regularization term and its connection to conditional entropy is mathematically dense and could benefit from more intuitive descriptions or examples.
   - The experimental section lacks sufficient detail about hyperparameter settings, training protocols, and noise levels, which are critical for reproducibility.
Suggestions for Improvement
1. Experimental Comparisons: Include direct comparisons with contractive autoencoders and other regularized autoencoder variants to better contextualize the contributions. Additionally, evaluate the method on more complex datasets and deeper architectures to demonstrate scalability and robustness.
2. Clarity in Figures and Results: Provide clearer explanations of figures and quantitative metrics to support the claims. For example, explicitly show how the proposed method improves robustness compared to baseline DAEs.
3. Theoretical Insights: While the theoretical grounding is solid, consider adding more intuitive explanations or visualizations to make the method accessible to a broader audience.
4. Reproducibility: Include detailed descriptions of experimental setups, such as hyperparameters, corruption processes, and optimization strategies, to ensure that the results can be replicated.
Questions for the Authors
1. How does the proposed method compare quantitatively with contractive autoencoders in terms of robustness and representation quality?
2. Can the method scale effectively to deeper architectures or more complex datasets like CIFAR-10 or ImageNet?
3. How sensitive is the performance to the choice of the tradeoff parameter Î» and the corruption process?
In conclusion, while the paper presents a promising idea, it requires significant improvements in experimental rigor, clarity, and positioning within the literature to be considered for acceptance.