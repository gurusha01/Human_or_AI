The paper proposes a novel multimodal learning model, the Joint Multimodal Variational Autoencoder (JMVAE), which models the joint distribution of multiple modalities using VAEs. The authors also introduce an extension, JMVAE-kl, designed to handle missing modalities by reducing the divergence between the multimodal encoder and modality-specific encoders. The method is evaluated on MNIST and CelebA datasets, demonstrating its ability to generate and reconstruct modalities bi-directionally and to extract joint representations effectively. The paper claims that JMVAE outperforms conventional VAEs and CVAEs in terms of log-likelihood and qualitative performance, especially on the more complex CelebA dataset.
Decision: Reject
Key reasons for rejection:
1. Dataset Choice and Multimodal Applicability: MNIST is not a truly multimodal dataset, and treating labels as a separate modality is questionable. This undermines the validity of the experiments and the generalizability of the proposed method.
2. Experimental Limitations: The experiments do not simulate scenarios where modalities are genuinely missing, which is a key claim of the paper. This raises concerns about the practical applicability of the method.
Supporting Arguments:
1. While the proposed JMVAE and JMVAE-kl methods are conceptually interesting, the choice of MNIST as a dataset is problematic. Labels are not a distinct modality but rather metadata, making the multimodal claims less compelling. CelebA is a better choice, but the paper does not explore other datasets with more diverse modalities, limiting the scope of the conclusions.
2. The experiments fail to convincingly demonstrate the handling of missing modalities. The paper assumes modalities are missing but does not explicitly simulate or evaluate this scenario. This weakens the claim that JMVAE-kl addresses missing modality issues effectively.
3. The differences in log-likelihoods between models are minimal and could be attributed to noise. Furthermore, comparing log-likelihoods for models not trained to maximize log-likelihood (e.g., GAN-based models) makes the conclusions unclear and less rigorous.
Additional Feedback:
1. Clarify the Definition of Modalities: The paper should provide a clearer justification for treating labels as a modality in MNIST and discuss the implications of this choice.
2. Expand Dataset Diversity: To strengthen the claims, experiments should include datasets with more distinct and complex modalities, such as text, audio, and video.
3. Simulate Missing Modalities: Explicitly simulate missing modalities during testing to validate the effectiveness of JMVAE-kl in such scenarios.
4. Improve Log-Likelihood Comparisons: Since log-likelihood is a key metric, the paper should focus on models trained to optimize this metric. Alternatively, provide a rationale for using log-likelihood as a comparison for GAN-based models.
Questions for the Authors:
1. How do you justify treating labels as a separate modality in MNIST? Wouldn't this approach limit the generalizability of your method to truly multimodal datasets?
2. Can you provide experiments where modalities are explicitly missing during testing to validate the claims about JMVAE-kl's robustness?
3. Why were log-likelihood comparisons made for models not trained to optimize log-likelihood? How do you account for the noise in these comparisons?
While the paper introduces an interesting approach to multimodal learning, the experimental design and dataset choices limit its impact and applicability. Addressing these issues could significantly strengthen the work.