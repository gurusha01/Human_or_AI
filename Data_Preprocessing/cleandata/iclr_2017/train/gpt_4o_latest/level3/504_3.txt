Review of the Paper
Summary of Contributions
This paper introduces a novel and simple method for learning reward functions in reinforcement learning (RL) using visual observations of expert demonstrations. The approach leverages pre-trained deep neural networks as feature extractors and represents the reward function as a weighted distance to features from automatically identified "key-frames" in expert trajectories. The method is particularly notable for its ability to learn dense and smooth reward functions from a small number of demonstrations without requiring explicit sub-goal supervision. The authors demonstrate the approach on real-world robotic tasks, including door opening and liquid pouring, providing both qualitative and quantitative evaluations. The paper is well-written, situates the work effectively within the inverse reinforcement learning (IRL) literature, and highlights the practical potential of the method for real-world robotics.
Decision: Reject
While the paper presents an interesting and practical approach, it falls short in several critical areas that undermine its contributions. The key reasons for rejection are: (1) insufficient baseline comparisons to validate the novelty and effectiveness of the proposed method, and (2) limited exploration of alternative feature representations, which weakens the empirical rigor of the results.
Supporting Arguments for the Decision
1. Baseline Comparisons: The paper does not include comparisons against several important baselines, such as using all features without key-frame selection or a simpler approach that uses all trajectory frames with time-weighted distances. These baselines are essential to establish the added value of the key-frame-based reward representation.
2. Controller Bias: The evaluation relies on a simple RL controller initialized from expert trajectories, which introduces bias and limits the assessment of the reward function's generality. A more robust evaluation with diverse controllers would strengthen the claims.
3. Alternative Feature Representations: The paper does not explore alternative feature representations, such as random projections, raw image distances, or principal components. This omission leaves open the question of whether the proposed method's reliance on pre-trained deep features is truly necessary or optimal.
4. Empirical Focus: The paper is primarily empirical and lacks significant theoretical contributions. Given this focus, the experimental evaluation needs to be more comprehensive and robust to justify the claims.
Additional Feedback for Improvement
1. Clarity and Writing: While the paper is generally well-written, there are minor issues with awkward phrasing, unclear definitions, and typos. For example, the term "key-frames" could be better defined, and the segmentation algorithm descriptions could be streamlined for clarity.
2. Reward Function Generality: The authors should evaluate the learned reward functions on tasks beyond those demonstrated, to test their generalization capabilities.
3. Theoretical Justification: While the method is described as a simple approximation to IRL, a more detailed theoretical analysis of its limitations and assumptions would strengthen the paper.
4. Failure Cases: The paper briefly mentions failure cases but does not analyze them in depth. Understanding why the reward function fails in certain scenarios (e.g., transparent liquid in the pouring task) could provide insights for improvement.
5. Unsupervised Segmentation: The reliance on manually specified segment counts for the recursive segmentation algorithm is a limitation. Exploring automatic methods for determining the number of segments would make the approach more robust.
Questions for the Authors
1. How does the proposed method compare to using all trajectory frames with time-weighted distances as a baseline? Would the key-frame selection still provide significant advantages?
2. Have you considered evaluating the learned reward functions with a more complex or diverse set of RL controllers to test their robustness?
3. Why were alternative feature representations, such as random projections or raw image distances, not included in the experiments? Could these provide comparable performance?
4. Can the segmentation algorithm be adapted to automatically determine the number of segments, rather than relying on manual specification?
In conclusion, while the paper presents a promising approach with potential applications in real-world robotics, it requires more rigorous evaluation and broader comparisons to establish its contributions convincingly. Addressing the identified weaknesses could make this work a strong candidate for future acceptance.