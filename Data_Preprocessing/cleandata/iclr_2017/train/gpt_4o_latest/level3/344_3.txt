The paper introduces challenging micromanagement tasks in StarCraft as benchmarks for reinforcement learning (RL) algorithms, emphasizing the difficulties posed by high-dimensional, variable action spaces. It proposes a novel Zero-Order (ZO) algorithm that combines policy gradient methods, deep neural networks for state embeddings, and gradient-free optimization to address these challenges. The authors claim that ZO enables structured exploration, outperforming traditional methods like Q-learning and REINFORCE in terms of both performance and robustness across several StarCraft scenarios. The paper also highlights the importance of normalized cumulative rewards and a joint state-action embedding for handling complex multi-agent environments.
Decision: Accept
Key Reasons:
1. Relevance and Contribution: The paper tackles a significant and underexplored problem in reinforcement learningâ€”effective exploration and policy learning in large, structured action spaces. By introducing StarCraft micromanagement tasks, it provides a challenging and realistic benchmark for RL research. The proposed ZO algorithm demonstrates clear improvements over existing methods, making a meaningful contribution to the field.
2. Empirical Validation: The experimental results are comprehensive and demonstrate that ZO consistently outperforms baselines like DQN and REINFORCE across multiple scenarios. The robustness of ZO, as evidenced by its ability to generalize to unseen tasks, further supports the claims.
Supporting Arguments:
- The paper is well-motivated and places its contributions within the broader RL literature, particularly in the context of multi-agent systems and structured action spaces. The discussion of prior work is thorough and highlights the novelty of ZO.
- The experimental setup is rigorous, with comparisons against multiple baselines and heuristic strategies. The inclusion of generalization tests and head-to-head comparisons between models strengthens the empirical evidence.
- The use of normalized cumulative rewards and a joint state-action embedding is a thoughtful design choice that addresses key challenges in StarCraft micromanagement.
Suggestions for Improvement:
1. Reproducibility: Sharing the source code and task specifications would significantly enhance the paper's impact by enabling other researchers to replicate and build upon the work.
2. Clarity in Section 5: The explanation of raw inputs and feature encodings is dense and could benefit from additional clarification or illustrative examples. Providing code snippets or pseudocode for the embedding network would help readers understand the implementation details.
3. Baseline Comparisons: The paper lacks comparisons with value-based approaches like Bootstrapped DQN, which could provide insights into alternative exploration strategies. Including these baselines would make the evaluation more comprehensive.
4. Action Embedding Models: A discussion of alternative action embedding techniques, such as energy-based models, could add depth to the analysis and highlight potential avenues for future work.
Questions for the Authors:
1. How does the ZO algorithm scale with larger and more diverse StarCraft scenarios, such as those involving hierarchical planning or more complex unit types?
2. Can the authors provide more details on the choice of hyperparameters for ZO and how sensitive the algorithm is to these settings?
3. Did the authors explore any methods to optimize the number of units assigned to a single target (e.g., in the Wraiths scenario) to address overkill more effectively?
In conclusion, this paper makes a strong contribution to reinforcement learning research by addressing a challenging problem and proposing an innovative solution. While there are areas for improvement, particularly in reproducibility and baseline comparisons, the paper's strengths outweigh its weaknesses, warranting acceptance.