Review
Summary of Contributions
This paper addresses the challenge of applying deep latent Gaussian models (DLGMs) to sparse, high-dimensional, non-negative datasets, such as word counts and product ratings. The authors propose two key contributions: (1) a novel training methodology that blends inference network predictions with iterative optimization of local variational parameters, improving model performance on sparse data, and (2) a method for interpreting the learned representations of DLGMs using Jacobian-based embeddings. The paper demonstrates the effectiveness of these techniques through experiments on text, medical, and movie datasets, showcasing improvements in held-out likelihood and interpretability. The introspection approach, particularly the use of Jacobian vectors for embedding generation, is original and provides valuable insights into the latent structure of the data.
Decision: Accept
The paper makes significant contributions to the field of deep generative modeling, particularly in improving the applicability of DLGMs to sparse datasets and introducing a novel introspection method. The proposed techniques are well-motivated, scientifically rigorous, and supported by empirical results. The originality of the Jacobian-based embeddings and their demonstrated utility across diverse domains make this work a valuable addition to the literature.
Supporting Arguments
1. Problem and Motivation: The paper addresses a well-defined and important problemâ€”extending the applicability of DLGMs to sparse data, which has been underexplored in the literature. The motivation for blending inference network predictions with iterative optimization is clearly articulated and grounded in prior work.
   
2. Methodology and Correctness: The proposed training procedure and introspection method are described in detail and appear technically sound. The use of tf-idf features and iterative optimization of local variational parameters is a thoughtful adaptation to the challenges of sparse data. The derivation of Jacobian-based embeddings is novel and well-explained.
3. Empirical Validation: The experiments demonstrate the effectiveness of the proposed methods. The improvements in held-out likelihood and the qualitative evaluation of embeddings (e.g., polysemic word embeddings and medical code clusters) provide strong evidence for the utility of the contributions. However, the lack of comparison with common embeddings in supervised tasks is a notable omission.
Suggestions for Improvement
1. Comparison with Standard Embeddings: While the polysemic word embedding experiment is interesting, it would benefit from a comparison with widely-used embeddings (e.g., Word2Vec, GloVe) on supervised tasks such as classification or clustering. This would provide a clearer benchmark for the proposed method's effectiveness.
   
2. Clarity in Presentation: Equation 2 has excessive closing parentheses, which should be corrected for readability. Additionally, the paper could benefit from a more concise presentation of the experimental results, as some sections feel overly detailed.
3. Broader Context: The discussion could be expanded to include potential limitations of the proposed methods, such as scalability to extremely large datasets or sensitivity to hyperparameters.
Questions for the Authors
1. How does the performance of the Jacobian-based embeddings compare to standard embeddings (e.g., Word2Vec) on supervised tasks such as text classification or clustering?
2. Can the proposed method for optimizing local variational parameters be extended to other types of sparse data, such as graphs or time-series data?
3. How sensitive are the results to the choice of the prior distribution or the architecture of the inference network?
Overall, this paper makes a strong contribution to the field of deep generative modeling and should be accepted with minor revisions to address the above points.