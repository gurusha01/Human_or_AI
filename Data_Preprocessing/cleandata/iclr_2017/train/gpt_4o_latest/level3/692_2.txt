Review
Summary of Contributions
The paper proposes a novel framework for sentiment classification that combines global and local context representations using a two-scan attention mechanism. The global context is computed using a Bi-LSTM, which is then used as attention to refine local context representations during a second scan. The authors also propose a simplified single-scan version of the model. Empirical results demonstrate that the proposed methods achieve competitive or state-of-the-art performance on three benchmark datasets (Amazon, IMDB, and Yelp 2013). The authors provide attention visualizations and case studies to illustrate the interpretability of their model, highlighting its ability to focus on important local contexts.
Decision: Reject
While the proposed method shows promising results, the lack of novelty in the core idea and insufficient experimental rigor make it unsuitable for acceptance in its current form. The use of Bi-LSTM for global context computation in attention mechanisms has been extensively explored in prior works, such as Luong et al. (2015) and Shen & Lee (2016). The paper does not sufficiently differentiate itself from these existing approaches. Additionally, the experimental setup lacks critical baselines and ablation studies, which weakens the scientific rigor of the claims.
Supporting Arguments
1. Lack of Novelty: The key idea of combining global and local context for attention is not new. Luong et al. (2015) and Shen & Lee (2016) have already explored similar approaches. While the paper introduces a two-scan mechanism, this does not constitute a significant conceptual advancement over prior work.
   
2. Experimental Weaknesses: The experiments lack comparisons with models that use pre-trained word embeddings or regularization techniques like dropout, which are standard in sentiment classification tasks. These omissions make it difficult to assess the true efficacy of the proposed method. Furthermore, the paper does not include ablation studies to isolate the contributions of the two-scan mechanism or the attention component.
3. Insufficient Placement in Literature: The related work section acknowledges prior studies but does not adequately position the proposed method in the context of these works. The authors fail to clearly articulate how their approach addresses gaps or limitations in existing methods.
Suggestions for Improvement
1. Clarify Novelty: The authors should explicitly highlight the unique contributions of their method compared to prior works. If the two-scan mechanism is the key innovation, its advantages over single-scan approaches should be rigorously demonstrated through ablation studies.
   
2. Enhance Experimental Rigor: Evaluate the model with pre-trained word embeddings (e.g., GloVe or Word2Vec) and regularization techniques like dropout. Compare the results with state-of-the-art models that use these enhancements. Include statistical significance tests to validate performance improvements.
3. Ablation Studies: Conduct experiments to isolate the contributions of the two-scan mechanism, global context attention, and local context incorporation. This would provide a clearer understanding of the importance of each component.
4. Broader Baseline Comparisons: Include comparisons with more recent and competitive baselines, such as transformer-based models (e.g., BERT), which are widely used in sentiment classification tasks.
5. Expand Interpretability Analysis: While attention visualizations are helpful, the authors could provide quantitative metrics (e.g., attention alignment scores) to assess the interpretability of their model.
Questions for the Authors
1. How does the two-scan approach compare to single-scan approaches in terms of computational efficiency and performance trade-offs?
2. Why were pre-trained word embeddings and dropout not included in the experiments? Would these techniques improve performance?
3. How does the proposed method perform on longer texts or datasets with more complex sentiment structures?
4. Can the authors provide more detailed comparisons with Luong et al. (2015) and Shen & Lee (2016) to clarify the novelty of their approach?
In summary, while the paper presents an interesting application of attention mechanisms for sentiment classification, the lack of novelty and insufficient experimental rigor limit its contributions. Addressing the suggested improvements could significantly strengthen the paper.