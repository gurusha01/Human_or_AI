Review of the Paper
Summary of Contributions
This paper introduces a novel LSTM reparametrization aimed at preserving the means and variances of hidden states and memory cells across time. The proposed method, termed "Normalized LSTM," is inspired by Batch Normalized LSTM (BN-LSTM) and Normalization Propagation (Norm Prop). The authors claim that their method achieves faster training convergence, similar to Layer Normalization (LN) and Recurrent Batch Normalization (BN), while requiring fewer computations. They also provide an in-depth analysis of gradient behavior in their architecture, highlighting the role of initialization and scaling parameters. Empirical evaluations on character-level language modeling and image generative modeling tasks demonstrate competitive performance, with the proposed method being computationally faster than BN-LSTM and LN-LSTM. The analysis in Section 4 is particularly commendable for its clarity and depth.
Decision: Reject  
The primary reasons for this decision are the marginal experimental improvements and the lack of clarity regarding statistical significance. Additionally, weight normalization, a simpler alternative, achieves similar performance, reducing the novelty and practical impact of the proposed method.
Supporting Arguments
1. Marginal Improvements: While the proposed method shows slight improvements in computational efficiency and generalization, the performance gains over existing methods (e.g., BN-LSTM, LN-LSTM) are not substantial. The reported results do not convincingly demonstrate that the method is a significant step forward in recurrent normalization techniques.
2. Statistical Significance: The paper does not provide a rigorous statistical analysis of the experimental results. Without confidence intervals or significance testing, it is difficult to assess whether the observed improvements are meaningful or due to random variation.
3. Simplicity of Alternatives: Weight normalization, which the authors acknowledge as a related method, achieves comparable performance with less complexity. This diminishes the practical appeal of the proposed approach.
Suggestions for Improvement
1. Writing Quality: The paper contains numerous typos and grammatical errors, such as "maintain -> maintain," "requisits -> requisites," "a LSTM -> an LSTM," and "beacause -> because." Additionally, there is an incorrect gradient description and a mistake in γx > γh on page 5. These errors detract from the paper's readability and professionalism.
2. Statistical Rigor: Include statistical significance testing (e.g., p-values, confidence intervals) for experimental results to strengthen the claims of improvement.
3. Comparison with Simpler Methods: Provide a more detailed comparison with weight normalization, both in terms of computational efficiency and performance. This would help clarify the unique advantages of the proposed method.
4. Broader Evaluation: The experiments are limited to two tasks (language modeling and image generation). Testing the method on more diverse and challenging tasks could better demonstrate its generality and robustness.
5. Clarity in Derivations: While the gradient analysis in Section 4 is thorough, some derivations are dense and difficult to follow. Adding more intuitive explanations or diagrams could improve accessibility for a broader audience.
Questions for the Authors
1. How does the proposed method perform on tasks with longer sequences or more complex dependencies? Does it scale well in such scenarios?
2. Could you clarify the computational trade-offs compared to weight normalization? How significant is the speedup in practical applications?
3. Why were statistical significance tests omitted from the experimental results? Would including them alter the conclusions?
In summary, while the paper presents an interesting approach to LSTM normalization and provides a commendable gradient analysis, the marginal improvements, lack of statistical rigor, and simpler alternatives make it difficult to justify acceptance in its current form. Addressing the above concerns could significantly strengthen the paper.