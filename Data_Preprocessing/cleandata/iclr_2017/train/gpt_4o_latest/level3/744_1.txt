Review
Summary of Contributions
The paper introduces Gated Residual Networks (GResNets), a novel layer augmentation technique that incorporates linear gating mechanisms into shortcut connections. The proposed method aims to simplify the learning of identity mappings, thereby improving optimization in deep networks. The authors demonstrate the method's efficacy on MNIST and CIFAR datasets, showing competitive performance and robustness to layer removal. The paper claims that the augmentation requires only one additional parameter per layer, making it computationally efficient. Furthermore, the authors suggest that their method raises questions about the design of shortcut connections in deep networks, particularly in the context of Highway Neural Networks and Residual Networks.
Decision: Reject
The primary reasons for rejection are the lack of sufficient novelty and empirical rigor. While the proposed method builds on established ideas from Residual Networks and Highway Neural Networks, the contribution is incremental rather than groundbreaking. Additionally, the empirical results are limited in scope, with no evaluations on large-scale datasets like ImageNet, which are critical for validating state-of-the-art claims.
Supporting Arguments
1. Lack of Novelty: The proposed method combines existing ideas from Highway Neural Networks and Residual Networks but does not introduce a fundamentally new concept. The use of a scalar gating parameter is a minor modification that does not sufficiently advance the field.
   
2. Empirical Limitations: The absence of ImageNet benchmarks significantly weakens the paper's claims of state-of-the-art performance. While the results on CIFAR-10 and CIFAR-100 are competitive, they are marginally better than Wide ResNets and require more parameters than DenseNet-BC, which achieves superior results with fewer parameters.
3. Inconsistent and Missing Results: The paper contains inconsistencies in reported results (e.g., He et al. (2015b) in Table 5) and omits key comparisons, such as Highway Neural Networks in Table 5. Additionally, the paper mentions a comparison of Total Squared Distance to Identity but does not provide the results.
4. Unclear Theoretical Justifications: The justification for substituting initialization mean and variance in derivations (Eq. 15 and 16) is unclear, and the squared parameter-wise distance as a surrogate for path length requires further clarification.
5. Speculative Claims: The claim of surpassing state-of-the-art results lacks empirical evidence and relies on speculative reasoning rather than rigorous experimentation.
Suggestions for Improvement
1. Expand Empirical Validation: Include evaluations on large-scale datasets like ImageNet to substantiate claims of improved optimization and performance.
2. Clarify Theoretical Contributions: Provide a more detailed explanation of the theoretical underpinnings, particularly regarding the squared parameter-wise distance and the substitution in Eq. 15 and 16.
3. Address Missing and Inconsistent Results: Include comparisons with Highway Neural Networks in Table 5 and resolve inconsistencies in reported results (e.g., He et al. (2015b)).
4. Parameter Efficiency: Explore ways to reduce the parameter overhead introduced by the gating mechanism, especially when compared to DenseNet-BC.
5. Provide Additional Insights: Include a more detailed analysis of the k parameter's behavior and its implications for network optimization and pruning.
Questions for the Authors
1. Why were ImageNet benchmarks omitted, and how do you plan to validate the method on larger datasets?
2. Can you clarify the theoretical justification for using the squared parameter-wise distance as a surrogate for path length?
3. How does the proposed method compare to DenseNet-BC in terms of parameter efficiency and performance trade-offs?
4. Why are Highway Neural Networks missing from Table 5, and how do they compare to the proposed method?
5. Can you provide the missing comparison of Total Squared Distance to Identity mentioned in the paper?
In conclusion, while the paper presents an interesting augmentation technique, it falls short in terms of novelty, empirical rigor, and theoretical clarity. Addressing these issues could significantly strengthen the contribution.