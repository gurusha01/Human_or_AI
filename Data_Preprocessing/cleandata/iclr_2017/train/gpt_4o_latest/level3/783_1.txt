Review of the Paper
Summary of Contributions
This paper revisits the trade-offs between synchronous and asynchronous stochastic optimization for distributed training of deep learning models. It identifies the limitations of both approaches: synchronous methods suffer from stragglers, while asynchronous methods are prone to gradient staleness, especially for non-convex functions like deep neural networks (DNNs). The authors propose a novel approach—synchronous stochastic optimization with backup workers—that mitigates the straggler issue without introducing gradient staleness. The paper makes several contributions, including empirical evidence of gradient staleness in asynchronous training, analysis of straggler effects in synchronous training, and experimental validation showing that the proposed method achieves faster convergence and better test accuracy compared to asynchronous training. The work is well-motivated and addresses a critical bottleneck in distributed deep learning.
Decision: Reject
While the paper presents an interesting idea and provides empirical evidence to support its claims, it falls short in several key areas. The primary concerns are the lack of rigorous theoretical analysis, insufficient exploration of parameter tuning, and limited experimental robustness. These issues undermine the reliability and generalizability of the proposed method.
Supporting Arguments for the Decision
1. Theoretical Rigorousness: The paper lacks a strong theoretical foundation to explain why the proposed method consistently outperforms asynchronous training. While empirical results are provided, a deeper theoretical analysis of the trade-off between gradient quality and straggler mitigation would strengthen the claims.
   
2. Experiment Design and Tuning: The experimental setup raises concerns. The tuning of learning rates for different batch sizes is not adequately justified, and the authors do not explore how sensitive the results are to hyperparameter choices. Additionally, the experiments report only average performance without addressing variability (e.g., best-case and worst-case outcomes), which limits the robustness of the conclusions.
3. Potential Bottleneck at the Parameter Server: The proposed method relies on gradients from the first "N" workers out of "N+b." This could create a bottleneck at the parameter server, especially as the scale of the system grows. The paper does not adequately address this issue or explore alternative designs to alleviate it.
Suggestions for Improvement
1. Theoretical Analysis: Include a theoretical framework to analyze the trade-offs between gradient staleness, straggler mitigation, and convergence speed. This would provide a stronger foundation for the empirical results.
   
2. Hyperparameter Sensitivity: Conduct a more thorough exploration of hyperparameter tuning, including learning rates and batch sizes. Report results for average, best-case, and worst-case scenarios to demonstrate robustness.
3. Scalability Analysis: Address the potential bottleneck at the parameter server by exploring alternative designs, such as hierarchical parameter servers or decentralized gradient aggregation.
4. Additional Metrics: Include metrics such as communication overhead and computational efficiency to provide a more holistic evaluation of the proposed method.
Questions for the Authors
1. How does the proposed method perform as the number of workers and backup workers increases? Does the parameter server become a bottleneck in larger setups?
2. How sensitive are the results to the choice of learning rates and other hyperparameters? Have you explored automated tuning methods?
3. Can you provide theoretical insights into why the proposed method achieves better convergence and accuracy compared to asynchronous training?
While the paper addresses an important problem and proposes a promising solution, the lack of theoretical depth and experimental rigor makes it difficult to recommend acceptance at this stage. Addressing the above concerns would significantly strengthen the paper.