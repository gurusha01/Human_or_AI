The paper addresses the problem of evaluating and generating out-of-distribution (OOD) novelty using generative models, proposing a framework based on holding out entire classes during training. The authors claim to provide actionable metrics (e.g., out-of-class count and max) and an experimental setup to assess the creative capacity of generative models, with a focus on distinguishing between in-distribution and OOD samples. The study evaluates autoencoders and GANs trained on MNIST digits, with the goal of generating novel "letters" as OOD samples, and introduces quantitative metrics to automate the evaluation of novelty generation.
Decision: Reject  
Key reasons for rejection include:  
1. Unclear definition of novelty and problematic metrics: The paper's definition of "novelty" as OOD generation is ambiguous, and the proposed metrics (e.g., out-of-class count and max) are ineffective in reliably distinguishing between in-distribution and OOD samples.  
2. Flawed experimental setup: The naive combination of MNIST digits and letters as datasets does not represent a natural distribution, leading to models that fail to properly distinguish between digits and letters. The generated "novel" samples (Fig. 3) are predominantly digits, undermining the claims of successful OOD generation.
Supporting Arguments:  
- Metrics and Evaluation: The proposed metrics fail to capture meaningful novelty. For instance, out-of-class count and max metrics are biased by the training of the discriminator on mixed datasets, which inherently skews results. The reliance on these metrics leads to the selection of models that generate digit-like samples rather than true OOD samples (letters).  
- Experimental Design: The use of binary quantization during sample generation likely distorts the appearance of samples, making them resemble digits even more. Additionally, the decision to train models on a combined dataset of digits and letters introduces noise and biases that compromise the validity of the results.  
- Generated Samples: The generated samples presented in Fig. 3 are visually indistinguishable from digits, contradicting the claim of generating novel letters. This raises concerns about the effectiveness of the proposed framework in achieving its stated goals.
Suggestions for Improvement:  
1. Clarify the definition of novelty: The authors should provide a more rigorous and actionable definition of novelty, grounded in computational creativity literature. This would help align the proposed metrics with the intended goals.  
2. Improve dataset design: Instead of naively combining MNIST digits and letters, consider using more realistic datasets that better represent the natural distribution of handwritten symbols. Alternatively, explore synthetic datasets designed specifically for novelty generation tasks.  
3. Refine metrics: The proposed metrics should be re-evaluated to ensure they effectively measure OOD novelty. For example, incorporating human evaluation or more robust unsupervised metrics could provide a better assessment of creative outputs.  
4. Address quantization issues: The impact of binary quantization on sample appearance should be thoroughly analyzed, and alternative approaches (e.g., 8-bit quantization) should be explored to preserve the fidelity of generated samples.  
5. Provide qualitative evidence: Beyond quantitative metrics, the paper should include qualitative evidence (e.g., human evaluation) to support claims of novelty generation.
Questions for the Authors:  
1. How do you define "novelty" in a way that distinguishes it from noise or trivial variations of in-distribution samples?  
2. Why was binary quantization chosen, and how does it affect the appearance and classification of generated samples?  
3. Can you provide examples of generated samples that were classified as letters by the discriminator but also pass a human evaluation test?  
4. How do you address the inherent biases introduced by training discriminators on combined datasets of digits and letters?  
In summary, while the paper tackles an interesting problem in computational creativity, the unclear definition of novelty, flawed experimental setup, and ineffective metrics significantly undermine its contributions. Addressing these issues could make the framework more impactful and scientifically rigorous.