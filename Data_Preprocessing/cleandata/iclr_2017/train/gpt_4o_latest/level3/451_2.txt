Review of the Paper
Summary of Contributions
This paper investigates the energy landscape of loss functions in neural networks, providing theoretical insights and practical algorithms. The authors demonstrate that level sets of the loss function become connected as neural networks become increasingly overparameterized. They quantify the degree of disconnectedness in terms of the increase in loss required to find a connected path and propose a simple algorithm to find geodesic paths between networks with decreasing loss along the path. The paper also explores the geometry of level sets, showing that the loss surface becomes more nonconvex as the loss decreases. Empirical results support these findings, with experiments on tasks ranging from polynomial regression to image classification and recurrent models. The work provides a fresh perspective on why stochastic gradient descent (SGD) performs well in practice, even in highly nonconvex settings.
Decision: Reject
While the paper presents interesting theoretical results and a novel algorithm, it has significant limitations that undermine its contribution. The primary reasons for rejection are: (1) the lack of analysis for deep networks, which limits the generalizability of the results, and (2) the insufficient practical relevance of the bound in Theorem 2.4, which is not adequately discussed or contextualized.
Supporting Arguments
1. Theoretical Contributions: The paper provides a rigorous analysis of the connectedness of level sets in single-layer ReLU networks and demonstrates that overparameterization leads to asymptotic connectedness. However, the results are restricted to shallow networks, leaving open questions about their applicability to deep architectures, which are more relevant in practice.
2. Algorithmic Contribution: The proposed algorithm for finding geodesic paths is simple and effective in demonstrating the connectedness of level sets empirically. However, the algorithm's scalability to larger, more complex networks is not addressed, and its practical utility in real-world optimization tasks remains unclear.
3. Empirical Results: The experiments are well-executed and provide qualitative insights into the geometry of loss surfaces. However, the datasets and architectures used are relatively simple (e.g., MNIST, CIFAR-10, and a small-scale RNN), which limits the generalizability of the findings to state-of-the-art deep learning models.
4. Limitations: The paper explicitly acknowledges its focus on oracle loss rather than empirical loss, which raises concerns about the practical relevance of the theoretical results. Additionally, the bound in Theorem 2.4 is not discussed in terms of its implications for real-world optimization, leaving its significance ambiguous.
Suggestions for Improvement
1. Extend Analysis to Deep Networks: The theoretical results should be extended to deeper architectures, as these are more representative of modern neural networks. This would significantly enhance the paper's relevance and impact.
2. Practical Relevance of Theorem 2.4: The authors should provide a detailed discussion of how the bound in Theorem 2.4 can be interpreted and applied in practice. For example, they could explore its implications for training dynamics or generalization performance.
3. Broader Empirical Validation: The experiments should include more complex architectures and datasets, such as ImageNet or large-scale language models, to validate the findings in realistic settings.
4. Comparison with Related Work: The paper could benefit from a more thorough comparison with existing work on loss surface geometry and optimization dynamics, particularly in terms of the novelty and advantages of the proposed algorithm.
Questions for the Authors
1. How do the results extend to deeper networks, and what challenges do you anticipate in generalizing the theoretical analysis?
2. Can you provide practical examples or scenarios where the bound in Theorem 2.4 would be useful for practitioners?
3. How does the proposed algorithm scale with the size and complexity of the network? Have you tested it on larger architectures beyond the ones reported in the paper?
4. Could the algorithm be adapted to address saddle points, which are also critical in optimization?
In summary, while the paper offers valuable theoretical insights and a novel algorithm, its limitations in scope and practical relevance prevent it from making a substantial contribution to the field in its current form. Addressing these issues in future work could significantly enhance its impact.