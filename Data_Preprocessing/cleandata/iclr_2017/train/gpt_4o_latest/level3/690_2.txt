The paper presents a comprehensive analysis of state-of-the-art deep neural networks (DNNs) submitted to the ImageNet challenge, focusing on practical metrics such as accuracy, memory footprint, parameter count, operations, inference time, and power consumption. The authors claim to provide insights into designing efficient DNNs for resource-constrained applications, with key findings including the independence of power consumption from architecture and batch size, a hyperbolic relationship between accuracy and inference time, and the utility of operations count as a predictor of inference time. The paper also introduces ENet, an efficient network architecture designed for ImageNet, which reportedly achieves high parameter utilization efficiency.
Decision: Reject
The decision to reject is based on two primary reasons: (1) the findings, while well-supported by data, are not particularly novel or surprising, and (2) the analysis has significant methodological limitations that undermine the generalizability and relevance of the results. Specifically, the reliance on an older GPU for experiments and the lack of direct evaluation of GPU utilization weaken the paper's claims. Furthermore, the absence of experiments involving compression techniques or production networks limits the practical applicability of the findings.
Supporting Arguments:
1. Lack of Novelty: The key findings, such as the independence of GPU utilization from architecture and batch size, and the proportionality of inference time to operations count, align with well-understood principles in the field. While the authors provide empirical evidence, these results do not offer new insights or challenge existing paradigms.
2. Methodological Limitations: The experiments were conducted on an outdated NVIDIA Jetson TX1 GPU, which may not reflect the behavior of modern GPUs. Additionally, GPU utilization was inferred indirectly rather than measured directly, which raises questions about the robustness of the conclusions. The hyperbolic relationship between accuracy and inference time, a central claim, is not convincingly demonstrated for newer model generations.
3. Limited Practical Relevance: The findings would be more impactful if tested with compression techniques or on production networks, as suggested by the authors themselves. Without this, the paper's contributions are of limited utility to practitioners.
Suggestions for Improvement:
1. Update Experimental Setup: Conduct experiments on modern GPUs to ensure the findings are relevant to current hardware. Additionally, directly measure GPU utilization to strengthen the claims.
2. Expand Scope: Include experiments with model compression techniques and production networks to demonstrate the practical applicability of the findings.
3. Strengthen Evidence: Provide more robust evidence for the hyperbolic relationship between accuracy and inference time, particularly for newer architectures.
4. Clarify Contributions: Highlight any unique insights or implications that differentiate this work from prior studies.
Questions for the Authors:
1. How do the findings generalize to modern GPUs with different architectures and power efficiency characteristics?
2. Can you provide additional evidence to support the hyperbolic relationship between accuracy and inference time, particularly for newer model generations?
3. Have you considered testing the findings with compressed models or production networks to enhance practical relevance?
In summary, while the paper provides a thorough analysis of DNN resource utilization, its lack of novelty, methodological limitations, and limited practical relevance prevent it from making a significant contribution to the field. Addressing these issues could substantially improve the paper's impact.