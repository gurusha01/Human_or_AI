Review of the Paper: "Homologically Functional Hashing (HFH) for Neural Network Compression"
Summary of Contributions
This paper introduces Homologically Functional Hashing (HFH), an extension of HashedNets, for compressing deep neural networks (DNNs). HFH employs multiple hash functions and a small reconstruction network (a multi-layer perceptron) to synthesize weights, addressing limitations of HashedNets such as high collision risks and suboptimal reconstruction. The authors highlight the homological compression space shared across layers, which simplifies the process of setting compression ratios. Experimental results demonstrate that HFH achieves higher compression ratios with less accuracy loss compared to HashedNets. Additionally, HFH is shown to be effective for compressing embeddings, offering benefits in memory usage and training efficiency. The authors suggest that HFH could serve as a standard tool for DNN compression and could be combined with other compression techniques like pruning for further improvements.
Decision: Reject
While the paper introduces an interesting extension to HashedNets, the proposed method falls short in practical utility and empirical performance compared to state-of-the-art pruning- and quantization-based techniques. The lack of runtime speedup and the 20% slower inference time raise concerns about its applicability in real-world scenarios. Furthermore, the absence of experiments combining HFH with pruning, as suggested by the authors, limits the paper's impact and completeness.
Supporting Arguments for Decision
1. Empirical Performance: While HFH improves upon HashedNets, it underperforms compared to pruning-based methods in terms of accuracy at similar compression ratios. This is particularly evident in experiments on large-scale datasets like ImageNet, where pruning achieves better results.
2. Runtime Efficiency: The 20% slower runtime compared to pruning methods is a significant drawback, especially for applications on resource-constrained devices. The lack of runtime speedup undermines the practical benefits of HFH's memory compression.
3. Limited Scope of Experiments: The authors suggest combining HFH with pruning but do not provide experimental evidence to support this claim. This omission weakens the paper's argument for HFH's broader applicability.
4. Misaligned Focus: HFH shows promise for embedding-heavy tasks, such as language modeling, where it offers marginal overhead and train-time benefits. However, the paper primarily evaluates HFH on CNNs and MLPs, where it is less competitive. A focus on embedding-heavy tasks could have made the paper more impactful.
Suggestions for Improvement
1. Combine HFH with Pruning: Conduct experiments that integrate HFH with pruning techniques to validate the claim that these methods can be used jointly for better compression. This could significantly strengthen the paper's contribution.
2. Runtime Optimization: Address the slower runtime of HFH, either through algorithmic improvements or hardware-specific optimizations, to make it more competitive in practical applications.
3. Focus on Embedding-Heavy Tasks: Shift the focus to tasks like language modeling or recommendation systems, where HFH's benefits for embedding compression are more pronounced. This could better align the method's strengths with its evaluation.
4. Broader Comparison: Include comparisons with more recent pruning and quantization techniques to provide a comprehensive evaluation of HFH's performance.
5. Ablation Studies: Provide deeper insights into the role of multiple hash functions and the reconstruction network by conducting ablation studies. This could help clarify the trade-offs between accuracy, memory savings, and runtime.
Questions for the Authors
1. How does HFH compare to state-of-the-art pruning methods in terms of training time and computational cost during training?
2. Can HFH be adapted to achieve runtime speedup, for example, by optimizing the reconstruction network or hash function implementation?
3. Why were embedding-heavy tasks not emphasized in the experiments, given HFH's potential advantages in this domain?
4. Have you explored the impact of varying the number of hash functions and the size of the reconstruction network on runtime efficiency and accuracy?
In conclusion, while HFH is a novel extension of HashedNets with theoretical and methodological contributions, its practical limitations and incomplete experimental validation make it less compelling for acceptance at this stage. Addressing the outlined issues could significantly enhance the paper's impact and relevance.