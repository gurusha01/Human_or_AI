The paper introduces the Energy-based Generative Adversarial Network (EBGAN), a novel extension of GANs that redefines the discriminator as an energy function rather than a binary classifier. This energy-based perspective allows for greater architectural flexibility and introduces a hinge loss for the discriminator and energy minimization for the generator. The authors provide theoretical guarantees, demonstrating that a Nash equilibrium under this framework leads to a generator that matches the data distribution, assuming infinite capacity. The paper also explores the use of an autoencoder-based discriminator and introduces a "pull-away term" (PT) to penalize high cosine similarity among generated samples, aiming to improve diversity.
Decision: Accept
Key Reasons:
1. Novelty and Theoretical Rigor: The energy-based reformulation of GANs is innovative and well-justified by theoretical proofs. The paper bridges GANs and energy-based models, offering a fresh perspective with potential for broader applications.
2. Experimental Contributions: The extensive hyperparameter exploration on MNIST and the semi-supervised learning results demonstrate the model's practical utility. The improved classification performance using contrastive samples is particularly compelling.
Supporting Arguments:
- The paper is well-written and addresses a topic of broad interest in the machine learning community. The theoretical results, particularly the proofs of Nash equilibrium optimality, are sound and add significant weight to the proposed framework.
- The use of an autoencoder as a discriminator is a thoughtful design choice, and the PT regularizer is a promising addition to encourage sample diversity.
- The experimental results on MNIST, LSUN, and CelebA datasets show that EBGANs produce high-quality samples and exhibit stable training behavior, a known challenge in GANs.
Suggestions for Improvement:
1. Clarity on Related Work: The relationship to prior works that broaden the discriminator's scope or use contrastive samples needs further elaboration. A clearer positioning of EBGAN within the GAN literature would strengthen the paper.
2. Evaluation Metrics: Sole reliance on visual inspection for LSUN and CelebA datasets is insufficient to conclude superiority over DCGANs. Including quantitative metrics like FID or IS would provide a more robust comparison.
3. PT Regularizer Analysis: The impact of the PT regularizer is not thoroughly assessed beyond visual inspection. A more detailed analysis, such as its effect on diversity metrics, would be valuable.
4. Minor Clarity Issues: The gradient directions for reconstruction loss (Sec 2.3) and the "PT" abbreviation (Sec 2.4) are unclear and should be clarified. Additionally, the use of the Inception model for MNIST KL scores (Sec 4.1) seems inappropriate and warrants reconsideration.
Questions for Authors:
1. Could you provide a more detailed comparison of EBGAN with other energy-based approaches, such as those using Gibbs distributions?
2. How does the PT regularizer quantitatively impact diversity in generated samples? Have you considered alternative regularization techniques?
3. Why was the Inception model chosen for MNIST KL scores, and how might this choice affect the results?
Overall, the paper makes a significant contribution to the GAN literature by introducing a novel energy-based perspective, supported by theoretical and experimental evidence. Addressing the outlined weaknesses would further solidify its impact.