The paper introduces the Latent Sequence Decompositions (LSD) framework, a novel method for jointly learning word decomposition and attention-based sequence-to-sequence (seq2seq) modeling. Unlike traditional approaches that rely on fixed, deterministic decompositions of output sequences, LSD dynamically learns decompositions conditioned on both input and output sequences. This probabilistic framework marginalizes over latent decompositions during training, enabling a more flexible representation of sequences. The method is particularly motivated by parallels with pronunciation mixture models in HMM-based speech recognition, where word-to-pronunciation mappings depend on acoustic input. Experimental results on the Wall Street Journal (WSJ) dataset demonstrate a reduction in Word Error Rate (WER) from 14.8% (character baseline) to 12.9% with LSD, and further to 9.6% when combined with a convolutional encoder.
Decision: Accept with minor revisions.  
The key reasons for this decision are the novelty of the proposed framework and its theoretical soundness. The paper addresses a significant limitation in seq2seq modeling by introducing a dynamic, input-conditioned decomposition mechanism. However, the experimental validation could be strengthened, and some aspects of the presentation require clarification.
Supporting Arguments:  
1. Novelty and Motivation: The LSD framework is well-motivated and addresses a clear gap in the literature. The analogy to pronunciation mixture models in HMM-based systems provides a strong conceptual foundation.  
2. Theoretical Rigor: The paper presents a detailed derivation of the training objective and gradient estimation, demonstrating a solid understanding of the underlying principles.  
3. Experimental Results: The reported improvements in WER are promising, particularly the significant gains achieved by combining LSD with a convolutional encoder.  
Suggestions for Improvement:  
1. Baseline Comparisons: The paper would benefit from including a word-level baseline for comparison, given the relatively small vocabulary size (20K) of the WSJ dataset. This would provide a more comprehensive evaluation of LSD's effectiveness.  
2. Computational Cost: The high computational cost (5 days to converge) is a concern. The authors should discuss potential optimizations or trade-offs to make the method more practical.  
3. Clarification of Results: Table 2 comparisons could be misleading, as the improvements with a language model appear marginal. The authors should provide a more nuanced discussion of these results.  
4. Phrasing Issues: The phrase "O(5) days to converge" is unconventional and should be rephrased for clarity.  
Questions for the Authors:  
1. How does the LSD framework scale to larger datasets with more complex vocabularies?  
2. Can the proposed method be adapted to other sequence modeling tasks, such as machine translation or text generation?  
3. What are the implications of the observed bias towards shorter decompositions during training, and how might this be mitigated?  
Overall, the paper makes a valuable contribution to the field of seq2seq modeling and is likely to inspire further research. Addressing the above points will enhance its impact and clarity.