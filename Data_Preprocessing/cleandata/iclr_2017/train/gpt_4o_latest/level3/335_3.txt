Review
The paper presents an extension of the Bell & Sejnowski infomax framework to noisy neural populations, offering a mathematical derivation and a hierarchical infomax approach for unsupervised representation learning. The authors propose an efficient optimization algorithm that works for complete, overcomplete, and undercomplete bases, demonstrating its application on natural image patches and the MNIST dataset. The results show comparable performance to existing methods, with advantages in training speed and robustness. The paper also claims biological plausibility and potential applicability to deep networks.
Decision: Reject
Key Reasons:
1. The paper is difficult to follow due to its length and complexity, which hinders accessibility and clarity.
2. The hierarchical model is poorly named and does not align with established deep net hierarchies, leading to confusion.
3. The work does not adequately situate itself in the context of prior literature, particularly the omission of Karklin & Simoncelli (2011), which is highly relevant.
Supporting Arguments
While the approach is mathematically rigorous and potentially generalizable, the presentation lacks clarity. The derivations, while detailed, overwhelm the reader and could benefit from being moved to a supplementary document. The hierarchical infomax model, a central contribution, is not well-explained or justified in terms of its naming and relationship to deep learning hierarchies. Additionally, the omission of Karklin & Simoncelli (2011) is a significant oversight, as their work on similar infomax frameworks could provide valuable context and comparisons.
The experimental results are promising, showing Gabor-like filters and robust performance on natural images and MNIST. However, the outcomes are not significantly novel compared to existing methods like ICA or sparse coding. The claim of faster convergence is compelling but not sufficiently substantiated with detailed comparisons or ablation studies.
Additional Feedback
1. Clarity and Organization: The paper would benefit from condensing the main text and moving detailed mathematical derivations to an appendix. This would make the core ideas more accessible.
2. Model Naming: The term "hierarchical model" is misleading. Consider renaming it to better reflect its structure and distinguish it from deep learning hierarchies.
3. Literature Context: The authors should reference and discuss Karklin & Simoncelli (2011) and other closely related works to better position their contribution in the field.
4. Experimental Validation: While the results are promising, additional experiments comparing the method to a broader range of baselines (e.g., variational autoencoders, modern sparse coding techniques) would strengthen the claims.
5. Biological Plausibility: The biological relevance of the model is mentioned but not explored in depth. Including a discussion or experiments related to this aspect would enhance the paper's impact.
Questions for the Authors
1. How does the hierarchical infomax approach compare to modern deep learning methods in terms of scalability and performance on larger datasets?
2. Can you clarify the biological constraints incorporated into the model and how they influence the optimization process?
3. Why was Karklin & Simoncelli (2011) not referenced, and how does your work differ from or build upon their framework?
In summary, while the paper introduces an interesting extension of the infomax principle, significant revisions are needed to improve clarity, situate the work in the literature, and better justify its contributions.