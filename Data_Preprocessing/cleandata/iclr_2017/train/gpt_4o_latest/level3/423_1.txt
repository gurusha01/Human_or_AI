The paper introduces the Generative Multi-Adversarial Network (GMAN), a novel extension of GANs that incorporates multiple discriminators to address stability and training challenges inherent in the original GAN framework. The authors claim that GMAN enables training with the original minimax objective, accelerates convergence, and improves the quality of generated samples. They also propose a new evaluation metric, GMAM, to compare GMAN variants and demonstrate the effectiveness of their approach on image generation tasks using datasets like MNIST, CIFAR-10, and CelebA. The paper explores multiple discriminator roles, ranging from harsh critics to lenient teachers, and introduces mechanisms like softmax-based feedback and automated regulation to balance generator-discriminator dynamics.
Decision: Accept with Major Revisions
Key Reasons for Decision:
1. Promising Direction: The use of multiple discriminators to improve GAN stability and convergence is a compelling direction, addressing a well-known challenge in GAN training. The empirical results on accelerated convergence and improved sample quality are encouraging.
2. Insufficient Comparisons: The paper lacks comparisons with alternative stabilization techniques, such as GANs with discriminator gradient penalties (e.g., GAN with DAE). These comparisons are crucial to establish the advantages of GMAN over existing methods.
Supporting Arguments:
- The paper is well-motivated and grounded in the literature, with a clear focus on addressing GAN stabilization issues. The introduction of multiple discriminators and the exploration of their roles are innovative and theoretically justified.
- The empirical results demonstrate faster convergence and improved sample quality, particularly with GMAN∗, which dynamically adjusts the difficulty of the adversarial game.
- The proposed GMAM metric is a useful addition for evaluating multi-discriminator frameworks, though its adoption in broader GAN research remains to be seen.
Additional Feedback for Improvement:
1. Comparative Analysis: Include a detailed comparison with stabilization techniques like GAN with DAE or gradient penalty methods (e.g., WGAN-GP). This would highlight GMAN's unique contributions and clarify its advantages.
2. Ablation Studies: Provide more ablation studies to isolate the impact of individual components, such as the softmax parameter λ and the automated regulation mechanism in GMAN∗.
3. Clarity on Diversity: Elaborate on how discriminator diversity is maintained and its impact on training dynamics. While dropout and architectural variations are mentioned, their specific contributions to stability and performance need further clarification.
4. Evaluation Metrics: While GMAM is introduced, it would be helpful to include standard metrics like FID or Inception Score for broader comparability with existing GAN models.
Questions for Authors:
1. How does GMAN compare to GANs with gradient penalties or other stabilization techniques in terms of training stability and sample quality?
2. Can the authors provide more insights into the computational overhead introduced by multiple discriminators, especially for large-scale datasets?
3. How sensitive is GMAN to the choice of the number of discriminators (N)? Are there diminishing returns with increasing N?
In summary, while the paper presents a promising and innovative approach, additional comparisons and analyses are necessary to fully establish the benefits of GMAN over existing methods. The revisions suggested above would significantly strengthen the paper.