Review of the Paper
Summary of Contributions
This paper introduces Iterative PoWER, an off-policy extension of the PoWER policy gradient algorithm, aimed at improving policy optimization with a limited number of rollouts. The authors propose a novel iterative framework that updates the lower bounds of the expected reward to achieve better performance without requiring fresh samples at each iteration. The paper also extends existing methods to handle negative rewards, enabling the use of control variates to reduce variance. The experimental results demonstrate the algorithm's potential on the Cartpole benchmark and real-world online advertising data, showing significant improvements in performance and efficiency. The work is motivated by practical constraints in production environments, such as robotics and online systems, where frequent policy updates are infeasible.
Decision: Reject
While the paper presents an interesting extension to the PoWER algorithm and highlights its practical relevance, the following key issues lead to the decision to reject:
1. Insufficient Experimental Details: The experimental setup lacks critical information for reproducibility, particularly in Section VI. Details about force application, state representation, initial state distribution, and policy noise are missing, making it difficult to evaluate the validity of the results.
2. Inadequate Policy Representation: The use of a linear policy for complex tasks like Cartpole swing-up and balance is inappropriate, and the paper does not justify this choice or provide sufficient details about task specifics and episode duration.
3. Unclear Theoretical Explanations: The footnote on page 8 is confusing, with unclear connections to Newton's method and missing discussions on gradients and Hessians, which weakens the theoretical rigor of the paper.
Supporting Arguments
1. Reproducibility Concerns: The lack of detailed experimental descriptions undermines the scientific rigor of the paper. Without clear information on critical parameters and setups, it is challenging to verify the claims or replicate the results.
2. Choice of Policy Representation: A linear policy is unlikely to capture the complexities of tasks like Cartpole swing-up, which typically require non-linear policies. This undermines the validity of the experimental results and raises questions about the generalizability of the approach.
3. Theoretical Ambiguities: The unclear explanation of the connection to Newton's method and the absence of a discussion on gradients and Hessians detracts from the theoretical soundness of the proposed algorithm.
Suggestions for Improvement
To strengthen the paper, the authors should:
1. Provide Detailed Experimental Descriptions: Include comprehensive details about the experimental setup, such as force application, state representation, initial state distribution, and policy noise. This will improve reproducibility and allow for a more thorough evaluation of the results.
2. Justify Policy Choices: Explain why a linear policy was used for the Cartpole task and discuss its limitations. Consider using more expressive policy representations, such as neural networks, to better handle complex tasks.
3. Clarify Theoretical Explanations: Expand the discussion on the connection to Newton's method, explicitly addressing gradients and Hessians. This will enhance the theoretical clarity and rigor of the paper.
4. Improve Task-Specific Details: Provide information on the specifics of the Cartpole task, including episode duration, success criteria, and any modifications made to the standard benchmark.
Questions for the Authors
1. Why was a linear policy chosen for the Cartpole task, and how does this choice affect the generalizability of the results?
2. Can you provide more details on the experimental setup, particularly regarding force application, state representation, initial state distribution, and policy noise?
3. How does the proposed method handle high-variance regions in the policy parameter space during multiple iterations of PoWER? Have you considered additional regularization techniques?
4. Could you elaborate on the connection to Newton's method and clarify the role of gradients and Hessians in the algorithm?
While the paper introduces a promising extension to the PoWER algorithm, addressing the above concerns is crucial to ensure scientific rigor and practical applicability.