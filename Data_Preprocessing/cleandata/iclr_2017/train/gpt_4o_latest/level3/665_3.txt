The paper introduces MS MARCO, a large-scale machine reading comprehension (RC) dataset designed to address limitations in existing datasets. It claims to be unique by using real-world user queries from Bing and Cortana, human-generated answers, and passages extracted from real web documents. The dataset currently includes 100,000 queries, with plans to scale to 1 million. The authors highlight the dataset's potential to inspire research in RC and question answering (QA) by providing a more realistic and challenging benchmark compared to synthetic or crowdworker-generated datasets like SQuAD. The paper also presents baseline evaluations using generative and cloze-style models, along with an analysis of answer type distributions.
Decision: Reject
Key Reasons for Rejection:
1. Incomplete Validation: The paper lacks critical validation metrics, such as human performance and inter-human agreement, to assess dataset difficulty and the reliability of evaluation metrics like ROUGE and BLEU. Without these, it is hard to gauge the dataset's quality and the validity of the baseline results.
2. Missing Comparisons: The authors fail to compare the answer type distributions in MS MARCO with existing datasets like SQuAD, which would strengthen claims of novelty and distinctiveness.
Supporting Arguments:
The dataset is well-motivated, addressing real-world challenges in RC and QA, such as noisy user queries and the need for multi-passage reasoning. Its reliance on human-generated answers and real-world web documents is a significant improvement over synthetic datasets. However, the absence of human performance benchmarks and inter-human agreement raises concerns about the dataset's difficulty and the reliability of automatic metrics. Additionally, the lack of a detailed description of the best-performing baseline model and the omission of classifier accuracy for query filtering undermine the scientific rigor of the paper. These gaps weaken the empirical support for the claims made.
Suggestions for Improvement:
1. Human Performance Benchmarks: Include experiments measuring human performance and inter-human agreement to validate dataset difficulty and metric reliability.
2. Comparative Analysis: Provide a detailed comparison of answer type distributions between MS MARCO and existing datasets like SQuAD to substantiate claims of novelty.
3. Metric Justification: Address the limitations of automatic metrics like ROUGE and BLEU, and explore alternatives that better correlate with human judgment.
4. Classifier Accuracy: Report the accuracy of the query filtering classifier and the information retrieval system to ensure transparency in dataset construction.
5. Baseline Model Details: Include a detailed description of the best-performing baseline model to enable reproducibility and better evaluation of results.
6. Formatting Corrections: Fix minor typographical issues, such as opening quotes, to improve the paper's presentation.
Questions for the Authors:
1. What is the inter-human agreement on the dataset, and how does it reflect the dataset's difficulty?
2. How does the answer type distribution in MS MARCO compare to other datasets like SQuAD or WikiQA?
3. Why were ROUGE and BLEU chosen as evaluation metrics despite their known limitations in correlating with human judgment?
4. What is the accuracy of the classifier used to filter answer-seeking queries, and how does it impact dataset quality?
5. Can you provide a detailed description of the best-performing baseline model and its experimental setup?
In summary, while MS MARCO is a promising dataset with unique characteristics, the paper requires stronger validation, comparative analysis, and methodological transparency to meet the standards for acceptance.