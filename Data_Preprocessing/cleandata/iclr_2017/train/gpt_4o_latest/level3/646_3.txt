Review of the Paper
Summary of Contributions
This paper introduces a novel approach to Interactive Question Answering (IQA) by proposing a Context-aware Attention Network (CAN) that integrates a two-level attention mechanism and an interactive mechanism for handling incomplete information scenarios. The authors extend the bAbI dataset to create ibAbI, which includes supporting questions and answers for ambiguous or insufficient information cases. The proposed model demonstrates significant improvements over baseline QA models on both traditional QA and IQA tasks, achieving a 40% improvement in IQA scenarios. The paper also highlights the ability of the model to dynamically generate supplementary questions and incorporate user feedback without additional training, making it self-adaptive and efficient.
Decision: Reject
While the paper addresses an interesting and well-motivated problem, the experimental evidence and methodological clarity are insufficient to warrant acceptance at ICLR. The following key reasons support this decision:
1. Insufficient Experimental Scope: The experiments are limited to only three of the twenty tasks in the bAbI dataset, which undermines the generalizability and reliability of the conclusions.
2. Lack of Clarity in Dataset Extension: The process for generating supporting questions and user feedback in the ibAbI dataset is inadequately explained, leaving critical gaps in reproducibility and evaluation.
Supporting Arguments
1. Experimental Limitations: By evaluating the model on only three tasks, the paper fails to demonstrate the robustness of the proposed approach across diverse reasoning and QA scenarios. This narrow scope weakens the claim of general effectiveness.
2. Dataset Clarity: The lack of details on how supporting questions were generated, including templates, vocabulary size, and user feedback mechanisms, raises concerns about the validity and reproducibility of the ibAbI dataset. Without this information, it is difficult to assess whether the dataset fairly represents interactive QA challenges.
3. Evaluation Methodology: Treating supporting questions as part of the same instance as the original question may inflate performance metrics. A fairer evaluation would treat all questions (original and supplementary) as separate instances.
Additional Feedback for Improvement
1. Simplify Synthetic Question Generation: The model's synthetic question generation could be streamlined by directly predicting template values instead of using an RNN decoder. This would reduce complexity and improve interpretability.
2. Clarify Dataset Construction: Provide detailed explanations of how the ibAbI dataset was constructed, including the rules/templates used for generating supplementary questions and the criteria for determining when user feedback is required.
3. Expand Experimental Scope: Evaluate the model on all twenty bAbI tasks to demonstrate its generalizability. Additionally, consider testing on other IQA datasets to validate the approach in diverse settings.
4. Improve Space Utilization: Reduce the exposition on basic concepts like GRUs and sentence encodings by referencing prior work. This would free up space for more critical discussions, such as dataset construction and evaluation methodology.
5. Enhance Realism of Generated Questions: The supplementary questions generated by the model often resemble supporting facts rather than genuine user interactions. Refining this aspect could make the model more applicable to real-world IQA scenarios.
Questions for the Authors
1. How were the templates and vocabulary for generating supplementary questions in the ibAbI dataset designed? Were they manually crafted or automatically generated?
2. Why were only three tasks from the bAbI dataset chosen for evaluation? How do you justify the generalizability of your model based on such a limited subset?
3. Could you provide a breakdown of how user feedback was simulated during training and testing? How does this simulation align with real-world IQA scenarios?
In conclusion, while the paper presents a compelling idea and a novel approach to IQA, the experimental limitations and lack of clarity in key aspects prevent it from meeting the standards for acceptance at ICLR. Addressing these issues could significantly strengthen the paper for future submissions.