Review of "Normalized LSTM: A Reparametrization for Efficient Training"
Summary of Contributions
This paper proposes a novel reparametrization of LSTMs, termed "Normalized LSTM," which aims to preserve the means and variances of hidden states and memory cells across time steps. The authors argue that this approach offers training benefits similar to Layer Normalization (LN) and Batch Normalization (BN) but with reduced computational overhead. The method builds upon weight normalization and normalization propagation, extending these ideas to recurrent neural networks. The paper also explores the impact of this reparametrization on gradient flow and provides a weight initialization scheme tailored to the proposed method. Empirical results on character-level language modeling and image generative modeling tasks suggest that the method performs comparably to LN and BN while being computationally faster.
Decision: Reject  
Key reasons: Limited novelty and unconvincing experimental results.
Supporting Arguments
1. Limited Novelty: While the paper extends weight normalization and normalization propagation to LSTMs, the modifications are relatively minor and incremental. The core idea—preserving normalization through reparametrization—has already been explored in prior work on normalization propagation and BN-LSTMs. The contribution primarily lies in adapting these concepts to recurrent architectures, which, while useful, lacks significant theoretical or methodological innovation.
2. Unconvincing Experimental Results: The empirical evaluation does not convincingly support the claims of superior performance. While the proposed method shows faster convergence, LN achieves better optimization results but suffers from overfitting. This raises concerns about the generalization capabilities of the proposed approach. Additionally, the authors did not use data-dependent parameter initialization for weight normalization, as originally proposed, which could have impacted the results. The experiments, though covering two tasks, are relatively limited in scope and fail to demonstrate clear advantages over existing methods.
Suggestions for Improvement
1. Stronger Empirical Validation: The experimental results could be strengthened by including a broader range of tasks, such as machine translation or speech recognition, where LSTMs are widely used. Additionally, comparisons with more recent normalization techniques and state-of-the-art baselines would provide a clearer picture of the method's effectiveness.
2. Address Overfitting: The paper should investigate why LN overfits more and whether the proposed method is prone to similar issues. Exploring regularization techniques or adaptive mechanisms to mitigate overfitting could improve the generalization performance.
3. Data-Dependent Initialization: Incorporating data-dependent parameter initialization for weight normalization, as suggested in prior work, could potentially improve the results and provide a fairer comparison with existing methods.
4. Theoretical Insights: While the paper provides some analysis of gradient flow, it would benefit from deeper theoretical insights into why the proposed method improves training efficiency and how it compares analytically to LN and BN.
Questions for the Authors
1. Why was data-dependent initialization for weight normalization not used in the experiments? Could this omission have impacted the results?
2. How does the proposed method perform on tasks with longer sequences or more complex dependencies, such as machine translation or speech recognition?
3. Can the authors provide more detailed analysis or visualizations of the gradient flow to better illustrate the benefits of the proposed reparametrization?
In conclusion, while the paper addresses an important problem and proposes a computationally efficient solution, the limited novelty and insufficient empirical evidence make it difficult to recommend acceptance at this time. Addressing the above concerns could significantly strengthen the paper.