Review of "Surprisal-Driven Recurrent Networks"
Summary of Contributions
This paper introduces a novel approach to incorporating "surprisal" as a top-down feedback signal in recurrent neural networks (RNNs). By using the discrepancy between previous predictions and actual observations as an additional input, the authors propose a feedback mechanism that enhances the prediction capabilities of Long Short-Term Memory (LSTM) networks. The paper claims that this surprisal-driven feedback improves generalization and achieves state-of-the-art performance on the enwik8 character-level text modeling task, achieving 1.37 bits per character (BPC). The authors argue that this feedback mechanism mimics certain aspects of human cognition, where prediction errors influence future decisions, and they position their work as a step toward understanding top-down feedback in neural systems.
Decision: Reject
While the paper presents an intriguing idea with potential, it falls short in several critical areas, including experimental validation, clarity of exposition, and fair benchmarking. These issues undermine the confidence in the paper's claims and its scientific rigor.
Supporting Arguments for Decision
1. Limited Experimental Validation: The experimental evaluation is restricted to a single dataset (enwik8), which is insufficient to demonstrate the generalizability of the proposed approach. Broader testing on diverse datasets is necessary to validate the effectiveness of surprisal-driven feedback across different tasks and domains.
   
2. Unsubstantiated State-of-the-Art Claim: The claim of achieving state-of-the-art performance is questionable. The paper does not compare its approach to dynamic evaluation methods, which also leverage prediction errors during inference. Additionally, cited methods like hypernetworks achieve better results, suggesting that the proposed method may not be as competitive as claimed.
3. Lack of Experimental Evaluation for Feedback RNN: The Feedback RNN architecture is described in detail but is not evaluated experimentally. This omission raises questions about its relevance and whether it contributes meaningfully to the paper's claims.
4. Clarity and Accessibility: Sections 2.4 and 2.5 are dense with equations but lack sufficient analysis or intuitive explanations. This makes it difficult to understand the significance of the mathematical formulations or how they contribute to the overall approach.
Suggestions for Improvement
1. Broader Experimental Validation: Evaluate the proposed method on additional datasets, such as Penn Treebank or WikiText, to demonstrate its generalizability and robustness.
   
2. Fair Benchmarking: Include comparisons with dynamic evaluation approaches and other methods that utilize prediction errors during inference. This would provide a more comprehensive assessment of the proposed method's performance.
3. Clarity in Exposition: Improve the explanation of equations in Sections 2.4 and 2.5 by providing accompanying insights, visualizations, or examples to make the mathematical details more accessible.
4. Feedback RNN Evaluation: Conduct experiments to evaluate the Feedback RNN architecture and clarify its role in the overall framework.
Questions for the Authors
1. Why was the experimental evaluation limited to the enwik8 dataset? Are there any specific challenges in applying the method to other datasets?
2. How does the proposed method compare to dynamic evaluation approaches in terms of performance and computational cost?
3. What motivated the choice of using surprisal as the feedback signal, and how does it compare to alternative feedback mechanisms?
In conclusion, while the idea of surprisal-driven feedback is promising, the paper requires significant improvements in experimental rigor, clarity, and benchmarking to substantiate its claims.