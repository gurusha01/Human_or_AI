Review of the Paper
Summary of Contributions
The paper proposes a supervised sequence-to-sequence transduction model with a hard attention mechanism, designed specifically for tasks with monotonic alignment between input and output sequences. The authors argue that their approach combines the strengths of traditional statistical alignment methods with the representational power of neural networks. The model is evaluated on the task of morphological inflection generation, where it achieves state-of-the-art results in low-resource settings and competitive performance on larger datasets. The paper also provides an analysis of the learned representations, comparing hard and soft attention mechanisms, and highlights the advantages of the proposed model in terms of simplicity and alignment quality.
Decision: Reject
The primary reasons for this decision are the lack of novelty in the proposed approach and the unfair evaluation methodology. While the model performs well in certain settings, its contributions overlap significantly with prior work, and the comparison setup undermines the validity of the results.
Supporting Arguments
1. Lack of Novelty: The proposed model is largely incremental, building on existing work in monotonic sequence transduction and attention mechanisms. While the use of externally learned alignments is an interesting choice, it does not represent a significant departure from prior methods. The paper does not sufficiently differentiate its contributions from related approaches, such as those by Graves (2012) and Yu et al. (2016).
2. Unfair Evaluation: The evaluation compares the proposed model, which uses external alignments during training, with a vanilla soft-attention model that learns alignments from scratch. This comparison is inherently biased, as pretraining soft-attention models with external alignments could similarly improve their performance, particularly in low-resource settings. The authors do not address this limitation, which undermines the validity of their claims about the superiority of the proposed approach.
3. Limited Applicability: The model's reliance on monotonic alignment restricts its applicability to a narrow set of tasks, such as morphological inflection generation. While this focus is appropriate for the datasets used, the broader utility of the approach is not demonstrated.
4. Marginal Improvements on Larger Datasets: On larger datasets like SIGMORPHON, the proposed model shows minimal improvements, and its advantages are limited to specific language classes. This raises questions about its scalability and generalizability to more diverse tasks.
Suggestions for Improvement
1. Fair Comparisons: The authors should include experiments where soft-attention models are pretrained with external alignments, to provide a more balanced evaluation of the proposed approach.
2. Broader Applicability: Demonstrating the model's effectiveness on a wider range of tasks, such as transliteration or abstractive summarization, would strengthen its contributions and address concerns about limited applicability.
3. Clarify Novelty: The authors should explicitly highlight the unique aspects of their approach and provide a more detailed comparison with related work to better position their contributions within the existing literature.
4. Ablation Studies: Including ablation studies to isolate the impact of the hard attention mechanism and the use of external alignments would provide deeper insights into the model's strengths and weaknesses.
Questions for the Authors
1. How does the performance of the proposed model compare to soft-attention models pretrained with external alignments? Would this reduce the observed performance gap in low-resource settings?
2. Can the model handle tasks with non-monotonic alignments, or is it strictly limited to monotonic scenarios? If not, how could it be extended to address such tasks?
3. How sensitive is the model to the quality of the external alignments? Have you tested its robustness to noisy or imperfect alignments?
While the paper presents an interesting approach to monotonic sequence transduction, the issues of limited novelty and unfair evaluation make it unsuitable for acceptance in its current form. Addressing these concerns and expanding the scope of the work could significantly improve its impact.