Review of "WikiSem500: A Multilingual Dataset for Outlier Detection in Word Representations"
Summary
The paper introduces WikiSem500, a novel dataset for evaluating word embeddings through the outlier detection task. The dataset is automatically generated using the Wikidata hierarchy and spans five languages: English, Spanish, German, Chinese, and Japanese. Entities within the same category form clusters, and outliers are sampled based on graph distances in Wikidata. Heuristics are applied to filter uninteresting or low-quality clusters. The authors claim that this dataset addresses limitations in existing resources, such as human subjectivity in annotations and lack of multilingual coverage. They also demonstrate a correlation between performance on WikiSem500 and sentiment analysis, suggesting its potential utility for intrinsic evaluation of semantic models.
Decision: Reject
While the paper presents a valuable contribution in the form of a new dataset, it falls short in its motivation, positioning within the literature, and empirical validation. The lack of comparison with existing evaluation methods and datasets, as well as insufficient evidence of the dataset's advantages, undermines its impact.
Supporting Arguments
1. Strengths:
   - The dataset is multilingual and automatically generated, addressing scalability and subjectivity issues in manually annotated datasets.
   - The use of Wikidata as a graph is innovative and allows for systematic generation of clusters and outliers.
   - The authors provide a detailed methodology for dataset creation and apply rigorous heuristics to improve quality.
2. Weaknesses:
   - Lack of Comparison with Existing Approaches: The paper does not adequately discuss how WikiSem500 compares to established benchmarks like word analogy datasets (e.g., Mikolov et al., 2013) or entity typing datasets. The claim that WikiSem500 is "more diverse and challenging" is not substantiated with empirical evidence or qualitative analysis.
   - Unclear Advantages: While the dataset is multilingual and large-scale, it is unclear whether these features translate into better evaluation of word embeddings. The correlation with sentiment analysis is promising but insufficient to demonstrate superiority over existing resources.
   - Relevance to ICLR: The focus on dataset creation and intrinsic evaluation makes this work more suited for a conference like LREC (Language Resources and Evaluation Conference), which emphasizes linguistic resources, rather than ICLR, which focuses on machine learning advancements.
Suggestions for Improvement
1. Positioning in the Literature: Provide a more thorough comparison with existing evaluation datasets and tasks, such as word similarity, analogy, and entity typing. Discuss the limitations of these approaches and explicitly highlight how WikiSem500 addresses them.
2. Empirical Validation: Conduct experiments to demonstrate the dataset's utility beyond sentiment analysis. For example, evaluate its correlation with other downstream tasks (e.g., machine translation, question answering) or compare its performance with existing benchmarks.
3. Broader Impact: Discuss the potential limitations of using Wikidata as the sole source of semantic knowledge. For instance, how does the dataset handle domain-specific or low-resource languages?
4. Relevance to ICLR: Emphasize the machine learning implications of the dataset, such as its potential for training or fine-tuning word embeddings, to align better with ICLR's scope.
Questions for the Authors
1. How does WikiSem500 compare quantitatively and qualitatively to existing datasets like SimLex-999 or Mikolov's analogy dataset in terms of diversity, difficulty, and relevance to downstream tasks?
2. What specific advantages does the multilingual aspect of WikiSem500 offer for evaluating embeddings trained on multilingual corpora?
3. Could the dataset generation methodology be extended to include syntactic tasks, as hinted at in the future work section?
In conclusion, while WikiSem500 is a promising resource, the paper requires stronger positioning in the literature, clearer empirical validation, and alignment with the conference's focus to warrant acceptance.