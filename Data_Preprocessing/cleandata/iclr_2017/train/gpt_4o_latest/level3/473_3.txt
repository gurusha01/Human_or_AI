Review of the Paper
Summary of Contributions
This paper introduces a novel theoretical framework for improving language modeling in recurrent neural networks (RNNs) by augmenting the conventional cross-entropy loss function. The proposed framework incorporates an additional term that minimizes the KL-divergence between the model's predictions and an estimated target distribution derived from word embedding similarities. This approach facilitates tying the input embedding and output projection matrices, reducing the number of trainable parameters. The authors provide both theoretical motivation and empirical validation, demonstrating state-of-the-art performance on the Penn Treebank (PTB) dataset and consistent improvements on Wikitext-2. The paper is well-written, with clear arguments, and offers an interesting perspective on leveraging word embeddings for better supervision in language modeling.
Decision: Accept
The paper makes a strong theoretical and empirical contribution to the field of language modeling. The proposed framework is novel, well-motivated, and supported by rigorous experiments. While there are some limitations, they do not detract significantly from the overall impact of the work.
Supporting Arguments
1. Novelty and Motivation: The paper addresses inefficiencies in conventional RNN language models by proposing a theoretically grounded loss augmentation and parameter tying mechanism. This is a novel contribution that builds on prior work but offers a unique perspective by leveraging word embedding metrics.
   
2. Empirical Validation: The experiments on PTB and Wikitext-2 datasets convincingly demonstrate the effectiveness of the proposed framework. The results show consistent improvements across different network sizes and datasets, with the combined approach (REAL) achieving the best performance.
3. Theoretical Rigor: The authors provide a thorough theoretical analysis of the augmented loss function and its implications for parameter tying. This adds depth to the paper and strengthens its claims.
Suggestions for Improvement
1. Augmented Loss Function: The augmented loss function lacks trainable parameters and primarily acts as a regularizer. While this is effective for smaller datasets, its utility for larger datasets appears limited. The authors could explore ways to make the loss function more adaptive or trainable to enhance its scalability.
2. Engineered Nature of the Loss: The loss function is heavily engineered for parameter tying, and its behavior under relaxation or alternative estimation methods is unclear. Future work could investigate the robustness of the framework to such modifications.
3. Clarity in Section 3: It would be helpful to clarify whether \( \tilde{y} \) is conditioned on the current example or the entire history. This detail is crucial for understanding the estimation of the target distribution.
4. Equation Correction: In Equation 3.5, the summation index \( i \) should be enumerated over \( V \) (the vocabulary) rather than \( |V| \) (the size of the vocabulary). Correcting this will improve the mathematical precision of the paper.
Questions for the Authors
1. How does the augmented loss function perform when applied to datasets significantly larger than Wikitext-2? Does its regularization effect diminish as the dataset size increases?
2. Have you explored alternative methods for estimating \( \tilde{y} \) beyond word embedding similarities? For example, could contextual embeddings or pre-trained language models improve the estimation?
3. What is the computational overhead introduced by the augmented loss term, and how does it scale with vocabulary size?
Conclusion
This paper makes a meaningful contribution to the field of language modeling by proposing a theoretically motivated and empirically validated framework. While there are some limitations, the novelty, rigor, and empirical results justify its acceptance. The suggestions and questions provided aim to further strengthen the work and address potential concerns.