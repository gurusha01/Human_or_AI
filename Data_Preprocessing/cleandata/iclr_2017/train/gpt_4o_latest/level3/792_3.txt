Review of the Paper
Summary of Contributions
This paper introduces SoftTarget regularization, a novel method aimed at mitigating overfitting in deep neural networks without reducing model capacity. The approach leverages co-label similarities by blending true labels with an exponential moving average of past model predictions during training. This method is positioned as an alternative to traditional regularization techniques like Dropout and weight decay, claiming to preserve model capacity while simplifying the learning problem. The authors provide experimental results on MNIST, CIFAR-10, and SVHN datasets, demonstrating that SoftTarget regularization can outperform or complement existing methods in certain scenarios. Additionally, the paper offers an intriguing hypothesis about co-label similarities as a marker of overfitting, supported by covariance matrix analysis.
Decision: Reject
While the paper introduces an interesting idea, it falls short in terms of novelty, experimental rigor, and clarity. The primary reasons for rejection are:
1. Limited Novelty: The proposed method is an incremental improvement over existing label softening/smoothing techniques, with unclear causal links to regularization benefits.
2. Weak Experimental Validation: Results on benchmark datasets are far from state-of-the-art, and key baselines (e.g., fixed or gradually softened labels) are missing, making it difficult to assess the practical utility of the method.
Supporting Arguments
1. Novelty and Motivation: The method is conceptually similar to scheduled sampling, SEARN/DAgger, and label smoothing. While the idea of using an online-generated target distribution is interesting, the novelty is incremental and not well-distinguished from prior work. The causal link between co-label similarities and regularization remains speculative and unsupported by theoretical analysis.
   
2. Experimental Rigor: 
   - The results for MNIST, CIFAR-10, and SVHN are underwhelming compared to established baselines. For instance, dropout baselines on MNIST fail to match known benchmarks, raising concerns about the experimental setup.
   - Missing baselines, such as fixed or gradually softened label distributions, make it hard to isolate the benefits of SoftTarget regularization.
   - Data hygiene issues persist, such as reliance on minimum test loss/maximum test accuracy instead of unbiased model selection methods. Established protocols like early stopping on validation sets are not explored.
3. Practical Utility: The method introduces additional hyperparameters (e.g., β, γ, nb, nt), complicating its adoption. The claim of reduced computational complexity is not substantiated, as the method still requires careful tuning and additional steps during training.
Suggestions for Improvement
1. Clarify Pseudocode: The pseudocode is misleading, as it implies reliance on lagged distributions rather than online target generation. This should be revised for accuracy.
2. Include Baselines: Compare SoftTarget regularization against fixed or gradually softened label distributions to better contextualize its performance.
3. Improve Experimental Rigor: Ensure that dropout and other baselines match known benchmarks. Use unbiased model selection methods (e.g., validation-based early stopping) to report results.
4. Theoretical Analysis: Provide a formal mathematical framework to explain the relationship between co-label similarities and regularization benefits.
5. State-of-the-Art Comparisons: Evaluate the method on competitive architectures and datasets (e.g., ImageNet) to demonstrate its practical utility in real-world scenarios.
6. Hyperparameter Sensitivity: Discuss the sensitivity of the method to its hyperparameters and explore ways to reduce their number.
Questions for the Authors
1. How does SoftTarget regularization compare to fixed or gradually softened label distributions in terms of performance and computational cost?
2. Can you provide a theoretical explanation or empirical evidence linking co-label similarities to regularization benefits?
3. Why are the dropout baselines on MNIST underperforming compared to established results? Were these experiments conducted with sufficient tuning and rigor?
4. How does the method scale to larger datasets and more complex architectures, such as ImageNet or transformer models?
In summary, while the idea of leveraging co-label similarities is intriguing, the paper lacks sufficient novelty, experimental rigor, and practical validation to warrant acceptance in its current form. Addressing the above concerns could significantly strengthen the work.