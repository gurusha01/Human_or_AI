Review
Summary of Contributions
This paper proposes a novel multi-view Bayesian non-parametric algorithm for learning multi-sense word embeddings using multilingual corpora. It extends prior work by leveraging multilingual (more than two languages) distributional information to improve word sense disambiguation and embedding quality. The approach is data-efficient, inferring a variable number of senses per word in a data-driven manner, and demonstrates that multilingual training can achieve competitive performance compared to monolingual models trained on significantly larger datasets. The authors provide qualitative and quantitative analyses, including experiments on Word Sense Induction (WSI) and contextual word similarity tasks, and explore the effects of language family distance and crosslingual window size.
Decision: Reject  
Key reasons for rejection:
1. Weak Evaluation: The paper lacks rigorous evaluation on downstream tasks or comparisons to other state-of-the-art methods, relying instead on limited metrics (e.g., ARI) and uninformative visualizations (e.g., t-SNE/PCA plots).
2. Insufficient Comparison with Prior Work: The paper does not adequately benchmark its approach against existing methods, making it difficult to assess its true effectiveness.
Supporting Arguments
1. While the proposed approach is novel in its use of multilingual data and Bayesian non-parametrics, it is not groundbreaking. The idea of leveraging multilingual corpora for word sense disambiguation is well-established, and the paper does not clearly articulate how its contributions significantly advance the state of the art.
2. The evaluation methodology is limited. The reliance on ARI for WSI and SCWS for contextual similarity is insufficient to demonstrate the practical utility of the embeddings. The absence of downstream task evaluations (e.g., machine translation, named entity recognition) undermines the claims of the model's effectiveness.
3. The paper does not provide a comprehensive comparison with prior work. For example, while it mentions the AdaGram model (Bartunov et al., 2016), the comparisons are limited to specific datasets and do not convincingly establish superiority. Furthermore, the lack of comparison with other multilingual embedding methods (e.g., Suster et al., 2016) is a significant oversight.
Suggestions for Improvement
1. Simplify the Model Section: The model description is overly complex and could benefit from clearer explanations and diagrams. For instance, the explanation of the Dirichlet process and variational inference could be streamlined for better accessibility.
2. Expand Evaluation: Include evaluations on downstream NLP tasks to demonstrate the practical utility of the embeddings. Additionally, provide more robust comparisons with prior work across multiple benchmarks.
3. Improve Visualization: Replace t-SNE/PCA plots with more informative quantitative analyses. For example, demonstrate how the embeddings improve performance on specific tasks or provide detailed error analyses.
4. Clarify Novelty: Clearly articulate how the proposed approach differs from and improves upon existing methods, particularly in terms of multilingual training and non-parametric modeling.
Questions for the Authors
1. How does the proposed model compare to other multilingual embedding methods (e.g., Suster et al., 2016) in terms of performance and computational efficiency?
2. Why were downstream tasks omitted from the evaluation? Would the authors consider including such tasks in future work?
3. How sensitive is the model to the choice of languages in the multilingual corpus? Can the authors provide more insights into the trade-offs between language similarity and disambiguation performance?
In summary, while the paper presents an interesting approach to multi-sense embedding learning, it falls short in terms of evaluation rigor and clarity of contributions. Addressing these issues could significantly strengthen the work.