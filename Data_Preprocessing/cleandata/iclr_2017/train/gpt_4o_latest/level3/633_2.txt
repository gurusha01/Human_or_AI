Review of "CoopNets: Cooperative Training of Descriptor and Generator Networks"
This paper introduces CoopNets, a novel cooperative training algorithm that combines a Deep-Energy Model (DEM) and an auxiliary generator network. The key contribution is the interweaving of two probabilistic models—descriptor and generator networks—through a cooperative mechanism that leverages Langevin dynamics for efficient sampling. The authors claim that this approach improves the stability of training and enhances the learning of both models. The idea is innovative and well-motivated, drawing inspiration from GANs but replacing adversarial dynamics with cooperation, which is a refreshing perspective in generative modeling.
Decision: Reject
While the paper presents an interesting and novel idea, it falls short in its experimental validation and methodological rigor. The primary reasons for rejection are:
1. Inadequate Experimental Protocols: The experiments lack proper train/test splits, raising concerns about overfitting. The datasets used are small, and the evaluation metrics are not robust enough to substantiate the claims.
2. Incomplete Baseline Comparisons: The inpainting task does not compare CoopNets against standard models like VAEs, RBMs, or standalone DEMs, making it difficult to assess the true efficacy of the proposed method.
Supporting Arguments:
1. Novelty: The cooperative training paradigm is a compelling contribution, and the idea of using the generator to "jumpstart" Langevin sampling in the descriptor is particularly intriguing. However, the novelty is undermined by the lack of rigorous empirical validation.
2. Experimental Validation: The absence of train/test splits and the use of small datasets (e.g., 10,000 CelebA images) limit the generalizability of the results. The paper does not provide sufficient evidence to demonstrate that CoopNets outperform existing methods in a statistically significant way.
3. Diagnostic Questions: The paper does not analyze critical components, such as the impact of the Langevin MCMC rejection step, the generator's effect on burn-in, or approximation errors in training. These omissions leave gaps in understanding the method's robustness and efficiency.
Additional Feedback for Improvement:
1. Clarity: The paper would benefit from clearer writing. The introduction is overly verbose, and the energy function is not explicitly defined early on, which could confuse readers unfamiliar with the context.
2. Baselines: Add comparisons with standard generative models (e.g., VAEs, RBMs, DEMs) for all tasks, especially the inpainting task, to provide a comprehensive evaluation.
3. Experimental Protocols: Introduce proper train/test splits and use larger, more diverse datasets to validate the method's generalization capabilities. Include quantitative metrics such as FID or IS for synthesis tasks.
4. Diagnostic Experiments: Conduct ablation studies to analyze the impact of key components, such as the Langevin dynamics and the cooperative mechanism, on the final performance.
Questions for the Authors:
1. How does the Langevin MCMC rejection step influence the convergence and stability of CoopNets? Have you tested the method without this step?
2. What is the impact of the generator's initialization on the burn-in period for Langevin sampling? Does it consistently reduce sampling time across datasets?
3. Can you provide a detailed analysis of approximation errors in training, particularly for the descriptor network?
In summary, while CoopNets is an innovative concept, the paper requires significant improvements in experimental rigor, baseline comparisons, and clarity to be suitable for acceptance.