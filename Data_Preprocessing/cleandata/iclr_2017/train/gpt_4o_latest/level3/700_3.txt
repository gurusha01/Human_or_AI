Review of the Paper: "Marginal Deep Architectures (MDA)"
Summary of Contributions:
This paper introduces a novel deep learning framework called Marginal Deep Architectures (MDA), which leverages Marginal Fisher Analysis (MFA) for layer-wise initialization of deep neural networks. The authors aim to address the challenges of training deep models on small- and medium-scale datasets by combining the advantages of feature learning models with deep architectures. The proposed method incorporates techniques such as backpropagation, dropout, and denoising to fine-tune the network. Experimental results on seven datasets demonstrate that MDA outperforms shallow feature learning models and achieves competitive results compared to state-of-the-art deep learning models in small- and medium-scale applications. The paper also evaluates the impact of architectural choices, such as the number of hidden layers and node configurations, on classification performance.
Decision: Reject
Key reasons for rejection:
1. Insufficient Experimental Validation: The paper evaluates MDA on small-scale datasets but lacks experiments on widely recognized benchmarks like CIFAR-10 without preprocessing (e.g., grayscale conversion). This limits the generalizability of the claims.
2. Missing Baselines: The absence of comparisons with standard deep learning baselines, such as discriminatively trained convolutional neural networks (CNNs), weakens the empirical validation.
3. Limited Novelty: While the use of MFA for initialization is novel, similar approaches (e.g., SPCANet) have been explored in prior work, raising concerns about the incremental nature of the contribution.
Supporting Arguments:
1. Experimental Validation: The datasets used are small in scale, and while the results are promising, they do not convincingly demonstrate the superiority of MDA over standard methods on larger, more challenging datasets. The results on CIFAR-10, for example, are suboptimal due to preprocessing, which undermines the claim of universality.
2. Baseline Comparisons: The lack of comparisons with widely used baselines, such as CNNs trained with random initialization or other modern initialization techniques, makes it difficult to assess the practical utility of MDA.
3. Computational Cost: The computational complexity of calculating the association matrix in Equation 4 is not clearly discussed, leaving concerns about scalability to larger datasets unresolved.
Suggestions for Improvement:
1. Stronger Baselines: Include comparisons with standard deep learning models, such as CNNs and ResNets, on benchmark datasets like CIFAR-10 and ImageNet. This would provide a more robust evaluation of MDA's effectiveness.
2. Scalability Analysis: Provide a detailed analysis of the computational cost of MFA-based initialization, particularly for large-scale datasets. This would clarify the practicality of the approach.
3. Ablation Studies: Conduct experiments to isolate the contribution of MFA-based initialization compared to random initialization or other initialization techniques. This would strengthen the argument for the novelty and utility of the proposed method.
4. Broader Dataset Selection: Test MDA on larger and more diverse datasets to demonstrate its applicability beyond small- and medium-scale applications.
Questions for the Authors:
1. How does the computational cost of calculating the association matrix in Equation 4 scale with the size of the dataset?
2. Why were standard baselines like CNNs or ResNets not included in the experimental comparisons?
3. Can the authors provide evidence that MFA-based initialization consistently outperforms random initialization across a broader range of tasks and datasets?
4. How does MDA handle overfitting when applied to larger datasets, given its focus on small- and medium-scale applications?
While the paper presents an interesting approach, addressing the above concerns is necessary to strengthen its contributions and demonstrate its relevance to the broader deep learning community.