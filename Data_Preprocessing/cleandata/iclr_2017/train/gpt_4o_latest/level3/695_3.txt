Review of the Paper
The paper introduces NoiseOut, a novel neuron pruning method based on the correlation of neuron activations, with an additional mechanism to encourage higher correlation through noise injection. The authors claim that NoiseOut achieves significant parameter reduction in dense layers of neural networks while maintaining accuracy. The proposed method is positioned as a computationally efficient alternative to traditional weight-based pruning approaches. The paper also provides theoretical insights into the effect of noise injection on neuron correlation and demonstrates the method's efficacy through experiments on MNIST and SVHN datasets.
Decision: Reject
While the paper proposes an interesting idea with potential, it suffers from significant weaknesses in experimental rigor and theoretical justification. The lack of clarity and inconsistencies in the experimental results, coupled with theoretical assumptions that are not well-aligned with practical conditions, undermine the credibility of the claims.
Supporting Arguments for Decision
1. Experimental Weaknesses: The experimental results are not clearly presented. For instance, Figure 5 and Table 2 show inconsistencies in accuracy degradation and pruning rates, which raises concerns about the reproducibility of the results. Additionally, the paper does not explicitly report accuracy degradation thresholds or provide sufficient statistical analysis to support its claims of "no accuracy loss." This lack of transparency makes it difficult to assess the true efficacy of NoiseOut.
2. Theoretical Limitations: The theoretical proofs rely on idealized assumptions, such as perfect correlation between neurons, which are unlikely to hold in real-world scenarios. Furthermore, the role of noise injection in achieving higher correlation is not convincingly justified. While the authors attempt to formalize the effect of noise outputs, the derivations are overly simplistic and do not account for the complexities of modern neural networks.
3. Positioning in Literature: Although the paper references relevant prior work, it does not sufficiently compare NoiseOut to state-of-the-art pruning methods, such as those based on sparsity-inducing regularization or structured pruning. This omission weakens the argument for the novelty and practical relevance of the proposed approach.
Suggestions for Improvement
1. Clarify Experimental Results: Ensure consistency between figures and tables, and explicitly report accuracy degradation thresholds. Include additional metrics, such as FLOPs reduction and memory savings, to provide a more comprehensive evaluation of NoiseOut's benefits.
2. Strengthen Theoretical Justification: Address the gap between the theoretical assumptions and practical conditions. For example, provide empirical evidence to validate the claim that noise injection consistently leads to higher neuron correlation across diverse architectures and datasets.
3. Broaden Comparisons: Compare NoiseOut to a wider range of baseline pruning methods, including recent advancements in structured pruning and lottery ticket hypothesis approaches. This will help position the method more effectively within the existing literature.
4. Address Limitations: Acknowledge the limitations of NoiseOut, such as its reliance on specific noise distributions or its applicability to dense layers only. This transparency will enhance the paper's credibility.
Questions for the Authors
1. Can you clarify the inconsistencies between Figure 5 and Table 2? How were the accuracy thresholds determined for pruning?
2. How does NoiseOut perform on larger-scale datasets and architectures, such as ImageNet or ResNet models? Are there scalability concerns?
3. Have you tested the sensitivity of the pruning process to different noise distributions? If so, how robust is the method to variations in noise parameters?
In summary, while the paper presents an intriguing idea, it requires significant improvements in experimental rigor, theoretical grounding, and positioning within the literature to be considered for acceptance.