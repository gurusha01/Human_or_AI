Review of the Paper
Summary of Contributions
This paper addresses the challenging problem of micromanagement in real-time strategy (RTS) games, specifically focusing on StarCraft scenarios. The authors propose a novel zero-order optimization algorithm that combines structured exploration with deep reinforcement learning (RL). The paper introduces several micromanagement tasks as benchmarks for RL, highlighting the complexity of these scenarios due to large state-action spaces, delayed rewards, and the need for coordinated multi-agent control. The proposed algorithm demonstrates clear improvements over baseline methods like Q-learning and REINFORCE, achieving higher win rates and more robust training. Additionally, the authors provide insights into the learned strategies and discuss the limitations of existing RL methods in this domain.
Decision: Reject
While the paper makes a significant contribution to RL for RTS games, it has several critical shortcomings that prevent acceptance at this stage. The primary reasons for rejection are: (1) insufficient justification for some methodological choices, and (2) limited generalization of the proposed algorithm beyond the StarCraft domain.
Supporting Arguments
1. Strengths:
   - The paper tackles a well-motivated and complex problem, with clear assumptions and a focus on a challenging subdomain of RTS games.
   - The proposed zero-order optimization algorithm is novel and demonstrates superior performance compared to Q-learning and REINFORCE in the presented scenarios.
   - The experiments are thorough, with detailed comparisons against baseline heuristics and RL algorithms, as well as evaluations of generalization to new scenarios.
2. Weaknesses:
   - Some methodological choices lack sufficient justification. For example, the decision to use only the sign of certain terms in the gradient update and the neglect of the `argmax` operation are not well-explained or experimentally validated.
   - The algorithm's applicability is limited to StarCraft micromanagement scenarios. While the authors acknowledge this limitation, they do not provide any experiments or evidence to suggest that the method would generalize to other domains, such as Atari games or other multi-agent environments.
   - The presentation of the paper is occasionally unclear. For instance, the role of the `w` vector in the optimization process is not adequately contextualized, leading to confusion for readers unfamiliar with zero-order optimization techniques.
   - Minor issues, such as unexplained design decisions in the network structure and unclear claims about the importance of Adagrad over RMSprop, detract from the overall clarity and rigor of the work.
Additional Feedback for Improvement
- Justification of Methodology: Provide a more detailed explanation and experimental validation for key design choices, such as the use of the sign operation and the exclusion of `argmax`. These decisions should be supported by ablation studies or theoretical insights.
- Generalization: Extend the evaluation to additional domains beyond StarCraft micromanagement. This could include simpler environments like Atari games or other multi-agent benchmarks to demonstrate the broader applicability of the algorithm.
- Presentation: Improve the clarity of the paper by introducing key concepts (e.g., the `w` vector) with sufficient context and detail. Consider adding a diagram or flowchart to illustrate the zero-order optimization process.
- Typographical Errors: Correct minor issues, such as replacing "perturbated" with "perturbed."
Questions for the Authors
1. How does the proposed algorithm handle scenarios with more diverse unit types or more complex action spaces? Would it scale effectively to such cases?
2. Can the authors provide more evidence or theoretical justification for the use of the sign operation in the gradient update? How does this choice impact convergence and stability?
3. Have the authors considered testing the algorithm in other domains, such as Atari or cooperative multi-agent environments? If so, what were the results?
In conclusion, while the paper addresses an important problem and proposes a novel solution, it requires significant improvements in methodology justification, generalization, and presentation to meet the standards of the conference.