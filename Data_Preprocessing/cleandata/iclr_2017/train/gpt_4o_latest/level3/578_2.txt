The paper presents an empirical study on the invariance and equivariance properties of Convolutional Neural Networks (CNNs) under various input transformations and proposes a novel loss function to improve equivariance. The authors evaluate 70 CNNs trained with different data augmentation techniques across two datasets and provide insights into how transformations impact learned representations. They also introduce a joint loss function that balances classification accuracy and equivariance, showing moderate improvements in performance.
Decision: Reject
The primary reasons for rejection are the limited scope of the empirical study and the lack of novelty in the proposed loss function. While the paper addresses an important topic, its contributions are fragmented, and the methodology does not provide sufficient generalizable insights.
Supporting Arguments:
1. Limited Scope of Empirical Study: The study focuses on a single network architecture (AlexNet) and a single layer (fc7), which restricts the generalizability of the findings. Broader evaluations across architectures and layers would have strengthened the conclusions.
2. Dataset Limitations: The reliance on the RVL-CDIP dataset, a scanned text dataset, is a drawback. Although the results on ImageNet are more promising, the findings are not robustly validated across diverse datasets.
3. Proposed Loss Function: The loss function, while addressing equivariance, lacks novelty and is reminiscent of prior work. Its discussion is brief, and the experimental results show only marginal improvements, which do not justify its significance.
4. Fragmented Contributions: The paper attempts to tackle two distinct problemsâ€”empirical analysis of invariance/equivariance and the introduction of a new loss function. Neither part is explored in sufficient depth, resulting in a lack of cohesion.
Suggestions for Improvement:
1. Expand the Empirical Study: Evaluate invariance and equivariance across multiple architectures (e.g., ResNet, VGG) and layers to provide more comprehensive insights. Additionally, consider testing on diverse datasets to improve generalizability.
2. Deeper Analysis of Transformations: While performance degradation with transformations is documented, the analysis could benefit from alternative evaluation methods, such as visualization techniques or theoretical metrics, to better understand the learned representations.
3. Strengthen Novelty of the Loss Function: Clearly differentiate the proposed loss function from prior work and provide a more detailed analysis of its impact. Consider ablation studies to isolate the effects of each term in the loss function.
4. Focus the Paper: A more focused treatment of either the empirical study or the proposed loss function would result in a stronger contribution. For example, the empirical study could be expanded into a standalone paper with deeper insights into invariance/equivariance.
Questions for the Authors:
1. Why was the fc7 layer chosen for analysis, and how do the findings generalize to other layers of the network?
2. Can the proposed loss function be applied to architectures other than AlexNet? If so, what are the results?
3. How does the proposed loss function compare to existing methods for improving equivariance in terms of computational cost and performance trade-offs?
4. Could the authors provide more detailed results on the impact of the loss function on unseen transformations?
In summary, while the paper addresses an important and timely topic, its contributions are limited in scope and novelty, and the fragmented structure weakens its impact. A more focused and rigorous approach would significantly enhance its value.