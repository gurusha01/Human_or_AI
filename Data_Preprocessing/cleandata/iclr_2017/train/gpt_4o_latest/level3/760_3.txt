Review of the Paper
Summary
This paper introduces the Hierarchical Compositional Network (HCN), a novel generative model for binary images that aims to discover and disentangle compositional building blocks without supervision. The model is structured hierarchically, with binary features composed via logical OR operations and pooling layers to introduce flexibility. A key contribution is the use of max-product message passing (MPMP) for inference and learning, which avoids the need for expectation-maximization (EM) and enables tasks like classification and inpainting by clamping variables. The authors demonstrate that HCN can learn reusable, interpretable features and show its functional equivalence to a CNN with binary weights for classification tasks. Experiments on synthetic datasets and MNIST illustrate the model's strengths and limitations.
Decision: Reject
The decision to reject is based on two key reasons:
1. Limited Scope and Applicability: The model is restricted to binary image patterns and has only been tested on synthetic datasets and MNIST. Its poor performance on MNIST, compared to CNNs, raises concerns about its scalability and utility for real-world, natural image data.
2. Unclear Semantic Interpretability: While the goal is to extract semantically meaningful features, the learned features often lack clear semantic meaning, potentially corresponding to sub-units or non-semantic elements. This undermines the claim of discovering meaningful building blocks.
Supporting Arguments
1. Performance Limitations: The model performs well on synthetic compositional data but struggles with MNIST, achieving worse test errors than CNNs unless synthetic corruption is added. This suggests that the model is not robust to real-world data variations, limiting its practical utility.
2. Feature Interpretability: The extracted features, while reusable, do not consistently align with semantically meaningful components, which is a central claim of the paper. Without stronger evidence or examples of semantic compositionality, this weakens the paper's contribution.
3. Binary Image Restriction: The focus on binary images significantly narrows the scope of the model. Extending it to grayscale or natural images would make it more relevant to broader applications.
Suggestions for Improvement
1. Expand to Natural Images: Extending the model to handle grayscale or natural images and testing on more diverse datasets would significantly enhance its applicability.
2. Improve Feature Semantics: Provide clearer evidence or metrics to demonstrate that the learned features are semantically meaningful. For example, visualizations or comparisons with human-interpretable components could strengthen the claims.
3. Address MNIST Performance: Investigate why the model underperforms on MNIST and explore architectural or training modifications to improve its robustness.
4. Scalability: The memory requirements for training (e.g., 150GB for MNIST) are prohibitive. Optimizing the implementation or introducing an efficient approximation could make the model more practical.
Questions for the Authors
1. How do you define and measure "semantically meaningful" features in the context of your model? Can you provide quantitative or qualitative evidence for this?
2. What are the primary reasons for the model's poor performance on MNIST compared to CNNs? Have you considered alternative ways to improve its generalization to real-world data?
3. Could the model be extended to grayscale or natural images? If so, what challenges would need to be addressed?
4. How does the computational complexity of MPMP compare to other inference methods, and what are the trade-offs in terms of scalability and accuracy?
While the paper presents an interesting approach to hierarchical compositionality, its limitations in scope, performance, and feature interpretability need to be addressed to make it a stronger contribution to the field.