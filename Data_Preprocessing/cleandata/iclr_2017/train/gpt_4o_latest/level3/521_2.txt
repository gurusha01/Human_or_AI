Review
This paper introduces a novel Fast Chirplet Transform (FCT) for bioacoustic signal processing and demonstrates its utility in pretraining Convolutional Neural Networks (CNNs) for tasks such as bird and vowel classification. The authors propose FCT as a trade-off between scattering transforms and raw audio-based CNNs, highlighting its computational efficiency and ability to enhance CNN performance. The results show significant improvements in training speed (28% faster for bird classification and 26% for vowels) and classification accuracy (+7.8% MAP for birds and +2.3% accuracy for vowels) when using FCT-pretrained CNNs compared to raw audio or Mel spectrograms.
Decision: Accept
The paper makes a strong case for acceptance due to its innovative contribution of FCT, which bridges the gap between scattering transforms and raw audio-based CNNs. The demonstrated improvements in computational efficiency and classification performance are compelling and relevant to the field of bioacoustics and deep learning.
Supporting Arguments
1. Novelty and Contribution: The introduction of FCT as a computationally efficient representation for bioacoustic signals is a significant contribution. The paper provides a clear motivation for using Chirplets, supported by neurophysiological and acoustic evidence, and demonstrates their practical implementation.
   
2. Empirical Validation: The numerical results are robust, with experiments conducted on diverse datasets (birds, whales, and speech vowels). The reported improvements in training speed and accuracy substantiate the claims made by the authors.
3. Practical Relevance: The proposed FCT has practical implications for bioacoustic monitoring and species classification, especially in scenarios with limited data and low signal-to-noise ratios. The availability of the source code further enhances the reproducibility and utility of the work.
Suggestions for Improvement
1. Clarity in Writing: The paper's writing could be improved for better readability. Some sections, particularly the mathematical descriptions, are dense and may be challenging for readers unfamiliar with the domain. Simplifying or restructuring these sections would enhance accessibility.
2. Algorithm Relevance: The algorithms in Section 3.1 appear tangential to the main contributions. Removing or condensing this section would improve focus. Additionally, providing more detailed explanations in the appendix for the algorithms could help readers better understand their role.
3. Connection to Scattering Transforms: The relationship between FCT and scattering transforms is not sufficiently clear. A more detailed discussion of how FCT builds upon or diverges from scattering transforms would strengthen the theoretical grounding.
4. Broader Evaluation: While the results are promising, the evaluation is limited to specific datasets. Expanding the experiments to include more diverse datasets or tasks would provide stronger evidence of FCT's generalizability.
Questions for the Authors
1. Could you elaborate on the theoretical connection between FCT and scattering transforms? How does FCT address the limitations of scattering transforms?
2. The results focus on specific datasets. How well does FCT generalize to other domains, such as music or industrial sound classification?
3. Could you provide more insights into the choice of hyperparameters for FCT and their impact on performance?
Overall, this paper presents a valuable contribution to the field and is recommended for acceptance, provided the authors address the minor issues outlined above.