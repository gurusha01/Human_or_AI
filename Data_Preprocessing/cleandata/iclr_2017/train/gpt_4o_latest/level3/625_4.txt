Review of the Paper
Summary of Contributions
This paper introduces a hierarchical deep reinforcement learning (DRL) architecture designed to address the challenge of zero-shot task generalization in reinforcement learning. The proposed method employs a meta controller and a subtask controller, enabling the agent to execute sequences of high-level instructions and generalize to unseen and longer instruction sequences. Key innovations include the use of analogy-making regularization to improve generalization and a differentiable temporal abstraction mechanism to enhance stability under delayed rewards. The approach is evaluated in both a 2D grid world and a 3D visual environment, demonstrating the ability to generalize to unseen tasks and longer instruction sequences. The paper also highlights the advantages of the hierarchical architecture over a flat controller and provides qualitative insights into the learned policies.
Decision: Reject
While the paper demonstrates technical novelty and provides promising results, it suffers from several critical issues that limit its contribution to the research community. The primary reasons for rejection are: (1) insufficient motivation for using deep reinforcement learning in a simple 2D maze domain where simpler models could suffice, and (2) a lack of engagement with existing literature on planning with skills and hierarchical reinforcement learning, which undermines the justification for the problem's novelty.
Supporting Arguments for Rejection
1. Motivation and Problem Framing: The choice of a simple 2D maze domain for demonstrating the architecture is not well-justified. The problem setting, as described, resembles supervised learning more than reinforcement learning due to the use of pre-trained skills and minimal delayed rewards. This raises questions about whether the proposed approach is addressing a meaningful RL challenge.
   
2. Literature Engagement: The paper does not sufficiently position its contributions within the context of existing work on hierarchical reinforcement learning and planning with skills. For example, prior work on parameterized options and portable skills is only briefly mentioned, and the novelty of the proposed approach is not clearly articulated in comparison to these methods.
3. Generalization Claims: While the paper claims generalization to unseen tasks, the scope of generalization appears limited to breaking sentences into subtasks. The experiments do not convincingly demonstrate the ability to generalize to fundamentally new task structures or domains.
4. Readability and Organization: The paper is difficult to follow due to excessive technical details in the main text, poor organization, and a lack of self-contained explanations. Key implementation details, such as architectural descriptions, should be moved to an appendix to improve readability.
Suggestions for Improvement
1. Clarify Motivation: Provide a stronger justification for using deep reinforcement learning in the chosen domain. Discuss why simpler models are insufficient and how the proposed approach addresses unique challenges.
2. Engage with Related Work: Expand the discussion of related work and clearly articulate how the proposed method advances the state of the art. Highlight specific limitations of prior approaches that the paper addresses.
3. Broaden Generalization Tests: Design experiments that test generalization to more diverse and complex task structures, such as tasks with conditional or loop instructions, to better demonstrate the scalability of the approach.
4. Improve Readability: Reorganize the paper to focus on high-level ideas and results in the main text, moving technical details to an appendix. Introduce notation and methods in a self-contained manner to make the paper accessible to a broader audience.
Questions for the Authors
1. Why was a simple 2D maze domain chosen for evaluating the proposed architecture? Could the approach be applied to more complex domains where hierarchical planning is essential?
2. How does the proposed method compare quantitatively and qualitatively to existing hierarchical RL methods, such as option-critic or programmable HAMs?
3. Can the analogy-making regularization be extended to handle more complex relationships, such as conditional or loop-based instructions?
4. How does the proposed architecture perform when the pre-trained subtask controller is replaced with a less accurate model? Would this affect the generalization capabilities?
Addressing these issues in future iterations could significantly enhance the paper's impact and contribution to the field.