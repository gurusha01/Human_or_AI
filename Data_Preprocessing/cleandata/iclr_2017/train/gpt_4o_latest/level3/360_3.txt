Review of the Paper
Summary of Contributions
This paper introduces a novel framework for semi-supervised reinforcement learning (SSRL), where the reward function is only available in a subset of "labeled" MDPs, and the agent must generalize its behavior to "unlabeled" MDPs without direct reward supervision. The authors propose the Semi-Supervised Skill Generalization (S3G) algorithm, which combines reinforcement learning (RL) with inverse reinforcement learning (IRL) principles. The method uses entropy-regularized policy optimization (REINFORCE) to learn a reward model from labeled MDPs and iteratively refines the policy in unlabeled MDPs using inferred rewards. The paper demonstrates the effectiveness of S3G on continuous control tasks, showing improved generalization compared to standard RL and supervised reward regression baselines. The work is well-written, conceptually clear, and provides a simple yet effective solution to a challenging problem.
Decision: Accept
The paper makes a meaningful contribution to the field of reinforcement learning by addressing the underexplored problem of semi-supervised RL. The proposed approach is novel, scientifically rigorous, and empirically validated. While the idea of leveraging unlabeled MDPs may seem intuitive, this is the first work to formalize and test such a framework with deep RL methods. The experimental results, though limited to continuous domains, are convincing and highlight the potential of the proposed method. The paper's clarity and the practical implications for real-world RL applications further strengthen its case for acceptance.
Supporting Arguments
1. Problem Definition and Motivation:  
   The paper addresses a critical gap in RL research by tackling the challenge of learning in environments where reward supervision is sparse or unavailable. The problem is well-motivated, particularly for real-world applications like robotics, where labeled data is expensive to obtain. The connection to semi-supervised learning in supervised ML is insightful and provides a strong conceptual foundation.
2. Novelty and Technical Soundness:  
   The S3G algorithm is a novel combination of RL and IRL techniques, leveraging entropy regularization and reward inference to improve policy generalization. The use of prior experience in labeled MDPs as pseudo-demonstrations for reward learning is innovative and avoids reliance on human experts. The theoretical underpinnings, including the use of maximum entropy RL and guided cost learning, are well-grounded in the literature.
3. Empirical Validation:  
   The experiments on continuous control tasks (e.g., obstacle navigation, robotic reacher tasks, and cheetah jumping) demonstrate the effectiveness of S3G in improving policy generalization. The comparisons to baselines (standard RL and reward regression) and the oracle provide a comprehensive evaluation. The results are compelling, with S3G outperforming baselines in most scenarios, and even surpassing the oracle in some cases due to better reward shaping.
Suggestions for Improvement
1. Inclusion of Discrete Domains:  
   While the experiments on continuous control tasks are interesting, the paper would benefit from including results on discrete action spaces (e.g., grid-worlds or Atari games). This would strengthen the generality of the proposed method.
2. Ablation Studies:  
   The paper could include ablation studies to isolate the contributions of individual components, such as the entropy regularization term, the reward model, and the iterative policy refinement process.
3. Scalability and Sample Efficiency:  
   While the authors mention the sample efficiency of guided policy search, more detailed analysis of the computational cost and scalability to larger, more complex environments would be valuable.
4. Real-World Validation:  
   Although the method is motivated by real-world applications, the experiments are limited to simulated environments. A demonstration on a physical robotic system would significantly enhance the paper's impact.
Questions for the Authors
1. How does the method handle scenarios where the distribution of labeled and unlabeled MDPs differs significantly (i.e., domain shift)?
2. Could the proposed framework be extended to handle multi-task or hierarchical RL settings?
3. What are the limitations of the reward model in terms of generalization to highly diverse or sparse unlabeled MDPs?
In conclusion, this paper makes a significant contribution to the field of RL by addressing a practical and challenging problem with a novel and effective approach. While there is room for improvement in terms of experimental diversity and real-world validation, the work is strong enough to merit acceptance.