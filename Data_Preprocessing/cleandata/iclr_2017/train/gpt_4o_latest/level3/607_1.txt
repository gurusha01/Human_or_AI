Review of the Paper
Summary of Contributions
The authors propose a novel hierarchical attention model for video captioning, incorporating three main components: a Temporal Modeler (TEM), a Hierarchical Attention/Memory mechanism (HAM), and a Decoder. The HAM is designed to utilize memories of past attention to better model the temporal and hierarchical structure of video data, drawing inspiration from the "central executive system" in human cognition. The proposed model is evaluated on the MSVD and Charades datasets, achieving state-of-the-art results on several metrics without relying on external features like optical flow or fine-tuned CNNs. The authors also conduct ablation studies to analyze the contributions of individual components, demonstrating the importance of HAM in the overall architecture.
Decision: Reject
While the paper introduces an interesting hierarchical attention mechanism and achieves competitive results, it lacks sufficient novelty and clarity in its contributions to warrant acceptance at ICLR. The following reasons support this decision:
1. Overstated Contribution of HAM: The use of LSTMs within the HAM is referred to as a "hierarchical memory mechanism," but this terminology appears to overstate the novelty of the approach. Memory-based attention mechanisms are well-established in the literature, and the paper does not provide sufficient evidence to distinguish its HAM component as a significant advancement.
   
2. Weak Case for TEM: The ablation study reveals that the TEM component contributes minimally to the overall performance, raising questions about its necessity and effectiveness. This weakens the claim of the model's hierarchical design.
3. Limited Benchmarking: The quantitative evaluation relies on a narrow set of "fair" comparators, which limits the broader impact of the results. While the authors achieve state-of-the-art performance, the lack of external features like optical flow makes the comparisons less compelling. Including experiments with such features would strengthen the case for the model's generalizability.
Supporting Arguments
The ablation study is a positive aspect of the paper, as it provides insight into the contributions of individual components. However, the results suggest that the TEM component is not critical, and the HAM's novelty is questionable. Furthermore, the paper's reliance on a limited set of baselines undermines the significance of the reported improvements. While the authors claim that their model can generalize to other sequence learning tasks, no evidence is provided to support this assertion.
Suggestions for Improvement
1. Clarify Novelty: Clearly articulate how the HAM differs from existing memory-based attention mechanisms. Providing theoretical or empirical evidence to support its unique contributions would strengthen the paper.
   
2. Strengthen TEM: Either demonstrate the necessity of the TEM component through additional experiments or consider simplifying the model by removing it.
3. Expand Benchmarks: Include comparisons with models that use external features (e.g., optical flow, fine-tuned CNNs) to provide a more comprehensive evaluation of the proposed approach.
4. Generalization: Provide experiments or theoretical analysis to demonstrate the applicability of the model to other sequence learning tasks, as claimed in the conclusion.
Questions for the Authors
1. How does the proposed HAM mechanism differ fundamentally from existing memory-based attention models in the literature? Can you provide specific examples or references to highlight these differences?
2. Given the weak contribution of the TEM component in the ablation study, what is its intended role in the architecture? Could the model perform equally well without it?
3. Why were external features like optical flow excluded from the experiments? Would their inclusion improve the model's performance, and how would the HAM handle such additional inputs?
In summary, while the paper presents an interesting approach to video captioning, its contributions are not sufficiently novel or well-supported to justify publication at ICLR. Addressing the above concerns could significantly improve the quality and impact of the work.