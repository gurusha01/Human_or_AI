Review of the Paper
Summary
The paper investigates the approximation capabilities of deep versus shallow neural networks, particularly focusing on strongly convex differentiable functions. It demonstrates that deep neural networks with ReLU and threshold units require exponentially fewer neurons than shallow networks to approximate such functions within a given error margin. The authors provide both upper and lower bounds on the network size, highlighting the exponential advantage of depth. The results are extended to certain classes of multivariate functions, and the theoretical findings are supported by rigorous mathematical proofs.
Decision: Accept
The paper makes a significant theoretical contribution to understanding the power of depth in neural networks, a topic of high relevance to the machine learning community. The results are novel, well-motivated, and scientifically rigorous. However, the scope of the main result is somewhat limited to strongly convex univariate functions, which restricts its general applicability. Despite this limitation, the paper's insights into the exponential efficiency of deep networks over shallow ones are valuable and warrant acceptance.
Supporting Arguments
1. Novelty and Contribution: The paper builds on prior work (e.g., Telgarsky, 2016; Montufar et al., 2014) by providing tight bounds for function approximation using deep networks. It extends the understanding of the exponential gap between deep and shallow networks, particularly for strongly convex functions.
2. Scientific Rigor: The theoretical results are derived with clear mathematical rigor, and the proofs are detailed and logically sound. The inclusion of both upper and lower bounds strengthens the paper's claims.
3. Relevance: Understanding the efficiency of deep networks is a critical question in modern AI research, and the paper addresses this with a focus on function approximation, a foundational topic in neural network theory.
Suggestions for Improvement
1. Clarification of "L": The paper should clarify the role of "L" (network depth) in the main text and emphasize the main result more prominently in the introduction and conclusion.
2. Comparison with Related Work: The paper should include a more explicit comparison with Montufar et al. (NIPS 2014), which also discusses exponential gaps between deep and shallow ReLU networks. This would better contextualize the contribution within the existing literature.
3. Minor Corrections:
   - Replace "i" with "x" in Lemma 3 for consistency.
   - Add "(x)" to "\(\tilde{f}\)" in Theorem 4 for clarity.
   - Clarify whether the lower bound in Theorem 11 always increases with \(L\).
   - Confirm if Theorem 11 assumes \(\mathbf{x} \in [0,1]^d\).
4. General Applicability: While the results for strongly convex univariate functions are compelling, extending the analysis to broader classes of functions (e.g., non-convex or piecewise smooth functions) would enhance the paper's impact.
Questions for the Authors
1. Can the results for strongly convex univariate functions be generalized to multivariate functions beyond the specific cases discussed in the paper?
2. How do the theoretical bounds compare empirically for real-world datasets and architectures? Are there practical scenarios where the exponential advantage of depth is observed?
3. Could the assumptions on smoothness and strong convexity be relaxed to include a wider class of functions?
In conclusion, the paper provides a valuable theoretical contribution to understanding the efficiency of deep neural networks. Addressing the suggested improvements would further strengthen the paper's impact and clarity.