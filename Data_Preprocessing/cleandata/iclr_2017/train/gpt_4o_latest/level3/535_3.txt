Review
Summary
This paper presents a novel video captioning model, ASTAR, which combines a C3D encoder and an LSTM decoder with an innovative attention mechanism. The attention mechanism operates at two levels: spatiotemporal localization within feature maps and dynamic abstraction across CNN layers. This dual-level attention enables the model to adaptively focus on both spatial-temporal regions and varying levels of feature abstraction, addressing limitations in prior work that relied on fixed-layer feature representations. The model is evaluated on three challenging datasets (YouTube2Text, M-VAD, MSR-VTT) and achieves state-of-the-art performance on YouTube2Text. The authors also compare soft and hard attention mechanisms, concluding that soft attention is more effective for this task.
Decision: Accept
The paper makes a significant contribution to the field of video captioning by introducing a robust and well-motivated attention mechanism that improves performance over prior models. The experimental results are rigorous and support the claims made in the paper. The novelty of the approach and its demonstrated effectiveness justify acceptance.
Supporting Arguments
1. Well-Motivated Approach: The paper is well-grounded in the literature, addressing key limitations of prior video captioning models, such as the inability to leverage multi-layer feature abstractions and the loss of fine-grained spatiotemporal details. The use of C3D features and dual-level attention is a natural and innovative extension of existing encoder-decoder frameworks.
   
2. Experimental Rigor: The authors evaluate their model on multiple datasets using standard metrics (BLEU, METEOR, CIDEr), providing a comprehensive comparison with prior work. The results convincingly demonstrate the superiority of the proposed approach, particularly on the YouTube2Text dataset.
3. Novelty and Practicality: The proposed attention mechanism is both novel and practical. By dynamically weighting CNN layers and spatiotemporal regions, the model effectively captures both coarse and fine-grained information, which is critical for generating semantically rich captions.
Suggestions for Improvement
1. Comparison with Hypercolumns: While the paper introduces a compelling attention mechanism, it would benefit from a comparison with hypercolumn-based approaches, which also aggregate multi-layer CNN features. This could provide additional insights into the advantages of the proposed method.
2. Generalization Across Datasets: Although the model achieves state-of-the-art results on YouTube2Text, its performance on other datasets (e.g., M-VAD, MSR-VTT) is not explicitly detailed in the abstract or conclusion. A more thorough discussion of its generalization capabilities would strengthen the paper.
3. Ablation Studies: While the paper compares soft and hard attention mechanisms, additional ablation studies isolating the contributions of spatiotemporal attention and abstraction-level attention would provide a clearer understanding of their individual impacts.
Questions for the Authors
1. How does the model perform on longer and more complex video sequences, where the spatiotemporal dynamics are more intricate?  
2. Have you considered the computational overhead introduced by the dual-level attention mechanism? How does the model's runtime compare to simpler attention-based approaches?  
3. Can the proposed attention mechanism be extended to other tasks, such as video question answering or summarization?  
In summary, this paper introduces a novel and effective approach to video captioning, supported by rigorous experimentation and well-motivated design choices. With minor improvements and clarifications, it has the potential to make a significant impact in the field.