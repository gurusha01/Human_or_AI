Review of "PixelCNN++: Improving the PixelCNN Architecture"
The paper proposes several extensions to the PixelCNN architecture, including the use of a discretized logistic mixture likelihood, downsampling for multi-resolution modeling, dropout for regularization, and specific skip connections to recover lost information. These modifications aim to simplify the architecture, improve training efficiency, and enhance performance. The authors demonstrate state-of-the-art results on the CIFAR-10 dataset, outperforming the previous best model, PixelRNN, in terms of log-likelihood. The paper also provides open-source code, which could benefit the broader research community.
Decision: Accept
The paper makes a significant contribution to the field of autoregressive generative models by addressing key limitations of the original PixelCNN. The proposed modifications are well-motivated, and the empirical results convincingly demonstrate their effectiveness. However, there are areas where the paper could be improved, particularly in its discussion of related work and the substantiation of certain claims.
Supporting Arguments:
1. Strengths:
   - The use of a discretized logistic mixture likelihood is a well-justified innovation, addressing the inefficiencies of the 256-way softmax in the original PixelCNN. This change not only improves training speed but also reduces memory requirements.
   - The introduction of downsampling and skip connections is a thoughtful design choice, enabling the model to capture both local and global dependencies efficiently.
   - Dropout regularization is effectively employed to mitigate overfitting, as demonstrated in the ablation studies.
   - The empirical results are robust, with the model achieving state-of-the-art log-likelihood on CIFAR-10, a widely used benchmark.
2. Weaknesses:
   - The paper does not sufficiently engage with prior work on mixture models and downsampling techniques, which have been explored in earlier research. This omission weakens the contextualization of the contributions.
   - While the integration of density modeling for discrete integers is a promising idea, the authors fail to provide strong evidence for their claims about the limitations of continuous distributions in earlier models.
   - The dataset size (60,000 images) is relatively small for high-dimensional generative modeling. The authors could have strengthened their claims by experimenting with larger datasets, such as "80 million tiny images."
Additional Feedback:
1. The authors should include references to prior work on mixture models and downsampling to better situate their contributions within the existing literature.
2. A performance comparison of PixelCNN++ without the proposed modifications (e.g., logistic mixture likelihood, downsampling) would provide a clearer understanding of the individual contributions of each change.
3. Reporting test-time speed comparisons between PixelCNN++, PixelCNN, and PixelRNN would highlight the efficiency gains more explicitly.
4. The potential for semi-supervised learning, given the model's tractable likelihood, is an exciting avenue that the authors could explore further.
Questions for the Authors:
1. Can you provide additional experimental evidence to support your claim that continuous distributions in earlier models are less effective than your discretized logistic mixture likelihood?
2. How does PixelCNN++ perform on larger datasets, such as "80 million tiny images"? Does the model scale well in terms of training and test-time performance?
3. Could you elaborate on the computational trade-offs between your downsampling approach and the dilated convolutions used in prior work?
In conclusion, the paper presents meaningful advancements to the PixelCNN architecture, with strong empirical results and practical contributions to the community. Addressing the noted weaknesses would further strengthen the paper, but they do not detract significantly from its overall merit.