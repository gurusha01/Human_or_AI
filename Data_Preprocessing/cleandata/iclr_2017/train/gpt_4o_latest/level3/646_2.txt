Review of the Paper
Summary of Contributions
The paper proposes a novel Context-aware Attention Network (CAN) for Interactive Question Answering (IQA). The model employs a two-stage encoding mechanism using word-level and sentence-level GRUs, coupled with context-dependent word-level attention and question-guided sentence-level attention. The authors introduce an interactive mechanism where the model generates supplementary questions when insufficient information is available to answer a query. A new dataset, ibAbI, extending the bAbI dataset to include IQA tasks, is also presented. Experimental results claim significant improvements over baseline models like DMN+ and MemN2N on both QA and IQA tasks, with qualitative analyses highlighting the effectiveness of the interactive mechanism.
Decision: Reject
While the paper introduces an interesting interactive mechanism and a new dataset, it fails to provide sufficient novelty and rigor to warrant acceptance. The key reasons for rejection are: (1) the lack of significant improvement over existing models like DMN+, and (2) insufficient clarity on the specific limitations of prior methods that the proposed approach addresses.
Supporting Arguments
1. Problem Definition and Motivation: The paper addresses the problem of incomplete information in QA tasks, which is a valid and challenging problem. However, the motivation for the proposed architecture is weak. The authors do not clearly articulate the specific shortcomings of DMN+ or MemN2N that CAN overcomes. The claim that CAN avoids multiple-hop attention is not convincingly demonstrated as a critical advantage, especially since multiple-hop attention is a well-established and effective mechanism in QA literature.
2. Methodology and Novelty: The two-stage encoding mechanism with attention is not novel, as similar ideas have been explored in prior works with more rigor. The interactive mechanism, while interesting, appears artificially advantageous because standard QA models could be adapted to handle similar tasks with minor modifications. The proposed question generation process, which involves generating questions word-by-word, seems unnecessarily complex and could be simplified using enumerated training questions and a softmax layer.
3. Empirical Results: The reported improvements over DMN+ and MemN2N are not substantial, especially on traditional QA tasks. While the model performs well on the ibAbI dataset, the dataset itself is a modification of bAbI, and the results may not generalize to more complex or real-world IQA tasks. Additionally, the evaluation metrics for supplementary question generation (e.g., BLEU, METEOR) are not sufficient to establish the practical utility of the approach.
Suggestions for Improvement
1. Clarify Contributions: Clearly articulate the specific limitations of DMN+ and MemN2N that CAN addresses. Provide a more detailed comparison of the architectures and justify why the proposed modifications are necessary.
2. Simplify Question Generation: Consider simplifying the question generation process by enumerating possible questions during training and using a classification-based approach instead of generating questions word-by-word.
3. Broader Evaluation: Evaluate the model on more diverse and complex datasets beyond ibAbI to demonstrate its generalizability. Include ablation studies to isolate the contributions of individual components, such as the two-stage attention mechanism and the interactive module.
4. Baseline Comparisons: Provide stronger baseline comparisons by adapting existing models like DMN+ to handle interactive QA tasks. This would help establish whether the proposed architecture offers a genuine advantage or if the results are due to the dataset design.
Questions for the Authors
1. What specific limitations of DMN+ and MemN2N does CAN address, and why are these limitations critical for IQA tasks?
2. Why was the word-by-word question generation approach chosen over simpler alternatives like enumerating training questions and using a softmax layer?
3. How does the model handle ambiguities in real-world datasets where user feedback may be noisy or incomplete? Would the interactive mechanism still be effective in such scenarios?
In conclusion, while the paper introduces an interesting interactive mechanism and dataset, the lack of significant novelty, insufficient clarity on addressed limitations, and limited empirical rigor make it unsuitable for acceptance in its current form.