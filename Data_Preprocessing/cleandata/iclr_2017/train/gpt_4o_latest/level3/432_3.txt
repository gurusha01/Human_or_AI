The paper introduces PGQL, a novel reinforcement learning algorithm that combines policy gradient methods with Q-learning by leveraging entropy-regularized policy gradient to link policies and Q-values. This hybrid approach aims to improve data efficiency and stability in reinforcement learning tasks. The authors establish theoretical connections between action-value fitting techniques and actor-critic algorithms, framing regularized policy gradient methods as advantage function learning. Empirical results on Atari benchmarks demonstrate that PGQL outperforms state-of-the-art methods like A3C and Q-learning in terms of performance and data efficiency.
Decision: Reject
While the paper presents an interesting contribution and promising empirical results, significant issues in clarity, conceptual explanation, and experimental details prevent acceptance in its current form.
Supporting Arguments:
1. Core Contribution and Insightful Links: The paper's primary contribution—PGQL—is well-motivated and builds on a solid theoretical foundation. The connection between regularized policy gradient and Q-values is insightful, and the hybridization of policy gradient and Q-learning is a valuable addition to the reinforcement learning literature. The empirical results, particularly on Atari games, are compelling and suggest practical benefits of the proposed method.
2. Clarity Issues: The paper struggles with clarity in several key areas:
   - The explanation of \(\tilde{Q}^\pi\) and its role in the algorithm is insufficiently detailed, leaving the reader uncertain about its practical implementation.
   - Section 3.2 is particularly opaque, with unclear notation and insufficient explanation of the optimization problem and its connection to the broader algorithm.
   - Section 3.3 introduces the "mu" distribution and its relationship to dueling networks, but the exposition is confusing and lacks sufficient grounding in prior work.
3. Experimental Details: The experimental section raises several questions:
   - The derivation of Equation 4 and its role in the algorithm is unclear.
   - The discounted state distribution and its impact on the results are not well-explained.
   - Figure 1 shows that algorithms do not converge to the same policy, but the reasons for this divergence are not discussed.
4. Unexplored Connections: The paper mentions connections to dueling networks (Wang et al., 2016) but does not explore them in depth. This omission weakens the theoretical contribution and leaves the reader with unanswered questions about the relationship between PGQL and existing architectures.
Additional Feedback:
1. Minor Issues: Several functions (e.g., \(r(s, a)\)) are undefined, and distinctions between tabular and non-tabular cases are not clearly articulated. The Boltzmann policy is mentioned but not formally defined.
2. Typos and Grammar: The paper contains multiple typos and grammatical errors, which detract from its readability. A thorough proofreading is necessary.
3. Appendix A: The lack of discounting in the updates is noted but not adequately justified. This could have implications for the theoretical guarantees and should be addressed.
Questions for the Authors:
1. Can you provide a clearer explanation of \(\tilde{Q}^\pi\) and its role in the algorithm? How is it estimated in practice?
2. What is the purpose of Section 3.2, and how does it connect to the overall algorithm?
3. Why do the algorithms in Figure 1 fail to converge to the same policy? Is this due to differences in initialization, hyperparameters, or other factors?
4. How does PGQL relate to dueling networks in terms of architecture and theoretical underpinnings? Can this connection be made more explicit?
5. Can you clarify the choice of fixed \(\alpha\) and its impact on the results? How does the algorithm handle varying \(\alpha\) values?
In summary, while the paper has potential, it requires significant revisions to improve clarity, address missing connections, and provide more detailed experimental explanations. These improvements would strengthen the paper and make it a valuable contribution to the field.