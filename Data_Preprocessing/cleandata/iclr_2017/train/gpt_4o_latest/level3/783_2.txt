The paper proposes a novel synchronous parallel stochastic gradient descent (SGD) approach that incorporates backup machines to mitigate the straggler effect, reducing synchronization overhead while avoiding gradient staleness. The authors argue that their method achieves faster convergence and better test accuracy compared to asynchronous SGD, which suffers from stale gradients, and traditional synchronous SGD, which is bottlenecked by the slowest worker. Through empirical experiments on models like Inception and PixelCNN, the paper demonstrates the efficacy of the proposed approach in distributed deep learning scenarios.
Decision: Reject
While the paper introduces an interesting method to address the limitations of both synchronous and asynchronous SGD, it falls short in addressing critical concerns about its generalizability and robustness. The decision to reject is based on two primary reasons: (1) the lack of evaluation under varying efficiency distributions of workers, and (2) insufficient analysis of idle time under uniform worker efficiency. These gaps limit the completeness of the empirical validation and the practical applicability of the proposed method.
Supporting Arguments:
1. Strengths:  
   - The paper is well-motivated, addressing a significant problem in distributed deep learning.  
   - The proposed method is innovative, combining the strengths of synchronous and asynchronous approaches.  
   - The empirical results convincingly show improved convergence speed and test accuracy for specific configurations.  
2. Weaknesses:  
   - The approach assumes that most workers operate at similar speeds, which may not hold in real-world distributed systems with heterogeneous hardware or network conditions.  
   - The experiments do not evaluate the method's performance under varying efficiency distributions of learners, leaving its robustness untested.  
   - There is no detailed analysis of idle time when all workers have uniform efficiency, which is critical for understanding the trade-offs of the proposed method.  
Suggestions for Improvement:
1. Additional Experiments:  
   - Include experiments where worker efficiency varies significantly (e.g., due to hardware heterogeneity or network delays). This would provide insights into the method's robustness in non-ideal conditions.  
   - Analyze the impact of backup workers on idle time and overall system efficiency when all workers have similar speeds.  
2. Theoretical Analysis:  
   - Provide a theoretical framework to quantify the trade-offs between gradient quality (due to dropped stragglers) and synchronization overhead.  
3. Scalability:  
   - Discuss the scalability of the approach as the number of workers increases, particularly in scenarios where straggler effects become more pronounced.  
Questions for the Authors:
1. How does the proposed method perform in environments with highly heterogeneous worker efficiencies?  
2. What is the impact of varying the number of backup workers on convergence speed and test accuracy?  
3. Can the method be extended to dynamically adjust the number of backup workers based on real-time worker performance?  
In conclusion, while the paper presents a promising approach to distributed SGD, the lack of comprehensive evaluation and theoretical analysis limits its potential impact. Addressing the above concerns could significantly strengthen the paper for future submissions.