Review of the Paper
Summary of Contributions
The paper introduces a novel model for short text classification, termed the Character-Aware Attention Residual Network (CAR-Net). The model combines character-level embeddings, word-level semantic embeddings, and n-gram features to construct enriched sentence representations. The authors employ attention mechanisms to weigh significant features and utilize residual networks to refine the sentence embeddings. The proposed approach is evaluated on three datasets (Tweets, Questions, and AG News) and claims to outperform state-of-the-art traditional and deep learning baselines, particularly on short and noisy text.
Decision: Reject  
While the paper proposes an innovative architecture with interesting components, it falls short in critical areas that undermine the validity and generalizability of its claims. Specifically, the lack of ablation studies and the absence of evaluations on widely-used standard datasets make it difficult to assess the true impact of the proposed architectural choices.
Supporting Arguments for the Decision
1. Lack of Ablation Studies:  
   The paper introduces several architectural components, such as character-aware embeddings, attention mechanisms, and residual networks. However, it does not provide ablation studies to isolate the contribution of each component. For example, while the authors claim that residual networks improve performance, the evidence is limited to a single comparison without deeper analysis. This omission makes it unclear which components are truly responsible for the reported improvements.
2. Evaluation on Non-Standard Datasets:  
   The datasets used (Tweets, Questions, AG News) are either small or domain-specific, and the paper does not test the model on widely-used benchmark datasets like SST-2, IMDB, or 20 Newsgroups. This limits the generalizability of the results and makes it difficult to compare the proposed method with existing state-of-the-art models in the literature.
3. Unconvincing Results Against Baselines:  
   While the model performs well on the Tweets and Questions datasets, its performance on AG News is only comparable to simpler baseline methods like TFIDF-SVM. This raises questions about the robustness of the proposed approach, especially on datasets with more structured or formal text.
Suggestions for Improvement
1. Incorporate Ablation Studies:  
   The authors should conduct experiments to evaluate the individual contributions of character-aware embeddings, attention mechanisms, and residual networks. For instance, removing the residual network or attention mechanism could provide insights into their importance.
2. Evaluate on Standard Datasets:  
   Testing the model on widely-used datasets in the text classification community would make the results more comparable and credible. This would also help demonstrate the model's generalizability across different text domains.
3. Provide More Rigorous Baseline Comparisons:  
   The paper should include comparisons with more recent and competitive deep learning models, such as BERT or other transformer-based architectures, which are widely used for text classification tasks.
4. Clarify Dataset Details:  
   The paper should provide more information about the datasets, such as the number of samples, class distributions, and preprocessing steps. This would help assess the validity of the experimental setup.
Questions for the Authors
1. How does the model perform on standard datasets like SST-2, IMDB, or 20 Newsgroups?  
2. Can you provide ablation studies to quantify the impact of character-aware embeddings, attention mechanisms, and residual networks?  
3. How sensitive is the model to hyperparameters, such as the number of residual blocks or the dimensionality of embeddings?  
4. Why were simpler baselines like TFIDF-SVM able to achieve comparable results on the AG News dataset?  
In summary, while the paper introduces an interesting model for short text classification, the lack of rigorous evaluation and ablation studies significantly weakens its contributions. Addressing these issues would greatly enhance the paper's impact and credibility.