Review of the Paper
Summary of Contributions
This paper introduces a novel modular approach for adapting neural networks to new tasks with limited training data. Instead of fine-tuning pre-trained networks, the authors propose "stitching" new modules at various hierarchical levels, allowing the network to learn complementary features while preserving the original model's representational power. The approach is evaluated across multiple domains, including image classification (CIFAR-10 to CIFAR-100, Stanford Cars), text classification (IMDB sentiment analysis), and MNIST. The results suggest that this method outperforms traditional fine-tuning, particularly in small-data regimes, by leveraging pre-trained networks without overwriting their learned features.
Decision: Reject
While the paper addresses an important problem and proposes an interesting modular approach, it falls short in several critical areas, including empirical rigor, clarity of presentation, and baseline comparisons. These shortcomings make it difficult to assess the true novelty and impact of the proposed method.
Supporting Arguments for Decision
1. Weak Empirical Results: The results in Section 3 are underwhelming, as the experiments rely on large networks, which may mask the benefits of the modular approach. Testing with smaller networks would provide a clearer picture of the method's effectiveness. Additionally, the claim of learning complementary features is supported only by qualitative visualizations (e.g., Figure 3) and lacks quantitative proof or constraints in the loss function to enforce this behavior.
   
2. Lack of Baseline Comparisons: The paper does not compare its results with previously published methods, such as Progressive Nets or ResNet-based approaches, which are conceptually similar. Furthermore, the potential of an ensemble of two pre-trained networks as a baseline is unexplored, particularly for tasks like car classification. This omission makes it difficult to contextualize the contributions of the proposed method.
3. Unclear Methodological Choices: The use of a batch five times in a row during training is unconventional, and its effectiveness compared to standard SGD is not justified or explored. Additionally, the concept of an "untrained model" in Figure 5 is unclear, and its relevance to the results is questionable.
4. Presentation Issues: Figures 4 and 5 lack proper labeling, making it challenging to interpret the results. Figure 5, in particular, would benefit from a format similar to Figure 4 for consistency and readability. The paper also fails to clearly explain the "stitching" mechanism and its advantages over simpler modular architectures.
Suggestions for Improvement
1. Stronger Baselines: Include comparisons with Progressive Nets, ResNet, and other state-of-the-art transfer learning methods. Additionally, explore the ensemble of two pre-trained networks as a baseline, particularly for fine-grained tasks like car classification.
   
2. Quantitative Proof of Complementary Features: Introduce explicit constraints in the loss function or quantitative metrics to demonstrate that the new modules learn complementary features instead of redundant ones.
3. Smaller Network Experiments: Test the modular approach with smaller networks to better highlight its advantages in resource-constrained scenarios.
4. Clarify Methodology: Justify the use of unconventional training strategies, such as iterating over the same batch multiple times. Provide a clearer explanation of the "stitching" mechanism and its benefits over simpler approaches.
5. Improve Figures: Ensure all figures are properly labeled and formatted for readability. Clarify the concept of the "untrained model" in Figure 5 and its role in the experiments.
Questions for the Authors
1. How does the proposed method compare with Progressive Nets and ResNet-based approaches in terms of performance and computational efficiency?
2. What is the rationale behind iterating over the same batch five times during training? How does this compare to standard SGD in terms of convergence and generalization?
3. Can you provide quantitative evidence or metrics to demonstrate that the new modules learn complementary features rather than redundant ones?
In summary, while the paper tackles an important problem and proposes an interesting approach, its lack of empirical rigor, baseline comparisons, and clarity in presentation undermine its contributions. Addressing these issues could significantly strengthen the paper.