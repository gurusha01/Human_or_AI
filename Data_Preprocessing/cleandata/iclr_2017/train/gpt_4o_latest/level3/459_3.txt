Review of the Paper
Summary of Contributions
The paper introduces a novel deep multi-task representation learning (DMTRL) framework that leverages tensor factorization to enable automatic cross-task sharing at every layer of a deep neural network. This approach eliminates the need for manually defined multi-task sharing strategies, a significant limitation in conventional multi-task learning (MTL) methods. The authors generalize matrix factorization techniques commonly used in shallow MTL to tensor factorization, enabling flexible and data-driven sharing of parameters in both fully connected and convolutional layers. The proposed framework is applicable to both homogeneous and heterogeneous MTL settings. Experimental results on diverse datasets (e.g., MNIST, AdienceFaces, Omniglot) demonstrate improved accuracy and reduced design complexity compared to traditional MTL approaches, including user-defined architectures. The method also achieves parameter savings through low-rank tensor factorization.
Decision: Accept
The paper is recommended for acceptance due to its novel and well-motivated approach, strong empirical results, and practical contributions to reducing the design complexity of deep MTL architectures. The key reasons for this decision are:
1. Novelty and Practical Impact: The application of tensor factorization to deep MTL is innovative and addresses a critical challenge in the fieldâ€”manual architecture design for task sharing.
2. Empirical Validation: The proposed method consistently outperforms both single-task learning (STL) and user-defined MTL (UD-MTL) across multiple datasets and tasks, demonstrating its robustness and effectiveness.
Supporting Arguments
1. Well-Motivated Approach: The paper provides a clear motivation for revisiting MTL in the context of deep learning, highlighting the limitations of existing methods that rely on manual sharing strategies. The use of tensor factorization is a natural extension of matrix-based methods, and its integration into deep networks is both theoretically sound and practically impactful.
2. Comprehensive Experiments: The authors evaluate their framework on diverse datasets, including homogeneous (MNIST) and heterogeneous (AdienceFaces, Omniglot) MTL settings. The results show that DMTRL achieves higher accuracy with fewer design choices, particularly in low-data regimes where MTL is most beneficial.
3. Parameter Efficiency: The method achieves parameter savings through low-rank tensor factorization, which is a desirable property for scaling to large models and datasets. While similar savings could theoretically be achieved using singular value decomposition (SVD) in conventional MTL, the proposed framework integrates this directly into the learning process.
Suggestions for Improvement
1. Clarification of Novelty: While the application of tensor factorization to MTL is novel, tensor factorization itself has been explored in other contexts. The authors could better emphasize the unique contributions of their work, particularly in comparison to prior neural network-based MTL methods in speech recognition.
2. Comparison to Earlier Work: The paper cites neural network-based MTL approaches from the speech recognition community but does not provide direct comparisons. Including such comparisons would strengthen the empirical validation of the proposed method.
3. Parameter Savings Discussion: The authors note that parameter savings stem from low-rank tensor factorization, which could also be achieved using SVD. A deeper discussion of the advantages of their approach over SVD-based methods would be helpful.
4. Ablation Studies: While the paper demonstrates the effectiveness of different tensor factorization methods (Tucker, TT, LAF), an ablation study to isolate the contributions of each component (e.g., initialization strategy, rank selection) would provide additional insights.
Questions for the Authors
1. How does the proposed method compare to earlier neural network-based MTL approaches in speech recognition, which predate the works cited in the paper (e.g., Caruana, 1997)?
2. Can the authors provide more details on the computational overhead introduced by tensor factorization, particularly during training?
3. How sensitive is the method to the choice of rank in the tensor factorization? While the authors mention that the framework is not sensitive to rank choice, more quantitative evidence would be helpful.
4. Could the method be extended to unsupervised or semi-supervised MTL settings? If so, what modifications would be required?
In conclusion, the paper makes a significant contribution to the field of multi-task learning by introducing a novel, flexible, and effective framework for deep MTL. The proposed method addresses key limitations of existing approaches and demonstrates strong empirical performance, making it a valuable addition to the literature.