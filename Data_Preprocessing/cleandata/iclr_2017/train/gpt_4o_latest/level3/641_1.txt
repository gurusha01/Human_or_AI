Review
Summary of Contributions
This paper introduces two novel methods for "wild variational inference," which enable sampling from variational approximate distributions without requiring the evaluation of their density \( q(z) \). This relaxation allows for the use of more flexible distribution families, addressing a significant limitation of traditional variational inference methods. The authors apply these methods to optimize the hyperparameters of stochastic gradient Langevin dynamics (SGLD), demonstrating their approach on two tasks: a 1D Gaussian mixture model and Bayesian logistic regression. The key contribution lies in connecting Stein variational gradient descent (SVGD) and kernelized Stein discrepancy (KSD) to inference networks, facilitating adaptive hyperparameter tuning for SGLD. The paper claims that these methods outperform hand-designed step-size schemes and achieve competitive results.
Decision: Reject  
The paper presents an interesting idea with potential, but it falls short in several critical areas. The primary reasons for rejection are the limited scope of experiments and insufficient clarity in writing, which undermines the paper's ability to convincingly support its claims.
Supporting Arguments
1. Limited Experimental Scope: The experiments are restricted to a 1D Gaussian mixture model and Bayesian logistic regression. While these serve as proof-of-concept demonstrations, they are overly simplistic and fail to establish the scalability or robustness of the proposed methods for high-dimensional or complex real-world problems. For example, the Bayesian logistic regression posterior is close to Gaussian, which does not adequately test the ability of the methods to handle multimodal or highly non-Gaussian distributions.
   
2. Computational Concerns: The reliance on evaluating the true gradient of the target distribution makes the proposed methods computationally expensive, particularly for large-scale datasets. This undermines the practical utility of the approach, especially when SGLD itself is known for its computational efficiency per update step.
3. Clarity and Differentiation: The paper's writing is unclear in several places, with insufficient differentiation between its contributions and prior work. A significant portion of the text is devoted to explaining existing methods, which detracts from the novelty of the proposed approach. The connection to prior work, such as Ranganath et al. (2016), is not adequately clarified, leaving the reader uncertain about the incremental nature of the contribution.
4. Missing Comparisons: The paper does not include comparisons with Hamiltonian Monte Carlo (HMC) methods, such as the No-U-Turn Sampler (NUTS), which are widely regarded as state-of-the-art for Bayesian inference. Such comparisons are essential to contextualize the performance of the proposed methods.
Suggestions for Improvement
1. Expand Experimental Validation: Conduct experiments on more challenging, high-dimensional, and multimodal distributions to demonstrate the scalability and robustness of the methods. Examples include deep Bayesian neural networks or large-scale probabilistic models used in real-world applications.
   
2. Include Baseline Comparisons: Compare the proposed methods with HMC (e.g., NUTS) and other adaptive MCMC methods to provide a more comprehensive evaluation of their performance.
3. Address Computational Efficiency: Provide a detailed analysis of the computational cost of the proposed methods, particularly in large-scale settings. Discuss strategies to mitigate the reliance on the true gradient of the target distribution.
4. Improve Writing Clarity: Clearly delineate the novel contributions of the paper from prior work. Reduce the emphasis on explaining existing methods and focus on the unique aspects of the proposed approach.
Questions for the Authors
1. How do the proposed methods perform on high-dimensional, multimodal distributions where the posterior is far from Gaussian?  
2. Can the computational overhead of evaluating the true gradient of the target distribution be reduced, and how does this impact scalability?  
3. Why were comparisons with HMC methods (e.g., NUTS) omitted, and how do the proposed methods compare in terms of accuracy and efficiency?  
4. How sensitive are the proposed methods to the choice of kernel in KSD, and does this impact their robustness across different tasks?
While the paper introduces a promising idea, addressing the above concerns would significantly strengthen its contribution and impact.