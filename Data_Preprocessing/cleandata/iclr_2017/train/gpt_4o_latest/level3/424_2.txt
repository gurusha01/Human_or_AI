Review of the Paper
Summary of Contributions
The paper proposes a novel framework for improving the optimization of deep neural networks by employing a mollification-based continuation method. The approach begins with a simplified, smoothed objective function and progressively transitions to the original, more complex objective. This is achieved by annealing a noise parameter that controls the complexity of the network during training. The method is inspired by curriculum learning and continuation methods but introduces a unique perspective by shaping both the cost function and the model architecture. The authors demonstrate the utility of their approach on tasks such as MNIST, CIFAR-10, and language modeling, claiming improvements in optimization and generalization performance.
Decision: Reject
The paper presents an interesting idea with potential, but it falls short in several critical areas: clarity of presentation, experimental rigor, and sufficient comparisons to related methods. These issues hinder the paper's ability to convincingly support its claims.
Supporting Arguments for the Decision
1. Clarity of Methodology: The explanation of the proposed method is often unclear, particularly in Algorithm 1 and the derivation of Eq. 25. The pseudo-code is difficult to follow, and key details about the implementation are either missing or overly convoluted. This lack of clarity makes it challenging to reproduce or fully understand the proposed approach.
2. Insufficient Experimental Evaluation: While the paper evaluates the method on several tasks, the experiments are not sufficiently rigorous. For example, the method is not tested on challenging architectures like deep and thin networks, which would better demonstrate its robustness. Additionally, the CIFAR-10 results raise concerns, as the model trains slower than other methods, and the use of residual connections is not adequately justified.
3. Lack of Comparisons: The paper does not provide a thorough comparison to other weight noise regularization techniques or related methods like highway networks. Given the thematic similarities, such comparisons are necessary to contextualize the contributions and highlight the advantages of the proposed approach.
Suggestions for Improvement
1. Clarify Methodology: Improve the explanation of the proposed framework, particularly Algorithm 1 and Eq. 25. Provide intuitive interpretations and step-by-step derivations where necessary. Simplify the pseudo-code and ensure it aligns with the textual description.
2. Expand Experimental Scope: Test the method on more challenging problems, such as deep and thin networks, to better demonstrate its effectiveness. Include experiments with larger datasets and more complex architectures to validate scalability.
3. Comparative Analysis: Include direct comparisons to other weight noise regularization methods, such as dropout or stochastic depth, as well as to highway and residual networks. This will help establish the unique contributions of the proposed approach.
4. Address Related Work: Discuss the differences between shaping, curriculum learning, and the proposed method in greater detail. This will help clarify the novelty of the approach and its relationship to existing literature.
Questions for the Authors
1. How does the proposed method compare to other weight noise regularization techniques, such as dropout or stochastic depth, in terms of both optimization and generalization performance?
2. Can you provide more details on the annealing schedules used in the experiments? How sensitive is the method to the choice of these schedules?
3. Why does the CIFAR-10 experiment show slower training compared to other methods? Could this be due to the use of residual connections, and how does this affect the validity of the results?
4. How does the method handle extremely deep and thin networks, which are known to be challenging to optimize?
In conclusion, while the paper introduces a promising idea, significant improvements in clarity, experimental rigor, and comparative analysis are required for it to be a strong candidate for acceptance.