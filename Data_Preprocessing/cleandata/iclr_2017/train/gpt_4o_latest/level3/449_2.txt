The paper introduces FractalNet, a novel neural network architecture inspired by fractal structures, which provides an alternative to residual networks (ResNets) for training ultra-deep neural networks. The authors propose a recursive expansion and merging of sub-networks to create a fractal structure, which enables effective training without explicit residual connections. Additionally, the paper introduces drop-path, a new regularization technique that randomly disables paths in the fractal network to prevent co-adaptation and improve generalization. The authors claim that FractalNet achieves comparable performance to ResNets on CIFAR-10, CIFAR-100, and ImageNet, while offering unique advantages such as anytime behavior and robustness to depth.
Decision: Reject
The paper presents an interesting and innovative architectural design, but it falls short in providing sufficient empirical evidence to support its claims. The experimental results lack rigor and fail to convincingly demonstrate the superiority of FractalNet over existing methods.
Supporting Arguments:
1. Novelty and Clarity: The paper proposes a unique approach to designing deep networks through fractal structures, which is conceptually intriguing. The design is clearly described, and the connection to existing techniques, such as residual learning and deep supervision, is well-articulated. The introduction of drop-path as a regularization mechanism is also a valuable contribution.
   
2. Experimental Weaknesses: The experimental results are underwhelming. While FractalNet performs comparably to ResNets on some datasets, it does not show significant improvements. Notably, the performance on SVHN is not improved, which raises concerns about the generalizability of the proposed method. Furthermore, the lack of parameter counts for all models makes it difficult to assess the efficiency of FractalNet compared to ResNets. This omission hinders fair comparisons and weakens the empirical validation of the proposed architecture.
3. Drop-Path and Data Augmentation: The paper claims that drop-path is an effective regularization technique, but its benefits diminish when data augmentation is applied. This limitation reduces the practical applicability of the method, as data augmentation is a standard practice in modern deep learning pipelines.
Additional Feedback:
1. Parameter Efficiency: The authors should provide detailed parameter counts for all models to enable a fair comparison of computational efficiency and memory requirements. This is particularly important given the increasing emphasis on lightweight architectures in the research community.
2. Broader Evaluation: The experiments should include additional datasets and tasks (e.g., object detection or segmentation) to better evaluate the generalizability of FractalNet. The current focus on CIFAR and ImageNet is too narrow.
3. Ablation Studies: While drop-path is introduced as a key contribution, the paper lacks thorough ablation studies to isolate its impact. For example, how does FractalNet perform without drop-path? How does it compare to other regularization techniques like stochastic depth?
4. Anytime Behavior: The paper highlights the anytime property of FractalNet, but this aspect is not sufficiently explored in the experiments. Quantitative results demonstrating the trade-off between latency and accuracy would strengthen this claim.
Questions for the Authors:
1. How does the computational cost of training and inference for FractalNet compare to ResNets of similar depth and accuracy?
2. Can the authors provide more detailed results on the impact of drop-path under different data augmentation schemes?
3. How does FractalNet perform on tasks beyond image classification, such as object detection or transfer learning?
In summary, while the paper introduces an innovative architectural concept, the lack of compelling experimental evidence and critical analysis prevents it from meeting the standards for acceptance at this time. Addressing the aforementioned issues could significantly strengthen the work for future submission.