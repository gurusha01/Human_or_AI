Review
Summary of Contributions
This paper introduces a novel heuristic, termed "intrinsic fear," to address the problem of catastrophic forgetting in deep reinforcement learning (DRL) agents. The authors propose a "danger model" that penalizes the Q-learning objective when the agent approaches states likely to lead to catastrophic outcomes. The approach is tested on two toy environments—Adventure Seeker and Cart-Pole—and preliminary experiments on the Atari game Seaquest. The results demonstrate that the intrinsic fear model significantly reduces the frequency of catastrophic failures compared to standard DQNs. The paper is well-written, with poetic expressions and vivid metaphors that make the technical content engaging and memorable. Notable quotes, such as "a self-driving car that had to periodically hit a few pedestrians to remember it's undesirable," effectively illustrate the motivation behind the work.
Decision: Reject
While the paper makes an interesting contribution and is well-motivated, the evaluation is insufficiently robust to warrant acceptance. The experiments are limited to toy domains and a single Atari game, which undermines the generalizability of the proposed approach. Additionally, the paper lacks a rigorous comparison against state-of-the-art methods in safe reinforcement learning, leaving the novelty and impact of the contribution somewhat unclear.
Supporting Arguments
1. Problem Definition and Motivation: The paper tackles an important problem in DRL—ensuring safety in real-world applications by avoiding catastrophic states. The motivation is compelling, and the authors effectively highlight the limitations of existing DQNs in handling catastrophic forgetting. The introduction of the "Sisyphean curse" as a framing device is particularly memorable and aligns well with the paper's goals.
2. Methodology: The intrinsic fear model is conceptually sound and well-explained. The use of a supervised danger model to shape rewards away from catastrophic states is a creative approach that builds on existing ideas in reward shaping and intrinsic motivation.
3. Evaluation: The primary weakness lies in the evaluation. While the toy environments (Adventure Seeker and Cart-Pole) are useful for illustrating the problem, they are insufficient to demonstrate the scalability and robustness of the method. The single Atari game (Seaquest) provides only preliminary insights, and the results are not compared against well-established benchmarks or alternative safety mechanisms. Without experiments on a broader range of environments, such as the full suite of Atari games or more complex robotics tasks, the claims of general applicability remain speculative.
4. Writing Style: The paper is exceptionally well-written, with poetic expressions and vivid analogies that make the technical content accessible and engaging. However, the literary style occasionally overshadows the technical rigor, and some sections (e.g., the discussion of philosophical definitions of safety) could be streamlined to focus more on empirical results.
Suggestions for Improvement
1. Expand Evaluation: Test the intrinsic fear model on a wider range of benchmarks, such as the full Atari suite or MuJoCo environments. This would provide stronger evidence of the method's generalizability and practical utility.
2. Comparative Analysis: Include comparisons against state-of-the-art methods in safe reinforcement learning, such as those that modify the objective function or use external knowledge for safe exploration. This would help clarify the novelty and advantages of the proposed approach.
3. Ablation Studies: Conduct ablation studies to isolate the contributions of key components, such as the fear radius, fear factor, and the danger model's architecture. This would provide deeper insights into the method's effectiveness and limitations.
4. Theoretical Analysis: Provide a more formal analysis of the conditions under which the intrinsic fear model is expected to succeed or fail. This could strengthen the theoretical foundation of the work.
Questions for the Authors
1. How does the intrinsic fear model compare to existing methods for safe reinforcement learning, such as those that use risk-sensitive objectives or hard constraints?
2. Can the danger model handle environments where catastrophic states are not spatially proximate but can occur due to a single action from any state?
3. How sensitive is the method to the choice of hyperparameters, such as the fear radius and fear factor? Could these parameters be learned adaptively during training?
Conclusion
The paper addresses an important problem and introduces a creative solution, but the limited evaluation and lack of comparison with existing methods weaken its impact. With additional experiments and theoretical analysis, this work could make a significant contribution to the field of safe reinforcement learning.