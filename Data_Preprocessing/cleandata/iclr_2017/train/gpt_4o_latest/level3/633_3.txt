Review
Summary of Contributions
The paper introduces a novel cooperative training scheme, termed CoopNets, for generator and descriptor networks. Unlike adversarial methods such as GANs, the proposed approach emphasizes collaboration between the two networks, where they assist each other during training. The generator network initializes synthesized examples, which are then refined by the descriptor network using Langevin dynamics. The descriptor, in turn, provides feedback to improve the generator. This cooperative mechanism is claimed to stabilize training and enhance synthesis quality. The paper also highlights theoretical insights into the convergence of the CoopNets algorithm and presents experiments on image synthesis and face completion tasks. The authors argue that the method offers a unique perspective on probabilistic modeling by interweaving maximum likelihood training for both networks.
Decision: Reject
Key reasons: (1) The empirical evaluation is insufficient to substantiate the claims of the paper, and (2) the lack of baseline comparisons makes it difficult to assess the true benefits of the proposed approach.
Supporting Arguments
1. Insufficient Empirical Evaluation: The use of small datasets (e.g., 10,000 images from CelebA and 1,000 images per category for synthesis) raises concerns about overfitting and the generalizability of the results. Larger and more diverse datasets, such as ImageNet, should be employed to validate the robustness of the method.
   
2. Lack of Baseline Comparisons: The paper does not compare CoopNets to other state-of-the-art methods, such as GANs, VAEs, or other deep auto-encoders, in a comprehensive manner. For example, the face completion experiments fail to include comparisons with individually trained networks or established inpainting techniques, making it unclear whether CoopNets offers any tangible improvement.
3. Related Work and Theoretical Placement: While the paper draws connections to GANs and contrastive divergence, it omits a critical discussion of variational auto-encoders (VAEs) (Kingma & Welling, 2013), which are highly relevant to the probabilistic modeling framework. This omission weakens the theoretical grounding of the paper.
4. Experimental Design: The face completion experiments are limited in scope, and the evaluation metrics (e.g., recovery errors) are not well contextualized. Additionally, the qualitative synthesis results lack diversity, and the paper does not provide quantitative metrics (e.g., FID scores) to objectively measure synthesis quality.
Suggestions for Improvement
1. Expand Empirical Studies: Use larger and more diverse datasets to demonstrate the scalability and robustness of CoopNets. Include experiments on challenging benchmarks like ImageNet or high-resolution datasets.
   
2. Baseline Comparisons: Incorporate comparisons with GANs, VAEs, and other probabilistic models to highlight the advantages of cooperative training. For face completion, compare against state-of-the-art inpainting methods and report standard metrics like PSNR or SSIM.
3. Related Work: Extend the discussion in the "Related Work" section to include VAEs and other probabilistic models. Highlight how CoopNets differs from or improves upon these approaches.
4. Quantitative Metrics: Include standard metrics such as FID, IS, or reconstruction error to objectively evaluate the quality of synthesized images.
5. Ablation Studies: Conduct ablation studies to isolate the contributions of individual components, such as Langevin dynamics and the cooperative mechanism.
Questions for the Authors
1. How does CoopNets perform on larger datasets, and does it generalize well to diverse data distributions?
2. Can you provide quantitative comparisons with GANs, VAEs, or other probabilistic models to demonstrate the benefits of cooperative training?
3. How sensitive is the method to hyperparameters such as the number of Langevin steps or the learning rate?
4. Why were VAEs omitted from the discussion in the "Related Work" section, given their relevance to probabilistic modeling?
In conclusion, while the paper presents an interesting and novel idea, its empirical and theoretical contributions are not sufficiently substantiated to warrant acceptance at this stage. Strengthening the experimental evaluation and addressing the aforementioned concerns would significantly improve its impact.