Review of the Paper
Summary of Contributions
The paper proposes a novel method for learning dependent subspaces across multiple views by preserving neighborhood relationships, measured via probabilistic distributions of neighbors. The method is motivated by its relevance to information retrieval tasks, emphasizing the importance of neighborhood preservation over traditional coordinate-based dependency measures like those used in Canonical Correlation Analysis (CCA). The authors claim that their approach is more flexible, capable of detecting nonlinear and local dependencies, and adaptable to different dimensionalities for each view. The method is evaluated on artificial and real-world datasets, with results suggesting improvements over CCA and LPCCA in preserving neighborhood similarities.
Decision: Reject
While the paper introduces an interesting idea with potential applications in information retrieval, it falls short in several critical areas. The primary reasons for rejection are the lack of robust empirical validation and the limited applicability of the experiments, which are conducted on small and unrealistic datasets. Additionally, the presentation suffers from vague terminology and unclear technical explanations, making it difficult to fully assess the contributions.
Supporting Arguments for the Decision
1. Empirical Validation: The experimental results are preliminary and lack robustness. The datasets used, such as MNIST with only 2000 examples, are small and do not reflect the complexity of real-world multi-view tasks. The absence of experiments on larger, more realistic datasets limits the generalizability of the method.
   
2. Overstated Claims: The authors claim that no other information retrieval-based approaches exist for this problem, which is inaccurate. For instance, related work such as Hermann & Blunsom (ACL 2014) is not adequately discussed or compared against.
3. Technical Clarity: Several critical aspects of the method are poorly explained, including the role of the penalty term \( C_{\text{Penalty}} \), the use of PCA between views, and the optimization process. The discussion of KL divergence and its smoothing is confusing and lacks sufficient justification.
4. Presentation Issues: The paper suffers from vague terminology (e.g., "dependency"), unclear task definitions (e.g., stock price and image patch experiments), and minor presentation flaws such as illegible figure fonts and grammatical errors.
Suggestions for Improvement
1. Empirical Validation: Conduct experiments on larger and more diverse datasets to demonstrate the scalability and applicability of the method. For example, datasets from domains like social networks, natural language processing, or bioinformatics could provide more compelling evidence of the method's utility.
2. Comparative Analysis: Include comparisons with more recent and relevant methods, such as Hermann & Blunsom (ACL 2014) or other multi-view learning approaches. This would strengthen the positioning of the proposed method within the literature.
3. Technical Clarifications: Provide clearer explanations of the penalty term \( C_{\text{Penalty}} \), the role of PCA, and the optimization strategy. The discussion of KL divergence and its smoothing should be revised for clarity and rigor.
4. Task Definitions: Clearly define the tasks and their relevance to the proposed method. For instance, the stock price and image patch tasks are not well-motivated or explained.
5. Presentation: Address minor presentation flaws, such as illegible figure fonts, grammatical errors, and awkward phrasing. Improving the overall readability would make the paper more accessible to a broader audience.
Questions for the Authors
1. How does the method perform on larger, real-world datasets with more complex multi-view relationships? Can you provide evidence of scalability?
2. Why were related works like Hermann & Blunsom (ACL 2014) not included in the comparison? How does your method differ from theirs?
3. Can you provide more detailed explanations of the penalty term \( C_{\text{Penalty}} \) and its role in avoiding local optima?
4. How do you justify the use of MNIST with only 2000 examples as a representative dataset for multi-view learning tasks?
In summary, while the paper introduces a promising idea, significant revisions are needed to address the empirical, technical, and presentation issues before it can be considered for publication. It may be better suited for a workshop track at this stage.