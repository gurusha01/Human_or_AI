Review of the Paper
Summary of Contributions
This paper investigates the transfer of attention mechanisms between teacher and student convolutional neural networks (CNNs) to improve the performance of the student network. The authors propose novel activation-based and gradient-based attention mechanisms for this transfer process, leveraging spatial attention maps derived from network activations or gradients. The paper demonstrates that attention transfer consistently improves student network performance across datasets such as CIFAR, CUB/Scene, and ImageNet. The authors also explore the combination of attention transfer with knowledge distillation, showing further gains in some cases. The work is well-written and motivated, with a clear focus on improving the understanding and application of attention mechanisms in neural networks.
Decision: Reject
While the paper is well-motivated and demonstrates consistent improvements in student network performance, the incremental novelty and lack of comprehensive analysis on key aspects weaken its contribution. Below are the primary reasons for this decision:
1. Incremental Contribution: The proposed methods, while interesting, are conceptually similar to prior work such as FitNet and knowledge distillation. The novelty of using attention maps for transfer is incremental and not sufficiently differentiated from existing approaches.
2. Incomplete Analysis: Several critical analyses are missing, such as the quantitative evidence supporting the claim that higher accuracy networks have higher spatial correlation between objects and attention maps. Additionally, the impact of hyperparameters (e.g., β) on performance is not thoroughly explored, leaving the methodology under-specified.
Supporting Arguments
1. Performance Gaps: While attention transfer improves student networks, the students still underperform compared to the teacher networks. The paper does not provide sufficient insights into why this gap persists or how it could be mitigated.
2. Unclear Attention Map Selection: The experiments do not clearly outline the pros and cons of different attention mapping functions. For example, while sum-based functions perform better than max-based ones, the underlying reasons for this behavior are not discussed.
3. Unexplored Scenarios: The benefit of attention transfer when teacher and student networks share identical architectures is not explored, which could provide a baseline for understanding the effectiveness of the proposed methods.
Additional Feedback for Improvement
1. Hyperparameter Analysis: The role of hyperparameters, particularly β, should be analyzed in greater depth. A sensitivity analysis would strengthen the robustness of the proposed methods.
2. Teacher Performance Reporting: Figure 7b lacks teacher network train and validation loss curves, which would provide additional insights into the transfer process and its limitations.
3. Speed-Up Characterization: Since student networks have fewer parameters, a characterization of computational speed-ups (e.g., inference time) would add practical value to the work.
4. Quantitative Evidence: The claim that higher accuracy networks exhibit higher spatial correlation between objects and attention maps should be supported with quantitative evidence, such as correlation metrics or statistical tests.
5. Broader Applications: Exploring attention transfer in tasks where spatial information is critical, such as object detection or weakly-supervised localization, could significantly enhance the paper's impact.
Questions for the Authors
1. Can you provide quantitative evidence to support the claim that higher accuracy networks have higher spatial correlation between objects and attention maps?
2. How does the choice of β impact the performance of attention transfer? Did you observe any trends or thresholds during your experiments?
3. Why does attention transfer fail to outperform the teacher network? Are there specific limitations in the transfer process that could be addressed in future work?
4. Have you considered applying attention transfer to architectures with identical teacher and student networks? If so, what were the results?
In summary, while the paper is well-written and demonstrates consistent improvements, it lacks sufficient novelty and depth in analysis to warrant acceptance in its current form. Addressing the identified gaps and questions could significantly strengthen the contribution.