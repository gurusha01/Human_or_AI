Review
Summary of Contributions
This paper introduces Dynamic Recurrent Acyclic Graphical Neural Networks (DRAGNN), a modular framework for constructing recurrent neural architectures. The core innovation is the Transition-Based Recurrent Unit (TBRU), which dynamically builds connections in a computation graph based on intermediate activations. DRAGNN generalizes existing architectures like sequence-to-sequence (seq2seq), attention mechanisms, and tree-structured models, enabling explicit structural representations for tasks such as dependency parsing and extractive summarization. The authors demonstrate that DRAGNN achieves state-of-the-art performance on dependency parsing and improves multi-task learning for summarization by leveraging structured intermediate representations.
Decision: Reject  
While the paper makes a commendable effort to unify neural architectures and demonstrates empirical improvements, the standalone contribution of the DRAGNN framework is insufficiently novel. The work leans more toward software engineering and modularization rather than advancing fundamental machine learning research. Additionally, the framework lacks compelling incentives for adoption compared to existing tools like TensorFlow, DyNet, or VW.
Supporting Arguments
1. Problem Tackled: The paper addresses the challenge of incorporating explicit structure into neural architectures, which is a well-motivated problem. However, the proposed solution—modularizing architectures via TBRUs—feels incremental rather than groundbreaking. Transition-based systems and dynamic computation graphs are not new concepts, and the novelty of DRAGNN lies primarily in its implementation rather than theoretical innovation.
   
2. Motivation and Placement in Literature: The paper is well-situated within the literature, referencing relevant works on seq2seq, attention mechanisms, and structured prediction. However, it does not sufficiently differentiate DRAGNN from existing frameworks. For instance, TensorFlow and DyNet already support dynamic computation graphs, and the paper does not convincingly argue why DRAGNN offers unique advantages.
3. Support for Claims: The empirical results are promising, particularly for dependency parsing and multi-task summarization. However, the gains are modest, and the paper does not explore the limitations of DRAGNN. For example, it is unclear whether certain architectures or tasks cannot be expressed within this framework. This lack of clarity diminishes confidence in the generality of the proposed approach.
Suggestions for Improvement
1. Clarify Unique Contributions: The authors should explicitly articulate what DRAGNN offers that existing frameworks do not. For example, are there specific tasks or architectures that cannot be implemented in TensorFlow or DyNet but can be expressed in DRAGNN? Concrete examples would strengthen the case for adoption.
   
2. Address Limitations: The paper should discuss the framework's limitations. Are there tasks or models that DRAGNN cannot handle effectively? This would provide a more balanced perspective and help readers understand its scope.
3. Provide Incentives for Adoption: The framework currently lacks "free things" that would incentivize practitioners to switch from established tools. For instance, does DRAGNN offer significant computational efficiency, ease of use, or pre-built components for common tasks? Highlighting these benefits would make the framework more appealing.
4. Empirical Comparisons: While the experiments demonstrate improvements, the paper should include comparisons with other modular frameworks (e.g., TensorFlow, DyNet) to contextualize the results. It would also be helpful to evaluate DRAGNN on a broader range of tasks to demonstrate its generality.
Questions for the Authors
1. Can you provide examples of neural architectures or tasks that cannot be effectively implemented in existing frameworks but can be expressed in DRAGNN?
2. What are the computational trade-offs of using DRAGNN compared to seq2seq with attention or recursive tree-structured models? Are there scenarios where DRAGNN is less efficient?
3. How does DRAGNN scale to larger datasets or more complex tasks? Have you tested it on tasks beyond dependency parsing and summarization?
In summary, while DRAGNN is an interesting effort to unify neural architectures, its contributions are more incremental than transformative. Addressing the above concerns could significantly strengthen the paper and its impact.