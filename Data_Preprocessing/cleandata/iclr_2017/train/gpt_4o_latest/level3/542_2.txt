Review of the Paper: "Policy Sketches for Hierarchical Multiagent Reinforcement Learning"
Summary of Contributions
This paper introduces a novel framework for multitask deep reinforcement learning (RL) using policy sketches, which are symbolic sequences that provide high-level structural guidance for task-specific policies. The authors propose a modular approach where each subtask in the sketch is associated with a reusable subpolicy, enabling transfer learning and temporal abstraction. The learning process is implemented using a decoupled actor-critic architecture, which allows the model to optimize subpolicies across diverse tasks. The framework is evaluated in two game-like environments—maze navigation and a Minecraft-inspired crafting game—demonstrating improved performance over baselines in terms of reward acquisition and convergence speed. The authors also show that the learned subpolicies can generalize to unseen tasks in both zero-shot and adaptation settings.
Decision: Reject
While the paper presents an interesting and promising approach, it falls short in several critical areas, particularly in its theoretical grounding and experimental rigor. These shortcomings prevent the paper from meeting the standards of acceptance at this time. Below are the key reasons for this decision:
Supporting Arguments for the Decision
1. Insufficient Depth in Policy Sketches:  
   The concept of policy sketches is novel but underdeveloped. The authors fail to establish a strong theoretical connection between policy sketches and established frameworks like semi-Markov decision processes (SMDPs) or the options framework. This lack of depth limits the interpretability and generalizability of the proposed approach.
2. Inadequate Experimental Comparisons:  
   The experimental evaluation does not adequately compare the proposed method to state-of-the-art hierarchical RL approaches, such as MAXQ-based methods. While the baselines used (e.g., independent and joint policies) are relevant, they are relatively simplistic and do not provide a comprehensive benchmark for assessing the method's true potential.
3. Empirical Validation of Claims:  
   While the results show that the proposed method outperforms baselines, the experiments lack sufficient diversity and scale. The environments used (maze and crafting) are relatively simple and do not convincingly demonstrate the scalability of the approach to more complex, real-world tasks. Furthermore, the ablation studies, while insightful, do not fully explore the limitations of the method, such as its sensitivity to sketch quality or its robustness to noisy or incomplete sketches.
Suggestions for Improvement
To strengthen the paper, the authors should consider the following:
1. Theoretical Grounding:  
   Provide a more rigorous theoretical framework for policy sketches, explicitly connecting them to SMDPs or other hierarchical RL models. This would enhance the interpretability and credibility of the approach.
2. Comprehensive Baseline Comparisons:  
   Include comparisons with more advanced hierarchical RL methods, such as MAXQ, options-based approaches, or other recent multitask RL frameworks. This would provide a clearer picture of the method's relative strengths and weaknesses.
3. Expanded Experimental Scope:  
   Evaluate the method in more complex and diverse environments, such as continuous control tasks or real-world robotics simulations. Additionally, consider testing the approach under more challenging conditions, such as noisy or incomplete sketches, to assess its robustness.
4. Clarify Scalability:  
   Discuss the computational complexity of the approach, particularly in terms of the number of subpolicies and tasks. Provide evidence that the method can scale to larger task sets without significant degradation in performance.
5. Address Generalization Limitations:  
   While the zero-shot and adaptation experiments are promising, they are limited in scope. The authors should explore how the method generalizes to tasks with significantly different structures or reward functions than those seen during training.
Questions for the Authors
1. How does the proposed method compare to MAXQ-based hierarchical RL approaches in terms of performance and scalability?  
2. Can the framework handle noisy or incomplete policy sketches? If so, how does it adapt to such scenarios?  
3. What are the computational trade-offs of the modular subpolicy approach, particularly as the number of tasks or subpolicies increases?  
4. How sensitive is the method to the quality of the sketches? Would suboptimal or overly long sketches significantly degrade performance?
Conclusion
While the paper introduces a novel and promising idea, the lack of theoretical depth, insufficient experimental comparisons, and limited scope of evaluation make it difficult to assess the true impact of the proposed approach. Addressing these issues would significantly improve the paper's quality and potential for acceptance in future iterations.