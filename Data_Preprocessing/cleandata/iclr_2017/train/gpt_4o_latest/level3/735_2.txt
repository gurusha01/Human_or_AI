Review of the Paper
Summary of Contributions
The paper proposes a novel approach to reformulate Kernel Principal Component Analysis (KPCA) by introducing rank constraints as a regularization term in an energy minimization framework. The authors claim that their method is the first to directly penalize the rank of the recovered factors in the implicit feature space, enabling robust KPCA in the presence of noise and missing data. They demonstrate the utility of their approach on two tasks: matrix completion and Non-Rigid Structure from Motion (NRSfM), reporting promising results on standard datasets. The paper also provides a closed-form solution for the dimensionality reduction subproblem and claims state-of-the-art performance in missing data prediction for the oil flow dataset.
Decision: Reject
The paper is rejected primarily due to a lack of clarity, focus, and insufficient justification of its claims. The following reasons underpin this decision:
1. Lack of Clarity and Focus: The writing is inconsistent, with the discussion shifting between unrelated topics such as causal factors, nonlinear dimensionality reduction, and ill-posed inverse problems without clear connections. Key terms like "causal factors" are not defined, and the purpose of certain steps (e.g., pre-image estimation in KPCA) is unclear.
2. Insufficient Theoretical and Empirical Justification: The proposed method lacks theoretical rigor and empirical comparisons with existing approaches. For instance, no comparisons are made with standard matrix completion algorithms (e.g., OptSpace, SVT) or established methods for NRSfM. Additionally, the proof of Theorem 3.1 is incomplete, with unclear steps that undermine its validity.
Supporting Arguments
1. Unclear Problem Formulation: The term "causal factors" is introduced in the abstract but is neither defined nor discussed in the problem formulation. Similarly, the notation used (e.g., $\mathcal{X} \times N$, $\mathcal{Y} \ll \mathcal{X}$) is ambiguous and undefined, making the mathematical formulation difficult to follow.
2. Lack of Novelty in Techniques: The reformulation of KPCA relies on standard techniques, such as trace norm regularization, without demonstrating clear advantages over existing methods. The absence of comparisons with baseline methods for matrix completion and NRSfM further weakens the contribution.
3. Experimental Gaps: The experiments are incomplete and fail to provide sufficient evidence for the claimed robustness and superiority of the proposed method. For example, the paper does not compare its results with standard matrix completion algorithms or other nonlinear dimensionality reduction techniques.
Additional Feedback for Improvement
1. Clarity and Focus: The paper would benefit from a more structured and focused presentation. Clearly define key terms like "causal factors" and provide a cohesive narrative connecting the problem, proposed solution, and experiments.
2. Notation and Definitions: Ensure that all notation is defined and unambiguous. For example, clarify the meaning of $\mathcal{Y} \ll \mathcal{X}$ and $\mathcal{S}^n$.
3. Theoretical Rigor: Address the gaps in the proof of Theorem 3.1, particularly the unclear steps involving Hölder's inequality and orthonormal constraints.
4. Empirical Comparisons: Include comparisons with standard methods for matrix completion (e.g., OptSpace, SVT) and NRSfM. This would provide a stronger empirical basis for the claims.
5. Purpose of Pre-Image Estimation: Justify the inclusion of pre-image estimation in the KPCA framework, as this is not standard practice and its relevance to the proposed method is unclear.
Questions for the Authors
1. What is the precise definition of "causal factors," and how does it relate to the proposed formulation?
2. Can you clarify the purpose of pre-image estimation in your KPCA reformulation? How does it contribute to solving the ill-posed inverse problem?
3. Why are standard matrix completion algorithms (e.g., OptSpace, SVT) and NRSfM methods not included in the experimental comparisons?
4. Can you provide a more detailed explanation of the steps from (16) to (17) in the proof of Theorem 3.1, particularly the application of Hölder's inequality?
By addressing these issues, the paper could significantly improve its clarity, rigor, and impact.