Review
This paper systematically investigates the performance and trainability of popular RNN architectures, addressing a critical question in the machine learning community: whether the observed superiority of gated RNNs (e.g., GRUs and LSTMs) arises from their inherent computational capacity or their trainability. The authors provide a thorough experimental analysis, leveraging Google's computational resources to conduct extensive hyperparameter tuning and training across multiple architectures, tasks, and parameter scales. The key contributions include clarifying the distinction between capacity and trainability, showing that all RNN architectures achieve nearly equivalent capacity when trained effectively, and highlighting the practical utility of GRUs in typical training environments. Additionally, the paper introduces two novel RNN variants, UGRNN and +RNN, with promising results for specific scenarios.
Decision: Accept
The paper makes a strong case for acceptance due to its significant contributions to understanding RNN architectures, its rigorous experimental methodology, and its practical insights for the community. The decisive findings on GRU performance and the emphasis on hyperparameter tuning are particularly impactful for practitioners. However, the preliminary nature of the contributions on UGRNN and +RNN, as well as some issues with replicability and clarity, slightly detract from the overall strength of the paper.
Supporting Arguments
1. Significance of Contributions: The paper addresses a fundamental question about RNN architectures, providing evidence that differences in performance stem from trainability rather than capacity. This insight challenges common assumptions and has practical implications for model selection and training strategies.
2. Experimental Rigor: The use of extensive hyperparameter tuning, large-scale computational resources, and diverse tasks ensures the robustness of the findings. The quantification of infeasible parameters and the distinction between per-parameter and per-unit capacity are innovative and valuable contributions.
3. Clarity and Practicality: The results are clearly presented, with actionable insights for practitioners, such as the recommendation to use GRUs for shallow architectures and +RNNs for deeper ones.
Suggestions for Improvement
1. Details on Hyperparameter Tuning: While the paper emphasizes the importance of hyperparameter tuning, the description of the tuning algorithm is insufficient for replication. Including more specifics or pseudocode in the appendix would enhance reproducibility.
2. Visual Clarity: Some figures, such as Figure 4, are visually cluttered and difficult to interpret. Simplifying these figures or providing additional explanations would improve readability.
3. Framing of Neuroscience Reference: The comparison to biological synapses feels tenuous and speculative. Reframing this discussion more cautiously would strengthen the scientific rigor of the paper.
4. UGRNN and +RNN Contributions: The results on UGRNN and +RNN are promising but feel preliminary. Further experiments and comparisons are needed to establish their utility and generalizability.
Questions for the Authors
1. Could you provide more details on the Gaussian Process-based hyperparameter tuning algorithm? Specifically, how were the kernel parameters and acquisition functions chosen, and how sensitive are the results to these choices?
2. The paper mentions that the +RNN performed best for deeper architectures. Could you elaborate on why this might be the case, and whether this trend holds across other tasks or datasets?
3. How do the findings on capacity and trainability generalize to other architectures, such as Transformer-based models, which are increasingly popular for sequence modeling tasks?
In conclusion, this paper makes a significant contribution to the understanding of RNN architectures and provides actionable insights for both researchers and practitioners. Addressing the suggested improvements would further enhance its impact.