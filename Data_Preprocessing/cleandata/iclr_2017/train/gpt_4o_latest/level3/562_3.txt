The paper introduces Generative Adversarial Parallelization (GAP), a novel framework for training multiple GANs in parallel by periodically swapping their discriminators. This approach aims to address challenges like mode collapse and improve convergence and mode coverage. The authors propose an improved Generative Adversarial Metric (GAM-II) to evaluate GANs under the GAP framework and present empirical results suggesting that GAP enhances generalization and mode coverage. The method is flexible and can be applied to various GAN architectures, making it a promising contribution to the field.
Decision: Reject
While the paper presents an interesting idea with potential, it falls short in several critical areas that limit its acceptance. The primary reasons for rejection are: (1) the lack of rigorous experimental validation and convincing qualitative results, and (2) insufficient theoretical grounding, particularly in comparing GAP to existing methods like dropout.
Supporting Arguments:
1. Single Generator Limitation: The reliance on the GAM metric to select the "best" generator is problematic. A single generator may fail to capture the full data distribution, and GAM scores are not robust enough to justify this selection. A more effective approach would involve combining all generators into a mixture model with appropriate weighting, such as rejection sampling, to better utilize the diversity of generators.
   
2. Comparison to Dropout: The conceptual similarity between GAP and dropout (where generators compete against all discriminators but drop N-1 at each epoch) is not adequately explored. A theoretical comparison would strengthen the paper's contributions and clarify its novelty relative to existing techniques.
3. Qualitative Results: The qualitative results are underwhelming, with limited baseline comparisons and visualizations that fail to convincingly demonstrate the benefits of GAP. Some figures, such as those using under-parameterized generators, detract from the paper's impact.
4. Algorithmic Clarity: The pseudocode in Algorithm 1 is incomplete, as the indices \(i_t\) are undefined. This oversight makes it difficult to reproduce the method and undermines the paper's rigor.
5. Figure 2 Issue: The inclusion of Figure 2, which has potential copyright issues and limited explanatory value, detracts from the paper's presentation. Removing it would improve clarity and focus.
Suggestions for Improvement:
1. Generator Mixture: Instead of selecting a single generator, explore combining outputs from all generators using a mixture model. This could address the single-generator limitation and improve mode coverage.
   
2. Dropout Comparison: Provide a detailed theoretical or empirical comparison between GAP and dropout to highlight the unique contributions of GAP.
3. Stronger Baselines: Include more comprehensive baseline comparisons and ensure that generators are adequately parameterized to showcase the full potential of GAP.
4. Algorithmic Clarity: Clearly define all variables in Algorithm 1 and provide additional details to ensure reproducibility.
5. Quantitative Mode Coverage: While the paper qualitatively addresses mode coverage, a quantitative metric would provide stronger evidence for GAP's effectiveness.
Questions for Authors:
1. How does GAP compare to dropout in terms of theoretical underpinnings and empirical performance? Could dropout achieve similar results with less complexity?
2. Why was the decision made to select a single generator based on GAM rather than combining outputs from all generators? Have alternative selection or combination strategies been explored?
3. Could you provide additional qualitative or quantitative evidence to demonstrate that GAP-trained models achieve better mode coverage in high-dimensional datasets?
In summary, while GAP is an intriguing idea, the paper requires significant improvements in experimental validation, theoretical grounding, and presentation to meet the standards of acceptance.