Review of the Paper
Summary
This paper introduces a framework for training and evaluating agents on their ability to seek information efficiently. The authors propose a set of tasks requiring agents to search partially observed environments for information fragments to achieve specific goals. They employ deep learning and reinforcement learning techniques, combining extrinsic and intrinsic rewards to shape agent behavior. The paper contributes to the discussion on information-seeking behavior by advocating for active information acquisition and demonstrating task-agnostic heuristics that improve task-specific performance. The authors present empirical results on synthetic and real-world datasets, showcasing the effectiveness of their approach in tasks like cluttered MNIST classification, positional reasoning, conditional classification, and Hangman.
Decision: Reject  
The paper explores an interesting and important research direction, but it falls short in several key areas that limit its impact and scientific rigor. Specifically, the lack of strong baselines, insufficient exploration of related work, and the overly simplified task settings reduce the generalizability and significance of the findings.
Supporting Arguments
1. Lack of Strong Baselines: The paper does not compare its approach to simpler models like frequency-based or n-gram models, which could provide valuable insights into the relative performance of the proposed framework. Without these comparisons, it is difficult to assess the true effectiveness of the method.
   
2. Simplified Task Settings: While the tasks are well-designed for controlled analysis, they rely on clean, noise-free environments with finite question sets. This reduces realism and limits the applicability of the findings to more complex, real-world scenarios where noise and ambiguity are prevalent.
3. Missed Opportunities in Related Work: The authors draw inspiration from games like 20 Questions and Battleships but do not directly explore these games or leverage existing studies on human performance in such settings (e.g., Cohen & Lake, 2016). This omission weakens the paper's connection to prior work and its ability to contextualize its contributions.
4. Limited Focus on Human Performance: The paper does not compare the agent's behavior to human strategies for information seeking, missing an opportunity to ground the results in cognitive science or behavioral benchmarks.
Suggestions for Improvement
1. Incorporate Strong Baselines: Include comparisons with simpler models, such as frequency-based or n-gram approaches, to establish a stronger foundation for the proposed method's effectiveness.
   
2. Explore Noisy and Ambiguous Settings: Extend the tasks to include noise and ambiguity, making the experiments more representative of real-world challenges.
3. Leverage Related Work: Integrate insights from studies on games like 20 Questions and Battleships, and compare the agent's performance to human benchmarks to strengthen the paper's connection to existing literature.
4. Expand Task Scope: Introduce tasks that involve natural language or more realistic environments to demonstrate the broader applicability of the framework.
Questions for the Authors
1. Why were games like 20 Questions and Battleships not directly explored, despite being cited as inspiration? How might these games inform task design or evaluation?
2. How does the framework perform in noisy or ambiguous environments? Could the clean settings be limiting the generalizability of the findings?
3. Have you considered comparing the agent's performance to human strategies for information seeking? If not, why?
4. Could simpler models, such as frequency-based or n-gram approaches, achieve comparable results on the proposed tasks? Why were such baselines not included?
In summary, while the paper addresses a compelling problem and presents promising initial results, it requires stronger baselines, more realistic task settings, and deeper engagement with related work to make a significant contribution to the field.