Review of the Paper
Summary of Contributions
This paper investigates the principle of identity parameterization in deep learning, focusing on its theoretical and empirical implications. The authors present two key theoretical results: (1) a simple proof that deep linear residual networks lack spurious local optima, and (2) a demonstration of the universal finite-sample expressivity of residual networks with ReLU activations. Inspired by these theoretical insights, the authors propose a simplified all-convolutional residual architecture that achieves competitive performance on CIFAR10, CIFAR100, and ImageNet benchmarks without using batch normalization, dropout, or max pooling. The work provides a compelling blend of theoretical rigor and practical experimentation, aiming to simplify deep learning architectures while maintaining state-of-the-art performance.
Decision: Reject
While the paper makes interesting contributions, it falls short in critical areas that limit its impact. The primary reasons for rejection are:
1. Insufficient clarity on the relevance of linear network analysis (Section 2): The connection between the optimization landscape of linear networks and non-linear networks is not adequately established. The authors need to clarify how the absence of spurious critical points in linear networks translates to practical benefits for non-linear networks.
2. Unclear necessity for new proofs in Section 3: The expressive power of residual networks is already comparable to feedforward networks. The paper does not convincingly justify why a new proof of finite-sample expressivity is necessary or how it advances the state of the art.
Supporting Arguments
1. Linear Network Analysis (Section 2): The results on the optimization landscape of linear residual networks are mathematically elegant. However, the authors do not provide sufficient evidence or discussion on how these results generalize to non-linear networks. The claim that deeper networks have better optimization landscapes is intriguing but requires stronger theoretical or empirical support.
2. Expressivity of Residual Networks (Section 3): While the construction of residual networks with finite-sample expressivity is interesting, it is not clear how this result differs significantly from existing results on feedforward networks. The necessity of the new proof is not well-motivated, and its practical implications are not thoroughly discussed.
3. Experimental Design (Section 4): The experiments are well-executed, and the use of random projection in the top layer is clever. However, the impact of this design choice is not isolated from the all-convolutional residual network architecture. Additional configurations, such as experiments with standard residual networks using random projections, would help clarify the contribution of this specific design choice.
Additional Feedback and Questions for the Authors
1. Batch Normalization Claim: The claim in the abstract that batch normalization reduces to the identity transformation is misleading without proper discussion. This should either be elaborated on or removed from the abstract.
2. Typographical Clarification: On page 5, the assumption "x^(i) = 1 ==> ||x^(i)||_2 = 1" is unclear. Please clarify whether this is a notational issue or a deeper assumption about the data.
3. Generalization to Non-Linear Networks: Can the authors provide more evidence or theoretical insights on how the results for linear networks extend to non-linear networks? This would significantly strengthen the paper's impact.
4. Expressivity Proof Motivation: Why is a new proof of finite-sample expressivity necessary? How does it advance our understanding of residual networks compared to existing results?
5. Impact of Random Projection: Can the authors conduct additional experiments to isolate the effect of random projection on the top layer? This would help clarify its role in the observed performance improvements.
Suggestions for Improvement
- Strengthen the discussion on the relevance of linear network results to non-linear networks.
- Provide a clearer motivation for the new proof of expressivity and its implications.
- Include additional experimental configurations to isolate the impact of specific design choices.
- Address minor issues such as the batch normalization claim and typographical ambiguities.
In summary, while the paper presents interesting theoretical and empirical contributions, the lack of clarity in key areas and insufficient justification for certain claims limit its overall impact. Addressing these issues could significantly improve the paper's quality and relevance.