Review
Summary of Contributions
This paper investigates memory-augmented neural language models, specifically focusing on key-value and key-value-predict attention mechanisms, alongside simpler near memory-less recurrent neural networks (RNNs). The authors propose a novel key-value-predict attention mechanism that separates the functions of output vectors into three distinct components: key, value, and next-word prediction. This decomposition improves performance on the Wikipedia corpus compared to baseline models and other memory-augmented architectures. A significant finding is that these models primarily utilize short-term memory (e.g., the last five tokens), and a simpler N-gram RNN achieves comparable results. The paper also contributes by releasing a new Wikipedia corpus for language modeling tasks. While the models perform well on the Wikipedia dataset, results on the Children's Book Test (CBT) are less convincing, particularly in demonstrating the benefits of model variations. The authors raise important questions about modeling spans in language and provide a foundation for future research into long-range dependencies.
Decision: Accept
The paper makes a meaningful contribution to the field of neural language modeling by introducing a novel attention mechanism and rigorously comparing it with simpler and more complex alternatives. The key reasons for acceptance are:
1. Novelty and Insight: The key-value-predict decomposition is a novel idea that addresses the overloading of output representations in traditional attention mechanisms, and the finding that short-term memory suffices for strong performance is both surprising and impactful.
2. Empirical Rigor: The experiments are thorough, with comparisons across multiple architectures and datasets, and the results are well-supported by empirical evidence.
Supporting Arguments
1. Well-Motivated Approach: The proposed key-value-predict mechanism is grounded in prior work on memory-augmented architectures and addresses a clear limitation of existing models. The motivation to explore simpler alternatives like N-gram RNNs is also commendable, as it challenges the assumption that more complex architectures are always better.
2. Strong Results on Wikipedia Corpus: The models demonstrate clear improvements over baselines, with the key-value-predict mechanism achieving the best perplexity scores. The release of the Wikipedia corpus is a valuable contribution to the community.
3. Clarity and Accessibility: The paper is well-written, with clear explanations of the methods and results. It raises important questions about the challenges of modeling long-range dependencies, providing a strong foundation for future work.
Suggestions for Improvement
1. Long-Range Dependencies: While the paper acknowledges the difficulty of modeling long-range dependencies, it does not propose concrete solutions. Future work could explore techniques to encourage attention over longer histories, such as penalizing attention to recent tokens or explicitly modeling hierarchical structures in text.
2. Analysis of Token Reoccurrence and Sentence Length: The authors mention that analyzing these factors could approximate optimal attention window lengths, but no such analysis is presented. Including this would strengthen the paper's insights.
3. CBT Results: The results on CBT are less compelling, particularly in demonstrating the benefits of the proposed architectures. Additional experiments or analysis could clarify why the models struggle with this dataset and whether task-specific modifications are necessary.
Questions for the Authors
1. Could you elaborate on why the models fail to leverage long-range dependencies effectively? Are there specific architectural or training challenges that you observed?
2. Did you explore varying the attention window size dynamically based on sentence length or token reoccurrence patterns? If so, what were the findings?
3. How does the performance of the proposed models compare to state-of-the-art language models on other datasets beyond Wikipedia and CBT?
In conclusion, this paper provides valuable insights into memory-augmented neural language models, introduces a novel attention mechanism, and challenges assumptions about the necessity of complex architectures. While there are areas for improvement, the contributions are significant enough to warrant acceptance.