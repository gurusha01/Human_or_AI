Review of the Paper
Summary of Contributions
The paper proposes two pruning methods—feature map pruning and kernel pruning—to reduce the computational complexity of deep convolutional neural networks (CNNs). The authors introduce a simple, generic strategy for selecting pruning masks using random sampling, aiming to minimize accuracy degradation. The proposed methods are evaluated on datasets such as MNIST, CIFAR-10, CIFAR-100, and SVHN, with results showing that 60-70% sparsity can be achieved in convolutional layers with minimal accuracy loss. The paper also includes GPU-based implementation and highlights the potential benefits of the proposed pruning granularities for VLSI and FFT-based implementations.
Decision: Reject
The paper is rejected primarily due to the lack of novelty and scalability in the proposed pruning methods, as well as insufficient experimental validation on large-scale datasets and practical hardware platforms.
Supporting Arguments
1. Lack of Novelty: The proposed pruning methods rely on random sampling for mask selection, which is a simplistic and well-explored approach in the literature. The paper does not introduce significant innovations beyond existing pruning techniques, such as those by Han et al. (2015) or Li et al. (2016). While the authors argue that their method is generic and holistic, this claim is not sufficiently substantiated with theoretical or empirical evidence.
2. Scalability and Practicality: The experiments are limited to small-scale datasets (e.g., MNIST, CIFAR-10, CIFAR-100) and relatively shallow networks. The paper does not evaluate the proposed methods on large-scale datasets like ImageNet or on deeper, more complex architectures such as ResNet or EfficientNet. Moreover, the practical time consumption of the pruning process on parallel platforms like GPUs is not adequately addressed, undermining the claim of computational efficiency.
3. Questionable Necessity of Pruning: The necessity of feature map pruning is debatable, as training a smaller network from scratch might achieve similar accuracy without the added complexity of pruning and retraining. The authors do not compare their approach to this baseline, leaving a critical gap in the evaluation.
4. Incomplete Validation: While the inclusion of GPU implementation is appreciated, the paper does not compare its efficiency against established frameworks like TensorFlow or Caffe. Additionally, the results focus on theoretical complexity reduction (e.g., sparsity) rather than actual runtime improvements, which are crucial for practical adoption.
Suggestions for Improvement
1. Novelty and Methodology: The authors should explore more sophisticated pruning strategies, such as those based on gradient sensitivity, information theory, or reinforcement learning, to enhance the novelty of their approach.
2. Scalability: Future work should evaluate the proposed methods on large-scale datasets (e.g., ImageNet, Places) and deeper architectures to demonstrate scalability and practical relevance.
3. Efficiency Metrics: The paper should include runtime benchmarks on GPUs or other hardware platforms to validate the claimed computational benefits. Comparisons with existing frameworks and pruning methods would strengthen the evaluation.
4. Baseline Comparisons: The authors should compare their approach to training smaller networks from scratch to justify the necessity of pruning.
Questions for the Authors
1. How does the proposed method perform on large-scale datasets like ImageNet or deeper architectures such as ResNet?
2. Can the authors provide runtime benchmarks on GPUs or other hardware platforms to validate the claimed computational efficiency?
3. How does the proposed method compare to training a reduced-size network from scratch in terms of accuracy and computational cost?
In summary, while the paper addresses an important problem in deep learning, the lack of novelty, scalability, and practical validation limits its contribution. Addressing these issues could significantly improve the paper's impact and relevance.