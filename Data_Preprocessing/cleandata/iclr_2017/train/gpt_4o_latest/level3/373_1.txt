The paper proposes transfer learning variants for neural network-based models applied to NLP tagging tasks, focusing on cross-domain, cross-application, and cross-lingual transfer. It introduces three parameter-sharing architectures (T-A, T-B, T-C) for hierarchical recurrent neural networks, aiming to improve sequence tagging performance, particularly in low-resource settings. The authors claim that their approach achieves significant improvements under limited labeled data and sets new state-of-the-art results on some benchmarks. However, the improvements diminish with abundant labeled data or loosely related tasks. The paper also highlights the importance of factors like task relatedness and parameter sharing in transfer learning effectiveness.
Decision: Reject
Key Reasons:
1. Limited Novelty: While the paper provides a specific architecture setup for NLP tagging tasks, the overall approach lacks novelty in the broader machine learning context. The proposed parameter-sharing methods are incremental extensions of existing transfer learning techniques.
2. Underwhelming Results: The experimental results show limited improvements in high-resource scenarios, and the fixed architecture size in Figure 2 raises concerns about the validity of the reported outcomes.
Supporting Arguments:
The paper is well-written and clearly articulates its goals and methods. However, the novelty lies primarily in the application-specific architecture design rather than a fundamentally new transfer learning methodology. The experimental results, while promising in low-resource settings, fail to demonstrate substantial gains in more general scenarios. Additionally, the fixed architecture size in Figure 2 might have skewed the results, as it does not account for the varying complexity of tasks, which could lead to misleading conclusions about the effectiveness of the proposed methods.
Suggestions for Improvement:
1. Clarify Novelty: The authors should better position their work within the existing literature, explicitly highlighting how their approach differs from and advances prior work.
2. Address Experimental Limitations: The fixed architecture size in Figure 2 should be revisited, and experiments should be conducted with task-specific architecture adjustments to ensure fair comparisons.
3. Expand Analysis: The paper could benefit from a deeper analysis of why the improvements diminish in high-resource settings and how this limitation could be addressed.
4. Broader Evaluation: Including comparisons with other state-of-the-art transfer learning methods, particularly in low-resource scenarios, would strengthen the empirical claims.
Questions for the Authors:
1. How does the proposed approach compare to other recent transfer learning methods in NLP, such as those leveraging pre-trained language models like BERT or GPT?
2. Can the authors provide additional experiments with varying architecture sizes to address the concerns raised about Figure 2?
3. How do the proposed architectures perform when combined with resource-based transfer methods, as mentioned in the conclusion?
While the paper tackles an important problem and demonstrates potential in low-resource settings, the lack of significant methodological novelty and the concerns around experimental rigor make it unsuitable for acceptance in its current form. Addressing the outlined issues could make it a stronger contribution.