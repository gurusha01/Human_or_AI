The paper proposes a method for pruning entire groups of filters in convolutional neural networks (CNNs) to reduce computational costs without introducing sparse connectivity. This approach is particularly advantageous as it maintains compatibility with standard dense linear algebra routines, avoiding the need for specialized sparse libraries or hardware. The authors demonstrate that their method can achieve significant reductions in inference costs—up to 34% for VGG-16 and 38% for ResNet-110 on CIFAR-10—while maintaining accuracy through retraining. The paper also highlights the practicality of one-shot pruning and retraining strategies, which simplify implementation and reduce retraining time. The results include a 10% improvement in ResNet-like models on ImageNet, showcasing the method's effectiveness for both simple and complex architectures.
Decision: Accept
The paper is well-motivated, provides practical contributions, and demonstrates scientifically rigorous results. The key reasons for acceptance are:
1. Practical Impact: The proposed method significantly reduces computational costs without sacrificing accuracy, making it highly relevant for resource-constrained environments like mobile devices.
2. Compatibility: By avoiding sparse connectivity, the method ensures compatibility with existing dense computation libraries, which is a critical advantage for real-world deployment.
Supporting Arguments:
1. Clear Problem Statement: The paper addresses a well-defined problem: reducing the computational overhead of CNNs without introducing sparsity. This is a timely issue given the increasing deployment of deep learning models in resource-constrained settings.
2. Empirical Validation: The results are robust and scientifically rigorous, with experiments conducted on multiple architectures (VGG-16, ResNet-56/110/34) and datasets (CIFAR-10, ImageNet). The sensitivity analysis and comparison with alternative pruning methods further strengthen the claims.
3. Simplicity and Generality: The method is simple to implement, does not require additional regularization, and can be applied to a wide range of architectures.
Suggestions for Improvement:
1. Comparison with Modern Architectures: The paper lacks comparisons with newer, more efficient architectures like MobileNet or EfficientNet. Including such comparisons would provide a clearer understanding of the method's relative benefits.
2. Broader Evaluation: While the method is tested on CIFAR-10 and ImageNet, additional datasets or tasks (e.g., object detection or segmentation) could demonstrate its generalizability.
3. Ablation Studies: Although the paper compares pruning strategies, more detailed ablation studies on the retraining process (e.g., varying retraining epochs or learning rates) could provide insights into optimizing the method.
4. Theoretical Insights: While the empirical results are strong, a deeper theoretical analysis of why the `l1`-norm criterion works well for filter pruning could enhance the paper's scientific contribution.
Questions for the Authors:
1. How does the method perform on architectures specifically designed for efficiency, such as MobileNet or EfficientNet? Are the gains still significant?
2. Could the proposed pruning strategy be combined with other compression techniques, such as quantization or knowledge distillation, to achieve further improvements?
3. How does the method handle transfer learning scenarios where pre-trained models are fine-tuned on smaller datasets? Does pruning before fine-tuning affect performance?
Overall, this paper makes a meaningful contribution to the field, and with minor improvements, it could have even broader impact.