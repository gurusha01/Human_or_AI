The paper introduces GRU-D, a novel GRU-based recurrent neural network model designed to address supervised multivariate time series tasks with missing values. By incorporating masking and time interval representations directly into the GRU architecture, the model captures informative missingness patterns and temporal dependencies. The authors propose a decay mechanism for both input variables and hidden states, which is particularly insightful for healthcare applications, where missing data often carries meaningful information. The experiments demonstrate GRU-D's superiority over baseline models in predictive tasks using synthetic and real-world clinical datasets, providing valuable insights into handling missing data in time series analysis.
Decision: Reject
While the paper presents an interesting and practical approach to handling missing values in multivariate time series, the decision to reject is based on two primary reasons: (1) limited novelty in the proposed method, and (2) concerns about the generalizability and scalability of the approach.
Supporting Arguments:
1. Limited Novelty: The primary contribution of the paper lies in adding a trainable decay mechanism to the GRU architecture. While this is a thoughtful and practical enhancement, it is incremental rather than groundbreaking. The paper does not sufficiently differentiate itself from existing works that incorporate missingness patterns into RNNs, such as GRU-simple and related baselines.
2. Dataset Limitations: The experiments are conducted on relatively small datasets (e.g., MIMIC-III and PhysioNet), which raises concerns about the robustness and scalability of the proposed method. The lack of evaluation on larger, more diverse datasets limits the confidence in the model's applicability to broader domains.
3. Generalizability of the Decay Mechanism: The decay mechanism, while effective in healthcare applications, may not generalize well to other domains where the relationship between missingness and target labels is less structured or less informative. This limits the broader impact of the work.
Suggestions for Improvement:
1. Expand the Scope of Experiments: Evaluate GRU-D on larger and more diverse datasets from different domains (e.g., finance, geoscience) to demonstrate its generalizability and scalability.
2. Theoretical Analysis: Provide a more rigorous theoretical justification for the decay mechanism and its impact on model performance. This could strengthen the paper's contribution beyond empirical results.
3. Comparison with Advanced Baselines: Include comparisons with more recent and advanced methods for handling missing values in time series data to better contextualize the contribution.
Questions for the Authors:
1. How does the proposed decay mechanism perform in domains where missingness is not strongly correlated with the target labels? Have you tested GRU-D in such scenarios?
2. Could the authors elaborate on the computational cost of GRU-D compared to baseline models? How does the added complexity of trainable decays impact training and inference times?
3. Have you considered alternative formulations for the decay mechanism, such as domain-specific priors or non-monotonic decay functions?
In summary, while GRU-D is a promising approach with practical applications in healthcare, the paper requires further work to address concerns about novelty, generalizability, and scalability. These improvements could significantly enhance the impact and acceptance of the work in future submissions.