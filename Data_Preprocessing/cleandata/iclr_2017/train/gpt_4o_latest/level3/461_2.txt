Review of the Paper
Summary of Contributions
This paper introduces a novel approach to semi-supervised learning, termed self-ensembling, which leverages stochastic augmentation and regularization to create ensemble predictions. The authors propose two implementations: the Π-model and temporal ensembling. These methods aim to improve the accuracy of predictions on unlabeled data by using ensemble predictions as training targets. The paper demonstrates state-of-the-art performance on standard benchmarks (CIFAR-10 and SVHN) and highlights the robustness of the proposed methods to label noise. The inclusion of a public codebase and detailed reproducibility instructions is commendable, as is the insightful discussion on the role of data augmentation. However, certain claims, such as achieving 30% accuracy on SVHN with 90% random labels, lack sufficient experimental details. The paper also raises scalability concerns for larger datasets and highlights sensitivity to architectural choices such as dropout placement.
Decision: Accept
The paper makes a significant contribution to semi-supervised learning by proposing a simple yet effective method that achieves state-of-the-art results. The robustness to label noise and the detailed reproducibility efforts further strengthen its impact. However, the authors should address the concerns raised about scalability, experimental clarity, and sensitivity to architecture to enhance the paper's rigor.
Supporting Arguments
1. Strengths:
   - The proposed methods (Π-model and temporal ensembling) are well-motivated and grounded in prior literature, building on ideas like dropout regularization and pseudo-ensembling.
   - The empirical results are compelling, with substantial improvements over prior methods on CIFAR-10 and SVHN benchmarks.
   - The robustness to label noise is a valuable property, particularly for real-world applications with noisy datasets.
   - The paper is well-written, includes reproducibility details, and provides a public codebase, which is a strong commitment to open science.
   - The discussion on the impact of data augmentation choices is insightful and provides practical guidance for researchers.
2. Weaknesses:
   - The claim of achieving 30% accuracy on SVHN with 90% random labels is unintuitive and lacks experimental details in the codebase, which raises concerns about reproducibility.
   - Scalability to larger datasets like ImageNet is not thoroughly addressed, particularly regarding memory and training time, which could limit the method's applicability.
   - The sensitivity of the methods to architectural choices, such as dropout placement, is mentioned but not explored in depth.
   - The discussion of the unsupervised loss weighting function \( w(t) \), which plays a critical role in label noise tolerance, is relegated to the appendix and should be included in the main paper.
Additional Feedback
1. Experimental Clarity: The authors should provide more details about the experimental setup for the 90% random label test on SVHN. Specifically, how was the randomization performed, and how consistent are the results across multiple runs?
2. Scalability: Addressing memory and computational requirements for larger datasets like ImageNet would strengthen the paper. Can temporal ensembling be adapted to reduce memory overhead further?
3. Dropout Sensitivity: A more detailed analysis of how dropout placement affects performance would be valuable. Are there specific architectural patterns that consistently yield better results?
4. Discussion of \( w(t) \): The unsupervised loss weighting function \( w(t) \) is critical to the method's success. A more prominent discussion in the main paper would help readers understand its impact.
5. Minor Correction: The grammatical error "without neither" should be corrected to "without either."
Questions for the Authors
1. Can you provide more experimental details or ablation studies for the 90% random label test on SVHN to clarify the unintuitive results?
2. How does the method scale to larger datasets like ImageNet in terms of memory and training time? Are there potential optimizations to address these concerns?
3. Have you explored the use of alternative regularization techniques or augmentation strategies beyond those discussed in the paper? How do they impact performance?
In conclusion, while the paper has some areas for improvement, its contributions to semi-supervised learning are significant and well-supported by empirical results. Addressing the raised concerns would further solidify its impact.