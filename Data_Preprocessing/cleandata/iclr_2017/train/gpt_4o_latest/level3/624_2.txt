Review of the Paper
Summary of Contributions
This paper investigates the error surface of deep rectifier networks, focusing on the existence of local minima and their implications for learning dynamics. The authors construct theoretical and empirical counterexamples to demonstrate that local minima can hinder learning, particularly under specific dataset characteristics and initialization schemes. The paper challenges the widely held belief that deep networks avoid bad local minima due to their high-dimensional error surfaces. It provides insights into how initialization and data structure interact with the error surface, emphasizing that learning dynamics are not universally well-behaved. The authors also propose that rectifier networks may have "blind spots," which can lead to suboptimal learning outcomes. The paper contributes to the broader understanding of deep learning optimization by highlighting failure modes and offering theoretical constructions to support these claims.
Decision: Reject
While the paper provides interesting insights and examples, it suffers from several technical and conceptual shortcomings that undermine its contributions. The key reasons for rejection are:
1. Unconvincing Claims: The assertion that the error surface structures are independent of datasets and model parameters is not well-supported. The examples provided often rely on contrived constructions that do not generalize to practical scenarios.
2. Known Results: The paper's emphasis on bad initialization as a cause of learning failure is already well-established in the literature, even for simpler models like linear networks. This limits the novelty of the work.
Supporting Arguments
1. The paper provides useful examples of training failures, but these examples often target theoretical edge cases rather than practical scenarios. For instance, the constructed datasets and initialization schemes are highly specific and unlikely to occur in real-world applications.
2. The claim that error surface structures are independent of datasets and model parameters is not rigorously demonstrated. The results appear to depend heavily on the specific constructions used, which weakens the generalizability of the findings.
3. The distinction between local minima and saddle points is insufficiently addressed. This is a critical oversight, as much of the literature suggests that saddle points, rather than local minima, are the primary obstacle in high-dimensional optimization.
4. Proposition 5's proof appears to contain an error regarding the probability of learning failure as the number of hidden units increases. This undermines the theoretical rigor of the paper.
5. The explanation of Figure 2 is counterintuitive, and the observed scaling effects may be misattributed to algorithmic limitations rather than the underlying error surface structure.
Suggestions for Improvement
1. Clarify Claims: The authors should provide stronger evidence to support their claim that error surface structures are independent of datasets and model parameters. This could involve more realistic datasets and initialization schemes.
2. Simplify Examples: Instead of using complex counterexamples, the authors could use simpler, more intuitive examples to demonstrate training failures.
3. Address Initialization: While the paper acknowledges the importance of initialization, it does not adequately explore modern initialization techniques (e.g., He or Xavier initialization) that mitigate many of the issues discussed.
4. Differentiate Local Minima and Saddle Points: The authors should explicitly address the distinction between these two phenomena and their respective impacts on learning dynamics.
5. Revisit Proposition 5: The proof for Proposition 5 should be carefully reviewed and corrected if necessary.
Questions for the Authors
1. How do the results generalize to real-world datasets and architectures, given that the examples provided are highly specific?
2. Can the authors provide empirical evidence that their findings hold for modern deep learning practices, such as large-scale datasets and advanced optimizers like Adam or RMSProp?
3. How do the authors reconcile their findings with the extensive empirical success of deep learning, where bad local minima rarely seem to hinder performance?
In summary, while the paper raises important questions about the error surface of deep networks, its reliance on contrived examples, lack of novelty, and technical issues limit its impact. Addressing these concerns could significantly strengthen the work.