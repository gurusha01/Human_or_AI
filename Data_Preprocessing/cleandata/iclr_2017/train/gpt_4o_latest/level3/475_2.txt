Review of the Paper
Summary of Contributions
This paper introduces a novel method for training deep generative models with discrete latent variables, leveraging the reparameterization trick to overcome the challenges of backpropagation through discrete variables. The proposed approach, termed discrete variational autoencoders (discrete VAEs), combines an undirected discrete component (RBM-like) with a directed hierarchical continuous component. This hybrid architecture enables the model to capture both discrete class-level information and continuous variations within classes. The authors achieve state-of-the-art results on datasets such as permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes. The paper is well-written, with detailed appendices that justify design decisions and provide theoretical and experimental insights.
Decision: Accept
The paper should be accepted due to its significant contributions to the field of generative modeling, particularly in handling discrete latent variables efficiently. The method demonstrates strong empirical performance and is supported by rigorous theoretical underpinnings. However, there are areas where additional analysis and clarification would strengthen the work.
Supporting Arguments
1. Novelty and Impact: The paper addresses a long-standing challenge in training models with discrete latent variables by introducing a reparameterization-based solution. This is a meaningful contribution that bridges the gap between discrete and continuous latent variable models.
2. Empirical Results: The reported log-likelihood results are compelling, outperforming existing methods on several benchmarks. This demonstrates the practical utility of the proposed approach.
3. Clarity and Rigor: The writing is precise, and the appendices provide comprehensive justifications for architectural and algorithmic choices. This level of detail enhances the paper's credibility and reproducibility.
Suggestions for Improvement
1. Gradient Variance Concerns: The paper raises concerns about gradient variance in the general formulation of the algorithm. While Appendix A addresses this issue, a more explicit discussion in the main text would help users understand the practical implications and limitations of the method.
2. Computational Efficiency: The sequential nature of inference in the RBM-like model may lead to inefficiencies. Although Appendix A proposes solutions, a quantitative comparison of computational costs with alternative architectures would be valuable.
3. Ablation Studies: The experiments lack sufficient analysis isolating the contributions of the architecture (e.g., RBM vs. hierarchical continuous layers) and the inference algorithm. Adding ablation studies would clarify the importance of each component.
4. Reference Issue: Section 3's second paragraph mentions Appendix A but does not provide a direct reference. This should be corrected for clarity.
Questions for the Authors
1. How does the proposed method scale to larger and more complex datasets, such as ImageNet? Have you considered extending the RBM to handle such datasets?
2. Could you provide more insights into the trade-offs between the hierarchical approximating posterior and a simpler factorial posterior? Specifically, how does the added complexity affect training stability and convergence?
3. Have you explored alternative smoothing transformations for the discrete-to-continuous latent space mapping beyond those discussed in the appendices? How sensitive are the results to the choice of transformation?
Overall, this paper makes a strong contribution to the field and warrants acceptance, provided the authors address the minor concerns and expand on the suggested areas for improvement.