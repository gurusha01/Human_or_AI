Review of the Paper
Summary of Contributions
The paper introduces Neural Equivalence Networks (EQNETs), a novel architecture for learning continuous semantic representations (SEMVECs) of symbolic expressions. The authors aim to address the challenge of representing semantic equivalence across syntactically diverse expressions, a problem central to combining symbolic reasoning with continuous neural representations. The proposed EQNET architecture builds on recursive neural networks (TREENNs) with key innovations, including multi-layer residual-like networks, output normalization, and a novel subexpression forcing (SUBEXPFORCE) loss. The paper provides an extensive evaluation across diverse datasets of boolean and polynomial expressions, demonstrating that EQNETs outperform existing baselines such as TREENNs, GRUs, and stack-augmented RNNs. The authors also evaluate the compositionality of EQNETs and their ability to generalize to unseen equivalence classes.
Decision: Reject
While the paper presents a promising approach and demonstrates strong empirical results, several issues limit its overall contribution. The primary concerns are (1) insufficient motivation and analysis of the SUBEXPFORCE loss, (2) the use of a non-standard evaluation metric that may skew results, and (3) the lack of a direct ablation study comparing models with and without SUBEXPFORCE under consistent conditions. These issues undermine the clarity and rigor of the claims.
Supporting Arguments for the Decision
1. Motivation for SUBEXPFORCE Loss:  
   While SUBEXPFORCE is a central component of EQNET, its motivation is not well-articulated. The paper briefly mentions that it encourages clustering of representations within equivalence classes, but the theoretical justification for why this specific loss is necessary or optimal is lacking. Additionally, the authors do not explore alternative clustering or regularization methods for comparison.
2. Evaluation Metric:  
   The use of "precision per query" as the primary evaluation metric is non-standard. While it provides some insight into the model's performance, it is unclear how it compares to more established metrics like precision-recall curves or ROC-AUC. Furthermore, the chosen metric may favor larger equivalence classes, potentially biasing the results. This issue is acknowledged but not adequately addressed in the paper.
3. Ablation Study:  
   The paper does not include a direct comparison between EQNETs with and without SUBEXPFORCE while keeping other components (e.g., residual connections, normalization) constant. This omission makes it difficult to isolate the contribution of SUBEXPFORCE to the observed performance gains.
4. Generalization to Real-World Tasks:  
   While the datasets used for evaluation are diverse, they are synthetic and limited to boolean and polynomial expressions. The paper does not discuss how EQNETs might generalize to real-world tasks involving more complex symbolic reasoning or procedural knowledge.
Suggestions for Improvement
1. Clarify and Justify SUBEXPFORCE:  
   Provide a stronger theoretical or empirical motivation for the SUBEXPFORCE loss. Compare it to alternative clustering or regularization methods to demonstrate its necessity and effectiveness.
2. Adopt Standard Metrics:  
   Replace or supplement "precision per query" with standard metrics like precision-recall curves or ROC-AUC. This would make the results more interpretable and comparable to other work.
3. Conduct Ablation Studies:  
   Include a direct comparison of EQNETs with and without SUBEXPFORCE, as well as with and without normalization, to isolate the contributions of each component.
4. Broaden Evaluation:  
   Test EQNETs on real-world datasets or tasks to demonstrate their practical applicability and generalization beyond synthetic settings.
Questions for the Authors
1. Why was SUBEXPFORCE chosen over other clustering or regularization techniques? Could alternative methods (e.g., contrastive loss) achieve similar results?
2. How does the choice of "precision per query" affect the reported results, especially for datasets with large equivalence classes?
3. Could you provide a detailed ablation study to isolate the contributions of SUBEXPFORCE, normalization, and residual connections?
4. How do EQNETs perform on real-world symbolic reasoning tasks, such as theorem proving or program synthesis?
In conclusion, while the paper presents an interesting approach with promising results, the issues outlined above need to be addressed to strengthen the contribution and ensure the claims are well-supported.