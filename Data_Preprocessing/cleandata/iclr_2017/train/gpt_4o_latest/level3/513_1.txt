Review
This paper explores the potential of unsupervised learning to enhance generalization in physical intuition tasks, specifically predicting the stability of block towers. The authors propose a novel approach where unsupervised models predict future video frames, and these predictions are used to improve a supervised stability prediction model. Two unsupervised architectures, ConvDeconv and ConvLSTMDeconv, are evaluated for their ability to generalize to out-of-domain data with different block counts. The results demonstrate that the generated data improves generalization performance, particularly when testing on towers with more blocks than those seen during training. This work contributes to the broader goal of leveraging unsupervised learning for tasks requiring physical reasoning and generalization.
Decision: Reject
While the paper presents a promising direction and a novel approach to evaluating generalization in unsupervised learning, it lacks the experimental rigor and depth required for acceptance. The primary concerns are overfitting, insufficient experimental analysis, and an overly simplified prediction task, which collectively weaken the paper's conclusions.
Supporting Arguments:
1. Overfitting Concern: The models, trained on 3-block towers, may overfit by focusing only on the bottom three blocks in larger towers. This raises doubts about whether the observed generalization is genuine or an artifact of the training setup. More rigorous testing, such as occlusion experiments or attention visualizations, is needed to confirm true generalization.
2. Experimental Analysis: The analysis is preliminary and lacks clarity. For example, visualizations of the generated frames and their impact on stability prediction are limited, making it difficult to assess the quality of the learned representations. Additionally, the testing conditions are not sufficiently detailed, and the dataset size is relatively small, which may limit the robustness of the results.
3. Simplified Prediction Task: The binary stability prediction task oversimplifies the problem. A more nuanced task, such as predicting the number of fallen blocks or higher frame rate sequence prediction, could better evaluate the models' generalization capabilities.
Additional Feedback:
1. Dataset and Testing: Provide more details about the dataset, including its diversity and how well it represents real-world variability. Consider testing on entirely unseen datasets to further validate generalization.
2. Visualization and Interpretability: Include visualizations of the models' attention or feature maps to better understand what aspects of the towers the models focus on. This could help address the overfitting concern.
3. Evaluation Metrics: Explore additional evaluation metrics beyond binary accuracy, such as precision, recall, or metrics that capture the degree of instability.
4. Acronym Expansion and Formatting: Expand the acronym "IPE" for clarity, and improve the formatting of Table 3 for better readability.
Questions for the Authors:
1. How do you ensure that the models are not overfitting to specific features of the training data, such as focusing only on the bottom blocks?
2. Have you considered testing the models on real-world block configurations or datasets from other sources to validate generalization?
3. Why was a binary stability prediction chosen over a more complex task, such as predicting the number of fallen blocks or the trajectory of collapse?
In summary, while the paper addresses an important problem and presents a novel approach, it requires more rigorous experiments, detailed analysis, and task complexity to substantiate its claims. With these improvements, the work could become a significant contribution to the field.