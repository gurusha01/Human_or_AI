Review
Summary
This paper introduces a convolutional network (ConvNet)-based encoder for neural machine translation (NMT) as an alternative to the widely used recurrent neural network (RNN)-based encoders. The authors propose a novel architecture with two convolutional stacks: one for computing attention scores (CNN-a) and another for aggregating representations (CNN-c). The paper demonstrates that this dual-stack convolutional encoder achieves competitive or superior performance compared to bi-directional LSTM (BiLSTM) encoders across multiple datasets, including WMT'16 English-Romanian, WMT'15 English-German, and WMT'14 English-French. Additionally, the ConvNet-based model is shown to be significantly faster during evaluation, with a speed-up of up to 2.1x on CPUs. The experimental evaluations are thorough, and the results are presented with clarity, highlighting the effectiveness of the proposed approach.
Decision: Reject
While the paper provides a well-executed empirical study and introduces a novel architectural design, the contribution is incremental and application-specific. The lack of theoretical insights into the necessity of the dual-stack design and the unclear source of the evaluation speed-up limit the broader impact of the work. Furthermore, the paper appears more suited for NLP-focused conferences like ACL or EMNLP rather than ICLR, which emphasizes foundational advancements in machine learning.
Supporting Arguments
1. Strengths:
   - The proposed convolutional encoder is innovative, with the dual-stack design addressing the distinct requirements of attention computation and representation aggregation.
   - The experimental evaluation is comprehensive, covering multiple datasets and comparing against strong baselines. The results convincingly demonstrate that the ConvNet-based encoder can match or outperform BiLSTM encoders in terms of BLEU scores.
   - The speed-up during evaluation is a notable practical advantage, especially for resource-constrained environments.
2. Weaknesses:
   - The paper lacks a theoretical explanation for the necessity of the dual-stack design. While empirical evidence supports its effectiveness, a deeper understanding of why this architecture works would significantly enhance the contribution.
   - The source of the evaluation speed-up is not fully explained. Although the authors hypothesize better cache locality, this claim is not rigorously analyzed or substantiated.
   - The contribution is primarily application-specific, targeting NMT tasks. The broader implications for other sequence-to-sequence tasks or machine learning domains are not explored in depth.
   - The work is incremental, building on existing convolutional and recurrent architectures without introducing fundamentally new concepts.
Suggestions for Improvement
1. Theoretical Insights: Provide a theoretical or intuitive explanation for the necessity of the dual-stack design. Why do CNN-a and CNN-c need to be separate, and what are the underlying principles that make this architecture effective?
2. Speed-Up Analysis: Conduct a detailed analysis of the sources of the evaluation speed-up. For example, compare FLOPs, memory access patterns, and cache utilization between the ConvNet and BiLSTM models.
3. Broader Applicability: Explore the applicability of the proposed architecture to other sequence-to-sequence tasks, such as summarization or dialogue modeling, to demonstrate its generalizability.
4. Positioning: Consider submitting the paper to an NLP-focused conference like ACL or EMNLP, where the application-specific contributions may be more appreciated.
Questions for the Authors
1. Why is the dual-stack design necessary? Could a single convolutional stack with shared parameters achieve similar results?
2. Can you provide more details on the source of the evaluation speed-up? Is it solely due to better cache locality, or are there other contributing factors?
3. How does the proposed architecture perform on tasks outside of NMT, such as summarization or parsing? Have you conducted any preliminary experiments in this direction?
In conclusion, while the paper is well-executed and demonstrates promising results, the incremental nature of the contribution and the lack of theoretical insights limit its suitability for ICLR. Addressing the above concerns could significantly strengthen the work.