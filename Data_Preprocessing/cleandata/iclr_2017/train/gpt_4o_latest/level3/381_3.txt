Review of the Paper
Summary of Contributions
The paper introduces a novel neural pruning technique leveraging a Taylor expansion-based criterion to approximate the change in the cost function induced by pruning parameters. This approach is computationally efficient and demonstrates superior performance compared to other heuristic criteria, such as weight norm and feature map activation. The proposed method is particularly effective in transfer learning scenarios, achieving significant reductions in computational cost (up to 12.6Ã— reduction in GFLOPs) with minimal accuracy degradation. The paper also integrates post-pruning fine-tuning to maintain generalization, achieving competitive results on datasets like Birds-200, Flowers-102, and ImageNet. The authors provide extensive empirical evaluations, showing the method's flexibility across different architectures, including VGG-16, AlexNet, and 3D-CNNs.
Decision: Reject
While the paper proposes an interesting pruning criterion and demonstrates promising results, it falls short in critical areas of comparative analysis and contextualization within the existing literature. The lack of strong baselines and insufficient inclusion of prior work in the GFLOPs graphs make it difficult to assess the true superiority of the proposed method.
Supporting Arguments for Rejection
1. Weak Comparative Analysis: The paper does not provide a robust comparison with state-of-the-art pruning methods, such as Optimal Brain Surgeon (OBS) or modern structured pruning techniques. While the Taylor criterion is compared to a few heuristic baselines, the omission of stronger baselines undermines the claim of superiority.
2. Limited Baselines in GFLOPs Graphs: The GFLOPs reduction graphs do not include prior work, making it challenging to contextualize the performance improvements. Without this, the reader cannot gauge whether the proposed method is truly advancing the state of the art.
3. Insufficient Theoretical Justification: While the Taylor criterion is well-motivated, the paper does not provide a rigorous theoretical analysis of why it outperforms second-order methods like OBS in all cases. This weakens the scientific rigor of the claims.
Suggestions for Improvement
1. Stronger Baselines: Include comparisons with state-of-the-art pruning methods, such as OBS, group sparsity regularization, and recent advancements in structured pruning. This would provide a more comprehensive evaluation of the proposed method's effectiveness.
2. Inclusion of Prior Work in Graphs: Add results from existing pruning techniques to the GFLOPs and accuracy trade-off graphs to contextualize the improvements achieved by the proposed method.
3. Theoretical Analysis: Provide a deeper theoretical exploration of the Taylor criterion, particularly its relationship to second-order methods and its empirical superiority.
4. Broader Dataset Evaluation: While the paper evaluates the method on several datasets, including more diverse tasks (e.g., object detection or segmentation) would strengthen the generalizability of the approach.
5. Ablation Studies: Conduct ablation studies to isolate the contributions of individual components, such as FLOPs regularization and per-layer normalization, to the overall performance.
Questions for the Authors
1. How does the proposed method compare to state-of-the-art pruning techniques like OBS in terms of both accuracy and computational efficiency?
2. Why were prior works not included in the GFLOPs graphs? Can you provide a direct comparison to existing methods in future revisions?
3. How sensitive is the method to hyperparameters such as the FLOPs regularization coefficient and the number of fine-tuning iterations?
4. Could the Taylor criterion be extended to other types of neural network layers, such as attention mechanisms in transformers?
In conclusion, while the paper presents a promising pruning criterion, the lack of robust comparative analysis and theoretical depth limits its impact. Addressing these issues would significantly strengthen the paper's contributions.