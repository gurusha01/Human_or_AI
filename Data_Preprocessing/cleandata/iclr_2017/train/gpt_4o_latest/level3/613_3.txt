Review of the Paper
Summary of Contributions
This paper introduces a novel "causal regularizer" designed to promote causal dependencies and penalize non-causal ones in predictive models, particularly in the context of healthcare datasets. The authors propose a neural network-based causality detector leveraging the "independence of mechanisms" (ICM) assumption and integrate it into a causal regularizer for both linear and non-linear models. The framework is evaluated on two datasets, including Sutter Health's heart failure data, demonstrating improved predictive and causal performance compared to L1 regularization and two-step procedures. Additionally, the paper explores the use of the causal regularizer for multivariate causal hypothesis generation, showing up to a 20% improvement in causality scores. The authors argue that their approach balances causal interpretability and predictive accuracy, addressing a critical need in healthcare applications.
Decision: Reject
While the paper addresses an important problem and is well-written, the decision to reject is based on two key issues: (1) insufficient novelty due to limited differentiation from prior work, particularly Lopez-Paz et al., and (2) lack of robustness and rigor in experimental evaluation, which undermines the validity of the claims.
Supporting Arguments
1. Novelty Concerns: The paper builds on existing work by Lopez-Paz et al. but does not sufficiently differentiate its contributions. While the causal regularizer is presented as a novel addition, the concept of using causality scores for regularization has been previously proposed. The paper does not adequately acknowledge or compare against this prior work, raising questions about its originality.
2. Experimental Weaknesses: 
   - The robustness of the approach with respect to hyperparameter choices is not discussed, which is particularly critical for medical datasets where model sensitivity can have significant implications.
   - The experimental results lack depth, with no clear statistical significance reported for the improvements. The qualitative evaluation of causal hypotheses is also ambiguous, relying heavily on expert judgment without sufficient quantitative metrics.
   - The reproducibility of the experiments is hindered by the lack of clarity on the availability of the heart failure dataset, which is not addressed.
Suggestions for Improvement
1. Strengthen Novelty: Clearly articulate how the proposed causal regularizer differs from and improves upon prior work, particularly Lopez-Paz et al. Consider including a side-by-side comparison of methodologies and results to highlight the unique contributions.
2. Robustness Analysis: Include a detailed analysis of the sensitivity of the causal regularizer to hyperparameter choices. For example, evaluate how changes in the regularization parameter affect predictive and causal performance.
3. Statistical Rigor: Provide statistical significance tests for the reported improvements in predictive and causal performance. This will strengthen the claims and provide more confidence in the results.
4. Dataset Availability: Address the availability of the Sutter Health dataset or provide a synthetic dataset that can be used for reproducibility. This is critical for ensuring that the research can be independently validated.
5. Expand Qualitative Evaluation: The qualitative evaluation of causal hypotheses could be expanded with more detailed case studies or additional expert feedback to provide a stronger justification for the claims.
Questions for the Authors
1. How does the proposed causal regularizer compare quantitatively to the methods introduced by Lopez-Paz et al.? Can you provide a direct comparison of results?
2. How sensitive is the performance of the causal regularizer to the choice of hyperparameters, such as the regularization strength?
3. Is the Sutter Health dataset publicly available, or can you provide a synthetic dataset for reproducibility?
4. Can you elaborate on how the expert judgment for causal hypotheses was conducted? Were multiple experts consulted, and how was inter-rater reliability assessed?
By addressing these concerns, the paper could make a stronger case for its contributions and improve its impact on the field.