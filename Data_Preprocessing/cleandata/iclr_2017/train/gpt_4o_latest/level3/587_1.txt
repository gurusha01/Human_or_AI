Review of "DeepRebirth: Accelerating Deep Neural Networks for Mobile Devices"
Summary of Contributions
The paper proposes DeepRebirth, a framework aimed at accelerating deep neural networks (DNNs) for mobile devices by merging non-tensor layers (e.g., pooling, normalization) with adjacent tensor layers into new "rebirth layers." The authors claim that this approach reduces computational latency and runtime memory usage while maintaining accuracy. The framework introduces two merging strategies: streamline merging (for sequential layers) and branch merging (for parallel branches, such as in GoogLeNet). Extensive experiments on GoogLeNet, AlexNet, and ResNet demonstrate significant speed-ups (3x-5x) on mobile CPUs with minimal accuracy loss (0.4%-2.4%). The authors also highlight compatibility with existing compression techniques, such as Tucker decomposition, and report improvements in energy efficiency and runtime memory usage.
Decision: Reject
While the paper addresses an important problem in deploying DNNs on mobile devices, the proposed approach lacks novelty and is not well-placed in the literature. The primary reasons for rejection are as follows:
1. Lack of Novelty: The use of strided convolutions to replace pooling layers is a well-known technique that has been explored in prior work (e.g., Springenberg et al., 2014). The authors present this as a novel "rebirth layer," which is misleading and unoriginal.
2. Insufficient Motivation from Literature: The paper does not adequately position itself against existing works on layer merging or strided convolutions. While the authors claim their approach is the first to optimize non-tensor layers, this claim is overstated, as related methods (e.g., replacing pooling with strided convolutions) already exist.
3. Overstated Contributions: The merging of layers and retraining to preserve functionality is a standard practice in model optimization. The paper does not provide sufficient theoretical insights or novel algorithms to justify its claims of being a "new acceleration framework."
Supporting Arguments
1. Novelty Concerns: The idea of replacing pooling layers with strided convolutions has been previously proposed (e.g., Springenberg et al., 2014). The paper does not sufficiently differentiate its approach from this prior work.
2. Empirical Results: While the experimental results are promising, they primarily demonstrate the effectiveness of an established technique (layer merging and retraining) rather than a novel contribution. The speed-ups and accuracy retention are noteworthy but do not substantiate the claim of a new framework.
3. Terminology Issues: Referring to the merged layers as "rebirth layers" is misleading and adds unnecessary complexity. The concept is essentially a re-implementation of strided convolutions and layer pruning.
Suggestions for Improvement
1. Clarify Novelty: Clearly articulate how DeepRebirth differs from existing works, such as Springenberg et al. (2014). If the novelty lies in the combination of streamline and branch merging, this should be explicitly highlighted and justified.
2. Theoretical Insights: Provide a deeper theoretical analysis of the merging process and its impact on computational complexity and accuracy. This would strengthen the contribution beyond empirical results.
3. Positioning in Literature: Include a more comprehensive discussion of related works on layer merging, strided convolutions, and model compression. Acknowledging prior art would improve the paper's credibility.
4. Terminology Refinement: Avoid overstating contributions with terms like "rebirth layers." Instead, use standard terminology to describe the approach (e.g., layer merging with retraining).
5. Broader Evaluation: While the experiments on GoogLeNet, AlexNet, and ResNet are thorough, it would be helpful to evaluate the framework on newer architectures (e.g., EfficientNet) to demonstrate broader applicability.
Questions for the Authors
1. How does DeepRebirth differ fundamentally from prior work on replacing pooling layers with strided convolutions (e.g., Springenberg et al., 2014)?
2. What are the specific advantages of the proposed streamline and branch merging strategies compared to existing layer optimization techniques?
3. Can the authors provide theoretical guarantees or bounds on the computational savings achieved by merging non-tensor layers?
In conclusion, while the paper addresses a relevant problem and presents promising results, the lack of novelty and insufficient positioning in the literature prevent it from making a significant contribution to the field.