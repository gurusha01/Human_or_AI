Review of the Paper: "L-SR1: A New Second Order Method for Training Deep Neural Networks"
Summary of Contributions
The paper introduces L-SR1, a limited-memory symmetric rank-one (SR1) quasi-Newton optimization method tailored for training deep neural networks. The authors argue that L-SR1 addresses key challenges faced by second-order methods, such as handling saddle points and poor Hessian conditioning, by leveraging trust-region methods and batch normalization. The paper claims competitive performance with first-order methods like Nesterov's Accelerated Gradient (NAG) and Adam, and superior performance to L-BFGS, a well-known second-order method. The authors also emphasize the potential of L-SR1 for distributed training due to its robustness to large mini-batch sizes and minimal hyperparameter tuning.
Decision: Reject
The paper presents an interesting idea but fails to provide sufficient evidence to justify its claims. Specifically, the experimental results do not convincingly demonstrate that L-SR1 outperforms or is competitive with state-of-the-art first-order methods like Adam. Additionally, critical details, such as the time complexity of L-SR1, are missing, which undermines the evaluation of its practical utility.
Supporting Arguments for Decision
1. Missing Time Complexity Analysis:  
   While the authors mention that L-SR1 has a per-iteration time complexity of \(O(mn)\), this crucial detail is absent in the main text. Given that \(m\) (memory size) is typically small but \(n\) (parameter dimensions) is very large in deep learning, this implies that L-SR1 is significantly slower than first-order methods like Adam, which have \(O(n)\) complexity. The experimental results do not adequately address this computational overhead, as the x-axis in the figures is based on epochs rather than wall-clock time.
2. Underwhelming Empirical Performance:  
   The experimental results suggest that L-SR1 does not consistently outperform Adam or Adadelta. While it shows competitive performance in some cases, the improvements are marginal and do not justify the added computational cost. Moreover, the performance gains appear to be primarily driven by momentum-like behavior rather than effective utilization of second-order information.
3. Limited Novelty and Impact:  
   The paper builds on existing SR1 and trust-region methods but does not provide a compelling theoretical or empirical breakthrough. The claim that L-SR1 can overcome saddle points is not rigorously validated, as the experiments do not isolate this aspect. Additionally, the robustness to hyperparameters and large mini-batch sizes, while promising, is insufficient to establish L-SR1 as a practical alternative to first-order methods.
Suggestions for Improvement
1. Include Time Complexity Analysis:  
   Clearly state the time complexity of L-SR1 in the main text and provide wall-clock time comparisons with first-order methods like Adam. This is critical for assessing the practical feasibility of L-SR1.
2. Improve Experimental Rigor:  
   Compare L-SR1 against Adam and Adadelta on a broader range of datasets and architectures, including larger-scale problems where second-order methods might have an advantage. Additionally, provide ablation studies to isolate the contributions of second-order information versus momentum.
3. Clarify Theoretical Contributions:  
   Strengthen the theoretical justification for why L-SR1 is expected to handle saddle points better than other quasi-Newton methods. Empirical evidence supporting this claim should also be included.
4. Address Skipped Updates:  
   The high frequency of skipped updates in some cases (e.g., MNIST with batch normalization) is concerning. Investigate and explain the underlying cause, as this could impact the reliability of L-SR1.
Questions for the Authors
1. How does the computational cost of L-SR1 compare to Adam and other first-order methods in terms of wall-clock time? Can you provide runtime benchmarks?
2. What specific evidence supports the claim that L-SR1 effectively handles saddle points? Have you conducted experiments to analyze its behavior near saddle points?
3. Why does the performance of L-SR1 seem to rely heavily on momentum-like behavior, and how does this differentiate it from first-order methods with momentum?
In conclusion, while L-SR1 is an interesting adaptation of SR1 for deep learning, the lack of compelling empirical results, missing time complexity analysis, and limited novelty make it difficult to recommend acceptance in its current form.