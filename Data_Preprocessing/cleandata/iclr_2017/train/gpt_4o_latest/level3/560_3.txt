Review
Summary
This paper addresses the critical challenge of vanishing and exploding gradients in optimizing recurrent neural networks (RNNs), particularly for tasks involving long-term dependencies. The authors propose a novel approach to control gradient stability by parameterizing weight matrices using a singular value decomposition (SVD) and introducing a configurable margin around the Stiefel manifold. This allows the transition matrices to deviate from strict orthogonality, balancing gradient stability and optimization convergence. The paper is well-written, with clear arguments and a detailed explanation of the proposed method. The authors also explore the impact of loosening orthogonality constraints on synthetic tasks (copy and adding tasks) and real-world datasets (sequential MNIST and Penn Treebank). Their findings suggest that while orthogonal initialization is beneficial, strict orthogonality constraints can hinder optimization and model performance.
Decision: Reject
While the paper presents an interesting and well-motivated approach, the experimental evaluation is insufficient to support its claims. The lack of comprehensive results on key benchmarks and the inconclusive nature of some experiments undermine the scientific rigor of the study.
Supporting Arguments
1. Insufficient Experimental Results: The paper does not report results on the adding problem and Penn Treebank language modeling task, which are essential for evaluating the method's effectiveness on tasks with long-term dependencies. The omission of these results leaves the claims about the method's generalizability unsubstantiated.
2. Inconclusive Copy Task Results: The removal of non-linearity in the copy task alters the optimization dynamics, making it unclear whether the observed improvements are due to the proposed method or the absence of non-linearity.
3. Minimal Impact of Orthogonality: Results in Tables 1 and 2 show only marginal differences between models with and without orthogonality constraints, suggesting that the proposed method may not be as impactful as claimed.
4. Lack of Key Details: The number of hidden units used for RNNs in different tasks is not specified, and there are no running time comparisons with standard optimization methods like SGD. Additionally, the neural network package used for implementation is only briefly mentioned (Theano) without sufficient detail.
Additional Feedback for Improvement
1. Expand Experimental Evaluation: Include results on the adding problem and Penn Treebank language modeling task to provide a more comprehensive evaluation of the method. These benchmarks are standard for assessing long-term dependency tasks and would strengthen the paper's claims.
2. Clarify Copy Task Setup: Provide a detailed discussion of the impact of removing non-linearity in the copy task. Consider re-evaluating the task with non-linearities to ensure the results are comparable to existing methods.
3. Discuss Figure 2: The insights from Figure 2 are not adequately discussed, leaving the gradient propagation dynamics unclear. A detailed analysis would help clarify the relationship between spectral margins and gradient stability.
4. Provide Implementation Details: Specify the number of hidden units used for each task and include running time comparisons with baseline methods like SGD. This would help assess the computational efficiency of the proposed approach.
5. Broaden the Scope of Tasks: While the MNIST and Penn Treebank tasks are useful, additional benchmarks such as character-level language modeling or tasks with more complex dependencies would provide stronger evidence of the method's applicability.
Questions for the Authors
1. Why were results on the adding problem and Penn Treebank language modeling task omitted? Are there any challenges in applying the proposed method to these tasks?
2. How does the removal of non-linearity in the copy task affect the generalizability of the results? Can the method perform well with standard non-linearities like tanh or ReLU?
3. Can you provide a more detailed explanation of the minimal differences observed in Tables 1 and 2? Do you believe the proposed method is task-specific, or are there broader implications for its applicability?
4. How does the computational cost of the proposed method compare to standard optimization techniques like SGD? Would the method scale well for larger models or datasets?
In conclusion, while the paper introduces an intriguing approach to addressing vanishing and exploding gradients, the experimental shortcomings and lack of clarity in some aspects prevent it from meeting the standards for acceptance at this time. Addressing the issues outlined above would significantly strengthen the paper.