The paper introduces the Gated Graph Transformer Neural Network (GGT-NN), a novel approach to representing dialogues and structured data as differentiable graphs. By extending Gated Graph Sequence Neural Networks (GGS-NNs), the GGT-NN model enables the construction, modification, and utilization of graph-structured intermediate representations based on textual input. The paper demonstrates the model's capabilities on bAbI tasks, cellular automata, and Turing machines, showcasing its potential for tasks requiring structured reasoning. This is the first implementation of differentiable memory as a graph, offering a compelling alternative to stack-based or vector-based memory representations.
Decision: Accept
The key reasons for this decision are the paper's originality and technical rigor. While the results on bAbI tasks are preliminary and comparable to existing memory networks, the proposed graph-based memory framework is innovative and thought-provoking. The paper contributes a new perspective to differentiable memory systems, which could inspire future research in graph-based reasoning and memory architectures.
Supporting Arguments:
1. Novelty and Originality: The paper introduces a fundamentally new way of integrating graph learning into the inference process, with differentiable graph transformations and sentence encoding. This approach is distinct from existing memory networks and differentiable neural computers.
2. Technical Soundness: The methodology is well-detailed, with clear descriptions of the graph transformations and their differentiability. The experiments on bAbI tasks, cellular automata, and Turing machines demonstrate the model's versatility.
3. Potential Impact: Despite its preliminary results, the GGT-NN framework opens up new avenues for research in structured reasoning and graph-based learning, making it a valuable contribution to the field.
Additional Feedback:
1. Clarity: The initial clarity of the paper was poor, but the authors have significantly improved it in subsequent revisions. However, certain sections, such as the detailed mathematical formulations, could benefit from additional simplification or illustrative examples to aid comprehension.
2. Performance Analysis: While the model performs comparably to memory networks on bAbI tasks, it lags behind rule-based approaches. A more in-depth analysis of why the graph-based memory does not outperform simpler methods would strengthen the paper.
3. Generalization Capacity: The paper acknowledges that the learning and generalization capacity of the graph-based memory remains uncertain. Future work could explore tasks beyond bAbI to better evaluate the model's scalability and robustness.
Questions for the Authors:
1. Could you clarify the computational complexity of the GGT-NN model compared to memory networks? How does the quadratic scaling with input size affect its applicability to larger datasets?
2. The paper mentions that the model requires strong supervision for graph construction. Have you explored semi-supervised or unsupervised approaches to reduce reliance on labeled data?
3. How does the GGT-NN handle noisy or incomplete input data? Would the model's performance degrade significantly in such scenarios?
In conclusion, the paper is a valuable contribution to the field of graph-based neural networks and structured reasoning. While the results are preliminary, the novelty and potential impact of the proposed approach justify its acceptance.