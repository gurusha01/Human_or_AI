Review of the Paper
Summary of Contributions
This paper proposes a novel "distributed transfer learning" method aimed at addressing two key challenges in transfer learning: optimization complexity and class imbalance. The approach involves fine-tuning individual convolutional filters separately, thereby reducing the complexity of non-convex optimization. Additionally, the method leverages basic probability assignment (BPA) from evidence theory to mitigate class imbalance by weighting classifiers based on their contributions to each class. The authors claim that their distributed strategy improves recognition performance on target domains, as demonstrated by experiments on MNIST, CIFAR, and SVHN datasets.
Decision: Reject
The primary reasons for this decision are the lack of clarity in the presentation and insufficient comparisons with relevant baselines. While the proposed method is interesting, the paper requires significant refinement to be suitable for publication.
Supporting Arguments for the Decision
1. Clarity and Writing Quality: The paper is difficult to follow due to numerous typos, unusual phrasing, and inconsistent terminology. For example, the term "distributed transfer learning" is not clearly defined, and the explanation of BPA is overly convoluted. Figure captions and table descriptions lack clarity, forcing readers to frequently cross-reference the text to understand the results.
   
2. Missing Comparisons: The paper does not compare the proposed method with transfer learning approaches that update convolutional layers, a standard practice in the field. This omission makes it difficult to assess the relative performance of the proposed method.
3. Notation and Assumptions: The notation could be simplified by focusing on the case where |C| = |L|, as this is the only scenario considered in the experiments. The inclusion of unnecessary generalizations adds to the paper's complexity without providing additional insights.
4. Unclear Concepts and Diagrams: The concept of "distributed transfer learning" and its corresponding diagram (Figure 1) are unclear and need significant revision. The diagram does not effectively illustrate the proposed method, and the text does little to clarify it.
5. Experimental Rigor: While the experiments show some improvement over conventional transfer learning, the lack of comparisons with state-of-the-art methods and the absence of statistical significance testing weaken the empirical claims.
Additional Feedback for Improvement
1. Writing and Structure: The paper would benefit from a thorough proofreading to eliminate typos and improve phrasing. The authors should also restructure the paper to clearly define key concepts early on and provide a concise explanation of BPA and its role in the method.
2. Figures and Tables: Improve the clarity of figure captions and table descriptions. For example, Figure 1 should explicitly show how the distributed strategy differs from conventional transfer learning.
3. Comparative Baselines: Include comparisons with transfer learning methods that update convolutional layers, as well as other state-of-the-art approaches. This will provide a more comprehensive evaluation of the proposed method.
4. Notation Simplification: Focus on the case |C| = |L| to streamline the notation and make the paper more accessible.
5. Conceptual Clarity: Provide a clearer explanation of "distributed transfer learning" and explicitly describe how the proposed method addresses optimization complexity and class imbalance.
Questions for the Authors
1. How does the proposed method compare to transfer learning approaches that update convolutional layers? Why were these not included as baselines?
2. Can you provide a clearer explanation of "distributed transfer learning" and how it differs from conventional transfer learning?
3. Why was the case |C| â‰  |L| mentioned if it was not explored in the experiments? Would the method generalize to such scenarios?
4. How does the proposed method scale with larger datasets and deeper architectures?
In its current form, the paper has potential but requires substantial revisions to improve clarity, rigor, and completeness.