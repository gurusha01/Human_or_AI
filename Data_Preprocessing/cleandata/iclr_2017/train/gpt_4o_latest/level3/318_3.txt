Review of the Paper
Summary of Contributions
This paper introduces the Gated Graph Transformer Neural Network (GGT-NN), an extension of Gated Graph Sequence Neural Networks (GGS-NNs), to enable complex graph transformations for solving tasks such as question answering (QA). The key contributions include the proposal of five differentiable graph transformations, which allow the model to construct and modify graph-structured intermediate representations in a supervised manner. The GGT-NN iteratively updates an internal graph state based on sequence inputs and demonstrates promising results on tasks like bAbI, achieving high accuracy on most tasks. Notably, the model effectively combines continuous and symbolic representations, offering a flexible, evolving state structure that surpasses fixed-state models like Memory Networks and Neural Turing Machines (NTMs). Additionally, the paper explores the model's ability to generalize to rule discovery tasks, such as cellular automata and Turing machines, showcasing its versatility in handling structured data.
Decision: Accept
The paper should be accepted due to its novel approach to graph-based neural networks, its strong empirical results on QA tasks, and its potential to advance the field of structured data processing. However, some limitations, such as reliance on supervised training with graph states and the dense presentation of content, should be addressed.
Supporting Arguments
1. Novelty and Motivation: The paper is well-motivated, as it addresses the limitations of fixed-state models by introducing a flexible, differentiable graph transformation framework. This innovation is significant for tasks requiring dynamic state updates, such as QA and rule discovery.
2. Empirical Results: The model achieves state-of-the-art performance on the bAbI dataset, outperforming or matching existing methods on 19/20 tasks. The ability to handle tasks like pathfinding and induction highlights the advantages of a graph-based representation.
3. Scientific Rigor: The proposed graph transformations are clearly defined, and the experimental results are thorough, including comparisons with baselines and ablation studies. The paper also explores generalization to longer sequences, demonstrating robustness.
Suggestions for Improvement
1. Supervised Training Limitation: The reliance on supervised training with graph states at each timestep limits the model's applicability to real-world tasks where such supervision is unavailable. Future work could explore semi-supervised or unsupervised approaches to reduce this dependency.
2. Scalability: The model's quadratic scaling with graph size is a concern for large-scale applications. Introducing sparse edge connections or selective node processing could improve efficiency.
3. Presentation: The paper is dense and better suited for a journal format. Simplifying the descriptions and focusing on key contributions would make it more accessible for a conference audience.
4. Broader Evaluation: While the bAbI tasks are a useful benchmark, additional experiments on real-world datasets (e.g., knowledge graphs or social networks) would strengthen the paper's impact.
Questions for the Authors
1. How does the model perform on real-world datasets beyond synthetic benchmarks like bAbI? Could you provide examples of potential applications?
2. Have you explored semi-supervised or unsupervised training methods to reduce reliance on graph state supervision?
3. Can the proposed graph transformations be extended to handle dynamic graph structures (e.g., graphs with changing node/edge types over time)?
In conclusion, the paper makes a significant contribution to the field of graph-based neural networks and structured data processing. Addressing the scalability and supervision limitations in future work could further enhance the model's applicability and impact.