The paper introduces a novel method, Homologically Functional Hashing (HFH), to compress deep neural networks (DNNs) by reducing memory usage at the expense of increased computation. HFH generalizes the HashedNets approach by using multiple hash functions to map parameters into a shared compression space with fewer collisions and employing a small reconstruction network to recover original parameter values. The paper provides theoretical justification for using multiple hash functions and demonstrates HFH's superior performance over HashedNets on various datasets, achieving higher compression ratios with minimal accuracy loss.
Decision: Accept
The paper presents a well-motivated and technically sound contribution to the field of DNN compression. The novelty of HFH lies in its shared compression space across layers, the use of multiple hash functions, and the integration of a reconstruction network, which collectively address key limitations of HashedNets. The experimental results convincingly demonstrate the method's effectiveness, and the theoretical analysis provides a solid foundation for its design. While the increased computational cost and unclear practical implications in comparison to other state-of-the-art methods are concerns, the paper's potential to inspire further research in parameter-sharing techniques makes it a valuable contribution.
Supporting Arguments:
1. Problem Relevance and Novelty: The paper tackles the critical issue of memory-efficient DNNs, particularly relevant for mobile and embedded devices. The proposed HFH method is a significant improvement over HashedNets, addressing its instability and collision issues through innovative techniques.
2. Theoretical and Empirical Rigor: The theoretical justification for using multiple hash functions is well-articulated, and the experiments are comprehensive, covering diverse datasets and scenarios. The results consistently show that HFH outperforms HashedNets in terms of compression ratio and accuracy retention.
3. Impact on the Field: HFH's design, particularly its shared compression space and reconstruction network, introduces a new perspective on parameter sharing in DNNs, which could inspire future research and practical applications.
Suggestions for Improvement:
1. Computation Costs: The paper should provide a more detailed discussion of the computational overhead introduced by HFH, particularly in comparison to HashedNets and other compression techniques. This would clarify its practical feasibility.
2. Experimental Reproducibility: While the experiments are thorough, running them multiple times and reporting standard deviations would strengthen the reliability of the results.
3. Missing Results and Table Inconsistencies: Some results are referenced but not included in the paper. Additionally, inconsistencies in tables (e.g., mislabeled rows or columns) should be corrected for clarity.
4. Grammar and Presentation: The paper contains grammatical errors and awkward phrasing that detract from its readability. A thorough proofreading is recommended to improve its presentation.
Questions for the Authors:
1. How does HFH compare to other state-of-the-art compression methods, such as pruning or quantization, in terms of both memory savings and computational cost?
2. Can the authors provide more details on the scalability of HFH for extremely large-scale networks, such as those used in industrial applications?
3. Have the authors considered hybrid approaches that combine HFH with other compression techniques, and if so, what are the potential benefits or challenges?
In conclusion, the paper makes a strong contribution to the field of DNN compression, with a novel and effective method that addresses key limitations of existing approaches. While there are areas for improvement, the paper's potential impact and technical rigor justify its acceptance.