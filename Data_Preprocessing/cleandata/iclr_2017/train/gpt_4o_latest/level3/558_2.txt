The paper presents a novel and straightforward exploration method for reinforcement learning (RL) that extends classic count-based approaches to high-dimensional and continuous state spaces using locality-sensitive hashing (LSH). By mapping states to hash codes and using these for state-based visit counts, the method introduces a reward bonus to encourage exploration. The simplicity of the approach, compared to alternatives like VIME, pseudo-counts, and density estimation, is a key strength, making it broadly applicable across diverse RL domains. Experimental results demonstrate significant improvements over baselines in classic benchmarks, continuous control tasks, and Atari games, though the comparative performance against other exploration methods varies by domain.
Decision: Accept.  
The primary reasons for this decision are the merit of the simplicity of the proposed hashing approach and its demonstrated effectiveness across multiple RL benchmarks. While there are concerns about robustness and reproducibility, the paper offers a valuable contribution as a baseline for exploration in RL, particularly given its computational efficiency and ease of integration with existing algorithms.
Supporting Arguments:  
1. Simplicity and Generality: The method is computationally efficient and avoids the complexity of learning density models or variational objectives, making it an attractive alternative for practitioners. The use of LSH offers a practical way to generalize count-based exploration to high-dimensional state spaces.  
2. Empirical Results: The method achieves near state-of-the-art performance in several challenging domains, including sparse-reward Atari games and continuous control tasks. The case study on Montezuma's Revenge highlights the potential for domain-specific adaptations to further enhance performance.  
3. Novelty and Contribution: The paper provides a fresh perspective on count-based exploration by leveraging hashing techniques, which could inspire further research into lightweight exploration strategies.
Additional Feedback:  
1. Robustness and Reproducibility: The paper acknowledges sensitivity to hyperparameters and design choices, but further analysis is needed to establish robustness. For example, the reliance on engineering tweaks and domain-specific knowledge (e.g., Montezuma's Revenge case study) may limit generalizability. Providing more detailed guidelines for hyperparameter selection and ablation studies could improve reproducibility.  
2. State vs. State-Action Counts: The decision to use state-based counts instead of state-action counts deviates from theoretical norms. While empirical results suggest similar performance in Atari, a deeper theoretical or experimental justification is warranted.  
3. DQN Omission: The absence of experiments with DQN is notable, given its relevance and the simplicity of integrating the proposed method. Including these results would strengthen the paper's claims about broad applicability.  
4. Granularity Sensitivity: The paper highlights the importance of hash function granularity but shows inconsistencies across games. A more systematic exploration of how granularity impacts performance could provide actionable insights for practitioners.
Questions for the Authors:  
1. Can you provide a detailed explanation for why state-based counts perform comparably to state-action counts in Atari games?  
2. Why was DQN omitted from the experimental evaluation? Are there specific challenges in integrating the method with DQN?  
3. How does the method perform when applied to other sparse-reward environments beyond the benchmarks tested, such as modern RL environments like Procgen or DMControl?  
4. Could the reliance on domain knowledge in the Montezuma's Revenge case study be reduced while maintaining performance?  
In conclusion, the paper presents a compelling and practical approach to exploration in RL. Addressing the concerns about robustness, reproducibility, and theoretical grounding in future work could significantly enhance its impact.