The paper introduces Neural Data Filter (NDF), a reinforcement learning-based framework for adaptively filtering training data during Stochastic Gradient Descent (SGD) to achieve faster convergence with reduced data usage. The authors frame the data filtering process as a Markov Decision Process (MDP) and propose two policy gradient-based algorithms, NDF-REINFORCE and NDF-ActorCritic, to learn optimal data filtration strategies. Experiments on IMDB sentiment classification and a corrupted MNIST dataset demonstrate that NDF can improve convergence speed and reduce data usage compared to baseline methods like Self-Paced Learning (SPL) and random data filtering. The paper is well-written and explains the proposed algorithm clearly, providing a novel perspective on data scheduling for deep learning.
Decision: Reject
The primary reasons for rejection are the insufficient rigor of the experimental evaluation and the inconclusive nature of the results. While the proposed framework is novel, the experiments fail to convincingly demonstrate its utility due to methodological issues and limited scope.
Supporting Arguments:
1. Insufficient Experiments and Dataset Diversity: The method is tested on only two datasets (IMDB and MNIST), both of which are relatively small and simple. The lack of evaluation on larger and more diverse datasets, such as ImageNet, limits the generalizability of the results. This is a significant shortcoming for a conference-level contribution.
2. Data Leakage in Feature Design: The use of validation accuracy as a feature for training the policy network introduces data leakage, undermining the validity of the experiments. This is a critical flaw that compromises the reliability of the reported results.
3. Unfair Baseline Comparisons: The paper compares NDF's performance to plain SGD but does not include stronger baselines like Adam or other advanced optimizers. Given that Adam alone might achieve comparable or better results without the added complexity of NDF, the utility of the proposed method remains questionable.
4. Inconclusive Results: While the authors claim faster convergence, the results are not robust or conclusive. For instance, NDF-ActorCritic performs poorly compared to NDF-REINFORCE, raising concerns about the general effectiveness of the framework.
Suggestions for Improvement:
1. Expand Experimental Scope: Evaluate the method on larger and more complex datasets, such as ImageNet or CIFAR-100, to demonstrate scalability and generalizability.
2. Address Data Leakage: Redesign the feature set to avoid using validation accuracy, ensuring that the training process is free from leakage.
3. Include Stronger Baselines: Compare NDF against advanced optimizers like Adam and other data scheduling methods to provide a fair assessment of its utility.
4. Clarify Utility: Provide a more detailed analysis of the types of data filtered at different training stages and their impact on model performance. This would help elucidate the behavior and advantages of NDF.
5. Improve Actor-Critic Performance: Investigate why NDF-ActorCritic underperforms and explore better critic function designs to enhance its effectiveness.
Questions for the Authors:
1. How does the proposed method perform on larger datasets like ImageNet? Have you considered testing it on tasks involving convolutional neural networks (CNNs)?
2. Can you provide a detailed explanation of how data leakage from using validation accuracy affects the results? How would you address this issue in future work?
3. Why were stronger baselines like Adam omitted from the comparisons? Would NDF still show an advantage over such optimizers?
While the paper presents an interesting idea, it requires significant improvements in experimental rigor and methodological soundness to meet the standards of a top-tier AI conference.