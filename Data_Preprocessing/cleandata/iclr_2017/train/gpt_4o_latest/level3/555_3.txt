Review of the Paper
Summary of Contributions
This paper investigates the application of meta-learning to the problem of ranking deep neural network (DNN) architectures. The authors propose a novel ranking classifier that leverages topological features and changes in weights, biases, and activation functions during early training steps to predict architecture performance. The study evaluates a diverse set of architectures across multiple tabular datasets, explores the effectiveness of parallel versus serial architectures, and compares DNNs to conventional classifiers. The authors also present a systematic method for generating architectures and provide preliminary insights into the characteristics of top-performing networks. This work represents an important step toward reducing the computational burden of architecture search and improving our understanding of DNN design.
Decision: Reject
The paper introduces an interesting perspective on architecture search using meta-learning, but several critical issues limit its scientific rigor and practical applicability. The lack of detailed explanations, unclear experimental protocols, and inconsistent ranker reliability undermine the validity of the results. Additionally, the limited diversity in generated architectures and the absence of insights into top-performing designs weaken the paper's contributions.
Supporting Arguments for Decision
1. Insufficient Explanation of the Ranking Classifier: While the ranking classifier is central to the proposed approach, its design and training process are inadequately detailed. This lack of clarity makes it difficult to assess the novelty and effectiveness of the method.
   
2. Fixed Hyperparameters and Unreliable Batch Normalization Results: The authors fix key hyperparameters, such as learning rates and decay schedules, which likely affect the conclusions. The unclear and potentially unreliable batch normalization experiments further reduce confidence in the results.
3. Lack of Diversity in Generated Architectures: The architecture generation protocol appears to produce many architectures with similar performance, limiting the exploration of the design space. A pruning mechanism to remove redundant architectures is necessary.
4. Inconsistent Ranker Reliability: The ranker's performance varies across datasets, raising concerns about its generalizability to new domains. This inconsistency limits the practical utility of the approach.
5. Unfair Comparisons in Table 2: The comparisons between parallel and serial architectures conflate differences in parameter capacities, making the results difficult to interpret.
Suggestions for Improvement
1. Detailed Explanation of the Ranking Classifier: Provide a thorough description of the classifier, including its architecture, training process, and hyperparameter settings. Explain how it handles imbalanced datasets and noisy meta-features.
2. Optimize Hyperparameters: Experiment with optimizing learning rates and decay schedules to ensure robust conclusions. Address the potential impact of these parameters on batch normalization results.
3. Enhance Diversity in Architecture Generation: Introduce a pruning mechanism to eliminate redundant architectures and ensure a more diverse exploration of the design space.
4. Provide Insights into Top-Performing Architectures: Include visualizations or trend analyses to help readers understand the characteristics of successful designs. This could include layer compositions, activation functions, or depth-to-width ratios.
5. Ensure Fair Comparisons: Normalize parameter capacities when comparing parallel and serial architectures to isolate the effect of topology.
6. Improve Ranker Reliability: Investigate why the ranker performs inconsistently across datasets and propose methods to improve its robustness.
Questions for the Authors
1. How does the ranking classifier handle datasets with highly imbalanced performance distributions? Are any techniques used to mitigate bias in the ranking process?
2. Can you provide more details on the fixed hyperparameter settings and their potential impact on the results? Why were these specific values chosen?
3. What measures were taken to ensure the reliability of batch normalization experiments, given the fixed learning rates?
4. How does the ranker perform when applied to entirely unseen datasets? Have you tested its generalizability beyond the datasets used in training?
While the paper introduces a promising approach, addressing these issues is essential for it to make a meaningful contribution to the field.