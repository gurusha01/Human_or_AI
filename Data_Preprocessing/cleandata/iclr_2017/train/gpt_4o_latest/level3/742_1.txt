Review of the Research Paper
Summary of Contributions
This paper investigates the expressivity of deep neural networks, focusing on random networks with Gaussian weights and specific activation functions. The authors propose new expressivity measures, such as trajectory length, and establish its exponential growth with depth both theoretically and empirically. They relate trajectory length to other expressivity measures, including neuron transitions, activation patterns, and dichotomies, and explore the implications for trained networks. Experiments on MNIST and CIFAR-10 are used to validate some of the claims, particularly regarding the influence of remaining depth on expressivity and the trade-off between stability and expressivity during training.
Decision: Reject
The paper is rejected due to insufficient justification for its key contributions, lack of contextual clarity, and limited applicability of its findings. While the theoretical results are intriguing, the paper does not convincingly establish the relevance or generalizability of its proposed measures, and several claims are either trivial or contradicted by prior literature.
Supporting Arguments
1. Weak Justification of Expressivity Measures: The introduction of trajectory length as a central measure of expressivity is novel, but the paper fails to provide a robust theoretical or empirical rationale for its validity beyond random networks. The proportionality between trajectory length and decision boundary crossings is shown only for random networks, limiting its applicability to trained networks.
   
2. Questionable Relevance of Random Networks: While random networks are a common baseline, their relevance to understanding the expressivity of trained networks is not convincingly argued. The paper assumes that insights from random networks can generalize to trained networks without sufficient evidence.
3. Trivial Findings: Some results, such as the dependence of expressivity on remaining depth, are intuitive and add limited value. Furthermore, the claim that earlier layers have more influence on expressivity is not novel and has been explored in prior work.
4. Contradiction with Existing Literature: The assertion that prior work relies on unrealistic assumptions is inaccurate. For example, the paper overlooks relevant counterexamples in the literature, such as Leroux et al., which address some of the claimed limitations.
5. Experimental Limitations: The MNIST experiments use networks with insufficient width, resulting in poor performance and limited applicability of the results. This undermines the empirical support for the paper's claims.
Suggestions for Improvement
1. Stronger Theoretical Justification: Provide a clearer theoretical foundation for the proposed expressivity measures, particularly trajectory length, and demonstrate their relevance to trained networks.
   
2. Broader Contextualization: Situate the work more effectively within the existing literature, acknowledging prior contributions and addressing potential contradictions.
3. Improved Experimental Design: Use wider networks and more complex datasets to validate the findings. This would strengthen the empirical evidence and improve the generalizability of the results.
4. Clarity and Conciseness: The paper is overly lengthy and obscure in its presentation. Simplify the exposition and focus on the most impactful contributions.
Questions for the Authors
1. Can you provide a more rigorous justification for the relevance of trajectory length as an expressivity measure for trained networks?
2. How do you address the contradiction between your claim about prior work relying on unrealistic assumptions and existing literature such as Leroux et al.?
3. Why were narrow networks used for the MNIST experiments, and how do you justify the applicability of these results to practical scenarios?
This paper has potential, but significant revisions are needed to address the concerns outlined above.