Review of the Paper
Summary of Contributions
This paper provides a theoretical exploration of why ResNet architectures, particularly those with 2-shortcuts, exhibit superior optimization properties. The authors analyze deep linear networks and extend their findings to non-linear networks, claiming that 2-shortcuts lead to depth-invariant Hessian condition numbers at the zero initialization point. This, they argue, makes training deep networks as feasible as training shallow ones. The paper also demonstrates that shortcuts of other depths (e.g., 1-shortcuts or 3-shortcuts) result in optimization challenges due to poor Hessian properties. The authors support their claims with experiments on MNIST, comparing 2-shortcut networks to Xavier and orthogonal initialization. The paper concludes that 2-shortcuts are special in enabling easier optimization and better final loss performance, even in non-linear settings.
Decision: Reject
The primary reasons for rejection are:  
1. Insufficient Evidence for Non-Linear Networks: The paper extends results derived from linear networks to non-linear networks without providing sufficient theoretical or empirical justification. The claim that the optimization behavior of non-linear networks mirrors that of linear networks is questionable, given the distinct properties of their Hessians.  
2. Weak Experimental Support: The experimental results are limited to a single MNIST plot, which is inadequate to generalize the findings to broader settings or more complex datasets.
Supporting Arguments
1. Linear to Non-Linear Generalization: While the analysis of linear networks is rigorous, the extension to non-linear networks is speculative. The authors claim that the Hessian structure remains consistent across linear and non-linear networks, but this is not substantiated. Non-linear networks typically have Hessians with large condition numbers and complex eigenvalue spectra, which significantly affect optimization dynamics. The lack of a formal proof or extensive empirical validation undermines this key claim.
   
2. Relevance of Zero Initialization: The focus on the Hessian at the zero initialization point is unconvincing. While the authors argue that zero initialization provides insight into optimization dynamics, modern training practices rarely use pure zero initialization. The relevance of this analysis to practical scenarios is unclear.
3. Limited Experiments: The experiments are confined to MNIST, a relatively simple dataset. The results are not sufficient to validate the claims about non-linear networks or to demonstrate the broader applicability of the findings. More diverse datasets and architectures (e.g., convolutional networks) should have been explored.
Suggestions for Improvement
1. Strengthen Non-Linear Analysis: Provide a formal theoretical justification for the claim that the optimization behavior of non-linear networks mirrors that of linear networks. Alternatively, conduct extensive experiments across diverse datasets and architectures to empirically validate this claim.
   
2. Expand Experimental Scope: Include experiments on more complex datasets (e.g., CIFAR-10, ImageNet) and architectures (e.g., convolutional ResNets). This would make the results more compelling and applicable to real-world scenarios.
3. Clarify the Role of Zero Initialization: Justify the choice of analyzing the Hessian at the zero initialization point in the context of modern training practices. If this is purely a theoretical exploration, clearly state its limitations and relevance.
4. Provide More Visualizations: Include additional plots and metrics to illustrate the optimization dynamics, such as loss landscapes, gradient norms, and eigenvalue distributions of the Hessian throughout training.
Questions for the Authors
1. How do you justify the claim that the optimization behavior of non-linear networks is consistent with that of linear networks, given the distinct properties of their Hessians?  
2. Why was MNIST chosen as the sole dataset for experiments? Can the results generalize to more complex datasets and architectures?  
3. How does the analysis of the Hessian at the zero initialization point translate to practical training scenarios, where initialization schemes like Xavier or orthogonal are more commonly used?  
4. Have you considered the impact of batch normalization or other regularization techniques on the optimization dynamics of networks with shortcuts?  
In conclusion, while the paper presents interesting insights into the role of 2-shortcuts in ResNet architectures, the lack of rigorous justification for non-linear networks and limited experimental validation make it unsuitable for acceptance in its current form.