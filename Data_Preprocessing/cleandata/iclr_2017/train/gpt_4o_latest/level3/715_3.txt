Review
This paper addresses the challenge of reducing the computational complexity of deep convolutional neural networks (CNNs) by exploring feature map and kernel pruning techniques. The authors propose a novel, simple, and generic method to select the least adversarial pruning mask from a pool of randomly generated masks, evaluated using a validation set. The method is tested on CIFAR-10, SVHN, and MNIST datasets, demonstrating that significant sparsity (60-70%) can be induced in convolutional layers with minimal degradation in classification performance. The approach is computationally efficient due to its one-shot pruning strategy, avoiding the iterative retraining process typical in prior works.
Decision: Accept
Key reasons for acceptance are:  
1. The paper introduces a novel and practical method for selecting optimal pruning masks, which is generic and applicable to multiple pruning granularities (feature map, kernel, and intra-kernel).  
2. The proposed method is computationally efficient and demonstrates competitive results across multiple datasets, achieving high sparsity with minimal accuracy loss.  
Supporting Arguments
The paper is well-motivated, addressing the critical issue of computational efficiency in CNNs, particularly for resource-constrained environments. The authors provide a thorough analysis of pruning techniques, comparing their method to existing approaches and demonstrating its superiority in terms of performance degradation and computational simplicity. The experiments are rigorous, with results validated on multiple datasets and architectures, including CIFAR-10, SVHN, and MNIST. The inclusion of both feature map and kernel pruning, as well as their combination, highlights the flexibility and scalability of the proposed method. The use of one-shot pruning further enhances the practicality of the approach for real-world applications.
However, the paper has some limitations. While the authors partially address the lack of evaluation on larger models by including VGG-style networks, the absence of experiments on widely-used architectures like AlexNet, GoogLeNet, or ResNet limits the generalizability of the results. Additionally, the paper does not quantify memory savings in megabytes (MB), which is critical for embedded systems. This omission weakens the practical applicability of the proposed method in memory-constrained environments.
Additional Feedback
1. Memory Savings: Quantifying memory savings in MB compared to other approaches would significantly enhance the paper's utility for embedded systems. This is especially important for practitioners aiming to deploy pruned networks on resource-constrained devices.  
2. Evaluation on Larger Models: Extending experiments to include larger and more modern architectures like ResNet or GoogLeNet would strengthen the paper's claims of scalability and generalizability.  
3. Clarity on "One-Shot" Pruning: While the paper emphasizes the efficiency of one-shot pruning, more details on the trade-offs between one-shot and iterative pruning (e.g., accuracy recovery, computational cost) would provide additional insights.  
4. Typographical Errors: The typo in Figure 6a ("Featuer" â†’ "Feature") has been corrected, but a more thorough proofreading of the paper is recommended to ensure clarity and professionalism.
Questions for Authors
1. How does the proposed method perform on larger and more complex architectures like ResNet or GoogLeNet? Are there any specific challenges in scaling the approach to these models?  
2. Can the authors provide quantitative comparisons of memory savings (in MB) achieved by their method versus other pruning techniques?  
3. How does the choice of the number of random pruning masks (N) affect the trade-off between computational cost and accuracy? Could adaptive strategies for selecting N be explored?  
In conclusion, the paper makes a valuable contribution to the field of network pruning with its novel pruning mask selection strategy and efficient one-shot pruning approach. Addressing the noted limitations would further enhance its impact and applicability.