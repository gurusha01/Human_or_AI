Review of the Paper: Group Orthogonal Convolutional Neural Networks (GoCNN)
Summary of Contributions
This paper introduces a novel approach to enhance feature diversity within a single deep convolutional neural network (CNN) by decorrelating neurons through the proposed Group Orthogonal Convolutional Neural Network (GoCNN). The key idea is to partition intermediate neurons into "foreground" and "background" groups, leveraging segmentation annotations as privileged information during training. By enforcing orthogonality between these groups, the method aims to maximize feature diversity and improve generalization. The authors demonstrate the effectiveness of GoCNN on two benchmark datasets, ImageNet and PASCAL VOC, using ResNet-18 and ResNet-152 architectures. The results show significant improvements in classification accuracy compared to vanilla CNNs, even when only partial privileged information is available. The approach is appreciated for its simplicity, effectiveness, and ability to integrate ensembling-like behavior within a single model.
Decision: Accept
The paper is well-motivated, presents a novel and effective method, and provides rigorous empirical evidence to support its claims. The simplicity and elegance of the approach, combined with its demonstrated performance improvements, make it a valuable contribution to the field of deep learning for image classification.
Supporting Arguments
1. Problem Tackled: The paper addresses the challenge of learning diverse and complementary feature representations within a single CNN, which is a well-motivated and underexplored problem. The use of privileged segmentation information to enforce feature diversity is novel and aligns with the broader goal of improving model generalization.
   
2. Methodology and Motivation: The proposed method is grounded in sound principles, such as leveraging orthogonality to reduce feature redundancy. The use of segmentation masks to define foreground and background groups is intuitive and effectively exploits auxiliary annotations that are often underutilized in classification tasks.
3. Empirical Validation: The experimental results are thorough and convincing. The authors demonstrate consistent improvements in classification accuracy across datasets and architectures. The visualization of feature maps further supports the claim that the method learns more diverse and discriminative features. Additionally, the method's robustness to partial privileged information is a practical advantage.
Suggestions for Improvement
1. Generality Beyond Vision Tasks: A major concern is the method's reliance on segmentation annotations, which limits its applicability to vision tasks. The authors should discuss how the approach could be adapted to domains where such privileged information is unavailable or infeasible to obtain.
2. Ablation Studies: While the paper includes some ablation studies, further exploration of the impact of the foreground-to-background group size ratio (fixed at 3:1) would strengthen the analysis. It would also be helpful to evaluate the sensitivity of the method to the quality of segmentation annotations.
3. Computational Overhead: Although the authors claim that no additional computational cost is incurred during testing, the training phase involves additional losses and constraints. A more detailed analysis of the computational overhead during training would be beneficial.
Questions for the Authors
1. How does the method perform when the segmentation annotations are noisy or incomplete? Can the model tolerate errors in the privileged information?
2. Could the proposed orthogonal grouping approach be generalized to other domains (e.g., natural language processing or speech recognition) where segmentation-like annotations are unavailable?
3. Have you considered alternative ways to enforce decorrelation, such as using adversarial training or other regularization techniques, instead of relying on segmentation masks?
In summary, this paper presents a simple yet effective method for improving feature diversity within CNNs, with strong empirical evidence to support its claims. While the method's reliance on segmentation annotations may limit its generality, the contributions are significant and warrant acceptance.