Review of the Paper
Summary of Contributions
This paper introduces a novel spatiotemporal attentional model for saliency prediction in videos, leveraging a combination of C3D for spatial and short-term temporal feature extraction, LSTM for long-term temporal modeling, and a Gaussian Mixture Model (GMM) for generating saliency maps. The proposed Recurrent Mixture Density Network (RMDN) is trained directly on human fixation data, bypassing the need for hand-engineered features. The authors demonstrate that their method achieves state-of-the-art performance on the Hollywood2 dataset for saliency prediction and improves action recognition accuracy on both Hollywood2 and UCF101 datasets. The paper also highlights the computational efficiency of the approach, requiring only 0.08 seconds per 16-frame clip for inference on a GPU.
Decision: Accept
The decision to accept this paper is based on two key reasons:
1. Innovative Approach: The application of LSTM to continuous output problems, such as saliency map generation, is a novel contribution that extends the typical use of RNNs/LSTMs in discrete sequence tasks. The hierarchical modeling of spatiotemporal attention is well-motivated and addresses a critical gap in video saliency prediction.
2. Strong Empirical Results: The proposed method outperforms state-of-the-art techniques in saliency prediction and demonstrates its utility in improving action recognition tasks. The generalization of the saliency model to unseen datasets (UCF101) further strengthens its practical relevance.
Supporting Arguments
1. Well-Motivated Problem: The paper identifies a clear problem—modeling spatiotemporal attention in videos—and situates it effectively within the existing literature. The authors provide a thorough comparison with prior work, highlighting the limitations of handcrafted features and short-term temporal modeling in previous methods.
2. Empirical Rigor: The experimental results are compelling, showing significant improvements in saliency prediction metrics (e.g., AUC, NSS) and action recognition accuracy. The ablation studies and comparisons with baselines (e.g., RNN vs. LSTM, different GMM components) provide a robust validation of the proposed architecture.
3. Efficiency: The computational efficiency of the model (0.08s per clip) is a notable strength, making it suitable for real-time applications.
Suggestions for Improvement
1. Baseline Comparisons: While the paper demonstrates the superiority of the recurrent approach, it lacks direct comparisons with fully convolutional networks (FCNs) for dense image labeling. Including such a baseline would clarify the specific contribution of the recurrent component.
2. Ablation Study on Recurrent Component: The authors briefly mention an ablation study where the recurrent link is removed, but the results are not detailed. A more comprehensive analysis of the impact of temporal recurrence would strengthen the claims.
3. Qualitative Analysis: While the paper provides quantitative results, more qualitative examples of saliency maps (e.g., visualizations comparing ground truth, predicted maps, and baselines) would enhance interpretability and showcase the model's strengths.
Questions for the Authors
1. How does the model perform on other datasets or tasks beyond action recognition, such as video summarization or object tracking? This would help assess its generalizability further.
2. Could the authors elaborate on the choice of 20 Gaussian components in the GMM? How sensitive is the model to this hyperparameter?
3. Have the authors considered integrating the saliency prediction and action recognition tasks into a single joint network, as suggested in the conclusion? If so, what challenges were encountered?
In conclusion, this paper makes a significant contribution to the field of video saliency prediction and action recognition. With minor improvements and additional baseline comparisons, it has the potential to be a highly impactful work.