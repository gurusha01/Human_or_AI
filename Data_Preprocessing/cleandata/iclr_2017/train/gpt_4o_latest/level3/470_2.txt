Review of the Paper
The paper proposes the Stick-Breaking Variational Autoencoder (SB-VAE), a novel extension of the Variational Autoencoder (VAE) that incorporates Bayesian nonparametric priors via the Stick-Breaking process. The authors address two key challenges: component collapsing and the need for stochastic dimensionality in latent representations. By leveraging the Kumaraswamy distribution for posterior inference, the paper introduces a differentiable and computationally efficient approach to handle stick-breaking weights, enabling the latent space to adapt dynamically to the data. The paper also extends the SB-VAE to a semi-supervised setting and demonstrates its effectiveness in learning discriminative latent representations, outperforming Gaussian VAEs in several tasks.
Decision: Reject
While the paper introduces an interesting and novel approach, the claims of stochastic latent dimensionality and the experimental comparisons lack sufficient rigor. Specifically, the paper does not adequately support its claim that the SB-VAE achieves stochastic dimensionality in its latent representations. Furthermore, the semi-supervised results are only compared to a weaker baseline, rather than the established M1+M2 model from Kingma et al. (2014), which undermines the validity of the performance claims.
Supporting Arguments for Decision
1. Stochastic Latent Dimensionality: The paper claims that the SB-VAE achieves stochastic dimensionality, but this is questionable. All latent variables are used, albeit parametrized differently, which resembles soft-gating mechanisms seen in models like LSTMs or attention mechanisms. This undermines the novelty of the claim and suggests that the model may not truly achieve stochastic dimensionality in the sense of dynamically selecting relevant latent dimensions.
2. Experimental Comparisons: The semi-supervised results, while promising, are compared only to a weaker baseline (k-NN) and not to the M1+M2 model from Kingma et al. (2014). This omission makes it difficult to evaluate the true competitiveness of the proposed approach, especially given that M1+M2 is a standard benchmark in the field.
Additional Feedback for Improvement
1. Clarify Stochastic Dimensionality: The authors should provide more evidence to substantiate the claim of stochastic latent dimensionality. Adding a histogram of latent variable activations could help clarify whether the associated weights are large due to actual usage or if inputs are being zeroed out.
2. Component Collapsing Analysis: The paper notes that component collapsing behaves differently with the Stick-Breaking process compared to a Gaussian prior. However, this behavior is not explored in depth. A more thorough analysis of how sparsity in the latent representation affects the decoder weights would strengthen the paper.
3. Comparative Baselines: The semi-supervised results should be compared to the M1+M2 model for a fairer evaluation. This would provide a stronger basis for the claim that the SB-VAE outperforms existing methods.
4. Reproducibility: While the paper mentions that code is available, the experimental setup and hyperparameter choices should be described in greater detail to ensure reproducibility.
Questions for the Authors
1. Can you provide quantitative evidence (e.g., histograms or sparsity metrics) to support the claim that the SB-VAE achieves stochastic latent dimensionality?
2. Why was the M1+M2 model not included in the semi-supervised comparisons, and how do you expect the SB-VAE to perform relative to it?
3. How does the choice of truncation level affect the performance of the SB-VAE? Would a higher truncation level lead to better results, or does it introduce overfitting?
In summary, while the paper presents an innovative approach, the lack of rigorous support for its claims and incomplete experimental comparisons prevent it from meeting the standards for acceptance at this time. Addressing these issues in a future revision could significantly strengthen the contribution.