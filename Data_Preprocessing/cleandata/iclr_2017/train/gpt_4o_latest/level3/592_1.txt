Review of "Epitomic Variational Autoencoder (eVAE)"
Summary of Contributions
This paper introduces the Epitomic Variational Autoencoder (eVAE), a novel extension of the Variational Autoencoder (VAE) framework designed to address the issue of model over-pruning. The authors propose a structured latent space composed of multiple "epitomes," which are sparse subspaces that share parameters across the model. This approach aims to improve the utilization of model capacity and enhance generalization. The paper provides both qualitative and quantitative results on MNIST and TFD datasets, claiming that eVAE outperforms baseline VAEs in terms of sample diversity and quality. The authors also highlight the use of balanced minibatch construction for epitome assignment and explore the impact of epitome size and model complexity on performance.
Decision: Reject
The paper is rejected due to insufficient motivation for the proposed approach, conceptual inaccuracies, and unconvincing experimental results. While the idea of using group sparsity in VAEs is interesting, the execution and evaluation fall short of the standards required for acceptance.
Supporting Arguments for Rejection
1. Motivation and Conceptual Issues: 
   - The choice of a group sparse prior is not well-motivated in the context of VAEs. While the authors draw parallels to sparse coding methods, they fail to provide a compelling theoretical or empirical justification for why this specific prior is suitable for addressing over-pruning.
   - Several conceptual claims in the paper are incorrect or poorly explained. For example, the explanation of the KL term deactivation for specific units is flawed, and the claim that overfitting in VAEs corresponds to data samples becoming more likely under the generative model is misleading.
2. Experimental Results:
   - The experimental results are unconvincing. The use of Parzen window estimates for log-likelihood evaluation is problematic, as this method is known to be unreliable. Furthermore, the paper does not report the variational lower bound on log-likelihood, which is a critical metric for evaluating VAEs.
   - The reported log-likelihoods in Table 1 lack clarity regarding whether they are measured in bits or nats, raising concerns about the validity of the results.
   - The MNIST sample quality is visually uncompetitive, with the presented images being probabilities rather than actual samples. Additionally, the experiments are limited to toy datasets (MNIST and TFD), which undermines the generalizability of the findings.
3. Evaluation Methods:
   - The paper incorrectly claims that the model can only be evaluated from its samples. Established methods like log-likelihood lower bounds and Annealed Importance Sampling (AIS) could have been used for a more rigorous evaluation.
   - The authors' reliance on Parzen window estimates as a primary evaluation metric is not justified, especially since log-likelihood is a more reliable measure of model quality.
Suggestions for Improvement
1. Motivation and Theoretical Justification:
   - Provide a stronger theoretical justification for the choice of a group sparse prior. Explain why this prior is particularly suited to address the over-pruning problem in VAEs.
   - Clarify the conceptual claims, particularly regarding the KL term and overfitting in VAEs.
2. Experimental Design:
   - Report the variational lower bound on log-likelihood and compare it across models. This is a standard metric for evaluating VAEs and would strengthen the empirical evaluation.
   - Avoid relying solely on Parzen window estimates for evaluation. Consider using AIS or other robust methods to estimate log-likelihood.
   - Include experiments on more diverse and complex datasets to demonstrate the generalizability of the proposed approach.
3. Sample Quality:
   - Improve the quality of generated samples and ensure that the presented images are actual samples rather than probabilities. This would provide a more accurate representation of the model's generative capabilities.
4. Clarity and Presentation:
   - Clearly specify whether log-likelihoods are reported in bits or nats in all tables and figures.
   - Provide a more detailed comparison with baseline models, including a discussion of the trade-offs introduced by the epitomic structure.
Questions for the Authors
1. Why was the variational lower bound on log-likelihood not reported, given its importance in evaluating VAEs?
2. How does the proposed group sparse prior compare to other priors (e.g., hierarchical or structured priors) in terms of addressing over-pruning?
3. Can you provide additional experiments on non-toy datasets to validate the generalizability of eVAE?
4. How does the epitomic structure affect the computational complexity of training and inference compared to standard VAEs?
In conclusion, while the paper introduces an interesting idea, it requires significant improvements in motivation, conceptual clarity, experimental rigor, and evaluation methods to meet the standards of the conference.