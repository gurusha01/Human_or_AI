Review of the Paper
Summary of Contributions
This paper investigates the dynamic behavior of deep residual networks (ResNets) during training, proposing that ResNets act as ensembles of networks with varying depths. The authors extend the spin glass model analysis from Choromanska et al. to explain how the effective depth of these ensembles increases as training progresses, driven by mechanisms such as batch normalization. The paper provides theoretical insights into the loss surface of ResNets and connects these findings to practical phenomena observed during training, such as the ease of optimization and improved performance of deep networks. The dynamic behavior of ResNets is presented as a novel contribution, with implications for understanding their effectiveness compared to conventional networks.
Decision: Reject
While the paper applies spin glass techniques to ResNets, which is an interesting and relevant contribution, I recommend rejecting the paper for two primary reasons:
1. Limited Novelty: The theoretical techniques and results heavily rely on Choromanska et al., with minimal extensions. Many assumptions and approximations from the original work are retained, limiting the originality of the contribution.
2. Similarity of Results: The results, both theoretical and empirical, are too similar to those in Choromanska et al. The novelty of applying these techniques to ResNets does not sufficiently compensate for the lack of significant new insights or methods.
Supporting Arguments
1. Lack of Novelty in Theoretical Techniques: The paper builds directly on the spin glass framework introduced by Choromanska et al., retaining key assumptions such as path independence and weight distribution uniformity. While the application to ResNets is new, the theoretical foundation and methodology are not significantly extended.
2. Empirical Results: The dynamic behavior of ResNets is an interesting observation, but the empirical results largely confirm existing hypotheses (e.g., ResNets as ensembles of shallow networks). The connection to batch normalization is insightful but does not constitute a breakthrough.
3. Assumptions and Limitations: The paper acknowledges that some assumptions (e.g., independence of paths) are unrealistic, as noted in Choromanska et al. This undermines the practical applicability of the theoretical results.
Suggestions for Improvement
1. Strengthen Novelty: The authors should aim to introduce new theoretical techniques or relax some of the restrictive assumptions from Choromanska et al. For example, addressing the independence assumption could significantly enhance the paper's contribution.
2. Empirical Validation: The paper could benefit from more extensive empirical validation, particularly on diverse datasets and architectures. Demonstrating how the proposed insights generalize beyond ResNets would strengthen the impact.
3. Practical Implications: While the theoretical analysis is rigorous, the paper should better connect these findings to practical implications for network design or training strategies. For instance, how can the insights about dynamic depth behavior be leveraged to design more efficient architectures?
Questions for the Authors
1. How do the results change if some of the assumptions from Choromanska et al. (e.g., path independence) are relaxed? Can the analysis be extended to more realistic settings?
2. Have you considered applying your framework to architectures other than ResNets, such as DenseNets or Transformers? If so, what were the findings?
3. Can the dynamic depth behavior observed in ResNets be explicitly controlled or optimized during training? If so, how would this affect performance?
In conclusion, while the paper provides an interesting perspective on ResNets, it falls short in terms of novelty and practical impact. Addressing the above concerns could make the work more compelling for future submissions.