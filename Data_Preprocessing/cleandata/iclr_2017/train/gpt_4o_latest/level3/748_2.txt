Review of the Paper
Summary of Contributions
This paper presents a novel approach to neural machine translation (NMT) by replacing the commonly used bi-directional LSTM (BiLSTM) encoders with convolutional neural networks (ConvNets). The authors demonstrate that ConvNets can be competitive with recurrent neural networks (RNNs) in terms of translation accuracy while offering significant speed advantages during decoding. The paper provides strong experimental results across multiple datasets, including WMT'16 English-Romanian, WMT'15 English-German, and WMT'14 English-French, showing that ConvNets can achieve comparable or better BLEU scores than BiLSTMs. Additionally, the authors conduct a detailed analysis of training and generation speed, highlighting the efficiency of ConvNets. The use of position embeddings is shown to be critical for the success of the proposed architecture. However, the exploration of alternative methods for capturing positional information is limited. 
Decision: Accept
The paper makes a significant contribution by demonstrating that ConvNets can serve as a viable alternative to RNNs for NMT encoders, achieving competitive accuracy with faster decoding speeds. The strong experimental results, detailed analysis of speed, and simplicity of the proposed architecture justify acceptance. However, the paper would benefit from additional analysis of positional embeddings and improved clarity in certain sections.
Supporting Arguments
1. Significance of Contribution: The paper establishes a simple yet impactful result that ConvNets can rival RNNs in NMT tasks. This finding challenges the dominance of RNNs in sequence-to-sequence tasks and opens up new avenues for research in NMT and other sequential problems.
2. Experimental Rigor: The authors provide comprehensive experimental results across multiple datasets and baselines, demonstrating the robustness of their approach. The detailed analysis of training and generation speed further strengthens the paper's contributions.
3. Practical Implications: The proposed ConvNet-based encoder is not only competitive in accuracy but also significantly faster during decoding, making it highly relevant for real-world applications where computational efficiency is critical.
Suggestions for Improvement
1. Positional Embeddings: While the paper highlights the importance of positional embeddings, it does not explore alternative methods for encoding positional information. A deeper investigation into this aspect could strengthen the paper's contributions.
2. Clarity of Presentation: A figure comparing the proposed ConvNet architecture with the BiLSTM architecture would greatly enhance the clarity of Section 2. This would help readers unfamiliar with the details of NMT architectures to better understand the differences.
3. Conference Fit: While the paper is technically sound, it may be better suited for an NLP-focused conference rather than a general AI conference, given its specific focus on NMT.
Questions for the Authors
1. Have you considered alternative methods for encoding positional information, such as sinusoidal embeddings or learned embeddings with different parameterizations? If so, how do they compare to the current approach?
2. Could you provide additional visualizations or examples to demonstrate how the attention mechanism behaves differently between ConvNet and BiLSTM encoders?
3. Given the speed advantages of ConvNets, have you explored their performance on other sequence-to-sequence tasks, such as summarization or dialogue generation?
Overall, this paper makes a strong case for the use of ConvNets in NMT and provides valuable insights into their performance and efficiency. With minor improvements in analysis and presentation, the paper could have an even greater impact.