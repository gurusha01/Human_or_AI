Review of the Paper
Summary of Contributions
This paper addresses the challenge of reinforcement learning in environments with limited policy updates, proposing a novel modification to the PoWER algorithm. The key contributions include the derivation of variational bounds on the value function (Lemmas 3 and 4), which enable efficient policy optimization even with negative rewards. The authors demonstrate the effectiveness of their approach through experiments on the Cartpole benchmark and a real-world online advertising dataset. The constrained optimization section introduces an interesting application of Lagrangian methods to maintain cost constraints in online advertising. The paper is well-written, with clear theoretical derivations and practical relevance for scenarios where frequent policy updates are infeasible.
Decision: Reject
While the paper makes strong theoretical contributions and addresses an important practical problem, the following key issues lead to my decision to reject:
1. Limited alignment with ICLR's focus on representation learning: The reliance on log-concavity assumptions restricts the applicability of the method to broader reinforcement learning problems, particularly those involving deep neural networks.
2. Insufficient experimental rigor: The lack of baselines and synthetic datasets in the numerical experiments undermines the empirical validation of the proposed approach.
Supporting Arguments
1. Theoretical Contributions: The variational bounds on the value function (Lemmas 3 and 4) are novel and provide a strong theoretical foundation for the proposed method. The extension to handle negative rewards is particularly valuable, as it enables the use of control variates to reduce variance.
2. Experimental Limitations: While the Cartpole and online advertising experiments demonstrate the feasibility of the approach, the absence of comparisons with standard baselines (e.g., REINFORCE, PPO, or other policy gradient methods) makes it difficult to assess the relative performance of the proposed algorithm. Moreover, the use of a single real-world dataset limits the generalizability of the results.
3. Constrained Optimization Section: The constrained optimization framework is intriguing but feels disconnected from the rest of the paper. The lack of integration with the experimental results weakens its impact.
4. Practical Concerns: The use of control variates as constant scalars is unclear, and treating them as hyperparameters raises questions about why they are not learned or estimated. This aspect requires further clarification and justification.
Suggestions for Improvement
1. Experimental Rigor: Include comparisons with standard baselines and explore synthetic advertising datasets to better validate the proposed approach. This would provide a more comprehensive evaluation of the algorithm's performance.
2. Integration of Constrained Optimization: Strengthen the connection between the constrained optimization framework and the experimental results. Demonstrating its practical utility in the online advertising setting would enhance the paper's impact.
3. Clarification on Control Variates: Provide a detailed explanation of why control variates are treated as hyperparameters rather than being learned. If possible, propose a method to estimate them dynamically.
4. Citations to Related Work: Include references to the constrained MDP literature to better contextualize the use of Lagrangian methods in constrained optimization.
5. Extension Beyond Log-Concavity: Address the limitations imposed by the log-concavity assumption. Exploring how the method could be adapted for non-log-concave policies, particularly those involving deep networks, would significantly broaden its applicability.
Questions for the Authors
1. Why were standard baselines not included in the experiments? How does the proposed method compare to state-of-the-art policy gradient algorithms?
2. Could you provide more details on the choice of control variates and why they are not learned or estimated dynamically?
3. How does the constrained optimization framework perform in practice? Can you provide experimental results demonstrating its utility in the online advertising setting?
4. Are there plans to extend the method to non-log-concave policies, particularly those involving deep neural networks?
In conclusion, while the paper presents interesting theoretical contributions and addresses a practical problem, the experimental and methodological limitations prevent it from meeting the standards for acceptance at ICLR. I encourage the authors to address the above concerns and resubmit to a future venue.