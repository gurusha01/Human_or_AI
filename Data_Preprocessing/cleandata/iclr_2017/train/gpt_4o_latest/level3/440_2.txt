Review
Summary of Contributions
The paper proposes a novel approach for policy search in stochastic dynamical systems using Bayesian Neural Networks (BNNs) trained via α-divergence minimization with α = 0.5. This method aims to improve upon standard variational Bayes (VB) by better capturing complex stochastic patterns such as multi-modality and heteroskedasticity. The authors claim their approach is the first model-based solution to the 20-year-old Wet-Chicken benchmark problem, a challenging task for model-based reinforcement learning. Additionally, the paper demonstrates promising results on industrial applications, including gas turbine control and a complex industrial benchmark. The work is positioned as a scalable and expressive alternative to Gaussian Processes (GPs) and other model-based methods, with an emphasis on real-world applicability.
Decision: Reject
While the paper introduces a technically sound and potentially impactful method, significant issues in clarity, presentation, and evaluation prevent it from meeting the standards for acceptance at this stage. The main reasons for rejection are:
1. Unclear Writing and Dense Notation: Sections 2-3 are particularly difficult to follow due to dense mathematical notation and insufficient explanation. This hinders accessibility and reproducibility.
2. Insufficient Distinction Between Novelty and Prior Work: The paper does not clearly delineate its contributions from existing methods, particularly in Section 2.3, which largely reviews black-box α-divergence minimization. This section could be moved to the appendix to improve focus.
3. Incomplete Evaluation: The fairness of simulating data using another neural network in Section 4.2.1 is questionable, and the computational complexity of the proposed approach compared to alternatives (e.g., stochastic gradient MCMC methods such as SGLD/SGHMC) is not adequately addressed.
Supporting Arguments
1. Technical Soundness: The use of α-divergence minimization with α = 0.5 is well-motivated, and the experiments demonstrate its ability to outperform VB and GPs in capturing complex stochastic dynamics. The results on the Wet-Chicken benchmark and industrial tasks are promising.
2. Clarity Issues: The dense notation and lack of intuitive explanations make it challenging to understand the methodology, especially for readers not already familiar with Bayesian neural networks or α-divergence minimization. This significantly limits the accessibility of the work.
3. Evaluation Gaps: While the paper provides empirical results, it does not address the computational complexity of the proposed method relative to alternatives. Additionally, the use of simulated data in Section 4.2.1 raises concerns about the generalizability of the results.
Suggestions for Improvement
1. Improve Clarity: Simplify the presentation of Sections 2-3 by reducing dense notation and providing more intuitive explanations. Consider moving the review of black-box α-divergence minimization (Section 2.3) to the appendix.
2. Clarify Novel Contributions: Clearly differentiate the novel aspects of the method from prior work, particularly in the context of α-divergence minimization and its application to policy search.
3. Expand Evaluation: Address the computational complexity of the proposed method compared to alternatives, such as stochastic gradient MCMC methods. Additionally, evaluate the fairness and validity of using simulated data in Section 4.2.1 by including comparisons to real-world data or alternative baselines like PSO-P.
4. Broaden Discussion: Discuss the applicability of stochastic gradient MCMC methods (e.g., SGLD/SGHMC) for this setup, referencing related work such as the NIPS 2016 paper mentioned in the guidelines.
Questions for the Authors
1. How does the computational complexity of your approach compare to stochastic gradient MCMC methods (e.g., SGLD/SGHMC)? Could these methods be applied to your setup?
2. Why was simulated data used in Section 4.2.1, and how does this affect the generalizability of the results? Could you evaluate the proposed method on real-world data instead?
3. Could you provide more details on how the α = 0.5 setting balances robustness and accuracy compared to α = 1.0 or VB (α → 0)?
4. How does your method perform when evaluated against PSO-P on the Wet-Chicken benchmark or other tasks?
By addressing these issues, the paper could make a stronger case for its contributions and improve its accessibility to a broader audience.