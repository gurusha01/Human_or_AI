Review
Summary
This paper introduces ISP-ML, a novel SSD-based simulation platform for in-storage processing (ISP) tailored to machine learning workloads. The authors focus on stochastic gradient descent (SGD) as a case study, implementing three parallel variants—synchronous SGD, Downpour SGD, and elastic averaging SGD (EASGD)—to demonstrate the platform's capabilities. The paper highlights the advantages of ISP over conventional in-host processing (IHP), particularly in reducing data transfer overheads and leveraging on-chip parallelism. The authors also propose a methodology for fair performance comparisons between ISP and IHP. While the work is innovative and provides a strong proof of concept, it is limited in scope to linear models and small-scale datasets, leaving its relevance to deep learning and large-scale tasks unaddressed.
Decision: Reject
The paper is not suitable for ICLR due to its limited focus on linear training algorithms and the lack of relevance to deep learning, which is the primary focus of the conference. The work would be better positioned at ICML, where the large-scale linear machine learning community could better appreciate its contributions.
Supporting Arguments
1. Relevance to ICLR's Scope: While the paper addresses a critical bottleneck in machine learning—data movement—it does not extend its findings to deep neural networks (DNNs), which are central to ICLR. The focus on linear models and the use of MNIST, a small-scale dataset, further limits its applicability to the deep learning community.
2. Experimental Limitations: The choice of MNIST as the primary dataset is problematic. MNIST is not representative of modern large-scale machine learning tasks, and its use undermines the scalability claims of the platform. Additionally, the paper does not report the performance of linear methods on MNIST, which is known to be suboptimal.
3. Benchmarking and Generalization: The lack of benchmarking against state-of-the-art large-scale linear learning platforms (e.g., Spark or distributed SGD frameworks) weakens the paper's claims of ISP's superiority. The results are promising but insufficiently generalized to broader machine learning tasks.
Suggestions for Improvement
1. Deep Learning Relevance: Extend the study to include simulations of deep neural networks (DNNs) to demonstrate the platform's applicability to modern machine learning workloads. This would align the work more closely with ICLR's focus.
2. Dataset Choice: Replace MNIST with larger and more challenging datasets (e.g., ImageNet or CIFAR-10/100) to better evaluate the platform's scalability and performance in realistic scenarios.
3. Benchmarking: Compare ISP-ML against established distributed learning frameworks like Spark or TensorFlow to provide a clearer picture of its advantages and limitations.
4. Hardware Considerations: Explore whether flash memory FPGAs can handle the computational demands of DNNs and provide insights into hardware-specific optimizations for ISP.
5. Communication Bottlenecks: Investigate the potential bottlenecks in CPU-GPU or GPU-GPU communication and propose how ISP-ML could address these in the context of deep learning.
Questions for the Authors
1. How does ISP-ML handle the memory requirements of deep neural networks, particularly for models with millions of parameters?
2. Can the proposed platform scale to datasets significantly larger than MNIST, and what are the associated performance trade-offs?
3. Have you considered optimizing ISP for adaptive optimization algorithms (e.g., Adam, Adagrad), which are widely used in deep learning?
4. What are the limitations of the current hardware design in supporting more complex models, and how do you plan to address them in future work?
In summary, while the paper presents an innovative approach to near-data processing for machine learning, its limited scope and lack of relevance to deep learning make it unsuitable for ICLR. With further development and benchmarking, it could make a strong contribution to ICML or other venues focused on large-scale linear machine learning.