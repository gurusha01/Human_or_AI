Review of the Paper
Summary of Contributions
This paper investigates the universality of halting time distributions in optimization algorithms applied to random systems, such as spin glasses and deep learning. The authors claim that, after centering and scaling, the halting time follows a universal distribution that is independent of the underlying data distribution. They identify two universality classes—a Gumbel-like distribution and a Gaussian-like distribution—across different algorithms and systems. The paper provides empirical evidence for this phenomenon using three examples: the conjugate gradient method, gradient descent in spin glasses, and stochastic gradient descent in deep learning. The authors argue that universality is a desirable property of numerical methods, as it indicates stability and robustness. They also propose that universality could guide algorithm development and tuning.
Decision: Reject
The paper presents an intriguing idea of universality in optimization algorithms, but it lacks sufficient empirical evidence and rigorous comparisons to support its claims. The primary reasons for rejection are:  
1. Insufficient Empirical Validation: The universality claim is based on a limited set of experiments, with only one algorithm tested in the deep learning example. Broader comparisons across multiple algorithms are necessary to substantiate the claim.  
2. Overlap with Prior Work: Section 2.1 and the definition of universality overlap significantly with Deift (2014), which provides a more comprehensive analysis of algorithmic universality. The current paper does not sufficiently extend or differentiate itself from this prior work.  
Supporting Arguments
1. Empirical Evidence: While the examples provided (e.g., conjugate gradient, spin glasses, and deep learning) are interesting, they are not comprehensive enough to establish universality as a general phenomenon. For instance, the deep learning example only considers stochastic gradient descent with minimal exploration of other optimization methods (e.g., Adam, RMSprop). This limits the generalizability of the findings.  
2. Practical Concerns: The paper does not address practical aspects such as the mean and variance of running times, which are critical for algorithm tuning. Without this information, the utility of the proposed universality framework remains unclear.  
3. Literature Placement: The paper does not sufficiently engage with existing literature, particularly Deift (2014), which already discusses universality in a broader context. The lack of new theoretical insights or significant empirical advancements weakens the paper's contribution.  
Suggestions for Improvement
1. Expand Empirical Comparisons: Include experiments with multiple algorithms (e.g., Adam, RMSprop, and other optimization techniques) across diverse datasets and systems to strengthen the universality claim.  
2. Address Practical Utility: Provide a detailed analysis of mean and variance in halting times and discuss how these insights can be applied to algorithm tuning in real-world scenarios.  
3. Clarify Novelty: Clearly articulate how this work extends beyond Deift (2014) and other prior studies. Highlight unique contributions, whether theoretical or empirical.  
4. Investigate Failure Cases: Discuss scenarios where universality might fail and provide insights into why this occurs. This would make the findings more robust and nuanced.  
Questions for the Authors
1. Why was only stochastic gradient descent tested in the deep learning example? How do you expect other optimization algorithms to behave in terms of universality?  
2. Can you provide more rigorous comparisons with the results in Deift (2014) to clarify how your work advances the field?  
3. How do you propose to use the universality framework for practical algorithm tuning, given the lack of detailed analysis on running time distributions?  
In summary, while the paper explores a compelling concept, it requires significant additional work to provide robust empirical evidence, address practical concerns, and clearly differentiate itself from prior literature.