Review
Summary of Contributions
This paper introduces a novel dataset and extraction method tailored for machine learning applications in higher-order logic (HOL) theorem proving. The dataset, derived from the HOL Light theorem prover, includes over 2 million training examples and nearly 200,000 testing examples, enabling various machine learning tasks such as proof step classification, premise selection, and intermediate statement generation. The authors propose baseline models, including logistic regression, convolutional neural networks (CNNs), and convolutional-recurrent networks, achieving impressive results with unconditioned classification accuracy exceeding 83%. The dataset and code are made publicly available, providing a valuable resource for the community. The paper is well-written, with clear explanations of the dataset, tasks, and baseline results, making a strong case for the potential of machine learning in HOL theorem proving.
Decision: Accept
The paper makes a significant contribution by introducing a new dataset and demonstrating its utility with baseline models. The dataset fills a gap in the literature, providing a foundation for future research in applying machine learning to HOL theorem proving. The impressive baseline results (accuracy > 0.83) further validate the dataset's potential. The paper is well-motivated, scientifically rigorous, and well-placed in the context of existing work. These factors collectively justify acceptance.
Supporting Arguments
1. Novelty and Impact: The dataset is a valuable addition to the field, addressing the lack of machine learning resources for HOL theorem proving. Its potential to catalyze research in this domain is significant, akin to the impact of datasets like ImageNet in computer vision.
2. Baseline Results: The authors demonstrate the dataset's utility with three baseline models, achieving strong performance. This provides a solid starting point for future work and highlights the dataset's relevance to practical theorem-proving tasks.
3. Presentation and Rigor: The paper is well-structured and clearly written, with detailed explanations of the dataset, tasks, and experimental results. The inclusion of both character- and token-level encodings, as well as conditioned and unconditioned classification tasks, reflects a thorough and scientifically rigorous approach.
Suggestions for Improvement
1. Leveraging Logical Structure: The baseline models primarily rely on pattern matching rather than logical reasoning. Future iterations could explore models that incorporate the logical structure of HOL statements, such as graph-based neural networks or recursive architectures.
2. Generalization Across Systems: The dataset focuses on HOL Light. Expanding the approach to other theorem provers or foundational logics could enhance its generalizability and impact.
3. Conditioned Models: The conditioned models did not outperform unconditioned ones, suggesting that the architectures may not effectively utilize conjecture information. Exploring alternative architectures or feature fusion techniques could address this limitation.
4. Evaluation Metrics: While accuracy is a useful metric, additional metrics such as precision, recall, and F1-score could provide a more nuanced understanding of model performance, especially in practical theorem-proving scenarios.
Questions for the Authors
1. Could you elaborate on why the conditioned models failed to outperform unconditioned models? Were there any specific challenges in integrating conjecture information?
2. How does the dataset handle potential biases introduced by human-guided proofs? Could this affect the generalizability of machine learning models trained on the dataset?
3. Have you considered evaluating the dataset's utility in tasks beyond proof step classification, such as theorem generation or automated proof synthesis?
4. Are there plans to extend the dataset to include proofs from other interactive or automated theorem provers?
Overall, this paper is a strong contribution to the field, and I recommend its acceptance with minor revisions to address the above points.