Review of the Paper
Summary of Contributions
This paper proposes an end-to-end learnable histogram filter (E2E-HF) for state estimation in robotics, combining algorithmic priors with data-driven learning. The authors argue that this hybrid approach balances data efficiency and generality, leveraging the structure of recursive state estimation while learning task-specific models from data. The paper demonstrates the method's advantages in two localization tasks—a hallway and a drone localization scenario—showing improved data efficiency compared to LSTMs and the ability to learn state estimation in an unsupervised manner. The authors also highlight the scalability challenges of their approach and suggest future directions for addressing them.
Decision: Reject  
Key Reasons:  
1. Limited Generalizability: The proposed model is overly tailored to the specific experimental setups, with a discrete state space and one-dimensional observations that restrict its applicability to broader robotics tasks.  
2. Lack of Comprehensive Benchmarking: The paper compares its method only to a basic LSTM model, ignoring more advanced state-of-the-art methods like "Embed to Control" (Watter et al., 2015), which undermines the evaluation's rigor.  
Supporting Arguments
1. Simplistic Assumptions: The state space discretization and the assumption of linear displacements in actions limit the model's ability to generalize to more complex, real-world robotics problems. While the authors acknowledge these limitations, they do not provide a clear path for extending the approach to higher-dimensional or continuous state spaces.  
2. Benchmarking Gap: The lack of comparison with more sophisticated baselines, such as methods that learn state representations directly from raw sensory inputs (e.g., pixels), weakens the empirical claims. The LSTM baseline, while relevant, does not represent the current state of the art in robotics state estimation.  
3. Literature Context: The paper does not sufficiently relate its approach to existing work on combining Hidden Markov Models (HMMs) and neural networks, which is a significant gap given the relevance of these methods to the proposed approach.  
Additional Feedback for Improvement
1. Expand Applicability: The authors should explore how the method can be extended to handle continuous state spaces and higher-dimensional observations. For instance, replacing the histogram with a particle filter or Gaussian mixture model could address scalability concerns.  
2. Broader Benchmarks: Future work should include comparisons with advanced methods like "Embed to Control" or other state-of-the-art approaches in robotics state estimation. This would provide a more comprehensive evaluation of the proposed method's strengths and weaknesses.  
3. Theoretical Justification: The paper could benefit from a deeper theoretical discussion on why the proposed combination of priors and end-to-end learning improves performance compared to isolated learning of models.  
4. Clarity on Unsupervised Learning: While the unsupervised learning results are intriguing, the paper should provide more details on how the learned models compare qualitatively and quantitatively to supervised ones.  
Questions for the Authors
1. How does the method perform when applied to tasks with continuous state spaces or higher-dimensional observations?  
2. Why were more advanced baselines, such as "Embed to Control," not included in the evaluation?  
3. Could the method be adapted to learn directly from raw sensory inputs, such as images, rather than relying on preprocessed observations?  
4. How does the unsupervised learning approach handle environments with more complex observation models or non-Markovian dynamics?  
In summary, while the paper presents an interesting approach to combining algorithmic priors with end-to-end learning, its limited generalizability, lack of comprehensive benchmarking, and insufficient placement in the literature make it unsuitable for acceptance in its current form. Addressing these issues could significantly strengthen the work.