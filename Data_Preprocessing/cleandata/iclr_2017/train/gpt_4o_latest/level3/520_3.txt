Review
Summary of Contributions
The paper proposes an extension of PixelCNN for text-to-image synthesis, incorporating spatial constraints such as keypoints and segmentation masks. It claims to improve interpretability and control in image generation while maintaining the simplicity and stability of autoregressive models. The authors present results on three datasets (CUB, MHP, and MS-COCO) and provide quantitative baselines for text- and structure-conditional pixel log-likelihood. The paper also highlights the advantages of autoregressive models over GANs, including principled likelihood evaluation and training stability.
Decision: Reject  
The paper demonstrates potential in advancing controlled image synthesis, but it falls short in providing sufficient evidence and rigorous comparisons to support its claims. Key limitations include low-resolution outputs, insufficient qualitative and quantitative evaluations, and a lack of clarity on critical aspects such as training/testing times and data replay concerns.
Supporting Arguments
1. Insufficient Evidence for Claims: The comparison between autoregressive models and GANs is inconclusive. Figure 9, which attempts to contrast the two approaches, includes only three examples, which is inadequate to draw meaningful conclusions. The paper also does not address the resolution limitations of autoregressive models effectively.
   
2. Low-Resolution Outputs: The generated images are limited to 32Ã—32 resolution, which makes it difficult to assess the model's ability to produce coherent object and scene details. For instance, objects like cows in Figure 5 appear as indistinct color blobs, undermining the practical impact of the results.
3. Training Data Replay Concerns: The paper raises concerns about whether the model is merely replaying training data but does not convincingly address this issue. Figure 8 attempts to explore disentanglement of keypoints and captions but does not provide a robust analysis.
4. Missed Opportunities for Comparison: The paper does not provide a thorough qualitative or quantitative comparison between PixelCNN and GANs. This omission weakens its claim that autoregressive models offer significant advantages.
5. Unanswered Questions: The paper does not address pre-review questions about training/testing times or justify why high-resolution comparisons are not feasible. This lack of transparency hinders reproducibility and evaluation.
Suggestions for Improvement
1. Expand Comparisons: Provide a more comprehensive comparison between PixelCNN and GANs, including both qualitative (e.g., visual fidelity) and quantitative (e.g., FID scores, likelihood metrics) evaluations. Include more examples in Figure 9 to strengthen the analysis.
2. Address Resolution Limitations: Explore techniques to scale the model to higher resolutions, as low-resolution outputs significantly limit the practical applicability of the work.
3. Clarify Training Data Replay: Conduct experiments to explicitly demonstrate that the model is not overfitting or replaying training data. For example, include metrics like inception scores or use unseen combinations of keypoints and captions during testing.
4. Acknowledge Limitations: Be more transparent about the disadvantages of the approach, such as resolution constraints and computational inefficiency during inference.
5. Provide Missing Details: Include information on training and testing times, as well as the computational resources required. This will help the community better understand the trade-offs of the proposed approach.
Questions for Authors
1. How does the model perform on higher-resolution image synthesis tasks? Are there specific architectural or computational constraints preventing this?
2. Can you provide more evidence to demonstrate that the model does not replay training data? For example, how does it generalize to unseen combinations of keypoints and captions?
3. Why were only three examples included in Figure 9 for the GAN comparison? Can you expand this analysis to include more examples and metrics?
While the paper presents an interesting extension of PixelCNN, it requires significant improvements in evidence, comparisons, and transparency to make a meaningful contribution to the field.