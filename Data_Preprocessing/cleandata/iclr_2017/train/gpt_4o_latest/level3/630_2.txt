Review of the Paper
Summary of Contributions
This paper introduces two novel mechanisms to improve encoder-decoder models for sequence-to-sequence tasks, specifically abstractive summarization. First, the authors propose a "Read-Again" mechanism, a 2-pass reading approach where the first pass informs the second, resulting in better word representations for LSTM and GRU models. Second, the paper presents a decoder with a copy mechanism that alternates between generating words from a fixed vocabulary and copying words from the input text. This approach enables the use of smaller vocabularies, reducing computational costs while handling out-of-vocabulary (OOV) words effectively. The proposed methods achieve state-of-the-art results on the DUC2004 dataset and demonstrate competitive performance on Gigaword, with significant improvements in decoding speed and storage efficiency.
Decision: Reject
While the paper introduces promising ideas, it falls short in several critical areas that prevent it from meeting the standards for acceptance. The primary reasons for rejection are (1) insufficient comparisons with recent benchmarks and methods, and (2) lack of clarity and rigor in evaluating computational efficiency and the impact of individual components.
Supporting Arguments
1. Novelty and Contributions: The "Read-Again" mechanism is an interesting contribution inspired by human reading behavior, and the copy mechanism addresses a practical issue of OOV words effectively. These ideas are well-motivated and align with the challenges in abstractive summarization.
   
2. Experimental Results: The paper demonstrates state-of-the-art results on DUC2004 and competitive performance on Gigaword. However, the evaluation lacks breadth. The authors do not compare their approach to recent advancements, such as self-attention mechanisms (e.g., Transformer models) or LSTMN-based methods, which are highly relevant in the current literature.
3. Efficiency and Scalability: While the paper claims faster decoding due to a smaller vocabulary, the computational cost of the 2-pass reading mechanism is not quantified. This omission raises concerns about the overall efficiency of the model, especially for large-scale datasets or longer input sequences.
4. Ablation Study: The impact of the decoder's small vocabulary trick without the 2-pass reading mechanism is unexplored. This makes it difficult to assess the individual contributions of the two proposed mechanisms.
Suggestions for Improvement
1. Comprehensive Benchmarks: Include comparisons with recent state-of-the-art methods, such as Transformer-based models, to better position the proposed approach in the current landscape.
2. Efficiency Analysis: Provide a detailed analysis of the computational cost of the 2-pass reading mechanism, including training and inference times, to justify its practicality.
3. Ablation Studies: Evaluate the performance of the copy mechanism and the 2-pass reading mechanism independently to clarify their individual contributions.
4. Gigaword Evaluation: Extend the evaluation on Gigaword to include more recent benchmarks and datasets to demonstrate the generalizability of the approach.
5. Qualitative Analysis: While the paper includes examples of the copy mechanism's effectiveness, a more systematic qualitative analysis of failure cases would strengthen the claims.
Questions for the Authors
1. How does the computational cost of the 2-pass reading mechanism compare to single-pass or self-attention-based models?
2. Can the proposed "Read-Again" mechanism be integrated with bidirectional RNNs or Transformer architectures? If not, why?
3. What is the impact of the small vocabulary trick on performance when the 2-pass reading mechanism is not used?
4. How does the model perform on longer input sequences or multi-paragraph summarization tasks?
In conclusion, while the paper introduces innovative ideas, it requires significant additional work to address its limitations in evaluation, efficiency analysis, and comparison with recent methods.