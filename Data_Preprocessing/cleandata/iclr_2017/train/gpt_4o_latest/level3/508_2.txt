Review of "Shift Aggregate Extract Network (SAEN)" Paper
Summary of Contributions
This paper introduces the Shift Aggregate Extract Network (SAEN), a novel neural network architecture designed for learning representations on graph-structured data, particularly social networks. SAEN employs a hierarchical object-part representation, decomposing input graphs into multi-level hierarchies (H-hierarchical decompositions). The proposed method uses a three-step schema (Shift, Aggregate, Extract) to compute vector representations for graph components, allowing for flexible weight sharing and representation dimensionality across strata. Additionally, the authors present a domain compression algorithm that leverages symmetries in hierarchical decompositions to reduce memory usage and computational costs. Empirical results demonstrate that SAEN achieves state-of-the-art performance on several social network datasets and provides significant computational efficiency gains through compression.
Decision: Reject
While the paper proposes a novel and promising approach with strong empirical results, it suffers from significant issues in presentation, clarity, and justification of design choices. These shortcomings hinder the accessibility and reproducibility of the work, making it unsuitable for acceptance in its current form.
Supporting Arguments
1. Strengths:
   - The hierarchical object-part representation and the Shift-Aggregate-Extract schema are innovative and well-suited for graph-structured data.
   - The domain compression algorithm is a valuable contribution, enabling substantial memory and runtime savings.
   - The empirical results are compelling, with SAEN outperforming or matching state-of-the-art methods on multiple datasets.
2. Weaknesses:
   - The paper's presentation is scattered and difficult to follow. Key concepts, such as \(\pi\)-labels and the network architecture, are not adequately explained, leaving the reader unclear about the method's implementation details.
   - The motivation for certain design choices, such as the use of ego-graphs and one-hot degree encoding, is not well-justified. The authors fail to explain why these choices are optimal or how they compare to alternatives.
   - Critical details about the network architecture, such as the number of layers, hidden units, and representation dimensionalities, are buried in the appendix and not integrated into the main discussion. This lack of transparency impedes reproducibility.
   - The final classification methodology is not clearly described, leaving questions about how the learned representations are used for downstream tasks.
Suggestions for Improvement
To enhance the paper's quality and impact, the following improvements are recommended:
1. Clarity and Structure: Reorganize the paper to present the methodology in a more structured and accessible manner. Include illustrative examples to clarify key concepts, such as \(\pi\)-labels and hierarchical decompositions.
2. Design Justifications: Provide a thorough explanation of the motivations behind design choices, such as the ego-graph representation and one-hot degree encoding. Compare these choices to alternative approaches to establish their advantages.
3. Implementation Details: Clearly specify the architecture's parameters (e.g., number of layers, hidden units) and how they were chosen. Integrate these details into the main text rather than relegating them to the appendix.
4. Illustrative Examples: Include visualizations or diagrams to demonstrate the hierarchical decomposition process and the Shift-Aggregate-Extract schema in action.
5. Reproducibility: Provide a more detailed description of the experimental setup, including hyperparameter tuning and training procedures, to facilitate reproducibility.
Questions for the Authors
1. How were the parameters for the neural network (e.g., number of layers, hidden units) chosen for each dataset? Were they optimized, or were they fixed across datasets?
2. What is the rationale behind using ego-graphs and one-hot degree encoding? How do these choices compare to other potential representations?
3. Can you provide more details on the final classification step? How are the learned representations used to produce the final predictions?
4. How does the domain compression algorithm affect the accuracy of the model? Are there trade-offs between compression and performance?
In summary, while the paper presents a novel and promising approach, significant revisions are needed to improve its clarity, structure, and justification of design choices. Addressing these issues will greatly enhance the paper's accessibility and impact.