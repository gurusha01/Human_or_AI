The paper presents a filter pruning method for convolutional neural networks (CNNs) aimed at reducing computational costs without introducing sparsity, thereby maintaining compatibility with existing dense BLAS libraries. The authors propose pruning filters with low `L1`-norm weights, a simple yet effective criterion, and demonstrate its efficacy across popular models like VGG-16 and ResNet on datasets such as CIFAR-10 and ImageNet. The method achieves significant FLOP reductions (e.g., 34% for VGG-16) while retaining accuracy through retraining. The paper also provides insights into layer sensitivity to pruning and compares its approach with other pruning strategies, showing its advantages in terms of simplicity and performance.
Decision: Reject
The primary reasons for this decision are the lack of critical baselines and insufficient empirical validation. While the proposed method shows promise, the absence of comparisons with random pruning and activation-based pruning, as well as the omission of wall-clock speedup metrics, limits the evaluation of its practical utility and novelty.
Supporting Arguments:
1. Strengths: The method demonstrates strong results in retaining accuracy while reducing computational costs on well-known models and datasets. The simplicity of the `L1`-norm criterion and its compatibility with dense BLAS libraries are practical advantages. The sensitivity analysis of layers to pruning adds valuable insights into network architecture design.
   
2. Weaknesses:
   - The paper does not justify why low `L1`- or `L2`-norms are optimal criteria for pruning filters. This omission weakens the theoretical foundation of the method.
   - Critical baselines, such as random filter pruning and pruning based on activation patterns, are missing. Without these comparisons, it is unclear whether the proposed method offers significant advantages over simpler or alternative approaches.
   - The paper lacks direct comparisons with other state-of-the-art pruning and acceleration methods, making it difficult to assess its relative performance.
   - While FLOP reductions are reported, the absence of empirical wall-clock speedup metrics raises concerns about the method's real-world applicability, especially for deployment on hardware-constrained devices.
Additional Feedback:
1. Justification for Norm-Based Pruning: The authors should provide a theoretical or empirical rationale for using low `L1`-norm as the pruning criterion. For example, why is this metric more effective than others like `L2`-norm or activation-based criteria?
2. Baseline Comparisons: Including comparisons with random pruning and activation-based pruning methods would strengthen the evaluation. Additionally, comparisons with recent structured pruning methods would highlight the novelty and effectiveness of the proposed approach.
3. Wall-Clock Speedup: Reporting empirical speedup on real hardware is crucial for demonstrating practical utility. The authors should include wall-clock time measurements alongside FLOP reductions to validate the claimed computational efficiency.
4. Broader Applicability: While the method is tested on VGG-16 and ResNet, evaluating its performance on other architectures (e.g., MobileNet, EfficientNet) would demonstrate broader applicability and relevance to modern, resource-efficient models.
Questions for the Authors:
1. Why is the `L1`-norm chosen as the pruning criterion? How does it compare theoretically and empirically to other norms or activation-based criteria?
2. Can the method be extended to other architectures, such as lightweight models designed for mobile devices?
3. How does the method perform in terms of wall-clock speedup on different hardware platforms (e.g., GPUs, CPUs, mobile devices)?
4. How does the proposed method compare to state-of-the-art pruning techniques in terms of accuracy, FLOP reduction, and retraining time?
In summary, while the paper addresses an important problem and shows promising results, the lack of critical baselines, theoretical justification, and practical speedup metrics limits its impact and readiness for acceptance. Addressing these issues would significantly strengthen the paper.