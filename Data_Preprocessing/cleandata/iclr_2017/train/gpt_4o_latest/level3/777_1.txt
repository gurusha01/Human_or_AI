The paper introduces a novel supervised deep learning approach that combines layer-wise reconstruction loss and class-conditional semantic additive noise to improve latent representation learning. The authors propose a joint learning framework that optimizes supervised and unsupervised objectives, aiming to maximize hierarchical mutual information between input, latent, and output variables. A key contribution is the semantic noise modeling, which stochastically perturbs latent representations during training to enhance generalization. Experimental results on MNIST and CIFAR-10 datasets demonstrate consistent performance improvements over baseline models, with qualitative analyses supporting the effectiveness of the proposed semantic perturbation.
Decision: Reject
Key Reasons:
1. Limited Novelty: The proposed semantic noise modeling resembles Gaussian dropout and other perturbation-based methods, with insufficient distinction from prior work. Additionally, the layer-wise reconstruction loss is conceptually similar to existing joint learning approaches.
2. Insufficient Theoretical Justification: The derivation of Equation (3) from total correlation appears hacky and lacks rigorous justification for estimating \(H(X|Z)\) and \(H(Z|Y)\). The encoding-decoding process involving \(Z\) and \(Y\) from \(X\) is also not well-grounded.
Supporting Arguments:
- While the semantic noise modeling shows empirical improvements, its novelty is questionable. The idea of adding noise for generalization is well-explored in the literature (e.g., Gaussian dropout, DisturbLabel). The paper does not adequately differentiate its approach from these prior works.
- The theoretical foundation of the proposed method is weak. The derivation of Equation (3) relies on assumptions that are not carefully justified, and the connection between total correlation and the proposed loss formulation is unclear.
- Experimental results, while promising, lack robustness. The authors only report mean performance without standard errors, and experiments are not repeated with different random subsets, which raises concerns about reproducibility and statistical significance.
Suggestions for Improvement:
1. Theoretical Rigor: Provide a more robust derivation of Equation (3) and justify the assumptions underlying the estimation of \(H(X|Z)\) and \(H(Z|Y)\). Clarify the role of the encoding-decoding process in the overall framework.
2. Clarifications: Specify whether \(\sigma\) in Equation (8) is a trainable parameter or a hyperparameter. If it is a hyperparameter, explain how it is set or tuned.
3. Novelty and Positioning: Clearly articulate how the proposed method differs from related works, such as Gaussian dropout and DisturbLabel. Include the missing reference to DisturbLabel (CVPR 2016) and discuss its relevance.
4. Experimental Robustness: Repeat experiments with multiple random subsets and report mean and standard error to ensure statistical reliability. Additionally, consider testing the method on more diverse datasets to validate its generalizability.
5. Qualitative Analysis: While the t-SNE visualizations are useful, further quantitative metrics (e.g., latent space clustering quality) could strengthen the argument for improved representation learning.
Questions for the Authors:
1. How does the proposed semantic noise modeling fundamentally differ from Gaussian dropout or DisturbLabel? Can you provide quantitative evidence of this distinction?
2. What is the rationale for using the specific reconstruction losses in Equation (6)? How sensitive is the method to the choice of these losses?
3. How is the noise distribution parameter \(\sigma\) determined, and does it significantly affect the results?
While the paper presents an interesting approach to latent space modeling, its limited novelty, weak theoretical grounding, and insufficient experimental rigor prevent it from meeting the standards for acceptance. Addressing these concerns could significantly improve the quality and impact of the work.