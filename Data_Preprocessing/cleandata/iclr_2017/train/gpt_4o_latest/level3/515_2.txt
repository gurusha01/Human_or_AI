Review of "Exponential Machines: Modeling All Feature Interactions Using Tensor Train Decomposition"
Summary of Contributions
The paper introduces Exponential Machines (ExM), a novel supervised classification model that captures all feature interactions of every order using the Tensor Train (TT) decomposition. This approach significantly reduces the parameter space from exponential to linear with respect to the number of features, enabling scalability. The authors propose a stochastic Riemannian optimization algorithm for training, which outperforms standard stochastic gradient descent (SGD) in convergence speed and final accuracy. The paper also demonstrates the utility of logistic regression initialization for stabilizing training and extends the model to handle categorical features and interactions between feature functions. Empirical results on synthetic and real-world datasets show competitive or superior performance compared to baselines such as high-order Factorization Machines (FMs) and logistic regression.
Decision: Reject
While the paper presents an elegant theoretical framework and promising empirical results, it lacks sufficient analysis and comparisons to establish its novelty and practical impact. Key reasons for rejection include:
1. Insufficient novelty focus: The combination of TT decomposition and tensor manifold geometry is theoretically elegant but not positioned as a novel contribution. Similar approaches (e.g., high-order FMs) are not adequately differentiated.
2. Missing baselines and comparisons: The absence of comparisons to alternative methods like Factorization Machines optimized over positive definite matrices or vanilla feedforward neural networks makes it difficult to assess the model's relative strengths and weaknesses.
3. Limited analysis of key findings: The significant performance boost from logistic regression initialization is intriguing but underexplored. The paper does not provide a detailed analysis of why this initialization works or its broader implications.
Supporting Arguments
1. Theoretical Contributions: The use of TT decomposition to model high-order interactions is well-motivated and mathematically sound. The Riemannian optimization approach is a notable improvement over SGD, particularly for high-dimensional tensors. However, these contributions are incremental rather than groundbreaking, as tensor-based methods have been explored in prior works.
2. Empirical Results: The model performs well on synthetic data and the MovieLens 100K dataset, but the lack of comparisons to neural networks and more advanced FMs limits the ability to contextualize these results. Additionally, the datasets used are relatively small, raising concerns about scalability to real-world, large-scale problems.
3. Clarity and Presentation: The paper is generally well-written, but several typos (e.g., "Bernoulli distrbution," "reproduce the experiemnts," "generilize better") detract from its polish. The mathematical exposition is clear, but the experimental section lacks depth in analyzing key findings.
Suggestions for Improvement
1. Broader Baseline Comparisons: Include comparisons with vanilla feedforward neural networks, Factorization Machines optimized over positive definite matrices, and other tensor-based models to better position the proposed method in the literature.
2. Analyze Initialization: Provide a detailed theoretical or empirical analysis of why logistic regression initialization significantly improves performance. This could enhance the paper's contributions and practical relevance.
3. Scalability Experiments: Test the model on larger datasets to demonstrate its scalability and robustness in real-world scenarios.
4. Clarify Novelty: Clearly articulate the novel aspects of combining TT decomposition with stochastic Riemannian optimization. Highlight how this approach advances the state of the art compared to prior tensor-based methods.
5. Error Corrections: Fix typographical errors and improve the overall presentation quality.
Questions for the Authors
1. How does the proposed method compare to Factorization Machines optimized over positive definite matrices in terms of both accuracy and computational efficiency?
2. Can the authors provide insights into why logistic regression initialization improves convergence? Is this effect consistent across datasets and tensor ranks?
3. How does the model handle sparsity in large-scale datasets, particularly when using Riemannian optimization?
In summary, while the paper introduces a promising approach, it requires stronger novelty positioning, broader comparisons, and deeper analysis to justify its acceptance.