Review of the Paper
Summary of Contributions
The paper investigates incremental learning in sequence prediction tasks using recurrent neural networks (RNNs) and introduces a novel training schedule termed "Incremental Sequence Learning." This approach gradually increases the length of sequences used for training as the model achieves performance thresholds, aiming to improve generalization and computational efficiency. The authors also propose a new artificial dataset derived from MNIST, where handwritten digits are transformed into pen stroke sequences. Experimental results demonstrate that the proposed incremental learning schedule significantly outperforms baseline methods and other curriculum learning strategies on this artificial dataset. The paper further explores transfer learning, using the trained sequence prediction model for sequence classification tasks, and provides insights into the underlying mechanisms of the observed improvements.
Decision: Reject
While the paper presents interesting empirical results, the decision to reject is based on two primary concerns: (1) lack of novelty in the proposed approach, and (2) limited applicability and evaluation of the method on real-world datasets. These issues undermine the broader impact and scientific contribution of the work.
Supporting Arguments
1. Excessive Length: At 17 pages, the paper far exceeds the standard 8-page limit, making it burdensome for reviewers and diluting the focus of the main contributions. Unrelated experiments (e.g., Sections 6.2 and 6.4) further exacerbate this issue.
   
2. Lack of Novelty: The proposed incremental learning schedule is conceptually simple and closely resembles prior work on curriculum learning (e.g., Bengio et al., 2015; Ranzato et al., 2015). The paper does not sufficiently differentiate its approach from these existing methods, nor does it provide theoretical insights or significant innovations.
3. Artificial Dataset: The reliance on a synthetic dataset derived from MNIST raises concerns about the real-world applicability of the method. While the authors argue for the dataset's relevance, there are numerous existing sequential datasets in domains like text, speech, and video that would better demonstrate the utility of the approach.
4. Limited Generalization: The experiments are confined to the artificial dataset, and the method's performance on real-world sequence learning tasks (e.g., language modeling, time-series forecasting) remains unexplored. This limits the paper's impact and practical relevance.
Suggestions for Improvement
1. Focus and Conciseness: Reduce the paper's length by removing unrelated experiments and focusing on the core contributions. For instance, Sections 6.2 and 6.4 could be omitted or moved to supplementary material.
2. Novelty and Positioning: Clearly articulate how the proposed incremental learning schedule differs from prior curriculum learning approaches. Providing theoretical insights or addressing limitations of existing methods would strengthen the contribution.
3. Real-World Evaluation: Test the proposed method on established real-world sequential datasets (e.g., Penn Treebank for language modeling or TIMIT for speech recognition) to demonstrate its broader applicability and relevance.
4. Dataset Justification: Provide a stronger justification for the artificial dataset and clarify its advantages over existing real-world datasets. Alternatively, consider using the artificial dataset as a supplementary evaluation rather than the primary focus.
Questions for the Authors
1. How does the proposed method compare to existing curriculum learning strategies on real-world datasets? Have you considered evaluating it on standard benchmarks like language modeling or speech recognition tasks?
2. Can you provide a more detailed discussion on how the proposed incremental learning schedule addresses limitations of prior work (e.g., Bengio et al., 2015)?
3. What is the practical significance of the artificial dataset, and why should it be preferred over existing real-world sequential datasets?
In summary, while the paper presents promising empirical results, its lack of novelty, excessive length, and limited evaluation on real-world tasks make it unsuitable for acceptance in its current form. Addressing these issues could significantly improve the paper's impact and clarity.