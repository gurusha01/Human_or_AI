Review of the Paper
Summary of Contributions
This paper introduces asynchronous layer-wise gradient descent methods aimed at improving the performance of distributed gradient descent across multiple compute resources. The authors propose several approaches, including a baseline asynchronous method, a layer-wise gradient descent method, and a delayed gradient update strategy. These methods are implemented in Caffe and evaluated on both a CPU cluster and NVIDIA DGX-1 GPUs using AlexNet and GoogleNet on the ImageNet dataset. The results show up to a 1.7x speedup compared to synchronous methods, with convergence maintained across all approaches.
Decision: Reject  
Key Reasons:
1. Clarity and Presentation Issues: The paper is difficult to follow due to a lack of pseudo-code or diagrams to clarify the compute flow and algorithmic updates. This hinders understanding of the proposed methods, particularly the layer-wise gradient descent approach.
2. Evaluation Limitations: The choice of AlexNet as a benchmark is outdated and does not reflect the challenges of modern architectures. Additionally, the evaluation does not adequately address the impact of AlexNet's fully-connected layers on compute/communication ratios, which may not generalize to other architectures.
3. Lack of Baseline Comparisons: The paper does not compare its methods against established asynchronous approaches like parameter servers, which are critical for contextualizing the claimed improvements.
Supporting Arguments
1. Clarity and Presentation: The absence of pseudo-code or diagrams makes it challenging to understand the compute flow and synchronization mechanisms. For example, the layer-wise gradient descent approach, which is claimed to provide equivalence to sequential methods, is not clearly explained or visualized.
2. Evaluation Concerns: Using AlexNet as a benchmark is problematic, as it is no longer representative of state-of-the-art architectures. Modern networks like ResNet or Transformer-based models would provide more relevant insights. Additionally, the paper does not address how the fully-connected layers in AlexNet influence the compute/communication trade-offs, which may skew the results.
3. Baseline Comparisons: The lack of comparison with parameter server-based methods or other asynchronous techniques makes it difficult to assess the significance of the proposed methods. Without such comparisons, the claimed 1.7x speedup lacks sufficient context.
Suggestions for Improvement
1. Clarity: Include pseudo-code and diagrams to illustrate the compute flow, synchronization mechanisms, and differences between the proposed methods. For example, a diagram showing the overlap of computation and communication in the delayed gradient approach would be helpful.
2. Evaluation: Replace AlexNet with more modern architectures like ResNet or Transformer-based models. Additionally, analyze how the architectural differences (e.g., fully-connected layers vs. convolutional layers) impact the compute/communication trade-offs.
3. Baseline Comparisons: Provide comparisons with other asynchronous methods, such as parameter servers or RDMA-based approaches, to contextualize the improvements.
4. Figure Clarity: Improve the labels and explanations in figures, such as Figure 1. For example, terms like "SGD task-wise, 1 comm" should be clearly defined.
Questions for the Authors
1. How does the proposed layer-wise gradient descent method handle dependencies between layers during backpropagation? Could you provide pseudo-code or a diagram to clarify this?
2. Why was AlexNet chosen as a benchmark, given its outdated architecture? How do you expect the proposed methods to perform on modern networks like ResNet or Transformer-based models?
3. Could you provide a comparison with parameter server-based methods or other asynchronous approaches? How does the proposed delayed gradient method compare in terms of convergence speed and accuracy?
In summary, while the paper addresses an important problem and proposes novel methods, it falls short in clarity, evaluation rigor, and contextualization. Addressing these issues would significantly strengthen the paper.