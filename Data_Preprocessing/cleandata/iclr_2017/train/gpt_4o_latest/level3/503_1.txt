Review of "Gated Multimodal Unit (GMU) for Multimodal Learning"
This paper introduces the Gated Multimodal Unit (GMU), a novel neural network component designed to effectively combine multiple modalities in a unified representation. The GMU uses multiplicative gating mechanisms to determine the contribution of each modality to the unit's activation, inspired by flow control in recurrent architectures like GRUs and LSTMs. The authors evaluate GMU in the context of multilabel movie genre classification using the newly introduced MM-IMDb dataset, which combines textual and visual data. The GMU demonstrates superior performance compared to unimodal baselines and other fusion strategies, such as concatenation and mixture of experts (MoE). The release of the MM-IMDb dataset is a valuable contribution to the research community, as it is one of the largest publicly available multimodal datasets for genre prediction.
Decision: Accept
The paper is well-motivated, addresses a relevant problem in multimodal learning, and introduces a novel and effective approach. The GMU's ability to learn modality-specific contributions is demonstrated through both synthetic experiments and real-world tasks. The release of the MM-IMDb dataset further enhances the paper's impact. However, there are areas where the paper could be improved, as detailed below.
Supporting Arguments for Acceptance:
1. Novelty and Contribution: The GMU is a compelling innovation in multimodal learning, offering a principled approach to modality fusion. Its differentiable design allows seamless integration into existing architectures.
2. Empirical Validation: The experiments convincingly demonstrate GMU's superiority over baseline methods, including unimodal approaches and other fusion strategies. The synthetic experiments provide additional evidence of GMU's ability to learn latent modality-specific contributions.
3. Dataset Release: The MM-IMDb dataset is a significant contribution, enabling further research in multimodal genre classification and related tasks.
Suggestions for Improvement:
1. Clarification on Weighted Activation: The paper should provide a more detailed explanation of how the GMU ensures weighted activation in the multimodal case. Specifically, the rationale for using the tanh activation function over alternatives like ReLU should be discussed, as ReLU could avoid saturation issues.
   
2. Handling Missing Modalities: The authors should address how GMU handles missing modalities during test time. Including synthetic examples or additional data in Table 2 to illustrate this scenario would strengthen the paper.
3. Baseline Comparisons: While the GMU outperforms other fusion strategies, a direct comparison with a fully-connected MLP of similar complexity would ensure a fair evaluation of the model's benefits.
4. Broader Contextualization: The related work section could benefit from a deeper discussion of mixture of experts models and multiplicative RNNs, as these are closely related to the GMU's design.
5. Minor Typo: In Section 3.3, "layers and a MLP" should be corrected to "layers and an MLP."
Questions for the Authors:
1. How does the GMU perform when one or more modalities are entirely missing during test time? Could you provide quantitative results or qualitative examples to illustrate its robustness in such scenarios?
2. Why was tanh chosen as the activation function for the GMU? Did you experiment with other nonlinearities like ReLU or Leaky ReLU, and if so, how did they compare?
3. Could you elaborate on the interpretability of the learned gating activations? For example, how do the gates behave for ambiguous or noisy inputs?
In conclusion, the paper makes a strong case for acceptance due to its novel contributions, empirical rigor, and the release of a valuable dataset. Addressing the suggested improvements would further enhance the paper's clarity and impact.