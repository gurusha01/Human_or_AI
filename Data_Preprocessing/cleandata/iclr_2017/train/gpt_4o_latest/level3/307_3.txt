Review
Summary of Contributions
The paper introduces a novel dataset designed to evaluate end-to-end goal-oriented dialog systems in the restaurant reservation domain. The dataset is generated using a simulation framework that leverages a knowledge base (KB) and rule-based transformations to produce natural language utterances. The authors propose a set of five tasks that test various dialog capabilities, such as dialog management, API call generation, and knowledge interpretation. Additionally, the dataset includes real-world data from the Dialog State Tracking Challenge (DSTC2) and an online concierge service, providing a mix of synthetic and realistic scenarios. The evaluation framework employs per-response and per-dialog accuracy metrics, and experimental results demonstrate that Memory Networks outperform traditional baselines like bag-of-words models and rule-based systems. The paper aims to highlight the strengths and limitations of end-to-end systems in goal-oriented settings, providing a reproducible and interpretable testbed for future research.
Decision: Accept
The paper should be accepted for publication. The key reasons for this decision are:
1. Novel Dataset Contribution: The dataset fills a critical gap in the literature by providing a controlled, reproducible, and interpretable benchmark for goal-oriented dialog systems.
2. Comprehensive Evaluation: The paper rigorously evaluates multiple models, including classical baselines, supervised embeddings, and Memory Networks, offering valuable insights into the current state of end-to-end dialog systems.
3. Practical Relevance: By including real-world data and focusing on a narrow domain, the work bridges the gap between synthetic benchmarks and practical applications.
Supporting Arguments
1. Well-Motivated Problem: The authors clearly articulate the limitations of traditional slot-filling systems and the challenges of scaling to new domains. The dataset's design aligns well with the goal of systematically analyzing end-to-end systems in a goal-oriented context.
2. Scientific Rigor: The experiments are thorough, with detailed comparisons across models and tasks. The inclusion of out-of-vocabulary (OOV) test sets and real-world data strengthens the validity of the findings.
3. Transparency and Reproducibility: The authors emphasize reproducibility by proposing a lightweight and open-access framework. However, publishing the rule set used for data generation would further enhance transparency.
Suggestions for Improvement
1. Response Generation vs. Ranking: The reliance on ranking candidate responses is a limitation, as it does not generalize well to new domains. Future work could explore response generation models to address this gap.
2. Data Variability: The rule-based natural language generation constrains linguistic diversity, which may limit the dataset's ability to model real-world variability. Expanding the rule set or incorporating crowd-sourced data could mitigate this issue.
3. Scalability Concerns: The paper does not clarify how candidate responses would be obtained in new domains, raising questions about the scalability of the proposed approach. Addressing this in future work would strengthen the practical utility of the dataset.
Questions for the Authors
1. Could you provide the complete list of rules used for generating the synthetic dataset? This would enhance reproducibility and allow other researchers to build upon your work.
2. How do you envision extending the dataset to more complex or multi-domain scenarios? Would the current framework generalize effectively to such settings?
3. Have you considered incorporating response generation models into your evaluation framework? If so, what challenges do you anticipate?
Overall, the paper makes a significant contribution to the field of goal-oriented dialog systems, and its publication will likely stimulate further research in this area.