Review of the Paper: Tartan (TRT) Accelerator for DNN Inference
Summary of Contributions:  
This paper proposes Tartan (TRT), a hardware accelerator for deep neural network (DNN) inference, which improves upon the DaDianNao (DaDN) architecture by incorporating hybrid bit-serial/bit-parallel arithmetic. The key contribution is enabling execution time and energy efficiency to scale inversely with activation and weight precision in both convolutional (CVL) and fully-connected layers (FCL). The authors demonstrate that TRT achieves an average speedup of 1.90× over DaDN for inference tasks without accuracy loss and allows further trade-offs between accuracy and performance. The paper also introduces mechanisms for cascading computations to handle smaller layers and explores a 2-bit variant for area efficiency. While the work builds on prior designs like STR (Judd et al., 2016), it extends precision scaling benefits to FCLs, a notable improvement.
Decision: Reject  
The primary reasons for this decision are the limited novelty of the proposed approach and the incremental nature of the contribution. Most of the ideas, such as exploiting variable precision and bit-serial arithmetic, have already been explored in prior work (e.g., STR and Judd et al., 2015). The improvements for FCLs, while useful, are relatively minor and do not justify a standalone publication. Additionally, the energy efficiency gains are marginal, and the paper lacks a thorough comparison with alternative variable precision approaches, such as bit-parallel units with data gating, which may offer better trade-offs.
Supporting Arguments:  
1. Limited Novelty: The core idea of scaling execution time with precision has been extensively discussed in prior work (e.g., STR and Judd et al., 2015). The main addition here is extending this concept to FCLs and introducing cascading for smaller layers, which are incremental improvements rather than groundbreaking innovations.  
2. Energy Efficiency Concerns: While TRT demonstrates speedup, the energy savings are modest (1.17× on average) and are partially offset by the additional overhead of flip-flops for bit-serial shifting. This undermines the practical impact of the proposed design.  
3. Lack of Comparative Analysis: The paper does not provide a direct comparison with other variable precision techniques, such as bit-parallel designs with data gating, which could potentially outperform TRT in terms of energy efficiency. This omission weakens the evaluation.  
4. Evaluation Gaps: The absence of place-and-route analysis limits the reliability of the area and energy estimates. Additionally, the results are based on synthesis rather than a physical implementation, which may not fully capture real-world constraints.
Suggestions for Improvement:  
1. Broader Comparisons: Include a detailed comparison with alternative variable precision approaches, such as bit-parallel units with gating or other state-of-the-art accelerators like EIE or Pragmatic.  
2. Energy Optimization: Address the energy overhead of flip-flops in the bit-serial design and explore techniques to mitigate this cost.  
3. Evaluation Depth: Incorporate place-and-route analysis to strengthen the claims about area and energy efficiency. Additionally, provide a more detailed breakdown of energy savings across different layers.  
4. Novelty Emphasis: Highlight any unique aspects of the design that go beyond prior work, such as dynamic precision adjustments during execution or its potential for training applications.  
5. Clarity and Focus: Streamline the presentation to focus on the novel contributions and their implications. The current paper is dense and could benefit from clearer organization and concise explanations.
Questions for the Authors:  
1. How does TRT compare to bit-parallel units with data gating in terms of energy efficiency and area?  
2. Can the authors provide a detailed breakdown of energy consumption for the flip-flop overhead in the bit-serial design?  
3. What are the practical implications of the cascading mechanism for smaller layers? Are there specific applications where this feature would be critical?  
4. How would TRT perform in training scenarios, and what modifications would be required to support training?  
5. Could TRT's approach be integrated into more general-purpose accelerators, such as GPUs or TPUs, to improve their performance?  
In conclusion, while the paper provides a useful extension to prior work, the incremental nature of the contribution and the lack of rigorous comparative analysis limit its impact. Addressing these concerns could make the work more compelling for future consideration.