Review of the Paper
Summary of Contributions
The paper introduces Variable Computation RNN (VCRNN) and Variable Computation GRU (VCGRU), novel modifications to standard RNN and GRU architectures. These models aim to adaptively vary the amount of computation performed at each time step, guided by a learned gating mechanism (m_t). The authors claim that this approach reduces computational overhead while maintaining or improving predictive performance. The paper provides experimental results on tasks such as music modeling, bit-level language modeling, and character-level language modeling, demonstrating that the models can learn meaningful temporal patterns and achieve computational savings.
Decision: Reject
The paper is not ready for acceptance due to significant issues in the proposed approach, evaluation, and positioning within the literature. The primary reasons for rejection are: (1) the gating mechanism is awkwardly implemented and lacks generalization, and (2) the experimental results fail to convincingly support the claims, as the proposed models do not outperform even baseline RNNs in key tasks.
Supporting Arguments
1. Gating Mechanism: The gating mechanism (mt) is a central component of the proposed models, but its design feels ad hoc and lacks theoretical justification. The decision to update only the first D states of the RNN is arbitrary and not well-motivated. Furthermore, the hyperparameter selection process for mt and λ is unclear, making reproducibility and generalization challenging.
2. Comparison to Existing Mechanisms: The paper does not adequately compare VCRNN and VCGRU to established soft-gating mechanisms in GRUs, LSTMs, or Multiplicative RNNs. Such comparisons are critical to demonstrate the novelty and effectiveness of the proposed approach. Without this, the contribution appears incremental rather than innovative.
3. Evaluation and Results: 
   - The experimental results are underwhelming. VCRNN fails to outperform a vanilla RNN baseline on key tasks, such as Penn Treebank (PTB) character-level language modeling. The reported bit-per-character (BPC) results are far from state-of-the-art.
   - The claim of computational savings is not substantiated with practical evidence. While the paper discusses theoretical reductions in operations, no wall-clock time comparisons are provided, which undermines the argument for real-world efficiency gains.
4. Baselines: The paper does not compare VCRNN and VCGRU to stronger baselines like LSTMs or stacked LSTMs, which are standard in sequence modeling tasks. This omission weakens the empirical evaluation.
Additional Feedback for Improvement
1. Clarify the Gating Mechanism: Provide a stronger theoretical justification for the gating mechanism and explore alternative designs that are less restrictive (e.g., updating arbitrary subsets of states). Include a detailed discussion of hyperparameter tuning for m_t and λ.
2. Stronger Baselines and Comparisons: Include comparisons to GRUs, LSTMs, and other adaptive computation models (e.g., Conditional RNNs or Graves' adaptive computation models). This would better contextualize the contribution and highlight any advantages of VCRNN/VCGRU.
3. Wall-Clock Time Analysis: Report wall-clock time savings on modern hardware (e.g., GPUs) to validate the claim of computational efficiency. Theoretical savings alone are insufficient, especially given that modern GPUs are rarely saturated by RNN computations.
4. Equation Numbering: Add equation numbers throughout the paper to improve readability and facilitate discussion during review.
5. Experimental Depth: Provide more detailed analyses of the scheduler's learned patterns and their impact on performance. For example, how does the gating mechanism behave in high-stakes scenarios like long-range dependencies or noisy inputs?
Questions for the Authors
1. Why was the decision made to update only the first D states of the RNN? Have you considered other, more flexible gating mechanisms?
2. How were the hyperparameters (e.g., λ, m̄) tuned, and how sensitive are the results to these choices?
3. Can you provide wall-clock time comparisons to demonstrate practical computational savings?
4. Why were comparisons to GRUs, LSTMs, and other adaptive computation models omitted?
In conclusion, while the paper explores an interesting idea, it requires significant improvements in design, evaluation, and positioning within the literature to be considered for acceptance.