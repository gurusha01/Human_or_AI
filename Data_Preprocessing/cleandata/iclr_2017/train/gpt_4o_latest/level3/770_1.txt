Review of "Neural Knowledge Language Model (NKLM)"
Summary
The paper proposes the Neural Knowledge Language Model (NKLM), which integrates symbolic knowledge from a knowledge base (KB) into recurrent neural network language models (RNNLMs). The model predicts whether a word is generated from the vocabulary or copied from a KB fact description, addressing the challenge of rare or unknown words. A novel dataset, WikiFacts, is introduced to align Wikipedia articles with Freebase facts, and a modified perplexity metric, Unknown-Penalized Perplexity (UPP), is proposed to penalize unknown word predictions. Experimental results demonstrate that NKLM outperforms traditional RNNLMs in both standard perplexity and UPP, while reducing the number of unknown words and enabling immediate adaptation to updated knowledge.
Decision: Reject
While the paper addresses an important challenge in language modeling and proposes a novel approach, the clarity and rigor of the presentation, particularly in the technical sections, are insufficient. Key definitions, notations, and processes are inadequately explained, which hinders reproducibility and understanding. Additionally, the handling of multi-word entities and word order in KB-based word generation lacks clarity, leaving critical implementation details ambiguous.
Supporting Arguments
1. Strengths:
   - The motivation for the work is compelling, as named entities are crucial for downstream tasks but are difficult to learn from statistical co-occurrences alone. The integration of KB facts addresses this gap effectively.
   - The introduction of the WikiFacts dataset is a valuable contribution to the community, as it aligns textual data with KB facts at the word level.
   - The proposed UPP metric is a meaningful improvement over standard perplexity for evaluating knowledge-related language models.
   - Experimental results convincingly demonstrate the effectiveness of NKLM, particularly in reducing unknown words and generating named entities.
2. Weaknesses:
   - The writing and clarity of Section 3 (Model) are poor. Key functions (e.g., \( f{\text{key}}, f{\text{voca}}, f_{\text{copy}} \)) and notations (\( w^v, w^s \)) are inadequately defined, making the model architecture difficult to follow.
   - The concept of "fact embeddings" and their computation (e.g., \( e_k \) as the average of embeddings) is not well-explained, and the rationale for this choice is unclear.
   - The use of LSTM states (\( ht, ct \)) is inconsistent, with \( c_t \) seemingly unused in the model.
   - The process of generating words from KB entities is vague, particularly regarding the handling of multi-word entities and maintaining word order. This is a critical aspect of the model and requires more detailed explanation.
   - The paper does not explore practical enhancements, such as incorporating prior information (e.g., abbreviating "Barack Obama" to "Obama"), which could improve the model's usability.
Additional Feedback
1. Clarity: The authors should significantly improve the clarity of Section 3. Each component of the model, including its inputs, outputs, and intermediate computations, should be explicitly defined with clear mathematical notation.
2. Fact Embeddings: Provide a more detailed explanation of how fact embeddings are computed and why averaging is an appropriate choice. Alternative approaches, such as attention mechanisms, should be discussed.
3. Multi-Word Entities: Elaborate on how the model handles multi-word entities in KB-based word generation. For example, how is word order maintained when copying from fact descriptions?
4. Practicality: Discuss how the model could incorporate prior information (e.g., abbreviations, synonyms) to improve its practicality in real-world applications.
5. Evaluation: While the UPP metric is a valuable contribution, its assumptions (e.g., uniform probability over OOV words) should be validated. Additionally, the authors should provide more qualitative examples of generated text to illustrate the model's strengths and weaknesses.
Questions for the Authors
1. How does the model handle multi-word entities in KB-based word generation? Is word order explicitly modeled, or is it inferred implicitly?
2. Why is \( c_t \) (the cell state of the LSTM) not utilized in the model? Could its inclusion improve performance?
3. Could you provide more details on the computation and role of the topic context embedding \( e_k \)? Why was mean-pooling chosen over other methods like attention?
4. How does the model handle ambiguous entities or facts with multiple valid descriptions (e.g., "Obama" vs. "Barack Obama")?
5. Have you considered evaluating the model on tasks beyond perplexity, such as question answering or dialogue generation, to demonstrate its broader applicability?
In conclusion, while the paper makes notable contributions, significant revisions are needed to improve clarity, address ambiguities, and enhance the practical applicability of the proposed approach.