Review of the Paper
Summary of Contributions
This paper introduces the "Deep Variational Information Bottleneck" (Deep VIB), a novel approach that leverages the information bottleneck (IB) principle to regularize deep neural networks and improve adversarial robustness. By parameterizing the IB model using neural networks and employing the reparameterization trick, the authors make the IB framework computationally feasible for high-dimensional data. The paper demonstrates that Deep VIB outperforms other regularization techniques on MNIST in terms of generalization and robustness to adversarial attacks. The authors also provide theoretical insights and experimental results, including comparisons to related methods.
Decision: Reject
While the paper presents an interesting idea and is well-written, it falls short in several key areas. The primary reasons for rejection are the limited scope of experiments and insufficient rigor in justifying certain design choices.
Supporting Arguments
1. Strengths:
   - The idea of combining the IB principle with deep learning is promising and well-motivated.
   - The paper is clearly written and accessible, making the concepts easy to follow.
   - The experiments on MNIST show some improvement in robustness and generalization, which supports the potential of the proposed method.
2. Weaknesses:
   - Experimental Limitations: The experiments are restricted to MNIST, a relatively simple dataset. The lack of results on more complex datasets (e.g., CIFAR-10, ImageNet) or architectures limits the generalizability of the findings.
   - Baseline Comparisons: The omission of dropout in the robustness comparison (Figure 4) is a significant oversight. Dropout is a standard regularization technique and should have been included for a fair comparison.
   - Unexplained Choices: The use of 12 posterior samples for evaluation is not justified, and the error bars in Figure 1(a) are not clarified, making it difficult to assess the statistical significance of the results.
   - Theoretical Contribution: While the variational approximation to the IB objective is useful, the theoretical novelty is relatively modest, as similar ideas have been explored in prior work (e.g., variational autoencoders and related IB formulations).
   - Unclear Claims: The claim about posterior covariance behavior with changes in Î² (page 7) is difficult to verify due to inconsistent figure scaling, which undermines the interpretability of the results.
Suggestions for Improvement
To strengthen the paper, the authors could:
1. Extend the experimental evaluation to larger datasets (e.g., CIFAR-10, ImageNet) and more complex architectures (e.g., convolutional neural networks).
2. Include dropout in the robustness comparisons to provide a more comprehensive baseline.
3. Justify the choice of 12 posterior samples and clarify the error bars in Figure 1(a).
4. Provide a detailed explanation of the posterior covariance behavior and ensure consistent figure scaling.
5. Compare the proposed method to variational fair autoencoders (Louizos et al., 2016) to highlight differences and potential advantages.
6. Explore additional applications of Deep VIB, such as sequence prediction or unsupervised learning, to demonstrate broader applicability.
Questions for the Authors
1. Why was dropout omitted from the robustness comparison in Figure 4? How would the results change if dropout were included?
2. What motivated the choice of 12 posterior samples for evaluation? Would fewer or more samples significantly affect the results?
3. Can you clarify the error bars in Figure 1(a)? Are they standard deviations, confidence intervals, or something else?
4. How does the proposed method compare to variational fair autoencoders in terms of robustness and generalization?
In conclusion, while the paper introduces a promising approach, it requires stronger experimental validation and clearer justifications for certain design choices to be suitable for acceptance.