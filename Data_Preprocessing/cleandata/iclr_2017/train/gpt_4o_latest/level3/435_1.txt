Review of the Paper
The paper introduces a novel warm restart technique for stochastic gradient descent (SGD) to improve training efficiency and anytime performance in deep neural networks (DNNs). The authors demonstrate the effectiveness of their method on CIFAR-10, CIFAR-100, EEG datasets, and a downsampled version of ImageNet, achieving state-of-the-art results on CIFAR datasets and competitive performance on others. The proposed method is simple, computationally efficient, and integrates seamlessly with existing training pipelines. The authors also highlight the potential of their approach for ensemble learning, enabling the creation of diverse models "for free" by leveraging snapshots from the training trajectory.
Decision: Accept
The decision to accept this paper is based on two key reasons: (1) the substantial revisions made by the authors, which elevate the work from workshop-level to a significant contribution, and (2) the empirical evidence supporting the claims, including experiments on diverse datasets beyond CIFAR, which demonstrate the generalizability and robustness of the proposed method.
Supporting Arguments
1. Problem Tackled: The paper addresses the challenge of improving the convergence speed and anytime performance of SGD in training DNNs. This is a critical problem in deep learning, as training large models often requires significant computational resources.
2. Motivation and Placement in Literature: The paper is well-motivated and builds on prior work on learning rate schedules and restart techniques. It provides a clear comparison with related methods, such as cyclical learning rates and cosine annealing, and positions its contribution as a practical and effective alternative.
3. Empirical Rigor: The results are scientifically rigorous, with extensive experiments on multiple datasets. The inclusion of EEG and downsampled ImageNet datasets strengthens the paper by demonstrating the method's applicability beyond image classification tasks. The authors also provide insights into the method's behavior, such as its ability to reduce overfitting and improve ensemble diversity.
Additional Feedback
1. Theoretical Insights: While the empirical results are compelling, the paper lacks theoretical support for why the proposed warm restart technique generalizes well across tasks. Future work could explore theoretical analyses to provide deeper insights into the method's effectiveness.
2. Comparison with Adam and Momentum-based Optimizers: The observations in the paper contradict common experiences with Adam and SGD with momentum in certain domains, such as NLP. A more detailed discussion or experiments in these domains would strengthen the paper's claims.
3. Hyperparameter Sensitivity: The paper mentions that the method reduces sensitivity to learning rate selection but does not provide a detailed analysis of hyperparameter robustness. Including such an analysis would make the contribution more comprehensive.
4. Clarity of Presentation: While the paper is well-written, some sections, such as the experimental results, could benefit from more concise summaries and clearer visualizations to enhance readability.
Questions for the Authors
1. Have you tested the proposed method on tasks beyond classification, such as regression or reinforcement learning? If so, how does it perform?
2. Could you provide more insights into why the method performs well with diverse architectures and datasets? Are there specific properties of the cosine annealing schedule that contribute to this?
3. How does the method compare with other advanced optimizers like AdamW or AdaBelief in terms of convergence speed and final performance?
In conclusion, the paper makes a meaningful contribution to the field of optimization in deep learning. The proposed method is simple, effective, and broadly applicable, making it a valuable addition to the literature. With minor improvements in theoretical grounding and domain-specific evaluations, the work could have an even greater impact.