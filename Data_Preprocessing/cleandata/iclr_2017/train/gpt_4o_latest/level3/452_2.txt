Review of the Paper: Fine Grained Action Repetition (FiGAR)
Summary of Contributions
The paper introduces FiGAR, a novel framework for enabling temporal abstractions in Deep Reinforcement Learning (DRL) algorithms. By representing actions as tuples \((a, x)\), where \(a\) is the action and \(x\) is the number of repetitions, FiGAR allows agents to dynamically decide both the action and its duration. This approach is claimed to improve policy optimization across discrete and continuous action spaces. The authors demonstrate FiGAR's efficacy by extending three DRL algorithms—A3C, TRPO, and DDPG—and report performance improvements in Atari 2600, MuJoCo, and TORCS domains. The paper highlights FiGAR's lightweight design, avoiding action-space blowup, and its ability to generalize across different action repetition sets.
Decision: Reject
While the paper presents an interesting idea and demonstrates its potential, several critical issues in clarity, rigor, and reproducibility prevent its acceptance in its current form. Below, I outline the key reasons for this decision and provide constructive feedback to improve the work.
Supporting Arguments for Decision
1. Discrepancy in Baseline Scores: The reported A3C scores differ significantly from those in the original Mnih et al. (2016) paper. This raises concerns about the experimental setup and whether the improvements attributed to FiGAR are due to the proposed method or differences in training regimes.
2. Clarity and Ambiguity: 
   - The section on DDPG is unclear, with terms like "concatenating loss" left undefined. The explanation of FiGAR's loss function and optimization approach is also insufficiently detailed, making it difficult to reproduce the results.
   - The inconsistent use of the notation \(r\) (sometimes for rewards, other times for repetition set \(R\)) adds to the confusion.
3. Computational Overhead: While FiGAR performs well in games with few repeated actions, its computational overhead compared to A3C is not adequately justified. This may limit its practical utility in real-time applications.
4. Equation Issues: The reward equation on page 5 does not correctly reflect the sum of discounted rewards between decision steps. This oversight undermines the theoretical foundation of the proposed method.
5. Unintentional Oversights: The inclusion of "namethisgame" in the tables suggests a lack of thorough proofreading, which detracts from the paper's professionalism.
6. Limited Exploration of Limitations: The fixed commitment to action repetitions is a potential drawback, especially in stochastic environments. While the authors acknowledge this, they do not explore dynamic schemes to address it, leaving a significant gap in the proposed framework.
Suggestions for Improvement
1. Experimental Validation: Provide a detailed comparison of training regimes to address the discrepancies in A3C scores. Include ablation studies to isolate the contributions of FiGAR from other factors.
2. Clarity in Presentation: 
   - Clearly define all terms and notations, especially in the DDPG and loss function sections.
   - Ensure consistent use of symbols like \(r\) throughout the paper.
3. Computational Efficiency: Analyze and report the computational overhead of FiGAR compared to baseline algorithms. Discuss scenarios where the trade-off between performance and efficiency is justified.
4. Equation Correction: Revise the reward equation to accurately reflect the sum of discounted rewards. This is critical for the theoretical soundness of the method.
5. Dynamic Action Repetition: Explore and evaluate a dynamic scheme where the agent can decide at each step whether to continue or change actions. This could address the limitation of fixed action repetitions in stochastic environments.
6. Proofreading: Carefully review the paper to eliminate errors like "namethisgame" and ensure professional presentation.
Questions for the Authors
1. What specific differences in training regimes could explain the discrepancy between your A3C scores and those reported by Mnih et al. (2016)?
2. Could you clarify the term "concatenating loss" in the DDPG section and provide a more detailed explanation of FiGAR's loss function?
3. How does FiGAR's computational overhead compare to baseline algorithms in terms of training and inference time?
4. Have you considered or tested dynamic action repetition schemes, and if so, what were the results?
In conclusion, while FiGAR is a promising framework with potential applications in DRL, the paper requires significant improvements in clarity, rigor, and experimental validation to meet the standards of the conference. I encourage the authors to address these issues and resubmit.