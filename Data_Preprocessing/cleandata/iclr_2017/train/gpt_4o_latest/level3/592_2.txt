Review of "Epitomic Variational Autoencoder (eVAE)"
Summary
The paper introduces the Epitomic Variational Autoencoder (eVAE), a novel extension of the Variational Autoencoder (VAE) that aims to address the issue of latent variable over-pruning. The key idea is to partition the latent space into multiple "epitomes," each of which activates only a subset of latent variables for a given data sample. This approach is intended to encourage better utilization of the model's capacity and improve generalization. The authors provide both qualitative and quantitative results on MNIST and TFD datasets, claiming that eVAE generates more diverse and higher-quality samples compared to standard VAEs.
Decision: Reject  
The paper is not ready for publication in its current form due to significant methodological and experimental shortcomings. While the proposed idea is interesting, the lack of rigorous evaluation, misleading experimental results, and insufficient baselines undermine the validity of the claims.
Supporting Arguments
1. Insufficient Baselines: The paper does not compare eVAE to strong baselines such as a mixture of VAEs (mVAE) with reliable evaluation methods. While mVAE is briefly mentioned, its performance is not thoroughly analyzed or contrasted with eVAE in key experiments.
2. Misleading Evaluation Metrics: The use of a Parzen window estimator to evaluate log-likelihood is problematic, as this method is known to provide unreliable estimates. A more accurate lower bound or other robust metrics should have been used to substantiate the claims.
3. Inconsistent Experimental Setup: The paper alternates between binary and continuous MNIST experiments without clarifying the rationale or addressing the differing modeling challenges. This inconsistency creates confusion and makes it difficult to interpret the results.
4. Unjustified Claims: The claim that VAEs "overfit" is not supported by evidence. No comparison of reconstruction likelihoods between training and test data is provided to substantiate this assertion.
5. Blurry Samples: The generated samples from eVAE are blurry and resemble those of a mixture of smaller VAEs rather than a higher-capacity model. This undermines the claim of superior performance and raises questions about the effectiveness of the proposed approach.
6. Unclear Methodology: The use of dropout in the dropout VAE baseline is not well-explained, leaving ambiguity about whether it was applied to the latent variables or the encoder/decoder layers.
Additional Feedback for Improvement
1. Stronger Baselines: Include comparisons to mVAE and other state-of-the-art models using reliable evaluation metrics. This will help contextualize the performance of eVAE.
2. Clarify Experimental Design: Clearly explain the choice of datasets (binary vs. continuous MNIST) and ensure consistency in experimental setups. Address how these choices impact the results.
3. Robust Evaluation: Replace the Parzen window estimator with more reliable metrics, such as the true variational lower bound or other standard benchmarks for generative models.
4. Provide Evidence for Claims: If overfitting is claimed, provide quantitative evidence (e.g., reconstruction likelihoods on training vs. test data). Similarly, justify the use of dropout and its implementation details.
5. Improve Sample Quality: Address the blurriness of generated samples and provide visual or quantitative evidence that eVAE generates higher-quality outputs compared to baselines.
6. Theoretical Justification: Provide a more rigorous theoretical analysis of why eVAE mitigates over-pruning and how it compares to existing solutions like annealing schedules or minimum KL constraints.
Questions for the Authors
1. How does eVAE compare to mVAE when evaluated using more reliable metrics, such as the variational lower bound or Frechet Inception Distance (FID)?
2. Can you provide evidence that eVAE avoids overfitting, such as reconstruction likelihoods for training and test datasets?
3. How does the choice of epitome size (K) affect the model's performance across different datasets? Is there a principled way to choose K?
4. Why were binary and continuous MNIST experiments mixed, and how do these differences impact the results?
In summary, while the idea of using sparse subspaces in the latent space is promising, the paper requires significant improvements in methodology, evaluation, and clarity to justify its claims.