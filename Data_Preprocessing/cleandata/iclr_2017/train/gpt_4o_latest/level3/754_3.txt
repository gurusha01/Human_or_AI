Review of the Paper
Summary of Contributions  
This paper introduces a neural language model augmented with a sparse pointer network for code suggestion in Python, a dynamically-typed programming language. The model addresses the challenge of capturing long-range dependencies, particularly for identifiers, by leveraging a fixed attention policy informed by the Abstract Syntax Tree. The authors release a large-scale Python dataset of 41 million lines of code, which is a valuable resource for the community. The proposed model demonstrates improved perplexity and accuracy compared to n-gram and standard LSTM baselines, particularly excelling in identifier prediction. The qualitative analysis further highlights the model's ability to capture long-range dependencies effectively.
Decision: Reject  
While the paper makes a meaningful contribution to the field of code suggestion, it falls short in terms of novelty and depth of analysis. The fixed attention policy, while effective, builds on prior work (e.g., Maddison and Tarlow 2014) and does not introduce fundamentally new ideas. Additionally, the evaluation could be strengthened with a deeper analysis of the contributions of different token types to perplexity and a more comprehensive comparison to related methods.
Supporting Arguments for the Decision  
1. Limited Novelty: The use of a fixed attention mechanism and sparse pointer networks is a valuable contribution, but the approach closely resembles prior work on probabilistic context-free grammars with context-aware variables (Maddison and Tarlow 2014) and pointer networks (Ling et al. 2016). The novelty lies primarily in the application to Python code suggestion, which, while important, does not represent a significant conceptual leap.  
2. Evaluation Gaps: The paper demonstrates improvements in perplexity and accuracy, but the analysis lacks granularity. For example, the contribution of different token types (e.g., identifiers, keywords, literals) to the overall perplexity reduction is not explored. This would provide a clearer understanding of where the model excels and where it may still struggle.  
3. Broader Context: While the paper positions itself well in the literature, it does not sufficiently compare its results to other recent advancements in neural code suggestion, such as transformer-based models, which are becoming increasingly popular in the field.
Additional Feedback for Improvement  
1. Deeper Analysis: A breakdown of perplexity improvements by token type would strengthen the results and provide more actionable insights for future work.  
2. Comparison to Transformers: Given the growing prominence of transformer architectures in code modeling, a comparison to such models would provide a more comprehensive evaluation.  
3. Dataset Details: While the release of the Python dataset is commendable, more details on its quality and diversity (e.g., domains, coding styles) would enhance its utility for the community.  
4. Scalability: The paper mentions plans to scale the approach to entire projects but does not provide preliminary results or insights into the feasibility of this extension. Including such discussions would strengthen the paper's impact.
Questions for the Authors  
1. How does the fixed attention policy compare to more recent transformer-based models for code suggestion?  
2. Can you provide a more detailed analysis of the contribution of different token types to the perplexity improvements?  
3. How does the model handle ambiguous cases where multiple identifiers could be valid suggestions?  
4. What measures were taken to ensure the quality and diversity of the released Python dataset?  
In summary, while the paper makes a meaningful contribution to code suggestion for Python, it requires a stronger emphasis on novelty, deeper evaluation, and broader contextualization to meet the standards of acceptance.