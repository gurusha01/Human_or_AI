Review
Summary of Contributions
This paper explores the role of feature regularization in low-shot learning, providing both theoretical analysis and empirical evidence. It establishes equivalence between weight Frobenius-norm, feature L2 norm, and gradient magnitude loss, and proposes a modified cost function incorporating both feature and weight regularization. The authors argue that their approach is a soft version of Batch Normalization (BN) and demonstrate its potential benefits for numerical stability and low-shot learning. The paper includes two case studies (XOR classification and two-layer regression) to illustrate the effects of feature regularization and evaluates the method on synthetic datasets, the Omniglot benchmark, and ImageNet. The proposed method achieves a 6.5% improvement over the baseline and is competitive with state-of-the-art methods.
Decision: Reject  
While the paper provides interesting theoretical insights into feature regularization and its connection to Batch Normalization, it lacks sufficient experimental validation and clarity in its claims. The results, though promising, fail to convincingly demonstrate the method's effectiveness for deep, high-dimensional networks and low-shot learning scenarios.
Supporting Arguments for Decision
1. Insufficient Experimental Validation:  
   The experimental results, while showing a 6.5% improvement over the baseline, do not adequately analyze the meaningfulness of the learned representations. The paper lacks ablation studies to isolate the contributions of feature regularization versus other factors. Furthermore, the comparison with Batch Normalization is limited to a single-layer application of the proposed regularization, leaving its scalability to deeper architectures unclear.
2. Limited Applicability to Deep Non-Linear Models:  
   The theoretical analysis is insightful for simple models like XOR classification and two-layer regression but does not convincingly extend to deep non-linear networks with ReLU activations. The connection to Batch Normalization, while intriguing, remains speculative without stronger empirical evidence.
3. Weak Relevance to Low-Shot Learning:  
   The claims about the method's benefits for low-shot learning are loosely connected to the theoretical analysis. The results on Omniglot and ImageNet, though competitive, do not establish a clear advantage over existing methods tailored for low-shot learning.
4. Formatting and Presentation Issues:  
   The paper suffers from formatting issues, including incorrect table references, inconsistent spacing, and improper citation formatting, which detract from its overall readability and professionalism.
Suggestions for Improvement
1. Strengthen Experimental Validation:  
   Include more comprehensive experiments on deep networks with ReLU activations to validate the scalability and effectiveness of the proposed method. Provide ablation studies to isolate the contributions of feature regularization and weight regularization.
2. Clarify Connection to Low-Shot Learning:  
   Provide a stronger theoretical and empirical link between the proposed method and its claimed benefits for low-shot learning. Analyze the learned representations to demonstrate their meaningfulness and generalization capabilities.
3. Improve Presentation:  
   Address formatting issues, ensure proper citation formatting, and provide clearer explanations of key equations and derivations. A more cohesive narrative connecting the theoretical analysis to the experimental results would enhance the paper's impact.
4. Extend Comparison with Batch Normalization:  
   Conduct more thorough comparisons with Batch Normalization across multiple layers and architectures to substantiate the claim that the proposed method is a "soft" version of BN.
Questions for Authors
1. How does the proposed method perform in deeper architectures with non-linear activations like ReLU? Can the theoretical insights be extended to such settings?
2. What is the specific advantage of the proposed method over Batch Normalization in terms of numerical stability and generalization?
3. Can you provide an analysis of the learned representations to demonstrate their relevance for low-shot learning tasks?
4. How does the choice of regularization parameters (λ1, λ2) affect the method's performance, and what guidelines can be provided for tuning them?
In summary, while the paper offers valuable theoretical insights, it requires stronger empirical evidence and a more cohesive presentation to justify its claims and contributions.