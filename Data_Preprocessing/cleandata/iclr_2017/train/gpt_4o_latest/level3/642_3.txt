Review
Summary of Contributions
This paper explores the use of customized precision hardware for large-scale deep neural networks (DNNs), focusing on unconventional narrow-precision floating-point representations. The authors demonstrate that these representations can achieve significant computational speedups—up to 7.6×—with minimal degradation in inference accuracy (<1%) compared to single-precision floating-point baselines. The paper also introduces a novel method to predict optimal precision configurations by analyzing neural network activations, significantly reducing the search time for customized precision settings. The evaluation spans several production-grade DNNs, including GoogLeNet and VGG, and highlights the superiority of floating-point representations over fixed-point ones for large networks. The work emphasizes the importance of tailoring numeric precision to specific DNN architectures to optimize hardware performance.
Decision: Reject  
Key Reasons:
1. Limited Novelty: While the paper provides a thorough exploration of numeric precision trade-offs during inference, it does not address training-time optimization, which is a more pressing research area for hardware efficiency. The focus on inference alone limits the broader impact of the work.
2. Incomplete Methodology: The exclusion of batch normalization—a standard component in modern convolutional networks—reduces the relevance of the findings for hardware manufacturers and real-world applications. This omission raises concerns about the generalizability of the results.
Supporting Arguments
- Clarity and Motivation: The proposed method for predicting precision parameters from neural network activations is insufficiently explained. Key details about how activations are used, the computational overhead of the prediction process, and its scalability are missing. This lack of clarity undermines confidence in the efficiency of the proposed approach.
- Relevance to the Field: While the study provides insights into precision trade-offs, the omission of batch normalization and the focus on inference rather than training diminish its practical applicability. Hardware manufacturers are unlikely to adopt the findings without a more comprehensive evaluation of real-world DNN components.
- Scientific Rigor: The empirical results are compelling but lack theoretical grounding. For example, the paper does not provide a robust explanation for why floating-point representations outperform fixed-point ones in large networks, beyond empirical observations.
Suggestions for Improvement
1. Incorporate Batch Normalization: Future work should evaluate the impact of customized precision on networks with batch normalization to ensure broader applicability.
2. Expand Scope to Training: Addressing precision trade-offs during training would significantly enhance the paper's contribution and relevance to the field.
3. Improve Clarity: Provide a more detailed explanation of the proposed precision prediction method, including computational overhead, scalability, and robustness across different network architectures.
4. Theoretical Insights: Strengthen the theoretical foundation for the observed superiority of floating-point representations, potentially by analyzing the propagation of numerical errors in large networks.
Questions for the Authors
1. How does the proposed precision prediction method scale with larger networks or more complex architectures? What is the computational overhead of this approach?
2. Why was batch normalization excluded from the evaluation? How might its inclusion affect the results?
3. Can the proposed method be extended to optimize precision during training, and if so, how?
In conclusion, while the paper provides valuable insights into inference-time precision trade-offs, its limited scope, lack of clarity, and omission of key components make it marginally below the acceptance threshold. Addressing these issues could significantly enhance its impact and relevance.