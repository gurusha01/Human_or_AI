Review of the Paper
Summary of Contributions
This paper investigates the capacity and trainability of recurrent neural network (RNN) architectures, including existing variants (vanilla RNN, LSTM, GRU) and two novel architectures (UGRNN and +RNN). It introduces a novel method for quantifying task capacity using mutual information, providing a "bits-per-parameter" metric to measure how well RNNs store task information. The authors demonstrate that all RNN architectures achieve similar capacity bounds (approximately 5 bits per parameter) and highlight that trainability, rather than capacity, is the primary differentiator between architectures. The paper also examines the per-unit capacity of RNNs to remember input history and finds that this is not a practical bottleneck. The proposed +RNN architecture shows improved trainability for deeper networks, while the UGRNN is a simpler alternative to gated architectures. The work is notable for its thorough experimental setup, including principled hyperparameter tuning and significant computational resources. Overall, the paper provides valuable insights into RNN design and the trade-offs between capacity and trainability.
Decision: Accept
The paper makes a significant contribution to understanding RNN architectures by rigorously quantifying their capacity and trainability. The introduction of the mutual information-based capacity metric is novel and practical, and the proposed architectures offer meaningful improvements in specific scenarios. While there are weaknesses in the presentation of experimental details and results, these do not detract from the overall scientific rigor and value of the work.
Supporting Arguments
1. Novelty and Contribution: The use of mutual information to quantify task capacity is a novel and insightful approach. The introduction of two new RNN architectures, particularly the +RNN, addresses practical challenges in training deeper networks.
2. Thorough Experimental Setup: The authors employ a robust experimental framework, including multiple tasks, extensive hyperparameter tuning, and large-scale computational resources. This ensures the reliability of the findings.
3. Practical Insights: The results provide actionable guidance for practitioners, such as when to use vanilla RNNs versus gated architectures depending on resource constraints and task complexity.
4. Relevance to the Field: By revisiting older architectures with modern computational resources, the paper bridges gaps in the literature and provides a fresh perspective on RNN design.
Suggestions for Improvement
1. Clarity of Experimental Details: The paper could improve the clarity of its experimental setup. For example, terms like "unrollings" and details about input scaling are not well-defined. Providing a more explicit description of these aspects would enhance reproducibility.
2. Results Presentation: Some graphs and findings could be moved to the appendix to improve focus on key results. For example, capacity results could be summarized more concisely in the main text, with detailed plots relegated to supplementary material.
3. Isolating Key Findings: The paper presents a wealth of results, but it is sometimes difficult to isolate the most critical findings. A more structured discussion highlighting the main takeaways would improve readability.
4. Comparison with Related Work: While the paper references prior work, a more explicit comparison of the proposed architectures with existing ones in terms of computational cost and performance would strengthen the argument for their adoption.
Questions for the Authors
1. How sensitive are the capacity and trainability results to the choice of tasks? Would the findings generalize to more complex real-world tasks, such as machine translation or speech recognition?
2. Could you provide more details on the "unrollings" parameter and its impact on capacity? How does this relate to practical scenarios where sequence lengths vary?
3. The +RNN architecture shows promise for deeper networks, but how does it compare to LSTM and GRU in terms of computational efficiency during inference?
4. Did you explore the impact of different initialization strategies on the trainability of vanilla RNNs? Could better initialization mitigate their training difficulties?
In conclusion, this paper makes a valuable contribution to the field of RNN research, offering both theoretical insights and practical guidance. Addressing the suggested improvements would further enhance its impact.