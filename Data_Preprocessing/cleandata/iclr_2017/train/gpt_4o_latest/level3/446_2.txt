Review of the Paper: Adversarially Learned Inference (ALI)
Summary of Contributions
The paper introduces the Adversarially Learned Inference (ALI) model, which integrates inference and generation networks within the GAN framework. ALI stabilizes GAN training by matching the joint distribution of latent variables and data, providing a coherent mechanism for both generation and inference. Notably, ALI achieves high-quality reconstructions without an explicit reconstruction loss, demonstrating the effectiveness of its learned inference network. The paper also explores ALI's utility in semi-supervised learning tasks, achieving competitive results on SVHN and CIFAR10 datasets. The authors highlight the model's ability to interpolate in latent space and its potential for disentangling factors of variation, although this aspect is not fully explored.
Decision: Reject
While the paper presents a novel and promising approach, it falls short in several critical areas. The lack of comprehensive comparisons with existing methods, insufficient quantitative metrics for sample quality, and suboptimal semi-supervised learning strategies weaken the paper's overall contribution. Additionally, the paper does not convincingly demonstrate how ALI improves GAN stability or disentangles factors of variation.
Supporting Arguments
1. ALI's Contribution and Reconstruction Quality: The paper effectively demonstrates that ALI can learn a coherent inference network and achieve reasonable reconstructions without an explicit reconstruction term. This is a significant contribution, as it highlights the potential of adversarial training for joint inference and generation. However, the reconstructions are not always faithful, and the paper attributes this to underfitting without providing a detailed analysis or solution.
2. Comparison with Existing Methods: The paper lacks a thorough comparison with alternative inference approaches like InfoGAN or post-hoc mapping. While it briefly discusses these methods, it does not provide empirical evidence to substantiate ALI's superiority. This omission makes it difficult to assess ALI's relative performance.
3. Quantitative Metrics and Sample Quality: Although ALI produces visually high-quality samples, the absence of quantitative metrics (e.g., Inception Score or FID) limits the ability to evaluate its generative performance rigorously. This is a significant oversight, as such metrics are standard in the field.
4. Semi-Supervised Learning: The semi-supervised learning approach is suboptimal, as the concatenation of hidden layers is not well-motivated. While ALI achieves competitive results, the improvement seems to stem from Salimans et al.'s adaptation rather than ALI's inference network. A more integrated semi-supervised training strategy could enhance the model's utility.
5. Stochastic vs. Deterministic Inference: The distinction between ALI's stochastic inference and BiGAN's deterministic approach is underexplored. While the authors claim that stochastic inference offers more flexibility, they do not provide empirical evidence to support this claim.
Suggestions for Improvement
1. Comprehensive Comparisons: Include extensive empirical comparisons with InfoGAN, post-hoc mapping, and other inference methods to highlight ALI's advantages.
2. Quantitative Metrics: Provide standard quantitative metrics for sample quality to strengthen the evaluation of ALI's generative performance.
3. Semi-Supervised Learning: Explore joint training of the semi-supervised path with the generative path to improve performance and coherence.
4. Disentangling Factors of Variation: Demonstrate ALI's ability to disentangle factors of variation using discrete latent variables, similar to InfoGAN.
5. GAN Stability: Provide clearer evidence, both theoretical and empirical, that ALI improves GAN training stability.
Questions for the Authors
1. How does ALI's stochastic inference network compare empirically to BiGAN's deterministic approach in terms of flexibility and performance?
2. Can you provide quantitative metrics (e.g., Inception Score, FID) to evaluate ALI's sample quality rigorously?
3. Why was the concatenation of hidden layers chosen for semi-supervised learning, and how does it compare to joint training strategies?
4. Could you provide additional experiments to demonstrate ALI's ability to disentangle factors of variation?
In conclusion, while ALI is a promising contribution, the paper requires significant improvements in evaluation, comparisons, and clarity to reach the standard for acceptance.