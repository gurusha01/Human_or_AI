Review of the Paper
Summary of Contributions  
The paper addresses the critical challenge of transferring reinforcement learning (RL) policies from simulation to real-world systems. It introduces the Ensemble Policy Optimization (EPOpt) algorithm, which trains robust policies using an ensemble of simulated domains and a form of adversarial training. The authors propose a two-phase approach: (1) training policies on an ensemble of models to ensure robustness to parametric model errors and unmodeled effects, and (2) adapting the source domain ensemble using approximate Bayesian updates based on data from the target domain. Experimental results on the MuJoCo Hopper and Half-Cheetah tasks demonstrate that EPOpt produces policies that generalize better than standard policy search methods and are robust to unmodeled effects. The paper also highlights the potential of EPOpt to reduce sample complexity in real-world applications.
Decision: Accept  
Key reasons:  
1. Novelty and Practical Relevance: The paper tackles an important problem in RL—sim-to-real transfer—by proposing a novel combination of ensemble-based training and adversarial sampling. The approach is well-motivated and demonstrates practical utility in reducing brittleness and improving robustness of policies.  
2. Empirical Rigor: The experimental results are comprehensive, addressing key questions about robustness, generalization, and adaptation. The comparisons against standard policy search methods (e.g., TRPO) are convincing, and the results are scientifically rigorous.
Supporting Arguments  
- The paper is well-placed in the literature, building on concepts from Bayesian RL, robust control, and risk-sensitive RL. While the notion of adversarial training differs from recent GAN-based approaches, the use of adversarial sampling to emphasize poor-performing trajectories is a reasonable and innovative adaptation for this context.  
- The experiments effectively demonstrate the robustness of EPOpt policies to both parametric discrepancies and unmodeled effects. The inclusion of model adaptation further strengthens the practical applicability of the method.  
- The authors provide a clear explanation of the EPOpt algorithm, including its theoretical grounding (e.g., CVaR optimization) and implementation details. The supplementary materials and open-source code enhance reproducibility.
Suggestions for Improvement  
1. Clarity on Adversarial Training: The paper could benefit from a more explicit comparison of its adversarial training approach to other adversarial methods in RL, such as those used in GANs. This would help situate the work more clearly in the broader literature.  
2. Baseline Comparisons: Including results with alternative policy gradient methods (e.g., REINFORCE) and analyzing baseline value function performance would provide additional insights into the generality of EPOpt and its compatibility with other RL algorithms.  
3. Component Details: Several aspects of the algorithm (e.g., the choice of percentile for CVaR, the impact of hyperparameters like ε) could be elaborated further. While these were partially addressed in the Q&A round, integrating them into the main text would improve clarity.  
4. Computational Costs: The paper briefly mentions the computational intensity of sampling-based Bayesian updates. A more detailed discussion of computational trade-offs and potential optimizations would be valuable.
Questions for the Authors  
1. How does the performance of EPOpt compare to other robust RL methods, such as risk-sensitive RL or robust control approaches, on similar tasks?  
2. Could the authors provide additional insights into the choice of ε for CVaR optimization? How sensitive is the algorithm to this hyperparameter?  
3. Have the authors considered extending EPOpt to tasks with high-dimensional sensory inputs (e.g., vision-based tasks)? If so, what challenges do they foresee?  
4. How does the computational cost of EPOpt scale with the dimensionality of the parameter space in the source domain ensemble?
Conclusion  
The paper presents a significant contribution to the field of RL by addressing the sim-to-real gap with a novel and well-motivated approach. While there are areas for improvement, the strengths of the paper outweigh its limitations, and it is likely to spark further research in robust policy learning and transfer.