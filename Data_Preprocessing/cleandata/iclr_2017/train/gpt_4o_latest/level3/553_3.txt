Review of the Paper
Summary of Contributions
This paper explores the integration of near-data processing (NDP) into NAND flash-based solid-state drives (SSDs) for accelerating machine learning (ML) workloads. The authors propose a novel simulation platform, ISP-ML, which models in-storage processing (ISP) capabilities for ML tasks. They implement three variants of stochastic gradient descent (SGD) — synchronous, Downpour, and elastic averaging (EASGD) — to evaluate the performance of ISP-based optimization compared to conventional in-host processing (IHP). The paper highlights the potential of ISP for reducing data transfer overheads and improving computational efficiency, particularly under memory-constrained scenarios. The authors also identify future research directions, including the scalability of ISP for more complex ML models and optimization algorithms.
Decision: Reject
The paper presents an interesting idea of leveraging ISP for ML workloads, but it falls short in several critical areas. The lack of algorithmic novelty, reliance on simulation rather than hardware implementation, and use of outdated datasets and models limit its impact and relevance to the broader ICLR audience. Below, I elaborate on the reasons for this decision and provide constructive feedback.
Supporting Arguments for Rejection
1. Limited Novelty: While the integration of ISP into SSDs is intriguing, the paper focuses on existing algorithms (SGD variants) without introducing any novel ML techniques or optimization methods. The comparisons of train/test performance are therefore less meaningful in the absence of algorithmic innovation.
   
2. Simulation-Only Results: The work relies entirely on a simulated platform (ISP-ML) rather than demonstrating real-world hardware implementations. This limits the practical applicability and credibility of the results, especially given the challenges of translating simulation findings into hardware.
3. Outdated Dataset and Model: The use of a single-layer perceptron on an amplified MNIST dataset raises concerns about the scalability and relevance of the proposed approach. Modern ML workloads often involve large-scale datasets and deep neural networks, which are not addressed in this paper.
4. Scalability Concerns: The paper does not adequately address how the proposed ISP approach would scale to contemporary ML models with high storage and bandwidth demands. This omission weakens the applicability of the work to real-world scenarios.
Suggestions for Improvement
1. Incorporate Algorithmic Innovation: Introduce novel ML algorithms or ISP-specific optimization techniques to strengthen the paper's contribution to the ML community.
   
2. Demonstrate Hardware Implementation: While simulation is a reasonable starting point, a prototype hardware implementation would significantly enhance the paper's impact and practical relevance.
3. Use Modern Datasets and Models: Evaluate the approach on large-scale datasets (e.g., ImageNet) and deep neural networks to demonstrate its scalability and relevance to current ML challenges.
4. Clarify Scalability: Provide a detailed discussion on how the ISP framework can handle the storage and computational demands of modern ML workloads, including potential bottlenecks and solutions.
5. Extend Comparative Analysis: Compare ISP-based optimization with state-of-the-art distributed ML systems to better contextualize the advantages and limitations of the approach.
Questions for the Authors
1. How does the proposed ISP approach scale to large-scale datasets and deep neural networks, given the limited storage and bandwidth of SSDs?
2. What are the key challenges in translating the ISP-ML simulation results into real-world hardware implementations?
3. Could the authors provide more details on the specific hardware modifications required to implement ISP in commercial SSDs?
4. How does the performance of ISP compare to distributed ML systems in terms of energy efficiency and computational throughput?
In conclusion, while the paper addresses an interesting niche, it lacks the depth, novelty, and scalability required for acceptance at a venue like ICLR. Addressing the outlined concerns and incorporating the suggested improvements would make the work more impactful and relevant to the ML community.