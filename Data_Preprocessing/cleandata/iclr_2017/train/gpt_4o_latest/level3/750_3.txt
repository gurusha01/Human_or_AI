Review of the Paper
Summary of Contributions
This paper introduces a last-layer feature penalty as a regularization technique for neural networks, with a focus on low-shot learning. The authors provide both theoretical analysis and empirical evidence to explain how feature penalty regularization works, including its connection to batch normalization (BN) and weight decay. They propose a modified cost function that incorporates both feature and weight penalties, aiming to improve performance. The paper evaluates the proposed method on synthetic datasets, the Omniglot one-shot learning benchmark, and ImageNet, demonstrating competitive results. The authors also explore the relationship between feature penalty and BN, suggesting that the former acts as a "soft" batch normalization. While the paper offers a novel perspective on feature regularization and its potential benefits, it falls short of achieving state-of-the-art results on Omniglot and lacks clarity in certain areas.
Decision: Reject  
Key Reasons:  
1. Preliminary Nature of the Work: The proposed feature penalty method, while conceptually interesting, feels underdeveloped. Batch normalization outperforms the feature penalty in most settings, and combining the two yields better results, which diminishes the standalone value of the proposed approach.  
2. Lack of Clarity and Depth: The paper does not convincingly explain why feature penalty is particularly suited for low-shot learning compared to classical supervised tasks. The theoretical insights, while mathematically sound, fail to provide actionable intuition or strong justification for the method's utility in low-shot scenarios.  
Supporting Arguments
- Performance: The results on Omniglot (91.5%) are ~2% worse than Matching Networks, which are referenced but not included in Table 1 for direct comparison. This omission weakens the empirical validation of the proposed method. Additionally, BN consistently outperforms the feature penalty, and the combination of both is required to achieve competitive results, suggesting the feature penalty alone is insufficient.  
- Motivation: While the paper draws connections between feature penalty, BN, and weight decay, it does not adequately justify why this regularization is particularly beneficial for low-shot learning. The explanation provided is basic and lacks depth, leaving the reader unconvinced of its unique advantages.  
- Clarity: The revised version of the paper is still overly lengthy and dense, making it difficult to extract key insights. The relationship between feature penalty and BN remains unclear despite additional experiments.  
Suggestions for Improvement
1. Clarify the Motivation for Low-Shot Learning: Provide a stronger theoretical or empirical justification for why feature penalty is particularly suited for low-shot learning tasks. Compare its performance more directly with methods specifically designed for low-shot learning, such as Matching Networks or Prototypical Networks.  
2. Improve Empirical Validation: Include a direct comparison with state-of-the-art methods in Table 1 to contextualize the performance of the proposed approach. Additionally, explore more diverse low-shot learning benchmarks to strengthen the generalizability of the results.  
3. Refine the Theoretical Analysis: While the mathematical derivations are thorough, they lack actionable insights. Focus on providing more intuitive explanations for why feature penalty improves generalization and optimization in low-shot settings.  
4. Simplify the Presentation: The paper is overly dense and could benefit from a more concise and focused presentation. Highlight the key contributions and insights upfront, and streamline the theoretical sections to improve readability.  
Questions for the Authors
1. Why is the feature penalty particularly suited for low-shot learning compared to classical supervised tasks? Can you provide more theoretical or empirical evidence to support this claim?  
2. Why were Matching Networks not included in Table 1 for direct comparison, despite being referenced in the text?  
3. How does the feature penalty perform on other low-shot learning benchmarks beyond Omniglot? Have you considered evaluating it on tasks like mini-ImageNet or tiered-ImageNet?  
4. Could you elaborate on the practical implications of the connection between feature penalty and batch normalization? How would one decide when to use one over the other?  
In conclusion, while the paper introduces an intriguing idea, it requires further refinement and stronger empirical validation to reach the level of acceptance. The authors are encouraged to address the above concerns and resubmit after significant improvements.