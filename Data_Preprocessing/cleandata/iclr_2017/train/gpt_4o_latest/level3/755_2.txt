Review of the Paper
Summary of Contributions
This paper provides a theoretical explanation for the success of ResNet by analyzing deep linear networks and their nonlinear variants. The authors demonstrate that for 2-shortcuts and zero initialization, the Hessian's condition number is depth-invariant, which facilitates the training of very deep models. They argue that this property is unique to 2-shortcuts, as 1-shortcuts lead to unbounded condition numbers, and shortcuts deeper than two result in higher-order stationary points that are difficult to escape. The paper also explores the role of initialization schemes, showing that zero initialization with small perturbations outperforms Xavier and orthogonal initializations. Extensive experiments validate these theoretical findings, including comparisons of learning dynamics and final losses across different initialization schemes and shortcut depths.
Decision: Reject
While the paper presents an interesting theoretical result regarding the depth-invariance of the Hessian's condition number for 2-shortcuts, it falls short in addressing critical aspects that limit its broader impact. Specifically, the paper does not adequately address the gap between linear and nonlinear networks, and its assumptions and results may not generalize to practical scenarios involving nonlinearities and other key components like pooling layers.
Supporting Arguments for Decision
1. Linear vs. Nonlinear Networks: The paper focuses heavily on linear networks, which do not fully capture the complexities of training nonlinear networks. While the authors extend their analysis to networks with ReLU activations, the choice of placing ReLUs only in mid positions is not justified and may not reflect real-world architectures. Furthermore, the non-differentiability of ReLU at zero raises questions about the validity of the theoretical results, especially when alternative smooth activations like sigmoid or tanh could have been considered.
2. Generalization to Nonlinearities: The paper notes that for nonlinear activations, the condition number depends on the derivative at zero. However, this observation alone does not sufficiently explain the performance differences between linear and nonlinear networks. The authors do not explore whether their findings extend to other nonlinearities, such as pooling layers, which are critical in ResNet's success for computer vision tasks.
3. Practical Relevance: While the theoretical results are compelling, their practical implications are unclear. For instance, the paper does not address whether the depth-invariant condition number translates to better generalization or test error performance, which is a key metric in evaluating deep learning models.
Suggestions for Improvement
1. Addressing Nonlinearities: The authors should explore the impact of other nonlinearities, such as sigmoid, tanh, or pooling layers, to assess the generalizability of their results. A more detailed discussion on the role of ReLU's non-differentiability at zero and its implications for the Hessian analysis would also strengthen the paper.
2. Practical Experiments: The paper should include experiments on real-world datasets and architectures (e.g., convolutional ResNets) to demonstrate the practical relevance of the theoretical findings. The inclusion of batch normalization and its interaction with 2-shortcuts could also provide valuable insights.
3. Clarifying Initialization: The meaning and scale of "zero initialization with small random perturbations" should be clarified. Specifically, the authors should explain how the small perturbations are chosen and their impact on training dynamics.
4. Generalization to Test Error: The paper should address whether the depth-invariant condition number leads to better test error performance, as low training error does not necessarily imply good generalization.
Questions for the Authors
1. Why were ReLU activations chosen only in mid positions, and how do you justify their non-differentiability at zero in the context of your theoretical results?
2. Can the results on depth-invariance of the Hessian's condition number generalize to other nonlinearities, such as pooling layers or batch normalization?
3. How does the proposed initialization scheme (zero initialization with small perturbations) compare to other initialization methods in terms of test error and generalization performance?
In summary, while the paper provides an interesting theoretical contribution, its limited scope and lack of practical validation prevent it from making a strong case for acceptance. Addressing the outlined concerns would significantly enhance the paper's impact and relevance.