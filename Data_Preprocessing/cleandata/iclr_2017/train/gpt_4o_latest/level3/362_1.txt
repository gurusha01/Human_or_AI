Review
Summary
This paper proposes a novel approach to automating the design of optimization algorithms by formulating it as a reinforcement learning (RL) problem. The authors represent optimization algorithms as policies and use guided policy search (GPS) to learn these policies. The learned optimizer, referred to as "predicted step descent," is trained on random objective functions and evaluated on unseen tasks. The paper demonstrates that the learned optimizer outperforms traditional hand-engineered algorithms in terms of convergence speed and final objective value across various convex and non-convex optimization problems. The authors emphasize the transferability of the learned optimizer to unseen tasks, a critical aspect of its utility. This work contributes to the growing literature on "learning to learn" and optimization algorithm design, which has gained significant attention recently.
Decision: Reject  
While the paper presents an innovative and timely idea, the experimental evaluation and clarity of certain arguments fall short of the standards required for acceptance. The key reasons for this decision are the lack of rigorous comparative experiments and insufficient attention to scalability and transferability to larger, more complex domains.
Supporting Arguments
1. Strengths:  
   - The paper addresses an important and challenging problem in optimization and contributes to the "learning to learn" literature.  
   - The use of reinforcement learning to learn optimization algorithms is novel and well-motivated.  
   - The emphasis on transferability to unseen tasks is commendable and aligns with the broader goals of meta-learning.  
2. Weaknesses:  
   - The argument for using RL over traditional gradient-based methods at the meta-level is not well-supported. Comparative experiments with gradient-based meta-optimization approaches are missing, making it difficult to assess the advantages of the proposed method.  
   - Scalability to larger domains and real-world tasks is not addressed. The experiments focus on small-scale problems (e.g., low-dimensional logistic regression and small neural networks), limiting the practical impact of the work.  
   - The experimental results, while promising, are not sufficiently robust. The evaluation lacks statistical rigor (e.g., confidence intervals or significance testing) and does not explore the sensitivity of the learned optimizer to hyperparameters or training conditions.  
Suggestions for Improvement
1. Comparative Analysis: Include experiments comparing the proposed RL-based approach with gradient-based meta-optimization methods to justify the choice of RL.  
2. Scalability: Address how the method scales to larger, high-dimensional problems and more complex neural network architectures. Experiments on real-world datasets would strengthen the paper significantly.  
3. Experimental Rigor: Provide more comprehensive evaluations, including statistical analyses, ablation studies, and sensitivity analyses.  
4. Clarity of Argumentation: Improve the discussion on why guided policy search is particularly suited for this problem and how it compares to other RL methods.  
Questions for the Authors
1. Why was reinforcement learning chosen over gradient-based meta-optimization methods for training the optimizer? Can you provide empirical evidence supporting this choice?  
2. How does the learned optimizer perform on higher-dimensional problems or larger neural networks?  
3. Can the learned optimizer generalize to tasks with significantly different structures or distributions than those seen during training?  
In conclusion, while the paper introduces a valuable idea, the lack of rigorous experiments and attention to scalability limits its impact. Addressing these issues would make the work more compelling and suitable for publication.