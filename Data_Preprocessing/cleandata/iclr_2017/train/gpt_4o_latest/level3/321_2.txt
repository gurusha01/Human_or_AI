Review of the Paper
Summary of Contributions
This paper proposes a novel hierarchical reinforcement learning (HRL) framework that leverages Stochastic Neural Networks (SNNs) and an information-theoretic regularizer to learn diverse sub-policies (skills) in a pre-training phase. These skills are then utilized in downstream tasks with sparse rewards via a hierarchical policy structure. The approach combines intrinsic motivation with hierarchical methods, requiring minimal domain knowledge for skill pre-training. The authors demonstrate that their method improves exploration and performance across several challenging tasks, such as mazes and food collection, compared to strong baselines. The use of bilinear integration and mutual information (MI) bonuses is highlighted as critical for achieving skill diversity. Experimental results validate the framework's effectiveness in reducing sample complexity and solving sparse reward tasks.
Decision: Accept
The paper makes a significant contribution to the field of HRL by addressing the challenges of sparse rewards and long-horizon tasks. The proposed method is well-motivated, scientifically rigorous, and empirically validated. The integration of SNNs and MI-based regularization is novel and provides actionable insights for improving skill diversity and hierarchical policy learning. However, some limitations and areas for clarification remain, which are detailed below.
Supporting Arguments
1. Problem Relevance and Novelty: The paper tackles a critical problem in RLâ€”efficient exploration and learning in sparse reward settings. The use of SNNs for multi-modal policy representation and the MI bonus for skill diversity are innovative contributions that advance the state of the art in HRL.
2. Empirical Validation: The experiments are comprehensive, covering multiple tasks and environments. The results convincingly demonstrate the superiority of the proposed method over baselines, particularly in terms of exploration and sample efficiency.
3. Scientific Rigor: The methodology is well-grounded in theory, with clear explanations of the use of SNNs, bilinear integration, and MI regularization. The experimental design is robust, and the results are reproducible, with code and videos provided.
Suggestions for Improvement
1. Sub-Policy Diversity in Non-Locomotion Tasks: While the method performs well in locomotion tasks, its applicability to non-locomotion tasks requiring more diverse sub-policies is unclear. Future work could explore this limitation and propose adaptations for such scenarios.
2. Fixed Sub-Policies and Time Steps: The use of fixed sub-policies and switching intervals may limit adaptability in dynamic environments. A discussion on end-to-end hierarchical policy learning or adaptive switching mechanisms would strengthen the paper.
3. Domain-Specific Intrinsic Rewards: The reliance on domain-specific proxy rewards during pre-training raises concerns about generalizability. Exploring more universal intrinsic reward signals could enhance the method's applicability.
Questions for the Authors
1. Baseline Performance: Can you clarify why smooth intrinsic rewards fail to solve tasks compared to your method? Were there specific design choices in the baselines that contributed to their poor performance?
2. Pre-Trained Policy Experience: How does the quality of pre-trained policies affect downstream task performance? Did you observe significant variability across different random seeds for pre-training?
3. Integration Mechanisms: In Figure 1, could you confirm the interpretation of concatenation and bilinear integration mechanisms? How critical is the choice of integration for skill diversity?
4. Multi-Policy vs. SNN: Could you elaborate on the differences in performance and sample efficiency between multi-policy approaches and SNNs? Are there scenarios where one is preferable over the other?
Conclusion
Overall, this paper presents a strong contribution to HRL by addressing key challenges in sparse reward tasks with a novel combination of SNNs and MI-based regularization. While some limitations remain, the work is well-executed and provides a solid foundation for future research. The paper is recommended for acceptance.