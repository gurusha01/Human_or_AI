The paper introduces the Neural Knowledge Language Model (NKLM), an innovative approach to enhance traditional Recurrent Neural Network Language Models (RNNLMs) by integrating symbolic knowledge from external Knowledge Bases (KBs). This integration allows the model to handle unknown words by either generating them from a vocabulary or copying them from KB facts. The authors demonstrate the model's superiority over standard RNNLMs on a new dataset, WikiFacts, particularly in reducing unknown words and improving perplexity metrics. The NKLM also introduces a novel evaluation metric, Unknown-Penalized Perplexity (UPP), to address the limitations of standard perplexity in knowledge-related tasks. The paper claims that the NKLM can adapt to changes in knowledge without retraining and provides a promising step toward knowledge-aware language modeling.
Decision: Reject
While the paper presents a compelling and novel approach, the decision to reject is based on two key reasons: (1) insufficient clarity in the writing, particularly in Section 3, which hampers the full comprehension of the proposed architecture, and (2) critical omissions in the evaluation, such as the lack of performance comparison on standard benchmarks like the Penn Tree Bank and the absence of training time analysis relative to standard RNNLMs.
Supporting Arguments:
1. Strengths:
   - The integration of KBs into language modeling is a challenging yet promising direction, distinguishing this work from prior approaches like Pointer Sentinel Mixture Models.
   - The proposed copy mechanism effectively addresses the rare/unknown word problem, as evidenced by the significant reduction in unknown words and improved perplexity metrics.
   - The introduction of the WikiFacts dataset and UPP metric adds value to the field and could benefit future research.
2. Weaknesses:
   - The writing quality, especially in Section 3, is suboptimal. Key concepts such as the role of the knowledge context \(e\) and the initialization of fact embeddings (\(a_{t-1}\)) for the first word are inadequately explained.
   - The dependency on outdated KBs like Freebase raises concerns about the model's applicability to real-world, dynamic knowledge sources.
   - The paper does not address how the model handles unknown words with no embeddings in the generation history, as illustrated in the "Michelle" example.
   - The lack of evaluation on standard benchmarks and the omission of training time comparisons limit the ability to assess the model's generalizability and efficiency.
Additional Feedback:
- The authors should improve the clarity of Section 3 by providing more detailed explanations and examples for key components such as the knowledge context \(e\) and the fact-copy mechanism.
- The paper would benefit from a discussion on how the model could handle dynamic or incomplete KBs, addressing the limitations of relying on static datasets like Freebase.
- Including experiments on standard benchmarks like the Penn Tree Bank would strengthen the paper's claims of generalizability.
- A comparison of training time and computational efficiency with standard RNNLMs is necessary to evaluate the practical feasibility of the proposed approach.
Questions for the Authors:
1. How is the knowledge context \(e\) computed, and what is its specific role in the model's decision-making process?
2. How is the fact embedding \(a_{t-1}\) initialized for the first word in a sequence?
3. How does the model handle cases where unknown words have no embeddings or prior occurrences in the generation history?
4. Can the model adapt to dynamic KBs, and if so, how does it handle incomplete or outdated knowledge?
5. Why were standard benchmarks like the Penn Tree Bank excluded from the evaluation, and how does the model perform on such datasets?
In conclusion, while the paper introduces a novel and promising approach, the lack of clarity and critical omissions in evaluation prevent it from meeting the standards for acceptance at this time. Addressing these issues could significantly strengthen the contribution.