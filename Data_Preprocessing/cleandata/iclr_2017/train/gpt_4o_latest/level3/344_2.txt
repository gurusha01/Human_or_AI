Review
The paper introduces a novel approach to reinforcement learning (RL) in the context of StarCraft micromanagement scenarios, addressing the challenges of combinatorial perception and action spaces. The authors propose a hybrid algorithm combining black-box optimization and REINFORCE, which enables structured exploration in policy space. The paper also establishes deep RL baselines (DQN and REINFORCE) on StarCraft subdomains and demonstrates the superiority of their zero-order (ZO) optimization approach in terms of robustness and generalization across scenarios. The work contributes to the field by presenting StarCraft micromanagement as a challenging RL benchmark and introducing a method that outperforms existing approaches in this domain.
Decision: Reject
While the paper makes valuable contributions, it suffers from several critical issues that undermine its scientific rigor and clarity. The primary reasons for rejection are: (1) misalignment between the measured objective and the optimization target, and (2) mischaracterization of related work, which affects the credibility of the claims.
Supporting Arguments
1. Misalignment of Optimization Objective: The authors optimize the "gradient of the average cumulative reward" rather than the "average reward," which is the measured objective. This misalignment introduces a conceptual inconsistency that could lead to suboptimal or misleading results. The paper does not provide sufficient justification for this choice, nor does it explore its implications on performance.
2. Mischaracterization of Related Work: The description of Deterministic Policy Gradient (DPG) is incorrect, as it uses a stochastic behavior policy and learns off-policy about deterministic policies. Similarly, the characterization of gradient-free optimization is outdated, ignoring recent advancements (e.g., Koutnik et al., 2013) that demonstrate scalability beyond a few parameters. These inaccuracies weaken the foundation of the proposed method and its positioning in the literature.
3. Experimental Concerns: The parameter-space exploration experiments may not adhere to best practices in neuroevolution, raising questions about the validity of the reported results. Additionally, the high win rate in DQN transfer experiments, despite poor performance in the training domain, is suspicious and requires clarification. This inconsistency suggests either a typo or a methodological flaw.
Additional Feedback
1. Clarity and Justification: The paper would benefit from a clearer explanation of the choice to optimize the gradient of the average cumulative reward. A discussion of its theoretical implications and comparison with optimizing the average reward would strengthen the paper.
2. Experimental Rigor: The authors should ensure adherence to best practices in neuroevolution and provide more details about the experimental setup. For example, how were hyperparameters tuned, and how sensitive are the results to these choices?
3. Transfer Learning Results: The authors should clarify the discrepancy in DQN transfer results and provide additional analysis to explain the observed high win rate in transfer scenarios.
4. Broader Evaluation: While the proposed algorithm performs well on StarCraft micromanagement tasks, its applicability to other domains (e.g., Atari) should be empirically validated to demonstrate generality.
Questions for the Authors
1. Why was the "gradient of the average cumulative reward" chosen over the "average reward" as the optimization target? How does this choice affect the performance and generalization of the proposed algorithm?
2. Can you clarify the DQN transfer results? Is the high win rate in transfer scenarios a typo, or is there an underlying explanation for this discrepancy?
3. How does the proposed ZO algorithm compare to recent advancements in gradient-free optimization (e.g., Koutnik et al., 2013) in terms of scalability and efficiency?
4. Were best practices in neuroevolution followed during the parameter-space exploration experiments? If not, how might this have impacted the results?
By addressing these issues, the paper could significantly improve its scientific rigor and impact.