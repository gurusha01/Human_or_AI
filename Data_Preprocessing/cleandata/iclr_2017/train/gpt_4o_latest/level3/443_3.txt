The paper proposes a neural noisy channel model, \( P(x|y) \), based on the Segment-to-Segment Neural Transduction (SSNT) framework, which enables incremental alignment and prediction without requiring the complete sequence \( y \) to be observed beforehand. The authors demonstrate the model's ability to leverage unpaired output data effectively, making it particularly useful in low-resource settings. The paper also introduces a decoding algorithm that balances computational tractability with performance, and the experimental results show improvements over direct models in tasks like abstractive summarization, machine translation, and morphological inflection generation.
Decision: Accept
The paper makes a strong case for acceptance due to its practical contributions to noisy channel modeling, its ability to exploit unpaired data, and its demonstrated empirical improvements across diverse tasks. However, there are areas for improvement, particularly in the clarity of experimental baselines and computational efficiency.
Supporting Arguments:
1. Novelty and Practical Impact: While the SSNT model itself is not novel, its application as a noisy channel model and the introduction of a decoding algorithm tailored to this setup are meaningful contributions. The ability to use unpaired data addresses a critical challenge in many NLP tasks.
2. Empirical Validation: The experimental results convincingly demonstrate the model's superiority over direct models, particularly in low-resource scenarios. The use of human preference evaluations further strengthens the case for the model's practical utility.
3. Clarity and Writing: The paper is well-written and accessible, making the technical contributions clear to the audience. The channel model's broad applicability is of significant interest to the research community.
Suggestions for Improvement:
1. Baseline Comparisons: The experimental results lack key baseline numbers, such as "direct + LM + bias" and "direct + bias," which would provide a more comprehensive evaluation of the noisy channel model's performance. Adding these baselines would strengthen the empirical claims.
2. Clarification of Direct Model: It is unclear whether the direct model used in the experiments is based on SSNT or a traditional sequence-to-sequence framework. This distinction is important for contextualizing the results.
3. Computational Efficiency: While the \( O(|x|^2 \cdot |y|) \) training complexity is acceptable, it remains costly for long sequences. Further optimization or discussion of scalability would enhance the paper's impact.
4. Mathematical Justification: The use of a language model (LM) for smoothing predictions in the direct model, while effective, is mathematically unconventional. A more detailed explanation of why this works in practice would be valuable.
Questions for the Authors:
1. Could you provide the missing baseline results ("direct + LM + bias" and "direct + bias") to better contextualize the noisy channel model's performance?
2. Is the direct model in the experiments based on SSNT or a standard sequence-to-sequence model? Please clarify.
3. Have you explored any strategies to reduce the computational cost of training for long sequences? If so, what were the results?
4. Could you elaborate on why the LM improves the direct model's performance despite its unconventional mathematical integration?
In conclusion, the paper makes a significant contribution to the field by advancing noisy channel modeling with neural networks. Addressing the above concerns would further solidify its impact and clarity.