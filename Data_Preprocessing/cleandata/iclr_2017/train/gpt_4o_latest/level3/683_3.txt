Review of "Boosted Residual Networks"
The paper introduces a novel ensemble method, Boosted Residual Networks (BRN), which incrementally builds ensembles of Residual Networks using a boosting criterion. The authors combine the principles of Residual Networks, which utilize shortcut connections for efficient training of deep networks, with Deep Incremental Boosting, a method that incrementally grows networks by adding layers at each boosting round. The proposed method aims to improve training efficiency and generalization by leveraging the architecture of Residual Networks in a "white-box" ensemble approach. The paper also explores distilled and bagged variants of the method, providing insights into their potential.
Decision: Reject
The primary reasons for rejection are: (1) insufficient empirical validation, and (2) inadequate comparison with existing methods. While the proposed method is conceptually interesting, the experimental results fail to convincingly demonstrate its utility or superiority over simpler alternatives.
Supporting Arguments:
1. Empirical Validation: The experiments are conducted on small-scale datasets (MNIST, CIFAR-10, CIFAR-100) without employing standard data augmentation techniques. These datasets and experimental setups are outdated by modern standards, limiting the relevance of the results. Furthermore, the method's performance on CIFAR-10 does not surpass simple single-network baselines, raising concerns about its practical utility.
2. Baseline Comparison: The paper does not adequately compare BRN's performance with state-of-the-art methods. While CIFAR-100 results are somewhat promising, likely due to the ensemble's benefit in low-data regimes, the lack of comparison to modern augmentation techniques or regularization methods (e.g., dropout) weakens the claims of superiority.
3. Positioning the Method: The method could be compelling for scenarios with scarce labeled data or where standard augmentation is non-trivial. However, the paper does not explore such scenarios or provide benchmarks tailored to this use case. This leaves the practical applicability of the method unclear.
Suggestions for Improvement:
1. Stronger Empirical Validation: Conduct experiments on larger, more challenging datasets (e.g., ImageNet) and include standard data augmentation techniques. This would provide a more robust evaluation of the method's effectiveness.
2. Baseline Comparisons: Compare BRN to state-of-the-art methods, including advanced augmentation techniques and regularization methods. Additionally, evaluate its performance against simpler ensemble techniques like bagging or dropout.
3. Clarify Use Cases: Position the method more clearly for scenarios where it offers unique advantages, such as limited labeled data. Use benchmarks and comparisons that highlight these strengths.
4. Ablation Studies: Include ablation studies to isolate the contributions of the boosting mechanism, residual connections, and other components of the method.
Questions for the Authors:
1. Why were standard data augmentation techniques not used in the experiments? How would the method perform with such techniques applied?
2. Could the method's utility in low-data regimes be better demonstrated by testing on semi-supervised or few-shot learning benchmarks?
3. How does the computational cost of BRN compare to simpler ensemble methods or single-network baselines on larger datasets?
In conclusion, while the paper presents an interesting idea, its current empirical and comparative shortcomings make it unsuitable for acceptance. Addressing these issues could significantly strengthen the paper's contribution to the field.