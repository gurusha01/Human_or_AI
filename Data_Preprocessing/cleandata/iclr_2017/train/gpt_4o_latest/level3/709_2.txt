Review
Summary of Contributions
The paper introduces a novel method for improving sequence-to-sequence (seq2seq) models by leveraging unsupervised pretraining. Specifically, the authors propose initializing the encoder and decoder of a seq2seq model with pretrained language model weights derived from separate source and target monolingual corpora. This initialization is followed by joint fine-tuning using parallel data and a language modeling loss. The approach is evaluated on machine translation (Englishâ†’German) and abstractive summarization tasks, achieving state-of-the-art results on WMT'14 and WMT'15 benchmarks and competitive performance on summarization. The authors also conduct ablation studies to analyze the contributions of various components, such as pretraining the encoder versus the decoder and the role of the language modeling objective.
Decision: Accept
The paper makes a significant contribution to the field of seq2seq learning by demonstrating how unsupervised pretraining can improve both generalization and optimization. The method is well-motivated, achieves strong empirical results, and provides insights through thorough ablation studies. However, the paper could benefit from addressing some conceptual gaps and providing additional comparisons to related work.
Supporting Arguments
1. Strengths:
   - The proposed method effectively utilizes non-parallel monolingual corpora, which is a practical advantage over approaches requiring large parallel datasets.
   - The empirical results are compelling, with a 1.3 BLEU improvement over the previous state-of-the-art in machine translation and competitive performance in summarization.
   - The ablation studies are detailed and provide valuable insights into the contributions of different components of the method, such as pretraining the encoder versus the decoder and the importance of the language modeling objective.
   - The method is flexible and generalizable to multiple seq2seq tasks, as demonstrated by its application to both translation and summarization.
2. Weaknesses:
   - The objective function lacks a clear theoretical explanation for how non-parallel data improves predictions. While the empirical results are strong, a connection to expectation-based cross-entropy objectives (e.g., Chen et al., 2016) would strengthen the theoretical grounding.
   - The pretraining approach is conceptually similar to earlier work on deep neural network (DNN) pretraining (e.g., Dahl et al., 2011, 2012), but the paper does not provide direct comparisons or justify its superiority over these methods.
   - The summarization results, while competitive, do not surpass the state-of-the-art, and the authors acknowledge limitations such as shorter context windows and the use of unidirectional LSTMs.
Suggestions for Improvement
1. Theoretical Justification: Provide a more rigorous explanation of how the proposed objective function leverages non-parallel data for improved predictions. A discussion of connections to expectation-based objectives (e.g., Chen et al., 2016) would enhance the paper's theoretical contributions.
2. Comparisons to Related Work: Include empirical comparisons to earlier pretraining methods, such as those by Dahl et al. (2011, 2012), to highlight the advantages of the proposed approach.
3. Summarization Task: Address the limitations in the summarization task, such as the use of unidirectional LSTMs and shorter context windows, and discuss potential solutions or future work to overcome these constraints.
4. Clarity on Extensions: While the paper mentions extensions like residual connections and attention over multiple layers, their contributions to the final results are not clearly quantified. Providing more detailed analyses of these extensions would strengthen the paper.
Questions for the Authors
1. How does the proposed method compare empirically to DNN pretraining approaches (e.g., Dahl et al., 2011, 2012) in terms of training efficiency and performance?
2. Can the authors provide a theoretical explanation or intuition for why the language modeling objective serves as a strong regularizer, particularly in low-resource settings?
3. For the summarization task, would using bidirectional LSTMs or larger context windows significantly improve performance? Have these been explored in preliminary experiments?
Overall, the paper presents a strong contribution to seq2seq learning and is a valuable addition to the field. Addressing the above points would further enhance its impact.