Review
Summary of Contributions
The paper proposes a novel approach to training agents for information-seeking tasks by framing them as reinforcement learning (RL) problems and optimizing them using Generalized Advantage Estimation (GAE). The key contribution lies in explicitly modeling information gain to encourage exploration, which is a meaningful shift from traditional attention models. The authors introduce a set of tasks designed to evaluate information-seeking behavior, including cluttered MNIST, BlockWorld, CelebA conditional classification, and a Hangman-inspired language task. They demonstrate that their models can actively and efficiently gather information to reduce uncertainty and improve task-specific performance. The integration of intrinsic and extrinsic rewards is particularly noteworthy, as it encourages curiosity-driven exploration while maintaining task relevance.
Decision: Reject  
While the paper presents an interesting perspective on information-seeking behavior and introduces novel tasks, it fails to provide sufficient empirical evidence to convincingly support its claims. The lack of rigorous comparisons with existing RL frameworks and the unclear experimental setup hinder the evaluation of the proposed method's generality and effectiveness.
Supporting Arguments
1. Insufficient Empirical Evidence: The experimental results, while promising, do not convincingly demonstrate the superiority or generality of the proposed approach. For instance, the cluttered MNIST experiment lacks an apples-to-apples comparison with prior RL frameworks that do not model information gain, making it difficult to isolate the benefits of the proposed method.
2. Unclear Experimental Setup: In the cluttered MNIST task, the inclusion of additional "summary information" significantly simplifies the task, but this setup is not directly comparable to prior work. The differences in input settings between methods obscure the validity of the reported improvements.
3. Cluttered Presentation of Results: The experimental section is overly detailed and lacks clarity. A summary table comparing performance across tasks and methods would greatly enhance readability and allow for a more straightforward evaluation of the contributions.
Suggestions for Improvement
1. Stronger Baseline Comparisons: Provide direct comparisons with existing RL frameworks, particularly those that do not explicitly model information gain. This would help isolate the impact of the proposed intrinsic reward mechanism.
2. Clarify Experimental Setup: Ensure that experimental conditions are consistent across methods to enable fair comparisons. For example, avoid introducing additional input information (e.g., downsampled summaries) unless it is uniformly applied.
3. Summarize Results: Include a summary table that consolidates key performance metrics across tasks and methods. This would make it easier for readers to assess the overall effectiveness of the approach.
4. Expand Empirical Validation: Test the proposed method on a broader range of tasks or real-world applications to demonstrate its generality and practical utility.
5. Theoretical Justification: Provide a more detailed theoretical analysis of why explicitly modeling information gain leads to better exploration and task performance.
Questions for the Authors
1. How does the proposed method compare to baseline RL approaches that do not model information gain, particularly in terms of exploration efficiency and task performance?
2. Can the authors clarify the rationale behind introducing "summary information" in the cluttered MNIST task? How does this impact the fairness of comparisons with prior work?
3. Could the authors provide more details on the scalability of their approach? For example, how does the method perform on larger, more complex datasets or environments?
4. Is the intrinsic reward mechanism sensitive to hyperparameter tuning? If so, how robust is the method to changes in these parameters?
In conclusion, while the paper introduces an interesting perspective on information-seeking behavior, the lack of rigorous empirical validation and clarity in experimental design limits its impact. Addressing these concerns could significantly strengthen the paper's contributions.