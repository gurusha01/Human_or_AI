The paper addresses the critical issue of training neural networks in the presence of significant label noise, a problem that frequently arises in large-scale datasets with poor annotation quality. The authors propose two methods: (1) a latent variable model leveraging the Expectation-Maximization (EM) algorithm, and (2) a direct optimization approach that integrates out the true label to model \( p(z|x) \). Both methods explicitly model the noise through an additional softmax layer, with one approach assuming noise independence from features and the other accounting for feature-dependent noise. The paper claims that these methods outperform prior approaches on noisy datasets, as demonstrated through experiments on MNIST and CIFAR-100.
Decision: Reject
Key Reasons for Rejection:
1. Limited Real-World Applicability: The experimental results are primarily based on synthetic noise injected into MNIST and CIFAR-100 datasets. While these datasets are widely used benchmarks, the lack of real-world noisy datasets raises concerns about the generalizability of the proposed methods to practical scenarios.
2. Lack of Novelty: The paper builds on well-established techniques, such as the EM algorithm and noise adaptation layers, which have been explored in prior work. The contributions, while incremental, do not demonstrate sufficient novelty or significant advancements over existing methods.
Supporting Arguments:
- The problem of label noise is well-motivated, and the paper provides a comprehensive review of related work. However, the proposed methods are conceptually similar to prior approaches, such as those by Sukhbaatar & Fergus (2014) and Reed et al. (2014), with only minor extensions.
- The computational complexity of Equation 11, particularly for large datasets like ImageNet with many classes, is a significant concern. Although the authors propose a sparsification strategy for the second softmax layer, its efficacy on truly large-scale datasets remains unexplored.
- The experimental results, while promising, are limited to synthetic noise settings. The absence of evaluations on real-world noisy datasets undermines the practical impact of the proposed methods.
Suggestions for Improvement:
1. Real-World Evaluation: The authors should validate their methods on real-world noisy datasets, such as web-scraped image datasets or mislabeled biomedical datasets, to demonstrate practical applicability.
2. State-of-the-Art Comparison: The paper should include comparisons with state-of-the-art methods on noisy datasets to substantiate its claims of superior performance.
3. Parameter Recovery: It would be valuable to evaluate the ability of the proposed methods to recover the parameters of the noise distribution, as this could provide additional insights into their robustness.
4. Scalability Analysis: A detailed analysis of the computational cost on large-scale datasets with many classes (e.g., ImageNet) is necessary to address concerns about scalability.
Questions for the Authors:
1. How does the proposed method perform on real-world noisy datasets where the noise distribution is unknown and potentially more complex than synthetic noise?
2. Can the authors provide a detailed comparison with recent state-of-the-art methods for handling label noise, particularly in terms of accuracy and computational efficiency?
3. How sensitive are the proposed methods to the initialization of the noise adaptation layer? Would poor initialization lead to suboptimal convergence?
In summary, while the paper addresses an important problem and proposes reasonable methods, the lack of real-world evaluations, limited novelty, and scalability concerns prevent it from meeting the standards for acceptance at this time.