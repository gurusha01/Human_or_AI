The paper proposes the Gated-Attention (GA) Reader, a novel model for cloze-style question answering that integrates a multi-hop architecture with a gated attention mechanism. This mechanism leverages multiplicative interactions between query embeddings and document representations to iteratively refine token representations and improve answer selection. The authors demonstrate state-of-the-art or competitive performance on several benchmark datasets, including CNN, Daily Mail, Who-Did-What (WDW), and CBT. The paper is well-written, with clear explanations of the model, comprehensive experimental comparisons, and insightful ablation studies.
Decision: Accept
The paper should be accepted due to its significant contributions to the field, including its novel gated-attention mechanism, strong empirical results, and thorough analysis. However, some areas require clarification and further exploration.
Supporting Arguments:
1. Problem and Motivation: The paper addresses the critical problem of combining multi-hop reasoning and attention mechanisms for text comprehension. The motivation is well-grounded in prior work, and the proposed gated-attention mechanism is a meaningful extension of existing methods.
2. Empirical Results: The GA Reader achieves state-of-the-art results on multiple datasets, outperforming strong baselines. The performance improvements are consistent across both large-scale datasets (CNN, Daily Mail) and smaller, more challenging datasets (WDW, CBT).
3. Ablation Studies: The ablation studies effectively highlight the importance of the gated-attention mechanism and other model components, such as token-specific attention and pre-trained embeddings. These experiments provide strong evidence for the model's design choices.
Additional Feedback for Improvement:
1. Clarity on Multi-Hop Attention: While the ablation studies emphasize the importance of gated attention, the specific contributions of the multi-hop architecture remain unclear. A comparison with single-hop models (beyond K=1 in Table 4) would help isolate its impact.
2. Token Representation Components: The role of token representation components, particularly the character-level embeddings (C(w)), is underexplored. The authors should provide a more detailed analysis of how these components contribute to the model's performance.
3. Performance Drop in Ablations: The performance drop in ablated models raises questions about whether the improvements stem primarily from gated attention or other factors, such as pre-trained embeddings. A more granular breakdown of these contributions would strengthen the claims.
4. Theoretical Justification: While the empirical results favor multiplicative gating, the theoretical reasoning behind its superiority over addition or concatenation is not provided. This could be an avenue for future work but should be acknowledged as a limitation.
Questions for the Authors:
1. How does the multi-hop architecture contribute to performance beyond the gated-attention mechanism? Could a single-hop model with gated attention achieve similar results?
2. Can you provide more insights into the significance of character-level embeddings (C(w)) and their interaction with word embeddings (L(w)) in smaller datasets like WDW and CBT?
3. The paper mentions that the GA Reader performs better with larger datasets. How does the model handle overfitting in smaller datasets, and what role do pre-trained embeddings play in this?
In conclusion, the paper makes a strong contribution to the field of machine reading comprehension and introduces a novel mechanism with broad applicability. Addressing the above points would further enhance the paper's impact and clarity.