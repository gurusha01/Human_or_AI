Review
Summary of the Paper
The paper proposes a novel video captioning model, ASTAR, that combines soft and hard attention mechanisms to dynamically focus on different levels of feature abstraction (CNN layers) and spatiotemporal regions within those layers. The model employs a C3D encoder to extract video features and an RNN decoder to generate captions. The key innovation lies in the dual attention mechanism, which adaptively selects features from multiple CNN layers and aligns them spatiotemporally. The approach is evaluated on YouTube2Text, M-VAD, and MSR-VTT datasets, demonstrating state-of-the-art performance on YouTube2Text. The authors argue that leveraging intermediate CNN layer features improves semantic richness and context-awareness in video captioning.
Decision: Reject  
While the paper presents a well-executed and technically sound approach, the contribution is too incremental for a high-impact venue like ICLR. The proposed method builds on existing attention-based frameworks and primarily extends them by incorporating multi-layer attention, which, while effective, does not represent a significant conceptual leap. The novelty is limited, and further experiments or analyses are needed to substantiate the broader applicability and implications of the proposed method.
Supporting Arguments
1. Strengths:
   - The paper is well-written, with clear explanations of the methodology and attention mechanisms.
   - The experiments are thorough, demonstrating the benefits of multi-layer attention through quantitative results on multiple datasets.
   - The use of C3D features and spatiotemporal alignment is a thoughtful design choice that aligns with the problem of video captioning.
   - The results on YouTube2Text are strong, achieving state-of-the-art performance.
2. Weaknesses:
   - The contribution is incremental, as the model primarily extends existing attention mechanisms by incorporating multi-layer attention. This builds on prior work without introducing a fundamentally new concept.
   - The experiments, while effective, are limited in scope. For instance, the paper could have explored ablation studies to isolate the impact of each component (e.g., spatiotemporal alignment vs. multi-layer attention).
   - The broader applicability of the method remains unclear. The paper focuses heavily on YouTube2Text, with limited discussion on how the approach generalizes to other datasets or tasks.
   - The paper lacks a deeper theoretical analysis or insights into why multi-layer attention works better, beyond empirical results.
Suggestions for Improvement
1. Expand Experiments: Conduct more comprehensive experiments, including ablation studies to isolate the contributions of individual components (e.g., spatiotemporal alignment, multi-layer attention). Additionally, evaluate the model on more diverse datasets to demonstrate generalizability.
2. Theoretical Insights: Provide a deeper theoretical analysis or intuition for why multi-layer attention improves performance. This could include visualizations of attention maps or qualitative comparisons of generated captions.
3. Comparison with Baselines: Include comparisons with more recent or advanced baselines, such as transformer-based models, to contextualize the improvements.
4. Broader Impact: Discuss potential applications of the proposed method beyond video captioning, such as video question answering or summarization, to highlight its versatility.
Questions for the Authors
1. How does the model perform on datasets with significantly different characteristics (e.g., longer videos, more complex scenes)?
2. Can you provide qualitative examples or visualizations of the attention mechanism to illustrate how the model focuses on different layers and regions?
3. Did you explore alternative architectures, such as transformers, for the decoder? If not, how do you justify the choice of RNNs given their limitations in handling long-range dependencies?
In conclusion, while the paper is well-executed and demonstrates promising results, the contribution is not substantial enough for ICLR. The authors are encouraged to address the above concerns and consider submitting to a more specialized venue focusing on video understanding or multimedia applications.