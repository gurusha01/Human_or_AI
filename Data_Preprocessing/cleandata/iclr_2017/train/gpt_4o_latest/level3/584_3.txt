Review
Summary of Contributions
The paper introduces a Joint Many-Task (JMT) model for training multiple NLP tasks in a single end-to-end framework. Unlike traditional pipeline approaches, the model predicts increasingly complex tasks at successively deeper layers, leveraging linguistic hierarchies. It incorporates shortcut connections to word representations and lower-level task predictions, ensuring information flow across layers. A novel successive regularization strategy is proposed to mitigate catastrophic interference, preserving the accuracy of lower-level tasks during high-level task training. The model achieves state-of-the-art results on chunking, dependency parsing, semantic relatedness, and textual entailment, while also performing competitively on POS tagging. The simplicity of the model, compared to prior works, is a notable strength, as it avoids complex mechanisms like beam search or attention while still delivering competitive results.
Decision: Reject
While the paper presents an interesting idea and achieves some promising results, the experimental evidence and analysis are insufficient to support its claims fully. The mixed results in Table 1, lack of clarity in the dependency score comparisons, and limited evidence for the effectiveness of successive regularization weaken the paper's overall impact.
Supporting Arguments for Decision
1. Mixed Results and Lack of Clear Patterns: The experimental results in Table 1 show that adding more tasks sometimes degrades the performance of higher-level tasks. This inconsistency raises concerns about the robustness and generalizability of the proposed approach. The paper does not adequately explain these patterns or provide insights into why performance decreases in some cases.
   
2. Insufficient Evidence for Successive Regularization: While successive regularization is a novel idea, its effectiveness is not convincingly demonstrated. The paper mentions that it helps when dataset sizes are imbalanced, but this is not systematically explored. The lack of strong empirical evidence limits the significance of this contribution.
3. Unfair Dependency Parsing Comparisons: The dependency parsing results (UAS/LAS) are compared to prior works without accounting for differences in tree formation guarantees. This undermines the validity of the reported improvements.
Suggestions for Improvement
1. Clarify Mixed Results: Provide a deeper analysis of why higher-level task performance sometimes decreases with the addition of more tasks. Investigate whether this is due to optimization challenges, task conflicts, or other factors.
2. Strengthen Evidence for Successive Regularization: Conduct ablation studies or experiments on more diverse datasets to demonstrate the consistent benefits of successive regularization. Explore its impact on tasks with varying dataset sizes and complexities.
3. Fair Comparisons for Dependency Parsing: Ensure that dependency parsing results are compared under similar conditions, or explicitly discuss the differences in evaluation setups to provide a fair context for the reported improvements.
4. Explore Task Iteration Strategies: The paper briefly mentions the need for further exploration of task iteration strategies. Including preliminary experiments or insights on this aspect could strengthen the paper's contributions.
5. Address Model Limitations: Discuss the limitations of the proposed approach, such as its inability to consistently improve higher-level tasks or the potential inefficiencies of training all tasks jointly.
Questions for the Authors
1. Can you provide more detailed insights into why higher-level task performance sometimes decreases when more tasks are added? Are there specific task combinations that conflict with each other?
2. How does successive regularization compare to other regularization techniques, such as elastic weight consolidation (EWC), in preventing catastrophic interference?
3. Can you clarify the differences in tree formation guarantees for dependency parsing and how they might affect the reported results?
4. Have you considered alternative training schedules or task iteration strategies to mitigate the observed performance drops for higher-level tasks?
In conclusion, while the paper introduces a promising approach to joint multi-task learning, the lack of robust experimental evidence and clarity in results limits its impact. Addressing these issues could significantly strengthen the paper for future submissions.