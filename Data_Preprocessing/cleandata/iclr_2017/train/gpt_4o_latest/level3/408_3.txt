Review
The paper proposes a multi-view learning approach for acoustic sequence representation, leveraging bidirectional LSTM models with contrastive losses. It introduces novel domain-specific objectives and demonstrates improvements over prior work in acoustic word discrimination, cross-view word discrimination, and word similarity tasks. By applying a known architecture (bi-LSTMs) to a new domain, the authors establish new benchmarks for multi-view models and suggest open-sourcing their implementation for reproducibility.
Decision: Accept
The paper is well-motivated, scientifically rigorous, and makes meaningful contributions to the field. The key reasons for acceptance are:  
1. Novelty and Impact: The multi-view learning approach and domain-specific objectives are innovative and address a gap in acoustic word embeddings research.  
2. Strong Empirical Results: The proposed method outperforms prior approaches on multiple tasks, establishing new benchmarks and demonstrating robustness across seen and unseen data.  
Supporting Arguments
1. Problem Definition and Motivation: The paper clearly identifies the limitations of prior single-view approaches to acoustic word embeddings and motivates the need for a multi-view framework. The use of bi-LSTMs and contrastive losses is well-placed in the literature, and the multi-view setup is a logical extension to improve embedding quality for both acoustic and orthographic representations.
2. Scientific Rigor: The experiments are thorough, with clear baselines and detailed ablation studies. The authors explore multiple contrastive loss functions, including cost-sensitive objectives, and provide insights into their effects on different tasks. The use of rank correlation for word similarity evaluation is a thoughtful addition, aligning well with the goals of the proposed approach.
3. Empirical Validation: The results convincingly support the claims. The combined objective (obj0 + obj2) achieves state-of-the-art performance on acoustic word discrimination and cross-view word discrimination tasks. The cost-sensitive loss improves word similarity rankings, demonstrating the flexibility of the proposed framework.
Additional Feedback for Improvement
1. Clarity of Presentation: While the methodology is sound, the paper could benefit from a more concise explanation of the contrastive loss objectives. The mathematical notation, while precise, might overwhelm readers unfamiliar with the domain. Including a high-level summary or visual diagram of the objectives would improve accessibility.
2. Broader Evaluation: The paper primarily focuses on intrinsic tasks (e.g., word discrimination). While these are valid proxies, evaluating the embeddings on downstream applications like spoken term detection or query-by-example search would strengthen the practical relevance of the work.
3. Negative Sampling Strategy: The negative sampling approach is briefly mentioned but could be elaborated. How does the uniform sampling strategy affect the performance, and could more sophisticated sampling methods (e.g., hard negatives) further improve results?
4. Phonetic Supervision: The authors suggest using phonetic sequences instead of orthography as a future direction. It would be helpful to discuss the feasibility of this approach and its potential impact on the proposed framework.
Questions for the Authors
1. How does the performance of the embeddings vary with different types of acoustic features (e.g., MFCCs vs. other representations)?  
2. Could the proposed framework be extended to handle non-word acoustic segments, as mentioned in the conclusion? If so, what challenges do you anticipate?  
3. Have you considered using hard negative mining instead of random sampling for the contrastive losses?  
In conclusion, this paper makes a significant contribution to the field of acoustic word embeddings by introducing a novel multi-view learning framework and achieving state-of-the-art results. With minor improvements in presentation and broader evaluation, it has the potential to set a new standard for research in this area.