Review of "Pruning Where It Matters: Filter-Level Pruning for CNN Acceleration"
Summary of Contributions
This paper introduces a novel approach to reduce computational costs in Convolutional Neural Networks (CNNs) by pruning filters with minimal impact on output accuracy. Unlike weight-based pruning, which often results in irregular sparsity and requires specialized libraries, the proposed method removes entire filters and their associated feature maps, maintaining dense connectivity. The authors employ an `l1-norm` criterion to identify less important filters, demonstrating that this approach achieves significant reductions in FLOP (e.g., up to 34% for VGG-16 and 38% for ResNet-110 on CIFAR-10) while preserving accuracy through retraining. The paper also explores sensitivity analysis across layers, proposes holistic pruning strategies for complex architectures like ResNets, and compares its effectiveness against random pruning, largest-filter pruning, and activation-based methods. The clarity of the paper, supported by detailed explanations and visuals, further strengthens its impact.
Decision: Accept
The paper is well-written, presents an innovative and practical idea, and provides strong empirical results. The key reasons for acceptance are:
1. Novelty and Practicality: The "pruning where it matters" approach is innovative, offering a structured and hardware-friendly alternative to weight-based pruning.
2. Clarity and Execution: The methodology is clearly described with excellent visuals, and the experiments are thorough, demonstrating the method's effectiveness across multiple architectures.
Supporting Arguments
1. Well-Motivated Approach: The paper builds on prior work in model compression and pruning, addressing limitations of weight-based pruning (e.g., irregular sparsity) and proposing a structured alternative. The use of `l1-norm` for filter selection is simple yet effective, and the authors justify its use with empirical comparisons.
2. Experimental Rigor: The experiments are well-executed, with results showing consistent FLOP reductions and minimal accuracy loss across VGG-16, ResNet-56/110, and ResNet-34. Sensitivity analysis and comparisons with alternative pruning criteria further validate the approach.
3. Clarity: The paper is exceptionally clear, with well-organized sections, precise notation, and helpful figures. The use of `\mathbb` notation for mathematical clarity is commendable.
Suggestions for Improvement
While the paper is strong overall, the following points could enhance its impact:
1. Comparison with Other Pruning Criteria: The paper briefly compares its `l1-norm` criterion to random and largest-filter pruning but does not include a direct comparison with other established criteria like "min sum of weights" or "pruning at random." Adding these comparisons would strengthen the claims.
2. Broader Applicability: The experiments focus on CIFAR-10 and ImageNet datasets. Exploring the method's performance on other tasks (e.g., object detection or segmentation) could demonstrate its generalizability.
3. Ablation Studies: While sensitivity analysis is conducted, an ablation study on the retraining process (e.g., varying retraining epochs or learning rates) could provide deeper insights into the trade-offs between pruning aggressiveness and retraining effort.
Questions for the Authors
1. How does the proposed `l1-norm` pruning criterion compare to other established criteria like "min sum of weights" or activation-based pruning in terms of computational efficiency and accuracy retention?
2. Can the method be extended to other types of neural networks (e.g., transformers or RNNs), and if so, what challenges might arise?
3. How does the approach perform on tasks beyond image classification, such as object detection or semantic segmentation?
In conclusion, this paper makes a significant contribution to the field of model compression and pruning, and I recommend its acceptance with minor revisions to address the above points.