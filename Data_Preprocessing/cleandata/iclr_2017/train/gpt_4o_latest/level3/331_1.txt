Review
Summary of Contributions
The paper addresses the problem of skill transfer in reinforcement learning (RL) between morphologically different agents, such as robots with varying numbers of links or actuation mechanisms. The authors propose a novel method for learning invariant feature spaces using shared proxy tasks, enabling skill transfer across agents. The approach involves training embedding functions with a contrastive loss and autoencoder losses to create a shared feature space. This shared space is then used to guide the learning of new tasks in the target domain through a transfer reward. The method is evaluated on simulated robotic tasks in MuJoCo, including state-based and pixel-based settings, demonstrating transfer between robots with different morphologies and actuation mechanisms. The results show that the proposed method outperforms several baselines, including canonical correlation analysis (CCA), random projections, and direct mapping.
Decision: Reject  
While the paper tackles an important problem and presents an interesting approach, several limitations in the experimental design, analysis, and connections to existing literature prevent it from meeting the standards for acceptance. The primary reasons for rejection are the lack of thorough experimental analysis and insufficient exploration of connections to related fields like metric learning and multi-task learning.
Supporting Arguments
1. Strengths:
   - The problem of transferring skills between morphologically different agents is highly relevant and underexplored in RL.
   - The proposed method of learning invariant feature spaces is novel and well-motivated.
   - The experiments demonstrate the feasibility of the approach in both state-based and pixel-based settings, which is a significant contribution.
2. Weaknesses:
   - Experimental Design: The experiments feel rushed and lack critical details. For instance, Figure 7.b is missing, and there is no discussion of the sample budget or embedding training iterations during transfer. These omissions make it difficult to assess the robustness and scalability of the proposed method.
   - Baselines: While the paper compares its approach to CCA and other baselines, additional comparisons to simpler methods like random projections or more advanced baselines (e.g., adversarial domain adaptation) could clarify the impact of embedding specialization.
   - Connections to Related Work: The paper does not adequately explore connections to metric learning or multi-task learning, which could provide valuable insights into the theoretical underpinnings of the proposed method.
   - Assumptions: The assumption of trivial time alignment between agents is a significant limitation. While the authors briefly mention dynamic time warping (DTW) as a potential solution, this aspect is not explored in depth.
   - Annealing α: The authors suggest that annealing the transfer reward weight (α) might improve results but do not provide empirical evidence to support this claim.
Suggestions for Improvement
1. Experimental Rigor: Provide a more comprehensive experimental analysis, including details on sample budgets, embedding training iterations, and complete results (e.g., Figure 7.b). Additionally, evaluate the method on more diverse tasks and environments to demonstrate generalizability.
2. Baselines: Include comparisons to additional baselines, such as random projections and adversarial domain adaptation. This would help isolate the benefits of the proposed embedding method.
3. Theoretical Connections: Explore connections to metric learning and multi-task learning, which could strengthen the theoretical foundation of the work.
4. Alignment Techniques: Address the limitation of trivial time alignment by incorporating and evaluating more robust alignment methods like DTW or learned matching functions.
5. Ablation Studies: Conduct ablation studies to analyze the impact of key components, such as the autoencoder losses and the transfer reward weight (α). Empirical evidence on annealing α would be particularly valuable.
Questions for the Authors
1. How does the method perform when the assumption of trivial time alignment is violated? Have you tested the robustness of the approach with noisy or misaligned data?
2. Can you provide more details on the sample budget and embedding training iterations during transfer? How do these factors affect the scalability of the method?
3. Why were certain baselines, such as adversarial domain adaptation, not included in the comparisons? Would these methods be applicable to your problem setting?
4. Have you considered annealing α during training? If so, what were the results?
While the paper makes a meaningful contribution to the study of skill transfer in RL, addressing the above concerns would significantly strengthen its impact and clarity.