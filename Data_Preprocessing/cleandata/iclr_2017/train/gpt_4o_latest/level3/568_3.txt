Review of the Paper
The paper introduces a Character-Aware Attention Residual Network (CAR-Net) for sentence embedding, aimed at improving short text classification. The authors propose a model that combines character-level and word-level embeddings, applies attention mechanisms to highlight significant features, and uses a residual network to refine the sentence representation. The model is evaluated on three datasets (Tweets, Question, and AG News), demonstrating superior performance on two datasets compared to baseline models but failing to outperform TFIDF-SVM on the AG News dataset. The authors claim that their approach effectively addresses the challenges of feature sparsity and noise in short text classification.
Decision: Reject
The primary reasons for rejection are the lack of novelty in the proposed approach and insufficient experimental rigor. While the paper combines character-level embeddings, attention mechanisms, and residual networks in an interesting way, all these components have been extensively studied in prior research. The work does not introduce a fundamentally new concept or methodology. Furthermore, the evaluation is limited to three datasets, two of which are not widely recognized benchmarks, and the baselines used for comparison are relatively weak, with the exception of TFIDF-SVM.
Supporting Arguments for Rejection
1. Lack of Novelty: The use of character-aware embeddings, attention mechanisms, and residual networks has been well-documented in the literature. The paper does not sufficiently differentiate its approach from existing methods or provide a compelling argument for its novelty.
   
2. Limited Evaluation: The datasets used (Tweets, Question, AG News) are either small or not standard benchmarks. The absence of widely-used datasets like TREC or SST limits the generalizability of the results. Moreover, the baselines (e.g., TF-SVM, Lg. Conv, Sm. Conv) are not state-of-the-art, making the performance comparison less meaningful.
3. Unclear Dataset Details: The creation process and domain of the "Question" dataset are not described in detail, raising concerns about reproducibility and potential biases.
4. Citation Formatting Issues: The paper does not adhere to standard academic citation formatting, which detracts from its professionalism and readability.
Suggestions for Improvement
1. Stronger Baselines: Compare the proposed model against more competitive baselines, such as BERT, RoBERTa, or other transformer-based models, which are the current state-of-the-art in text classification.
2. Use Standard Datasets: Evaluate the model on widely-used datasets like TREC, SST, or IMDb to enable fair comparisons with existing work.
3. Clarify Dataset Details: Provide a detailed description of the "Question" dataset, including its domain, size, and label distribution, to ensure transparency and reproducibility.
4. Highlight Novelty: Clearly articulate the unique contributions of the proposed approach and how it advances the state of the art beyond existing methods.
5. Fix Citation Formatting: Ensure that all citations follow the conference's required format to improve the paper's presentation quality.
Questions for the Authors
1. What is the specific domain and creation process of the "Question" dataset? How were the labels determined?
2. Why were widely-used datasets like TREC or SST not included in the evaluation?
3. How does the proposed model compare to transformer-based models like BERT or RoBERTa in terms of performance and computational efficiency?
In summary, while the paper addresses an important problem in short text classification, the lack of novelty, limited evaluation, and insufficient rigor in experimental design prevent it from making a significant contribution to the field.