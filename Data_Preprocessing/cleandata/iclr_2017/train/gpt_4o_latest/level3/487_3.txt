Review
Summary of Contributions
The paper proposes a sparsely-connected neural network architecture aimed at reducing the memory and energy requirements of deep neural networks (DNNs), particularly in hardware implementations. By leveraging linear-feedback shift registers (LFSRs) to generate sparse connection masks, the authors claim to achieve up to 90% reduction in connections while improving or maintaining accuracy on datasets such as MNIST, CIFAR-10, and SVHN. The paper also introduces a hardware-efficient VLSI implementation of the proposed architecture, demonstrating significant reductions in silicon area (up to 90%) and energy consumption (up to 84%) compared to conventional fully-connected networks. The authors argue that their method acts as a regularizer, preventing overfitting and achieving state-of-the-art results in certain configurations.
Decision: Reject
While the paper presents an interesting approach to sparsely-connected networks and hardware efficiency, it suffers from several critical shortcomings that undermine its contributions. The primary reasons for rejection are the subpar baselines used for comparison and the lack of scientific rigor in validating the claims.
Supporting Arguments
1. Subpar Baselines: The floating-point misclassification rate of 1.33% reported in Table 2 for MNIST is far from the achievable state-of-the-art (~0.8%) for the given topology. Similarly, the CIFAR-10 floating-point precision results fail to meet the expected baseline of <9% error rate for convolutional networks. This raises concerns about the validity of the baseline comparisons and the claimed improvements.
2. Unfair Comparisons: The comparison between floating-point and binary precision is flawed due to the lack of regularization techniques (e.g., Gaussian noise, dropout) applied to the floating-point models. This omission biases the results in favor of binary precision, making the comparisons scientifically unsound.
3. Unexplained Phenomena: The observation that binary precision outperforms floating-point precision in certain CIFAR-10 experiments is not adequately explained. This discrepancy warrants further investigation and discussion.
4. Limited Generalization: The proposed method is only tested on relatively simple datasets (MNIST, CIFAR-10, and SVHN). The paper does not evaluate its performance on more challenging datasets or tasks, which would better demonstrate the robustness and scalability of the approach.
5. Reviewer Results: A reviewer claims to achieve a 0.6% error rate on MNIST with two fully connected hidden layers, further highlighting the inadequacy of the baselines used in the paper.
Additional Feedback
1. Clarity of Results: The paper should provide a more detailed justification for the observed performance gains, particularly in cases where sparsely-connected networks outperform fully-connected ones. Are these gains due to the sparsity itself, or are there other factors at play?
2. Experimental Rigor: The authors should incorporate stronger baselines and apply standard regularization techniques to ensure fair comparisons. Additionally, testing the proposed approach on more complex datasets (e.g., ImageNet) or tasks (e.g., object detection) would strengthen the paper's contributions.
3. Hardware Implementation Details: While the VLSI implementation results are promising, the paper would benefit from a more thorough discussion of the trade-offs involved (e.g., latency, scalability) and comparisons with other hardware-efficient architectures.
4. Reproducibility: The authors should provide open-source code and detailed experimental setups to facilitate reproducibility and independent validation of their claims.
Questions for the Authors
1. Why were regularization techniques not applied to the floating-point models in the CIFAR-10 experiments? How might this omission affect the results?
2. Can the proposed sparsely-connected network be applied to more complex datasets or tasks? If so, what are the expected challenges?
3. How does the proposed method compare to other sparsity-inducing techniques, such as structured sparsity learning (SSL), in terms of accuracy and hardware efficiency?
In conclusion, while the paper introduces an intriguing approach to sparsely-connected networks and hardware efficiency, its shortcomings in baseline selection, experimental rigor, and generalization limit its impact. Addressing these issues could significantly improve the quality and relevance of the work.