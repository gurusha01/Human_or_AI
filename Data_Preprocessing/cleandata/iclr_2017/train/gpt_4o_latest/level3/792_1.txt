The paper introduces a novel regularization method, SoftTarget regularization, aimed at mitigating overfitting in deep neural networks. The method leverages a weighted combination of true labels and soft-targets derived from model predictions over time, preserving co-label similarities observed in early training stages. The authors claim that this approach reduces overfitting without sacrificing model capacity, outperforming traditional regularization techniques like Dropout in certain scenarios. The paper also provides a detailed analysis of co-label similarities and their relationship to overfitting, supported by experiments on MNIST, CIFAR-10, and SVHN datasets.
Decision: Reject
The primary reasons for rejection are the lack of novelty and weak experimental baselines. While the paper provides a thorough analysis of co-label similarities, the proposed method is conceptually very similar to Hinton et al.'s (2016) work on soft targets and knowledge distillation. The incremental nature of the contribution does not sufficiently advance the state of the art. Additionally, the experimental results are undermined by the use of weak baselines and potentially suboptimal hyperparameter tuning, which raises concerns about the validity of the reported performance gains.
Supporting Arguments:
1. Lack of Novelty: The proposed method is highly similar to existing approaches, particularly Hinton et al.'s work on soft targets. While the authors frame their method as a regularization technique rather than a distillation method, the underlying idea of leveraging soft-labels is not new. The paper does not provide sufficient differentiation or innovation to justify its contribution as a significant advance.
   
2. Weak Baselines: The experiments compare SoftTarget regularization to standard techniques like Dropout and Batch Normalization. However, the baselines are not state-of-the-art, and the hyperparameter tuning for these baselines appears insufficient. For example, the dropout rates and other hyperparameters are not optimized rigorously, which may exaggerate the effectiveness of the proposed method.
3. Empirical Validation: While the results show some performance improvements, the gains are modest and may not generalize across datasets or architectures. The lack of ablation studies to isolate the impact of individual hyperparameters (e.g., β, γ) further weakens the empirical claims.
Suggestions for Improvement:
1. Clarify Novelty: The authors should explicitly differentiate their method from prior work, particularly Hinton et al. (2016). Highlighting unique theoretical insights or practical advantages would strengthen the contribution.
   
2. Stronger Baselines: Include comparisons with more competitive baselines, such as modern regularization techniques (e.g., CutMix, MixUp, or Label Smoothing). Ensure that all baselines are rigorously tuned to provide a fair comparison.
3. Ablation Studies: Conduct ablation studies to analyze the sensitivity of the method to its hyperparameters (β, γ, nb, nt). This would provide deeper insights into the method's behavior and robustness.
4. Broader Evaluation: Test the method on more challenging datasets and architectures to demonstrate its generalizability. For example, experiments on ImageNet or transformer-based models would strengthen the empirical validation.
Questions for the Authors:
1. How does the proposed method compare to more recent regularization techniques like MixUp or CutMix?  
2. Can the authors provide more details on the hyperparameter tuning process for the baselines? Were the same rigor and search space applied to all methods?  
3. How sensitive is the method to the choice of β and γ? Could these parameters be learned dynamically during training?  
While the paper provides an interesting perspective on co-label similarities and overfitting, its lack of novelty and insufficient experimental rigor prevent it from making a strong contribution to the field. Addressing these issues in a future revision could significantly improve its impact.