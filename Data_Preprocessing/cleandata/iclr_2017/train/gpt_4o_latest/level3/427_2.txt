Review of the Paper
The paper introduces a novel method for visualizing the areas of input images that influence the responses of deep neural networks (DNNs), building upon the prediction difference analysis method by Robnik-Šikonja and Kononenko (2008). The proposed method incorporates conditional sampling, multivariate analysis, and deep visualization of hidden layers, making it more applicable to images and DNNs. The authors demonstrate the utility of their method on both natural images (ImageNet) and medical images (MRI brain scans), showcasing its potential for improving interpretability in critical domains like healthcare.
Decision: Accept
The paper makes a significant contribution to the field of explainable AI by addressing the critical problem of understanding DNN decision-making. The proposed method is well-motivated, rigorously developed, and empirically validated. The improvements over existing methods, particularly in terms of conditional sampling and multivariate analysis, are clearly demonstrated. The inclusion of applications in medical imaging further highlights the method's practical relevance. While there are areas for improvement, the paper is strong overall and merits acceptance.
Supporting Arguments
1. Thoroughness and Rigor: The authors provide a detailed explanation of their methodology, supported by mathematical formulations and algorithmic descriptions. The experiments on ImageNet and MRI datasets convincingly demonstrate the method's effectiveness, particularly in producing fine-grained and interpretable saliency maps.
2. Novel Contributions: The paper introduces three key improvements—conditional sampling, multivariate analysis, and deep visualization of hidden layers—that address limitations in prior methods. These contributions are well-placed in the literature and represent a meaningful advancement in the field.
3. Clarity and Presentation: The paper is well-written and engaging, with clear explanations and illustrative examples. Figures, such as those comparing results across different DNN architectures (Figure 8), effectively communicate the method's strengths.
Additional Feedback
1. Broader Validation: While the handpicked examples are compelling, the inclusion of results on random samples (as mentioned in the appendix) would strengthen the paper's claims. A more systematic evaluation across a larger dataset would provide broader validation.
2. Comparison with Existing Methods: The paper briefly mentions gradient-based and deconvolution-based methods in the introduction but does not provide a direct comparison in the experiments. Including quantitative or qualitative comparisons with these methods would enhance the paper's impact.
3. Computational Efficiency: The method's computational cost is noted as a limitation. While this does not detract from the paper's contribution, discussing potential optimizations or future directions to address this issue would be helpful.
Questions for the Authors
1. How does the method perform on adversarial examples or noisy inputs? Can it still produce meaningful visualizations in such cases?
2. Could the authors elaborate on the choice of parameters, such as patch size (k, l), and their impact on the results?
3. Are there plans to extend the method to real-time applications, particularly in clinical settings?
Overall, this paper represents a valuable contribution to the field of explainable AI and provides a strong foundation for future research. With minor improvements, it has the potential to become a benchmark in the domain of DNN interpretability.