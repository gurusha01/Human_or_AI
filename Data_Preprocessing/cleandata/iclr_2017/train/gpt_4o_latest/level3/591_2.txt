Review of the Paper
Summary of Contributions
This paper investigates the concept of "Sample Importance" (SI) in training deep neural networks and its influence on the learning process. The authors define SI as the impact of individual samples on parameter updates during training and provide a quantitative framework for its measurement. Through empirical analysis on MNIST and CIFAR-10 datasets, the paper uncovers several intriguing findings: (1) "Easy" samples influence top-layer parameters during early training stages, while "hard" samples shape bottom-layer parameters in later stages; (2) Contrary to curriculum learning paradigms, mixing easy and hard samples in training batches outperforms homogeneous batch constructions; and (3) Ordering samples by SI or negative log-likelihood (NLL) leads to worse performance compared to random or mixed batch strategies. The paper challenges established curriculum learning ideas and provides insights into optimizing batch construction for stochastic gradient descent (SGD) training. 
Decision: Accept
The paper makes a novel and well-supported contribution to understanding the role of individual training samples in deep learning. The findings are significant, as they challenge conventional wisdom in curriculum learning and propose actionable insights for improving training dynamics. The empirical results are thorough and scientifically rigorous, and the work is well-positioned within the existing literature. However, there are areas for improvement, particularly in clarifying certain methodological details and exploring broader implications.
Supporting Arguments
1. Novelty and Relevance: The paper introduces the concept of SI and provides a detailed analysis of its role in training dynamics. This is a fresh perspective that complements existing work on curriculum learning and self-paced learning. The findings have practical implications for designing better batch selection algorithms.
   
2. Empirical Rigor: The experiments are well-designed, with consistent results across two datasets. The use of multiple batch construction strategies and their comparison is a strength. The stability of SI across different initializations adds robustness to the findings.
3. Challenging Established Paradigms: The paper's results contradict the widely accepted curriculum learning approach, offering a compelling case for rethinking how training samples are prioritized. This is a valuable contribution to the field.
Suggestions for Improvement
1. Factor Out Gradient Magnitudes: To isolate the true contribution of SI, the authors should factor out the influence of gradient magnitudes arising from weight initialization or parameter scaling. This would reduce noise and provide a clearer picture of SI's role.
   
2. Broader Applicability: The analysis is limited to a three-layer ReLU network. Extending the study to more complex architectures like convolutional or recurrent neural networks would strengthen the paper's generalizability.
3. Batch Selection Algorithms: While the paper highlights the importance of mixed batches, it stops short of proposing concrete algorithms for batch construction based on SI. Developing such algorithms would make the work more actionable.
4. Clarity in Methodology: Some details, such as the clustering process for visualizing SI and the exact procedure for calculating SI across epochs, could be better explained for reproducibility.
Questions for the Authors
1. How does the proposed SI metric compare to other measures of sample influence, such as influence functions or gradient norm-based metrics? Could these alternative metrics provide complementary insights?
2. Did you observe any differences in the impact of SI when using different optimizers (e.g., Adam vs. SGD)? If not explored, this could be an interesting direction for future work.
3. How do the findings generalize to datasets with significantly higher complexity or imbalance in class distributions?
Overall, this paper provides valuable insights into the dynamics of SGD training and challenges established norms in curriculum learning. With some refinements and extensions, it has the potential to make a significant impact on the field.