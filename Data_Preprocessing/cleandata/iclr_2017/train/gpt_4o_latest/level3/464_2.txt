Review
Summary of Contributions
This paper proposes a novel approach to learning parse trees for natural language processing (NLP) tasks using reinforcement learning (RL). The method employs a shift-reduce framework with minimal actions (SHIFT and REDUCE), allowing the model to discover task-specific tree structures without relying on predefined syntactic annotations. The authors evaluate their approach on four datasets (SST, SICK, IMDB, SNLI) across tasks such as sentiment analysis, semantic relatedness, natural language inference, and sentence generation. The results demonstrate that the learned tree structures outperform both sequential encoders and recursive models based on predefined linguistic syntax. The paper contributes to the ongoing debate on the role of syntax in NLP by showing that task-specific tree structures can lead to better performance, even when they deviate from conventional syntactic trees. The qualitative analysis highlights the model's ability to capture linguistically intuitive structures, such as noun phrases and simple verb phrases.
Decision: Accept
The paper is well-motivated, novel, and provides strong empirical results. The use of RL to learn parse trees is an innovative contribution to the field, and the shift-reduce framework is a practical and elegant choice. The demonstrated performance improvements across multiple datasets and tasks substantiate the claims. However, there are areas for improvement, particularly in positioning the work within the broader literature and addressing computational efficiency.
Supporting Arguments
1. Novelty and Motivation: The idea of using RL to discover task-specific tree structures is novel and addresses a significant gap in the literature. The authors convincingly argue that predefined syntactic trees may not always be optimal for downstream tasks, and their results support this hypothesis.
2. Empirical Rigor: The experiments are comprehensive, covering multiple datasets and tasks. The proposed method consistently outperforms baselines, including models with predefined syntactic trees, demonstrating its effectiveness.
3. Contribution to Syntax Debate: The paper provides valuable insights into the relevance of syntax in NLP, showing that learned tree structures can deviate from conventional syntax while still being effective.
Suggestions for Improvement
1. Related Work: The paper does not adequately discuss related work, particularly the study by Andreas et al. (2016), which also uses RL to learn structures for composition. A comparison with this work would strengthen the paper's positioning in the literature.
2. Impact of Composition Functions: The choice of Tree LSTM as the composition function is not thoroughly analyzed. Exploring alternatives (e.g., GRU, simpler functions) could provide insights into the robustness of the proposed method.
3. Model Expressiveness: The authors could investigate whether reducing model expressiveness (e.g., lower-dimensional embeddings) helps focus on discovering more interpretable or useful tree structures.
4. Training Efficiency: Training is computationally expensive due to the dynamic construction of computational graphs. Exploring tools like Dynet or other optimizations could make the approach more practical for larger datasets and higher-dimensional models.
Questions for the Authors
1. How does the proposed method compare to Andreas et al. (2016) in terms of both methodology and empirical performance?
2. Did the authors experiment with alternative composition functions (e.g., GRU) or simpler architectures? If not, why was Tree LSTM chosen as the default?
3. Could restricting model expressiveness (e.g., reducing embedding dimensions) improve interpretability or efficiency without sacrificing performance?
4. Have the authors considered using more efficient frameworks like Dynet to address the computational overhead of dynamic graph construction?
In conclusion, this paper makes a significant contribution to the field by introducing a novel and effective approach for learning task-specific tree structures. While there are areas for improvement, the strengths of the work outweigh its limitations, and I recommend acceptance.