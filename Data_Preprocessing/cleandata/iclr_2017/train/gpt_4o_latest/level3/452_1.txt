Review of "Fine Grained Action Repetition (FiGAR)" Paper
Summary of Contributions
This paper introduces FiGAR, a novel framework that extends deep reinforcement learning (DRL) algorithms by allowing agents to decide both the action and the duration for which it is repeated. This structured policy representation enables temporal abstractions in action spaces, enhancing the efficiency of DRL algorithms. The authors demonstrate FiGAR's applicability across discrete and continuous action spaces by integrating it with three policy gradient methods: A3C, TRPO, and DDPG. Empirical results show significant performance improvements in diverse domains, including Atari games, MuJoCo physics tasks, and TORCS car racing. The framework is lightweight, generalizable, and demonstrates the potential for widespread adoption in DRL.
Decision: Accept  
Key Reasons:  
1. Novelty and Impact: FiGAR introduces a simple yet impactful extension to DRL algorithms, addressing a critical limitation of static action repetition. Its ability to generalize across different domains and algorithms makes it a valuable contribution to the field.  
2. Thorough Evaluation: The paper provides extensive empirical evidence across multiple domains, demonstrating FiGAR's effectiveness in both discrete and continuous action spaces. The results highlight significant performance gains, particularly in exploration-heavy Atari games and smoother policies in TORCS.  
Supporting Arguments
1. The paper is well-motivated and builds on prior work, such as STRAW and Dynamic Frameskip DQN, while addressing their limitations (e.g., action-space blowup and restriction to discrete spaces). The factored policy representation is a key innovation that avoids these issues.  
2. The empirical results are robust, showing consistent improvements across various tasks and domains. The experiments also explore different action repetition sets, demonstrating the framework's flexibility.  
3. The paper's simplicity and generalizability make it a strong candidate for adoption in future DRL research and applications.  
Additional Feedback for Improvement
1. Presentation of Results:  
   - Table 1 (MuJoCo results) could be more interpretable if presented as histograms, allowing readers to visually compare performance across tasks.  
   - Figure 3 (action repetition distributions) might be clearer as a table, especially since the values are rounded and do not sum to 1.  
2. Atari Evaluation:  
   - The subset of Atari games used for evaluation should be explicitly justified. While the results on exploration-heavy games are impressive, a full evaluation on all 57 games would provide a standard benchmark comparison.  
3. Comparison with STRAW:  
   - A direct empirical comparison with STRAW would strengthen the paper, especially since both methods address temporal abstractions. The authors briefly mention STRAW's limitations but do not provide quantitative comparisons for overlapping games.  
4. Tradeoff Between Action Repetition and Information Loss:  
   - FiGAR discards intermediate frames during action repetition, raising concerns about potential loss of critical information. The authors should explore models that process intermediate frames to mitigate this tradeoff.  
Questions for the Authors
1. How was the subset of Atari games chosen for evaluation? Would the results generalize to all 57 games?  
2. Could you elaborate on the tradeoff between action repetition and information loss? Have you considered incorporating intermediate frame processing into FiGAR?  
3. Why was the action repetition set \( W \) arbitrarily chosen as {1, 2, ..., 30} for most experiments? Did you observe any performance sensitivity to the choice of \( W \)?  
In summary, FiGAR presents a compelling and practical extension to DRL algorithms, with strong empirical evidence to support its claims. Addressing the above feedback would further enhance the paper's clarity and impact.