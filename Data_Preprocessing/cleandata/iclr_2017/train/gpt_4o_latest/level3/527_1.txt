Review of the Paper
Summary of Contributions
The paper introduces multiplicative LSTM (mLSTM), a hybrid recurrent neural network architecture that combines features of multiplicative RNNs (mRNNs) and long short-term memory (LSTM) networks. The authors argue that mLSTM offers more expressive input-dependent transition functions while retaining the gating mechanisms of LSTM, which helps manage long-term dependencies. The paper demonstrates the model's performance on character-level language modeling tasks, showing modest improvements over standard LSTM and its deep variants. The authors also highlight the model's competitive performance on large-scale datasets and its state-of-the-art results on the Hutter Prize dataset when combined with dynamic evaluation. The paper is well-written and provides clear explanations of the proposed architecture and its motivations.
Decision: Reject
The paper is not recommended for acceptance due to limited contributions and insufficient experimental validation. While the proposed mLSTM architecture is a reasonable extension of existing work, it does not provide significant novelty or insights beyond prior models. Additionally, the experimental results are narrowly focused on character-level language modeling and fail to demonstrate competitive performance without dynamic evaluation.
Supporting Arguments
1. Minimal Contributions: The paper primarily extends mRNNs to LSTMs, a near-trivial modification that builds on well-established prior work. While the combination of mRNN's factorized hidden weights and LSTM's gating units is interesting, the novelty is incremental and does not introduce fundamentally new ideas.
   
2. Limited Experimental Scope: The experiments are restricted to character-level language modeling, which limits the generalizability of the findings. The results are not state-of-the-art without dynamic evaluation, and the paper does not explore other tasks such as word-level language modeling or continuous input scenarios.
3. Non-Standard Algorithmic Choices: The use of non-standard modifications, such as the "l" parameter in RMSProp and adjustments to the output gate, is not well-justified. These choices may hinder reproducibility and raise questions about their impact on the reported results.
4. Lack of Broader Impact: The paper does not sufficiently discuss how the proposed architecture could generalize to other sequence modeling tasks or provide insights into the broader implications of the work.
Suggestions for Improvement
1. Broaden Experimental Validation: Evaluate mLSTM on a wider range of tasks, such as word-level language modeling, machine translation, or speech recognition, to demonstrate its versatility and impact.
   
2. Ablation Studies: Conduct ablation studies to isolate the contributions of the multiplicative transitions and gating mechanisms. This would clarify the specific advantages of mLSTM over existing architectures.
3. Comparison with Dynamic Evaluation: Provide a more detailed analysis of the model's performance with and without dynamic evaluation to better contextualize its contributions.
4. Justify Algorithmic Modifications: Offer a clearer rationale for the non-standard choices in the optimization process and output gate adjustments. Additionally, evaluate whether these modifications are necessary for the model's performance.
5. Explore Broader Applications: Investigate how mLSTM can be adapted for tasks with continuous or non-sparse inputs, as mentioned in the discussion, to highlight its potential beyond character-level modeling.
Questions for the Authors
1. How does mLSTM perform on tasks beyond character-level language modeling, such as word-level modeling or other sequence tasks?
2. Can you provide more justification for the non-standard modifications to RMSProp and the output gate? How do these changes impact the results?
3. What insights can be drawn from the model's performance with and without dynamic evaluation? Could the improvements be attributed primarily to dynamic evaluation rather than the architecture itself?
4. Have you considered ablation studies to isolate the contributions of the multiplicative transitions and gating mechanisms?
In conclusion, while the paper presents a well-written and technically sound extension of existing architectures, the contributions are incremental, and the experimental results are limited in scope. Addressing the above concerns could significantly strengthen the paper and its potential impact.