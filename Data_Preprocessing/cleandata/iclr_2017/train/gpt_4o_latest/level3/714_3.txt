Review
Summary of the Paper
This paper proposes a method for learning low-dimensional state representations from images using robotic priors implemented via a siamese neural network architecture. The authors claim that their approach is unsupervised, leveraging prior knowledge of physical rules to constrain the learned representations. The experiments involve a Baxter robot simulation, where the model learns a one-dimensional representation of the robot's head position. The authors report a high correlation (97.7%) between the learned representation and the ground truth, and they suggest that the learned features could be useful for transfer learning in other tasks.
Decision: Reject  
The primary reasons for rejection are:  
1. The paper's claim of unsupervised learning is contradicted by the use of reward signals, which inherently introduce supervision.  
2. The experimental setup is overly simplistic, focusing only on basic robot head movements, which limits the generalizability and impact of the approach.  
3. Key claims, such as the potential for transfer learning, are made without any experimental evidence or validation.  
Supporting Arguments
1. Contradiction in Claims: The paper repeatedly emphasizes its unsupervised nature, but the use of rewards as part of the causality prior introduces supervision. This undermines the central claim and raises concerns about the paper's conceptual clarity.  
2. Simplistic Experiments: The experiments are limited to a single, basic task (robot head movement), which does not demonstrate the robustness or scalability of the proposed method. The lack of diversity in tasks or environments makes it difficult to assess the broader applicability of the approach.  
3. Lack of Baseline Comparisons: The paper does not compare its method to existing baseline approaches for state representation learning, such as autoencoders or other unsupervised learning techniques. This omission makes it impossible to evaluate the novelty or effectiveness of the proposed method relative to prior work.  
4. Unsupported Transfer Learning Claims: While the authors suggest that the learned features could be useful for transfer learning, no experiments are provided to substantiate this claim. This weakens the paper's contribution and leaves the reader with unverified assumptions.  
5. Maturity and Clarity: The paper is overall confusing and lacks polish. Key terms and concepts, such as "robotic priors" and "unsupervised learning," are not clearly defined or consistently used. Additionally, the writing is verbose and occasionally redundant, making it difficult to follow the core contributions.
Suggestions for Improvement
1. Clarify the Learning Paradigm: Clearly define whether the method is supervised, unsupervised, or semi-supervised. If rewards are used, the paper should acknowledge this and adjust its claims accordingly.  
2. Expand Experimental Scope: Test the method on more complex tasks and diverse environments to demonstrate its robustness and generalizability. For example, include tasks involving object manipulation or navigation.  
3. Include Baseline Comparisons: Compare the proposed method to standard approaches in state representation learning, such as autoencoders or energy-based models, to contextualize its performance.  
4. Validate Transfer Learning Claims: Provide experiments that demonstrate the utility of the learned features in transfer learning scenarios. For example, show how the features improve performance on a related task.  
5. Improve Writing and Organization: Streamline the paper to focus on the core contributions and ensure that key terms are clearly defined. This will help readers better understand the novelty and significance of the work.
Questions for the Authors
1. How do you reconcile the use of rewards with your claim of unsupervised learning?  
2. Why were no baseline methods included for comparison? How does your method compare to existing approaches like autoencoders or other state representation learning techniques?  
3. Can you provide experimental evidence to support your claims about transfer learning?  
4. How does the method perform in more complex environments or with real-world data, as opposed to the simplified Baxter simulation?  
In conclusion, while the paper introduces an interesting approach to state representation learning, it fails to substantiate its claims with rigorous experiments or comparisons. Addressing the above concerns could significantly improve the quality and impact of the work.