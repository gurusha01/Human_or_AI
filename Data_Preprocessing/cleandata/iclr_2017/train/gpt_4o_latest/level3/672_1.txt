Review of the Paper
Summary of Contributions
This paper introduces the Joint Multimodal Variational Autoencoder (JMVAE), a novel extension of VAEs designed to model multimodal data using a shared latent representation. The key innovation lies in its ability to generate modalities bi-directionally, addressing limitations in prior multimodal generative models that enforce unidirectional relationships. The authors propose an additional method, JMVAE-kl, which minimizes the KL divergence between the multimodal encoder and modality-specific encoders to handle missing modalities effectively. The paper demonstrates the model's effectiveness on MNIST and CelebA datasets, showcasing its ability to reconstruct, generate, and edit data across modalities. The authors also highlight the scalability of the approach to high-dimensional data and its potential for extensions to more than two modalities.
Decision: Reject
While the paper presents an interesting extension of VAEs and tackles an important problem in multimodal learning, the submission falls short in several critical areas. The key reasons for rejection are:  
1. Marginal Contribution of Modality-Specific Encoders: The proposed JMVAE-kl method, while conceptually appealing, shows only marginal improvements in performance. The experimental results do not convincingly demonstrate the necessity or effectiveness of this addition, and Section 3.3 lacks clarity in explaining its theoretical underpinnings.  
2. Unprincipled Inference Methods: The approach used to generate Figure 5 (bi-directional generation) is criticized as unprincipled. Iterative sampling, a standard probabilistic inference technique, would have been more appropriate and scientifically rigorous.  
3. Incomplete Experimental Results: The paper omits results on joint image-attribute generation, which is a critical evaluation for a multimodal generative model. This omission undermines the claims of the paper.
Supporting Arguments
1. Clarity and Motivation: The paper is well-written and provides a clear motivation for bi-directional multimodal generation. However, the explanation of JMVAE-kl lacks depth, particularly in Section 3.3. The authors should provide more theoretical justification for why minimizing KL divergence between encoders improves performance in the presence of missing modalities.  
2. Experimental Rigor: While the experiments on MNIST and CelebA are thorough, the marginal improvements in test log-likelihoods and conditional generation do not justify the added complexity of modality-specific encoders. Additionally, the lack of results on joint image-attribute generation is a significant gap.  
3. Baseline Comparisons: The paper compares JMVAE against strong baselines (e.g., CVAE, CMMA), but the performance gains are inconsistent. For instance, while JMVAE-kl improves conditional log-likelihoods in some cases, it underperforms when α is large, indicating a trade-off that is not well-explored.
Suggestions for Improvement
1. Clarify and Justify JMVAE-kl: Provide a more detailed explanation of the JMVAE-kl method, including its theoretical foundation and the choice of α. A sensitivity analysis of α would also help clarify its impact on performance.  
2. Adopt Principled Inference Methods: Replace the current inference methods for conditional image generation with iterative sampling or other probabilistic techniques to ensure scientific rigor.  
3. Include Missing Results: Provide results on joint image-attribute generation to strengthen the paper's claims. This is particularly important for demonstrating the model's ability to handle multimodal data effectively.  
4. Broader Evaluation: Extend the evaluation to additional datasets or scenarios with more than two modalities to highlight the scalability and generalizability of the approach.  
5. Ablation Studies: Conduct ablation studies to isolate the contribution of modality-specific encoders and demonstrate their necessity beyond marginal improvements.
Questions for the Authors
1. How does the choice of α in JMVAE-kl affect the trade-off between reconstruction quality and bi-directional generation? Could you provide a more detailed analysis of this trade-off?  
2. Why were iterative sampling methods not used for conditional image generation in Figure 5? How would the results differ if such methods were employed?  
3. Can you provide results on joint image-attribute generation for CelebA? This is critical for evaluating the model's multimodal capabilities.  
4. How does the model perform on datasets with more than two modalities? Have you considered extending the experiments to such settings?
In conclusion, while the paper introduces a promising extension to VAEs for multimodal learning, the lack of clarity, marginal improvements, and gaps in experimental results prevent it from meeting the standards for acceptance. Addressing these issues in a future submission could significantly strengthen the work.