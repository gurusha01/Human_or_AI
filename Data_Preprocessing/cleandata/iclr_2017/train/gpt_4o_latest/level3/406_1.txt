Review
Summary of Contributions
This paper introduces the Ensemble Policy Optimization (EPOpt) algorithm, which addresses the challenge of learning robust policies in reinforcement learning (RL) under model discrepancies between simulated and real-world domains. The paper proposes a two-pronged approach: (1) training on an ensemble of simulated models using adversarial training to improve robustness to parametric and unmodeled discrepancies, and (2) adapting the source domain ensemble using approximate Bayesian updates based on limited target domain data. The authors demonstrate the effectiveness of EPOpt on complex robotic control tasks (hopper and half-cheetah) in the MuJoCo simulator, showing improved robustness and generalization compared to standard policy search methods. The paper also explores robustness to unmodeled effects and the efficiency of model adaptation, providing a promising direction for transferring RL policies from simulation to real-world applications.
Decision: Accept
Key reasons for this decision:
1. Novelty and Relevance: The paper addresses a long-overlooked topic in RL—ensemble optimization in policy-gradient training—offering a novel and practical solution for robust policy learning.
2. Scientific Rigor: The claims are well-supported by theoretical grounding and extensive empirical results, which convincingly demonstrate the advantages of EPOpt over standard methods.
3. Clarity and Accessibility: The paper is well-written, with clear explanations of the problem, methodology, and experiments, making it accessible to a broad audience.
Supporting Arguments
1. Problem Tackling: The paper effectively addresses the critical issue of transferring RL policies from simulation to the real world, where model discrepancies often degrade performance. The use of adversarial training and Bayesian adaptation is well-motivated and aligned with the literature on robust control and Bayesian RL.
2. Experimental Validation: The experiments are comprehensive, covering both robustness to parametric discrepancies and adaptation to unmodeled effects. The results demonstrate that EPOpt achieves superior direct-transfer performance and robustness compared to standard policy search methods like TRPO.
3. Broader Impact: The proposed approach has significant implications for real-world RL applications, particularly in robotics, where safety and sample efficiency are paramount.
Suggestions for Improvement
While the paper is strong overall, there are areas where it could be further improved:
1. Broader Experimental Coverage: The experiments focus on specific tasks (hopper and half-cheetah) and parameter choices. Expanding the evaluation to additional domains, such as high-dimensional tasks or tasks with sparse rewards, would strengthen the generalizability of the claims.
2. Computational Efficiency: The paper acknowledges the computational intensity of sampling-based Bayesian adaptation. Future work could explore more scalable alternatives, such as non-parametric methods or neural network-based simulators, as hinted at in the conclusion.
3. Ablation Studies: While the paper discusses the importance of the adversarial sub-sampling step and baseline for policy optimization, more detailed ablation studies would clarify the contribution of each component to the overall performance.
Questions for the Authors
1. How sensitive is the performance of EPOpt to the choice of the hyperparameter \( \epsilon \) in the CVaR objective? Could you provide guidance on selecting this parameter in practice?
2. Have you considered applying EPOpt to tasks with discrete action spaces or partially observable environments? If so, how would the algorithm need to be adapted?
3. Can the proposed Bayesian adaptation method handle non-stationary target domains, where the parameters evolve over time? If not, what modifications would be required?
Conclusion
This paper makes a valuable contribution to the field of RL by addressing the critical challenge of robust policy learning under model discrepancies. While the narrow experimental focus is a limitation, the novelty, rigor, and practical relevance of the proposed approach justify acceptance. The suggestions for improvement and questions aim to guide the authors toward further strengthening their work for a potential journal version.