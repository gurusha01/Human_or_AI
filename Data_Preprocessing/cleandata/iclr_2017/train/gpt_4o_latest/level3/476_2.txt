Review
Summary
This paper investigates whether shallow non-convolutional networks can match the performance of deep convolutional networks (CNNs) for image classification when constrained to the same number of parameters. Using the CIFAR-10 dataset, the authors demonstrate a significant performance gap favoring deep CNNs, even when shallow models are trained using advanced techniques like distillation. The experiments are well-designed, employing Bayesian hyperparameter optimization and comprehensive data augmentation. The results confirm that deep CNNs outperform shallow networks due to their inherent ability to encode translation invariance, which is critical for high-level image recognition tasks. Among non-convolutional architectures, models with 2-3 hidden layers perform better than those with fewer or more layers, a finding that warrants further discussion. The paper also highlights the limitations of shallow models on CIFAR-10 and anticipates worse performance on larger datasets like ImageNet, though this claim lacks detailed justification.
Decision: Reject
While the paper is well-executed and provides valuable insights, it falls short in several critical areas. The key reasons for rejection are:  
1. Limited Scope of Experiments: The study is restricted to CIFAR-10, a relatively small dataset, and does not include experiments on a second dataset to assess generalizability.  
2. Unclear Parameter Budget Justification: The rationale for limiting experiments to 30M parameters is not well-explained, and the results do not appear saturated, suggesting that higher parameter budgets could yield more insights.  
3. Speculative Claims: The claim that shallow networks would perform worse on ImageNet is not substantiated with experiments or detailed theoretical reasoning.
Supporting Arguments
The paper makes a strong case for the superiority of deep CNNs over shallow networks for image classification, particularly under a parameter budget. The use of distillation and Bayesian optimization ensures that the shallow models are trained as effectively as possible, and the results are presented in a clear and comprehensive manner. However, the study's conclusions are limited by its narrow experimental scope. CIFAR-10, with its low resolution and small dataset size, may not fully represent the challenges of real-world image classification tasks. Additionally, the decision to cap the parameter budget at 30M is arbitrary and leaves open the question of whether larger shallow models could close the performance gap.
Suggestions for Improvement
1. Expand Dataset Scope: Including experiments on a second dataset, such as ImageNet, would strengthen the paper's conclusions and provide a more robust assessment of the generalizability of the findings.  
2. Justify Parameter Budget: Provide a clear rationale for the 30M parameter limit and explore whether increasing the parameter budget for shallow models could narrow the performance gap.  
3. Address Speculative Claims: The authors should either conduct experiments on ImageNet or provide a detailed theoretical justification for their claim that shallow networks would perform worse on larger datasets.  
4. Discuss Layer Depth in Non-Convolutional Models: The observation that 2-3 hidden layers outperform 1, 4, or 5 layers in non-convolutional architectures is intriguing but underexplored. A deeper analysis could yield valuable insights.
Questions for the Authors
1. Why was the parameter budget capped at 30M, and do you anticipate different results with larger budgets?  
2. Could the inclusion of higher-resolution images or larger datasets like ImageNet alter the performance gap between shallow and deep models?  
3. Why do non-convolutional models with 2-3 hidden layers outperform those with 4-5 layers? Could this be due to optimization challenges or overfitting?  
4. Have you considered alternative architectures or techniques, such as attention mechanisms, that might mitigate the limitations of shallow networks?  
In summary, while the paper provides valuable insights into the limitations of shallow networks, its narrow experimental scope and speculative claims limit its impact. Addressing these issues in future work could significantly enhance its contribution to the field.