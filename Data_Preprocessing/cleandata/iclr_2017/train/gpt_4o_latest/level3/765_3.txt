Review of the Paper
Summary of Contributions
The paper addresses the problem of model-based reinforcement learning (RL) in high-dimensional visual environments where both system dynamics and reward functions are unknown. It proposes extending the video frame prediction model of Oh et al. (2015) by adding a reward prediction head to jointly predict future states and rewards using a shared latent representation. The authors empirically evaluate their approach on five Atari games and demonstrate accurate cumulative reward predictions up to 200 frames. The paper claims this is a significant step toward data-efficient RL in complex environments and highlights potential applications for planning and exploration.
Decision: Reject
While the paper is well-written and tackles an important and timely topic, the contributions are too incremental, and the methodology lacks sufficient novelty. The proposed extension—adding a reward prediction head to the frame prediction network—is sensible but not surprising, as similar ideas have already been demonstrated in prior work, particularly by Oh et al. (2015). The results, while promising, do not provide enough evidence of substantial advancement over existing methods.
Supporting Arguments
1. Incremental Contribution: The primary novelty lies in jointly predicting rewards and states using a single latent representation. However, this extension is relatively straightforward and does not significantly advance the state of the art. The methodology closely follows Oh et al. (2015), with only minor modifications to the network architecture and loss function.
   
2. Lack of Novelty: The idea of learning a joint model for state and reward prediction is not new. Previous works, including Oh et al. (2015), have explored similar directions, albeit without explicitly predicting rewards. The paper does not sufficiently differentiate itself from these prior efforts.
3. Empirical Results: While the results demonstrate accurate cumulative reward prediction in some games, the performance is inconsistent across others (e.g., Seaquest). The analysis does not convincingly show how the proposed method outperforms existing baselines or why it should be preferred over alternative approaches.
Suggestions for Improvement
1. Highlight Novelty: The authors should better articulate how their approach differs from and improves upon Oh et al. (2015) and other related works. For example, demonstrating substantial gains in data efficiency or planning performance would strengthen the paper.
2. Broader Evaluation: The evaluation is limited to five Atari games. Expanding the experiments to more diverse environments, including those with sparse rewards or non-deterministic dynamics, would provide stronger evidence of the method's generality and robustness.
3. Comparison with Baselines: The paper lacks a thorough comparison with state-of-the-art model-based and model-free RL methods. Including such comparisons would clarify the practical benefits of the proposed approach.
4. Theoretical Insights: Providing theoretical analysis or insights into why joint state and reward prediction improves RL performance would add depth to the paper.
Questions for the Authors
1. How does the proposed method compare to other model-based RL approaches in terms of data efficiency and planning performance?
2. Can the authors provide more details on how the joint model could be integrated into a full RL pipeline for online learning or planning?
3. How does the method handle environments with sparse or delayed rewards, where reward prediction might be more challenging?
In conclusion, while the paper addresses an important problem and presents some promising results, the lack of novelty and incremental nature of the contributions make it unsuitable for acceptance in its current form. Addressing the above concerns could significantly improve the paper's impact and relevance.