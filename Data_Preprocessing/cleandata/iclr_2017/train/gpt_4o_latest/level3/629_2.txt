Review of the Paper
Summary of Contributions
This paper investigates the computational similarities between human visual perception and deep neural networks (DNNs) trained on large-scale image recognition tasks. By comparing DNN responses to human psychophysical experiments across various visual tasks—such as sensitivity to image changes, segmentation, crowding, and shape perception—the authors demonstrate that DNNs capture some properties of human vision. The results suggest that mid- and end-computation stages of DNNs align with human perceptual sensitivities in certain contexts. Additionally, the paper highlights intriguing cases of divergence, such as 3D perception and symmetry, raising questions about the limitations of feedforward architectures. The authors also propose potential applications of their findings, including perceptual loss metrics for engineering tasks and insights for brain modeling. Overall, the paper offers a promising direction for understanding the intersection of biological and artificial vision systems.
Decision: Reject
While the paper presents an interesting exploration of DNN-human perceptual alignment, it lacks the depth of analysis and rigor necessary for acceptance. Specifically, the paper does not sufficiently address the underlying mechanisms driving the observed similarities and discrepancies, nor does it provide a clear theoretical framework or hypotheses to guide future investigations. The work, while valuable, feels preliminary and exploratory rather than conclusive.
Supporting Arguments for Decision
1. Lack of Mechanistic Insights: While the paper establishes correlations between DNN computations and human perception, it does not delve into why these similarities arise or what architectural features of DNNs contribute to them. For instance, the role of first-layer filters in contextual effects is mentioned but not analyzed in detail.
   
2. Adversarial Examples and Limitations: The presence of adversarial examples in DNNs, which have no analog in human vision, is acknowledged but not explored in depth. This omission weakens the broader implications of the findings, as it suggests that DNNs may capture only superficial aspects of human perception.
3. Superficial Treatment of Divergences: The paper notes cases where DNNs fail to align with human perception (e.g., 3D perception, symmetry) but does not investigate these discrepancies systematically. Understanding these failures is critical for advancing both AI and neuroscience.
4. Exploratory Nature: The experiments, while interesting, appear anecdotal and lack a unifying theoretical framework. The findings are presented as observations rather than being tied to specific hypotheses or predictions.
Suggestions for Improvement
1. Deeper Analysis of Mechanisms: The authors should examine how specific architectural features (e.g., first-layer filters, pooling mechanisms) contribute to the observed contextual effects and perceptual similarities. This could provide insights into the neural mechanisms underlying these phenomena.
2. Address Adversarial Examples: A more thorough discussion of adversarial examples and their implications for the DNN-human comparison would strengthen the paper. Are adversarial vulnerabilities a fundamental limitation of feedforward architectures, or could they be mitigated with additional constraints?
3. Systematic Study of Divergences: The paper would benefit from a focused investigation into cases where DNNs fail to align with human perception. For example, why do DNNs struggle with 3D perception and symmetry? Are these failures due to the lack of recurrent processing or insufficient training data?
4. Theoretical Framework: The authors should propose a theoretical framework or set of hypotheses to guide future research. For instance, under what conditions can we expect DNNs to converge with human perception, and when will they diverge?
Questions for the Authors
1. How do you explain the alignment of mid-computation DNN stages with human perception? Are there specific architectural features responsible for this?
2. Could the inclusion of recurrent or feedback mechanisms in DNNs address the observed divergences (e.g., 3D perception, symmetry)?
3. How do adversarial examples affect the interpretation of your findings? Do they suggest fundamental differences between DNNs and human vision?
4. Have you considered testing DNNs trained on tasks other than object recognition (e.g., reinforcement learning or unsupervised learning) to see if the observed similarities persist?
In conclusion, while the paper provides an intriguing exploration of DNN-human perceptual alignment, it requires significant refinement and deeper analysis to meet the standards of this conference. The work is appreciated for its direction but currently only scratches the surface of this important topic.