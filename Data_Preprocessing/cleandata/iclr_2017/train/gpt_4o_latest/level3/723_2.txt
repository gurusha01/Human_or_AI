Review
The paper presents a novel approach to handwritten word recognition by leveraging an LSTM model to predict open bigrams, which are then used in a decoding step to predict written words. The authors claim that their method, inspired by cognitive neuroscience, slightly outperforms a Viterbi baseline on two public datasets (Rimes and IAM). The proposed open-bigram representation is argued to capture long-range dependencies in handwriting data, and the decoding process employs a simple cosine similarity measure to match predictions to a vocabulary.
Decision: Reject
The primary reasons for rejection are: (1) the lack of scientific rigor in supporting key claims, particularly the cortical inspiration of open bigrams, and (2) significant methodological and presentation issues that undermine the validity and clarity of the work.
Supporting Arguments
1. Unfounded Cortical Inspiration Claim: The paper asserts that the approach is "cortically inspired" but fails to provide direct neuroscience evidence to substantiate this claim. While open bigrams have been hypothesized in cognitive psychology, there is no evidence of their existence as a distinct cortical layer for handwriting recognition. This weakens the biological plausibility argument, which is central to the paper's motivation.
2. Methodological Issues: The model's explanation is incomplete, lacking details about the objective function and design choices. The tasks chosen for evaluation appear arbitrary, as the decoding mechanism could be applied to any text corpus, not just handwriting data. Furthermore, the comparison to the Viterbi baseline is unfair, as the proposed model has access to more information, and the dataset seems biased toward the proposed approach.
3. Effectiveness of Open Bigrams: The paper does not convincingly demonstrate that open bigrams improve handwriting recognition. While the results show slight improvements, they are not substantial enough to justify the added complexity of the approach.
4. Presentation and Writing Issues: Numerous typos and grammatical errors detract from the paper's professionalism and clarity. Additionally, the results in Table 5 are identical to Table 2 (with different precision), which is confusing and poorly presented.
Additional Feedback
1. Decoding Process: The authors could explore incorporating bigram occurrence counts into the decoding process, which might improve performance. This is an obvious extension that remains unaddressed.
2. Baseline Comparisons: The paper should ensure fair comparisons by using equivalent datasets and information access for all models. A more rigorous evaluation against state-of-the-art methods would strengthen the claims.
3. Clarification of Model Design: The authors should provide a detailed explanation of the LSTM model, including the objective function, architecture, and training process. This would make the work more reproducible and interpretable.
4. Error Analysis: A deeper analysis of the types of errors made by the open-bigram model versus the baseline would provide valuable insights into the strengths and weaknesses of the proposed approach.
Questions for the Authors
1. What specific evidence supports the claim that open bigrams are "cortically inspired"? Can you provide neuroscientific studies that directly link open bigrams to cortical processes in handwriting recognition?
2. Why were the chosen tasks (handwriting datasets) particularly relevant for evaluating the open-bigram approach? Could the same decoding mechanism be applied to other text corpora?
3. How does the proposed model handle cases where bigram predictions are ambiguous or noisy? Is there a mechanism to mitigate such errors?
4. Can you clarify the discrepancy between Tables 2 and 5? Are these results intended to be identical, or is this a mistake?
In summary, while the paper introduces an interesting idea, it falls short in terms of scientific rigor, clarity, and methodological soundness. Addressing these issues could significantly improve the quality and impact of the work.