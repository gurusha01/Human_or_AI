Review
Summary of Contributions
This paper proposes a novel method to address the variance introduced by dropout in neural networks, focusing on two key contributions: (1) a weight initialization technique that compensates for the variance caused by dropout and arbitrary nonlinearities, and (2) a method to re-estimate Batch Normalization (BN) parameters after training by feeding forward the training data without dropout. The proposed weight initialization ensures unit variance across layers, preventing exploding or vanishing activations, while the BN re-estimation improves test-time performance. Experimental results demonstrate that these techniques lead to faster convergence and marginal accuracy improvements, achieving state-of-the-art results on CIFAR-10 and CIFAR-100 without data augmentation.
Decision: Reject
While the paper introduces a technically sound and well-motivated method, the limited experimental validation and marginal performance gains make it difficult to justify acceptance. The contributions, while incremental, are not sufficiently supported by rigorous empirical evidence or broader applicability.
Supporting Arguments for Decision
1. Limited Experimental Validation: The experiments primarily focus on CIFAR-10 and CIFAR-100 datasets, with minimal exploration of other architectures or tasks. The results, while promising, lack sufficient breadth to demonstrate the generalizability of the proposed method.
2. Marginal Accuracy Improvements: The reported accuracy gains, though measurable, are relatively small (e.g., ~1% improvement in some cases). This raises questions about the practical significance of the proposed methods compared to existing techniques.
3. Incomplete Validation of Claims: While the paper claims to stabilize activation and gradient statistics, it does not provide sufficient empirical evidence (e.g., detailed gradient or activation variance analysis during training) to support these assertions. Additionally, the variance correction for backpropagation is mentioned but not thoroughly validated.
4. Overemphasis on Batch Normalization Comparison: The paper frequently compares its method to Batch Normalization, which serves broader purposes (e.g., regularization, faster convergence). This comparison feels somewhat misaligned, as the proposed method is primarily an initialization technique.
Suggestions for Improvement
1. Broader Experimental Validation: Evaluate the proposed method on a wider range of datasets (e.g., ImageNet) and architectures (e.g., transformers or recurrent networks) to demonstrate its generalizability and practical utility.
2. Deeper Analysis of Stability: Provide empirical evidence of activation and gradient statistics during training to substantiate claims of improved stability. This could include visualizations or quantitative metrics.
3. Validation of Backpropagation Variance: The paper introduces a correction factor for backpropagation variance but does not adequately validate its impact. Including experiments that isolate and test this component would strengthen the contribution.
4. Clarify Practical Significance: Highlight scenarios where the proposed method offers clear advantages over existing techniques, such as in resource-constrained settings where Batch Normalization is infeasible.
5. Comparison with More Techniques: Compare the proposed method with other initialization techniques (e.g., LSUV, orthogonal initialization) to better position it in the literature.
Questions for Authors
1. How does the proposed method perform on larger-scale datasets (e.g., ImageNet) and architectures beyond VGG-like networks?
2. Can you provide empirical evidence (e.g., plots or statistics) of activation and gradient variance during training to support claims of improved stability?
3. How does the backpropagation variance correction influence training dynamics and final performance? Could you isolate its impact in experiments?
4. What are the computational overheads of the proposed BN re-estimation step, and how does it compare to standard BN training?
In summary, while the paper presents an interesting and technically sound approach, the limited scope of experiments and incremental nature of the contributions make it difficult to recommend acceptance at this time. Addressing the above concerns could significantly strengthen the paper.