Review of the Paper
Summary of Contributions
This paper introduces the Gaussian Error Linear Unit (GELU), a novel activation function that bridges the gap between stochastic regularizers and traditional nonlinearities in neural networks. The authors propose a stochastic regularizer, the SOI map, which probabilistically applies a zero or identity transformation to a neuron's input, and derive the GELU as its expected transformation. They claim that GELU outperforms existing nonlinearities like ReLU and ELU across diverse tasks in computer vision, natural language processing, and speech recognition. Additionally, the paper suggests that the SOI map can replace traditional nonlinearities, challenging the necessity of deterministic activation functions. The authors provide experimental results to support these claims, showcasing improvements in accuracy and robustness.
Decision: Reject
The paper is rejected due to the following key reasons:
1. Insufficient Distinction from Adaptive Dropout: The proposed SOI map closely resembles Adaptive Dropout, yet the distinction between the two approaches is not clearly articulated. This lack of clarity undermines the novelty of the contribution.
2. Lack of Comprehensive Experimental Validation: The experimental results fail to compare GELU with state-of-the-art (SOTA) methods beyond ReLU and ELU. Without such comparisons, the empirical claims remain unconvincing.
Supporting Arguments
1. Unclear Novelty: While the paper introduces the SOI map and GELU, the connection to Adaptive Dropout is acknowledged but not adequately differentiated. The paper states that Adaptive Dropout modifies the output of a nonlinearity, whereas the SOI map operates on the input. However, this distinction is not sufficiently explored or justified to establish the SOI map as a fundamentally new concept. The lack of theoretical or empirical evidence to highlight the unique advantages of the SOI map over Adaptive Dropout weakens the contribution.
   
2. Limited Experimental Scope: The experiments focus on comparing GELU with ReLU and ELU but omit comparisons with more recent SOTA activation functions, such as Swish or Mish. This omission is critical, as the field of activation functions has advanced significantly, and the paper does not convincingly demonstrate that GELU is competitive with the latest methods. Furthermore, the datasets used (e.g., MNIST, CIFAR-10/100) are standard benchmarks but do not provide sufficient evidence for generalization to more complex real-world tasks.
Suggestions for Improvement
1. Clarify Novelty: The authors should provide a more rigorous theoretical and empirical comparison between the SOI map and Adaptive Dropout. Highlighting scenarios where the SOI map outperforms Adaptive Dropout or offers unique benefits would strengthen the paper's contribution.
   
2. Expand Experimental Validation: Include comparisons with SOTA activation functions like Swish and Mish across a broader range of datasets and tasks. This would provide stronger evidence for the practical utility of GELU.
3. Theoretical Insights: The paper could benefit from a deeper theoretical analysis of why the GELU's probabilistic weighting mechanism leads to improved performance. For instance, exploring its impact on optimization dynamics or generalization could provide valuable insights.
Questions for the Authors
1. How does the SOI map fundamentally differ from Adaptive Dropout in terms of theoretical formulation and practical impact? Could you provide specific experiments to highlight these differences?
2. Why were SOTA activation functions like Swish or Mish excluded from the experimental comparisons? How does GELU perform relative to these methods?
3. Can you provide more details on the computational efficiency of GELU compared to ReLU and ELU, particularly in large-scale models?
In summary, while the paper presents an interesting idea, the lack of clear novelty and insufficient experimental validation limit its impact. Addressing these concerns could make the work more compelling in future submissions.