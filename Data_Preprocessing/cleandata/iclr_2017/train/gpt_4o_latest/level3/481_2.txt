The paper presents a significant contribution to the field of adversarial robustness in machine learning, particularly through its exploration of adversarial training on large-scale datasets like ImageNet. The key contributions include the introduction of the "label leaking" phenomenon, which highlights a critical issue in adversarial training when true labels are used in adversarial example generation. This observation is novel and has implications for future research on robust model training. Additionally, the paper proposes a new metric, the ratio of "clean accuracy" to "adversarial accuracy," as a more reasonable measure of robustness, which is a thoughtful improvement over existing evaluation metrics.
However, there are notable concerns. The title and introduction emphasize the scalability of adversarial training to large datasets, but the findings in Sections 4.3 and 4.4 do not fully support this claim. While the experiments on ImageNet are valuable, the conclusions drawn from these experiments may be overgeneralized, as the study does not validate its findings on other datasets or model architectures. This limitation weakens the broader applicability of the results. Nevertheless, the paper is well-written, with clear explanations of methods and results, making it accessible to readers.
Decision: Accept
The paper should be accepted for its valuable contributions, particularly the identification of label leaking and the introduction of a novel robustness metric. These insights advance the understanding of adversarial training and provide a foundation for future work. However, the authors should address the concerns outlined below to strengthen the paper.
Supporting Arguments:
1. Novel Contributions: The identification of label leaking is a significant finding that sheds light on an overlooked issue in adversarial training. The proposed robustness metric is also a meaningful addition to the field.
2. Clarity and Writing: The paper is well-structured and easy to follow, making its contributions accessible to a broad audience.
Suggestions for Improvement:
1. Title and Scope: The title and introduction should be revised to better align with the findings. The emphasis on "large-scale" applicability is overstated, given that the experiments are limited to ImageNet.
2. Experimental Validation: To strengthen the claims, the authors should validate their findings on additional datasets (e.g., CIFAR-10, MNIST) and model architectures. This would provide more evidence for the generalizability of their conclusions.
3. Iterative Methods: The paper notes the limited effectiveness of adversarial training against iterative attack methods. Exploring why this is the case and proposing potential solutions would enhance the paper's impact.
4. Transferability Analysis: While the paper discusses transferability, it would benefit from a deeper exploration of the inverse relationship between transferability and attack success rates, as briefly mentioned.
Questions for the Authors:
1. How does the proposed robustness metric perform compared to existing metrics across other datasets and models?
2. Can the label leaking phenomenon be quantified more rigorously, and how does it vary across different datasets or adversarial methods?
3. Have you considered combining adversarial training with other defense mechanisms (e.g., defensive distillation) to address iterative attacks?
In summary, while the paper has some limitations, its contributions are significant and merit publication. Addressing the above concerns would further enhance its quality and impact.