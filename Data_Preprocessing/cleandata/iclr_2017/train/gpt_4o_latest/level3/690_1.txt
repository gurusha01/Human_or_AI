Review
Summary of Contributions
This paper evaluates recent Convolutional Neural Network (CNN) architectures in terms of resource utilization, including metrics such as accuracy, memory footprint, parameter count, operations count, inference time, and power consumption. The authors aim to provide insights into designing efficient networks for practical applications, emphasizing the trade-offs between accuracy and computational requirements. The paper introduces ENet, which is claimed to achieve state-of-the-art efficiency in parameter utilization. Key findings include a hyperbolic relationship between accuracy and inference time, the utility of operation counts as a proxy for inference time, and the role of energy constraints in bounding accuracy and model complexity. The authors also highlight inefficiencies in widely used architectures like VGG and propose ENet as a more compact alternative.
Decision: Reject  
The paper addresses an important topic, but it falls short in providing meaningful contributions for practical applications. The evaluations, while thorough, lack novelty and fail to address critical real-world scenarios such as fine-tuning and compressed network comparisons. Additionally, the analysis does not offer actionable insights for practitioners seeking to optimize CNNs for deployment.
Supporting Arguments
1. Lack of Practical Relevance: While the paper provides a detailed evaluation of resource utilization metrics, the findings are largely unsurprising to experienced CNN practitioners. For example, the hyperbolic relationship between accuracy and inference time is well-known, and the utility of operation counts as a proxy for inference time is already widely accepted. The paper does not offer new or actionable strategies for improving resource efficiency in real-world deployments.
   
2. Overlooked Scenarios: The evaluation does not consider critical practical scenarios such as fine-tuning pre-trained networks, which is a common practice in real-world applications. Additionally, the paper does not adequately compare compressed networks with higher accuracy-to-parameter density ratios, which are increasingly relevant for deployment on resource-constrained devices.
3. Incomplete Analysis: The paper lacks an in-depth analysis of network topologies and bottlenecks, which could have added significant value. For instance, understanding how specific architectural choices impact resource utilization and performance would provide more actionable insights for designing efficient networks.
4. Minor Concerns: The rationale for using batch normalization in NiN and AlexNet is unclear and should be better justified, as it deviates from the original implementations of these architectures.
Suggestions for Improvement
1. Incorporate Fine-Tuning Scenarios: The authors should evaluate the resource utilization of fine-tuned networks, as this is a critical step in many practical applications.
   
2. Compare Compressed Networks: A comparison with compressed or pruned networks, such as those using techniques like quantization or pruning, would make the analysis more relevant to real-world deployment scenarios.
3. Analyze Network Topologies: An exploration of architectural bottlenecks and their impact on resource utilization would provide deeper insights. For example, how do specific design choices (e.g., depth, width, or skip connections) influence power consumption and inference time?
4. Clarify Batch Normalization Usage: The authors should explain why batch normalization was applied to NiN and AlexNet, as this choice may affect the validity of the comparisons.
Questions for the Authors
1. Why were fine-tuning scenarios not included in the evaluation? How do you envision your findings being applied to real-world use cases where fine-tuning is common?
2. How does ENet compare to state-of-the-art compressed networks in terms of accuracy-to-parameter density and real-world deployment efficiency?
3. Can you provide more details on the architectural bottlenecks that limit resource efficiency in the evaluated networks? How does ENet address these bottlenecks?
4. What was the motivation for applying batch normalization to NiN and AlexNet, and how does this affect the comparability of results?
In summary, while the paper addresses an important topic and provides a detailed evaluation, it lacks novelty and practical relevance. Addressing the suggested improvements could significantly enhance its impact and applicability.