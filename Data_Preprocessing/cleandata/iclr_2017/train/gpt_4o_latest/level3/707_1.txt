Review
Summary of Contributions
This paper introduces a novel framework for language models that explicitly incorporates reference decisions as stochastic latent variables. The proposed approach is applied to three tasks: task-oriented dialogue generation, recipe generation, and text generation with coreference mentions. The authors claim that their models outperform deterministic attention-based baselines by explicitly modeling reference decisions, allowing for better handling of rare words and external knowledge sources. The paper also presents three custom datasets for evaluation and reports improvements in perplexity and BLEU scores.
Decision: Reject  
While the paper introduces an interesting and potentially impactful idea, it suffers from significant weaknesses in execution, clarity, and scientific rigor. These issues undermine the validity of the claims and the overall contribution of the work.
Supporting Arguments for Decision
1. Latent Variable Misrepresentation: The primary novelty of the paper lies in treating entity references as latent variables. However, in empirical evaluations, these variables are treated as observed rather than marginalized over, which contradicts the core claim of the paper. This undermines the theoretical contribution and raises concerns about the validity of the results.
   
2. Evaluation Metrics: The use of perplexity as the primary evaluation metric is problematic, as it does not account for differences in event spaces between models. This leads to unfair comparisons. Alternative metrics like BLEU, METEOR, or human evaluations should have been used to provide a more robust assessment of the model's performance.
3. Clarity and Technical Accuracy: The paper is poorly written, with many technical details either missing or confusing. For example, the handling of latent variables, mention detection, and entity updates is not clearly explained, making it difficult to evaluate the proposed methods. The rushed nature of the work is evident in the lack of clarity and coherence.
4. Diluted Focus: By attempting to address three distinct tasks in one paper, the authors fail to provide sufficient depth and focus on any single task. This results in a lack of clarity regarding the key contributions and technical details. A more focused approach on one task would have been more effective.
5. Empirical Comparisons: The paper lacks strong empirical comparisons with state-of-the-art models. While the authors claim improvements over deterministic attention-based models, the baselines used are not clearly described, and the results are not sufficiently contextualized within the broader literature.
Suggestions for Improvement
1. Latent Variable Treatment: The authors should either marginalize over all possibilities for true latent variable treatment or explicitly clarify why treating mentions as observed variables is justified. This is critical to the validity of the proposed approach.
2. Evaluation Metrics: Incorporate alternative evaluation metrics such as BLEU, METEOR, or human evaluations, especially for tasks like dialogue and recipe generation where perplexity is insufficient.
3. Clarity and Completeness: Rewrite the paper to improve clarity and technical accuracy. Provide detailed explanations of key components, such as the handling of latent variables, mention detection, and entity updates. Include diagrams or pseudocode where necessary.
4. Focus on One Task: Narrow the scope of the paper to focus on one task (e.g., dialogue generation) and provide a more in-depth analysis. This would allow for a clearer presentation of contributions and stronger empirical results.
5. Stronger Baselines: Include comparisons with state-of-the-art models and provide detailed descriptions of the baselines used. This would strengthen the empirical validation of the proposed approach.
Questions for the Authors
1. Why are the latent variables treated as observed in empirical evaluations? How does this align with the claim of treating references as latent variables?
2. Why was perplexity chosen as the primary evaluation metric, given its limitations? Have you considered human evaluations or other task-specific metrics?
3. Can you provide more details on the baselines used for comparison? Are they state-of-the-art for the respective tasks?
4. How does the inclusion of three tasks in one paper contribute to the clarity and impact of the work? Would a more focused approach be more effective?
In its current form, the paper introduces an interesting idea but fails to execute it rigorously or communicate it effectively. Addressing the above concerns would significantly enhance its quality and impact.