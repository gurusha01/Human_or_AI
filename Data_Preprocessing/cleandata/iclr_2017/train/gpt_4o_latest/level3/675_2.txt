Review of the Paper
Summary of Contributions
The paper introduces a novel sequence prediction learning method, Incremental Sequence Learning (ISL), which leverages incremental and curriculum learning principles by gradually increasing sequence length as a measure of complexity. The authors evaluate ISL on a custom dataset derived from MNIST, where handwritten digits are transformed into pen stroke sequences. The method demonstrates significant improvements in training speed (20x faster) and test error reduction (74%) compared to regular sequence learning. The paper also explores transfer learning, showing that a trained sequence prediction model can improve sequence classification performance. Ablation studies and comparisons with other curriculum learning strategies are provided to analyze the effectiveness of ISL.
Decision: Reject
While the paper presents an interesting idea with promising results, it suffers from critical limitations that prevent its acceptance in its current form. The primary reasons for rejection are the limited evaluation on a single, non-standard dataset and the lack of clarity and organization in the presentation.
Supporting Arguments for the Decision
1. Limited Evaluation: The method is tested solely on a custom MNIST-derived dataset, which, while novel, is insufficient to demonstrate the generalizability of ISL. Broader testing on standard sequence learning datasets (e.g., sequential CIFAR, text datasets, or time-series data) and real-world applications is necessary to validate the claims.
2. Presentation Issues: The paper is overly lengthy and poorly organized, resembling a sequential report rather than a cohesive research paper. Key sections, such as the transfer learning experiments, lack a clear connection to the main contribution, and the loss function on page 6 is inadequately explained, with unclear term definitions and symbols.
3. Clarity and Consistency: Table 2 names do not align with the descriptions in Section 4, and the calculation of the "Best value for the average over 10 runs" is not clearly explained. These inconsistencies hinder reproducibility and understanding.
Suggestions for Improvement
1. Broader Evaluation: Test ISL on multiple public datasets and diverse sequence learning tasks to demonstrate its generalizability. Consider applications in natural language processing, speech recognition, or time-series forecasting.
2. Conciseness and Organization: Trim redundant content and reorganize the paper to improve readability. Focus on the core contributions and ensure that all sections are tightly connected to the main narrative.
3. Clarity in Methodology: Provide a detailed explanation of the loss function, including clear definitions of all terms and symbols. Ensure that tables, figures, and text descriptions are consistent and self-explanatory.
4. Transfer Learning Section: Strengthen the connection between the transfer learning experiments and the main contribution. Discuss how ISL specifically aids in transfer learning and provide more detailed comparisons with baseline methods.
5. Reproducibility: While the code and dataset are made available, ensure that all experimental details (e.g., hyperparameters, evaluation metrics) are explicitly stated in the paper.
Questions for the Authors
1. How does ISL perform on standard sequence learning datasets, such as sequential CIFAR or text-based datasets? Can the method generalize to tasks beyond pen stroke sequences?
2. Could you clarify the calculation of the "Best value for the average over 10 runs"? How is this metric derived, and why is it chosen?
3. The transfer learning results are interesting but somewhat disconnected from ISL. Could you elaborate on how ISL specifically contributes to the observed improvements in classification performance?
In summary, the paper proposes a compelling idea with strong initial results, but it requires broader validation, improved clarity, and better organization to meet the standards of the conference.