Review
Summary of Contributions
This paper investigates the use of attention mechanisms in neural language models (LMs) to query context from recent history, addressing the challenge of modeling mid- and long-range dependencies in sequences. The authors propose a novel Key-Value-Predict Attention mechanism that separates output vectors into three distinct components—key, value, and prediction—which improves performance compared to prior memory-augmented models. Surprisingly, they find that a simpler N-gram RNN model, which concatenates recent activation vectors, performs comparably to more complex attention-based architectures. The paper also highlights the persistent difficulty of training models to leverage long-term dependencies effectively. Experimental results demonstrate improved perplexity on the Wikipedia corpus and the Children's Book Test (CBT), with the Key-Value-Predict model achieving state-of-the-art results among memory-augmented LMs.
Decision: Accept
The paper makes a meaningful contribution to the field of neural language modeling by introducing a novel attention mechanism and rigorously comparing it to simpler baselines. The findings challenge assumptions about the utility of complex attention mechanisms and provide valuable insights into the limitations of current models in capturing long-term dependencies. However, there are areas for improvement, particularly in the clarity of model descriptions and the exploration of larger context sizes.
Supporting Arguments
1. Novelty and Impact: While memory-augmented RNNs and attention mechanisms are not new, their application to language modeling is less explored. The explicit separation of key, value, and prediction vectors is an innovative approach that yields significant performance improvements. The finding that simpler models can rival complex architectures is both surprising and impactful, prompting a reevaluation of design priorities in neural LMs.
2. Experimental Rigor: The methodology is sound, with well-constructed baselines (e.g., the N-gram RNN) and thorough evaluation on two diverse corpora. The use of perplexity and accuracy metrics provides a clear basis for comparison.
3. Clarity of Results: The results are well-documented, with clear evidence supporting the claims. The paper convincingly demonstrates that current attention mechanisms struggle to utilize long-term dependencies, a critical insight for future research.
Suggestions for Improvement
1. Model Descriptions: Some aspects of the proposed architectures (e.g., dimensionality choices for key, value, and prediction vectors) are unclear. Explicitly detailing these design decisions would improve reproducibility and understanding.
2. Hyperparameter Sensitivity: The success of the Key-Value-Predict model warrants further investigation under controlled hyperparameter settings. For instance, how does the dimensionality of the separated vectors affect performance? 
3. Exploration of Larger Context Sizes: The paper notes that models primarily attend to the last five tokens, but it does not adequately explore why longer contexts fail. Additional experiments (e.g., forcing attention over distant tokens) could provide deeper insights.
4. Relationship Between Attention and Hidden Units: The unclear interaction between attention mechanism types and hidden unit configurations should be clarified, as this could influence the generalizability of the findings.
Questions for the Authors
1. How does the dimensionality of the key, value, and prediction vectors impact the performance of the Key-Value-Predict model? Did you experiment with alternative splits or non-uniform dimensionalities?
2. Could the failure to leverage long-term dependencies be attributed to optimization challenges or inherent limitations in the architecture? Did you explore techniques like curriculum learning or pretraining to address this?
3. Why does the N-gram RNN perform so well compared to more sophisticated models? Is it possible that the simplicity of this model avoids overfitting or optimization issues?
In conclusion, this paper provides a strong contribution to neural language modeling, with novel insights and practical implications. Addressing the suggested improvements would further strengthen its impact.