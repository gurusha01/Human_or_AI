Review of the Paper: Semi-Supervised Reinforcement Learning for Generalization
Summary of Contributions
This paper proposes a novel framework for semi-supervised reinforcement learning (SSRL), termed Semi-Supervised Skill Generalization (S3G), which aims to address the challenge of generalizing learned policies to real-world scenarios where reward functions are unavailable in many settings. The authors introduce the concept of "labeled" and "unlabeled" Markov Decision Processes (MDPs), where the former provides reward supervision and the latter does not. S3G leverages inverse reinforcement learning (IRL) to infer reward functions in unlabeled MDPs, using prior experience from labeled MDPs as demonstrations. The method is evaluated on continuous control tasks in the Mujoco simulator, demonstrating improved policy generalization compared to standard RL and supervised reward regression baselines. The authors claim that S3G provides better reward shaping and generalization, even outperforming oracle methods in some cases.
Decision: Reject
While the paper addresses an important and underexplored problem in reinforcement learning and proposes an interesting method, it falls short in providing a sufficiently general framework for semi-supervised RL that would appeal broadly to the RL community. The approach is limited to linearly solvable MDPs, which restricts its applicability to more general RL problems. Additionally, the analogy between semi-supervised learning in supervised contexts and SSRL in RL is not fully coherent, particularly regarding the interpretation of "labels" versus "rewards."
Supporting Arguments for Decision
1. Problem Definition and Motivation:  
   The paper tackles a relevant problem—generalizing RL policies with limited reward supervision—and is well-motivated by real-world applications such as robotics. However, the analogy between semi-supervised learning in supervised tasks and SSRL in RL is tenuous. Rewards are fundamentally different from labels, as they are not directly observable and often depend on complex, extrinsic factors. This conceptual gap weakens the foundation of the proposed framework.
2. Methodology:  
   The proposed S3G method is technically sound and builds on established frameworks like maximum entropy RL and guided cost learning. However, the restriction to linearly solvable MDPs significantly limits its applicability. The paper does not address how the method could be extended to more general RL settings, which reduces its impact and generalizability.
3. Empirical Results:  
   The experimental results are promising, showing that S3G outperforms baselines in terms of policy generalization. However, the evaluation is limited to a small set of simulated tasks, and the scalability of the approach to more complex or real-world scenarios is not demonstrated. Furthermore, the claim that S3G outperforms the oracle in some cases is intriguing but insufficiently analyzed, leaving open questions about the robustness of the method.
Additional Feedback for Improvement
1. General Framework:  
   The authors should consider extending their framework beyond linearly solvable MDPs to make it more broadly applicable. Addressing this limitation would significantly enhance the paper's relevance to the RL community.
2. Theoretical Justification:  
   The paper would benefit from a stronger theoretical analysis of the relationship between labeled and unlabeled MDPs. For example, a formal discussion of the conditions under which the inferred rewards generalize well to unlabeled MDPs would strengthen the methodology.
3. Experimental Scope:  
   The experiments are limited to simulated tasks in Mujoco. Including evaluations on more diverse or real-world tasks, such as robotics or dialogue systems, would provide stronger evidence of the method's practical utility.
4. Clarification of Contributions:  
   The paper should clarify its contributions relative to prior work on inverse RL and reward shaping. While the authors claim novelty in applying IRL to SSRL, the connection to existing methods like guided cost learning is not fully disentangled.
Questions for the Authors
1. How does the method handle scenarios where the distribution of states in labeled and unlabeled MDPs differs significantly? Would domain adaptation techniques be necessary in such cases?
2. Can the proposed method be extended to non-linearly solvable MDPs, and if so, what are the key challenges?
3. The claim that S3G outperforms the oracle in some tasks is intriguing. Could the authors provide a deeper analysis of why this occurs and whether it generalizes to other settings?
In conclusion, while the paper addresses an important problem and proposes a promising method, it requires significant improvements in generality, theoretical grounding, and experimental scope to be suitable for acceptance.