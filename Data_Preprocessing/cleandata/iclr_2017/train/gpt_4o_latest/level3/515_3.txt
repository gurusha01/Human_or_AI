Review of the Paper
Summary of Contributions
The paper introduces Exponential Machines (ExM), a novel machine learning model that efficiently captures all feature interactions of every order by leveraging Tensor Train (TT) decomposition. This representation significantly reduces the computational complexity of handling exponentially large tensors of parameters. The authors propose a stochastic Riemannian optimization algorithm tailored to the TT-format, which outperforms standard stochastic gradient descent (SGD) in training efficiency and convergence. The paper demonstrates the efficacy of the proposed method through empirical experiments on both synthetic and real-world datasets, achieving state-of-the-art performance in modeling high-order interactions. Additionally, the authors extend the model to handle interactions between functions of features, broadening its applicability.
Decision: Accept
The paper presents a novel and well-motivated approach to a challenging problem in machine learning: efficiently modeling high-order feature interactions. The use of TT decomposition and Riemannian optimization is innovative and addresses key computational bottlenecks. The empirical results convincingly validate the claims, showing competitive or superior performance compared to existing methods. The paper is well-placed in the literature, with a thorough discussion of related work and clear distinctions from prior approaches. However, there are minor areas for improvement, as detailed below.
Supporting Arguments for Acceptance
1. Novelty and Relevance: The use of TT decomposition for representing high-order polynomial models is novel and addresses the scalability challenges of traditional methods like kernel SVMs and high-order factorization machines. The introduction of Riemannian optimization for this specific problem is a significant contribution.
2. Theoretical Rigor: The paper provides strong theoretical foundations, including proofs of computational complexity and initialization strategies. The use of TT decomposition ensures a linear parameter scaling with respect to the number of features, which is a critical advancement.
3. Empirical Validation: The experimental results are comprehensive, covering synthetic and real-world datasets. The proposed method consistently outperforms baselines like SGD and high-order factorization machines, particularly in scenarios with high-order interactions.
4. Broader Applicability: The extension of the model to handle interactions between functions of features and categorical data demonstrates its flexibility and potential for real-world applications.
Suggestions for Improvement
1. Computational Complexity Per Iteration: While the paper discusses the overall complexity of inference and training, it would be beneficial to explicitly include the computational complexity per iteration of the Riemannian optimization algorithm. This would provide clearer insights into the trade-offs compared to SGD.
2. Sparse Data Support: The authors note that the Riemannian optimization algorithm does not support sparse data, which limits its scalability to very large datasets. Addressing this limitation or providing a discussion on potential solutions (e.g., sparse TT representations) would strengthen the paper.
3. Initialization Sensitivity: The paper highlights the sensitivity of the training process to initialization. While the proposed initialization strategy is effective, further exploration of robust initialization methods or adaptive strategies could enhance the model's usability.
4. Ablation Studies: Including additional ablation studies to isolate the contributions of TT decomposition and Riemannian optimization would provide deeper insights into the model's performance gains.
Questions for the Authors
1. Could you provide a detailed breakdown of the computational complexity per iteration for the Riemannian optimization algorithm, particularly in comparison to SGD?
2. How does the proposed method scale with increasing TT-rank in practice? Are there any observed trade-offs between accuracy and computational cost as the TT-rank increases?
3. Have you explored alternative tensor decomposition methods (e.g., CP decomposition) for comparison, and if so, how do they perform relative to TT decomposition in this context?
4. Are there plans to extend the Riemannian optimization algorithm to support sparse data? If so, what challenges do you foresee?
In conclusion, the paper makes a significant contribution to the field of machine learning by addressing the challenge of modeling high-order feature interactions in a computationally efficient manner. With minor revisions, it has the potential to be a highly impactful work.