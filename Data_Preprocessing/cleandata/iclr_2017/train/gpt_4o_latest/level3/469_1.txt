Review of the Paper
Summary of Contributions
This paper presents a novel approach to pruning convolutional neural networks (CNNs) by focusing on inference speed improvements rather than just memory savings. The authors propose a high-performance sparse convolution design that supports arbitrary sparsity patterns, a performance model to predict speedups at various sparsity levels, and a Guided Sparsity Learning (GSL) algorithm that incorporates speedup awareness into the pruning process. The method achieves significant speedups (3.1–7.3×) on AlexNet across different hardware platforms (Intel Atom, Xeon, and Xeon Phi) without accuracy loss. The availability of source code is a valuable contribution, enabling reproducibility and further exploration.
Decision: Accept
The paper is well-motivated, methodologically sound, and offers significant contributions to the field of CNN optimization. The focus on inference speed, combined with the introduction of a performance model and GSL, represents a novel and impactful advancement. The results are rigorously evaluated, and the source code availability strengthens the paper's credibility and utility.
Supporting Arguments
1. Novelty and Relevance: The paper addresses a critical gap in CNN pruning research by targeting inference speed improvements, which is often overlooked in favor of memory savings. The proposed sparse convolution design and performance model are innovative and practical.
2. Comprehensive Evaluation: The authors thoroughly evaluate their method on AlexNet and GoogLeNet across multiple hardware platforms, demonstrating both the effectiveness and generalizability of their approach.
3. Future Potential: The performance model and GSL algorithm offer a framework that can be extended to optimize throughput in new architectures, making this work highly relevant for future research.
Suggestions for Improvement
1. GPU Transferability: The paper lacks a discussion on the applicability of the performance model and sparse convolution design to GPUs, which are widely used in deep learning. Addressing this limitation would enhance the paper's impact and adoption potential.
2. Figure 4 Clarity: The distinctions in Figure 4 are unclear and need better labeling or visual separation to improve readability.
3. Terminology Clarification: The term "base learning rate" should be explicitly defined to avoid ambiguity.
4. Typos: Minor typographical errors such as "punning" (should be "pruning") and "spares" (should be "sparse") should be corrected for professionalism.
Questions for the Authors
1. How does the performance model generalize to GPUs, and what modifications would be required to adapt the proposed method for GPU architectures?
2. Could the authors provide more details on the computational overhead (e.g., memory access patterns) introduced by the sparse convolution design, particularly in comparison to dense methods?
3. How does the proposed GSL algorithm handle trade-offs between accuracy, speed, and model size in scenarios where these objectives conflict?
Overall, this paper makes a strong contribution to the field of CNN optimization and is recommended for acceptance with minor revisions to address the outlined concerns.