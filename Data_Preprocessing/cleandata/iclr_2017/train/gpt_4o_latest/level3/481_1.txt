Review
Summary of Contributions
This paper investigates adversarial training on ImageNet, a significantly larger dataset than those used in prior studies, and provides several key contributions. The authors present recommendations for scaling adversarial training to large models and datasets, demonstrate the robustness of adversarial training to single-step attack methods, and explore the transferability of adversarial examples across models. A notable contribution is the identification of the "label leaking" effect, where adversarially trained models perform better on adversarial examples than on clean examples due to the adversarial example construction process. The paper also examines the influence of model capacity on adversarial robustness and provides extensive empirical results comparing different adversarial training methods. Overall, the work is clear, well-written, and makes a valuable contribution to understanding adversarial training, particularly in the context of large-scale datasets.
Decision: Accept
The paper should be accepted for the following reasons:
1. Novelty and Scope: The application of adversarial training to ImageNet is novel and addresses a gap in the literature, as prior work has primarily focused on smaller datasets like MNIST and CIFAR-10.
2. Significant Contributions: The identification and explanation of the label leaking effect are particularly impactful, as they provide new insights into adversarial training dynamics.
3. Comprehensive Empirical Analysis: The paper presents a wide range of experiments that rigorously evaluate adversarial training methods, transferability, and model capacity, making the findings robust and actionable for future research.
Supporting Arguments
The paper is well-motivated and positioned within the existing literature. It builds on foundational work in adversarial training and extends it to a challenging and practical setting. The empirical results are thorough and scientifically rigorous, supporting the claims made by the authors. For example, the experiments on label leaking are carefully designed to isolate and explain the phenomenon, and the findings on transferability provide actionable insights for defending against black-box attacks. The authors also provide practical recommendations, such as using random perturbation magnitudes during training and avoiding methods that rely on true labels for adversarial example construction.
Suggestions for Improvement
While the paper is strong, there are areas where it could be further improved:
1. Iterative Methods: The paper notes that adversarial training with iterative methods is computationally expensive and does not yield significant benefits. However, a more detailed discussion of why iterative methods fail and potential alternatives would strengthen this section.
2. Transferability Analysis: The results on transferability are intriguing, particularly the inverse relationship between transferability and attack success. A deeper theoretical exploration of this phenomenon could enhance the paper's impact.
3. Model Capacity: The findings on model capacity are insightful, but the paper could include a discussion of the trade-offs between robustness and computational cost for larger models.
4. Practical Implications: While the paper provides recommendations, a more explicit discussion of how these findings could be applied in real-world scenarios (e.g., autonomous vehicles or medical imaging) would broaden its appeal.
Questions for the Authors
1. Could you elaborate on why adversarial training with iterative methods fails to improve robustness and whether alternative approaches (e.g., hybrid methods) were considered?
2. The label leaking effect is a significant finding. Do you believe this effect could generalize to other datasets or adversarial training methods not explored in this paper?
3. Given the computational cost of training larger models, how do you envision balancing robustness and efficiency in practical applications?
In conclusion, this paper makes a substantial contribution to the field of adversarial training, particularly in scaling to large datasets and identifying novel phenomena like label leaking. With minor improvements, it has the potential to become a highly influential work in the domain.