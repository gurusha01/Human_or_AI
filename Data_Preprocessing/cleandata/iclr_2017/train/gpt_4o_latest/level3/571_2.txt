Review of the Paper
Summary of Contributions
The paper proposes a novel boosting-inspired framework for combining weak generative models into a stronger, unnormalized product of experts model. The authors extend the concept of boosting, traditionally used in supervised learning, to unsupervised settings, focusing on generative modeling. The method trains intermediate models greedily to optimize the joint model, leveraging both generative and discriminative approaches. The framework is flexible, allowing the integration of various base learners, including restricted Boltzmann machines (RBMs), variational autoencoders (VAEs), and convolutional neural networks (CNNs). The authors provide theoretical guarantees for improvement in model fit at each boosting round and demonstrate the method's effectiveness on density estimation, sample generation, and unsupervised feature learning tasks. The experiments show promising results on toy datasets and some improvement over baselines on MNIST.
Decision: Reject
While the paper presents an interesting idea and makes a solid attempt to extend boosting to generative modeling, it suffers from significant limitations that hinder its acceptance. The primary reasons for rejection are (1) the lack of quantitative evaluation and rigorous comparisons, and (2) the unresolved issues of intractable log-likelihood and sampling, which are critical for generative modeling.
Supporting Arguments
1. Strengths:
   - The paper introduces a novel and theoretically grounded approach to boosting generative models, which is a valuable contribution to the field.
   - The flexibility of the framework to incorporate both generative and discriminative models is appealing and could inspire future research.
   - The experiments on 2D toy data convincingly demonstrate the method's ability to correct model misspecification and outperform bagging.
2. Weaknesses:
   - The joint model's intractable log-likelihood and sampling issues, stemming from the unknown normalization constant, are not adequately addressed. These are fundamental challenges in generative modeling and limit the practical applicability of the proposed method.
   - The results on MNIST are underwhelming. Generated samples appear weak compared to simpler models like NADE, and the paper lacks quantitative metrics (e.g., likelihood scores or FID) to substantiate its claims.
   - The experiments are limited in scope, focusing primarily on weak learners. The method's performance with stronger base models or on more complex datasets remains unexplored.
   - The paper does not estimate the partition function \( Z \) or provide a proxy, which is essential for meaningful quantitative evaluation.
Suggestions for Improvement
1. Quantitative Evaluation: Incorporate metrics such as log-likelihood, FID, or classification accuracy on downstream tasks to provide a more rigorous evaluation of the proposed method.
2. Partition Function Estimation: Explore techniques for estimating the partition function \( Z \) or its proxy to enable quantitative comparisons with other generative models.
3. Model Diversity: Experiment with a wider variety of base learners, including stronger generative models, and analyze how their inclusion impacts performance.
4. Sampling Efficiency: Address the computational overhead of MCMC sampling when using discriminators as intermediate models. Investigate alternative sampling strategies to improve efficiency.
5. Generated Sample Quality: Provide a more detailed analysis of sample quality, possibly including visual comparisons with state-of-the-art generative models.
Questions for the Authors
1. How does the proposed method scale to more complex datasets, such as CIFAR-10 or ImageNet? Have you considered testing on such datasets?
2. Can the authors provide quantitative results (e.g., log-likelihood or FID) for the MNIST experiments to better support their claims?
3. Have you explored alternative weighting schemes for intermediate models? How sensitive is the method to the choice of weights?
4. What are the computational trade-offs of using discriminators versus generative models as intermediate learners? Could this impact scalability?
In conclusion, while the paper presents an interesting and theoretically grounded approach, it falls short in empirical rigor and practical applicability, necessitating further refinement before acceptance.