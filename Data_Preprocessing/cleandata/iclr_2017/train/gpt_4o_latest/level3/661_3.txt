Review of "Neural Graph Machines: A Semi-Supervised Learning Approach for Graph-Augmented Neural Networks"
Summary of Contributions
The paper proposes a semi-supervised learning framework, Neural Graph Machines (NGM), which integrates neural network architectures with graph-based label propagation. The key contribution is a novel training objective that combines supervised learning on labeled data with graph regularization to enforce similarity between neighboring nodes' hidden representations. The authors validate their approach on diverse tasks, including multi-label classification on social graphs, text categorization, and semantic intent classification, demonstrating improvements over baseline methods. The framework is scalable, supports stochastic gradient descent, and can handle large graphs efficiently. The paper also explores the use of graph adjacency matrices as direct inputs to neural networks, which eliminates the need for pre-trained embeddings.
Decision: Reject
While the paper addresses an important and relevant topic—semi-supervised learning on graphs—the proposed approach lacks sufficient novelty and rigorous experimental validation. Below, I provide detailed reasoning for this decision.
---
Supporting Arguments for Decision
1. Lack of Novelty:  
   The idea of using graph regularization to enforce similarity between neighboring nodes is not new and has been explored in prior works (e.g., Weston et al., 2012; Jacob et al., 2014). While the paper extends these methods to neural networks, the proposed modifications (e.g., separating edge types and using adjacency matrices as inputs) are incremental rather than groundbreaking. The authors acknowledge that their approach is a non-linear extension of label propagation, which further underscores the limited novelty.
2. Weak Experimental Comparisons:  
   The experimental results fail to provide strong comparisons with state-of-the-art graph-based models, such as Graph Neural Networks (e.g., Graph Convolutional Networks or Graph Attention Networks). The paper primarily compares NGM to simpler baselines (e.g., node2vec embeddings with linear classifiers), which does not convincingly establish the superiority of the proposed method. Additionally, the results for some tasks (e.g., text classification) show only marginal improvements, raising questions about the practical significance of the approach.
3. Insufficient Theoretical and Empirical Support:  
   While the proposed objective function is well-motivated, the paper does not provide a thorough theoretical analysis of its properties or advantages over existing methods. Empirically, the experiments are limited in scope, with no ablation studies to isolate the contributions of specific components (e.g., the impact of edge-type separation or the choice of distance metric). Furthermore, the datasets used are relatively small, and it is unclear how the method performs on larger, more complex graphs.
---
Suggestions for Improvement
1. Stronger Baselines and Comparisons:  
   Compare NGM against modern graph-based methods, such as Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), or GraphSAGE. This would provide a clearer picture of where NGM stands in the current landscape of graph-based learning.
2. Ablation Studies:  
   Conduct ablation studies to evaluate the impact of key design choices, such as the use of adjacency matrices as inputs, the separation of edge types, and the choice of hyperparameters (e.g., α values). This would help clarify the contributions of individual components.
3. Scalability Analysis:  
   While the paper claims that NGM is scalable, there is no empirical evaluation of its performance on large-scale graphs. Including experiments on larger datasets would strengthen the paper's claims.
4. Clarify Novelty:  
   Clearly articulate how NGM differs from prior work beyond being a non-linear extension of label propagation. Highlight any unique theoretical insights or practical advantages.
---
Questions for the Authors
1. How does NGM compare to state-of-the-art graph-based models like GCNs or GATs in terms of both accuracy and computational efficiency?  
2. Can you provide more details on the scalability of the proposed method? Have you tested it on graphs with millions of nodes and edges?  
3. How sensitive is the performance of NGM to the choice of hyperparameters (e.g., α values and distance metrics)?  
4. Why were certain baselines (e.g., GCNs) excluded from the experimental comparisons?  
---
In conclusion, while the paper addresses an important problem and proposes a potentially useful framework, the lack of novelty and insufficient experimental validation significantly weaken its contribution. I encourage the authors to address these issues in a future revision.