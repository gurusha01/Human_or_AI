Review of the Paper
Summary of Contributions
This paper provides a theoretical analysis of the convergence dynamics of two-layer ReLU networks under Gaussian i.i.d. input assumptions. The authors derive novel gradient update rules for such networks and analyze the behavior of gradient descent in both single-node (K=1) and multi-node (K≥2) settings. For K=1, the paper provides a closed-form solution for the nonlinear dynamics and proves convergence under specific initialization conditions. For K≥2, the authors analyze the dynamics under orthonormal teacher parameters and demonstrate that symmetry-breaking initialization leads to global convergence. The paper claims to be the first to provide such global convergence results for nonlinear networks without assuming independence of ReLU activations. Empirical simulations are presented to validate the theoretical findings.
Decision: Reject
While the paper addresses an important theoretical problem and makes some novel contributions, the lack of clarity, inconsistent notation, and questionable generalizability of the results make it unsuitable for acceptance in its current form.
Supporting Arguments for Rejection
1. Clarity and Readability: The second half of the paper (K≥2) is difficult to follow due to unclear relationships (e.g., between θ, φ, and φ*), inconsistent notation (e.g., D vs. Dw), and insufficient explanation of key terms like F(e, w) and D(e). The derivations are dense and lack intuition, making it challenging for readers to grasp the key insights.
2. Generalizability: The strong assumptions of Gaussian and i.i.d. inputs limit the applicability of the results to real-world data, where such conditions rarely hold. This undermines the practical relevance of the theoretical findings.
3. Contradictory Results: Theorem 3.3 suggests that the probability of convergence is maximized as r → 0, but this conflicts with Figure 2, where convergence behavior is shown for non-zero r. Such contradictions raise concerns about the rigor of the analysis.
Suggestions for Improvement
1. Clarity and Structure: Move detailed derivations and dense symbolic formulations to an appendix. Focus on providing high-level intuition and clear explanations in the main text. For instance, explain the significance of symmetry-breaking in Section 4 more intuitively.
2. Notation Consistency: Ensure consistent and meaningful notation throughout the paper. For example, explicitly indicate dependencies (e.g., use Dw instead of D) and clarify the relationships between variables like θ, φ, and φ*.
3. Generalizability: Discuss the implications of the Gaussian and i.i.d. assumptions more critically and explore whether the results can be extended to more realistic data distributions.
4. Figures and Visualizations: Improve the clarity of figures, such as Figure 5, by explicitly labeling key components (e.g., what a_j represents). Provide more intuitive visualizations to support the theoretical results.
Questions for the Authors
1. How do the results change if the Gaussian and i.i.d. assumptions are relaxed? Can the analysis be extended to more general input distributions?
2. Theorem 3.3 and Figure 2 seem to conflict regarding the role of r in convergence probability. Can you clarify this discrepancy?
3. What is the practical significance of the symmetry group's role in Section 4? Could you provide an example or intuition to illustrate its importance?
In summary, while the paper tackles an important problem and offers novel theoretical insights, significant revisions are needed to improve clarity, address contradictions, and enhance the generalizability of the results.