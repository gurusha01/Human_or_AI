The paper introduces a novel regularization method, the "density-diversity penalty," for compressing weight matrices in fully connected layers of neural networks. This method combines the L2-norm of weights (density) and the L1-norm of pairwise differences (diversity) to encourage high sparsity and low diversity in weight matrices. The authors propose a weight-sorting trick to efficiently compute gradients and a weight-tying strategy to update weights with averaged gradients. The training alternates between applying the penalty with untied weights and optimizing tied weights without the penalty. Experiments on MNIST and TIMIT datasets demonstrate impressive compression rates (up to 200x for fully connected layers) without significant performance degradation. The paper claims state-of-the-art results in compression and suggests potential applications of weight-tying beyond compression, such as learning data regularities.
Decision: Accept.  
The paper presents a well-motivated, innovative, and computationally efficient approach to compressing neural networks. The proposed method achieves high compression rates without performance loss, addressing a critical challenge in deploying deep learning models on resource-constrained devices. The novelty of combining sparsity and diversity regularization, along with the efficient gradient computation trick, makes this work a valuable contribution to the field.
Supporting Arguments:  
1. Novelty and Impact: The density-diversity penalty is a unique approach that simultaneously enforces sparsity and low diversity during training, unlike prior methods that address these aspects separately. The weight-tying strategy and alternating training phases are also innovative.  
2. Experimental Results: The method achieves impressive compression rates on MNIST and TIMIT datasets, outperforming or matching state-of-the-art methods like "deep compression." The results demonstrate the practical utility of the approach.  
3. Clarity and Rigor: The paper is well-written, with clear explanations of the methodology and experiments. The use of a sorting trick to reduce computational complexity is particularly noteworthy.  
Additional Feedback:  
1. Result Tables: The tables presenting sparsity and diversity values (p6-p7) are confusing and inconsistent with claims in the text. For example, sparsity values appear inverted, which could mislead readers. Clarifying these inconsistencies is essential.  
2. Minor Errors: There is a grammatical error on page 1 that should be corrected.  
3. Broader Applications: While the paper briefly mentions potential applications of weight-tying beyond compression, this aspect could be elaborated further to highlight the broader impact of the method.  
Questions for the Authors:  
1. Can you clarify the inconsistencies in sparsity values between the text and the tables?  
2. How sensitive is the method to the choice of hyperparameters, such as the penalty strength (Î»)?  
3. Have you considered extending the density-diversity penalty to convolutional or recurrent layers, as mentioned in the future work section? If so, what challenges do you anticipate?  
Overall, the paper makes a strong contribution to model compression and is recommended for acceptance, provided the authors address the minor issues and clarify the experimental results.