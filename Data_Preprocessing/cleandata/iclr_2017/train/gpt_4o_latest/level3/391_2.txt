Review
Summary of Contributions
The paper presents a novel method for pruning Recurrent Neural Networks (RNNs) during training to achieve high sparsity (up to 90%) without requiring additional retraining. This approach reduces storage requirements and improves inference speed, making it suitable for deployment on resource-constrained devices like mobile phones and embedded systems. The authors demonstrate that their method can compress RNNs and Gated Recurrent Units (GRUs) significantly while maintaining competitive accuracy. The technique is computationally efficient, integrates seamlessly with existing training frameworks, and is shown to outperform baseline dense models of equivalent parameter size. Additionally, the paper provides detailed experiments, including comparisons with prior pruning methods, and benchmarks the speedup achieved through sparse matrix operations.
Decision: Accept
The paper makes a significant contribution to the field of model compression, particularly for RNNs, by addressing both storage and inference speed challenges. Its novelty lies in achieving high sparsity during training without retraining, which is a practical improvement over prior methods. The experimental results are robust and scientifically rigorous, supporting the claims made in the paper. However, there are areas where the paper could be improved, particularly in clarifying hyperparameter selection and addressing sparsity-accuracy trade-offs.
Supporting Arguments
1. Novelty and Practical Impact: The proposed method is novel in its ability to achieve high sparsity during training without retraining, which is a notable improvement over prior work like Han et al. (2015). The method is practical for real-world applications, especially for deployment on mobile and embedded devices.
2. Experimental Rigor: The paper provides extensive experimental results, including comparisons with dense baselines, hard pruning methods, and larger sparse models. The results convincingly demonstrate the effectiveness of the proposed approach in terms of both accuracy and computational efficiency.
3. Relevance to the Community: The paper addresses a critical problem in deploying large RNNs on resource-constrained devices, making it highly relevant to the AI and machine learning community.
Suggestions for Improvement
1. Hyperparameter Selection: While the authors provide heuristics for selecting pruning hyperparameters, the process remains complex and may not generalize well across different models and tasks. A more detailed discussion or an automated approach to hyperparameter tuning would strengthen the paper.
2. Sparsity-Accuracy Trade-off: The paper mentions a 10-20% relative accuracy loss at 90% sparsity but does not explore this trade-off in depth. For instance, how does accuracy degrade as sparsity increases beyond 90%? Providing a more detailed analysis of this trade-off would enhance the paper's practical utility.
3. Sparsity Discrepancies: The inconsistencies in sparsity percentages between Table 3 and Table 5, though later resolved, could have been avoided with clearer initial reporting. Ensuring consistency in reported metrics is critical for reproducibility.
4. Comparison with Prior Work: The claim of reduced training time compared to Han et al. (2015) is not fully substantiated, as both methods involve weight pruning. A more detailed comparison of computational overhead during training would clarify this point.
Questions for the Authors
1. Can the proposed method be extended to other architectures, such as Transformer models, or tasks like language modeling and machine translation? If so, what challenges might arise?
2. How sensitive is the method to the choice of pruning hyperparameters? Could an automated or adaptive pruning schedule be developed to simplify deployment?
3. The paper mentions that pruning improves accuracy when starting with a larger dense model. Could this result in diminishing returns for very large models, and how does this scale with dataset size?
In conclusion, the paper makes a strong contribution to the field of model compression and is well-suited for acceptance at the conference. Addressing the above suggestions would further enhance its impact and clarity.