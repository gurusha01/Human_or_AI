Review of "A2T: Attend, Adapt, and Transfer - An Attentive Deep Architecture for Adaptive Transfer Learning"
Summary of Contributions
The paper proposes A2T (Attend, Adapt, and Transfer), a novel deep neural network architecture for adaptive transfer learning in reinforcement learning (RL). A2T addresses two key challenges in transfer learning: negative transfer (where transferred knowledge hampers learning) and selective transfer (where only relevant knowledge is transferred). The architecture employs a soft attention mechanism to dynamically weigh the contributions of multiple source task solutions and a base network trained from scratch. A2T is generic and supports both policy and value function transfer. The authors demonstrate the efficacy of A2T through extensive experiments on simulated environments (Chain World, Puddle World) and Atari 2600 games (Pong, Breakout). The results highlight A2T's ability to selectively transfer knowledge, avoid negative transfer, and adapt to imperfect or partially useful source tasks.
Decision: Reject
While the paper introduces a promising architecture and provides strong empirical results, there are significant limitations in the depth of analysis and exploration of broader implications. These limitations, particularly around practical efficiency, hierarchical RL connections, and reusable policy libraries, prevent the paper from making a sufficiently robust contribution to the field.
Supporting Arguments for the Decision
1. Strengths:
   - Novelty: The use of a soft attention mechanism for selective transfer is innovative and well-motivated. The ability to dynamically adapt to different parts of the state space is a key strength.
   - Empirical Validation: The experiments convincingly demonstrate A2T's ability to avoid negative transfer and selectively leverage source tasks. The visualization of attention weights adds interpretability to the results.
   - Clarity: The architecture and its integration into RL setups are well-explained, making it accessible for future research.
2. Weaknesses:
   - Limited Practical Insights: The paper does not analyze data efficiency or wall-clock time, which are critical for evaluating the practical utility of A2T. For instance, the inefficiency in identifying a perfect Pong policy in the expert library raises concerns about scalability.
   - Underexplored Hierarchical RL Connections: The architecture's connection to hierarchical RL is acknowledged but not explored in depth. This omission limits the paper's contribution to broader RL paradigms, such as meta-learning or lifelong learning.
   - Reusable Policy Libraries: The paper does not address how to construct or manage reusable libraries of policies, a key challenge in real-world applications of hierarchical RL.
   - Negative Transfer in Complex Scenarios: While A2T avoids negative transfer in the presented experiments, the paper does not explore its performance in more challenging, compositional reuse scenarios highlighted in prior work.
   - Overemphasis on Synthetic Tasks: The experiments primarily focus on synthetic or modified tasks (e.g., blurred Pong), which, while illustrative, limit the generalizability of the results to real-world problems.
Suggestions for Improvement
1. Data Efficiency and Scalability: Provide a detailed analysis of A2T's data efficiency and computational overhead. Compare its performance to simpler baselines in terms of training time and resource usage.
2. Hierarchical RL and Meta-Learning: Explore the connection between A2T and hierarchical RL more rigorously. For instance, how can A2T be extended to learn reusable sub-policies or meta-controllers for lifelong learning?
3. Broader Applications: Test A2T on more diverse and realistic tasks, such as continuous control benchmarks or multi-agent RL scenarios, to demonstrate its generalizability.
4. Policy Library Construction: Discuss strategies for building and maintaining reusable policy libraries, including how A2T could be integrated with existing approaches like Progressive Neural Networks or modular RL.
5. Negative Transfer in Complex Tasks: Investigate A2T's robustness in scenarios with highly conflicting source tasks or tasks requiring compositional reuse of multiple policies.
Questions for the Authors
1. How does A2T perform in terms of wall-clock time and data efficiency compared to simpler transfer learning methods (e.g., fine-tuning)?
2. Can the architecture be extended to tasks with different state-action spaces? If so, how would the attention mechanism adapt?
3. How does A2T handle catastrophic forgetting when the base network is trained alongside the attention mechanism?
4. What are the implications of A2T for hierarchical RL? Could the attention network be interpreted as a soft meta-controller for sub-goals?
5. How would A2T perform in tasks requiring compositional reuse of multiple source task policies, such as combining skills learned in different domains?
In conclusion, while A2T is a promising step forward in adaptive transfer learning, addressing the above concerns would significantly strengthen the paper's contribution and impact.