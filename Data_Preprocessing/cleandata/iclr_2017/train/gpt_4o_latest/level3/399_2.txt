The paper introduces a novel approach to conditional computation in deep learning by leveraging Sparsely-Gated Mixture-of-Experts (MoE) layers to significantly increase model capacity while maintaining computational efficiency. The authors address key challenges in implementing conditional computation, such as batch size reduction, network bandwidth limitations, and load balancing, and propose innovative solutions to overcome these obstacles. The paper demonstrates the effectiveness of MoE layers in language modeling and machine translation tasks, achieving state-of-the-art results with models containing up to 137 billion parameters. The proposed method achieves greater than 1000x improvements in model capacity with only minor computational overhead, making it a significant contribution to scaling deep learning models.
Decision: Accept
Key reasons for acceptance:
1. Innovative and impactful contribution: The paper successfully demonstrates the practical realization of conditional computation using MoE layers, addressing long-standing challenges in the field. The results show significant improvements in both model quality and computational efficiency.
2. Strong experimental validation: The authors provide extensive empirical evidence across multiple benchmarks, including language modeling and machine translation, to support their claims. The results are rigorously analyzed and demonstrate clear advantages over existing state-of-the-art methods.
Supporting Arguments:
- Strengths: 
  - The use of MoE layers to expand model capacity without a proportional increase in computational cost is a major advancement. The hierarchical MoE design and load-balancing techniques are particularly noteworthy.
  - The paper presents insightful experimental results on scaling MoE layers, showing how increased capacity benefits large datasets.
  - The method achieves impressive performance improvements on real-world benchmarks, such as the 1-billion-word language modeling dataset and WMT'14 machine translation tasks.
- Weaknesses: 
  - The paper lacks a detailed discussion comparing MoE with alternative methods for increasing model capacity, such as dense scaling or other sparse computation techniques. This comparison would provide a more comprehensive understanding of the trade-offs in computational efficiency, memory usage, and scalability.
Additional Feedback:
1. Comparative Analysis: Including a comparison with alternative methods, such as sparsity-inducing techniques (e.g., pruning, quantization) or dense scaling approaches, would strengthen the paper. This would help contextualize the advantages and limitations of MoE layers.
2. Broader Applicability: While the paper focuses on text-based tasks, a discussion on the potential applicability of MoE layers to other domains, such as vision or speech, would enhance its impact.
3. Scalability to trillion-scale models: The authors mention the potential to scale to trillion-parameter models but do not provide experimental evidence. Including preliminary results or simulations would bolster this claim.
Questions for Authors:
1. How does the computational efficiency of MoE layers compare to other sparse computation techniques, such as sparsely-activated transformers or pruning methods, in terms of hardware utilization and latency?
2. Can the proposed load-balancing loss functions generalize to other sparse architectures, or are they specific to MoE layers?
3. Have you explored the impact of MoE layers on tasks beyond text, such as image classification or reinforcement learning? If not, what challenges do you anticipate in extending this approach to other domains?
In conclusion, the paper makes a significant contribution to the field of deep learning by addressing the challenges of conditional computation and demonstrating the scalability and effectiveness of MoE layers. While there are areas for improvement, the strengths of the work far outweigh its weaknesses, and I recommend acceptance.