Review of "NewsQA: A Machine Comprehension Dataset"
Summary of Contributions:
The paper introduces NewsQA, a large-scale machine comprehension dataset comprising over 100,000 question-answer pairs derived from 10,000 CNN news articles. The dataset is collected using a four-stage process: article filtering, question collection, answer collection, and validation. NewsQA is designed to encourage reasoning-based comprehension, with questions requiring synthesis, inference, and paraphrasing. The dataset is compared to SQuAD, highlighting its unique challenges, such as longer articles, diverse answer types, and questions with null answers. The authors also propose the BARB model, which achieves competitive performance while being computationally efficient.
Decision: Accept
The paper makes a significant contribution to the field of machine comprehension by introducing a challenging dataset that fills gaps in existing benchmarks. The dataset's diversity, reasoning requirements, and inclusion of null answers provide a valuable resource for advancing state-of-the-art models. However, improvements in human evaluation and dataset analysis are needed to strengthen the paper.
Supporting Arguments:
1. Strengths:
   - Novel Dataset: NewsQA introduces a dataset sourced from news articles, offering a distinct challenge compared to Wikipedia-based datasets like SQuAD. Its emphasis on reasoning and synthesis is a valuable addition to the field.
   - Diversity and Size: The dataset's size and linguistic diversity make it suitable for training deep learning models, addressing limitations of smaller datasets like MCTest.
   - Null Answers: The inclusion of questions with no answers (9.5% of the dataset) adds realism and complexity, encouraging models to handle unanswerable questions.
   - Efficient Model: The proposed BARB model achieves state-of-the-art performance while being computationally faster than mLSTM, making it a practical baseline for future research.
2. Weaknesses:
   - Limited Human Evaluation: The human evaluation is conducted on only 1,000 examples, which is insufficient to generalize findings across the entire dataset.
   - Reasoning-Type Analysis: Only 500 examples are labeled for reasoning types, which is inadequate for robust analysis. Additionally, model performance on these reasoning-labeled examples is not reported.
   - Test Set Size: The test set is small, potentially limiting the robustness of model evaluation.
   - Inconsistencies: Agreement statistics between Sections 3.5 and 4.1 are inconsistent, and Figure 1 lacks clarity regarding the evaluated model.
   - Hyperparameter Tuning: The paper does not explore hyperparameter tuning specific to NewsQA, which could provide insights into model optimization.
Suggestions for Improvement:
1. Expand Human Evaluation: Conduct a more comprehensive human evaluation across a larger subset of the dataset to validate the dataset's quality and difficulty.
2. Reasoning-Type Analysis: Increase the number of reasoning-labeled examples and report model performance on these subsets to better understand the challenges posed by different reasoning types.
3. Dataset Versions: Release two versions of the dataset—one with raw answers and another with validated answers—to provide flexibility for different research needs.
4. Clarify Methodology: Address ambiguities, such as whether multiple questioners for the same article are prevented from asking similar questions and whether the two "students" mentioned in the paper are researchers or undergraduates.
5. Improve Figures: Enhance the clarity of Figure 1 by explicitly stating which model is being evaluated.
Questions for the Authors:
1. How were the 500 reasoning-labeled examples selected? Are they representative of the entire dataset?
2. Were measures taken to prevent multiple questioners from asking similar questions for the same article?
3. Could you provide more details on the hyperparameter tuning process for BARB and mLSTM? Were the same settings used for both models?
In conclusion, the paper presents a valuable dataset and baseline model that can significantly contribute to machine comprehension research. Addressing the outlined weaknesses would further enhance its impact and utility.