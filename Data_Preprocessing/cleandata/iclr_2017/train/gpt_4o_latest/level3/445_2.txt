The paper explores the use of End-to-End Memory Networks for dialogue learning in an online reinforcement learning setting, focusing on teacher feedback and non-positive rewards. It builds on Weston (2016) by addressing two limitations: the lack of reinforcement learning and the absence of real-language data. The authors demonstrate that their model can improve its question-answering ability through online interactions with teachers, using both synthetic and real human feedback collected via Mechanical Turk. The work emphasizes the feasibility of transitioning from a fixed dataset-trained model to one that learns iteratively through human interaction, showcasing the potential for practical applications.
Decision: Borderline Accept
The decision leans toward acceptance due to the paper's emphasis on real-world applicability and its demonstration of iterative learning with real human feedback. However, the limited novelty and incremental contributions compared to Weston (2016) remain significant concerns.
Supporting Arguments:
1. Strengths:  
   - The paper is clearly written and systematically evaluates several models, including reward-based and forward prediction methods.  
   - It introduces real-world experiments with Mechanical Turk, which adds practical value and moves beyond synthetic datasets.  
   - The iterative batch learning approach demonstrates that the model can improve over time, addressing a key limitation of Weston (2016).  
   - The use of textual feedback in addition to numerical rewards is an important step toward natural human-bot interaction.  
2. Weaknesses:  
   - The novelty is limited, as the work primarily extends Weston (2016) to an online setting with real data.  
   - The contributions, while useful, are incremental. The main advance—showing that the model works in an online data collection setting—is not groundbreaking.  
   - The reliance on Mechanical Turk experiments, while appreciated, may not meet the rigor expected for a top-tier conference paper.  
Additional Feedback for Improvement:  
- The paper could benefit from a more thorough discussion of its contributions relative to Weston (2016). For example, emphasizing the practical challenges addressed (e.g., instability in online learning) and how they were overcome would strengthen the case for novelty.  
- The Mechanical Turk experiments should be expanded or complemented with additional real-world scenarios to validate the robustness of the approach.  
- The authors could explore more diverse dialogue tasks beyond question-answering to demonstrate broader applicability.  
- A deeper analysis of the trade-offs between textual feedback and numerical rewards, including their impact on learning efficiency, would add value.  
Questions for the Authors:  
1. How does the model handle ambiguous or contradictory feedback from real human teachers?  
2. Can the approach generalize to more complex dialogue tasks, such as multi-turn conversations or open-domain dialogue?  
3. What are the implications of the observed instability in forward prediction methods, and how might this be mitigated in future work?  
4. Could the authors provide more details on how the clustering for data balancing was implemented in the real data experiments?  
In summary, while the paper makes incremental contributions, its focus on real-world implementation and iterative learning provides value. Addressing the outlined weaknesses and questions could significantly strengthen the work.