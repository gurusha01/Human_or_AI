The paper introduces Deep Variational Bayes Filters (DVBF), a novel method for unsupervised learning of latent Markovian state-space models. By leveraging Stochastic Gradient Variational Bayes (SGVB), DVBF addresses the challenge of intractable inference distributions and enables learning from raw, non-Markovian sequence data, such as image sequences. The authors claim that their approach enforces state-space assumptions, allowing for reliable system identification and plausible long-term predictions. The method is validated through experiments on simulated dynamical systems, demonstrating its ability to recover latent states that reflect underlying physical quantities.
Decision: Reject.  
While the paper presents an interesting application of SGVB to state-space models and shows promise in its experimental results, it has significant shortcomings that hinder its acceptance. The primary reasons for rejection are the lack of clarity in the exposition and insufficient discussion on key theoretical aspects, particularly the role of priors and their dependence on past states.
Supporting Arguments:  
1. Strengths:  
   - The paper successfully applies SGVB to state-space models, treating innovation variables as latent variables and performing inference over these variables instead of states.  
   - The experimental results, though limited to toy examples, demonstrate the potential of DVBF in capturing latent dynamics and enabling long-term predictions.  
   - The approach of enforcing the latent space to fit the transition is novel and addresses limitations in prior work, such as Deep Kalman Filters (DKF).  
2. Weaknesses:  
   - The explanation of the methodology is verbose and delves into specifics too quickly, making it difficult to follow. For instance, Section 2.1 could better clarify the distinction between Bayesian and non-Bayesian treatment of parameters like \(Ft\) and \(Bt\).  
   - The paper does not adequately address how priors over innovation variables (\(\beta_t\)) could depend on past states, which is a critical aspect of modeling temporal dependencies.  
   - The contribution of forcing the latent space to fit the transition, while interesting, appears trivial without a deeper exploration of its implications.  
   - Equation 9 lacks clarity in its factorization of the recognition model, leaving room for alternative interpretations that are not discussed.  
Additional Feedback for Improvement:  
- The authors should provide a more compact and structured explanation of their approach, particularly in Sections 2.1 and 2.2. Introducing the key ideas before diving into technical details would improve readability.  
- A discussion on how priors over \(\beta_t\) could incorporate temporal dependencies would strengthen the theoretical foundation of the work.  
- The experimental validation should extend beyond toy examples to real-world datasets or more complex systems to demonstrate broader applicability.  
- Clarify the implications of the factorization in Equation 9 and discuss alternative factorizations that could be explored.  
- Highlight the limitations of the proposed method, such as scalability to high-dimensional latent spaces or sensitivity to hyperparameters.
Questions for the Authors:  
1. How would the performance of DVBF change if priors over \(\beta_t\) were made time-dependent? Could this improve the model's ability to capture temporal dependencies?  
2. Can you provide more insight into the choice of factorization in Equation 9? Why was this specific form chosen, and what alternatives were considered?  
3. How does DVBF perform on real-world datasets or more complex systems beyond the simulated pendulum and bouncing ball experiments?  
In conclusion, while the paper introduces a promising approach, it requires significant revisions to its clarity, theoretical depth, and experimental scope to meet the standards of acceptance.