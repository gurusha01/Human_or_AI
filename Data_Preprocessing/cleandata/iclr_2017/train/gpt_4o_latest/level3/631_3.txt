Review of the Paper
Summary of Contributions
This paper proposes a novel Layer-RNN (L-RNN) module as a hybrid architecture that interleaves convolutional neural network (CNN) layers with 1D recurrent neural networks (RNNs). The L-RNN module is designed to efficiently capture global context within layers, offering an alternative to deep stacking for increasing receptive fields. The authors claim three primary contributions: (i) introducing the L-RNN module for adaptive contextual learning, (ii) demonstrating seamless integration of L-RNN into pre-trained CNNs with fine-tuning, and (iii) reporting competitive results on CIFAR-10 and PASCAL VOC2012 tasks. They also highlight an elegant initialization strategy for integrating L-RNNs into pre-trained networks.
Decision: Reject
While the paper explores an interesting and underexplored topic of integrating 1D RNNs into CNNs, the overall contributions are incremental relative to prior work, and the experimental results lack rigor and generalizability. The following key reasons support the rejection:
1. Limited Novelty and Contribution Concerns: The proposed L-RNN module is conceptually similar to prior work, such as Bell et al.'s Inside-Outside Net and Visin et al.'s ReNet. The differences are minor, and the paper does not provide a compelling argument for its advantages over these existing methods. The claim of general applicability is undermined by the fact that L-RNN is only integrated at the network's end in experiments, contrary to its stated potential for use at multiple levels.
2. Weak Experimental Validation: The results on CIFAR-10 are not convincing, as they lack comparisons to state-of-the-art methods on larger datasets like ImageNet. Additionally, the absence of direct comparisons between models with and without L-RNN modules makes it difficult to assess the module's true impact. The reported improvements in semantic segmentation tasks are modest and do not clearly establish the superiority of L-RNN over simpler alternatives like dilated convolutions.
Supporting Arguments
- Implementation Gap: Despite claims of general applicability, the L-RNN module is only evaluated when placed at the final stage of the network. This contradicts the introduction's claim that L-RNN can be interleaved at multiple levels.
- Comparison Deficiency: The paper does not directly compare its method to Bell et al.'s design or other relevant baselines, leaving its advantages unclear.
- Evaluation Gap: The lack of ablation studies and direct comparisons between models with and without L-RNN modules weakens the empirical support for the proposed method.
- Limited Generalizability: The experiments are restricted to CIFAR-10 and PASCAL VOC2012, which are insufficient to demonstrate the scalability and robustness of the proposed approach.
Suggestions for Improvement
1. Strengthen Novelty: Clearly articulate the differences and advantages of L-RNN over related works like Bell et al. and Visin et al. Provide theoretical or empirical evidence to support these claims.
2. Expand Experiments: Evaluate the method on larger datasets like ImageNet to demonstrate generalizability. Include ablation studies to isolate the impact of L-RNN modules.
3. Direct Comparisons: Compare the proposed method directly with Bell et al.'s Inside-Outside Net and other relevant baselines to establish its relative performance.
4. Clarify General Use: Demonstrate the integration of L-RNN at multiple levels of the network, as claimed in the introduction, to validate its flexibility.
5. Improve Presentation: Address minor issues such as the pixelation and distracting design elements in Figure 4 to enhance clarity.
Questions for the Authors
1. How does the proposed L-RNN module compare to Bell et al.'s Inside-Outside Net in terms of computational efficiency and performance?
2. Why were experiments limited to integrating L-RNN at the network's end? Can you provide results for interleaving L-RNN modules at multiple levels?
3. Can you provide ablation studies comparing models with and without L-RNN modules to isolate their impact?
4. Have you considered evaluating the method on larger datasets like ImageNet to demonstrate scalability and generalizability?
In conclusion, while the paper introduces an interesting idea, the lack of substantial novelty, rigorous comparisons, and comprehensive experimental validation prevents it from making a strong case for acceptance.