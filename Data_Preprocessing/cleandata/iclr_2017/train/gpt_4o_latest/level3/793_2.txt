The paper introduces a novel approach to enhancing the modeling power of recurrent neural networks (RNNs) by incorporating previous error signals (surprisal) as an additional input during inference. This feedback mechanism is proposed to improve the generalization capabilities of RNNs, particularly Long Short-Term Memory (LSTM) networks, for temporal data. The authors demonstrate their method on the enwik8 character-level text prediction task, reporting state-of-the-art performance of 1.37 bits per character (BPC). The paper positions its contribution as a step toward understanding the role of top-down feedback in neural networks, inspired by mechanisms observed in human cognition.
Decision: Reject
The primary reasons for rejection are the unrealistic assumptions made by the proposed approach and the lack of comprehensive experimental validation. Specifically, the method assumes access to test label information during inference, which is impractical in real-world scenarios and creates an unfair comparison with models that do not leverage such information. Additionally, the experimental evaluation is limited to a single dataset, and the paper fails to cite or compare against more recent works that achieve better performance on the same task. These issues undermine the scientific rigor and generalizability of the claims.
Supporting Arguments:
1. Unrealistic Assumption: The use of test error signals as input during inference is problematic. While the authors argue that the model does not see test data during training, the reliance on test labels during inference is a significant limitation that reduces the practical applicability of the method.
2. Limited Experimental Scope: The experiments are restricted to the enwik8 dataset, which is insufficient to demonstrate the generalizability of the proposed approach. Furthermore, the paper does not compare its results against more recent state-of-the-art methods, which weakens the claim of achieving superior performance.
3. Lack of Model Details: The manuscript does not provide sufficient details about the model size or architecture, making it difficult to assess the contribution of the proposed feedback mechanism independently of other factors.
Suggestions for Improvement:
1. Address the unrealistic assumption by exploring ways to incorporate feedback mechanisms that do not rely on test label information.
2. Expand the experimental evaluation to include multiple datasets and provide comparisons with more recent and competitive baselines.
3. Include detailed descriptions of the model architecture, hyperparameters, and computational costs to enable reproducibility and fair assessment.
4. Provide additional theoretical or empirical evidence to justify the use of surprisal as a feedback signal and its role in improving generalization.
Questions for the Authors:
1. How do you justify the use of test label information during inference, and how would the model perform without this assumption?
2. Why were more recent state-of-the-art methods not cited or compared against in the experimental results?
3. Can you provide details on the model size and architecture used in the experiments?
While the paper presents an interesting idea, addressing these issues is critical for the work to be considered scientifically rigorous and practically relevant.