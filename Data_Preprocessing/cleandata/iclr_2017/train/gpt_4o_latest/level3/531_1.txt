Review
The paper proposes a novel generative model for video synthesis that leverages a background and 2D objects (sprites) within a Variational Autoencoder (VAE) framework. The authors introduce an innovative approach to image translation using the outer product of softmaxed vectors combined with convolution, which simplifies the process while maintaining differentiability. The proposed Perception Updating Networks (PUNs) aim to decouple "what" (content) and "where" (location) in video frames, inspired by computer graphics pipelines. This work contributes to the field by offering a potentially interpretable and computationally efficient framework for video generation.
Decision: Reject
Key reasons for rejection are the lack of clarity and rigor in the experimental section and the absence of experiments on real-world data. These shortcomings significantly limit the paper's scientific rigor and practical relevance.
Supporting Arguments
1. Clarity and Completeness of Experiments: The experimental section is poorly presented and appears rushed. Key results are incomplete or missing, even in the appendix. For instance, while the authors claim that their method outperforms baseline LSTMs in some cases, the results are not sufficiently detailed or statistically validated. Additionally, the Moving MNIST experiment, while a standard benchmark, does not provide enough evidence of the model's generalizability or real-world applicability.
2. Lack of Real-World Validation: The paper exclusively focuses on synthetic datasets (e.g., bouncing shapes and Moving MNIST), which limits its practical relevance. The proposed method's performance on real-world videos remains unexplored, making it difficult to assess its utility in realistic scenarios. The authors suggest aerial videos as a potential application but fail to include any experiments or analysis in this domain.
3. Writing and Presentation Issues: The paper requires significant proofreading. There are missing references, unfinished sentences, and unclear explanations, particularly in the experimental section. These issues detract from the paper's readability and make it challenging to evaluate the contributions fully.
Additional Feedback
1. Experimental Design: The authors should include experiments on real-world datasets to demonstrate the practical applicability of their method. Aerial videos, as suggested in the paper, would be a good starting point, given the planar assumption.
2. Comparative Analysis: A more comprehensive comparison with state-of-the-art methods, such as Video Pixel Networks or other VAE-based approaches, is necessary. This should include quantitative metrics (e.g., negative log-likelihood, FID scores) and qualitative visualizations.
3. Model Interpretability: While the paper emphasizes interpretability, the authors should provide more explicit visualizations or analyses to demonstrate how the "what" and "where" components are decoupled and learned.
4. Architectural Improvements: The authors mention potential improvements, such as adding a memory unit to avoid recalculating sprites at every time step. Implementing and evaluating these enhancements would strengthen the paper.
Questions for the Authors
1. Can you provide more details on the experimental setup, including hyperparameter tuning, training stability, and computational efficiency?
2. Why were real-world datasets not included in the evaluation? Are there specific challenges in applying the proposed method to such data?
3. How does the proposed method handle occlusions or overlapping sprites in more complex scenes?
4. Could you elaborate on the limitations of the spatial transformer-based approach and its higher computational cost compared to the convolutional approach?
In summary, while the paper introduces an interesting and potentially impactful idea, it falls short in execution, particularly in terms of experimental rigor and real-world validation. Addressing these issues would significantly enhance the paper's quality and relevance.