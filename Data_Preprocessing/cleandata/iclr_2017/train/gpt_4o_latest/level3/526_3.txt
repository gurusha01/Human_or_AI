Review of the Paper
The paper presents a novel contribution by deriving a convergence guarantee for modern convolutional networks (convnets) with rectifiers and max-pooling, which are inherently nonsmooth and nonconvex. The key innovation lies in introducing the neural Taylor approximation and Taylor loss, which provide a convex snapshot of the network's loss landscape. The authors demonstrate that the regret bound matches the theoretical lower bound for convex nonsmooth functions. Empirical evidence is provided on CIFAR-10 (cross-entropy loss) and an autoencoder (MSE loss), showcasing the practical relevance of the theoretical insights.
The paper also explores the role of activation configurations in optimization and hypothesizes that adaptive optimizers like RMSProp and Adam explore the space of activation configurations more thoroughly than SGD. This hypothesis is supported by empirical studies showing that optimizers with greater exploration achieve better training loss. The authors further suggest that fixed learning rates in SGD may limit its performance and propose learning rate annealing as a potential improvement.
Decision: Accept
The decision to accept this paper is based on two primary reasons:
1. Novelty and Theoretical Contribution: The paper provides the first convergence guarantee for modern convnets, bridging a critical gap in the literature on nonsmooth and nonconvex optimization.
2. Empirical Validation: The theoretical claims are rigorously supported by empirical studies across multiple network scales (layer, neuron, and whole network), demonstrating the practical utility of the proposed framework.
Supporting Arguments
1. The neural Taylor approximation is a significant conceptual tool that enables the analysis of nonsmooth loss surfaces, particularly at kinks caused by rectifiers. This addresses a longstanding challenge in understanding optimization dynamics in deep learning.
2. The empirical results are thorough and well-aligned with the theoretical claims. The correlation between training loss differences (observed vs. Taylor loss) and optimizer performance is compelling and provides actionable insights for improving optimization techniques.
3. The multi-scale regret analysis (network, layer, and neuron) is a valuable addition, offering a granular understanding of optimization dynamics.
Suggestions for Improvement
1. Cross-Validation for Generalization: While the paper focuses on training loss, incorporating cross-validation results could provide deeper insights into the generalization performance of the proposed methods.
2. Learning Rate Annealing: The authors suggest that annealing the learning rate in SGD might improve performance. Including empirical results to validate this claim would strengthen the paper.
3. Clarification on Jacobian Subscript: The change in the Jacobian subscript to "a_l" is unclear and requires further explanation to avoid confusion.
4. Broader Exploration: The empirical studies are limited to CIFAR-10 and MNIST. Extending the experiments to more complex datasets and architectures would enhance the generalizability of the findings.
Questions for the Authors
1. Can you provide more details on why the Jacobian subscript changes to "a_l"? Is this a notational simplification or does it have a deeper mathematical implication?
2. How sensitive are the empirical results to the choice of hyperparameters for the optimizers? For example, would RMSProp and Adam still outperform SGD with a finely tuned learning rate schedule?
3. Have you considered the impact of batch size on the exploration of activation configurations? Could larger or smaller batch sizes influence the observed differences between optimizers?
In summary, the paper makes a significant theoretical and empirical contribution to understanding optimization in nonsmooth neural networks. With minor clarifications and additional experiments, the work could become a foundational reference in the field.