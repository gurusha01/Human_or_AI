Review of "Eve: Feedback-Driven Learning Rate Adjustment for Stochastic Gradient Descent"
Summary
This paper introduces Eve, an extension of the Adam optimizer, which adaptively adjusts the learning rate by incorporating feedback from the objective function. The authors hypothesize that tracking relative changes in the cost function can improve optimization by dynamically modulating the learning rate. The method is empirically evaluated on convolutional neural networks (CNNs), recurrent neural networks (RNNs), and logistic regression tasks, demonstrating superior performance over state-of-the-art optimization algorithms. The authors also provide an analysis of the behavior of the feedback mechanism during training and propose a thresholding scheme to stabilize the learning rate adjustments.
Decision: Reject
While the empirical results are promising, the paper lacks sufficient theoretical justification, clarity, and rigor to warrant acceptance. The key reasons for this decision are the following:
1. Theoretical Weakness and Lack of Novelty: The paper does not provide a theoretical foundation for the proposed method, nor does it discuss potential failure modes. The novelty of the approach is limited, as it builds incrementally on existing optimizers like Adam without introducing fundamentally new concepts.
2. Concerns About Fairness and Robustness: The method is sensitive to arbitrary shifts and scaling of the cost function, which undermines its general applicability. Additionally, the comparison with other methods could be improved by incorporating exponential decay learning rate schedules for baselines.
3. Clarity and Writing Quality: Section 3.2, which describes the algorithm, is difficult to follow due to poor organization and unclear explanations. This hinders reproducibility and understanding of the method.
Supporting Arguments
1. Empirical Results: The experiments convincingly show that Eve outperforms Adam and other optimizers across various tasks, including CIFAR-10, CIFAR-100, Penn Treebank, and bAbI datasets. However, the lack of theoretical analysis makes it difficult to generalize these results or understand why Eve works better in practice.
2. Hyperparameter Complexity: Eve introduces three additional hyperparameters (β₃, k, K), which increases the complexity of tuning. The authors do not provide sufficient justification for the chosen default values or discuss the sensitivity of the method to these hyperparameters.
3. Variant to Cost Function Scaling: The reliance on relative changes in the cost function makes the method sensitive to arbitrary scaling or shifting of the objective function. This is a significant limitation that is not adequately addressed in the paper.
Suggestions for Improvement
1. Theoretical Justification: Provide a theoretical analysis of the feedback mechanism, including its convergence properties, stability, and potential failure modes. This would strengthen the paper's contribution and address concerns about robustness.
2. Fair Comparisons: Include experiments with exponential decay learning rate schedules for baseline methods to ensure a fairer comparison. Additionally, discuss the computational overhead introduced by Eve compared to Adam.
3. Clarity and Writing: Rewrite Section 3.2 with clearer explanations and better organization. Use diagrams or pseudocode to illustrate the algorithm more effectively.
4. Hyperparameter Sensitivity Analysis: Conduct a thorough analysis of the sensitivity of the method to the new hyperparameters (β₃, k, K) and provide practical guidelines for their selection.
5. Scaling Invariance: Address the sensitivity to cost function scaling by either modifying the method or explicitly discussing its implications and limitations.
Questions for the Authors
1. How does Eve perform when the cost function includes regularization terms (e.g., L1 or L2 penalties) that violate the assumption of a known minimum value?
2. What is the computational overhead of Eve compared to Adam, and how does it scale with larger models or datasets?
3. Can you provide more insight into the choice of default values for β₃, k, and K? How sensitive is the method to these hyperparameters?
4. Have you tested Eve on tasks with highly noisy or non-smooth objective functions? How does it perform in such scenarios?
While the paper presents an interesting idea with promising empirical results, it requires significant improvements in theoretical grounding, clarity, and robustness to be suitable for publication.