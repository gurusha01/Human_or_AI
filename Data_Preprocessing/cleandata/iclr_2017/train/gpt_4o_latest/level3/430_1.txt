The paper introduces the Latent Sequence Decompositions (LSD) framework, which learns to decompose sequences into variable-length tokens dynamically, based on both input and output sequences. This approach addresses the limitations of fixed token decompositions in sequence-to-sequence models, particularly for tasks like speech recognition. The authors demonstrate the effectiveness of LSD on the Wall Street Journal speech recognition task, achieving a Word Error Rate (WER) of 12.9%, improving upon a character-based baseline (14.8%). When combined with a convolutional encoder, the WER further reduces to 9.6%. The paper claims that LSD can learn decompositions that are more efficient and better aligned with the input-output relationship compared to static methods.
Decision: Reject.  
While the paper presents a novel and well-motivated approach, it lacks critical baseline comparisons and broader evaluations, which are necessary to fully substantiate its claims. Specifically, the absence of a comparison with Byte Pair Encoding (BPE), a widely used decomposition method, weakens the empirical rigor of the work.
Supporting Arguments:  
1. Novelty and Motivation: The LSD framework is innovative and addresses a pertinent challenge in sequence modeling. The idea of learning decompositions dynamically, rather than relying on static, predefined ones, is compelling and has potential applications beyond speech recognition.  
2. Empirical Results: The reported WER improvements are promising, indicating that LSD can outperform character-based baselines. The combination with a convolutional encoder further highlights the potential of the method.  
3. Lack of Baseline Comparisons: The omission of BPE as a baseline is a significant limitation. BPE is a standard in sequence modeling, and its performance across varying vocabulary sizes should have been evaluated to contextualize LSD's improvements. Without this, it is unclear whether LSD offers a meaningful advantage over existing techniques.  
4. Phonetic Interpretability: The paper does not clarify whether the learned decompositions correspond to phonetically meaningful units or are merely frequent character n-grams. This limits the interpretability and potential linguistic insights of the method.  
Additional Feedback:  
1. Comparison with BPE: Future iterations of the paper should include a comprehensive comparison with BPE across multiple vocabulary sizes. This would strengthen the empirical claims and provide a clearer picture of LSD's advantages.  
2. Phonetic Analysis: Analyzing whether the learned decompositions align with phonetic units could provide valuable insights and enhance the interpretability of the results.  
3. Broader Applications: While the focus on speech recognition is valid, exploring LSD's applicability to other tasks, such as machine translation or text generation, could significantly bolster the contribution.  
4. Ablation Studies: Additional experiments to analyze the impact of vocabulary size, token length, and other hyperparameters would provide a deeper understanding of the framework's behavior.  
Questions for the Authors:  
1. How does LSD compare to BPE in terms of WER and computational efficiency?  
2. Do the learned decompositions have any phonetic significance, or are they purely data-driven n-grams?  
3. Could the framework be extended to other domains, such as machine translation? If so, what modifications would be required?  
In summary, while the paper presents a novel and promising approach, the lack of critical baseline comparisons and broader evaluations limits its impact and warrants rejection in its current form.