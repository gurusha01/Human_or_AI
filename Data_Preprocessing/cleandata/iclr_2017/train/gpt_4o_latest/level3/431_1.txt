The paper introduces PALEO, an analytical performance model designed to estimate training and evaluation times for deep learning systems across various software, hardware, and communication strategies. By leveraging the declarative specifications of neural network architectures, PALEO maps these to a design space to predict execution times without requiring full implementation. The paper claims robustness across diverse architectures, hardware setups, and parallelization strategies, and demonstrates alignment with empirical results from existing literature. The authors also provide open-source code and a live demo, with plans for future updates to support customized networks and model splits.
Decision: Accept
The paper should be accepted due to its significant contribution to the field of scalable deep learning systems. The model is well-motivated, scientifically rigorous, and addresses a critical gap in the literature by providing a tool for performance estimation without exhaustive benchmarking. The alignment of results with existing literature and the open-source nature of the work further enhance its impact and reproducibility.
Supporting Arguments:
1. Clear and Comprehensive Contribution: The paper is well-written and clearly articulates the problem of estimating scalability in deep learning systems. It considers a wide range of variables, including hardware, software, bandwidth, and parallelization strategies, making the model broadly applicable.
2. Scientific Rigor and Validation: The results are consistent with empirical findings from prior work, demonstrating the reliability of PALEO. The case studies on NiN, Inception, and AlexNet validate the model's accuracy across diverse architectures and setups.
3. Open-Source and Practical Utility: The availability of open-source code and a live demo enhances the usability of the model for practitioners. The planned updates to support customized networks further increase its potential impact.
Suggestions for Improvement:
1. Testing on Newer Architectures: While the paper demonstrates robustness across several architectures, testing PALEO on more recent models like ResNet and DenseNet would strengthen its applicability and relevance.
2. Platform-Specific Efficiency: The paper introduces the Platform Percent of Peak (PPP) parameter to account for inefficiencies, but a deeper exploration of how PPP varies across different platforms and its impact on predictions would be valuable.
3. Scalability Beyond CNNs: While the paper briefly discusses applying PALEO to GANs, further exploration of its applicability to other architectures, such as transformers, would broaden its scope.
4. User-Friendliness of the Tool: The live demo is promising, but additional details on the interface and ease of use for non-expert users would be helpful.
Questions for the Authors:
1. How does PALEO handle hybrid parallelization strategies that combine data and model parallelism in more complex configurations?
2. Can the authors provide more details on the planned updates for supporting customized networks? Will this include an interface for users to input their own hardware and software specifications?
3. Have you considered incorporating energy consumption or cost efficiency into PALEO's predictions, given the growing focus on sustainable AI?
In conclusion, the paper makes a strong contribution to the field of scalable deep learning and provides a practical tool for researchers and practitioners. Addressing the suggested improvements would further enhance its impact and utility.