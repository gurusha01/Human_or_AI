The paper introduces a novel method for network morphism, enabling the transformation of a trained neural network into different architectures without retraining from scratch. By abstracting modules as directed acyclic graphs (DAGs) and formulating the morphing process as a graph transformation problem, the authors propose a systematic approach to morph convolutional layers into arbitrary modules. They define two atomic morphing operations and classify modules into simple morphable and complex modules, providing practical algorithms for both. Experimental results on ResNet architectures demonstrate performance improvements with minimal computational overhead, showcasing the potential of this approach for efficient model exploration and enhancement.
Decision: Reject
While the paper presents an interesting and well-motivated approach to network morphism, the results are unconvincing due to the use of outdated baselines and insufficient comparisons with state-of-the-art models. Additionally, the experimental setup lacks rigor in presenting fair comparisons, such as including the number of parameters in performance tables.
Supporting Arguments:
1. Strengths:
   - The paper is well-written and easy to follow, with clear explanations of the proposed method and its theoretical underpinnings.
   - The graph-based abstraction for network morphism is novel and provides a systematic framework for addressing the problem.
   - The proposed algorithms for morphing both simple and complex modules are well-structured and theoretically sound.
2. Weaknesses:
   - The experimental results rely heavily on comparisons with ResNet, which, while influential, is no longer representative of the current state-of-the-art. Comparisons with more recent architectures, such as Wide Residual Networks (WRNs) or EfficientNets, are necessary to validate the method's relevance.
   - The performance improvements reported are modest and may not justify the additional complexity introduced by the morphing process.
   - Tables lack critical information, such as the number of parameters for each model, which is essential for fair comparisons of computational efficiency and performance.
Suggestions for Improvement:
1. Experimental Comparisons: Include comparisons with state-of-the-art models like WRNs or EfficientNets to demonstrate the broader applicability and competitiveness of the proposed method.
2. Parameter Analysis: Add the number of parameters for each model in the performance tables to provide a clearer picture of the trade-offs between accuracy and computational cost.
3. Baseline Selection: Update the baselines to reflect more recent advancements in neural network architectures, ensuring that the results are relevant to the current landscape of deep learning.
4. Ablation Studies: Conduct more detailed ablation studies to isolate the contributions of different components of the proposed method, such as the impact of morphing specific modules or using different kernel sizes.
5. Clarity on Practicality: Provide more discussion on the practical implications of the proposed method, such as its scalability to very large networks or its utility in real-world applications.
Questions for the Authors:
1. How does the proposed method compare to state-of-the-art architectures like WRNs or EfficientNets in terms of both accuracy and computational efficiency?
2. Can the morphing process be applied to non-convolutional layers or other types of architectures, such as transformers?
3. What are the limitations of the proposed graph-based abstraction, and how might they affect its applicability to more complex network architectures?
In conclusion, while the paper presents an interesting and theoretically sound approach to network morphism, the lack of convincing experimental results and comparisons with state-of-the-art models limits its impact and relevance. Addressing these issues could significantly strengthen the work.