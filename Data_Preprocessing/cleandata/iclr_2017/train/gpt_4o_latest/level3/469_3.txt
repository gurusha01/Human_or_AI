Review of the Paper
Summary of Contributions
This paper addresses the critical problem of compressing convolutional neural networks (CNNs) to reduce memory usage and improve inference speed, particularly by focusing on convolutional layers, which dominate computational costs. The authors propose a novel sparse convolution design that supports arbitrary sparsity patterns, enabling both size reduction and significant speedups. They complement this with a performance model to identify optimal sparsity levels for different layers and architectures, and introduce a Guided Sparsity Learning (GSL) algorithm to integrate speedup awareness into the pruning process. The method is evaluated on AlexNet and GoogLeNet across various platforms, achieving impressive speedups (3.1–7.3×) without accuracy loss. The authors also provide an optimized implementation, making their work reproducible and accessible.
Decision: Accept
The paper is well-motivated, methodologically sound, and makes significant contributions to the field of CNN compression. The proposed method achieves both theoretical and practical advancements, addressing a key limitation in prior pruning approaches—limited inference speedups. The inclusion of a performance model and the GSL algorithm demonstrates a thoughtful co-design approach that is both novel and impactful. The results are rigorously validated across multiple platforms, and the availability of code adds to the paper's reproducibility and utility.
Supporting Arguments
1. Problem Significance: The paper tackles a well-known bottleneck in CNN pruning—achieving meaningful inference speedups alongside size reduction. This is a critical issue for deploying CNNs on resource-constrained devices.
2. Novelty and Technical Depth: The sparse convolution design supports arbitrary sparsity patterns, overcoming limitations of prior structured sparsity methods. The performance model and GSL algorithm further distinguish this work by enabling architecture-aware and layer-specific pruning.
3. Empirical Validation: The method is evaluated on widely used models (AlexNet, GoogLeNet) and diverse platforms, demonstrating its generalizability. The reported speedups (up to 7.3×) are substantial and align with the theoretical predictions of the performance model.
4. Clarity and Contextualization: The paper is well-written, with a clear explanation of the methodology and its placement within the broader literature. The authors effectively contrast their approach with prior work, highlighting its advantages.
Suggestions for Improvement
1. Summary of Gains: While the paper provides detailed sparsity levels and speedup results, a concise table summarizing the relative speedup and memory gains compared to prior methods would improve clarity and help readers quickly grasp the benefits.
2. Translation of Sparsity to Gains: The sparsity levels achieved are well-documented, but the translation of these levels into relative speedup and memory gains could be more explicitly discussed, particularly in comparison to structured sparsity methods.
3. Broader Applicability: While the focus on AlexNet and GoogLeNet is valuable, extending the evaluation to more recent architectures like ResNet or MobileNet would strengthen the paper's relevance to modern CNNs.
4. End-to-End Speedup: Although the paper focuses on layer-wise performance, reporting end-to-end inference speedups would provide a more holistic view of the method's practical impact.
Questions for the Authors
1. How does the proposed method perform on GPUs, which are commonly used for CNN inference? The current evaluation is limited to CPU platforms.
2. Can the performance model be extended to guide pruning for other FLOP-reduction techniques (e.g., tensor factorization, Winograd)?
3. How does the method handle scenarios where sparsity levels vary significantly across layers? Are there any challenges in maintaining accuracy in such cases?
In conclusion, this paper makes a strong contribution to the field of CNN compression and is recommended for acceptance, with minor improvements to enhance clarity and broaden applicability.