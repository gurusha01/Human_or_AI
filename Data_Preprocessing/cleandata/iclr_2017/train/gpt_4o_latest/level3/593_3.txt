The paper introduces a semi-supervised variant of the Variational Autoencoder (VAE) framework that incorporates observed variables into the recognition network, enabling the use of domain knowledge to disentangle latent representations. The authors propose a flexible stochastic computation graph framework that supports both continuous and discrete latent variables, with the potential for easier software automation and future extensions to probabilistic programming. The paper demonstrates the approach's utility through experiments on MNIST, SVHN, Yale B, and Multi-MNIST datasets, showing qualitative and quantitative improvements in disentangled representation learning and classification tasks.
Decision: Reject
While the paper presents a novel variant of semi-supervised VAE and demonstrates its utility in disentangling latent representations, it fails to provide sufficient justification for its complexity and lacks clarity in key comparisons with prior work. The main reasons for rejection are:
1. Unnecessary Complexity in Inference: The use of auxiliary variables in the recognition network introduces additional complexity without a clear demonstration of its necessity. The authors do not convincingly argue why this approach is superior to simpler extensions of VAE, such as those by Kingma et al. (2014).
   
2. Insufficient Comparison with Prior Work: The experimental results show deviations from Kingma et al. (2014), but the paper does not adequately explain the superiority of the proposed method. Furthermore, the comparison with Jampani et al. (2015) is unclear, particularly regarding supervision rates and experimental setups.
Supporting Arguments
1. Motivation and Placement in Literature: While the paper is well-motivated in its aim to incorporate domain knowledge for disentangled representation learning, it does not sufficiently differentiate itself from existing methods like Kingma et al. (2014) and Ranganath et al. (2015). The lack of a general approximation scheme for the variational posterior is a notable limitation compared to Ranganath et al. (2015).
2. Experimental Rigor: The experiments demonstrate the proposed approach's utility, particularly in Section 4.3, where the model exhibits a useful property of handling variable-dimensional latent spaces. However, the surprising deviations from prior work are not well-explained, and it is unclear whether the experimental setups (e.g., CNN usage) are consistent with those in Kingma et al. (2014).
3. Clarity and Presentation: The title and introduction create expectations of a broader contribution, but the paper focuses narrowly on a semi-supervised VAE variant. Additionally, the ambiguous citation of Kingma et al. (2014) detracts from clarity.
Suggestions for Improvement
1. Simplify the Approach: Reconsider the necessity of auxiliary variables and explore whether a simpler extension of VAE could achieve similar results.
2. Clarify Comparisons: Provide a detailed explanation of the experimental setup and ensure consistency with prior work. Clearly articulate why the proposed method outperforms existing approaches.
3. Improve Presentation: Revise the title and introduction to better reflect the paper's scope. Use distinct labels (e.g., Kingma et al. 2014a, 2014b) for ambiguous citations.
4. Future Directions: The planned extension to probabilistic programming is promising. Expanding on this direction and providing preliminary results could strengthen the paper's contribution.
Questions for the Authors
1. Why is the use of auxiliary variables in the recognition network necessary, and how does it compare to simpler alternatives?
2. Can you clarify whether the experimental setups (e.g., CNN usage) are consistent with those in Kingma et al. (2014) and Jampani et al. (2015)?
3. What accounts for the surprising deviations in experimental results compared to Kingma et al. (2014)?
In conclusion, while the paper introduces an interesting variant of semi-supervised VAE, its contributions are not sufficiently justified, and the comparisons with prior work lack clarity. Addressing these issues could make the paper more compelling for future submission.