The paper proposes two likelihood ratio-based approaches for boosting generative models—DiscBGM and GenBGM—and evaluates their effectiveness on synthetic data and the MNIST dataset for tasks such as density estimation, sample generation, and semi-supervised learning. The authors extend the concept of boosting to unsupervised settings by iteratively combining weak generative models to improve performance. They provide theoretical guarantees for progress in each boosting round and demonstrate the flexibility of their framework by incorporating both generative and discriminative models as intermediate learners. The paper claims that their approach improves upon baseline generative models without significant computational overhead.
Decision: Reject
The primary reasons for rejection are: (1) the experimental evaluation lacks rigor and clarity, and (2) the paper fails to convincingly demonstrate the superiority of the proposed methods over existing baselines. While the theoretical contributions are interesting, the empirical results are inconclusive and raise questions about the robustness of the approach.
Supporting Arguments:
1. Unclear Baseline Justification: The bagging baseline in Section 3.1 simply refits the model to the same dataset without reweighting, making it an ineffective comparison. The authors fail to explain why this is a meaningful baseline, especially given the availability of more sophisticated ensemble methods.
   
2. Sample Generation Concerns: The Markov chain used for generating samples in Section 3.2 converges slowly, making it unlikely that the samples are from the stationary distribution. This undermines the validity of the qualitative evaluation. The authors should consider using Annealed Importance Sampling (AIS) for a more robust assessment.
3. Arbitrary Hyperparameters: The choice of weights (alphas) for intermediate models appears arbitrary, and the impact of simpler choices like setting all alphas to 1 is not explored. This raises concerns about the reproducibility and generalizability of the results.
4. Inconclusive Semi-Supervised Results: The semi-supervised classification results do not show a clear advantage for the proposed methods, as the baseline RBM performs comparably to the boosted models. This weakens the claim that the proposed framework significantly improves generative modeling.
5. Writing and Originality: While the paper is mostly well-written and appears original, the lack of clarity in experimental design and interpretation of results detracts from its overall quality.
Suggestions for Improvement:
1. Strengthen Baseline Comparisons: Replace the bagging baseline with more meaningful alternatives, such as ensembles that use reweighting or other modern ensemble techniques.
   
2. Improve Sample Evaluation: Use AIS or other robust methods to evaluate the quality of generated samples and ensure they are representative of the stationary distribution.
3. Hyperparameter Sensitivity Analysis: Conduct experiments to analyze the sensitivity of the results to the choice of alphas and explore simpler configurations to validate the robustness of the approach.
4. Clarify Semi-Supervised Results: Provide a more detailed analysis of why the proposed methods fail to outperform the baseline RBM in semi-supervised classification. Consider testing on additional datasets to strengthen the empirical claims.
5. Address Computational Trade-offs: Discuss the computational cost of MCMC sampling in DiscBGM and propose potential optimizations.
Questions for the Authors:
1. Why was the bagging baseline chosen, and how does it provide meaningful insights into the performance of the proposed methods?
2. How do the results change when using AIS for sample evaluation instead of the current Markov chain approach?
3. What motivated the specific choice of alphas in the experiments, and how sensitive are the results to these choices?
4. Can the authors provide more evidence or theoretical justification for why the proposed methods should outperform baseline models in semi-supervised learning?
In conclusion, while the paper presents an interesting theoretical framework for boosting generative models, the empirical evaluation is insufficient to support its claims. Addressing the above concerns could significantly strengthen the paper.