Review of "L-SR1: A New Second-Order Method for Training Deep Neural Networks"
Summary of Contributions
The paper introduces L-SR1, a second-order optimization method for training deep neural networks. The method combines the Symmetric Rank-One (SR1) algorithm with limited-memory representations to address two critical challenges in second-order optimization: poor Hessian conditioning and saddle point issues. The authors propose a trust-region framework to replace line search and hypothesize that batch normalization improves the Hessian's conditioning. Experimental results on MNIST and CIFAR10 datasets suggest that L-SR1 performs comparably to or slightly better than first-order methods like Nesterov's Accelerated Gradient Descent (NAG) and significantly outperforms L-BFGS, another second-order method. The authors also explore the sensitivity of L-SR1 to hyperparameters and its potential for distributed training due to its insensitivity to large mini-batch sizes.
Decision: Reject
The paper is not ready for publication due to unconvincing experimental results, insufficient theoretical grounding, and lack of practical evidence supporting its claims.
Supporting Arguments
1. Theoretical Motivation: While the paper highlights the potential advantages of SR1 updates over BFGS, it lacks rigorous theoretical arguments or intuition to support its claims. Prior works, such as Dauphin et al. (2014) and Martens (2010), provide stronger theoretical frameworks for addressing saddle points and Hessian conditioning. The paper does not adequately compare or position L-SR1 against these established methods.
2. Experimental Results: The experimental evaluation is weak and fails to convincingly demonstrate the superiority of L-SR1:
   - The reported improvements over first-order methods like NAG are marginal and inconsistent. For example, while L-SR1 shows competitive performance on LeNet-like networks, it makes slower progress over time on deeper architectures like residual networks.
   - The lack of wall-clock time comparisons is a significant omission, especially for a second-order method, where computational overhead is a critical concern.
   - The claim that L-SR1 is suitable for distributed training is not substantiated with experiments on distributed systems or large-scale datasets.
3. Practicality: The method's practicality is questionable due to its computational complexity and skipped updates. While the authors argue that skipped updates do not affect performance, the high skip rates observed in some cases (e.g., MNIST with batch normalization) warrant further investigation. Additionally, the method's sensitivity to hyperparameters like trust-region radius and mini-batch size is not thoroughly explored.
Suggestions for Improvement
1. Theoretical Rigor: Provide stronger theoretical arguments or proofs to justify the advantages of SR1 updates in addressing saddle points and poor Hessian conditioning. A comparison with prior works like Dauphin et al. (2014) and Martens (2010) would strengthen the paper's positioning.
2. Experimental Design: 
   - Include wall-clock time comparisons to demonstrate the computational efficiency of L-SR1 relative to first-order methods.
   - Test the method on larger-scale datasets (e.g., ImageNet) and distributed training setups to validate its scalability and potential for parallelism.
   - Provide more detailed analyses of skipped updates and their impact on convergence.
3. Practical Insights: Offer more practical insights into hyperparameter tuning and robustness. For example, explain why L-SR1 performs poorly on residual networks compared to LeNet-like architectures and explore strategies to mitigate this.
Questions for the Authors
1. How does L-SR1 compare to other second-order methods like Hessian-Free Optimization (Martens, 2010) in terms of computational cost and convergence behavior?
2. Can you provide wall-clock time comparisons to demonstrate the efficiency of L-SR1 relative to first-order methods?
3. What is the intuition behind the high skip rates observed in some cases (e.g., MNIST with batch normalization), and how do you plan to address this issue?
4. Have you tested L-SR1 on larger-scale datasets or distributed systems to substantiate its claim of suitability for distributed training?
In conclusion, while L-SR1 is an interesting contribution, the paper requires significant improvements in theoretical grounding, experimental validation, and practical insights to be considered for publication.