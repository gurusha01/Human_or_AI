The paper introduces RaSoR, a novel method for efficiently representing and scoring all possible spans in extractive question answering (QA) tasks. By leveraging recurrent span representations, RaSoR avoids the computational inefficiencies of naive approaches while enabling globally normalized training and exact decoding. The paper demonstrates that explicitly modeling answer spans improves performance over methods that rely on independent predictions for start and end markers. While the results on the SQuAD test set are not state-of-the-art, the proposed method achieves a 5% improvement over prior baselines and reduces error by more than 50% compared to earlier benchmarks. The authors also provide detailed ablation studies and insightful analyses, particularly in Figure 2, which highlights the benefits and limitations of the approach.
Decision: Accept
The paper is recommended for acceptance due to its potential to positively impact SQuAD-related research and extractive QA tasks more broadly. The key reasons for this decision are:
1. Novel Contribution: RaSoR introduces an efficient and elegant approach to span representation that addresses limitations in existing methods, such as greedy decoding and independence assumptions.
2. Broader Applicability: While the results are not state-of-the-art, the method is modular and could enhance other architectures in the extractive QA domain.
Supporting Arguments:
1. Well-Motivated Approach: The paper is well-grounded in the literature, clearly identifying gaps in prior work (e.g., greedy decoding in Match-LSTM) and motivating the need for explicit span representations. The use of recurrent networks for shared substructure computation is a thoughtful design choice.
2. Rigorous Evaluation: The experiments are thorough, with comparisons to multiple baselines, ablation studies, and cross-validation to ensure robustness. The analyses, such as the impact of question representations and the exploration of failure cases, provide valuable insights.
3. Clarity and Writing: The paper is well-written, with clear explanations of the model architecture, training procedure, and evaluation metrics. Figures and tables are used effectively to support the claims.
Suggestions for Improvement:
1. State-of-the-Art Comparison: While the method shows promise, the lack of state-of-the-art results on SQuAD may limit its immediate impact. The authors could explore ensembling RaSoR with other competitive models to boost performance.
2. Test Set Results: The absence of test set results due to copyright restrictions is a limitation. The authors should prioritize resolving this issue to strengthen the paper's empirical contributions.
3. Generalization: The paper focuses exclusively on SQuAD. It would be helpful to test RaSoR on other extractive QA datasets (e.g., NewsQA or Natural Questions) to demonstrate its generalizability.
Questions for the Authors:
1. How does RaSoR perform on longer passages or datasets with more complex linguistic structures? Would the quadratic complexity of span enumeration become a bottleneck?
2. Could the proposed method be adapted for abstractive QA tasks, or is it strictly limited to extractive QA?
3. How sensitive is the model to hyperparameter choices, particularly the dimensionality of LSTM hidden states and the number of layers?
In conclusion, RaSoR is a promising contribution to extractive QA research. While there is room for improvement, the method's potential to enhance other architectures and its thoughtful design justify its acceptance.