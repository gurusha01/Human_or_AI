Review of the Paper
Summary of Contributions
This paper proposes a novel framework for nonparametrically learning activation functions in deep neural networks using Fourier basis expansion. Unlike traditional approaches that treat activation functions as fixed hyperparameters, the proposed method allows activation functions to be learned during training, thereby expanding the class of functions that each node can represent. The authors provide theoretical guarantees on generalization using algorithmic stability and demonstrate empirical improvements of up to 15% in test performance on benchmark datasets such as MNIST and CIFAR-10. The paper also introduces a two-stage training process to stabilize training for networks with convolutional layers, which further enhances performance.
Decision: Reject
While the paper presents an interesting idea, the novelty and practical impact of the proposed approach are not sufficiently convincing. The method appears to be conceptually equivalent to increasing network capacity by adding more layers or nodes with fixed activation functions, raising concerns about whether the proposed approach offers unique advantages. Additionally, the empirical results, though promising, lack sufficient ablation studies to isolate the contribution of learned activation functions from other factors. The theoretical analysis, while rigorous, could benefit from clearer exposition and more intuitive connections to the practical results.
Supporting Arguments
1. Novelty Concerns: The approach of learning activation functions via Fourier basis expansion may be seen as equivalent to constructing a larger network with constrained weights and fixed non-linearities. The paper does not sufficiently address how this approach fundamentally differs from or improves upon existing methods of increasing network capacity.
2. Practical Advantages: The paper does not convincingly demonstrate that the constraints implied by the learned non-linearities offer specific advantages over optimizing network capacity with fixed non-linearities. For example, it is unclear whether the method provides better interpretability, computational efficiency, or robustness.
3. Clarity and Completeness: The initialization of the NPFC(L,T) activation functions in the two-stage training process is not clearly explained. While the authors mention initializing them to approximate tanh, it is unclear how this initialization affects the results compared to random coefficients or other initialization strategies.
4. Empirical Validation: The results on MNIST and CIFAR-10 are promising but do not convincingly establish the superiority of the proposed method. Additional experiments, such as comparisons with other methods for learning activation functions or ablation studies to isolate the impact of Fourier basis expansion, would strengthen the empirical claims.
Suggestions for Improvement
1. Clarify Novelty: Explicitly address how the proposed method differs from increasing network capacity with fixed activation functions and why it is preferable. Highlight any unique advantages, such as interpretability or efficiency.
2. Theoretical-Experimental Connection: Strengthen the connection between the theoretical analysis and the empirical results. For example, discuss how the algorithmic stability guarantees translate to practical improvements in generalization.
3. Empirical Studies: Include ablation studies to isolate the impact of learned activation functions. Compare the proposed method with other approaches for learning activation functions, such as piecewise linear functions or parametric activation functions.
4. Minor Corrections: Address the following:
   - Correct the interval on Pg. 2 from "[-L+T, L+T]" to "[-L+T, L-T]."
   - Verify the equation for f(x) on Pg. 2 regarding the inclusion of "x" in the sin and cos terms.
   - Remove the word "algorithm" from "algorithm \eps-uniformly stable" in Theorem 4.2.
   - Define "SGM" in Theorem 4.5, as it is currently undefined.
Questions for the Authors
1. How does the proposed method compare to simply increasing the number of layers or nodes in the network with fixed activation functions?
2. What specific advantages do the constraints implied by the learned non-linearities provide over traditional methods?
3. How does the initialization of NPFC(L,T) activation functions affect the results? Have you compared different initialization strategies?
In conclusion, while the paper introduces an interesting idea, it requires further refinement and validation to establish its novelty and practical impact.