Review of the Paper
Summary of Contributions
This paper provides a theoretical analysis of the convergence behavior of Stochastic Gradient Descent (SGD) and its asynchronous variant (ASGD) in the context of mini-batch sizes and distributed learners. The authors justify that smaller mini-batches and fewer learners lead to faster convergence for non-convex objective functions, both theoretically and empirically. They highlight inefficiencies in speed-up when parallelizing gradient descent using hardware, demonstrating that increasing mini-batch size or the number of learners introduces inherent inefficiencies in convergence. The work effectively bridges algorithmic design and hardware properties, offering insights into the trade-offs of parallelism in optimization. The experimental results on CIFAR-10 support the theoretical findings.
Decision: Accept with Minor Revisions
Key reasons for this decision:
1. Novelty and Relevance: The paper addresses an important and underexplored issue in distributed optimization, providing both theoretical and empirical evidence for its claims.
2. Clarity and Contribution: The work is well-motivated, connects algorithmic properties with hardware constraints, and offers practical insights for the machine learning community.
Supporting Arguments
1. Theoretical Justification: The paper extends prior analyses of SGD and ASGD to show that smaller mini-batches and fewer learners yield better convergence guarantees before reaching the asymptotic regime. This is a significant contribution, as it explains observed inefficiencies in distributed training.
2. Empirical Validation: The experiments on CIFAR-10 convincingly demonstrate the slower convergence of larger mini-batches and more learners, aligning with the theoretical predictions.
3. Connection to Hardware: By linking convergence behavior to hardware parallelism, the paper provides actionable insights for practitioners designing distributed systems.
Additional Feedback for Improvement
1. Lemma 1: There is a potential issue with the correctness of Lemma 1, specifically the factor \(Df / S\), which might need to be \(Df / (S \cdot M)\). This discrepancy could affect the subsequent theorems. The authors should carefully verify and clarify this.
2. Clarity of Proofs: While the proofs are detailed, they can be streamlined for better readability. For instance, the derivation of Theorem 2 involves several approximations that could be more explicitly justified.
3. Experimental Setup: The experiments are well-executed, but additional datasets or tasks could strengthen the generalizability of the results. For example, testing on larger-scale datasets or different neural network architectures would provide more robust evidence.
4. Practical Implications: While the paper discusses inefficiencies in parallelism, it would benefit from a more detailed discussion of potential mitigation strategies, such as adaptive mini-batch sizing or hybrid approaches.
Questions for the Authors
1. Could you clarify the potential issue in Lemma 1 regarding the factor \(Df / S\) versus \(Df / (S \cdot M)\)? How does this affect the subsequent theorems?
2. How sensitive are the theoretical results to the assumptions (e.g., Lipschitzian gradient, bounded variance)? Are there practical scenarios where these assumptions might not hold?
3. Have you considered the impact of communication overhead in distributed ASGD? While the paper focuses on inherent inefficiencies, adding this perspective could make the analysis more comprehensive.
4. Could you provide more details on the choice of learning rate and its impact on the observed convergence behavior in the experiments?
In conclusion, this paper makes a strong contribution to understanding the trade-offs in distributed optimization and is well-suited for acceptance after addressing the minor concerns raised.