The paper proposes Generative Adversarial Parallelization (GAP), an extension of GANs that trains multiple generators and discriminators in parallel, periodically shuffling their pairings. The authors claim that this reduces overfitting between paired networks, improves convergence, and enhances mode coverage. GAP is described as a regularization technique and is evaluated using synthetic datasets, MNIST, CIFAR-10, and LSUN. The paper introduces GAM-II, a modified metric for evaluating GANs, and uses Annealed Importance Sampling (AIS) and Inception Score for additional evaluation.
Decision: Reject
The primary reasons for rejection are: (1) the paper does not provide sufficient evidence to conclusively support its claims of improved convergence and mode coverage, and (2) the proposed GAM-II metric is flawed due to its dependence on the choice of baselines, making the results less generalizable. While the approach is simple and replicable, the lack of clarity in the writing and incomplete sections further detract from the paper's quality.
Supporting Arguments:
1. Insufficient Evidence for Claims: The paper claims that GAP improves convergence and mode coverage, but the evidence is inconclusive. While qualitative results on synthetic datasets and t-SNE visualizations suggest some improvement, these are not robust enough to draw strong conclusions. The quantitative results using GAM-II and AIS-based likelihood estimation show mixed outcomes, with no significant advantage for GAP in some cases (e.g., GAPC4). The authors also acknowledge the difficulty of validating mode coverage in high-dimensional spaces, leaving a critical claim underexplored.
2. Flawed Metric (GAM-II): The proposed GAM-II metric is circular, as its evaluation depends on the discriminators trained under GAP, which biases the results. This undermines the objectivity of the comparisons. Additionally, the metric lacks sufficient detail in Appendix A.1, making it difficult to assess its validity.
3. Writing and Presentation Issues: The paper is difficult to follow due to unclear explanations and incomplete sections. For example, Section 2 contains a "to-do" remark, and Appendix A.1 lacks sufficient detail on GAM-II. These issues hinder the reader's ability to fully understand and evaluate the proposed method.
Suggestions for Improvement:
1. Stronger Empirical Validation: Conduct a systematic grid search comparing GANs and GAP-GANs across a broader range of datasets and hyperparameters. Provide more robust evidence for claims of improved convergence and mode coverage, particularly in high-dimensional spaces.
2. Metric Refinement: Address the circularity of GAM-II by designing a metric that does not depend on the choice of baselines. Provide a more detailed explanation of the metric in the paper and its appendix.
3. Clarity and Completeness: Revise the paper for clarity, ensuring that all sections are complete and well-explained. For example, resolve the "to-do" remark in Section 2 and expand Appendix A.1 to fully describe GAM-II.
4. Additional Evaluation Methods: Incorporate more widely accepted evaluation metrics, such as Fr√©chet Inception Distance (FID), to complement GAM-II and AIS-based likelihood estimation. This would provide a more comprehensive assessment of GAP's performance.
Questions for the Authors:
1. How does GAP perform when applied to more recent GAN architectures, such as StyleGAN or BigGAN? Can the benefits of GAP generalize to these models?
2. How sensitive are the results to the choice of swapping frequency (K) and the number of GANs (N)? Could these hyperparameters be optimized for better performance?
3. Can you provide more quantitative evidence for improved mode coverage, especially in high-dimensional datasets? For example, how does GAP compare to baseline GANs in terms of FID or other mode-dropping metrics?
In summary, while GAP is an interesting and simple idea, the lack of conclusive evidence, the flawed evaluation metric, and the paper's presentation issues prevent it from meeting the standards for acceptance at this time.