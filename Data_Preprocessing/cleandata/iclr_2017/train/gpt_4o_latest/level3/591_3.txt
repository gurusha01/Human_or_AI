Review of the Paper
Summary of Contributions
The paper introduces the concept of "sample importance" (SI) as a metric to measure the influence of individual training examples on the training of deep neural networks. The authors define SI based on the squared L2 norm of gradients and propose summing these norms across training iterations to determine "overall importance." They empirically analyze how easy and hard samples affect different layers and training stages, finding that easy samples shape output layers early in training, while hard samples influence input layers later. Additionally, the paper challenges curriculum learning paradigms, showing that mixing easy and hard samples in batches improves training performance. The authors provide extensive experiments on MNIST and CIFAR-10 datasets to support their claims.
Decision: Reject
The primary reasons for rejection are (1) the questionable validity and theoretical grounding of the proposed SI metric, and (2) the lack of convincing experimental evidence to support the claims. While the paper is ambitious and includes extensive experiments, the proposed metric appears to have significant limitations, and the results are inconsistent with established findings in the literature.
Supporting Arguments
1. Validity of the SI Metric: The proposed metric, based on gradient norms, is highly sensitive to factors such as learning rate and gradient magnitude changes during training. This raises concerns about its robustness and generalizability. The metric also disproportionately emphasizes the input layer due to its larger parameter count, which undermines its utility across architectures. The reviewer suggests that alternative metrics, such as the "input Fisher" norm, could provide a more meaningful measure of sample influence.
2. Experimental Flaws: The experimental results are inconsistent and raise doubts about the generalizability of the findings. For example, the claim that hard samples predominantly shape input layers is debatable for MNIST and inconsistent with CIFAR-10. Additionally, the results contradict established curriculum learning findings, but the paper does not provide sufficient theoretical or empirical justification for this divergence.
3. Practical Utility: The proposed SI metric does not lead to improvements in curriculum learning or other practical applications, limiting its impact. The experiments on batch construction, while interesting, fail to demonstrate significant advantages over existing methods.
4. Heuristic Nature: The SI metric is presented as a heuristic with limited theoretical justification. Its relationship to the final trained model is unclear, and the authors acknowledge that the metric does not accurately measure a sample's influence on the final model parameters.
Suggestions for Improvement
1. Theoretical Justification: Provide a stronger theoretical foundation for the SI metric. Address its sensitivity to learning rate and gradient magnitude changes, and compare it to alternative metrics like the input Fisher norm.
2. Experimental Rigor: Include experiments on more diverse architectures (e.g., CNNs, RNNs) and datasets to demonstrate the generalizability of the findings. Clarify inconsistencies in the results, particularly the differences between MNIST and CIFAR-10.
3. Practical Applications: Explore ways to leverage the SI metric for practical tasks, such as curriculum learning, subset selection, or model interpretability. Demonstrating tangible benefits would significantly strengthen the paper's impact.
4. Address SI Dominance in Input Layers: Investigate methods to normalize or adjust the SI metric to account for the disproportionate influence of input layers due to parameter count.
Questions for the Authors
1. How does the proposed SI metric compare to alternative metrics, such as the input Fisher norm, in terms of robustness and interpretability?
2. Can the authors provide a theoretical explanation for the observed differences in learning dynamics between MNIST and CIFAR-10 datasets?
3. How does the sensitivity of the SI metric to learning rate and gradient magnitude changes affect its reliability across different training setups?
4. Have the authors considered normalizing the SI metric to mitigate the dominance of input layers? If so, what were the results?
In conclusion, while the paper introduces an interesting concept and provides extensive experiments, the limitations of the SI metric and the lack of convincing evidence for its utility ultimately undermine its contributions. Addressing these concerns in future work could significantly strengthen the paper.