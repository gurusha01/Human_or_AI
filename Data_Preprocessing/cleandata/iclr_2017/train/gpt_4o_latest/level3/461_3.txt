The paper introduces a novel semi-supervised learning technique called "self-ensembling," which leverages consensus predictions from previous epochs combined with supervised loss to improve classification accuracy in scenarios with limited labeled data. The authors propose two implementations: the Π-model and temporal ensembling. While the Π-model is computationally expensive, temporal ensembling offers a more efficient alternative by aggregating predictions over multiple epochs, albeit at the cost of increased memory usage and an additional hyperparameter. The paper demonstrates state-of-the-art performance on standard benchmarks like CIFAR-10 and SVHN, and it claims robustness to noisy labels and scalability to large datasets.
Decision: Reject
While the paper presents an interesting and well-motivated approach, there are significant concerns regarding the validity of some claims and the feasibility of the method for large-scale applications. The skepticism surrounding the reported tolerance to noisy labels (e.g., 78% accuracy with 90% random labels) undermines confidence in the results. Additionally, the memory-intensive nature of temporal ensembling limits its practicality for datasets like ImageNet, which is not adequately addressed in the paper.
Supporting Arguments:
1. Strengths:
   - The paper builds on established concepts like "dark knowledge" and ladder networks, situating itself well within the literature.
   - The proposed methods achieve impressive results on semi-supervised benchmarks, significantly outperforming prior work.
   - The temporal ensembling method is computationally efficient compared to the Π-model, making it a promising direction for further exploration.
2. Weaknesses:
   - The claim of robustness to noisy labels is not convincing. Achieving 78% accuracy with 90% random labels is extraordinary, yet the paper does not provide sufficient evidence or analysis to substantiate this result.
   - Temporal ensembling's memory requirements and scalability issues for large datasets are inadequately addressed. The paper does not propose practical solutions for handling datasets like ImageNet.
   - The exclusion of additional SVHN examples is poorly justified, raising questions about the generalizability of the results.
Suggestions for Improvement:
1. Experimental Validation: Provide more rigorous experiments to validate the claim of robustness to noisy labels. Include ablation studies or additional analysis to explain how the method achieves such high accuracy under extreme noise.
2. Scalability: Address the memory and infrastructure challenges of temporal ensembling for large-scale datasets. Consider proposing optimizations or alternative approaches to mitigate these limitations.
3. Presentation: Highlight the best-in-category results in bold for easier comparison. Include state-of-the-art fully supervised results in the tables to contextualize the improvements achieved by the proposed methods.
4. Ramp-up Discussion: Elaborate on the choice of the ramp-up function \( w(t) \) and its impact on training stability and performance.
5. SVHN Justification: Provide a stronger rationale for excluding additional SVHN examples or include experiments with these examples to demonstrate the robustness of the method.
Questions for Authors:
1. Can you provide additional evidence or analysis to support the claim of 78% accuracy with 90% random labels? How does the method handle such extreme noise?
2. How do you propose to address the memory constraints of temporal ensembling for large-scale datasets like ImageNet?
3. Why were additional SVHN examples excluded, and how might their inclusion affect the reported results?
In summary, while the paper presents a promising approach with strong empirical results, the concerns regarding noisy label tolerance, scalability, and experimental justification prevent its acceptance in its current form. Addressing these issues could significantly strengthen the contribution.