The paper introduces a novel approach to enhance feature diversity in convolutional neural networks (CNNs) by proposing a Group Orthogonal Convolutional Neural Network (GoCNN). The key contribution is the introduction of a "group-wise model diversity" loss term, which explicitly enforces orthogonality between feature groups (foreground and background) during training. By leveraging segmentation masks as privileged information, the method aims to improve the generalization ability of CNNs for image classification. Experiments on ImageNet and PASCAL VOC datasets demonstrate the potential of GoCNN, particularly in scenarios with partial privileged information.
Decision: Accept
Key Reasons for Acceptance:
1. Novelty and Motivation: The paper introduces a novel loss term and a principled way to use segmentation masks as privileged information, which is a unique contribution to the literature on CNN training. The idea of explicitly enforcing feature diversity within a single model is well-motivated and addresses a gap in existing methods.
2. Empirical Validation: The method is rigorously evaluated on two large-scale datasets, showing consistent improvements over baseline models. The results on partial privileged information are particularly compelling, demonstrating the method's robustness and practical applicability.
Supporting Arguments:
- The use of segmentation masks as privileged information is innovative and provides a new perspective on how auxiliary annotations can be utilized in CNN training.
- The experimental results on both ImageNet and PASCAL VOC datasets validate the effectiveness of GoCNN, with improvements in classification accuracy and generalization ability.
- The visualization of feature maps and the analysis of foreground and background contributions provide additional evidence for the method's interpretability and effectiveness.
Additional Feedback for Improvement:
1. Baseline Comparison: The lack of a baseline without the background suppression term makes it difficult to isolate the contribution of each component (foreground and background) to the overall performance. Including such a baseline would strengthen the evaluation.
2. Incomplete Privileged Information: While the paper evaluates partial privileged information, results on the full ImageNet dataset with partial annotations (e.g., 10%) could further demonstrate scalability and robustness.
3. Clarity and Presentation: Section 4.2 is difficult to follow due to dense technical details. Simplifying the explanation or providing a diagram summarizing the architecture and training flow would improve readability.
4. Terminology and Notation: Referring to Equation 3 as a "regression loss" is confusing, as it functions more as a suppression term. Additionally, unconventional notation in some equations adds unnecessary complexity.
5. Figure and Table Issues: Figure 1 has swapped FG and BG suppression labels, and the "0" and "1" labels remain inconsistent. Results in Table 4 with 100% privileged information differ from Tables 1-2, raising concerns about consistency. These discrepancies should be clarified.
Questions for Authors:
1. How does the performance of GoCNN compare to a baseline that uses only the foreground information without background suppression? This would help isolate the contribution of the background features.
2. Can the authors provide more details on the discrepancy between Table 4 and Tables 1-2? Are there differences in experimental settings or evaluation protocols?
3. How does the method scale to other tasks, such as object detection or segmentation, where privileged information might also be available?
In conclusion, while there are some presentation and evaluation limitations, the paper makes a significant contribution to the field and is worthy of acceptance. Addressing the above feedback would further enhance its impact and clarity.