Review
The paper proposes a Context-aware Attention Network (CAN) for Interactive Question Answering (IQA), which incorporates a two-level attention mechanism (word-level and sentence-level) and an interactive mechanism to handle incomplete information by generating supplementary questions. The authors claim that CAN achieves significant improvements over existing QA models on both traditional QA datasets (bAbI) and their newly introduced IQA dataset (ibAbI). The model is designed to adapt dynamically to user feedback without requiring additional training, making it self-adaptive and interactive.
Decision: Reject
The decision to reject this paper is based on two primary reasons: (1) the lack of novelty in the proposed QA model, which closely resembles existing encoder-decoder frameworks with attention mechanisms, and (2) the impracticality of the interactive mechanism, which relies on predefined questions and human interaction during training, undermining its claim of true interactivity.
Supporting Arguments
1. Lack of Novelty: While the paper introduces a two-level attention mechanism, this approach is not sufficiently distinct from existing attention-based QA models like MemN2N and DMN+. The use of GRUs for encoding and decoding, as well as attention mechanisms for context modeling, is well-established in the literature. The paper does not convincingly demonstrate how its attention mechanism or interactive framework fundamentally advances the state of the art.
2. Impractical Interactive Mechanism: The interactive aspect of the model is a key contribution claimed by the authors. However, the reliance on predefined supplementary questions during training limits the model's adaptability in real-world scenarios, where user queries are often unpredictable. Furthermore, the interactive mechanism does not exhibit true interactivity, as it merely updates sentence-level attention weights based on user feedback without dynamically learning from new information.
3. Claims vs. Evidence: While the paper reports strong empirical results on both QA and IQA datasets, the experimental setup raises concerns. The ibAbI dataset is artificially constructed and may not reflect the complexity of real-world IQA tasks. Additionally, the paper does not provide sufficient qualitative analysis of failure cases or scenarios where the model struggles, which would help assess its robustness.
Additional Feedback
1. Dataset Limitations: The ibAbI dataset appears to be a synthetic extension of bAbI, which may not capture the diversity and ambiguity of real-world IQA tasks. The authors should consider evaluating their model on more challenging and realistic datasets to validate its effectiveness.
2. Interactive Mechanism: The authors should explore ways to make the interactive mechanism more dynamic and less reliant on predefined questions. For example, incorporating reinforcement learning or unsupervised feedback mechanisms could improve adaptability.
3. Clarity and Focus: The paper is dense and overly detailed in some sections (e.g., GRU equations), which detracts from the main contributions. A more concise presentation of the model and its key innovations would improve readability.
Questions for the Authors
1. How does the proposed model handle scenarios where user feedback is noisy or ambiguous? Does the model degrade gracefully in such cases?
2. Can the interactive mechanism be extended to handle multi-turn interactions, where multiple rounds of feedback are required to resolve ambiguity?
3. How does the model perform on real-world IQA datasets or tasks beyond the synthetic ibAbI dataset? Are there plans to evaluate it on more diverse and realistic benchmarks?
In conclusion, while the paper presents an interesting attempt to incorporate interactivity into QA systems, the lack of novelty and practical limitations of the proposed approach prevent it from making a significant contribution to the field. Addressing these concerns and evaluating the model on more realistic datasets could strengthen its impact in future iterations.