Review of "DeepRebirth: Accelerating Deep Neural Networks on Mobile Devices"
Summary of Contributions
The paper proposes DeepRebirth, a novel acceleration framework for deep neural networks (DNNs) on mobile devices. The approach focuses on merging non-tensor layers (e.g., pooling, normalization) with adjacent tensor layers (e.g., convolution) to form a single dense "rebirth" layer, followed by retraining to preserve functionality. This technique is applied through two mechanisms: streamline merging and branch merging, targeting both sequential and parallel structures in DNNs. The method achieves 3x-5x speed-up and 2.5x runtime memory savings on mobile devices like the Samsung Galaxy S6, with only a 0.4% accuracy drop on GoogLeNet for ImageNet classification. Additionally, the framework is compatible with other compression methods (e.g., Tucker decomposition) and demonstrates generalizability across architectures like AlexNet and ResNet.
Decision: Reject
While the paper presents a promising approach to accelerate DNNs on mobile devices, it falls short in several critical areas, including insufficient comparison with prior work, limited applicability, and lack of novelty in the core methodology.
Supporting Arguments
1. Insufficient Comparison with Related Work:  
   The paper does not adequately compare its approach with other established techniques, such as the teacher-student model compression paradigm or methods like SqueezeNet and XNOR-Net. While some comparisons are made (e.g., with SqueezeNet), they are limited and do not provide a comprehensive evaluation of trade-offs in accuracy, speed, and memory usage. This omission weakens the paper's claim of achieving state-of-the-art results.
2. Limited Applicability:  
   The proposed method is tailored to specific architectures (e.g., GoogLeNet, ResNet) and relies on merging non-tensor layers, which may not be present in all modern DNNs. For example, many recent architectures already optimize or omit non-tensor layers (e.g., LRN layers). This limits the generalizability of the approach across a broader range of models.
3. Lack of Novelty:  
   The idea of merging layers (e.g., absorbing batch normalization into convolution layers) is not new and has been explored in prior work. The paper does not sufficiently differentiate its contribution from these existing techniques. Additionally, the use of low-rank approximation techniques, as cited from Xue et al. (2013), further diminishes the novelty.
Suggestions for Improvement
1. Broader Comparisons:  
   Include comparisons with state-of-the-art compression and acceleration methods, such as teacher-student models, quantization, and pruning. Highlight scenarios where DeepRebirth outperforms these methods or explain its unique advantages.
2. Generalization:  
   Demonstrate the applicability of DeepRebirth to a wider range of architectures, including those without non-tensor layers. Alternatively, clarify the scope and limitations of the method in the context of modern DNN designs.
3. Novelty and Positioning:  
   Clearly articulate how DeepRebirth advances the state of the art beyond existing layer-merging techniques. For example, emphasize any unique insights or innovations in the retraining process or the merging strategy.
4. Ablation Studies:  
   Provide detailed ablation studies to isolate the contributions of streamline merging, branch merging, and retraining. This would help clarify the effectiveness of each component.
Questions for the Authors
1. How does DeepRebirth compare to teacher-student compression frameworks in terms of speed, accuracy, and memory usage?
2. Can the proposed method handle architectures without non-tensor layers, such as those using global average pooling instead of fully connected layers?
3. What is the computational overhead of the retraining process, and how does it scale with model size and complexity?
4. How does the method perform on edge cases, such as very small or very large DNNs?
In conclusion, while the paper addresses an important problem and shows promising results, it requires more rigorous evaluation, broader applicability, and clearer differentiation from prior work to make a compelling case for acceptance.