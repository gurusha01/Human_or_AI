Review
Summary of Contributions
The paper presents a novel approach to deep learning by implementing a matrix library (Sushi2) and a deep learning framework (Sukiyaki2) in JavaScript, enabling deep learning tasks to be performed directly in web browsers. The authors leverage WebCL for GPGPU acceleration, allowing the training of large-scale convolutional neural networks (CNNs) such as VGGNet and ResNet without the need for dedicated computer systems or software installations. The work also explores distributed training using web browsers as computation clients, demonstrating the feasibility of collaborative computation on ordinary personal computers and smartphones. The source code is made available on GitHub, and live demos are accessible, showcasing the practical utility of the framework.
Decision: Accept  
Key Reasons:  
1. Technical Validity and Novelty: The work is technically sound and addresses a significant gap in the deep learning ecosystem by enabling browser-based training of large-scale CNNs. This is a novel contribution with practical implications for democratizing access to deep learning.  
2. Empirical Rigor: The experiments convincingly demonstrate the feasibility of training large-scale CNNs using the proposed framework. While the performance is not on par with CUDA-based frameworks like Caffe, the results are scientifically rigorous and highlight the potential for further optimization.
Supporting Arguments
1. Problem Relevance: The paper tackles the high cost and complexity of deploying deep learning frameworks on specialized hardware, a critical barrier for many users. By leveraging JavaScript and web browsers, the proposed solution offers a cost-effective and accessible alternative.  
2. Empirical Validation: The authors provide detailed experiments comparing their framework with existing JavaScript-based libraries and CUDA-based frameworks. The results demonstrate significant performance improvements over existing JavaScript libraries and reasonable performance relative to CUDA-based systems. Distributed training experiments further validate the scalability of the approach.  
3. Open-Source Contribution: The availability of the source code and live demos enhances the reproducibility and practical impact of the work.
Suggestions for Improvement
1. Performance Optimization: While the framework achieves reasonable performance, it lags behind CUDA-based systems due to slower matrix multiplication. The authors should explore advanced optimization techniques, such as those proposed by Lavin (2015), to improve convolution performance.  
2. WebCL Limitations: The reliance on WebCL, which is not natively supported in most web browsers, limits the framework's accessibility. Exploring alternatives like WebGPU or WebGL could broaden the framework's usability.  
3. Communication Overhead in Distributed Training: The paper identifies communication bottlenecks in distributed training as a key limitation. Future work could explore more efficient gradient compression techniques or alternative parallelization strategies to mitigate this issue.  
4. Broader Benchmarking: While the experiments focus on VGGNet and ResNet, additional benchmarks on smaller networks or other domains (e.g., natural language processing) could provide a more comprehensive evaluation of the framework's capabilities.
Questions for the Authors
1. How does the framework handle memory management for large-scale networks in environments with limited GPU memory, such as mobile devices?  
2. Have you considered integrating WebGPU, which is gaining traction as a modern alternative to WebCL, to address the limitations of browser compatibility?  
3. Can the framework support other neural network architectures (e.g., transformers), and if so, what modifications would be required?  
4. What is the expected performance improvement if advanced convolution optimization techniques, such as those by Lavin (2015), are implemented?
In conclusion, the paper makes a significant contribution to the field by enabling browser-based deep learning with distributed training capabilities. While there are areas for improvement, the work is well-motivated, technically valid, and empirically rigorous, warranting acceptance.