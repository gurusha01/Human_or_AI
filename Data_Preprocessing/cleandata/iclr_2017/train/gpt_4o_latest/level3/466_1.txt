Review of the Paper
Summary of Contributions
This paper provides a rigorous theoretical foundation for identity parameterization in deep neural networks, focusing on deep linear residual networks and ReLU-activated residual networks. The authors demonstrate two significant results: (1) deep linear residual networks have no spurious local optima, ensuring a favorable optimization landscape, and (2) ReLU-activated residual networks exhibit universal finite-sample expressivity, meaning they can represent any function of a dataset given sufficient parameters. The paper also introduces a simplified residual architecture inspired by these theoretical insights, achieving competitive performance on CIFAR10, CIFAR100, and ImageNet benchmarks without relying on techniques like batch normalization or dropout. The work addresses a fundamental problem in deep learning by linking identity parameterization to optimization stability and expressivity, offering both theoretical and empirical contributions.
Decision: Accept
The paper is recommended for acceptance due to its strong theoretical contributions, practical relevance, and alignment with the conference's focus on advancing understanding of deep learning principles. The key reasons for this decision are:
1. Theoretical Rigor: The paper provides novel and scientifically rigorous proofs for the absence of spurious local optima in deep linear residual networks and the universal expressivity of ReLU-activated residual networks.
2. Practical Impact: The proposed simplified residual architecture demonstrates competitive performance on standard benchmarks, validating the theoretical insights.
Supporting Arguments
1. Problem Significance: The paper tackles a fundamental question in deep learning: how identity parameterization influences optimization and representation. This is a critical topic given the widespread use of residual networks in modern architectures.
2. Theoretical Contributions: The proofs are well-structured and address key challenges in optimization and expressivity. The results on the absence of spurious local optima and universal finite-sample expressivity are impactful and advance the theoretical understanding of deep networks.
3. Empirical Validation: The experiments on CIFAR10, CIFAR100, and ImageNet demonstrate that the proposed architecture, inspired by the theoretical findings, performs competitively without relying on additional regularization techniques, highlighting the practical utility of the work.
Suggestions for Improvement
1. Extension to Nonlinear Activations: While the paper provides strong results for ReLU activations, extending the theoretical analysis to more general nonlinear activation functions would significantly enhance its impact. This is noted as an open problem, but further discussion or preliminary results would be valuable.
2. Notation Clarification: The notation "U ∈ R ? × k" before Eq. (3.1) is unclear and should be explicitly defined to improve readability.
3. Empirical Analysis on Larger Models: The underperformance of the proposed architecture on ImageNet compared to its original counterpart suggests room for improvement in scaling. A deeper analysis of this limitation would strengthen the empirical contributions.
Questions for the Authors
1. Can the theoretical results on optimization landscapes be extended to nonlinear residual networks with activations other than ReLU? If so, what are the main challenges or obstacles?
2. How does the absence of batch normalization and dropout in the proposed architecture affect training stability, especially for larger datasets like ImageNet?
3. Could the initialization strategy for convolutional layers (using smaller weights) be generalized to other architectures, and what is its theoretical justification?
Overall, this paper makes a significant contribution to the theoretical and practical understanding of identity parameterization in deep networks and is a strong candidate for acceptance.