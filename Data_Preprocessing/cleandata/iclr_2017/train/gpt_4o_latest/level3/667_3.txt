The paper proposes GRAM, a graph-based attention model that leverages hierarchical medical ontologies to address two key challenges in healthcare predictive modeling: data insufficiency and interpretability. GRAM enriches medical concept representations by adaptively combining information from parent nodes in an ontology using an attention mechanism. This mechanism dynamically adjusts based on the frequency of medical concepts, favoring general concepts for rare cases and specific ones for frequent cases. The model is trained end-to-end with an RNN and evaluated on three tasks: two sequential diagnosis predictions and one heart failure prediction. GRAM demonstrates superior performance, particularly in scenarios with limited data, achieving up to 10% higher accuracy for rare diseases and 3% improved AUC for heart failure prediction compared to baseline models.
Decision: Accept
Key reasons for acceptance include the novelty of the proposed attention mechanism and its demonstrated effectiveness in addressing data insufficiency, a critical challenge in healthcare analytics. The method is simple yet impactful, and the results are scientifically rigorous, with clear empirical evidence supporting the claims.
Supporting Arguments:
1. Novelty and Motivation: The use of hierarchical medical ontologies to enrich concept representations via an attention mechanism is well-motivated and fills a gap in the literature. The paper effectively integrates domain knowledge into deep learning, enhancing both performance and interpretability.
2. Empirical Results: The experiments are thorough, covering multiple datasets and tasks. GRAM consistently outperforms baselines, particularly under data insufficiency, which underscores its practical utility.
3. Interpretability: The attention mechanism provides intuitive insights into how the model generalizes across hierarchical levels, making it more interpretable than traditional deep learning models.
Suggestions for Improvement:
1. Ontology Representation: The current representation of ontology concepts is relatively naive. Incorporating more advanced techniques like graph convolutional networks or knowledge base factorization could potentially improve performance further.
2. Fine-Tuning Justification: The decision to fine-tune only leaf nodes and not inner nodes in the ontology requires clearer justification. Authors should elaborate on the rationale and explore whether fine-tuning inner nodes could enhance results.
3. Figure Quality: Figure 2 should use consistent image formats and higher resolution to improve readability.
4. Scalability Discussion: While the paper briefly mentions training overhead, a more detailed analysis of scalability, particularly for larger ontologies, would strengthen the practical applicability of the method.
Questions for Authors:
1. How does GRAM perform when tested with ontologies other than CCS or ICD-9? Would the approach generalize well to other domains or hierarchical structures?
2. Could you provide more details on the initialization of embeddings for inner nodes? How sensitive is the model to this initialization?
3. Have you considered alternative methods for handling infrequent concepts, such as hierarchical smoothing or transfer learning approaches? How would these compare to GRAM?
Overall, GRAM is a promising contribution to healthcare predictive modeling, addressing critical challenges with a novel and interpretable approach. With minor revisions, the paper would be a strong addition to the conference.