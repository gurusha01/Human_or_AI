Review of the Paper
The paper introduces an open-vocabulary neural language model (NLM) that generates word representations on-the-fly using character n-grams and convolutional networks. It aims to address the limitations of finite vocabulary-based models, particularly for morphologically rich languages like Czech. The proposed model is evaluated in a machine translation (MT) reranking task, showing a slight improvement of up to 0.7 BLEU points over baseline models. The authors also explore challenges in training such models, particularly instability and contamination of character n-gram representations.
Decision: Reject
The primary reasons for rejection are:  
1. The proposed model shows only marginal improvements over the baseline, with limited empirical evidence to support its claims.  
2. The paper lacks sufficient comparison with related work and does not adequately position itself within the existing literature.  
3. There is no intrinsic evaluation or in-depth analysis of how and why the proposed approach is better than alternatives.  
Supporting Arguments
1. Performance and Results: While the paper claims promising results, the reported BLEU score improvement (+0.7) is minimal and does not convincingly demonstrate the utility of the proposed model. Furthermore, the CWE-CWE model, which is central to the open-vocabulary claim, underperforms compared to CWE and WE models. This raises concerns about the practical value of the proposed approach.
2. Positioning in Literature: The paper does not sufficiently compare its approach to other recent works that address open-vocabulary or character-level language modeling. For example, methods using LSTMs or highway networks for character-based representations are mentioned but not directly compared in experiments. This omission makes it difficult to assess the novelty and competitiveness of the proposed model.
3. Lack of Analysis: The paper does not provide an intrinsic evaluation of the quality of the generated word representations. Additionally, there is no detailed analysis of why the proposed model performs poorly in certain settings, such as re-ranking reinflected Czech translations. The absence of such insights limits the reader's understanding of the model's strengths and weaknesses.
4. Limited Scope: The experiments are restricted to English-to-Czech translation. Given the claim of handling morphologically rich languages, the model should have been tested on other languages and translation directions (e.g., MRLX → English or MRLX → MRL_Y) to validate its generalizability.
Suggestions for Improvement
1. Broader Evaluation: Extend the experiments to include other morphologically rich languages (e.g., German, Russian) and diverse translation directions to demonstrate the model's robustness and generalizability.
2. Comparative Analysis: Include direct comparisons with state-of-the-art methods for open-vocabulary and character-based language modeling. This will help position the work more clearly in the context of existing research.
3. Intrinsic Evaluation: Provide intrinsic evaluations of the generated word representations, such as their quality in capturing semantic or morphological relationships. This would offer deeper insights into the model's capabilities.
4. Address Training Challenges: Investigate and propose solutions for the instability and contamination issues in training character-level output representations. This could involve exploring alternative noise distributions for NCE or more sophisticated padding schemes.
5. Detailed Analysis: Include a thorough analysis of why the proposed model performs better or worse in specific scenarios. For example, why does the CWE-CWE model underperform despite its theoretical advantages?
Questions for the Authors
1. How does the proposed model compare to other recent works that use LSTMs or highway networks for character-based representations?  
2. Can you provide more insights into the instability issues during training and how they might be addressed?  
3. Why was the evaluation limited to English-to-Czech translation? Would the model perform similarly on other morphologically rich languages?  
4. What specific steps could be taken to improve the BLEU score gains observed in the experiments?  
While the paper addresses an important problem, the limited empirical improvements, lack of comparative analysis, and insufficient exploration of the model's behavior make it difficult to recommend acceptance at this stage. However, the suggestions provided above could help strengthen the work for future submissions.