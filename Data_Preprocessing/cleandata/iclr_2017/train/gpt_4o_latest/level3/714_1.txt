Review
Summary of Contributions
This paper applies the method proposed by Jonschkowski & Brock (2015) to learn a low-dimensional state representation of a robot's head position from synthetic images using a deep convolutional neural network. The authors employ robotic priors as loss functions within a Siamese network architecture to guide the unsupervised learning process. The primary contribution lies in demonstrating the robustness of a deep neural network to noise and luminosity variations compared to a simpler one-layer model. The experimental results show a high correlation (97.7%) between the learned representation and the ground truth, with data augmentation improving robustness and feature utilization. The authors also suggest that learned feature detectors could be useful for transfer learning in similar tasks.
Decision: Reject  
Key Reasons:  
1. Lack of Novelty: The paper does not propose any new methods or significant extensions to Jonschkowski & Brock's approach. The contribution is limited to applying an existing method to a single, simple task.  
2. Insufficient Experimental Rigor: The experiments are preliminary and lack comparisons to prior state representation learning methods (e.g., Lange et al. '12, Watter et al. '15, Finn et al. '16). Additionally, no baseline methods, such as PCA, are included for benchmarking.  
Supporting Arguments
While the paper is clearly written and demonstrates the feasibility of using robotic priors with a deep neural network, it fails to address critical gaps in the literature. The lack of comparisons to other state representation learning methods makes it difficult to assess the significance of the results. Furthermore, the experimental setup is limited to a single, synthetic task (robot head position), which does not adequately test the generalizability or scalability of the approach. The discussion of related work is incomplete, omitting key references that would help situate the contribution within the broader field. Finally, the paper does not address more challenging tasks or environments, which would make the contribution more compelling.
Suggestions for Improvement
1. Expand Experimental Scope: Include comparisons to prior methods (e.g., Lange et al. '12, Watter et al. '15, Finn et al. '16) and simpler baselines like PCA to contextualize the performance of the proposed approach.  
2. Address More Complex Tasks: Extend the method to higher-dimensional representations or more challenging tasks (e.g., 3D object positions or real-world images). This would demonstrate the scalability and robustness of the approach.  
3. Improve Related Work Discussion: Provide a more comprehensive review of state representation learning methods, including autoencoders and energy-based models, to better situate the contribution.  
4. Evaluate Practical Utility: Test the learned representation in downstream tasks, such as reinforcement learning, to validate its utility beyond correlation with ground truth.  
5. Clarify Novelty: Highlight any specific innovations or improvements over Jonschkowski & Brock's method, if applicable.  
Questions for the Authors
1. How does the proposed method compare quantitatively to prior state representation learning approaches in terms of accuracy, robustness, or computational efficiency?  
2. Why was PCA not included as a baseline for comparison, given the simplicity of the task?  
3. Have you considered applying the method to real-world images or more complex environments to test its generalizability?  
4. How would the method perform in scenarios where ground truth is unavailable, and how could the quality of the learned representation be assessed in such cases?  
While the paper demonstrates a promising application of robotic priors, it requires significant improvements in experimental rigor, novelty, and scope to make a meaningful contribution to the field.