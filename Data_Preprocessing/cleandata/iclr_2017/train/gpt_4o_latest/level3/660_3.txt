Review of the Paper
Summary of Contributions
The paper proposes a novel optimization algorithm, Eve, which builds on Adam by incorporating a feedback mechanism to adaptively tune the learning rate based on the relative changes in the objective function. The authors hypothesize that this feedback mechanism can improve the performance of stochastic gradient descent (SGD) methods, particularly in navigating plateaus and saddle points in the loss surface. The paper provides experimental evidence showing that Eve outperforms state-of-the-art optimizers like Adam, RMSProp, and SGD with Nesterov momentum on tasks involving convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The authors also analyze the behavior of the feedback mechanism and its impact on the learning dynamics, claiming that Eve achieves faster convergence and better stability compared to Adam.
Decision: Reject
While the paper introduces an interesting idea and provides empirical results, the claims made are not fully substantiated, and several methodological and interpretational issues undermine the validity of the conclusions. The following key reasons lead to the rejection decision:
1. Selective Presentation of Results: The claim that "not all good learning rates make Adam fail" appears to be selectively supported by Figure 6 Right, raising concerns about the robustness of the comparisons.
2. Unconvincing Generality of Claims: The assertion that "Eve always converges" is demonstrated only for a specific case (learning rate = 0.1), making the claim overly broad and insufficiently supported.
3. Lack of Novelty in Feedback Mechanism: The feedback mechanism, parameterized by three hyperparameters, is not particularly innovative and could be directly parameterized in simpler ways.
Supporting Arguments
1. Learning Rate Comparisons: The paper does not adequately address the issue that Adam's and Eve's learning rates (e.g., 0.1) are not directly comparable due to differences in the behavior of the feedback term \( d_t \). This undermines the fairness of the comparisons and the claim of Eve's superiority.
2. Role of Manual Tuning: The paper acknowledges that learning rate tuning is performed manually for each problem, which diminishes the practical advantage of Eve. If Eve's performance gains stem primarily from such tuning, its contribution is less compelling.
3. Initial Speed of Adam: The observation that Adam is initially faster than Eve (as noted in the experiments) casts doubt on Eve's overall superiority, especially in scenarios where rapid initial convergence is critical.
Suggestions for Improvement
1. Broader Empirical Validation: The authors should provide results across a wider range of learning rates and initializations to demonstrate the robustness of Eve. This would address concerns about selective presentation of results.
2. Theoretical Analysis: A theoretical justification for the feedback mechanism and its impact on convergence properties would strengthen the paper. The current work relies heavily on empirical observations without deeper theoretical insights.
3. Comparison of Feedback Mechanisms: The authors should compare their feedback mechanism with simpler alternatives to establish its necessity and effectiveness. For instance, directly parameterizing \( d_t \) or using simpler clipping strategies could serve as baselines.
4. Clarify Generality of Claims: The claim that "Eve always converges" should be either rigorously proven or rephrased to reflect the empirical scope of the experiments.
Questions for the Authors
1. How does the feedback mechanism perform when applied to other optimizers (e.g., RMSProp or SGD) instead of Adam? Is the improvement specific to Adam?
2. Can the authors provide additional experiments to validate the claim that "not all good learning rates make Adam fail"? How was this conclusion drawn from the presented results?
3. How sensitive is Eve to the choice of its three hyperparameters (\( \beta_3, k, K \))? Would a simpler parameterization achieve similar results?
In summary, while the paper presents an intriguing idea, the lack of rigorous support for its claims and concerns about the novelty and robustness of the proposed method prevent its acceptance in its current form. Addressing these issues could significantly strengthen the work.