Review
Summary of Contributions
This paper proposes a novel reparametrization of LSTMs, termed "Normalized LSTM," which aims to preserve the means and variances of hidden states and memory cells across time steps. The authors position their work as a computationally efficient alternative to existing normalization techniques like Batch Normalization (BN) and Layer Normalization (LN). The proposed method avoids the need to estimate statistics at each time step, thereby reducing computational overhead. The paper includes a theoretical analysis of gradient flow in the proposed architecture and introduces a weight initialization scheme tailored to the reparametrization. Empirical evaluations on character-level language modeling and image generative modeling tasks demonstrate comparable or slightly better performance than BN and LN, with a reported 30% speed-up in computation.
Decision: Reject  
The primary reasons for this decision are the lack of compelling experimental evidence to support the claims of superiority over existing methods and the limited theoretical novelty. While the work is a solid incremental contribution, it does not meet the significance threshold for ICLR acceptance.
Supporting Arguments
1. Experimental Evidence: The experiments fail to convincingly demonstrate the superiority of the proposed method over alternatives like LN. While the reported speed-up is appreciated, it is not substantial enough to offset the lack of clear performance gains. For instance, the results on character-level language modeling show only marginal improvements, and the DRAW experiments do not provide a compelling case for adoption of the method in more complex tasks.
   
2. Theoretical Contributions: The theoretical analysis of gradient flow in Normalized LSTMs does not offer new insights beyond what is already known for BN-LSTM and LN-LSTM. The derivations largely extend existing work without introducing fundamentally novel ideas.
3. Significance: While the proposed method is an interesting variation of normalization techniques, it does not address a critical gap in the field or provide a transformative improvement in performance or efficiency. The computational speed-up, while useful, is incremental rather than groundbreaking.
Suggestions for Improvement
1. Stronger Empirical Validation: The authors should evaluate the proposed method on more challenging and diverse tasks, such as machine translation or speech recognition, to better demonstrate its practical utility. Additionally, comparisons with state-of-the-art methods beyond LN and BN would strengthen the empirical claims.
   
2. Ablation Studies: It would be helpful to include ablation studies to isolate the contributions of different components, such as the weight initialization scheme and the variance compensation mechanism, to the overall performance.
3. Theoretical Depth: The theoretical analysis could be expanded to provide deeper insights into why the proposed method might perform better in certain scenarios. For example, exploring the interplay between the normalization mechanism and gradient dynamics in long sequences could add value.
4. Clarity and Presentation: The paper would benefit from clearer explanations of the derivations and a more intuitive discussion of the proposed method's advantages. Visualizations of gradient flow or variance propagation could help elucidate the theoretical claims.
Questions for the Authors
1. How does the proposed method perform on tasks with longer sequences or more complex dependencies? Does it mitigate vanishing or exploding gradients better than LN or BN in such cases?
2. Can the authors provide a breakdown of the computational speed-up (e.g., where exactly the savings occur)?
3. Have the authors considered dynamic variance estimation during training instead of fixing it at initialization? If so, what were the results?
In conclusion, while the paper presents a well-executed incremental improvement, it lacks the empirical and theoretical significance required for acceptance at ICLR. The suggestions provided aim to help the authors strengthen their work for future submissions.