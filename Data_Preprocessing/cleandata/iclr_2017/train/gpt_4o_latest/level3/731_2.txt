Review of "Binary Paragraph Vector Models for Document Retrieval"
Summary of Contributions
This paper proposes a novel method for representing text documents as short binary codes using hashing techniques, enabling fast similarity search. The authors extend Le & Mikolov's paragraph vector models by introducing a stochastic binary layer to the neural network architecture. Two binarization methods—adding noise to sigmoid activations and straight-through estimation—are compared, with the latter yielding superior results. The proposed Binary Paragraph Vector (Binary PV) models are evaluated on the 20 Newsgroups and RCV1 datasets, demonstrating competitive performance with 128- and 32-bit binary codes. The paper also explores the transfer learning potential of the models and introduces a Real-Binary PV-DBOW model that simultaneously learns short binary codes and longer real-valued representations. The exposition is clear, and the application is both interesting and important for large-scale document retrieval tasks.
Decision: Reject
The paper is not recommended for acceptance at ICLR. While the application is valuable and the results are promising, the paper lacks sufficient novelty from a machine learning perspective and fails to adequately situate its contributions within the broader literature on binary hashing. Additionally, critical baseline comparisons and discussions of related work are missing, limiting the paper's scientific rigor and broader impact.
Supporting Arguments
1. Limited Novelty: The proposed method is a straightforward extension of Le & Mikolov's paragraph vectors with a binary layer. While the application is compelling, the methodological contribution is incremental and does not introduce fundamentally new insights into machine learning or representation learning.
2. Insufficient Literature Context: The paper primarily compares its approach to semantic hashing and Krizhevsky's binary autoencoders but neglects broader work on binary hashing methods. Key references, such as "Hashing for Similarity Search: A Survey" by Wang et al., are missing, and relevant methods like multi-probe hashing are not discussed.
3. Missing Baselines: The paper does not compare its end-to-end approach with simpler alternatives, such as two-stage quantization methods or off-the-shelf hashing algorithms. This omission makes it difficult to assess the practical advantages of the proposed method.
Suggestions for Improvement
1. Expand Related Work: Include a thorough discussion of prior work on binary hashing, including surveys and recent advances. For example, referencing Bengio et al.'s "Estimating or Propagating Gradients Through Stochastic Neurons" and Wang et al.'s survey on hashing would strengthen the paper's positioning.
2. Baseline Comparisons: Compare the proposed method with simpler two-stage quantization approaches and standard hashing techniques, such as locality-sensitive hashing or iterative quantization. This would provide a clearer picture of the method's relative strengths.
3. Clarify Binary Code Limitations: The claim that binary codes are unsuitable beyond 32 bits is inaccurate. Multi-probe hashing mechanisms and other techniques can effectively handle longer binary codes. Addressing this would improve the paper's technical accuracy.
4. Explore Hybrid Models: Investigate combining Binary PV-DBOW and Binary PV-DM representations, as suggested by Le & Mikolov for paragraph vectors. This could lead to improved performance and provide a more comprehensive evaluation of the proposed methods.
Questions for the Authors
1. How does the proposed method compare to off-the-shelf hashing techniques in terms of computational efficiency and retrieval performance?
2. Can you provide more details on why Binary PV-DM underperforms compared to Binary PV-DBOW? Would combining the two improve results?
3. Have you considered using multi-probe hashing or other mechanisms to extend the applicability of binary codes beyond 32 bits?
In conclusion, while the paper addresses an important application and demonstrates promising results, it requires significant improvements in novelty, baseline comparisons, and literature coverage to meet the standards of ICLR. It may be better suited for an NLP-focused conference where the applied contributions would be more impactful.