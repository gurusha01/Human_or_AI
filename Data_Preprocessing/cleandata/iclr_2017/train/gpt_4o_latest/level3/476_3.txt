Review of the Paper
Summary of Contributions
This paper provides a comprehensive empirical study on the necessity of both depth and convolution in deep convolutional neural networks (CNNs) for achieving high accuracy on CIFAR-10. The authors employ data augmentation and Bayesian hyperparameter optimization to rigorously train deep teacher models, which are then distilled into student models of varying architectures. A key finding is that shallow or non-convolutional student models fail to match the accuracy of deep convolutional teacher models, even when trained with the same parameter budget. The study builds on prior work by Ba and Caruana (2014), addressing limitations in their CIFAR-10 experiments and providing stronger evidence for the importance of depth and convolution in vision tasks. The paper also introduces a highly accurate teacher ensemble (93.8% test accuracy) and demonstrates that distillation improves the performance of shallow models, though not enough to close the "convolutional gap."
Decision: Accept
The paper is well-motivated, scientifically rigorous, and addresses an important question in deep learning: whether depth and convolution are essential for high performance in vision tasks. The key reasons for acceptance are:
1. Thorough Experimental Design: The authors employ state-of-the-art techniques, including Bayesian hyperparameter optimization and data augmentation, to ensure robust comparisons between models.
2. Significant Insights: The results convincingly demonstrate that both depth and convolution are critical for high accuracy, even when using distillation techniques.
Supporting Arguments
1. Strong Empirical Evidence: The experiments are exhaustive, covering a wide range of student architectures (non-convolutional and convolutional) and parameter budgets. The use of a highly accurate teacher ensemble ensures that the comparisons are meaningful.
2. Relevance to Literature: The study builds on and extends prior work, particularly Ba and Caruana (2014), by addressing their limitations and providing more conclusive results on CIFAR-10.
3. Scientific Rigor: The use of Bayesian optimization and careful control of experimental variables (e.g., parameter budgets, data augmentation) ensures that the findings are robust and reproducible.
Suggestions for Improvement
While the paper is strong overall, there are areas where it could be improved:
1. Clarity in Terminology: The term "logits" should be explicitly defined as unnormalized log-probabilities early in the paper to avoid confusion.
2. English Language and Style: The manuscript contains minor grammatical errors and typos. Additionally, adherence to the ICLR citation style should be ensured.
3. Discussion of Negative Results: While the paper convincingly argues that shallow models cannot match deep convolutional models, a more detailed discussion of why this might be the case (e.g., representational limitations) would strengthen the conclusions.
4. Broader Implications: The paper could benefit from a discussion on how these findings might generalize to other datasets or domains, such as ImageNet or non-vision tasks.
Questions for the Authors
1. How do the findings generalize to larger datasets like ImageNet? Are there preliminary results or theoretical insights that suggest similar trends?
2. Could the authors elaborate on why dropout consistently reduces the performance of student models? Is this specific to the distillation framework or the CIFAR-10 dataset?
3. Is there a theoretical explanation for the observed "convolutional gap"? For instance, could it be related to the hierarchical feature extraction capabilities of convolutional layers?
In conclusion, this paper makes a valuable contribution to understanding the role of depth and convolution in deep learning. With minor revisions to improve clarity and address the broader implications of the findings, it will be a strong addition to the conference.