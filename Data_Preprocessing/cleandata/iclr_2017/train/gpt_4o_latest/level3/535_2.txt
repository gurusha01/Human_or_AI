Review
Summary of Contributions
This paper proposes a novel video captioning model, ASTAR, which extends the image captioning framework of Xu et al. (2015) to video captioning by incorporating attention mechanisms over multiple convolutional layers of a 3D CNN (C3D). The model introduces two attention mechanisms: spatiotemporal-localization attention and abstraction-level attention, enabling adaptive focus on both spatial regions and feature abstraction levels. The authors evaluate their model on the YouTube2Text dataset and report state-of-the-art results, demonstrating the effectiveness of leveraging multi-layer CNN features for video captioning. The paper also highlights the importance of aligning spatiotemporal features across layers to achieve semantic consistency.
Decision: Reject  
While the paper demonstrates solid execution and achieves improved performance on the YouTube2Text dataset, it lacks sufficient novelty and technical contributions to warrant acceptance. The proposed model primarily builds on existing work, such as attention mechanisms and multi-layer feature extraction, without introducing significant new insights or methods to excite the research community.
Supporting Arguments for the Decision
1. Lack of Novelty: The core idea of leveraging multi-layer CNN features with attention mechanisms is not groundbreaking. The extension from image captioning to video captioning, while logical, does not introduce a fundamentally new approach or methodology. The novelty claimed in the spatiotemporal and abstraction-level attention mechanisms is incremental and builds on well-established techniques in the literature.
   
2. Unnecessary Complexity: The inclusion of hard attention and other aspects feels tangential and detracts from the paper's focus. These additions do not appear to contribute significantly to the model's performance or clarity, making the overall approach unnecessarily complex.
3. Limited Exploration of Tradeoffs: While the results demonstrate improved performance, the paper lacks a detailed exploration of the tradeoffs involved in using multi-level features. For example, the computational cost and scalability of the proposed model are not discussed, which could be critical for practical applications.
Suggestions for Improvement
1. Broader Evaluation: The experiments are limited to the YouTube2Text dataset. Including results on additional datasets like M-VAD and MSR-VTT, as mentioned in the key points, would strengthen the empirical claims and provide a more comprehensive evaluation of the model's generalizability.
2. Deeper Analysis of Multi-Level Features: The paper could benefit from a more detailed analysis of the contribution of each CNN layer to the final performance. For instance, ablation studies that systematically remove certain layers or attention mechanisms would help clarify their individual roles and importance.
3. Simplify the Model: Removing unnecessary components, such as hard attention, could make the model more focused and accessible. A streamlined approach might also make the contributions more apparent.
4. Address Computational Tradeoffs: A discussion of the computational overhead introduced by the multi-layer attention mechanism and its scalability to larger datasets or real-time applications would add practical value to the paper.
Questions for the Authors
1. How does the proposed model perform on additional datasets like M-VAD and MSR-VTT? Are the improvements consistent across datasets, or are they specific to YouTube2Text?
2. What is the computational cost of the proposed attention mechanisms compared to simpler alternatives, such as using a single CNN layer?
3. Could you provide ablation studies to quantify the individual contributions of spatiotemporal-localization attention and abstraction-level attention to the overall performance?
In conclusion, while the paper demonstrates strong execution and achieves competitive results, its lack of novelty and limited exploration of tradeoffs prevent it from making a significant contribution to the field. Addressing the above concerns could make the work more impactful in future iterations.