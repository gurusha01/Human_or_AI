Review of "Steerable Convolutional Neural Networks"
Summary of Contributions
The paper introduces a novel inductive bias in convolutional neural network (CNN) architectures by leveraging steerable representations to achieve equivariance to transformations. The authors provide rigorous mathematical derivations to support their claims, connecting steerable filters to "steerable fibers" and demonstrating how these concepts can reduce parameter requirements. This is particularly advantageous in small data regimes, where the proposed architecture outperforms standard ResNets on CIFAR10. The work builds on prior research, including capsule networks (Hinton, 2011), and extends the theory of equivariant CNNs to a broader mathematical framework. The empirical results showcase the utility of steerable CNNs, though the evaluation is limited to small-scale datasets.
Decision: Reject
While the paper presents a theoretically sound and innovative approach, the lack of evaluation on large-scale datasets and limited empirical exploration of equivariance properties weaken its overall contribution. These gaps must be addressed to justify the broader applicability of the proposed method.
Supporting Arguments for Decision
1. Strengths:
   - The paper provides a strong theoretical foundation, with detailed mathematical derivations that enhance the understanding of steerable representations.
   - The connection between steerable filters and steerable fibers is novel and insightful, contributing to the broader literature on equivariant networks.
   - The proposed architecture demonstrates improved performance over ResNets in small data scenarios, highlighting its potential for resource-constrained applications.
2. Weaknesses:
   - The empirical evaluation is limited to CIFAR10, a small-scale dataset, and does not include benchmarks on large-scale datasets like ImageNet or COCO. This limits the practical relevance and scalability of the proposed method.
   - While the architecture is shown to be effective on CIFAR10, the advantage over ResNets is not intuitively clear. A more targeted evaluation on tasks like action recognition or pose estimation, where equivariance is critical, would better demonstrate its strengths.
   - The paper lacks a thorough empirical analysis of the equivariance properties of the learned representations, which is a key claim of the work.
Suggestions for Improvement
1. Expand Empirical Evaluation:
   - Test the architecture on large-scale datasets (e.g., ImageNet, COCO) to demonstrate its scalability and generalization capabilities.
   - Include experiments on tasks where equivariance is crucial, such as action recognition or pose estimation, to better illustrate the practical benefits of steerable CNNs.
2. Clarify Intuition:
   - Provide more intuitive explanations or visualizations of why the proposed architecture outperforms ResNets in small data regimes. For instance, analyze the learned representations to highlight the benefits of steerability.
3. Equivariance Analysis:
   - Conduct detailed experiments to empirically validate the equivariance properties of the learned features. This could include measuring the robustness of the representations to transformations or comparing them to other equivariant architectures.
4. Comparison with Related Work:
   - While the paper references prior work on equivariant CNNs and capsules, a more comprehensive comparison with state-of-the-art equivariant methods would strengthen its positioning in the literature.
Questions for the Authors
1. How does the proposed architecture scale computationally when applied to larger datasets or higher-resolution images?
2. Can you provide empirical evidence or visualizations to demonstrate the equivariance properties of the learned representations?
3. Why was CIFAR10 chosen as the primary benchmark, and how do you anticipate the architecture would perform on tasks like action recognition or pose estimation?
4. How does the proposed method compare to other recent equivariant architectures, both theoretically and empirically?
In conclusion, the paper presents a promising theoretical contribution, but its limited empirical evaluation and lack of large-scale benchmarks prevent it from making a compelling case for acceptance at this stage. Addressing these issues would significantly strengthen the work.