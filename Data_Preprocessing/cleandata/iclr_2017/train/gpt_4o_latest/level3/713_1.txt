Review of "Parametric Exponential Linear Unit (PELU)"
Summary of Contributions
This paper proposes the Parametric Exponential Linear Unit (PELU), a novel parameterization of the ELU activation function, aimed at addressing vanishing gradients and bias shift in Convolutional Neural Networks (CNNs). The authors introduce a parameterization that learns the shape of the activation function during training, providing more flexibility to the network. Theoretical analysis suggests that PELU can mitigate vanishing gradients by adjusting its parameters dynamically. Empirical results on CIFAR-10/100 and ImageNet datasets demonstrate modest performance improvements over ELU, with minimal parameter overhead. The paper also provides insights into how PELU adapts its behavior across layers and epochs, suggesting that the proposed parameterization facilitates learning better features.
Decision: Reject  
While the paper presents an interesting idea, it falls short in several critical areas. The experimental results, while promising, lack sufficient rigor and clarity to substantiate the claims. Furthermore, the theoretical analysis, though intriguing, is not presented with sufficient clarity or depth. The paper also fails to adequately compare PELU's performance to simpler and widely-used activation functions like ReLU in terms of computational efficiency and practical trade-offs.
Supporting Arguments for Decision
1. Experimental Weaknesses:  
   - The experimental results, though showing relative improvements, lack a bold and clear performance claim. The improvements (e.g., 4.45% on CIFAR-10 and 7.28% on ImageNet) are modest and not contextualized against simpler baselines like ReLU.  
   - The speculative language in the discussion (e.g., "we believe," "it seems") undermines the strength of the conclusions. For example, the claim that PELU reduces vanishing gradients is not directly validated through experiments explicitly designed to measure gradient flow.  
   - Overfitting issues observed with PELU are acknowledged but not systematically addressed, leaving doubts about its robustness.  
2. Theoretical Clarity:  
   - The theoretical analysis of vanishing gradients is not sufficiently accessible. While the authors provide mathematical derivations, the implications of these results are not clearly tied back to practical scenarios or empirical observations.  
   - The constraints imposed to maintain differentiability (e.g., parameter positivity) are not thoroughly justified in terms of their impact on optimization dynamics.  
3. Comparison to Simpler Alternatives:  
   - The paper does not adequately discuss the trade-offs between PELU and simpler activation functions like ReLU. While PELU adds minimal parameter overhead, its computational complexity and potential overfitting issues are not compared to the simplicity and efficiency of ReLU.  
Suggestions for Improvement
1. Stronger Experimental Validation:  
   - Include experiments explicitly designed to validate the claims about vanishing gradients and bias shift, such as gradient norm analysis across layers.  
   - Provide a more comprehensive comparison to ReLU and other activation functions, focusing on computational efficiency, robustness, and generalization performance.  
2. Clarity in Theoretical Analysis:  
   - Simplify the presentation of the theoretical results and explicitly connect them to the observed empirical behaviors.  
   - Provide a clearer explanation of the constraints and their implications for optimization and performance.  
3. Performance Claims:  
   - Make a bold and clear performance claim, supported by statistically significant results across diverse tasks and architectures.  
   - Address the overfitting issues observed with PELU and propose potential solutions or regularization techniques.  
4. Speculative Language:  
   - Avoid speculative language and provide concrete evidence for all claims. For instance, the discussion of parameter progression and its benefits should be backed by controlled experiments.  
Questions for the Authors
1. How does PELU compare to ReLU in terms of computational efficiency, especially in large-scale tasks like ImageNet?  
2. Can you provide empirical evidence directly demonstrating that PELU mitigates vanishing gradients (e.g., gradient flow analysis)?  
3. How does the choice of initial learning rate and weight decay affect PELU's performance, and how do these hyperparameters interact with the proposed parameterization?  
4. Have you considered applying PELU to tasks beyond image classification, such as object detection or recurrent architectures?  
In summary, while the idea of learning activation functions is compelling, the current paper lacks the rigor and clarity needed to justify its claims and contributions. With stronger experimental validation, clearer theoretical exposition, and a more thorough comparison to simpler baselines, this work could make a more significant impact.