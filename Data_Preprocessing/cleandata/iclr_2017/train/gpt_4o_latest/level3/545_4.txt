The paper introduces Compositional Kernel Machines (CKMs), a novel framework that combines kernel methods with the compositional properties of convolutional neural networks (convnets) using sum-product network (SPN) techniques. The authors propose that CKMs can efficiently compute discriminant functions over an exponential number of virtual instances, mitigating the curse of dimensionality and improving sample complexity. They claim CKMs offer faster training compared to convnets while maintaining competitive performance, particularly in scenarios requiring compositionality and symmetry recognition. The paper demonstrates CKM's potential through experiments on small datasets like NORB, highlighting its advantages over SVMs and its ability to generalize better in certain synthetic tasks.
Decision: Reject
The primary reasons for rejection are the lack of convincing empirical evidence supporting the claims and the preliminary nature of the work. While the paper explores an interesting and relatively unexplored area of combining kernel methods with deep learning concepts, it fails to demonstrate clear advantages over existing methods in practical settings.
Supporting Arguments:
1. Weak Experimental Validation: The experiments are limited to small datasets like NORB, and the results show only slight improvements over SVMs. CKMs perform poorly compared to convnets on real-world datasets, with significant improvements observed only in synthetic tasks. This limits the practical applicability of the proposed method.
2. Scalability Concerns: The inference time of CKMs depends on both the network structure and the training set size, raising concerns about scalability to larger datasets. While the paper claims faster training compared to convnets, this is not rigorously justified, especially given that both methods rely on gradient descent.
3. Dense and Ambiguous Explanations: Section 3.1, which defines the leaf kernel and architecture, is dense and difficult to follow. The purpose and effectiveness of the leaf kernel, particularly when comparing raw pixel intensities, are not well-explained. Additionally, the architecture design for the sum-product function appears arbitrary and lacks clarity.
Additional Feedback:
1. The authors should provide a more thorough comparison with state-of-the-art methods on larger and more diverse datasets to better demonstrate CKM's advantages.
2. The claim that CKMs are faster to train than convnets requires stronger empirical evidence and a detailed analysis of computational complexity.
3. The paper would benefit from a clearer explanation of the leaf kernel's role and its design choices. Simplifying Section 3.1 and providing illustrative examples would improve readability.
4. The architecture design for the sum-product function should be better motivated, with justifications for the chosen structure and its impact on performance.
Questions for the Authors:
1. How does CKM's performance scale with larger and more complex datasets? Have you tested CKMs on benchmarks beyond NORB?
2. Can you provide a more detailed comparison of training times and computational costs between CKMs and convnets, including hardware specifications?
3. What motivated the specific design choices for the leaf kernel and sum-product function architecture? Could alternative designs improve performance?
In summary, while the paper presents an intriguing idea, it is not yet mature enough for acceptance. Addressing the concerns raised and providing stronger empirical evidence would significantly strengthen the work.