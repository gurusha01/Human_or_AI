Review
Summary of Contributions
This paper proposes a novel framework for boosting generative models (BGMs) by extending the classical boosting methodology to unsupervised learning. The authors present a meta-algorithm that ensembles weak generative models, such as Variational Autoencoders (VAEs) and Restricted Boltzmann Machines (RBMs), and even discriminative models trained to distinguish real from synthetic data. The method is backed by strong theoretical guarantees, ensuring log-likelihood improvement under specific conditions. The paper provides two distinct approaches—discriminative and generative—for constructing intermediate models, and demonstrates the framework's utility in density estimation, sample generation, and unsupervised feature learning. The idea of boosting generative models is innovative and holds promise for improving generative modeling tasks.
Decision: Reject  
Key Reasons:
1. Weak Experimental Validation: The experimental results are limited to a synthetic dataset and MNIST, with baseline models that are too simplistic to convincingly demonstrate the framework's effectiveness. The lack of comparisons with state-of-the-art models further weakens the empirical claims.
2. Incomplete Execution: The paper does not address a fundamental question—whether boosting small models is superior to training a single large model. Additionally, the reliance on heuristic weighting and the absence of a normalization constant for the strong learner are significant limitations.
Supporting Arguments
1. Theoretical Strengths: The paper provides rigorous theoretical guarantees for log-likelihood improvement, which is a notable contribution. The conditions for sufficient and necessary progress are well-defined, and the derivations are sound.
2. Computational Challenges: The discriminative approach requires MCMC sampling at each boosting iteration, which is computationally expensive. While the authors acknowledge this, they do not propose efficient alternatives, making the method less practical for large-scale applications.
3. Experimental Weaknesses: The evaluation on MNIST and synthetic data is insufficient to establish the generalizability of the approach. The results do not convincingly demonstrate that the proposed method outperforms existing generative models. Moreover, experimental details, such as sampling procedures and likelihood estimation via Annealed Importance Sampling (AIS), are inadequately described, making reproducibility difficult.
4. Literature Gaps: The paper fails to cite and compare with closely related work, particularly "Self-Supervised Boosting" by Welling et al., which appears conceptually similar to the proposed approach.
Additional Feedback for Improvement
1. Broader Evaluation: Include experiments on more complex datasets (e.g., CIFAR-10, ImageNet) and compare against state-of-the-art generative models like GANs, VAEs, and diffusion models. This would strengthen the empirical claims.
2. Address Fundamental Questions: Provide a thorough analysis of whether boosting small models offers advantages over training a single large model. This is critical to justify the proposed framework.
3. Heuristic Weighting: Explore principled methods for determining the weights of intermediate models instead of relying on heuristics. This could improve the robustness of the approach.
4. Normalization Constant: Address the issue of the missing normalization constant for the strong learner. This is a significant limitation that affects the interpretability and applicability of the method.
5. Computational Efficiency: Propose strategies to reduce the computational overhead of MCMC sampling in the discriminative approach.
6. Reproducibility: Provide detailed descriptions of experimental setups, including sampling procedures and AIS-based likelihood estimation, to facilitate reproducibility.
Questions for the Authors
1. How does the proposed method compare to training a single large generative model in terms of both performance and computational cost?
2. Can the authors provide a more detailed explanation of the heuristic weighting strategy and its impact on the results?
3. Why was "Self-Supervised Boosting" by Welling et al. not cited, and how does the proposed approach differ from it?
4. Are there plans to address the computational inefficiency of the discriminative approach, particularly the reliance on MCMC sampling?
In conclusion, while the idea of boosting generative models is novel and theoretically sound, the paper falls short in experimental rigor, practical applicability, and addressing key questions. Significant improvements in these areas are necessary for the work to be considered for acceptance.