The paper introduces the concept of hypernetworks, where a smaller network generates the weights for a larger network, and applies this approach to recurrent networks (RNNs), particularly LSTMs. The authors claim that hypernetworks, by relaxing weight-sharing constraints, achieve state-of-the-art results in sequence modeling tasks, including language modeling, handwriting generation, and machine translation. The paper highlights the scalability and efficiency of hypernetworks, trained end-to-end with backpropagation, and demonstrates their integration with techniques like layer normalization.
Decision: Marginal Accept
The decision to marginally accept this paper is based on its innovative application of hypernetworks to RNNs and the promising results in various tasks. However, significant issues in writing clarity, organization, and experimental reporting prevent a stronger recommendation.
Supporting Arguments:
1. Novelty and Contribution: The paper presents a novel approach to weight generation for RNNs, challenging traditional weight-sharing paradigms. The integration of hypernetworks with LSTMs and their demonstrated success in diverse tasks is a noteworthy contribution.
2. Empirical Results: While the proposed structure with more parameters shows significant gains (e.g., in language modeling and machine translation), the authors fail to achieve better results with fewer parameters, which undermines their claim of efficiency.
3. Clarity and Rigor: The paper lacks clarity in its organization and contains inconsistencies, such as dependency differences between feedforward and recurrent networks. Additionally, the absence of reported trainable parameters in Table 6 raises concerns about the completeness of the experimental results.
Feedback for Improvement:
1. Reorganization and Conciseness: The paper is overly verbose and could benefit from reorganization. Reducing the number of results presented would improve readability and focus.
2. Parameter Reporting: The authors should provide the number of trainable parameters for all models, especially in Table 6, to allow for a fair comparison of efficiency.
3. Consistency: The paper should address inconsistencies, such as the differences in dependency assumptions between feedforward and recurrent networks, and clarify these distinctions.
4. Writing Style: The writing style needs improvement for better clarity and flow. Simplifying technical descriptions and providing more intuitive explanations would make the paper more accessible.
5. Efficiency Claims: The authors should revisit their claim that reducing trainable parameters effectively reduces training and recognition time, as their results do not support this assertion.
Questions for the Authors:
1. Can you clarify the specific trade-offs between model size and performance in your experiments? How do hypernetworks compare to traditional weight-sharing methods in terms of computational efficiency?
2. Why were the number of trainable parameters omitted from Table 6? Providing this information is critical for evaluating the scalability of your approach.
3. How do you justify the claim of efficiency when the proposed models with fewer parameters fail to achieve better results?
In conclusion, while the paper introduces an exciting concept with promising results, its shortcomings in clarity, organization, and experimental rigor need to be addressed for it to make a stronger impact.