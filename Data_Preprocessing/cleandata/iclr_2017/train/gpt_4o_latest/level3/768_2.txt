Review of "Group Orthogonal Convolutional Neural Networks (GoCNN)"
Summary of Contributions
The paper introduces a novel method, Group Orthogonal Convolutional Neural Networks (GoCNN), which aims to improve feature diversity in convolutional neural networks (CNNs) for image classification. By leveraging privileged information (e.g., segmentation annotations) during training, the method enforces orthogonality between feature groups (foreground and background) to reduce feature correlation and enhance generalization. The authors claim that GoCNN achieves ensemble-like benefits within a single model, thus reducing computational overhead. The method is evaluated on ImageNet and PASCAL VOC datasets, showing improved performance over baseline models.
Decision: Reject
While the paper presents an interesting idea, it suffers from several critical shortcomings that undermine its contributions. The lack of comprehensive experimentation, insufficient motivation for key components, and failure to support key claims with evidence are the primary reasons for this decision.
Supporting Arguments
1. Experimental Weaknesses:  
   - The experiments lack depth and focus. For instance, the evaluation under "partial privileged information" is incomplete, with no full experiments on ImageNet, which weakens the generalizability of the results.  
   - The paper does not compare GoCNN with ensemble models, despite claiming to replace them. This omission leaves the claim of "ensemble-like benefits" unsubstantiated.  
   - Group orthogonality and background suppression are not independently analyzed, making it unclear which component drives the observed improvements.  
2. Core Idea Limitations:  
   - The motivation for background feature suppression is weak and not well-justified. The authors fail to provide a decoupled analysis to demonstrate its necessity or effectiveness.  
   - The use of hand-coded supervision to assign features to groups, rather than automatic discovery, limits the scalability and applicability of the proposed method to real-world datasets where privileged information is scarce or incomplete.  
3. Misalignment with Claims:  
   - The abstract claims ensemble-like benefits with reduced computational costs, but no experimental evidence supports this. The computational overhead of GoCNN during training is also not discussed.  
   - Minor issues, such as an incorrect normalizing factor in Definition 2 and potential errors in Figure 1, further detract from the paper's rigor.
Suggestions for Improvement
1. Experimental Design:  
   - Conduct full experiments on ImageNet under "partial privileged information" to validate the method's scalability and robustness.  
   - Compare GoCNN with ensemble models to substantiate the claim of ensemble-like benefits.  
   - Provide a decoupled analysis of group orthogonality and background suppression to clarify their individual contributions.  
2. Motivation and Clarity:  
   - Strengthen the motivation for background suppression with theoretical or empirical evidence.  
   - Explore automatic methods for grouping features to enhance scalability and reduce reliance on privileged information.  
3. Additional Analysis:  
   - Include a detailed computational cost analysis to validate the claim of reduced overhead.  
   - Address minor issues, such as the normalization error in Definition 2 and figure inconsistencies, to improve clarity and correctness.
Questions for the Authors
1. How does GoCNN compare to ensemble models in terms of accuracy and computational cost?  
2. What is the specific contribution of background suppression to the performance gains? Can it be decoupled from group orthogonality for independent evaluation?  
3. How does the method perform when privileged information is noisy or incomplete?  
In summary, while the paper proposes an intriguing approach, its lack of rigorous experimentation, unclear motivation for key components, and failure to substantiate claims make it unsuitable for acceptance in its current form. Addressing these issues could significantly strengthen the paper.