The paper introduces the UNREAL (UNsupervised REinforcement and Auxiliary Learning) agent, which enhances the A3C deep reinforcement learning (RL) framework by incorporating unsupervised auxiliary tasks. These tasks include auxiliary control (e.g., pixel control and feature control) and reward prediction, designed to improve representation learning, data efficiency, and robustness. The authors demonstrate that UNREAL significantly outperforms the baseline A3C agent on both 3D Labyrinth tasks and Atari games, achieving faster learning and higher final performance. Notably, the UNREAL agent achieves a mean speedup of 10× in Labyrinth and an average human-normalized score of 880% on Atari, underscoring its efficacy.
Decision: Accept
Key Reasons:
1. Significant Contribution: The paper demonstrates a clear improvement over the state-of-the-art A3C agent by leveraging auxiliary tasks, which is a meaningful advancement in deep RL.
2. Empirical Validation: The results are robust and scientifically rigorous, with extensive experiments across diverse environments (Labyrinth and Atari) supporting the claims.
Supporting Arguments:
- Problem Motivation and Placement in Literature: The paper addresses a well-motivated problem—improving learning efficiency and robustness in RL agents, particularly in sparse reward settings. The approach is well-grounded in prior work, building on methods like A3C and auxiliary tasks while introducing novel mechanisms such as pixel control and reward prediction.
- Empirical Results: The experiments convincingly demonstrate the benefits of auxiliary tasks, with UNREAL achieving substantial performance gains and faster learning compared to A3C. The ablation studies further validate the contributions of individual components.
- Clarity and Accessibility: The paper is well-written and accessible to readers familiar with deep RL, with clear explanations of the architecture, auxiliary tasks, and experimental setup.
Additional Feedback:
1. Computational Resources: The paper does not provide sufficient details about the computational resources required to train the UNREAL agent. Given the complexity of the architecture and the use of auxiliary tasks, this information is critical for reproducibility and practical adoption.
2. Architecture Complexity: While the auxiliary tasks improve performance, the added complexity of the architecture raises concerns about scalability to more complex environments or tasks. A discussion of potential trade-offs between performance gains and computational overhead would strengthen the paper.
3. Source Code Availability: The paper does not mention whether the source code will be made available. Open-sourcing the implementation would greatly enhance reproducibility and facilitate further research in this area.
Questions for the Authors:
1. What are the specific computational requirements (e.g., GPU hours, memory usage) for training the UNREAL agent on Labyrinth and Atari tasks?
2. How does the architecture scale to more complex environments or tasks with higher-dimensional state/action spaces?
3. Will the source code and hyperparameter configurations be released to the community?
In conclusion, the paper makes a strong contribution to the field of deep RL by demonstrating the efficacy of auxiliary tasks in improving learning efficiency and performance. While addressing the concerns about computational resources and scalability would further strengthen the work, the overall contribution is significant and merits acceptance.