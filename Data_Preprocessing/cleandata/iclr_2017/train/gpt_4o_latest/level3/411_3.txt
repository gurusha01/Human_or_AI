Review of "Learning-Based Stochastic Super-Optimization"
Summary of Contributions
This paper introduces a novel learning-based approach for code super-optimization, leveraging reinforcement learning to improve the proposal distribution in Markov Chain Monte Carlo (MCMC) sampling. The authors argue that existing stochastic search methods, such as Stoke, fail to exploit program-specific semantics or learn from past behavior, leading to suboptimal performance. By learning a data-dependent proposal distribution using unbiased gradient estimators, the proposed method achieves significant improvements in performance. Empirical results on two datasets—Hacker's Delight and a more diverse set of automatically generated programs—demonstrate that the learned proposal distribution outperforms the state-of-the-art, achieving up to ~33% speedup compared to Stoke's ~15%. The paper also highlights the scalability of the approach and its potential applicability to other stochastic search problems.
Decision: Accept
The decision to accept is based on two key reasons:
1. Strong Empirical Results: The proposed method achieves substantial performance improvements over the state-of-the-art, demonstrating both faster convergence and higher-quality solutions in super-optimization tasks.
2. Novelty and Insights: While the idea of learning proposal distributions is not entirely new, its application to code super-optimization is innovative and provides valuable insights into improving stochastic search methods.
Supporting Arguments
1. Problem Relevance and Motivation: The paper addresses a well-defined and important problem in code optimization, which has practical implications for compiler design and program synthesis. The motivation to improve MCMC-based super-optimization by incorporating learning is clear and well-argued.
2. Scientific Rigor: The methodology is grounded in established techniques, such as the REINFORCE algorithm, and the experiments are carefully designed. The results are robust, with clear comparisons to baselines and ablation studies to evaluate the impact of different components.
3. Broader Applicability: The authors convincingly argue that their approach could generalize to other stochastic search problems, adding to its potential impact.
Suggestions for Improvement
1. Relevance to ICLR: While the paper is technically strong, its focus on code optimization may not align closely with ICLR's core themes of deep learning and representation learning. The authors could strengthen the connection by discussing how their approach relates to broader machine learning challenges, such as program synthesis or neural-guided search.
2. Representation Learning: The paper acknowledges the limitations of the current program representations (e.g., Bag-of-Words) but does not explore alternatives. Future work could investigate more expressive representations, such as graph-based or transformer-based models, to capture program structure and semantics more effectively.
3. Scalability: While the results on Hacker's Delight and synthetic datasets are promising, it would be helpful to evaluate the method on larger, real-world programs to assess its scalability and practical utility.
4. Ablation Studies: The paper could include more detailed ablation studies to isolate the contributions of different components, such as the MLP-based conditioning versus the simpler bias model.
Questions for the Authors
1. How does the learned proposal distribution generalize to unseen program types or architectures? Have you tested the method on datasets beyond Hacker's Delight and synthetic programs?
2. Could the proposed approach be integrated with neural program synthesis frameworks to further improve performance?
3. What are the computational overheads of training the proposal distribution, and how do they compare to the runtime savings achieved during optimization?
In conclusion, this paper presents a compelling contribution to the field of code optimization and stochastic search, with strong empirical results and a clear methodological framework. While its fit to ICLR's focus is debatable, the insights and innovations it offers warrant acceptance.