The paper proposes a method to evaluate generative models such as VAEs, GANs, and GMMNs using Annealed Importance Sampling (AIS) for log-likelihood estimation, addressing the community's over-reliance on visual inspection. The authors validate AIS's accuracy using Bidirectional Monte Carlo (BDMC) and provide insights into the performance, overfitting, and mode-missing behavior of these models. The paper highlights that VAEs achieve higher log-likelihoods than GANs and GMMNs, even though the latter produce visually appealing samples. The authors also critique existing evaluation methods like Kernel Density Estimation (KDE) and the IWAE bound, demonstrating their limitations in high-dimensional settings.
Decision: Reject.  
The primary reasons for this decision are the minimal technical contribution beyond the NIPS 2016 paper by Grosse et al. and the unclear novelty of applying AIS to generative models. While the paper provides a useful application of existing techniques, it lacks sufficient innovation to warrant acceptance at a top-tier conference.
Supporting Arguments:  
1. Limited Novelty: The paper builds on the AIS and BDMC techniques introduced by Grosse et al. (2016), but the technical contribution appears minimal. The main novelty lies in applying these methods to generative models, which, while valuable, does not constitute a significant advancement.  
2. Potential Errors and Clarity Issues: Section 2.3 contains an apparent error in the inequality \(E[p'(x)] \leq p(x)\), which undermines the rigor of the theoretical claims. Additionally, the initialization of the AIS procedure with \(q(z|x)\) is unclear, particularly regarding the choice of \(x\). These issues raise concerns about the correctness of the methodology.  
3. Terminology Confusion: The use of "overfitting" on page 8 is misleading and seems to describe "underfitting" instead. This lack of clarity detracts from the paper's overall coherence.  
Additional Feedback:  
1. Improving Clarity: The authors should address the potential error in Section 2.3 and clarify the AIS initialization process. Providing more detailed explanations and examples would enhance the paper's accessibility.  
2. Strengthening Novelty: To increase the paper's impact, the authors could explore extending AIS or BDMC in a novel way rather than solely applying them to generative models. For example, introducing a new evaluation metric or improving the computational efficiency of AIS could add significant value.  
3. Terminology Refinement: The term "overfitting" should be revised or clarified to avoid confusion. A discussion of how the observed phenomena differ from traditional overfitting would be helpful.  
4. Broader Experimental Validation: The experiments are limited to MNIST. Expanding the evaluation to more complex datasets would strengthen the empirical claims and demonstrate the method's generalizability.
Questions for the Authors:  
1. Can you clarify the inequality \(E[p'(x)] \leq p(x)\) in Section 2.3? Are there typos or missing assumptions?  
2. How is \(x\) chosen for initializing the AIS procedure with \(q(z|x)\)? Does this choice affect the results?  
3. Could you elaborate on why GANs and GMMNs achieve lower test accuracy despite producing visually appealing samples?  
While the paper addresses an important problem in evaluating generative models, the limited novelty and unresolved issues prevent it from meeting the standards for acceptance. Addressing these concerns could make the work more impactful in future iterations.