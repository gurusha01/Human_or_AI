Review of "Count-Based Exploration via Hashing for High-Dimensional RL"
Summary of Contributions
This paper proposes a novel count-based exploration technique for reinforcement learning (RL) in high-dimensional state spaces. By discretizing states using hash functions, the method extends classical count-based exploration to continuous and high-dimensional domains. The authors explore both static (e.g., SimHash) and learned (e.g., autoencoder-based) hashing approaches, assigning exploration bonuses based on state visitation counts. The method is evaluated on challenging benchmarks, including rllab continuous control tasks and Atari 2600 games, demonstrating near state-of-the-art performance in some cases. The paper highlights the importance of hash function granularity and domain-specific preprocessing for effective exploration. The approach is simple, computationally efficient, and complementary to existing RL algorithms.
Decision: Reject
While the paper presents a promising idea and achieves competitive results in some domains, it falls short in several critical areas. Specifically, the lack of clarity in key methodological sections and insufficient generalization across diverse environments limit its impact.
Supporting Arguments
1. Strengths:
   - The core idea of adapting count-based exploration to high-dimensional RL tasks using hashing is innovative and well-motivated.
   - The experiments on Montezuma's Revenge and other sparse-reward tasks demonstrate the potential of the approach, particularly in environments with clustering-friendly state structures.
   - The analysis of hash function granularity and the impact of learned embeddings provides valuable insights into designing effective exploration strategies.
2. Weaknesses:
   - Clarity: Section 2.3 on learned embeddings is confusing and lacks sufficient detail. The description of the autoencoder-based hashing process would benefit from an algorithm block or clearer exposition.
   - Generalization: While the method performs well on Montezuma's Revenge and a few other tasks, its performance on a broader range of Atari games is not convincing. The approach appears to rely heavily on domain-specific preprocessing, limiting its general applicability.
   - Hash Code Updates: The rationale for updating hash codes during training is not adequately justified. While the authors suggest it balances early adaptiveness and later stabilization, this claim is not rigorously analyzed or empirically validated.
   - Continuous State Spaces: The method's applicability to continuous control tasks is limited. While some success is reported, the challenges posed by continuous state spaces are not sufficiently addressed.
Suggestions for Improvement
1. Clarify Section 2.3: Provide a detailed algorithm block for the learned hash code approach. Include a step-by-step explanation of the autoencoder training process and how binary codes are generated and updated.
2. Generalization: Extend the experimental evaluation to a more diverse set of Atari games and continuous control tasks. Demonstrate the method's robustness across environments with varying state structures.
3. Hash Code Updates: Provide a theoretical or empirical analysis of the impact of updating hash codes during training. Explain how this affects exploration and stability.
4. Continuous State Spaces: Address the challenges of applying the method to continuous domains more thoroughly. For example, discuss how granularity and bonus scaling can be adapted dynamically.
5. Algorithmic Simplicity: Highlight the computational efficiency of the method compared to other exploration strategies, such as pseudo-counts or curiosity-based approaches.
Questions for the Authors
1. Can you provide a more detailed explanation of the learned embedding approach in Section 2.3? Specifically, how does the autoencoder ensure that binary codes remain stable over time while retaining sufficient granularity?
2. How does the method perform on a broader range of Atari games, particularly those with less structured state spaces than Montezuma's Revenge?
3. Have you considered dynamically adjusting the hash function's granularity during training to better handle continuous state spaces?
4. What is the computational overhead of your approach compared to other exploration strategies, such as VIME or pseudo-counts?
Conclusion
While the paper introduces an interesting and computationally efficient exploration strategy, its limited generalization, lack of clarity in key sections, and insufficient analysis of critical design choices prevent it from making a significant contribution to the field. Addressing these issues could greatly enhance the paper's impact in future iterations.