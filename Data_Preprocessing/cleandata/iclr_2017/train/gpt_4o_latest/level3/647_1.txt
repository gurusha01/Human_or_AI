The paper introduces Recurrent Inference Machines (RIMs), a novel framework for solving inverse problems by unrolling the variational inference procedure for Maximum a Posteriori (MAP) estimation into an end-to-end trainable model. The authors argue that RIMs eliminate the traditional separation between model and inference, allowing both components to be learned jointly. The framework is demonstrated on image restoration tasks, including denoising and super-resolution, where RIMs outperform several state-of-the-art methods. The paper emphasizes the modularity and scalability of the approach, highlighting its potential for broader applications in inverse problems.
Decision: Reject
Key Reasons for Rejection:
1. Limited Novelty: While the paper provides an interesting perspective on a "model-free" configuration, the concept of unrolling inference is not new. Prior works, such as [1] and [2], have also explored learnable inference procedures using neural networks. The authors' claim of novelty in separating model and inference is not convincingly substantiated, as the cited methods similarly integrate learnable components into the inference process.
2. Architectural Constraints: The use of GRU and tanh in the RNN block restricts the flexibility of the framework. These choices implicitly define the variational energy and inference algorithm, which undermines the claim of generality. The paper does not provide sufficient justification for these design decisions or explore alternatives.
Supporting Arguments:
- The empirical results are promising, particularly in image restoration tasks, where RIMs demonstrate competitive performance. However, the improvements over existing methods are incremental rather than groundbreaking.
- The paper does not adequately address the overlap with prior work. The inference procedures in [1] and [2] also become learnable neural networks with implicitly defined energy through parameter learning, which challenges the authors' claim of novelty.
Suggestions for Improvement:
1. Clarify Novelty: The authors should explicitly differentiate their approach from prior works, particularly [1] and [2]. A detailed discussion of how RIMs advance beyond these methods is necessary.
2. Architectural Justifications: Provide a rationale for the choice of GRU and tanh in the RNN block. Discuss the implications of these design choices on the flexibility and generality of the framework. Consider experimenting with alternative architectures or nonlinearities to demonstrate robustness.
3. Theoretical Insights: While the empirical results are strong, the paper would benefit from deeper theoretical analysis. For instance, how does the learned inference procedure relate to classical optimization methods? What guarantees, if any, can be provided for convergence or generalization?
Questions for the Authors:
1. How does the proposed framework handle non-linear inverse problems, given that the experiments focus on linear tasks?
2. Could you elaborate on the specific advantages of RIMs over LISTA or Andrychowicz et al. (2016) in terms of scalability and performance?
3. How sensitive is the framework to the choice of architecture (e.g., GRU vs. LSTM) and hyperparameters (e.g., number of iterations, learning rate)?
While the paper presents a promising framework with strong empirical results, the limited novelty and lack of architectural justification prevent it from making a significant contribution to the field. Addressing these concerns could strengthen the work for future submissions.