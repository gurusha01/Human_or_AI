Review
Summary of Contributions
The paper introduces the concept of "sample importance" as a measure of a sample's contribution to parameter updates during training in deep neural networks. It presents an empirical analysis of sample importance on MNIST and CIFAR-10 datasets, exploring how "easy" and "hard" samples influence different layers of the network at different stages of training. The authors claim that easy samples primarily shape the top layers early in training, while hard samples influence bottom layers later in training. They also argue that mixing easy and hard samples in training batches leads to better performance compared to homogeneous batches, challenging existing curriculum learning paradigms. The paper provides detailed experiments and visualizations to support its observations.
Decision: Reject  
Key reasons:  
1. The definition of "sample importance" is unclear and involves an unusual scaling by the squared learning rate, which lacks theoretical justification and makes interpretation difficult.  
2. The experimental setup is flawed, particularly for CIFAR-10, where the network size is too small to produce meaningful results (50% error rate). This undermines the generalizability of the findings.  
Supporting Arguments
1. Clarity and Motivation: While the concept of sample importance is intriguing, its mathematical definition is not well-motivated. The use of squared learning rates in the calculation is unconventional, and no theoretical or empirical justification is provided for this choice. This weakens the foundation of the paper's claims.  
2. Experimental Design: The experiments on CIFAR-10 use a network architecture that is too simplistic (three hidden layers with 512 nodes) to achieve competitive performance. The resulting 50% error rate is far below state-of-the-art, raising concerns about the validity of the conclusions drawn from these experiments. Additionally, MNIST error rates are not clearly reported, making it difficult to assess the significance of the results.  
3. Trivial Observations: Many of the findings, such as the evolution of sample importance aligning with expected gradient behavior, are unsurprising and do not provide novel insights. The comparison between sample importance and negative log-likelihood in Figure 4 is particularly problematic, as the two metrics are conceptually distinct and the comparison appears arbitrary.  
Suggestions for Improvement
1. Clarify the Definition of Sample Importance: Provide a more rigorous explanation of the sample importance metric, including the role of the squared learning rate. Consider alternative formulations that are more interpretable and theoretically grounded.  
2. Improve Experimental Design: Use a more appropriate network architecture for CIFAR-10 to ensure meaningful results. Report error rates for both MNIST and CIFAR-10 to provide a complete picture of the model's performance.  
3. Strengthen Comparisons: Avoid arbitrary comparisons, such as between sample importance and negative log-likelihood, unless a clear theoretical connection is established.  
4. Expand Scope: The paper could benefit from extending its analysis to more complex architectures, such as convolutional or recurrent neural networks, to demonstrate the broader applicability of the findings.  
Questions for the Authors
1. Why is the squared learning rate included in the definition of sample importance? Can you provide a theoretical or empirical justification for this choice?  
2. How do you address the poor performance of the CIFAR-10 experiments? Do you believe the findings are still valid given the high error rate?  
3. What are the MNIST error rates for the experiments? Why were they not explicitly reported?  
4. How does the proposed concept of sample importance compare to existing metrics, such as influence functions or gradients, in terms of interpretability and utility?  
While the paper raises an interesting question about the role of individual samples during training, the unclear definition of sample importance, flawed experimental setup, and lack of novel insights limit its contribution. Addressing these issues could significantly enhance the paper's impact.