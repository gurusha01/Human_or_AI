The paper addresses the problem of reducing communication overhead in parallel neural network training by introducing a new technique called Linear Pipelining (LP) for collective operations. The authors claim that LP achieves communication cost invariance with the number of GPUs, outperforms existing methods like Minimum Spanning Tree (MST) and Bidirectional Exchange (BE), and integrates well with Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD). The paper provides theoretical analysis, implementation details, and experimental results demonstrating significant speedups in training large-scale neural networks like AlexNet and GoogLeNet.
Decision: Reject
The primary reasons for rejection are the lack of novelty and insufficient exploration of alternative approaches. While the LP method shows promising results, it is essentially a re-implementation of the ring-based AllReduce approach, which is already supported by NVIDIA's NCCL library. The authors claim an earlier implementation, but this is not substantiated with clear evidence. Furthermore, the proposed communication-computation overlap is only partial and does not leverage advanced scheduling techniques, leaving room for improvement. The paper also fails to provide a comprehensive comparison with tree-shaped reduction and all-to-all methods, which would have strengthened its contributions.
Supporting Arguments:
1. Lack of Novelty: The LP method is conceptually similar to the ring-based AllReduce approach, and the claimed innovations are incremental rather than groundbreaking. The authors do not convincingly differentiate their work from existing implementations like NCCL.
   
2. Partial Communication-Overlap: While the paper attempts to overlap communication and computation, the approach is limited. A dependency scheduler could have been employed to achieve better efficiency, as seen in other state-of-the-art methods.
3. Incomplete Analysis: The omission of tree-shaped reduction and all-to-all approaches limits the paper's scope. A broader analysis would have provided a more comprehensive understanding of the trade-offs between different collective communication strategies.
Additional Feedback for Improvement:
1. Replace the term "Linear Pipelining" with the standard "ring-based approach" to align with established terminology and avoid confusion.
   
2. Provide evidence to substantiate the claim of an earlier implementation compared to NCCL. This could include timestamps, code repositories, or publication dates.
3. Explore advanced scheduling techniques to improve communication-computation overlap. A dependency scheduler could significantly enhance the proposed method's efficiency.
4. Include a detailed comparison with tree-shaped reduction and all-to-all approaches, discussing their advantages and disadvantages relative to LP.
5. Clarify the scalability claims with more rigorous experiments, particularly for GPU counts beyond the tested range.
Questions for the Authors:
1. Can you provide evidence or references to support your claim of implementing LP before NCCL introduced ring-based AllReduce?
   
2. Why did you choose not to include tree-shaped reduction and all-to-all approaches in your analysis? How would these methods compare to LP in terms of scalability and efficiency?
3. Have you considered using a dependency scheduler to improve communication-computation overlap? If so, why was it not included in this work?
4. How does LP perform on systems with more than 8 GPUs, especially in configurations with more complex interconnect topologies?
By addressing these points, the paper could significantly improve its clarity, rigor, and contribution to the field.