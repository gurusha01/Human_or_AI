Review of "Incremental Sequence Learning for Sequence Prediction and Classification"
Summary of Contributions
This paper provides a detailed exploration of curriculum learning in sequence learning tasks, introducing a novel method called Incremental Sequence Learning (ISL). The authors propose training recurrent neural networks (RNNs) on progressively longer prefixes of sequences, hypothesizing that this approach facilitates the development of internal representations necessary for learning later parts of sequences. The paper demonstrates ISL's efficacy using a custom dataset derived from MNIST, where digit images are transformed into pen stroke sequences. ISL achieves significant improvements in training speed (20x faster) and test error reduction (74%) compared to regular sequence learning. The paper also explores transfer learning, showing that models trained with ISL generalize well to sequence classification tasks. The authors provide a thorough analysis of the underlying mechanisms behind ISL's success, attributing its performance gains to the ability of RNNs to build internal representations when trained incrementally.
Decision: Reject
While the paper presents an interesting and well-executed study, it falls short in critical areas that prevent acceptance. The key reasons for this decision are the lack of justification for the use of a highly specific dataset and the absence of experiments on standard benchmarks, which limits the generalizability of the findings. Additionally, the unusually high performance of feed-forward neural networks (FFNNs) in some experiments is not adequately explained, raising questions about the robustness of the results.
Supporting Arguments
1. Dataset Specificity and Generalizability: The paper's reliance on a custom MNIST-derived dataset is a significant limitation. While the dataset is novel, its specificity makes it difficult to assess whether ISL's benefits extend to more standard sequence learning tasks, such as language modeling or time-series forecasting. Experiments on widely-used benchmarks (e.g., Penn Treebank, IMDB, or human motion datasets) would strengthen the paper's claims.
   
2. Unexplained FFNN Performance: The unusually high performance of FFNNs in certain experiments is not sufficiently addressed. This raises concerns about whether the observed improvements are due to ISL or artifacts of the experimental setup.
3. Length and Presentation: At 18 pages, the paper is excessively long for a conference submission. Many details, particularly those in the experimental results section, could be moved to an appendix to improve readability and focus.
Suggestions for Improvement
1. Expand Experiments to Standard Datasets: To demonstrate the generalizability of ISL, the authors should evaluate their method on standard sequence learning benchmarks. This would provide stronger evidence for ISL's applicability across diverse domains.
   
2. Clarify FFNN Results: The authors should provide a more detailed explanation for the high performance of FFNNs in some experiments. Additional ablation studies or theoretical insights could help clarify this anomaly.
3. Condense the Paper: The paper should be shortened to focus on the key contributions and results. Detailed experimental setups and secondary analyses could be moved to an appendix.
4. Comparison with Related Work: The method described closely resembles Zaremba et al. (2016) on Reinforcement Learning Neural Turing Machines. The authors should explicitly discuss how ISL differs from or builds upon this prior work.
Questions for the Authors
1. Why was the MNIST-derived pen stroke dataset chosen as the primary evaluation task, and how do you justify its relevance to broader sequence learning problems?
2. Can you provide additional insights into the unusually high performance of FFNNs in some experiments? Are there specific factors in the dataset or experimental setup that might explain this?
3. Have you considered testing ISL on tasks with longer and more complex sequences, such as language modeling or video frame prediction?
In conclusion, while the paper introduces a promising method and provides compelling results, its limitations in dataset choice, generalizability, and clarity prevent it from meeting the standards for acceptance at this time. Addressing these issues could significantly enhance the impact and rigor of the work.