The paper presents a novel approach to sensorimotor control in immersive environments by leveraging the cotemporal structure of high-dimensional sensory streams and lower-dimensional measurement streams. The model reframes sensorimotor learning as a supervised learning problem, eschewing traditional reinforcement learning (RL) paradigms reliant on scalar rewards. Empirical results demonstrate the approach's superiority over state-of-the-art RL methods, particularly in complex tasks within the Doom-based simulation environment. Notably, the model generalizes well across environments and goals, and its success in the Visual Doom AI Competition underscores its practical utility. The authors argue that their use of vectorial feedback and multivariate measurements provides a richer supervisory signal, stabilizing and accelerating training.
Decision: Reject
While the paper demonstrates clear empirical advantages and introduces a promising integration of ideas, it falls short in articulating its contributions and addressing critical limitations. The lack of clarity in the abstract and introduction regarding how this work advances beyond prior universal value function approaches diminishes its accessibility. Furthermore, the absence of a detailed analysis of failure modes, particularly with respect to generalization to changing goals and catastrophic forgetting in on-policy methods, raises concerns about the robustness of the proposed method. Lastly, the paper's impact would be significantly strengthened by extending experiments to other domains, such as Atari, to validate the generality of the approach.
---
Supporting Arguments for Decision:
1. Clarity of Contributions: The method appears to be a special case of universal value function approaches (e.g., Schaul et al., 2015), but the paper does not sufficiently articulate how it advances the state of the art. The abstract and introduction focus heavily on the empirical results without clearly delineating the novel theoretical or methodological contributions.
2. Failure Mode Analysis: The paper lacks a discussion of failure cases, such as potential issues with generalization to dynamically changing goals or catastrophic forgetting in on-policy training. These are critical aspects that could undermine the method's scalability and applicability to real-world scenarios.
3. Experimental Scope: While the results on Doom-based tasks are compelling, the paper does not evaluate the approach on other domains, such as Atari, which are standard benchmarks in RL research. This omission limits the paper's broader impact and raises questions about the generality of the proposed method.
---
Suggestions for Improvement:
1. Clarify Contributions: The abstract and introduction should explicitly highlight the novel aspects of the work, particularly in comparison to universal value function approaches and other supervised learning formulations for sensorimotor control.
2. Analyze Failure Modes: Include experiments or discussions addressing the method's limitations, such as its ability to handle catastrophic forgetting, robustness to goal changes, and scalability to more complex or real-world environments.
3. Expand Experimental Domains: Evaluate the method on additional benchmarks, such as Atari games, to demonstrate its generality and applicability beyond Doom-based simulations.
4. Ablation Studies: While the paper includes some ablation studies, further analysis of the impact of key architectural choices (e.g., the normalization layer in the action stream) and training regimes (e.g., single vs. randomized goals) would strengthen the paper's claims.
---
Questions for the Authors:
1. How does the proposed approach compare to universal value function approximators in terms of theoretical novelty and computational efficiency?
2. Did the authors observe any instances of catastrophic forgetting during training? If so, how were these mitigated?
3. Could the method be extended to continuous action spaces, and if so, what modifications would be required?
4. Why were experiments on Atari or other standard RL benchmarks omitted, and do the authors plan to evaluate the method on these domains in future work?
In summary, while the paper presents a promising approach with strong empirical results, it requires clearer articulation of its contributions, a deeper analysis of limitations, and broader experimental validation to meet the standards of acceptance.