Review of "Fine Grained Action Repetition (FiGAR) for Deep Reinforcement Learning"
Summary of Contributions
This paper introduces FiGAR, a novel framework for incorporating temporal repetition into the action space of Deep Reinforcement Learning (DRL). FiGAR enables agents to dynamically decide both the action to execute and the duration for which it is repeated, making it applicable to both discrete and continuous action spaces. The authors demonstrate FiGAR's effectiveness by integrating it with three DRL algorithms (A3C, TRPO, and DDPG) across three domains (Atari, MuJoCo, and TORCS). Results show significant performance improvements in many tasks, with learned temporal abstractions enhancing policy efficiency and gameplay smoothness. The paper also highlights FiGAR's lightweight design, avoiding action-space blowup through a factored policy representation.
Decision: Accept
The paper makes a meaningful contribution to the field of reinforcement learning by addressing the underexplored problem of dynamic temporal abstraction in action spaces. The proposed framework is generic, lightweight, and empirically validated across diverse domains and algorithms. However, some issues in presentation and experimental rigor need to be addressed before publication.
Supporting Arguments for Decision
1. Novelty and Contribution: The idea of dynamically learning action repetition is both novel and impactful. By extending temporal abstractions to continuous action spaces, FiGAR addresses a limitation of prior work (e.g., Lakshminarayanan et al., 2017) while maintaining computational efficiency.
2. Empirical Validation: The authors evaluate FiGAR across three domains and show consistent improvements over baseline algorithms. The results are particularly compelling in Atari games (e.g., >900% improvement in Enduro) and TORCS, where smoother policies are observed.
3. Generality: FiGAR is shown to be adaptable to different DRL algorithms and action repetition sets, demonstrating its robustness and flexibility.
Suggestions for Improvement
1. Introduction Issue: The claim that all DRL algorithms execute actions for a fixed number of steps is overly broad and contradicted by experiments. This should be rephrased to acknowledge exceptions and clarify the scope of the problem FiGAR addresses.
2. Related Work: A discussion on the connection between FiGAR and Semi-MDPs (SMDPs) is necessary to provide better theoretical context and highlight FiGAR's relationship to existing temporal abstraction frameworks.
3. Experimental Rigor:
   - Error Bars: Include error bars in all experimental results to account for variability across random seeds, especially in stochastic domains like Atari.
   - TRPO Experiments: The smaller improvement observed in TRPO experiments warrants further investigation. Testing parameter sharing between the action and repetition networks could clarify whether the lack of improvement is due to architectural inefficiencies.
   - Benchmark Consistency: The TRPO evaluation should align with the benchmarks used in Duan et al. (ICML 2016) for fair comparison.
4. Visualization: Presenting tabular results as histograms would improve clarity and accessibility for readers.
5. Policy Comparison: Videos comparing policies learned with and without FiGAR would provide qualitative insights into the benefits of temporal abstractions.
6. DDPG Clarification: The large reward difference in TORCS (FiGAR-DDPG vs. DDPG) should be explained in terms of the number of laps completed without FiGAR.
7. Minor Issues:
   - Fix the label in Figure 2 from 1000 to 3500.
   - Rephrase "idea of deciding when necessary" to "idea of only deciding when necessary."
   - Add a missing space in "spaces.Durugkar et al."
   - Replace "R={4}" with a more general notation.
Questions for Authors
1. How does FiGAR perform in highly stochastic environments where action repetition might lead to suboptimal behavior? Could a "stop action" mechanism mitigate this?
2. Why does FiGAR-TRPO show only marginal improvements in certain MuJoCo tasks? Would parameter sharing or deeper architectures help?
3. Can you provide more details on the computational overhead introduced by FiGAR compared to baseline algorithms?
In conclusion, FiGAR is a promising contribution to DRL, offering a practical and effective way to incorporate temporal abstractions. Addressing the above issues will further strengthen the paper's impact and clarity.