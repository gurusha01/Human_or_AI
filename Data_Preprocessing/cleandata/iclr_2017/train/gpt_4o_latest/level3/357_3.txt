The paper introduces MCNet, a novel two-stream encoder-decoder architecture designed for future frame prediction in videos. By decoupling motion and content information, the model simplifies the task of predicting future frames. The architecture employs separate encoder pathways for motion and content, a multi-scale motion-content residual mechanism, and a decoder that combines these features to generate pixel-level predictions. The model is end-to-end trainable and achieves state-of-the-art performance on several challenging datasets, including KTH, Weizmann, and UCF-101. The authors demonstrate that the explicit separation of motion and content improves generalization and prediction quality, particularly in periodic motion scenarios.
Decision: Accept.  
The paper makes a significant contribution to the field of video prediction by proposing a novel architecture that effectively addresses the challenges of modeling spatio-temporal dynamics. The approach is well-motivated, leveraging insights from two-stream networks for action recognition, and is positioned effectively within the existing literature. The experimental results, both qualitative and quantitative, convincingly support the claims made by the authors. The architecture's ability to generalize to unseen datasets and its superior performance on challenging benchmarks further solidify its value.
Supporting Arguments:  
1. Novelty and Motivation: The idea of separating motion and content streams is innovative and addresses a key limitation in prior work, where motion and content were often entangled in a single representation. The asymmetric encoder design is particularly compelling and well-justified.  
2. Experimental Rigor: The paper provides extensive evaluations on multiple datasets, demonstrating both qualitative and quantitative improvements over baselines. The results on UCF-101, in particular, highlight the model's robustness in handling complex real-world scenarios.  
3. Generalization: The ability of MCNet to generalize to unseen datasets, such as Weizmann, underscores the effectiveness of the motion-content decomposition.  
Suggestions for Improvement:  
1. Dataset Diversity: While the results on KTH, Weizmann, and UCF-101 are strong, the inclusion of additional datasets, such as Hollywood 2 or KITTI, could provide further validation of the model's robustness in more diverse and complex scenarios.  
2. Action Recognition Validation: Performing additional experiments on action recognition tasks, such as UCF-101, would help confirm whether the model's learned motion and content features are transferable to downstream tasks.  
3. Static Dataset Bias: The authors should analyze the impact of static dataset bias by quantifying the motion present in the datasets and discussing how this might affect the results.  
Questions for the Authors:  
1. How does the model handle scenarios with significant camera motion, such as panning or zooming? Are there any specific limitations in such cases?  
2. Could the authors provide more details on the computational efficiency of MCNet compared to baseline models?  
3. Have the authors explored the impact of varying the number of observed frames on prediction quality?  
In conclusion, the paper presents a well-motivated and rigorously evaluated approach to video frame prediction. While there are areas for further exploration, the contributions and results are substantial, making this paper a strong candidate for acceptance.