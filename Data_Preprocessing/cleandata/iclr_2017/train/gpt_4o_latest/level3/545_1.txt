Review of "Compositional Kernel Machines (CKMs)"
Summary of Contributions
The paper introduces Compositional Kernel Machines (CKMs), a novel extension of kernel methods that incorporates compositionality and symmetry, inspired by convolutional neural networks (convnets). CKMs leverage sum-product networks (SPNs) to efficiently compute discriminant functions over an exponential number of virtual instances, addressing the curse of dimensionality. The authors highlight CKMs' ability to generalize from fewer samples without data augmentation, offering a computationally efficient alternative to convnets. Preliminary experiments on the small NORB dataset demonstrate CKMs' potential, particularly in representing compositions and symmetries, where they outperform convnets in certain scenarios. The paper also proposes a weight-learning mechanism to improve CKM performance and outlines scalability advantages over deep learning methods.
Decision: Reject
While the paper presents an interesting and promising idea, it falls short in terms of experimental rigor and broader applicability. The decision is primarily based on the following reasons:
1. Limited Experimental Validation: The results are confined to the small NORB dataset, with no evaluation on standard benchmarks like MNIST, CIFAR10/100, or ImageNet. This limits the generalizability of the claims.
2. Unclear Representation Power: The reliance on pre-extracted ORB features, rather than learning features directly from data, raises concerns about CKMs' ability to compete with convnets on more complex datasets.
Supporting Arguments
1. Experimental Scope: While CKMs show promise on small NORB, the lack of results on widely-used datasets undermines the claims of general applicability. The comparison to convnets is also incomplete, as the convnets used in the experiments may not have been fully optimized (e.g., insufficient training epochs or suboptimal architectures).
2. Feature Descriptor Dependency: Unlike convnets, which learn hierarchical features directly from raw data, CKMs rely on pre-defined descriptors (e.g., ORB features). This dependency may limit their representation power, especially for tasks requiring rich, learned features.
3. Preliminary Nature: The proposed architecture and weight-learning mechanism are promising but underdeveloped. The paper acknowledges that further work is needed to improve CKMs' ability to represent richer image structures.
Suggestions for Improvement
1. Expand Experimental Validation: Evaluate CKMs on standard benchmarks like MNIST, CIFAR10/100, and ImageNet to demonstrate their scalability and generalization. Include comparisons with state-of-the-art convnets trained to convergence.
2. Clarify Experimental Details: Provide more details on the experimental setup, including the training procedure for convnets and the choice of hyperparameters for CKMs. This would help clarify whether the observed performance differences are due to fundamental advantages of CKMs or suboptimal convnet training.
3. Address Feature Learning: Explore ways to integrate feature learning into CKMs, potentially by combining CKMs with learned feature extractors or end-to-end training pipelines.
4. Theoretical Insights: Provide a deeper theoretical analysis of CKMs' representation power compared to convnets, particularly in terms of their ability to model complex data distributions.
5. Related Work: The paper should discuss its relationship to Anselmi et al.'s "Deep Convolutional Networks are Hierarchical Kernel Machines" in greater depth, as this work appears highly relevant to the proposed approach.
Questions for the Authors
1. How do CKMs perform on larger and more complex datasets like CIFAR10/100 or ImageNet? Can they scale effectively to such tasks?
2. Could CKMs be extended to learn features directly from raw data, similar to convnets? If so, how would this impact their computational efficiency?
3. Were the convnets in the experiments trained to convergence? If not, how might this affect the reported results?
4. How do CKMs handle tasks requiring hierarchical feature representations, such as object detection or segmentation?
In conclusion, while CKMs are an intriguing contribution to kernel methods and compositional learning, the paper requires stronger experimental validation and theoretical grounding to justify its claims. The suggestions provided aim to help the authors strengthen their work for future submissions.