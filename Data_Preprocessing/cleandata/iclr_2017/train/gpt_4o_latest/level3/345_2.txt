The paper proposes a novel method for compressing neural networks by retraining with a Gaussian mixture prior on weights, achieving both quantization and pruning in a single step. This approach is grounded in the Minimum Description Length (MDL) principle, providing a principled framework for compression. The method demonstrates state-of-the-art results on MNIST datasets, achieving high compression rates with minimal accuracy loss. The authors also highlight the scalability of their method to larger architectures like ResNet, though it struggles with deeper networks like VGG due to computational limitations.
Decision: Reject
While the paper presents a compelling and principled approach to neural network compression, the limitations in scalability and practical applicability to modern, deeper architectures like VGG or ResNet significantly reduce its impact. Additionally, the lack of clarity in certain experimental results and insufficient hyperparameter optimization insights further weaken the paper's contributions.
Supporting Arguments:
1. Strengths:
   - The proposed method is original and principled, leveraging a Gaussian mixture prior for weight clustering and sparsity.
   - The theoretical introduction to MDL in Section 2 is well-written and provides a solid foundation for the method.
   - The results on MNIST networks (LeNet-300-100 and LeNet-5-Caffe) are promising, achieving competitive compression rates with minimal accuracy loss.
   - The latest version of Figure 7 is appreciated for its clarity.
2. Weaknesses:
   - The method's inability to handle deeper architectures like VGG limits its practical significance, especially given the growing importance of such models in real-world applications.
   - Figure 2 shows improved accuracy for some points but not for LeNet-5-Caffe, raising questions about the consistency of the method's performance.
   - Hyperparameter optimization insights are lacking, as noted in Section 6.2. The Bayesian optimization results are unclear, and the exploration of the 13-dimensional parameter space seems insufficient.
   - The claim about variances in Section 6.1 is difficult to verify from Figure 1. Better visualizations, such as log histograms, are needed.
   - Sections 4-6 are concise but lack sufficient details on the models used, such as layer and parameter counts.
Additional Feedback:
1. Clarity:
   - The large points in Figure 2, which may indicate a good compression/accuracy loss ratio, need further clarification.
   - The formatting errors, typos, and unclear figure references should be addressed to improve readability.
2. Scalability:
   - The proposed solution for scaling to larger networks (Appendix C) is promising but lacks experimental validation. Including results for VGG or similar architectures would strengthen the paper.
3. Visualization:
   - Improved visualizations for variance claims (e.g., log histograms) and clearer representation of hyperparameter optimization results would enhance the paper's clarity and rigor.
Questions for the Authors:
1. Can you provide more insights into why the method fails to scale to deeper networks like VGG? Are there specific bottlenecks in computation or optimization?
2. How were the hyperparameters chosen for the experiments? Could a more systematic approach, such as grid search or additional Bayesian optimization runs, improve the results?
3. Could you elaborate on the significance of the large points in Figure 2 and their implications for compression/accuracy trade-offs?
In summary, while the paper introduces an interesting and theoretically sound approach, its practical limitations and lack of clarity in some areas prevent it from being ready for acceptance. Addressing these issues in future work could make this method a valuable contribution to the field of neural network compression.