The paper introduces NEWSQA, a large-scale machine comprehension dataset with over 100,000 question-answer pairs derived from CNN news articles. Its key contribution lies in its four-stage collection methodology, designed to generate challenging questions that require reasoning beyond simple word matching. The authors compare human and machine performance on the dataset, demonstrating a significant performance gap, which highlights the dataset's difficulty. Additionally, the paper proposes a computationally efficient model, BARB, as an alternative to the match-LSTM model, and provides a detailed analysis of the dataset's reasoning and answer types.
Decision: Reject.  
While the paper makes a valuable contribution by introducing a new dataset and proposing a novel model, several issues undermine its overall impact. The lack of sufficient human performance evaluation and weak empirical evidence for BARB's computational efficiency are significant concerns. Furthermore, the dataset's similarity to SQuAD raises questions about its distinctiveness and added value to the field.
Supporting Arguments:  
1. Human Performance Evaluation: The evaluation of human performance is based on a small subset of 200 questions, which is insufficient to provide a robust benchmark. A more comprehensive evaluation across the entire dataset is necessary to validate the claims about the dataset's difficulty.  
2. Comparison with SQuAD: While the authors argue that NEWSQA is more challenging than SQuAD, the evidence provided is not compelling. The reasoning type analysis is insightful but does not conclusively establish that NEWSQA offers unique challenges beyond those already addressed by SQuAD.  
3. BARB Model Claims: The paper claims that BARB is computationally more efficient than match-LSTM but does not provide quantitative evidence to substantiate this claim. Without such evidence, the novelty and utility of the proposed model remain unclear.  
Additional Feedback:  
1. Clarification of Ambiguities: The explanation of "s" in "n_s" under the "Boundary pointing" section is unclear and should be elaborated. Precise definitions and examples would improve the paper's readability.  
2. Empirical Comparison: A more thorough empirical comparison between NEWSQA and SQuAD, including performance metrics across various reasoning types, would strengthen the argument for NEWSQA's distinctiveness.  
3. Dataset Validation: The authors should validate the dataset's quality by conducting a more extensive human evaluation and providing inter-annotator agreement statistics. This would enhance confidence in the dataset's reliability.  
4. Computational Efficiency of BARB: The authors should include quantitative benchmarks comparing BARB's computational efficiency and performance against match-LSTM to substantiate their claims.  
Questions for the Authors:  
1. How does the dataset's reasoning type distribution compare to other datasets beyond SQuAD, such as CNN/Daily Mail or MCTest?  
2. Can you provide quantitative evidence for BARB's computational efficiency, such as training time or memory usage comparisons with match-LSTM?  
3. Why was the human evaluation limited to 200 questions, and how do you plan to address this limitation in future work?  
In conclusion, while the paper presents a promising dataset and an interesting model, the lack of sufficient validation and empirical evidence weakens its contributions. Addressing these issues would significantly enhance the paper's impact and relevance.