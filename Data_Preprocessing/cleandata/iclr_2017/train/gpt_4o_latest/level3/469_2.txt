Review
Summary of Contributions
The paper presents a well-engineered solution to accelerate convolutional neural networks (CNNs) by leveraging sparsity in convolutional layers through a novel sparse matrix-vector multiplication approach. The authors introduce a high-performance sparse convolution design that supports arbitrary sparsity patterns, a performance model to predict optimal sparsity levels for speedup, and a Guided Sparsity Learning (GSL) algorithm that integrates speedup awareness into the pruning process. The proposed method achieves significant speedups (3.1–7.3×) on AlexNet across various platforms (Intel Atom, Xeon, and Xeon Phi) without accuracy loss. The paper also provides practical insights for practitioners, such as identifying layers where pruning is ineffective for speedup. The implementation is made publicly available, which enhances reproducibility and practical utility.
Decision: Reject
While the paper demonstrates impressive engineering and practical contributions, it falls short in terms of novel research contributions. The work primarily focuses on performance optimization rather than introducing fundamentally new ideas or advancing the theoretical understanding of sparsity in CNNs. This lack of a strong research contribution makes it less suitable for a conference like ICLR, which prioritizes innovative and foundational research.
Supporting Arguments
1. Strengths:
   - The proposed sparse convolution design is efficient, practical, and well-optimized for real-world hardware.
   - The performance model is insightful and provides actionable guidelines for practitioners to balance sparsity, accuracy, and speedup.
   - The results are rigorously validated across multiple platforms and architectures, demonstrating significant speedups without accuracy degradation.
   - The inclusion of Guided Sparsity Learning (GSL) adds a layer of sophistication by tailoring pruning to maximize speedup potential.
2. Weaknesses:
   - The paper's primary focus is on performance engineering rather than advancing the state of the art in machine learning research. The sparse convolution design and performance model, while useful, are incremental improvements rather than groundbreaking innovations.
   - The Guided Sparsity Learning algorithm is more of an application of existing concepts (e.g., sparsity-aware pruning) rather than a novel methodological contribution.
   - The paper does not explore broader implications or theoretical insights that could generalize beyond the specific implementation.
Suggestions for Improvement
1. Broader Research Contribution: The authors could explore theoretical insights into sparsity patterns or propose a new framework for sparsity-aware training that generalizes beyond the specific hardware optimizations presented.
2. Comparison with State-of-the-Art: While the paper compares its method to dense convolution and some sparse methods, a more comprehensive comparison with other sparsity-aware techniques (e.g., Winograd or FFT-based methods) would strengthen its claims.
3. End-to-End Evaluation: While the focus on layer-wise performance is justified, an evaluation of the overall end-to-end speedup and energy efficiency on real-world tasks would provide a more holistic view of the method's impact.
4. Theoretical Analysis: A deeper theoretical analysis of the trade-offs between sparsity, accuracy, and speedup could make the paper more appealing to a research-focused audience.
Questions for the Authors
1. How does the proposed method compare to other sparsity-aware techniques, such as structured pruning or tensor factorization, in terms of both speedup and accuracy?
2. Could the performance model be extended to incorporate other FLOP-reduction techniques like Winograd or FFT-based methods? If so, how would these methods interact with the proposed sparse convolution design?
3. What are the limitations of the proposed method when applied to modern architectures like GPUs or TPUs, which are optimized for dense computations?
In conclusion, while the paper offers a robust and practical solution for leveraging sparsity in CNNs, its lack of a novel research contribution makes it less suitable for ICLR. However, the work has significant potential for application-focused venues or journals.