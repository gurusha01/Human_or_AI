Review of the Paper
Summary of Contributions
The paper introduces a novel nonlinear regularizer for solving ill-posed inverse problems by assuming that latent variables lie near a low-dimensional subspace in a Reproducing Kernel Hilbert Space (RKHS). It extends the linear low-rank assumption by employing a nuclear norm penalty on the Cholesky factor of the kernel matrix, enabling robust Kernel PCA (KPCA) without pre-training. The proposed method demonstrates state-of-the-art performance on missing feature imputation using the oil flow dataset and shows promising results for non-rigid 3D structure estimation (NRSfM) on the CMU mocap dataset. The paper claims to provide a closed-form solution for robust KPCA and highlights its potential for generalization to other energy minimization frameworks.
Decision: Reject
While the paper introduces an interesting nonlinear dimensionality regularizer, the decision to reject is based on two primary reasons: (1) insufficient clarity and motivation for key methodological choices, and (2) significant empirical and computational limitations that undermine the paper's claims.
Supporting Arguments
1. Clarity and Motivation: 
   - The introduction lacks coherence, with unclear transitions between dimensionality reduction techniques and inverse problems. The rationale for relaxing the rank to a nuclear norm penalty is not adequately justified, especially given the resulting non-convexity and computational expense.
   - The paper does not sufficiently explain why the proposed regularizer is preferable to existing alternatives, such as probabilistic KPCA or other nonlinear dimensionality reduction techniques.
2. Empirical and Computational Limitations:
   - The empirical evaluation is limited to outdated datasets (e.g., the oil flow dataset) and does not compare against more recent nonlinear regularization techniques. For NRSfM, the comparison is restricted to linear regularizers, leaving the broader applicability of the method untested.
   - The alternating optimization approach is computationally expensive, requiring a full singular value decomposition (SVD) of the kernel matrix in every iteration. This makes the method impractical for large-scale problems, which is acknowledged but not addressed in the paper.
   - The lack of convergence analysis for the alternating optimization procedure raises concerns about the robustness and reliability of the proposed method.
Suggestions for Improvement
1. Clarity and Motivation:
   - Revise the introduction to clearly articulate the connection between dimensionality reduction and inverse problems. Provide a stronger theoretical or empirical justification for the use of a nuclear norm penalty on the Cholesky factor of the kernel matrix.
   - Include a discussion of alternative approaches and explicitly highlight the advantages of the proposed method.
2. Empirical Evaluation:
   - Use more recent and diverse datasets for evaluation, particularly for missing data imputation. Compare the proposed method against state-of-the-art nonlinear regularizers and robust KPCA variants.
   - Extend the evaluation of NRSfM to include comparisons with other nonlinear dimensionality reduction techniques, such as those based on deep learning.
3. Computational Efficiency:
   - Explore scalable optimization techniques, such as approximate SVD or low-rank kernel approximations, to reduce the computational burden.
   - Provide a convergence analysis or empirical evidence to demonstrate the stability of the alternating optimization procedure.
Questions for the Authors
1. Can you provide a more detailed explanation of why the nuclear norm penalty on the Cholesky factor is a suitable relaxation for the rank minimization problem in the RKHS?
2. How does the proposed method compare to more recent nonlinear dimensionality reduction techniques, such as those based on deep generative models or variational approaches?
3. Have you considered using approximate kernel methods (e.g., Nystr√∂m approximation) to address the computational challenges associated with full SVD of the kernel matrix?
4. Could you provide insights into the scalability of the method for larger datasets, such as dense 3D reconstruction tasks?
While the paper presents a novel idea, addressing the above concerns would significantly strengthen its contributions and impact.