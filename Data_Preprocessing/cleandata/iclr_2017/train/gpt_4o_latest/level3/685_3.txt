Review of the Paper
Summary of Contributions
This paper presents an open-vocabulary neural language model (NLM) that computes word representations on-the-fly using character n-grams via convolutional networks and pooling layers. The proposed approach addresses challenges in handling morphologically rich languages and out-of-vocabulary (OOV) words, especially in machine translation (MT). The authors test their model in a reranking setup for English-to-Czech translation and report a modest improvement of up to 0.7 BLEU points. The paper also introduces an extension (CWE-CWE) that uses character-based embeddings for both input and output representations, enabling open-vocabulary modeling. While the results are mixed, the work raises important questions about character-based representations and their limitations.
Decision: Reject
The paper is not ready for publication due to the following reasons:
1. Lack of Novelty: The approach to character-based modeling is not novel, as it closely resembles prior work by Kim et al. (2015) and Jozefowicz et al. (2016). The primary contribution lies in applying this method to MT, but the novelty in this application is limited.
2. Insufficient Comparative Analysis: The paper does not adequately compare its approach to existing character-level models, particularly those discussed in Jozefowicz et al. (2016). This omission weakens the contextualization of the work.
3. Unsubstantiated Claims: Claims such as "making the notion of vocabulary obsolete" are exaggerated and inconsistent with the model's reliance on subword and character representations, which still implicitly define a vocabulary.
4. Incomplete Explanations: Key aspects, such as the N-best list reranking for OOV handling and the configuration of Noise Contrastive Estimation (NCE), are insufficiently detailed.
Supporting Arguments
1. Novelty of Approach: While the application to MT is interesting, the core methodology of character-based modeling has been extensively explored in prior work. The paper does not sufficiently differentiate itself from these methods.
2. Comparative Discussion: The absence of a thorough comparison with existing character-level models limits the paper's ability to demonstrate its contributions relative to the state of the art.
3. Scientific Rigor: The results, while promising, are modest and do not convincingly support the claims made in the paper. For example, the CWE-CWE model underperforms, and the reasons for this are not fully explored.
Additional Feedback for Improvement
1. Comparative Analysis: The paper should include a detailed discussion of related work, particularly Kim et al. (2015) and Jozefowicz et al. (2016), and highlight how the proposed approach differs or improves upon them.
2. Elaboration on NCE: The configuration and parameterization of NCE (e.g., noise distribution, sampling strategy) should be described in greater detail to allow reproducibility and better understanding.
3. Character Embeddings for Output: The use of character embeddings for output (Sec. 2.4) needs clearer explanation, particularly regarding how it handles OOV words and avoids contamination of frequent short words.
4. Perplexity for Unknown Words: The paper should address methods for calculating and interpreting perplexity for unknown words, as discussed in Shaik et al. (2013).
5. Training Vocabulary Size: Include the size of the full training vocabulary in Sec. 4.4 and Table 4 to provide context for the results.
6. Clarity and Presentation: Correct minor grammatical and typographical errors, and ensure all notations (e.g., P^H, e^{out}, e^{char-out}) are properly defined. Consider renormalizing perplexity to the character level for open-vocabulary approaches.
Questions for the Authors
1. How does the proposed model compare quantitatively and qualitatively to Kim et al. (2015) and Jozefowicz et al. (2016) in terms of performance and training stability?
2. Can you provide more details on the N-best list reranking process and how it specifically addresses OOV issues?
3. How does the choice of noise distribution in NCE affect the performance of the CWE-CWE model, and have alternative distributions been explored?
4. Why do CWE-CWE models underperform in the reranking task, and what steps could be taken to mitigate the "rich get richer" issue in character n-gram representations?
In conclusion, while the paper addresses an important problem and shows some promise, it lacks sufficient novelty, comparative analysis, and clarity to warrant acceptance in its current form. With significant revisions and additional experiments, it could become a stronger contribution to the field.