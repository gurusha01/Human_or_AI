Review
Summary of the Paper
The paper introduces a novel method called Interior Gradients for quantifying feature importance in deep neural networks. The authors argue that traditional gradient-based methods fail to capture feature importance due to the phenomenon of saturation, where gradients become vanishingly small even for important features. To address this, the proposed method examines gradients of counterfactual inputs, specifically scaled versions of the original input, to compute Integrated Gradients that satisfy desirable axioms like Sensitivity and Implementation Invariance. The method is computationally efficient, easy to implement, and applicable across various architectures, including convolutional, graph-based, and recurrent networks. The authors provide visualizations, empirical evaluations, and theoretical justifications to demonstrate the effectiveness of their approach. They also highlight its utility in debugging and understanding deep networks.
Decision: Reject
While the paper presents an interesting visualization technique and provides some empirical results, the approach lacks sufficient theoretical justification and clarity in key areas. The following reasons underpin this decision:
1. The method is ad hoc and does not convincingly explain why gradients of counterfactual inputs are better proxies for feature importance than regular gradients.
2. The correlation between gradients, feature importance, and network confidence remains unexplored, leaving critical questions unanswered.
3. The impact of the scaling parameter (\(\alpha\)) on feature importance and its relation to attention mechanisms is unclear, and the choice of scaling lacks rigorous justification.
Supporting Arguments
1. Lack of Theoretical Rigor: While the authors argue that saturation undermines the utility of regular gradients, they do not provide a strong theoretical foundation for why scaling inputs resolves this issue. The choice of counterfactual inputs (scaled versions of the original input) appears arbitrary and is not well-motivated in the context of feature importance.
2. Unanswered Questions: The paper does not address whether confident predictions correlate with low gradients and uncertain predictions with high gradients. This is a critical gap, as it directly relates to the validity of the proposed method.
3. Empirical Results: Although the visualizations are compelling, the evaluation metrics (e.g., AOPC and localization) are limited in scope and do not provide a comprehensive assessment of the method's effectiveness. The reliance on subjective "eyeballing" further weakens the empirical claims.
4. Clarity and Completeness: The paper does not mention whether the models used in experiments employ batch normalization, which could significantly affect gradient behavior and saturation. This omission raises concerns about the reproducibility and generalizability of the results.
Suggestions for Improvement
1. Theoretical Justification: Provide a more robust theoretical explanation for why counterfactual gradients are meaningful proxies for feature importance. Explore the mathematical relationship between saturation, gradients, and feature importance.
2. Scaling Parameter Analysis: Investigate the role of the scaling parameter (\(\alpha\)) in detail. Explain how it influences feature importance and whether it aligns with attention mechanisms in neural networks.
3. Broader Evaluation: Compare the proposed method against other state-of-the-art attribution techniques (e.g., DeepLift, LRP) to establish its relative strengths and weaknesses. Additionally, provide quantitative metrics beyond AOPC and localization to assess the method's utility.
4. Model Details: Clearly state whether batch normalization or other architectural components are used in the experiments. This will help clarify the generalizability of the findings.
5. Address Open Questions: Explore the relationship between gradients, feature importance, and prediction confidence. This would strengthen the paper's claims and provide deeper insights into the proposed method.
Questions for the Authors
1. Why are counterfactual inputs obtained by scaling the original input particularly suited for quantifying feature importance? Could other types of counterfactuals (e.g., additive noise) yield similar or better results?
2. How does the choice of scaling parameter (\(\alpha\)) affect the results? Is there an optimal range or method for selecting \(\alpha\)?
3. Does the proposed method generalize to models with batch normalization or other architectural components that alter gradient behavior?
4. How does the method perform compared to other attribution techniques like DeepLift or LRP in terms of computational efficiency and accuracy?
In conclusion, while the paper introduces an intriguing idea, it requires stronger theoretical grounding, more comprehensive evaluations, and clearer explanations to be suitable for acceptance.