Review of "Hierarchical Compositional Networks (HCN)"
The paper introduces the Hierarchical Compositional Network (HCN), a directed generative model designed to learn and disentangle interpretable building blocks of binary images without supervision. The model employs AND, OR, and POOL operations to define hierarchical features and uses max-product message passing (MPMP) for inference and learning. The authors claim that HCN can efficiently compress and classify binary images, with its forward pass resembling a convolutional neural network (CNN) but producing qualitatively different features.
Decision: Reject
The primary reasons for rejection are the lack of proper contextualization within existing literature and insufficient experimental validation. While the topic of interpretable hierarchical representations is compelling, the paper fails to adequately compare its contributions to prior work and does not provide robust empirical evidence on real-world datasets.
Supporting Arguments:
1. Lack of Literature Contextualization: The paper does not sufficiently cite or compare itself to prior work in hierarchical models, such as AND-OR graphs, sum-product networks (SPNs), or grammars. The claim of being the first to learn interpretable parts is inaccurate, as prior works (e.g., SPNs, AND-OR templates) have addressed similar objectives. The authors should explicitly discuss differences and relationships with these models.
2. Experimental Limitations: The experiments are restricted to toy datasets, and no results are presented on real-world datasets like MNIST or comparisons to benchmarks such as Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs). This limits the generalizability and practical relevance of the proposed approach.
3. Incorrect and Overstated Claims: The claim that inference is feed-forward is misleading, as message passing inherently involves recurrent computations. Additionally, the assertion that compression demonstrates "understanding" is debatable and lacks theoretical or empirical justification.
Additional Feedback for Improvement:
1. Technical Clarity: Critical technical details, such as constraints during message passing and the optimization challenges of max-product inference, are relegated to the appendix. These should be moved to the main text for better accessibility.
2. Heuristic Reliance: The learning and inference algorithms rely heavily on heuristics, such as binarizing activations and specific message-passing schedules. These heuristics need further analysis, justification, and discussion of their limitations.
3. Comparison to Deep Rendering Models: The paper should explicitly link its approach to the Deep Rendering Model (DRM) and discuss how HCN differs in terms of feature overlap, interpretability, and tractability.
4. Iterative Steps vs. Backward Pass: The iterative steps described in the learning algorithm are not equivalent to a single backward pass. This distinction should be clarified.
Questions for the Authors:
1. How does HCN compare quantitatively to SPNs, AND-OR graphs, or grammars in terms of feature interpretability and computational efficiency?
2. Can the proposed model scale to real-world datasets, and how does it perform compared to VAEs, GANs, or CNNs on benchmarks like MNIST or CIFAR-10?
3. What specific constraints are imposed during message passing, and how do they address optimization challenges with max-product inference?
In summary, while the paper proposes an interesting model for hierarchical feature learning, it lacks sufficient contextualization, experimental rigor, and clarity to warrant acceptance in its current form. Addressing these concerns could significantly strengthen the contribution.