Review
This paper introduces an iterative decoding scheme for machine translation (MT) that seeks to address the inability of traditional decoding algorithms to revisit and correct earlier decisions. The proposed approach uses a convolutional neural network (CNN) with attention mechanisms to iteratively refine translations generated by a phrase-based MT system. The model predicts word substitutions based on the source sentence and the current translation, and the refinement process stops when no further improvements can be made. The authors report a modest improvement of up to 0.4 BLEU on the WMT15 German-English dataset, with an average of 0.6 word substitutions per sentence.
Decision: Reject
The primary reasons for rejection are the limited scope of the proposed approach and insufficient experimental evidence to substantiate its claims. While the idea of iterative refinement is interesting and aligns with human translation processes, the current implementation lacks the depth and rigor necessary for acceptance.
Supporting Arguments
1. Limited Scope and Capabilities: The proposed model is restricted to basic word substitutions and does not address more complex translation errors, such as sentence restructuring or reordering. This limits its applicability to real-world translation tasks where such issues are common.
2. Insufficient Experimental Evidence: The reported improvement of 0.4 BLEU is modest, and the average of 0.6 word substitutions per sentence suggests that the model's impact is minimal. Additionally, the lack of comparison with a neural baseline or k-best re-ranking models makes it difficult to assess the true value of the proposed approach.
3. Lack of Global Context: The model's reliance on a local context window (2k words) for decision-making is restrictive and fails to incorporate global sentence-level information. This limitation likely contributes to the observed underperformance in more complex scenarios.
4. Counterintuitive Scoring Mechanism: Scoring at the word level rather than the sentence level is suboptimal, as it prevents the model from considering richer, non-greedy editing strategies that could lead to more substantial improvements.
Suggestions for Improvement
1. Incorporate Global Context: Consider using an encoder-decoder architecture with attention mechanisms that span the entire sentence, allowing the model to make more informed decisions.
2. Expand the Scope of Edits: Extend the model to handle insertions, deletions, or multi-word edits, which are often necessary for meaningful translation improvements.
3. Benchmark Against Stronger Baselines: Compare the proposed approach with neural MT systems and k-best re-ranking models to better contextualize its performance.
4. Acknowledge Related Work: The paper should more thoroughly discuss prior work on iterative text improvement and clarify how the proposed method advances the state of the art.
5. Improve Experimental Rigor: Conduct experiments on additional datasets and initial guess translations (e.g., neural MT outputs) to demonstrate the generalizability of the approach. Additionally, provide more detailed ablation studies to isolate the contributions of different components.
Questions for the Authors
1. How does the model perform when starting with neural MT outputs instead of phrase-based translations?  
2. Have you considered using global sentence-level scoring to enable richer, non-greedy edits?  
3. Can you provide a comparison with k-best re-ranking models to contextualize the performance gains?  
4. How does the model handle cases where sentence restructuring or reordering is required?  
In summary, while the paper presents an interesting idea, it falls short in terms of scope, experimental rigor, and novelty. Addressing these issues could significantly strengthen the contribution.