The paper presents a novel approach for learning hierarchical sentence representations using reinforcement learning, specifically the REINFORCE algorithm. It introduces a neural shift-reduce parser that induces tree structures optimized for downstream tasks, such as sentiment analysis, semantic relatedness, textual entailment, and sentence generation. The authors compare two training settings: one without external structure supervision and another combining supervision from an external parser with downstream task supervision. The paper claims that task-specific tree structures learned through this method outperform traditional sequential and syntactic tree-based models.
Decision: Reject.  
While the paper explores an interesting and promising direction—learning task-specific sentence structures—the experimental results fail to convincingly support the claims. The performance of the proposed method lags behind state-of-the-art models across most tasks, and the reported improvements over baselines are marginal. Additionally, the computational overhead of the proposed method is significant, making it less practical for large-scale applications.
Supporting Arguments:  
1. Strengths:  
   - The idea of using reinforcement learning to discover task-specific sentence structures is innovative and well-motivated.  
   - The paper is well-written and provides a clear explanation of the model, training process, and experimental setup.  
   - The qualitative analysis of the induced tree structures is insightful, showing that the model captures some linguistically intuitive patterns.  
2. Weaknesses:  
   - The experimental results do not demonstrate a clear advantage over existing methods. The proposed approach underperforms state-of-the-art models on key tasks like natural language inference and sentiment analysis.  
   - The computational cost of training the model is prohibitively high, with training times spanning several days for larger datasets. This limits the scalability and practicality of the approach.  
   - The paper lacks a thorough comparison with more recent baselines, which could provide a stronger context for evaluating the proposed method's effectiveness.  
Suggestions for Improvement:  
1. Enhance Experimental Rigor: Include comparisons with stronger baselines and state-of-the-art models to better contextualize the performance of the proposed method.  
2. Optimize Training Efficiency: Address the high computational cost by exploring techniques to reduce training time, such as batching or more efficient policy gradient methods.  
3. Expand Evaluation Metrics: Beyond task performance, consider evaluating the interpretability and generalizability of the learned tree structures.  
4. Clarify Claims: The paper should temper its claims about outperforming existing methods, as the results do not consistently support this.  
Questions for the Authors:  
1. Can you provide more details on why the proposed method underperforms state-of-the-art models on tasks like SNLI? Are there specific limitations in the model architecture or training process?  
2. How sensitive is the model to hyperparameter choices, such as the number of epochs for semi-supervised training?  
3. Have you considered alternative reward functions or reinforcement learning algorithms that might improve both performance and training efficiency?  
In summary, while the research direction is promising and the methodology is innovative, the current results and computational challenges do not justify acceptance. Addressing these issues in future work could significantly strengthen the paper's contributions.