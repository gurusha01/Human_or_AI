The paper presents a novel approach to non-factoid question answering by proposing a Neural Answer Construction Model that bridges the gap between answer selection and answer generation. It incorporates topic-specific word embeddings inspired by Paragraph2vec and leverages semantic biases from question categories and titles to improve contextual understanding. Additionally, the model employs a biLSTM-based architecture with an attention mechanism to optimize the selection and combination of sentences, ensuring coherence in multi-unit answers. The authors evaluate their method on the Japanese QA site "Oshiete goo," demonstrating a 20% improvement in answer construction accuracy over state-of-the-art baselines.
Decision: Reject
While the paper addresses an important problem and demonstrates practical improvements, the innovations it introduces are incremental and lack sufficient novelty to warrant acceptance at a top-tier AI conference. The proposed method builds on existing techniques, such as biLSTMs and attention mechanisms, without introducing fundamentally new ideas or theoretical insights. Additionally, the evaluation is limited to a single dataset in a specific domain ("Love advice"), raising concerns about the generalizability of the approach.
Supporting Arguments:
1. Problem Tackling and Motivation: The paper identifies two key limitations in current QA systemsâ€”contextual ambiguity in word usage and the inability to generate new answers. These are well-motivated problems, and the proposed solution is positioned appropriately within the literature. However, the reliance on existing techniques (e.g., biLSTMs, attention mechanisms) limits the originality of the contribution.
2. Scientific Rigor and Results: The empirical results are promising, showing a 20% improvement in answer construction accuracy over baselines. The use of human evaluation adds credibility to the findings. However, the evaluation is restricted to a narrow domain, and the lack of experiments on diverse datasets undermines the broader applicability of the model.
3. Incremental Contribution: The integration of topic-specific word embeddings and sentence combination optimization is a logical extension of prior work but does not represent a significant leap in methodology. The attention mechanism used for coherence retention is borrowed from existing studies, and its application here is relatively straightforward.
Suggestions for Improvement:
1. Broader Evaluation: The authors should test their model on additional datasets and domains to demonstrate its generalizability. This would strengthen the claim that the method is applicable to non-factoid QA tasks beyond "Love advice."
2. Theoretical Insights: The paper could benefit from a deeper theoretical analysis of the proposed model, particularly regarding the interplay between semantic biases and the attention mechanism.
3. Comparison with Generative Models: Since the paper positions itself as bridging the gap between answer selection and generation, a direct comparison with state-of-the-art generative models (e.g., GPT-based approaches) would provide a more comprehensive evaluation.
4. Clarity and Focus: The paper is dense and could benefit from clearer explanations of key concepts, particularly in the methodology section. Simplifying the narrative and focusing on the unique contributions would improve readability.
Questions for the Authors:
1. How does the model perform on datasets from other domains, such as health or travel, where the semantics of questions and answers differ significantly?
2. Could the proposed method be extended to generate entirely new sentences rather than relying on pre-extracted ones? If so, how would this impact performance?
3. How does the model handle questions with multiple valid answers or those that require subjective judgment?
In summary, while the paper demonstrates practical utility and incremental improvements, the lack of significant novelty and limited evaluation scope make it unsuitable for acceptance in its current form. Addressing the above concerns could make it a stronger contribution in the future.