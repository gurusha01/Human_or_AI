Review of "Boosted Residual Networks"
The paper introduces a novel ensemble method, Boosted Residual Networks (BRN), which combines the principles of Residual Networks (ResNets) and Deep Incremental Boosting to create ensembles of networks that grow incrementally. The authors propose a "white-box" ensemble approach, leveraging the architecture of ResNets to improve training efficiency and generalization. The method is evaluated on MNIST, CIFAR-10, and CIFAR-100 datasets, demonstrating improvements over Deep Incremental Boosting (DIB), single ResNets, and AdaBoost ensembles of ResNets. The paper also explores distilled and bagged variants of BRN, providing insights into their performance and trade-offs.
Decision: Reject
While the paper presents an interesting concept, it falls short in several critical areas that prevent it from meeting the standards for acceptance. The primary reasons for rejection are: (1) insufficient placement in the existing literature, and (2) lack of robust experimental validation to support the claims.
Supporting Arguments:
1. Literature Review and Motivation:  
   The paper does not adequately situate its contributions within the broader context of existing work. For example, Wide ResNets, which are highly relevant to the discussion of ResNet ensembles, are not cited or discussed. This omission weakens the motivation for the proposed method and fails to clarify how BRN compares to or improves upon related approaches.
2. Experimental Validation:  
   The experimental results, while promising, are limited to relatively small datasets (MNIST, CIFAR-10, CIFAR-100) and do not explore the scalability of BRN to larger, more complex datasets. Additionally, the underperformance of the ensemble compared to single networks in some cases suggests that the method requires further refinement and testing. The authors explicitly state that achieving state-of-the-art (SOTA) results is not their goal, but the lack of experiments on larger datasets makes it difficult to assess the broader applicability of the method.
3. Scientific Rigor:  
   The paper claims improved training efficiency and accuracy, but the results are not statistically robust. For instance, the authors acknowledge that the differences in accuracy fall within a 95% confidence interval, which raises concerns about the significance of the improvements. Furthermore, the choice to align random initializations across experiments, while reducing noise, may not reflect real-world variability.
Suggestions for Improvement:
1. Expand the Literature Review:  
   Include discussions of related works, such as Wide ResNets and other ensemble methods for convolutional networks. This will help clarify the novelty and relevance of BRN.
2. Experiment on Larger Datasets:  
   Evaluate BRN on more challenging datasets (e.g., ImageNet) to demonstrate its scalability and practical utility. Additionally, compare BRN to SOTA methods to provide a clearer benchmark.
3. Statistical Analysis:  
   Provide statistical significance tests for the reported results to strengthen the claims. Also, consider exploring the conditions under which BRN underperforms compared to single networks.
4. Clarify Methodology:  
   The paper could benefit from a more detailed explanation of the algorithm's design choices, such as the impact of the injection point for new blocks and the rationale for freezing layers in certain experiments.
Questions for the Authors:
1. How does BRN compare to Wide ResNets or other ResNet variants in terms of accuracy and training efficiency?  
2. Can the method be extended to larger datasets or networks with more complex architectures?  
3. What specific factors contribute to the underperformance of the ensemble compared to single networks in some cases?  
In conclusion, while the paper introduces an intriguing concept, it requires significant improvements in its literature review, experimental validation, and scientific rigor to be considered for acceptance. The suggestions provided aim to help the authors refine their work for future submissions.