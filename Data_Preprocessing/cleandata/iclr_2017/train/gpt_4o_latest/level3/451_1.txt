Review of the Paper
Summary of the Paper's Contributions
The paper investigates the loss surface of deep neural networks, aiming to provide theoretical and empirical insights into the conditions under which bad local minima are avoided. It claims to formalize two folklore observations: (i) the topology of deep linear networks differs significantly from that of half-rectified ones, and (ii) the interplay between data smoothness and over-parameterization governs the energy landscape in non-linear cases. The authors propose a theoretical result (Theorem 2.4) that half-rectified single-layer networks are asymptotically connected and introduce an algorithm to estimate the regularity of level sets. Empirical results are presented to suggest that level sets remain connected during training, with increasing curvature at lower energy levels.
Decision: Reject
The paper is not ready for acceptance due to significant issues in its theoretical rigor, clarity of presentation, and experimental validation. The main reasons for rejection are:
1. Unconvincing Theoretical Results: Theorem 2.4, the central theoretical contribution, is difficult to verify and lacks clear intuition. The assumptions and results are not well-motivated or clearly connected to prior work.
2. Weak Empirical Validation: The experimental section is poorly executed, with limited insights and unclear connections to the theoretical claims. The formatting issues further detract from the clarity of the results.
Supporting Arguments for the Decision
1. Clarity and Writing Quality: The paper is poorly written, with convoluted explanations and unclear presentation of results. For example, the description of the algorithm in Section 3 is unnecessarily verbose and lacks focus, making it difficult to follow. The theoretical sections are dense and fail to provide intuition for the results.
2. Incremental Contribution: The results appear incremental compared to prior work, such as Kawaguchi (2016) and Safran & Shamir (2015). The claim of removing technical assumptions is not convincingly demonstrated, and the results are weaker than those in existing literature.
3. Theoretical Weakness: Theorem 2.4 is the main theoretical result, but it is unconvincing. The proof is overly complex, and the assumptions are not well-justified. The practical implications of the result are unclear, and it does not provide strong guarantees for multi-layer networks.
4. Experimental Section: The empirical results are weak and fail to substantiate the claims. The experiments on MNIST, CIFAR-10, and Penn Treebank are superficial, with no meaningful analysis of the results. The formatting issues (e.g., inconsistent tables and figures) further undermine the credibility of the experimental section.
Additional Feedback for Improvement
1. Improve Clarity and Writing: The paper needs significant revision for clarity. The theoretical results should be explained with more intuition, and the algorithm should be described concisely.
2. Strengthen Theoretical Contributions: The authors should provide a more rigorous and convincing proof of Theorem 2.4, with clear assumptions and implications. Extending the results to multi-layer networks would significantly enhance the contribution.
3. Enhance Empirical Validation: The experiments should be more comprehensive and directly tied to the theoretical claims. For example, the authors could analyze the curvature of level sets more systematically and compare their findings with prior work.
4. Address Formatting Issues: The paper contains several formatting problems, such as inconsistent figure captions and poorly formatted equations. These issues should be resolved to improve readability.
Questions for the Authors
1. Can you provide a more intuitive explanation of Theorem 2.4 and its practical implications? How does it compare to prior results, such as those in Kawaguchi (2016)?
2. How do the empirical results support the theoretical claims? Specifically, how do the experiments on MNIST and CIFAR-10 validate the connectedness of level sets?
3. What are the limitations of your algorithm for estimating level set regularity, and how do you address potential failure cases?
In summary, while the paper addresses an interesting problem, it requires major revisions to improve its clarity, rigor, and experimental validation. The current submission does not meet the standards for acceptance at this conference.