Review of the Paper: GRAM - A Graph-Based Attention Model for Predictive Healthcare
Summary of Contributions
The paper proposes GRAM, a novel graph-based attention model that incorporates hierarchical domain knowledge from medical ontologies into deep learning models for electronic health record (EHR) analysis. GRAM addresses two key challenges in healthcare predictive modeling: data insufficiency and interpretability. By leveraging a directed acyclic graph (DAG) structure, the model represents medical concepts as a convex combination of their ancestors, weighted by an attention mechanism. This approach allows GRAM to generalize to higher-level concepts when data is sparse, while still preserving interpretability. The authors demonstrate the efficacy of GRAM through three predictive tasks on two medical datasets, achieving significant improvements in accuracy and interpretability over baseline models, particularly in scenarios with limited data. The paper is well-written, with a thorough evaluation methodology and clear justification for design choices, such as the use of MLP attention over simpler alternatives like dot product.
Decision: Accept
Key reasons for acceptance:
1. Novelty and Impact: The integration of DAG-based hierarchical knowledge into neural embeddings via attention is a novel and impactful contribution, particularly for healthcare, where data sparsity is a common challenge.
2. Empirical Rigor: The experiments are comprehensive, showing consistent improvements in predictive performance and interpretability across multiple tasks and datasets. The results convincingly support the claims made in the paper.
Supporting Arguments
1. Well-Motivated Approach: The paper is well-situated in the literature, addressing limitations of existing methods like RNNs and GloVe in handling sparse medical data. The use of DAGs to encode hierarchical relationships is a logical and effective solution, aligning well with the structure of medical ontologies.
2. Thorough Evaluation: The authors evaluate GRAM on three tasks, including sequential diagnosis prediction and heart failure prediction, using two datasets with varying characteristics. The consistent performance gains, particularly for rare diseases, highlight the model's robustness and generalizability.
3. Interpretability: The qualitative analysis, including t-SNE visualizations and attention weight analysis, demonstrates that GRAM learns representations that align well with medical knowledge, making it more interpretable than baseline models.
Suggestions for Improvement
1. Clarification of Softmax in Equation 4: The use of Softmax for multivariate cross-entropy loss is not explicitly justified. While this is a standard approach, a brief explanation of its suitability for the task would strengthen the paper.
2. Embedding Dimensionality (m): The dimensionality of the embeddings (m) is not specified in the main text. Including this information would improve reproducibility and provide insights into the trade-offs between model complexity and performance.
3. Scalability Discussion: While the paper mentions the additional training time required by GRAM, a more detailed discussion of its scalability to larger datasets or DAGs with higher complexity would be valuable.
Questions for the Authors
1. How does the choice of embedding dimensionality (m) affect the model's performance and computational efficiency? Was there a systematic approach to selecting this hyperparameter?
2. Could the authors elaborate on why the MLP-based attention mechanism was chosen over simpler alternatives like dot product attention? Were there any trade-offs in terms of computational cost or performance?
3. How does GRAM perform when applied to other domains with hierarchical data structures, such as biology or finance? Would the approach generalize effectively beyond healthcare?
In summary, GRAM is a significant contribution to the field of healthcare predictive modeling, addressing critical challenges of data insufficiency and interpretability. With minor clarifications and additional scalability analysis, the paper would be even stronger. I recommend acceptance.