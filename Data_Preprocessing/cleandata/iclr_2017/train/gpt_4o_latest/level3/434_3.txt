The paper proposes a novel adaptation of batch normalization (BN) for Long Short-Term Memory (LSTM) networks, specifically applying it along the horizontal depth (hidden-to-hidden transitions) of recurrent neural networks (RNNs). The authors demonstrate that this approach, combined with proper initialization of BN parameters (notably gamma), improves optimization and generalization across various sequential tasks, including sequence classification, language modeling, and question answering. The paper claims faster convergence and better performance compared to standard LSTMs, supported by empirical results on datasets such as MNIST, Penn Treebank, and Text8.
Decision: Reject
While the paper introduces an interesting modification to LSTMs and provides comprehensive experimental results, the contribution is incremental, and the practical utility is limited due to minor performance gains and increased computational overhead. The lack of a detailed analysis of computational costs further weakens its case for acceptance.
Supporting Arguments:
1. Contributions and Novelty: The paper addresses a well-known challenge in RNN optimization by extending batch normalization to hidden-to-hidden transitions. This is a novel approach compared to prior unsuccessful attempts. However, the contribution is incremental, as it builds on existing techniques without fundamentally advancing the state of the art. The reliance on multiple "tricks" (e.g., specific gamma initialization, noise injection) raises concerns about robustness and generalizability.
   
2. Significance and Empirical Gains: While the method shows consistent improvements in convergence speed and generalization, the performance gains are modest and task-specific. The experiments are limited to character/pixel-level tasks, leaving out more impactful word-level or real-world applications. This restricts the broader applicability of the method.
3. Computational Overhead: The proposed method introduces additional complexity, such as per-time-step statistics and separate normalization for input and recurrent terms. However, the paper does not provide a wall-clock comparison or detailed analysis of the computational cost, making it difficult to assess the trade-off between performance gains and added overhead.
Suggestions for Improvement:
1. Address Computational Costs: Include a detailed comparison of training time (wall-clock) and memory usage between the proposed BN-LSTM and standard LSTMs. This would clarify the practical feasibility of the method.
   
2. Broader Task Evaluation: Extend experiments to word-level tasks (e.g., machine translation, sentiment analysis) to demonstrate the generalizability of the approach. This would strengthen the paper's significance.
3. Simplify the Approach: Investigate whether the reliance on multiple tricks (e.g., gamma initialization, noise injection) can be reduced or eliminated to improve robustness and ease of implementation.
4. Theoretical Insights: Provide a deeper theoretical analysis of why the proposed BN-LSTM works better than prior attempts, particularly in terms of gradient flow and optimization dynamics.
Questions for the Authors:
1. How does the computational overhead of BN-LSTM compare to standard LSTMs in terms of training time and memory usage?
2. Have you tested the method on word-level tasks or larger-scale datasets? If not, what are the anticipated challenges?
3. Can the reliance on specific initialization schemes (e.g., gamma = 0.1) and noise injection be avoided without degrading performance?
In summary, while the paper presents an interesting adaptation of batch normalization for LSTMs, the incremental nature of the contribution, modest empirical gains, and lack of computational cost analysis make it unsuitable for acceptance in its current form. Addressing the above concerns could significantly strengthen the work.