Review of "Intrinsic Fear: Avoiding Catastrophes in Deep Reinforcement Learning"
Summary of Contributions
This paper addresses the critical issue of safety in deep reinforcement learning (DRL), particularly focusing on the challenge of avoiding catastrophic states during training and deployment. The authors identify a fundamental problem with deep Q-networks (DQNs), termed the "Sisyphean curse," wherein agents periodically forget catastrophic experiences due to distributional shifts. To mitigate this, the paper introduces Intrinsic Fear, a novel reward-shaping mechanism that penalizes agents for entering states likely to lead to catastrophes. This is achieved by training a supervised danger model alongside the DQN, which predicts the probability of catastrophe within a defined "fear radius." The approach is evaluated on toy environments (Adventure Seeker, Cart-Pole) and the Atari game Seaquest, demonstrating improved safety and accelerated learning in certain cases.
Decision: Reject  
While the paper tackles an important problem and proposes an interesting heuristic, it lacks sufficient rigor and principled design to justify acceptance. The key reasons for this decision are:  
1. Insufficient Baseline Comparisons: The paper does not compare its approach against standard baselines like expected SARSA or other safety-focused RL methods, which limits the ability to contextualize the contribution.  
2. Unclear Design Choices: The decision to use a separate danger model instead of directly incorporating catastrophe signals into the Q-learning process is not well-motivated, raising concerns about the principled nature of the approach.  
3. Limited Empirical Support: While the experiments show some promise, the results are preliminary and lack robustness, particularly in more complex environments like Seaquest.
Supporting Arguments
1. Problem Importance: The paper addresses a critical issue in DRLâ€”ensuring safety in real-world applications where catastrophic failures can have severe consequences. The focus on avoiding catastrophic forgetting due to function approximation is well-motivated and highlights a genuine limitation of DQNs.  
2. Novelty: The idea of using a supervised danger model to shape rewards is novel and draws inspiration from intrinsic motivation literature. However, the implementation lacks theoretical grounding, and the choice of hyperparameters (e.g., fear radius, fear factor) appears ad hoc.  
3. Empirical Weaknesses: While the approach outperforms standard DQNs on toy problems, the lack of comparisons with memory-based methods or alternative reward-shaping techniques (e.g., prioritized replay buffers) weakens the empirical claims. Additionally, the results on Seaquest are inconclusive, with the method trading off reward for safety in a manner that is not well-analyzed.
Suggestions for Improvement
1. Baseline Comparisons: Include comparisons with expected SARSA and other safety-focused RL methods (e.g., risk-sensitive Q-learning, constrained policy optimization). This would help contextualize the contribution and demonstrate the relative advantages of the proposed approach.  
2. Principled Design: Provide a stronger justification for using a separate danger model instead of directly incorporating catastrophe signals into the Q-learning process. For example, could the danger signal be integrated into the Bellman update directly?  
3. Hyperparameter Sensitivity: Conduct a thorough analysis of the sensitivity of the approach to the fear radius, fear factor, and phase-in length. This would help clarify the robustness of the method across different environments.  
4. Broader Evaluation: Extend experiments to more complex environments and provide a detailed analysis of the trade-offs between safety and performance. For example, how does the method balance avoiding catastrophes with maximizing cumulative reward?  
5. Theoretical Insights: Formalize the notion of danger zones and provide theoretical guarantees (e.g., convergence properties or bounds on catastrophe rates) to strengthen the contribution.
Questions for the Authors
1. Why was expected SARSA not included as a baseline, given its relevance to addressing off-policy learning issues?  
2. How does the danger model handle false positives (safe states classified as dangerous) and false negatives (dangerous states classified as safe)? What impact do these errors have on the agent's performance?  
3. Could the danger signal be incorporated directly into the Q-function or policy update, rather than as a separate penalty?  
4. How does the fear radius interact with the stochasticity of the environment? For example, in highly stochastic environments, does the danger model become less effective?  
5. What are the computational overheads of maintaining and updating the danger model alongside the DQN?
In summary, while the paper addresses an important problem and proposes a creative solution, it falls short in terms of rigor, empirical validation, and theoretical grounding. Addressing these issues would significantly strengthen the contribution.