The paper introduces the Dynamic Neural Turing Machine (D-NTM), an extension of the Neural Turing Machine (NTM), incorporating learnable addressing and a curriculum learning strategy with hybrid discrete and continuous attention mechanisms. The authors aim to address the limitations of rigid memory access in the original NTM by introducing a trainable memory addressing scheme and discrete attention mechanisms. Empirical results demonstrate improvements over the original NTM on tasks like the Facebook bAbI dataset, sequential permuted MNIST, and algorithmic tasks such as copy and associative recall.
Decision: Reject
While the paper makes notable contributions, including the introduction of learnable addressing and curriculum learning with hybrid attention, there are significant weaknesses that prevent acceptance at this stage. The primary concerns are the weak NTM baseline used for comparison and the lack of clarity in certain sections, particularly Section 3, which hinders reproducibility and understanding.
Supporting Arguments:
1. Strengths:
   - The proposed D-NTM introduces innovative mechanisms, such as learnable addressing and discrete attention, which address key limitations of the original NTM.
   - The empirical evaluation on the Facebook bAbI tasks demonstrates the potential of the D-NTM, particularly with discrete attention, which outperforms continuous attention in tasks requiring precise memory retrieval.
   - The curriculum learning strategy for training discrete attention is a valuable contribution, improving performance for feedforward controllers.
   - Comprehensive comparisons between feedforward and recurrent controllers provide useful insights into the model's behavior.
2. Weaknesses:
   - The NTM baseline used for comparison has a 31% error rate on the bAbI tasks, which is significantly higher than prior work (20% error). This raises concerns about the hyperparameter settings and whether the improvements are due to the proposed methods or suboptimal baselines.
   - Section 3, which details the addressing mechanism, is difficult to follow due to dense mathematical notation and insufficient explanation. This lack of clarity makes it challenging for readers to reproduce the results or fully understand the contributions.
   - While the D-NTM shows improvements over the original NTM, it performs worse than other memory-based models, such as Memory Networks and Dynamic Memory Networks, on the bAbI tasks. The paper does not adequately address this performance gap.
Additional Feedback:
- The authors should provide a stronger baseline for comparison, ensuring that the hyperparameters for the original NTM are optimized.
- Section 3 should be rewritten with clearer explanations and visual aids (e.g., diagrams) to enhance understanding of the addressing mechanism.
- The paper would benefit from a broader discussion of the limitations of the D-NTM compared to other memory-based models, along with suggestions for future improvements.
- The empirical results on the bAbI tasks should include comparisons with state-of-the-art models to contextualize the performance of the D-NTM.
Questions for the Authors:
1. Can you clarify the hyperparameter settings used for the NTM baseline? Were they consistent with prior work?
2. How does the D-NTM compare to state-of-the-art models on the bAbI tasks, such as Memory Networks or Dynamic Memory Networks, in terms of both accuracy and computational efficiency?
3. Could you provide more detailed explanations or visualizations of the addressing mechanism in Section 3 to improve clarity?
In summary, while the paper presents promising ideas, the weaknesses in baseline comparison, clarity, and performance relative to state-of-the-art models warrant rejection. Addressing these issues could significantly strengthen the paper for future submission.