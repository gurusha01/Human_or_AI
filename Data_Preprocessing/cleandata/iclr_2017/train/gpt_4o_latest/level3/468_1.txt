Review of the Paper
Summary of Contributions
The paper proposes a novel network quantization method aimed at compressing non-pruned neural network parameters to reduce storage requirements. The key contributions include the introduction of Hessian-weighted k-means clustering for minimizing performance loss due to quantization, and the formulation of the network quantization problem as an entropy-constrained scalar quantization (ECSQ) problem. The authors present two heuristic solutions for ECSQ—uniform quantization and an iterative algorithm—and demonstrate their effectiveness in compressing neural networks such as LeNet, ResNet, and AlexNet. The method achieves significant compression ratios with minimal accuracy loss, and the paper also explores the use of second moment estimates from the Adam optimizer as a computationally efficient alternative to Hessian computation.
Decision: Reject
While the paper addresses a relevant and well-motivated problem, the decision to reject is based on two primary concerns: (1) limited novelty, as the proposed method marginally extends prior work (Han et al., 2015), and (2) incomplete experimental evaluation, particularly the lack of a direct comparison with Han et al. (2015) on ResNet in Table 1. These issues undermine the paper's contribution to the field.
Supporting Arguments
1. Novelty Concerns: The proposed method builds on Han et al. (2015) by introducing Hessian-weighted clustering and ECSQ-based quantization. However, the novelty is incremental, as the core idea of k-means clustering for network quantization is not new. While the use of Hessian weighting and entropy constraints is interesting, these extensions lack sufficient theoretical or empirical justification to establish their significance over existing methods.
   
2. Experimental Gaps: The experimental results demonstrate good compression ratios with minimal accuracy loss, but the lack of a direct comparison with Han et al. (2015) on ResNet in Table 1 is a critical omission. Without this comparison, it is difficult to assess whether the proposed method offers a meaningful improvement over prior work. Additionally, the unclear novelty of Figure 1 and the impact of approximating the Hessian matrix with a diagonal matrix remain unresolved.
Suggestions for Improvement
1. Clarify Novelty: Clearly distinguish the proposed method from prior work, particularly Han et al. (2015). Highlight the unique contributions of Hessian-weighted clustering and ECSQ-based quantization, and provide theoretical or empirical evidence to demonstrate their advantages.
   
2. Expand Experiments: Include a direct comparison with Han et al. (2015) on ResNet in Table 1 to validate the proposed method's performance. Additionally, provide more detailed results on the impact of Hessian approximation and the use of second moment estimates as an alternative.
3. Improve Paper Structure: Reduce redundancy in the text, particularly in the introduction and methodology sections, to make room for a more detailed experimental analysis. Ensure that the experiments section starts earlier in the paper.
4. Clarify Figures: Clearly indicate whether the procedure in Figure 1 is novel or derived from existing literature. If it is novel, explain its significance; if not, provide proper attribution.
5. Address Typos: Correct the minor typos identified throughout the paper to improve readability and professionalism.
Questions for the Authors
1. How does the proposed method compare to Han et al. (2015) on ResNet in terms of compression ratio and accuracy loss? Can you provide quantitative results in Table 1?
2. What is the empirical impact of approximating the Hessian matrix with a diagonal matrix on both compression performance and accuracy?
3. Is the procedure in Figure 1 novel, or is it adapted from existing literature? If novel, what specific contribution does it make?
While the paper addresses an important problem and demonstrates promising results, addressing the concerns outlined above is necessary to establish its originality and significance within the field.