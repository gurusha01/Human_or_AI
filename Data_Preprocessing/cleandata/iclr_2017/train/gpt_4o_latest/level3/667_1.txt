Review of the Paper
Summary of Contributions
The paper presents GRAM, a graph-based attention model that addresses two key challenges in healthcare predictive modeling: data insufficiency and interpretability. By leveraging hierarchical information from medical ontologies, GRAM adaptively combines ancestor embeddings via an attention mechanism to create robust and interpretable representations of medical concepts. The model is evaluated on three predictive tasks, demonstrating improved performance, particularly for rare diseases and small datasets. The authors also provide qualitative insights into the interpretability of GRAM's learned representations and attention behaviors, showcasing its alignment with medical knowledge.
Decision: Reject
While the paper tackles an important problem and demonstrates promising results, several methodological and presentation issues remain unresolved. These issues hinder the clarity and scientific rigor necessary for acceptance at this stage.
Supporting Arguments for Decision
1. Methodological Concerns:
   - The representation of patient visits as a sum of medical codes is not well-justified. Alternatives such as averaging or learned weighted averages could provide more nuanced representations. The authors should clarify why summation was chosen and whether other approaches were tested.
   - The paper does not address whether the basic embeddings are fine-tuned during training. Fine-tuning could significantly impact performance, and its absence or presence should be explicitly discussed and evaluated.
2. Presentation Issues:
   - Figures, particularly Figure 2, are unclear and do not effectively communicate the significance of the results. The t-SNE plots, while visually appealing, lack quantitative evaluation to substantiate claims of interpretability.
   - The novelty of the proposed method for the ICLR community is not sufficiently articulated. While the use of medical ontologies is relevant, the paper does not clearly differentiate GRAM from existing graph-based or attention-based models in the literature.
3. Results and Rigor:
   - The reported performance improvements, while promising, are not consistently significant across all tasks. For example, the AUC improvement for heart failure prediction is marginal compared to RNN+. The authors should provide statistical significance tests to validate their claims.
   - Scalability concerns are briefly mentioned but not thoroughly addressed. The 50% increase in training time and epochs could be a limitation in real-world applications, especially in large-scale healthcare systems.
Suggestions for Improvement
1. Methodology:
   - Justify the choice of summation for visit representation and compare it with alternatives like averaging or learned weighted averages.
   - Clearly state whether the basic embeddings are fine-tuned during training and evaluate the impact of fine-tuning on performance.
2. Results and Analysis:
   - Include statistical significance tests to validate performance improvements.
   - Provide quantitative metrics to support claims of interpretability (e.g., alignment scores with ontology structure).
3. Figures and Presentation:
   - Improve the clarity of figures, particularly Figure 2, and ensure they effectively convey the results.
   - Highlight the novelty of GRAM in the context of existing graph-based and attention-based models.
4. Scalability:
   - Discuss the implications of the increased training time and epochs in more detail. Could optimizations reduce this overhead?
Questions for the Authors
1. Why was summation chosen for visit representation? Did you evaluate alternatives like averaging or learned weighted averages?
2. Are the basic embeddings fine-tuned during training? If not, why? If yes, how does fine-tuning impact performance?
3. Can you provide quantitative metrics to substantiate claims of interpretability beyond t-SNE visualizations?
4. How does GRAM compare to other graph-based models in terms of novelty and contributions to the ICLR community?
In conclusion, the paper addresses a critical problem in healthcare predictive modeling and shows potential. However, methodological gaps, unclear presentation, and insufficient novelty articulation prevent it from meeting the standards for acceptance. Addressing these issues could significantly strengthen the paper for future submissions.