Review
Summary of Contributions
The paper introduces a novel method for training Generative Adversarial Networks (GANs) by unrolling the inner optimization of the discriminator in the min-max game and treating it as a computational graph. This approach aims to stabilize GAN training and address common issues such as mode collapse and oscillatory dynamics. The authors provide clear explanations and strong theoretical justifications for their method, demonstrating its potential to improve mode coverage and diversity in generated samples. Experiments on synthetic and real datasets illustrate the method's effectiveness in stabilizing training and reducing mode collapse, particularly in challenging scenarios like recurrent generator architectures. The paper also explores the trade-off between computational cost and stability by varying the number of unrolling steps.
Decision: Reject
While the paper presents an interesting and well-motivated approach, it falls short in its experimental rigor and evaluation metrics, which are critical for assessing the practical impact of the proposed method. The lack of convincing quantitative results and reliance on suboptimal metrics weaken the overall contribution.
Supporting Arguments
1. Novelty and Motivation: The unrolling approach is novel in the context of GANs and is well-motivated, building on prior work in differentiable optimization. However, similar ideas have been explored in non-GAN contexts, which slightly diminishes the originality.
   
2. Experimental Weaknesses: 
   - The first experiment (finding z for training examples) is criticized as tangential to the core problem of diversity or mode-dropping. It primarily reflects smoother optimization rather than meaningful improvements in GAN performance.
   - The second experiment (mean pairwise distance) uses pixel-space distances, which can favor models generating low-quality or "garbage" samples. This metric is not a reliable indicator of diversity or sample quality.
   - The paper lacks standard quantitative metrics such as Inception Score or Fr√©chet Inception Distance (FID), which are widely used to evaluate GANs. These metrics would provide a sanity check for the quality of generated samples.
   - The authors do not test their method on the tri-MNIST dataset, which could serve as a better benchmark for mode coverage and diversity.
3. Computational Cost: The method introduces significant computational overhead, which scales linearly with the number of unrolling steps. While the authors acknowledge this trade-off, they do not provide sufficient analysis of the practical feasibility of their approach in large-scale settings.
Suggestions for Improvement
1. Additional Quantitative Metrics: Incorporate standard metrics like Inception Score, FID, or semi-supervised learning (SSL) performance to provide a more comprehensive evaluation of the method.
2. Tri-MNIST Experiment: Test the method on the tri-MNIST dataset to better demonstrate its ability to cover diverse modes.
3. Ablation Studies: Perform ablation studies to isolate the impact of unrolling steps and the second gradient term on performance.
4. Computational Efficiency: Explore ways to reduce the computational cost of unrolling, such as approximations or partial unrolling.
5. Broader Comparisons: Compare the proposed method against other state-of-the-art techniques for stabilizing GAN training, such as spectral normalization or Wasserstein GANs.
Questions for Authors
1. How does the method perform on larger and more complex datasets, such as ImageNet or CelebA-HQ? 
2. Can the authors provide a detailed analysis of the trade-off between computational cost and performance for different numbers of unrolling steps?
3. How does the method compare to other stabilization techniques in terms of both performance and computational efficiency?
In conclusion, while the paper introduces a promising approach, the lack of rigorous and convincing experimental validation prevents it from making a strong case for acceptance. Addressing the outlined weaknesses could significantly strengthen the paper.