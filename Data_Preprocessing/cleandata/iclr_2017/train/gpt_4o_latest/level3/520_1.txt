Review of the Paper
Summary of Contributions
This paper proposes an extension of PixelCNN to generate images conditioned on both text and spatially-structured constraints like segmentation masks and keypoints. The authors demonstrate the model's ability to synthesize interpretable and controllable images across three datasets: MS-COCO, CUB, and MHP. The paper highlights the advantages of autoregressive models, such as stability during training and the ability to compute likelihoods, while presenting qualitative and quantitative results to establish baselines for text- and structure-conditional image synthesis. The proposed approach is positioned as a complement to GAN-based methods, emphasizing its simplicity and robustness. The results show reasonable diversity and adherence to spatial constraints, though some limitations in color accuracy and resolution are noted.
Decision: Accept
The paper is relevant to the ICLR community, addressing the important problem of controllable image synthesis. While the technical novelty is incremental, the work provides a meaningful extension of PixelCNN and establishes baselines for future research. The qualitative results demonstrate reasonable diversity and adherence to constraints, and the paper's discussion of autoregressive models versus GANs adds valuable insights to the field. These contributions outweigh the limitations, making the paper a suitable candidate for acceptance.
Supporting Arguments
1. Relevance and Contributions: The paper tackles a timely and important problem in deep image synthesis, presenting a novel application of PixelCNN to text- and structure-conditional generation. Its focus on interpretability and control aligns well with current research trends.
2. Complementarity to GANs: While the paper does not claim superiority over GANs, it provides a compelling case for the advantages of autoregressive models, such as stable training and likelihood computation. This perspective enriches the discussion in the field.
3. Baseline Establishment: The paper provides quantitative baselines for three datasets, which will be valuable for future research and comparisons.
Suggestions for Improvement
1. Comparison with GANs: The discussion of the pros and cons of PixelCNN versus GANs could be expanded. For example, the paper could delve deeper into the trade-offs between sampling efficiency and image quality, as well as the implications for high-resolution image synthesis.
2. Sampling Complexity: The O(N) sampling complexity of PixelCNN is a known limitation, and its impact on the low resolution (32x32) experiments should be analyzed more thoroughly. Including training and generation time comparisons would strengthen the paper.
3. More Visualizations: The paper provides only three examples of generated images. Including more visualizations, especially for failure cases, would improve the qualitative analysis and give readers a clearer understanding of the model's strengths and weaknesses.
4. Color Accuracy: While the paper acknowledges issues with color mistakes, further analysis of why these errors occur and how they might be mitigated would be valuable.
Questions for the Authors
1. Can you provide more details on why the model struggles with certain color constraints (e.g., white birds appearing with dark wings)? Is this due to dataset biases or limitations in the model architecture?
2. How does the model scale to higher resolutions (e.g., 128x128 or 256x256)? Have you explored strategies to mitigate the inefficiency of PixelCNN's sampling process?
3. Could you elaborate on how the proposed model might be extended to generate segmentation masks jointly with images, as suggested in the discussion section?
Overall, this paper makes a meaningful contribution to the field of controllable image synthesis, and its relevance and insights justify its acceptance. Addressing the suggested improvements would further enhance its impact.