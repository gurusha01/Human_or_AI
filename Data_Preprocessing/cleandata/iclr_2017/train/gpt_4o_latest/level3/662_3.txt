Review
Summary of Contributions
This paper introduces the Dynamic Neural Turing Machine (D-NTM), an extension of the Neural Turing Machine (NTM) that incorporates a trainable memory addressing scheme. The D-NTM partitions memory cells into content and address vectors, enabling the model to learn nonlinear location-based addressing strategies. The paper explores both continuous/differentiable and discrete/non-differentiable addressing mechanisms, with a focus on episodic question-answering tasks (e.g., Facebook bAbI) and algorithmic tasks like sequential MNIST and associative recall. The authors also propose a curriculum learning strategy to mitigate training challenges with discrete attention. While the D-NTM outperforms baseline NTMs and LSTMs, its performance lags behind memory networks like MemN2N and DMN+.
Decision: Reject
The paper introduces an interesting extension to NTMs, but several limitations prevent its acceptance:
1. Performance Gap: The D-NTM underperforms compared to state-of-the-art models (e.g., MemN2N, DMN+) on key benchmarks like bAbI tasks.
2. Reproducibility Concerns: The code is not provided, making it difficult to verify results or extend the work.
3. Limited Scope of Experiments: The experiments are restricted to a narrow set of tasks, with insufficient exploration of real-world applications.
Supporting Arguments
1. Motivation and Novelty: The paper is well-motivated, addressing the limitations of linear location-based addressing in NTMs. The introduction of discrete attention and curriculum learning for training discrete mechanisms is novel and valuable.
2. Experimental Results: The results demonstrate that D-NTM outperforms NTMs and LSTMs on bAbI tasks, sequential MNIST, and toy tasks. However, the performance gap with MemN2N and DMN+ raises concerns about the practical utility of the proposed model.
3. Scientific Rigor: The paper provides detailed descriptions of the architecture, training strategies, and regularization techniques. However, the absence of code and limited task diversity undermine the rigor of the empirical evaluation.
Suggestions for Improvement
1. Provide Code: Sharing the implementation would greatly enhance reproducibility and allow the community to build upon this work.
2. Expand Experiments: Evaluate the D-NTM on a broader range of real-world tasks, such as text summarization, machine translation, or visual question answering, to demonstrate its generalizability.
3. Address Performance Gap: Investigate why D-NTM underperforms compared to MemN2N and DMN+. For example, explore hybrid approaches that combine explicit fact storage with learnable addressing.
4. Clarify Missing Footnote: Ensure all references and footnotes are complete to avoid confusion.
5. Optimize Training: The paper notes challenges with training feedforward controllers and discrete attention. Further analysis or alternative strategies to address these issues would strengthen the contribution.
Questions for the Authors
1. Why does the D-NTM perform significantly worse than MemN2N and DMN+ on bAbI tasks? Could this be due to the complexity of learning to write and manipulate memory?
2. How does the proposed model scale to larger datasets or tasks with longer input sequences? Are there memory or computational constraints?
3. Could the curriculum learning strategy for discrete attention be applied to other memory-based models, such as MemN2N or DMN+?
4. Why was the code not included? Are there plans to release it in the future?
In conclusion, while the D-NTM introduces promising ideas, its current limitations in performance, reproducibility, and scope of evaluation make it unsuitable for acceptance at this time.