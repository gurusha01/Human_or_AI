Review
Summary of Contributions
This paper introduces a novel extension to attention mechanisms in neural language models by proposing the use of separate key, value, and predict vectors, termed the Key-Value-Predict Attention mechanism. This approach addresses the overloaded functionality of standard attention mechanisms, where a single output vector is used for multiple roles. The authors demonstrate that their model outperforms existing memory-augmented neural language models on two corpora: a subset of Wikipedia articles and the Children's Book Test (CBT). Additionally, the paper presents a significant insight that short attention spans (e.g., the last five tokens) are sufficient for effective language modeling. This observation leads to the development of a simpler N-gram RNN, which achieves performance comparable to more complex memory-augmented models. The paper is well-supported by thorough experimental analysis, including comparisons with state-of-the-art models and ablation studies.
Decision: Accept
The paper makes a strong case for acceptance due to its novel contributions to attention mechanisms, its insightful findings on the sufficiency of short attention spans, and its rigorous experimental evaluation. The introduction of the Key-Value-Predict Attention mechanism is a meaningful advancement in neural language modeling, and the simplicity and effectiveness of the N-gram RNN provide a valuable practical takeaway for the community.
Supporting Arguments
1. Novelty and Significance: The Key-Value-Predict Attention mechanism addresses a critical limitation in standard attention models by explicitly separating the roles of key, value, and prediction vectors. This functional disentanglement is a novel and well-motivated contribution, supported by prior work (e.g., Miller et al., Ba et al., Reed & de Freitas, Gulcehre et al.).
2. Experimental Rigor: The authors provide comprehensive experimental results on two datasets, demonstrating that their proposed models outperform baseline and state-of-the-art memory-augmented neural language models. The use of perplexity and accuracy metrics ensures robust evaluation.
3. Practical Insight: The finding that short attention spans suffice for language modeling is both surprising and impactful. The introduction of the N-gram RNN, which leverages this insight, offers a simpler yet competitive alternative to more complex architectures.
4. Clarity and Responsiveness: The paper is well-written and clearly structured, with detailed explanations of methods and results. The authors have also adequately addressed pre-review questions, demonstrating their responsiveness and commitment to clarity.
Suggestions for Improvement
1. Related Work: While the paper references several key works, it would benefit from explicitly discussing additional related studies, such as Ba et al. (2016), Reed & de Freitas (2015), and Gulcehre et al. (2016). This would further contextualize the contributions and highlight connections to prior research.
2. Long-Range Dependencies: The authors note the difficulty of training models to leverage long-range dependencies but do not explore potential solutions in depth. Future work could investigate methods to encourage attention over longer histories, such as curriculum learning or explicit architectural constraints.
3. Broader Applications: While the paper focuses on language modeling, it would be valuable to discuss how the proposed Key-Value-Predict Attention mechanism could generalize to other sequence modeling tasks, such as machine translation or summarization.
Questions for the Authors
1. Could you elaborate on why the Key-Value-Predict Attention mechanism struggles to utilize long-range dependencies? Are there specific architectural or optimization challenges that you observed during training?
2. How does the N-gram RNN compare to the Key-Value-Predict model in terms of computational efficiency and training time?
3. Did you experiment with varying the number of tokens used in the N-gram RNN (e.g., beyond three or five)? If so, how did this impact performance?
Overall, this paper makes a valuable contribution to the field of neural language modeling and provides both theoretical and practical insights. With minor improvements to related work and a deeper exploration of long-range dependencies, it could have an even broader impact.