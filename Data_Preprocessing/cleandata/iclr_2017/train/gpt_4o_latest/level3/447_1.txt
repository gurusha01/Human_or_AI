Review of the Paper
Summary of Contributions
The paper introduces a novel simulator and synthetic question-answering tasks designed to explore how agents can improve their performance by asking questions and receiving feedback. It addresses the problem in both offline supervised and online reinforcement learning (RL) settings, demonstrating that question-asking improves the agent's learning outcomes. The study focuses on three task types—Question Clarification, Knowledge Operation, and Knowledge Acquisition—where user feedback benefits the agent. The authors validate their approach using both simulated data and real-world data collected via Amazon Mechanical Turk. The paper represents a promising first step toward developing dialogue agents capable of learning through unstructured user interactions.
The paper is well-written, with clear descriptions of the tasks, models, and experimental setups. It provides a thorough analysis of the benefits of question-asking in various scenarios and explores the trade-offs between asking questions and the associated costs in RL settings. The inclusion of both vanilla-MemN2N and Cont-MemN2N models adds depth to the analysis, although their distinct contributions require further clarification.
Decision: Accept
The paper should be accepted due to its novel contribution to the field of dialogue systems, its rigorous experimental evaluation, and its potential to inspire future research on interactive learning in conversational agents. The key strengths include the introduction of a new simulator and tasks, the exploration of both offline and online learning paradigms, and the validation of results using real-world data.
Supporting Arguments
1. Novelty and Relevance: The paper addresses an underexplored area in dialogue systems—learning through interaction by asking questions. This is a significant step toward creating more adaptive and intelligent conversational agents.
2. Scientific Rigor: The experiments are well-designed, with comprehensive evaluations across multiple settings (offline supervised, online RL, simulated, and real-world data). The results consistently support the paper's claims, demonstrating the benefits of question-asking.
3. Clarity and Organization: The paper is well-structured, with clear explanations of the tasks, models, and evaluation metrics. The inclusion of both synthetic and real-world data strengthens the generalizability of the findings.
Additional Feedback for Improvement
1. Clarification on Model Contributions: The paper uses both vanilla-MemN2N and Cont-MemN2N models but does not clearly articulate their distinct contributions. A more detailed comparison of their roles and performance would enhance the paper's clarity.
2. Misspelled Words Distribution: The distribution of misspelled words in the Question Clarification setting is unclear. The authors should explain how this impacts task difficulty and whether it introduces bias in the results.
3. Result Discrepancy: A discrepancy is noted in the results for Task 2 on Page 10 compared to Tables 2 and 4. The authors should address this inconsistency to ensure the validity of the findings.
4. Unexplored Factors: The effect of smaller or no conversational history on performance is not explored. This could provide additional insights into the robustness of the proposed approach.
5. Figure 5 Observations: The accuracy drop for the "good student" in Task 6 when it stops asking questions and the unexpectedly high accuracy of the "poor student" in Task 2 without asking questions require further explanation.
6. Correction in Figure 1: The last sentence in Task 2 AQ should have a negative response instead of a positive one. This needs to be corrected for consistency.
Questions for the Authors
1. How does the distribution of misspelled words in the Question Clarification setting affect the model's ability to generalize to real-world scenarios with varied errors?
2. Can you provide a more detailed explanation of the discrepancy in Task 2 results between Page 10 and Tables 2 and 4?
3. Why does the "poor student" achieve unexpectedly high accuracy in Task 2 without asking questions, and what does this imply about the model's learning process?
4. Have you considered the impact of varying the size of the conversational history on the model's performance? If so, what were the findings?
Overall, this paper makes a valuable contribution to the field and is recommended for acceptance after addressing the above points.