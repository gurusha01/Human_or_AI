Review
Summary of Contributions
This paper presents a novel weakly supervised, end-to-end neural network model for inducing programs to answer natural language queries on database tables. The approach builds on the Neural Programmer framework, enhancing its objective function to handle real-world datasets with weak supervision. The model is evaluated on the WikiTableQuestions dataset, achieving 34.2% accuracy with a single model and 37.7% accuracy using an ensemble of 15 models, which is competitive with state-of-the-art traditional semantic parsers. The paper highlights the model's ability to operate without domain-specific grammars or annotations, relying solely on question-answer pairs for training. The authors also demonstrate the importance of regularization techniques, such as dropout and weight decay, in mitigating overfitting on this small dataset. This work represents a significant step toward integrating neural networks with discrete operations for semantic parsing tasks.
Decision: Reject
While the paper makes a meaningful contribution to the field of natural language interfaces for tables, it falls short in certain critical areas. The key reasons for this decision are the lack of sufficient error analysis and the unanswered questions regarding the model's headroom and oracle performance. These gaps limit the paper's ability to provide a comprehensive understanding of its limitations and potential.
Supporting Arguments
1. Strengths:
   - The paper addresses a challenging and important problem by proposing a weakly supervised, end-to-end neural network model for semantic parsing.
   - The ensemble approach achieves performance on par with traditional semantic parsers, showcasing the potential of neural networks for this task.
   - The use of strong regularization techniques is well-motivated and empirically validated, demonstrating their impact on mitigating overfitting.
2. Weaknesses:
   - The reported accuracy (37.7%) is relatively low compared to other NLP tasks, and the paper does not sufficiently explore the reasons for this gap. A deeper error analysis could provide insights into specific failure cases and guide future improvements.
   - The discussion of headroom and oracle performance is limited. For instance, the oracle performance of 50.5% for the ensemble suggests significant room for improvement, but the paper does not explore how this could be achieved.
   - The paper does not adequately address the scalability of the proposed model to larger datasets or more complex tasks. This is particularly important given the small size of the WikiTableQuestions dataset.
Suggestions for Improvement
1. Error Analysis: Conduct a detailed error analysis to identify common failure modes. For example, are errors primarily due to incorrect column selection, operation selection, or overfitting to specific table structures? This would provide actionable insights for improving the model.
2. Oracle Performance: Investigate why the ensemble's oracle performance is significantly higher than its actual accuracy. Could more sophisticated ensemble techniques or additional training data bridge this gap?
3. Comparison with Other Tasks: Provide a discussion on why the model's accuracy is lower than typical NLP tasks. Is this due to the inherent difficulty of the dataset, the weak supervision signal, or limitations in the model architecture?
4. Scalability: Address the scalability of the model to larger datasets or more complex table structures. For instance, how does the model perform as the number of rows or columns increases?
5. Ablation Studies: Expand the ablation studies to include other model components, such as the impact of different operations or the choice of RNN architectures.
Questions for the Authors
1. What are the primary sources of error in the model's predictions? Could you provide examples of failure cases?
2. How does the model handle ambiguity in questions or tables with multiple valid answers? Is this a significant source of error?
3. Could the ensemble approach be improved with more sophisticated combination techniques, such as weighted averaging or stacking?
4. What is the potential for transfer learning in this context? For example, could pre-trained embeddings or models trained on related tasks improve performance?
5. How does the model's performance scale with additional training data? Would a larger dataset significantly reduce overfitting?
In conclusion, while the paper introduces a promising approach to semantic parsing with weak supervision, it requires more thorough analysis and discussion to fully understand its limitations and potential. Addressing these issues would strengthen the paper's contributions and make it more suitable for acceptance.