Review
The paper proposes strategies for selecting a small subset of target vocabulary per source sentence in neural machine translation (NMT) systems, achieving significant speed-ups in decoding and training while maintaining comparable accuracy. The authors extend prior work by Mi et al. (2016) by exploring additional selection methods, such as bilingual embeddings, word alignments, phrase pairs, and SVM classifiers, and by conducting a more detailed analysis of speed and accuracy trade-offs. The results demonstrate up to a 90% reduction in decoding time on CPUs and a 25% reduction in training time on GPUs, with negligible impact on translation quality. The contributions are validated on two language pairs (English-German and English-Romanian) using standard benchmarks, achieving state-of-the-art or near state-of-the-art performance.
Decision: Reject
While the paper presents a thorough and well-executed study with clear practical value, it lacks sufficient novelty to warrant acceptance at a general AI conference. The work primarily builds on Mi et al. (2016) by adding incremental improvements and additional experiments, rather than introducing fundamentally new ideas. Furthermore, the focus on NMT-specific techniques and datasets makes the paper more appropriate for a specialized NLP venue.
Supporting Arguments:
1. Strengths:
   - The problem of improving NMT efficiency is well-motivated, and the paper is positioned effectively within the existing literature.
   - The experimental setup is rigorous, with detailed comparisons of multiple selection strategies and comprehensive evaluations on speed and accuracy trade-offs.
   - The results are convincing, demonstrating significant practical benefits for real-world applications, particularly in resource-constrained settings.
2. Weaknesses:
   - The paper's primary contribution lies in extending Mi et al. (2016) with additional strategies and experiments, which, while valuable, does not represent a substantial leap in innovation.
   - The focus on vocabulary selection for NMT limits the broader applicability of the work, making it less relevant for a general AI audience.
Suggestions for Improvement:
1. Highlight Novelty: To strengthen the paper, the authors could emphasize any unique theoretical insights or methodological contributions that distinguish their work from prior studies.
2. Broaden Scope: Consider exploring the applicability of the proposed methods to other sequence-to-sequence (seq2seq) tasks beyond machine translation, such as text summarization or dialogue generation, to appeal to a wider audience.
3. Efficiency Gains: While the paper demonstrates speed-ups, it would be helpful to provide a more detailed breakdown of computational costs (e.g., encoder vs. decoder contributions) and discuss potential optimizations for encoder architectures.
4. Clarity on SVMs: The SVM-based selection method is interesting but computationally expensive. A deeper analysis of its trade-offs compared to simpler methods like word alignments would be valuable.
Questions for the Authors:
1. How do the proposed methods generalize to other seq2seq tasks beyond NMT? Have you explored their performance in different domains or datasets?
2. Could you provide more details on the computational cost of training and evaluating the SVM-based selection method? How does it scale with larger vocabularies or datasets?
3. The paper mentions that adding frequent words to the selection strategies had little impact on BLEU scores. Could you elaborate on why this might be the case, especially in comparison to Mi et al. (2016)?
In summary, while the paper is well-executed and provides practical insights, its incremental nature and narrow focus make it better suited for an NLP-specific venue rather than a general AI conference.