Review of the Paper
Summary of Contributions
This paper presents a novel end-to-end neural architecture for machine comprehension, specifically targeting the Stanford Question Answering Dataset (SQuAD) and MSMARCO datasets. The proposed model combines match-LSTM, which computes attention between passage and question words, with Pointer Networks (Ptr-Net), which are used to predict answer tokens. Two variants of the model are introduced: the sequence model, which selects answer tokens sequentially, and the boundary model, which predicts the start and end positions of the answer span. The boundary model achieves state-of-the-art performance on the MSMARCO dataset and competitive results on SQuAD. The paper also provides insightful analyses, including the impact of answer length and question types on performance, as well as visualizations of attention mechanisms.
Decision: Accept
The paper makes a strong case for acceptance due to its significant contributions to the field of machine comprehension. The proposed model demonstrates clear improvements over baseline methods, avoids reliance on hand-crafted features, and provides valuable insights into model behavior. However, there are areas where the paper could be improved, particularly in terms of ablation studies and deeper analysis of design choices.
Supporting Arguments
1. Novelty and Performance: The integration of match-LSTM and Pointer Networks is innovative, and the boundary model achieves state-of-the-art results on MSMARCO while significantly outperforming baseline methods on SQuAD. The results are well-supported by empirical evidence, including detailed comparisons with existing approaches.
   
2. Motivation and Placement in Literature: The paper is well-motivated, addressing the limitations of prior models that rely on candidate answers or single-token outputs. It is positioned effectively within the literature, building on prior work on match-LSTM and Pointer Networks.
3. Insightful Analyses: The paper provides a thorough analysis of model performance across different question types and answer lengths, as well as visualizations of attention mechanisms. These insights are valuable for understanding the strengths and weaknesses of the proposed approach.
Suggestions for Improvement
1. Ablation Studies: The paper lacks a quantitative analysis of the impact of the attention mechanism in match-LSTM and the answer pointer layer. Including such studies would strengthen the claims about the importance of these components.
2. Performance Gaps: The authors should provide an explanation for the large performance gap between the sequence and boundary models in the answer pointer layer. This would help clarify the advantages of the boundary model.
3. Reasoning Types: The paper does not analyze performance variation across different reasoning types (e.g., multi-sentence reasoning). A deeper exploration of this aspect would provide a more comprehensive evaluation of the model.
4. Design Choices: The reasoning behind repeating activations across dimensions in Equation 2 instead of learning distinct activations is unclear. Clarifying this design decision would improve the paper's transparency.
5. Ensemble Model: The Bi-Ans-Ptr variant, which improves performance, is not included in the ensemble model. The authors should discuss why this choice was made and whether including it could further improve results.
6. Comparison with DCR: The discussion and comparison of the DCR model in Table 2 are insufficient. A more detailed analysis would provide better context for the proposed model's performance.
Questions for the Authors
1. Can you provide a quantitative analysis of the impact of the attention mechanism in match-LSTM and the answer pointer layer?
2. What is the reason for the significant performance gap between the sequence and boundary models? Could this be due to differences in how the models handle longer answers?
3. Why was the Bi-Ans-Ptr variant excluded from the ensemble model? Would including it improve the ensemble's performance?
4. Could you provide more details on how the model performs across different reasoning types, particularly for multi-sentence reasoning and "why" questions?
5. What motivated the design choice of repeating activations across dimensions in Equation 2? Have you considered alternatives?
Conclusion
This paper makes a substantial contribution to machine comprehension by introducing a novel model that achieves state-of-the-art results on a challenging dataset. While there are areas for improvement, particularly in terms of ablation studies and analysis of design choices, the strengths of the paper outweigh its weaknesses. I recommend acceptance with minor revisions to address the aforementioned points.