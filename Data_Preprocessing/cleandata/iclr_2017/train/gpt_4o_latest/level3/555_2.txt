Review of the Paper
Summary of Contributions
This paper explores the challenge of preselecting deep learning model architectures for new domains using a meta-learning-based ranking algorithm. The authors propose a novel approach that leverages topological features of neural networks and the transformations in weights, biases, and activation functions during early training to rank architectures by their predicted performance. The study evaluates over 11,000 feed-forward deep neural networks (DNNs) across 13 tabular datasets, offering insights into architecture transferability, the utility of parallel layers, and the feasibility of early-stage performance prediction. The paper also introduces a systematic architecture generation method and provides an extensive analysis of meta-features for architecture ranking. While the results are preliminary, the authors claim that their findings lay the groundwork for further exploration of DNN architecture spaces.
Decision: Reject
The paper presents an interesting and promising direction for meta-learning and architecture ranking, but it is not yet ready for publication. The primary reasons for this decision are:
1. Limited Scope of Architectures: The study focuses exclusively on feed-forward DNNs, neglecting task-specific architectures such as CNNs and RNNs, which are critical for many real-world applications.
2. Fixed Hyperparameters: The use of fixed hyperparameters, particularly the learning rate schedule, undermines the generalizability and validity of the conclusions about architecture performance.
3. Neglect of Data Size Effects: The experiments do not account for the impact of training data size, a crucial factor in determining the appropriate model scale and architecture.
Supporting Arguments
1. Architectural Scope: By excluding CNNs, RNNs, and other specialized architectures, the study limits its applicability to domains where feed-forward DNNs are not optimal. This omission weakens the claim of generalizability across domains.
2. Hyperparameter Constraints: Fixed hyperparameters introduce a significant confounding variable, as the performance of DNNs is highly sensitive to hyperparameter tuning. This limitation calls into question the reliability of the proposed ranking algorithm.
3. Data Size Oversight: The paper does not explore how varying dataset sizes influence architecture performance, which is a critical consideration for practical deployment in diverse domains.
Suggestions for Improvement
1. Expand the Scope: Future work should include task-specific architectures such as CNNs for image data and RNNs/LSTMs for sequential data. This would significantly enhance the applicability and impact of the study.
2. Incorporate Hyperparameter Tuning: The authors should explore methods for automatic hyperparameter optimization, such as Bayesian optimization or grid search, to ensure fair comparisons across architectures.
3. Address Data Size Effects: Include experiments that vary the training dataset size to analyze its impact on architecture performance and scalability.
4. Revise the Title: The current title does not accurately reflect the paper's focus on feed-forward DNNs and meta-learning-based ranking. A more specific title would better align with the content.
5. Correct Literature Attribution: The paper misattributes the breakthrough in speech recognition to Sainath et al. (2015) while overlooking foundational work from 2010 and 2012. Proper attribution is essential for credibility.
Questions for the Authors
1. How does the proposed ranking algorithm perform when applied to architectures beyond feed-forward DNNs, such as CNNs or RNNs?
2. Have you considered the impact of varying hyperparameter configurations on the ranking results? If so, how do you plan to address this in future work?
3. Can you provide more details on the computational efficiency of the meta-learning approach compared to random sampling or other baseline methods?
4. How would the ranking algorithm handle datasets with significantly different feature distributions or class imbalances?
In conclusion, while the paper introduces a novel and promising approach, its current limitations in scope, methodology, and experimental design prevent it from meeting the standards for publication. Addressing these issues would greatly strengthen the paper and its contributions to the field.