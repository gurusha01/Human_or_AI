Review
Summary of Contributions
This paper introduces structured attention mechanisms as a generalization of standard attention models, incorporating segmental and syntactic attention layers. By embedding graphical models (e.g., CRFs and dependency parsers) within neural networks, the authors enable richer structural dependencies to be modeled while maintaining end-to-end differentiability. The paper demonstrates the feasibility of this approach through applications to synthetic and real-world tasks, including neural machine translation (NMT), natural language inference (NLI), question answering (QA), and tree transduction. Notably, the authors pioneer the use of backpropagation through forward-backward and inside-outside algorithms, extending the applicability of structured attention to arbitrary graphical models. The results show small but consistent performance gains over standard attention mechanisms, and the learned latent structures are interpretable and meaningful.
Decision: Accept
The paper is novel, technically sound, and provides a strong proof of concept for structured attention mechanisms. While the performance gains are modest, the work opens up promising avenues for future research, particularly in leveraging structured attention for neural phrase-based MT and incorporating latent syntax with (semi-)supervision. The clarity of the exposition and the breadth of experiments further strengthen the case for acceptance.
Supporting Arguments
1. Novelty and Technical Contribution: The integration of structured attention mechanisms into neural networks is a significant advancement. The use of differentiable inference algorithms (e.g., forward-backward, inside-outside) within deep learning frameworks is an underexplored area, and this paper provides a robust implementation and analysis.
   
2. Empirical Validation: The experiments span diverse tasks, demonstrating the versatility of structured attention. The authors provide evidence that structured attention improves performance over simple attention in NMT, NLI, QA, and synthetic tasks. The visualization of learned latent structures (e.g., dependency trees) highlights the interpretability of the approach.
3. Clarity and Reproducibility: The paper is well-written, with clear descriptions of the models, algorithms, and experimental setups. Code availability further supports reproducibility.
Suggestions for Improvement
1. Underexplored Potential in NMT: While the paper evaluates structured attention in NMT, the exploration is limited to small performance gains. Future work could investigate its application to neural phrase-based translation or multi-lingual translation tasks, where structural biases may have a greater impact.
2. Numerical Stability: The authors mention potential numerical instability due to second-order statistics in the signed log-space field but do not provide empirical evidence or mitigation strategies. A deeper analysis of this issue would strengthen the paper.
3. Minor Typos and Clarity: There are minor typos and unclear phrasing in Sections 1, 3.2, and 4.1. For example, the explanation of the normalization term γ in NMT experiments could be clarified.
4. Runtime Complexity: Structured attention increases training time significantly (e.g., 5× slower for NMT). While the authors acknowledge this, a more detailed discussion of computational trade-offs and potential optimizations would be valuable.
Questions for the Authors
1. Can you provide more insights into the numerical stability issues mentioned in Section 3.3? Did you observe any practical instability during training, and how did you address it?
2. In the NMT experiments, how does structured attention perform on larger datasets or more complex language pairs? Do you anticipate greater gains in such scenarios?
3. Could structured attention be extended to tasks beyond NLP, such as computer vision or time-series analysis? If so, what challenges do you foresee?
Overall, this paper makes a compelling case for structured attention mechanisms and lays the groundwork for future exploration in this area. The combination of technical rigor, empirical validation, and potential for broader impact makes it a strong candidate for acceptance.