Review
This paper addresses the critical challenge of novelty generation in machine learning by proposing a framework for generating and evaluating out-of-distribution (OOD) novelty. It unifies prior work on novelty generation, introduces a novel experimental setup based on hold-out classes, and repurposes existing generative model evaluation metrics to assess OOD novelty. The authors conduct extensive experiments on various generative models, including autoencoders and GANs, demonstrating that specific architectures and hyperparameter combinations can surpass prior work in generating OOD novelty. The paper makes a significant contribution by providing actionable metrics for evaluating novelty generation and laying the groundwork for future research in computational creativity.
Decision: Accept (with major revisions)  
The paper tackles an important and underexplored problem, and its contributions are well-motivated and impactful. However, the clarity and organization of the paper need substantial improvement to make it more accessible to a broader audience. The theoretical sections are overly abstract and lack concrete examples, making them difficult to follow. Additionally, the exposition is poorly ordered, requiring frequent back-and-forth referencing, which hampers readability. Experimental descriptions are too brief, raising concerns about omitted details. These issues must be addressed to ensure the paper's broader appeal and usability.
Supporting Arguments  
1. Strengths:  
   - The paper provides a novel and actionable definition of creativity as OOD novelty generation, which is a meaningful contribution to both machine learning and computational creativity research.  
   - The experimental framework using hold-out classes is innovative and aligns well with the goal of evaluating generative models for OOD novelty.  
   - The proposed metrics, particularly out-of-class objectness and out-of-class count, are well-designed and provide a solid foundation for systematic evaluation.  
   - The experiments demonstrate that existing generative models can be repurposed for novelty generation, opening up new research directions.  
2. Weaknesses:  
   - The theoretical sections (e.g., Sections 1 and 3) are overly abstract and lack concrete examples or intuitive explanations, making them inaccessible to readers unfamiliar with the domain.  
   - The paper's organization is suboptimal, requiring readers to frequently jump between sections to understand the flow of ideas. A reordering of sections could significantly improve clarity.  
   - The experimental setup and results are described too briefly, leaving out critical details such as hyperparameter tuning processes and dataset specifics. Supplementary materials could address these gaps.  
Additional Feedback  
1. Clarity and Accessibility:  
   - Provide concrete examples or case studies to illustrate key concepts, such as the distinction between in-distribution and out-of-distribution generation.  
   - Reorganize the paper to improve logical flow. For instance, move the experimental setup (Section 4.3) earlier to provide context before discussing metrics.  
2. Experimental Details:  
   - Expand on the experimental setup, including hyperparameter search strategies, dataset preprocessing, and computational resources used.  
   - Include visualizations or qualitative examples of generated OOD samples to complement the quantitative metrics.  
3. Broader Impact:  
   - Discuss potential applications of OOD novelty generation in real-world scenarios, such as design, art, or scientific discovery, to highlight the broader relevance of the work.  
Questions for the Authors  
1. How robust are the proposed metrics across different datasets and domains? Have you tested the framework on datasets beyond MNIST and Google Fonts?  
2. Could you provide more details on how the hyperparameters were selected for the generative models? Was the process automated or manual?  
3. How do you address the potential trade-off between generating meaningful novelty and avoiding trivial noise? Are there additional constraints or regularizations that could improve this balance?  
In conclusion, while the paper makes a strong contribution to the field, significant revisions are needed to improve its clarity, organization, and accessibility. These changes will ensure that the paper reaches its full potential and has a broader impact on the research community.