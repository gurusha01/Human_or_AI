The paper proposes a novel Linear Pipeline (LP) approach for collective communication in multi-GPU systems, specifically targeting the communication bottlenecks in Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD) during neural network training. The authors claim that LP achieves significant theoretical and practical performance improvements over existing methods like Minimum Spanning Tree (MST) and Bidirectional Exchange (BE). The paper provides a detailed theoretical analysis, implementation insights, and experimental results demonstrating LP's superior communication efficiency and its impact on reducing training time for large-scale neural networks like AlexNet and GoogLeNet.
Decision: Reject  
While the proposed LP approach shows promise and achieves strong empirical results, the paper fails to convincingly support its claims due to unclear writing, theoretical inconsistencies, and insufficient motivation for the approach.
Supporting Arguments:
1. Unclear Writing and Organization: The paper is difficult to follow due to its dense and poorly organized writing. The introduction lacks a clear articulation of the problem, and Section 3, which describes the LP approach, is overly technical without providing sufficient intuition or clarity. This makes it challenging to understand the key contributions and their significance.
   
2. Theoretical Inconsistencies: The analysis in Section 3.2 does not convincingly support the claimed 2x and \( O(\log P) \) speedups. The inequality provided in the analysis does not align with the stated advantages, raising questions about the rigor of the theoretical claims.
3. Unclear Source of Improvement: While the experimental results demonstrate impressive speedups, it is unclear whether these gains stem from the LP algorithm itself or from implementation optimizations (e.g., GPU-specific enhancements like P2P access). This ambiguity undermines the generalizability of the approach.
4. Insufficient Motivation: The paper does not sufficiently emphasize the differences between designing parallel algorithms for CPUs versus GPUs, which is critical for motivating the need for LP in the context of multi-GPU systems.
Additional Feedback for Improvement:
- Clarity and Intuition: The authors should improve the clarity and organization of the paper, particularly in the introduction and Section 3. Providing high-level intuition and visual explanations for the LP approach would make the work more accessible.
- Theoretical Rigor: The theoretical analysis in Section 3.2 needs to be revisited to ensure that the claims are mathematically sound and adequately supported.
- Motivation and Context: The paper should better articulate the challenges of GPU-specific parallelism and how LP addresses these challenges compared to existing methods.
- Implementation Details: To address concerns about the source of improvement, the authors should provide a more detailed breakdown of the experimental results, isolating the contributions of the algorithm from those of the implementation.
Questions for the Authors:
1. Can you clarify the specific assumptions under which the claimed 2x and \( O(\log P) \) speedups hold? The current analysis does not seem to support these claims.
2. How does LP perform in scenarios with fewer GPUs or smaller message sizes? Are the results consistent across different hardware configurations?
3. Can you provide more details on the implementation optimizations (e.g., P2P access) and their impact on the observed speedups?
In summary, while the LP approach has potential, the paper requires significant revisions to address its theoretical and presentation shortcomings.