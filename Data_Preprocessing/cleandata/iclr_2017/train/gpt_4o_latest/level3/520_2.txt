Review
Summary of Contributions
This paper extends PixelCNN for conditional image synthesis by incorporating text, segmentation masks, and keypoints as conditioning inputs. The authors demonstrate the model's capabilities on three datasets: CUB (birds), MHP (human poses), and MS-COCO (general objects). The primary contribution lies in adapting PixelCNN to handle structured spatial constraints, such as segmentation masks and keypoints, alongside text descriptions. The paper highlights the robustness of PixelCNN against artifacts compared to GAN-based methods, particularly in scenarios with complex conditioning inputs. Qualitative results show that the model adheres well to spatial constraints and generates diverse samples, while quantitative evaluation is conducted using log-likelihoods. However, the paper acknowledges limitations in disentangling location and appearance and in generating high-resolution, artifact-free images.
Decision: Reject
The paper presents a solid engineering contribution, but it lacks sufficient novelty and comprehensive evaluation to warrant acceptance. The extension of PixelCNN to structured conditioning is incremental and does not introduce groundbreaking insights. Additionally, the qualitative comparisons and quantitative evaluations are limited, making it difficult to fully assess the model's advantages over existing methods.
Supporting Arguments
1. Limited Novelty: While the extension of PixelCNN to handle keypoints and segmentation masks is a useful contribution, it is a relatively straightforward adaptation of existing methods. The paper does not introduce fundamentally new techniques or theoretical insights that significantly advance the field of conditional image synthesis.
   
2. Evaluation Gaps: The qualitative comparisons with GAN-based methods are limited in scope and difficult to interpret. While the paper claims robustness against artifacts, the qualitative results do not convincingly demonstrate superiority over GANs. The quantitative evaluation using log-likelihoods, though principled, is insufficient to capture perceptual quality or diversity of generated images.
3. Resolution and Artifacts: The generated images are limited to low resolution (32×32), and the paper acknowledges issues with noisy outputs and inaccurate color generation for certain objects. This limits the practical applicability of the proposed approach.
Suggestions for Improvement
1. Broader Comparisons: Provide more comprehensive qualitative and quantitative comparisons with state-of-the-art GANs and other autoregressive models. Metrics such as FID (Fréchet Inception Distance) or human evaluation could strengthen the claims about robustness and diversity.
   
2. Higher-Resolution Outputs: Explore methods to scale the model to generate higher-resolution images, as this is a critical requirement for real-world applications.
3. Disentanglement Analysis: Conduct a more detailed analysis of the disentanglement between location and appearance. For example, evaluate the model's ability to generalize to unseen combinations of keypoints and text descriptions.
4. Ablation Studies: Include ablation studies to quantify the impact of different conditioning inputs (e.g., text vs. keypoints vs. segmentation masks) on the quality of generated images.
Questions for the Authors
1. How does the model perform when generating images with unseen combinations of keypoints and text during training? Does it generalize well to novel conditioning inputs?
2. Can the authors provide quantitative metrics (e.g., FID) to evaluate the perceptual quality of the generated images compared to GAN-based methods?
3. What are the computational trade-offs (e.g., training time, memory usage) of using PixelCNN compared to GANs for conditional image synthesis?
In summary, while the paper demonstrates a useful extension of PixelCNN, the lack of novelty, limited evaluation, and low-resolution outputs prevent it from making a strong contribution to the field. Addressing the above concerns could significantly improve the paper's impact and clarity.