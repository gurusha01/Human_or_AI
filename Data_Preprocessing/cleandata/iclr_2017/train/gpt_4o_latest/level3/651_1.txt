Review of the Paper
Summary of Contributions
This paper introduces the Gaussian Error Linear Unit (GELU), a novel neural network activation function derived as the expected transformation of a stochastic regularizer called the Stochastic 0-I (SOI) Map. The authors propose that GELU bridges the gap between nonlinearities and stochastic regularizers, offering a probabilistic interpretation of activation functions. Empirical evaluations across diverse tasks, including computer vision, natural language processing, and speech recognition, suggest that GELU outperforms or matches the performance of existing nonlinearities like ReLU and ELU. The paper also claims that the SOI Map, without any traditional nonlinearity, can achieve competitive results, challenging the necessity of deterministic nonlinearities in neural networks.
Decision: Reject  
Key Reasons:
1. Lack of Novelty: The proposed regularizer and activation function appear to be a combination of existing ideas (e.g., dropout, zoneout, and ReLU), with limited innovation beyond their integration.
2. Weak Empirical Evidence: The empirical performance improvements of GELU over ReLU and ELU are marginal and insufficient to substantiate the claim that GELU is a superior alternative across tasks.
Supporting Arguments
1. Lack of Novelty in the Approach: While the connection between nonlinearities and stochastic regularizers is an intriguing idea, it is not convincingly demonstrated as a fundamentally new contribution. The SOI Map is conceptually similar to adaptive dropout, and the GELU is presented as a smooth variant of ReLU, which limits the originality of the work.
2. Empirical Results Are Not Convincing: The reported performance gains of GELU over ReLU and ELU are minor (e.g., differences of 0.2-1% in error rates across tasks). These improvements are not statistically significant or consistent enough to justify the adoption of GELU over well-established activation functions.
3. Theoretical Claims Are Underdeveloped: The probabilistic interpretation of GELU and its connection to the SOI Map is interesting but lacks rigorous theoretical grounding. The paper does not adequately explain why these connections are meaningful or how they lead to better performance.
Suggestions for Improvement
1. Strengthen the Theoretical Justification: Provide a more rigorous analysis of the connection between stochastic regularizers and nonlinearities. Explain why this connection is significant and how it can lead to practical benefits in neural network training.
2. Expand Empirical Validation: Include statistical significance tests to demonstrate the reliability of the observed performance improvements. Additionally, compare GELU to a broader range of activation functions, such as Leaky ReLU or Swish, to position it more comprehensively within the literature.
3. Clarify Novelty: Clearly articulate what is fundamentally new about the proposed approach compared to existing methods like adaptive dropout, zoneout, and ReLU. Highlight specific scenarios where GELU offers a distinct advantage.
4. Broader Applicability: Explore the performance of GELU in more challenging or large-scale tasks, such as ImageNet classification or large language models, to demonstrate its scalability and robustness.
Questions for the Authors
1. How does the marginal performance improvement of GELU justify its adoption over simpler and more established activation functions like ReLU or ELU?
2. Can you provide a theoretical explanation or empirical evidence to support the claim that the SOI Map fundamentally challenges the necessity of traditional nonlinearities?
3. Have you conducted ablation studies to isolate the contributions of the SOI Map and GELU separately? How do they compare when used independently?
In conclusion, while the paper introduces an interesting perspective on the relationship between stochastic regularizers and nonlinearities, the lack of significant novelty, insufficient empirical evidence, and underdeveloped theoretical insights make it difficult to recommend acceptance in its current form.