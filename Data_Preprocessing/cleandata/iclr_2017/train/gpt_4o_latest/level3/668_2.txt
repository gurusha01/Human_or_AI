Review
Summary
This paper introduces a novel approach to regularizing neural networks by penalizing low-entropy output distributions, thereby encouraging high-entropy predictions. The authors argue that this form of regularization, which directly targets model behavior rather than parameters, is both interpretable and effective. They systematically evaluate two output regularizers—confidence penalty and label smoothing—across six diverse benchmarks: image classification, language modeling, machine translation, and speech recognition. The results demonstrate consistent improvements in state-of-the-art models without requiring hyperparameter modifications, suggesting the wide applicability of these techniques. The paper also establishes a connection between the confidence penalty and label smoothing, providing a theoretical foundation for the proposed approach.
Decision: Revise and Resubmit  
The paper presents a sound and promising idea, but it falls short in contextualizing its contributions within the broader machine learning literature and lacks thorough experimental rigor. Specifically, the absence of significance tests, error analysis, and robustness evaluations weakens the empirical claims. These issues prevent acceptance in its current form, but the work has potential for resubmission after addressing these shortcomings.
Supporting Arguments
1. Strengths:
   - The proposed idea is natural, interpretable, and aligns with sensible priors over model behavior. Regularizing output distributions rather than parameters is an underexplored yet promising direction in neural networks.
   - The experimental results are promising, showing consistent improvements across diverse tasks and datasets. The wide applicability of the regularizers is a notable strength.
   - The connection between the confidence penalty and label smoothing is insightful, providing a theoretical basis for the approach.
2. Weaknesses:
   - The paper does not adequately situate its contributions within existing literature. While the related work section mentions prior studies, it fails to critically compare the proposed methods with similar approaches, such as Virtual Adversarial Training or other entropy-based regularizers.
   - The experimental evaluation lacks statistical rigor. There are no significance tests to validate the reported improvements, and robustness analyses (e.g., sensitivity to hyperparameters, performance on noisy or adversarial data) are missing.
   - Error analysis is absent, which could provide insights into the types of errors mitigated by the regularizers and their impact on generalization.
Additional Feedback
To improve the paper:
1. Contextualization: Expand the related work section to include a more comprehensive comparison with existing methods, particularly those that also regularize output distributions or use entropy-based techniques.
2. Experimental Rigor: Include significance tests (e.g., t-tests or bootstrap confidence intervals) to validate the reported improvements. Evaluate robustness to hyperparameter variations, noise, and adversarial attacks.
3. Error Analysis: Provide qualitative and quantitative analyses of the errors reduced by the proposed regularizers. This would strengthen the claim that the methods improve generalization.
4. Ablation Studies: Investigate the impact of different components of the proposed methods, such as annealing and thresholding for the confidence penalty, to better understand their contributions.
5. Clarity: While the paper is generally well-written, some sections (e.g., the connection between label smoothing and the confidence penalty) could benefit from additional explanation for accessibility to a broader audience.
Questions for the Authors
1. How do the proposed regularizers perform under adversarial settings or on datasets with significant label noise?  
2. Did you explore combining the confidence penalty with other regularization techniques, such as Virtual Adversarial Training? If so, what were the results?  
3. Can you provide more details on the computational overhead introduced by the confidence penalty compared to standard training?  
In summary, the paper introduces a compelling idea with promising results, but it requires better contextualization and more rigorous evaluation to substantiate its claims. I encourage the authors to address these issues and resubmit.