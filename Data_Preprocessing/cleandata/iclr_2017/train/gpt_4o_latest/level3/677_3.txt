The paper presents an unsupervised method for learning graph embeddings using random walks and an encoder-decoder model inspired by the skip-thought framework. The authors aim to generate general-purpose graph representations that can be used for downstream tasks like classification. By linearizing graphs through random walks and training the model to predict neighboring sequences, the method captures both structural and functional similarities in graphs. The approach is evaluated on chemical compound datasets and demonstrates competitive performance compared to state-of-the-art methods.
Decision: Accept
The paper is well-executed, addresses a relatively underexplored area of whole-graph representation learning, and demonstrates promising results. However, the inclusion of additional baselines and further comparisons with node representation methods using pooling would strengthen the evaluation.
Supporting Arguments:
1. Problem Tackled: The paper addresses the problem of learning unsupervised graph embeddings, a critical area with applications in domains like chemistry and social networks. Unlike many existing methods that focus on node embeddings or task-specific supervised approaches, this work proposes a general-purpose, unsupervised method for whole-graph embeddings.
   
2. Motivation and Novelty: The use of random walks to linearize graphs and the adaptation of the skip-thought model to graph data are well-motivated. The authors effectively position their work in the context of prior literature, highlighting the novelty of their approach in focusing on whole-graph embeddings rather than node-level representations.
3. Scientific Rigor: The experimental results are robust, with the proposed method outperforming state-of-the-art techniques on three out of four datasets. The authors also provide a detailed analysis of hyperparameters, aggregation strategies, and the impact of training epochs, demonstrating a thorough understanding of their method's behavior.
Suggestions for Improvement:
1. Additional Baselines: While the paper compares against several state-of-the-art methods, it would benefit from including node representation methods with pooling (e.g., DeepWalk with pooling) as baselines. This would provide a more comprehensive evaluation, especially since pooling is a key component of the proposed method.
   
2. Broader Evaluation: Testing the method on more diverse datasets, including larger and more complex graphs, would help establish its generalizability. Additionally, exploring heterogeneous graphs or multi-label classification tasks could expand the scope of the work.
3. Ablation Studies: While the paper evaluates different aggregation strategies, it would be useful to include ablation studies to isolate the contributions of key components, such as the encoder-decoder architecture and random walk generation process.
Questions for the Authors:
1. How does the method perform when compared to node representation approaches with pooling? Could this comparison provide insights into the advantages of whole-graph embeddings?
2. Have you considered alternative random walk strategies (e.g., biased walks) to better capture specific graph properties?
3. Could the proposed method be extended to handle dynamic graphs or graphs with temporal information?
In conclusion, the paper makes a valuable contribution to unsupervised graph representation learning and is well-suited for acceptance. Addressing the suggested improvements could further enhance its impact.