Review of the Paper
Summary of Contributions
The paper presents a set of design recommendations for constructing an end-to-end differentiable programming language aimed at learning programs from input-output examples. Drawing inspiration from functional programming, it emphasizes good programming practices such as immutable data, structured control flow, and type systems. The authors propose a series of models, culminating in a differentiable functional programming language, and empirically evaluate these recommendations on a range of program induction tasks. The paper claims that these recommendations improve the success rate of learning algorithms compared to existing baselines.
Decision: Reject
The primary reasons for rejection are the marginal contribution of the paper compared to the existing TerpreT system and the lack of convincing evidence supporting the proposed gradient-based evaluators for program induction. Additionally, the paper suffers from issues of readability, poor defense of its claims, and irreproducibility.
Supporting Arguments
1. Marginal Contribution: The paper's contributions are incremental and largely overlap with the TerpreT system, which already provides a strong theoretical foundation and extensive experimentation. While the paper proposes some new design choices, these are not sufficiently novel or impactful to justify acceptance.
   
2. Contradiction Without Evidence: The paper contradicts a key finding of the TerpreT paper, which questions the viability of gradient-based evaluators for program induction. However, it fails to provide rigorous theoretical or empirical evidence to substantiate its claims. The experimental results are not compelling enough to challenge the conclusions of prior work.
3. Insufficient Comparisons: The lack of careful vetting of gradient-based methods and insufficient comparisons to existing alternatives, such as λ² and other program synthesis systems, undermines the validity of the claims. The paper does not convincingly demonstrate that its proposed approach outperforms or complements existing methods.
4. Readability and Reproducibility: The paper is difficult to follow due to its dense and poorly organized presentation. Furthermore, the lack of publicly available code or clear implementation details raises concerns about reproducibility.
Suggestions for Improvement
1. Stronger Empirical Evidence: Provide more comprehensive experiments with diverse benchmarks and include comparisons to state-of-the-art systems like TerpreT and λ². Highlight scenarios where the proposed approach excels.
2. Theoretical Justification: Offer a stronger theoretical argument for why gradient-based evaluators are suitable for program induction, especially in light of contradictory findings in prior work.
3. Clarity and Organization: Improve the structure and readability of the paper. Clearly outline the contributions in the introduction and provide a concise summary of results.
4. Reproducibility: Release the code and models used in the experiments to enable independent verification of the results.
Questions for the Authors
1. How does the proposed approach address the limitations of TerpreT, and in what specific scenarios does it outperform existing methods?
2. Can you provide additional evidence or analysis to support the claim that gradient-based evaluators are effective for program induction?
3. What steps have been taken to ensure the reproducibility of the experiments? Will the code and models be released?
In summary, while the paper tackles an important problem and proposes interesting ideas, the lack of significant contributions, insufficient evidence, and poor presentation place it below the acceptance threshold.