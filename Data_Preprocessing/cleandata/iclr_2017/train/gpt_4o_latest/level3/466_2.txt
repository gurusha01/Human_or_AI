Review of the Paper
Summary of Contributions
This paper provides a theoretical and empirical investigation into the role of identity parameterizations in residual networks (ResNets). The authors make two key contributions. First, they prove that linear residual networks have no spurious critical points, ensuring that any critical point is a global optimum. This result is significant as it simplifies the optimization landscape of deep linear networks, which are typically non-convex. Second, the paper demonstrates that non-linear residual networks with ReLU activations exhibit universal finite-sample expressivity, meaning they can represent any function of a sample given sufficient parameters. Inspired by these theoretical insights, the authors propose a radically simplified residual architecture that achieves competitive performance on CIFAR10, CIFAR100, and ImageNet benchmarks without relying on batch normalization, dropout, or max-pooling.
Decision: Accept
The paper makes substantial theoretical contributions to understanding the optimization and representational properties of residual networks, which are central to modern deep learning. The results are novel, rigorous, and well-motivated, and the proposed simplified architecture demonstrates competitive empirical performance. However, there are areas where clarity and additional exploration are needed, particularly regarding dimension mismatches, the role of ResNet-specific identity mappings, and the implications of the network's depth.
Supporting Arguments
1. Theoretical Contributions: The proof that linear residual networks have no spurious critical points is a significant advancement in understanding the optimization landscape of deep networks. The universal finite-sample expressivity result for non-linear residual networks is also compelling, as it provides a theoretical foundation for their empirical success.
2. Empirical Validation: The proposed architecture achieves competitive results on standard benchmarks, demonstrating the practical relevance of the theoretical insights. The simplicity of the architecture (e.g., no batch normalization or dropout) is particularly noteworthy.
3. Novelty and Placement in Literature: The paper builds on and extends prior work on residual networks and optimization landscapes, such as Kawaguchi (2016) and He et al. (2015). The focus on identity parameterization as a unifying principle is both novel and impactful.
Suggestions for Improvement
1. Dimension Mismatch in Eq. 3.4: The paper needs to clarify the dimensional compatibility between \(qj \in \mathbb{R}^k\) and \(ej \in \mathbb{R}^r\). This mismatch is critical to understanding the construction of the final mapping layer.
2. Role of ResNet Identity: While the paper emphasizes the importance of identity mappings, it does not sufficiently explain why ResNets are uniquely suited for the proposed constructions. Intuition on how identity mappings facilitate optimization in the non-linear case would strengthen the paper.
3. Overfitting and Representation Power: The authors should explore the relationship between the overfitting tendencies of residual networks and their superior representation power. For instance, why does the proposed architecture avoid overfitting despite its large parameter count?
4. Depth Implications: The paper briefly mentions that deeper networks may have more desirable optimization landscapes. A more detailed discussion on how the theoretical results influence the required depth of the network would be valuable.
5. Connection to Pretraining and Clustering: The clustering-based construction for non-linear networks resembles older pretraining methods. The authors could explore connections to techniques like the Nystr√∂m approximation to provide additional context.
Questions for the Authors
1. Can you provide a detailed explanation or example to resolve the dimension mismatch in Eq. 3.4?
2. How does the identity mapping specifically aid optimization in the non-linear case with ReLU activations? Could other parameterizations achieve similar results?
3. What are the practical implications of your theoretical results for determining the optimal depth of a residual network?
4. How does the proposed architecture avoid overfitting despite its large number of parameters? Could this be attributed to the absence of batch normalization or dropout?
5. Have you considered connections between your clustering-based construction and older pretraining methods? How might these insights generalize to other architectures?
In conclusion, this paper makes significant theoretical and practical contributions to the understanding and design of residual networks. Addressing the above points would further strengthen its impact.