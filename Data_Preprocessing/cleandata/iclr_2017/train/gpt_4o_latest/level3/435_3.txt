The paper proposes a novel warm restart mechanism for Stochastic Gradient Descent (SGD) to improve convergence rates and anytime performance in training deep neural networks (DNNs). The method involves periodic increases in the learning rate, followed by cosine annealing, and is evaluated on CIFAR-10, CIFAR-100, EEG datasets, and a downsampled ImageNet dataset. The authors demonstrate that their approach achieves state-of-the-art results, reduces training time by 2-4x, and enables efficient ensemble creation from intermediate models. The method is simple, parameterized, and applicable to a wide range of tasks beyond image classification.
Decision: Accept.  
Key reasons:  
1. The proposed method is effective, achieving strong results on well-known benchmarks and demonstrating relevance beyond image classification.  
2. The simplicity and generality of the approach make it a valuable contribution to the optimization literature.  
Supporting Arguments:  
The paper provides empirical evidence supporting its claims, with rigorous evaluations on multiple datasets. The results on CIFAR-10 and CIFAR-100 are particularly compelling, achieving state-of-the-art accuracy while reducing training time significantly. The approach is well-motivated, leveraging insights from warm restarts in optimization and adapting them effectively for stochastic settings. The inclusion of EEG and downsampled ImageNet datasets highlights the method's versatility. Additionally, the ability to create ensembles "for free" from intermediate models is a practical advantage.  
However, the paper has some weaknesses. The introduction lacks clarity in connecting the proposed method to the broader context of optimization challenges in DNN training. While the techniques are well-presented, the figures are difficult to interpret, and the baseline comparisons are incomplete due to non-convergence.  
Suggestions for Improvement:  
1. Introduction: Provide a clearer connection between the challenges in DNN optimization and the motivation for warm restarts. Highlight the novelty of the proposed method relative to existing learning rate schedules.  
2. Figures: Improve the readability of figures, particularly those illustrating learning rate schedules and performance comparisons. Consider adding annotations or alternative visualizations.  
3. Baseline Analysis: Address the non-convergence of the baseline by either improving its setup or providing a detailed discussion of its limitations.  
4. Loss Surface Analysis: Include a visualization of the loss surface for T0 vs. Tmult to better understand the impact of these parameters.  
5. Network Depth Analysis: Explore the relationship between network depth and performance to provide deeper insights into the method's scalability.  
Questions for the Authors:  
1. How does the proposed method compare to other advanced optimization techniques like Adam or AdaDelta in terms of convergence speed and final accuracy?  
2. Can the approach be extended to other training algorithms beyond SGD, such as RMSProp or adaptive gradient methods?  
3. Did you observe any trade-offs between the frequency of restarts and the stability of training?  
Overall, this paper provides a valuable contribution to the field of optimization for deep learning, and with minor revisions, it could have an even greater impact.