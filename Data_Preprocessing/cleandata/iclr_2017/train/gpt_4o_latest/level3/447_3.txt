Review of the Paper
Summary of Contributions
This paper introduces a simulator and synthetic tasks to evaluate dialogue agents' ability to learn from user feedback, specifically through question-asking and clarification. The authors employ memory networks trained via supervised and reinforcement learning to explore how a dialogue agent can benefit from asking questions in both offline and online settings. The key contribution is the demonstration that agents capable of learning from user feedback outperform those that do not. The paper also validates its approach with real human experiments using Amazon Mechanical Turk, representing a step toward real-world applicability. This work is well-motivated, as dialogue agents that can learn from unstructured human feedback have significant potential for real-world utility.
Decision: Accept
The paper should be accepted, primarily because:
1. Strong Motivation and Novel Contribution: The authors address an important problem in dialogue systems—learning through interaction—by introducing a novel simulator and tasks. The results convincingly show that agents benefit from asking questions, which is a meaningful step forward in interactive AI research.
2. Empirical Rigor: The experiments are thorough, with both synthetic and real-world data validating the hypothesis. The use of Mechanical Turk data strengthens the paper's claims, despite the challenges of working with real-world variability.
Supporting Arguments
1. The motivation for the work is compelling, as enabling dialogue agents to learn from unstructured feedback is a critical capability for real-world applications. The authors provide a clear and well-placed discussion of related work, situating their contribution as a natural extension of prior research.
2. The experimental results are robust and scientifically rigorous. The findings consistently show that agents trained to ask questions perform better across various tasks, even when tested with real human data. The reinforcement learning framework for deciding when to ask questions is particularly interesting and demonstrates practical relevance.
3. The inclusion of Mechanical Turk experiments is a significant strength, as it bridges the gap between artificial simulations and real-world interactions.
Suggestions for Improvement
While the paper is strong overall, the following points could improve its clarity and impact:
1. Artificial Setup: The reliance on a highly artificial simulator limits the generalizability of the findings. While the Mechanical Turk experiments mitigate this to some extent, additional real-world experiments with more diverse datasets would strengthen the evidence for the claims.
2. Clarification of Claims: The paper claims that the agent learns "what to ask" in the reinforcement learning section, but this is not convincingly demonstrated. The authors should either clarify this claim or revise it to better reflect the results.
3. Overwhelming Results Presentation: The sheer volume of results presented is overwhelming and could confuse readers. Tables 2 and 3, as well as results like "TrainAQ(+FP)" and "TrainMix," should be streamlined or moved to the appendix unless they are central to the hypothesis.
4. Terminology: The term "interactive dialogue agents" in the abstract is redundant and should be revised for clarity.
Questions for the Authors
1. How does the simulator's artificial nature (e.g., reliance on a factoid QA framework and assumption of correct user feedback) affect the generalizability of the results? Would the approach still work in more complex, noisy environments?
2. Could you expand on how the agent learns "what to ask" in the reinforcement learning setting? The results seem to focus more on "when to ask," and this distinction should be clarified.
3. Did you explore combining simulated and real data during training? If so, how did this impact performance compared to using real data alone?
In conclusion, this paper makes a valuable contribution to the field of interactive dialogue systems, and its strengths outweigh its limitations. With minor revisions to address clarity and generalizability, it will be a strong addition to the conference.