Review of "Surprisal-Driven Feedback in Recurrent Neural Networks"
Summary of Contributions
This paper proposes a novel feedback mechanism for recurrent neural networks (RNNs) called "surprisal-driven feedback," which leverages prediction errors (surprisal) from previous time steps as an additional input signal during inference. The authors argue that this feedback mechanism mimics top-down feedback in human cognition and improves generalization capabilities. The approach is evaluated on the enwik8 dataset for character-level text modeling, where the authors claim state-of-the-art (SOTA) performance with 1.37 BPC. The proposed method is simple, requiring only the addition of a feedback matrix, and is compatible with existing RNN architectures like LSTMs.
Decision: Reject
The paper introduces an interesting idea, but it falls short in several critical areas, including clarity, rigor, and experimental validation. The reliance on ground-truth labels during testing also limits the practical applicability of the approach.
Supporting Arguments
1. Unclear Motivation and Justification: While the idea of using surprisal as feedback is intriguing, the paper does not provide a compelling theoretical or empirical justification for why this approach should work. The connection to human cognition is speculative and not substantiated with evidence.
   
2. Dependence on Ground-Truth Labels: The proposed method requires access to ground-truth labels during testing to compute surprisal, which is impractical in real-world scenarios. This reliance severely limits the applicability of the approach and undermines its utility.
3. Experimental Limitations: The evaluation is restricted to a single dataset (enwik8), which is insufficient to demonstrate the generalizability of the method. Furthermore, the claim of achieving SOTA performance is incorrect, as other models like HyperNetworks outperform the reported results.
4. Writing Quality and Organization: The paper is poorly written and lacks clear organization. Key ideas are buried in dense mathematical descriptions, and many equations (e.g., those related to BPTT) are unnecessary in the main text and should be moved to the appendix. This makes the paper difficult to follow.
5. Incorrect SOTA Claim: The claim of achieving SOTA performance on enwik8 is factually incorrect, as other models achieve better results. This undermines the credibility of the paper.
Suggestions for Improvement
1. Clarify Testing Procedure: The authors should explicitly address whether ground-truth labels are used during testing and discuss how this limitation could be addressed in future work.
   
2. Broader Validation: Experiments should be conducted on multiple datasets to demonstrate the generalizability of the approach. Comparisons with a wider range of baselines, including HyperNetworks, are necessary.
3. Improve Writing and Organization: The paper should be rewritten for clarity and conciseness. Mathematical details, especially those unrelated to the core contribution (e.g., standard BPTT equations), should be moved to an appendix.
4. Theoretical Justification: Provide a stronger theoretical foundation for the proposed method. Why should surprisal-driven feedback improve generalization? How does it compare to other feedback mechanisms in the literature?
5. Address Practical Applicability: Explore ways to eliminate the dependence on ground-truth labels during testing, such as using unsupervised or self-supervised methods to approximate surprisal.
Questions for the Authors
1. Does the proposed method require ground-truth labels during testing? If so, how do you envision this being applied in real-world scenarios where such labels are unavailable?
2. Why was enwik8 chosen as the sole dataset for evaluation? Can the method generalize to other temporal datasets, such as speech or time-series data?
3. How does the proposed method compare to other feedback mechanisms, such as Gated Feedback RNNs or Ladder Networks, in terms of theoretical motivation and empirical performance?
In conclusion, while the idea of surprisal-driven feedback is novel and potentially impactful, the paper falls short in execution and validation. Significant revisions and additional experiments are needed to make the work suitable for publication.