Review of the Paper
Summary of Contributions
This paper investigates the topology and geometry of deep neural network loss surfaces, with a focus on single-layer ReLU networks. The authors provide theoretical insights into the conditions under which poor local minima do not exist, linking these conditions to the interplay between data distribution smoothness and model overparameterization. A novel heuristic algorithm is proposed to compute normalized geodesics, offering a measure of path curvature between solution points. Empirical results demonstrate the near-convex behavior of level sets in various architectures and datasets, including CNNs on MNIST and CIFAR-10, and LSTMs on Penn Treebank. The work is original and contributes to understanding the optimization landscape of neural networks, particularly through its theoretical findings and the proposed algorithm.
Decision: Accept
The paper makes significant theoretical and empirical contributions to an important area of deep learning research. While there are limitations, such as the focus on single-layer networks and the reliance on heuristic methods, the originality and potential for inspiring further research outweigh these concerns.
Supporting Arguments
1. Specific Problem Tackled: The paper addresses the critical question of why neural network optimization often avoids poor local minima, despite the non-convexity of the loss surface. It also explores the geometric regularity of level sets and proposes a heuristic to measure connectivity.
   
2. Motivation and Placement in Literature: The work is well-motivated and builds on prior studies of loss surface topology, such as those using spin glass models and mean-field approximations. It extends these studies by avoiding linearity assumptions and focusing on ReLU networks, which are more relevant to practical deep learning.
3. Support for Claims: The theoretical results are rigorously derived, and the empirical studies are thorough, spanning multiple architectures and datasets. The proposed heuristic algorithm is novel and demonstrates practical utility in estimating geodesics.
Additional Feedback for Improvement
1. Extension to Deeper Architectures: The focus on single-layer networks limits the practical relevance of the theoretical results. Extending the analysis to multi-layer networks would significantly enhance the paper's impact.
2. Heuristic Algorithm Reliability: While the normalized geodesic heuristic is innovative, its reliability and sensitivity to hyperparameters are not fully explored. A more detailed analysis of failure cases and comparisons with alternative methods would strengthen the empirical contributions.
3. Saddle Point Dynamics: The paper acknowledges that saddle points are not addressed, which is a notable limitation. Including a discussion on how the proposed methods might generalize to handle saddle points would be valuable.
4. Empirical Risk vs. Oracle Risk: The distinction between empirical and oracle risk is critical but underexplored. Future work could investigate how generalization error affects the theoretical guarantees.
Questions for the Authors
1. How sensitive is the normalized geodesic heuristic to the choice of hyperparameters, such as the threshold loss \(L_0\)?
2. Can the proposed theoretical framework be extended to multi-layer ReLU networks, and if so, what challenges are anticipated?
3. How does the algorithm perform on datasets with highly non-smooth distributions, and are there specific failure cases to be aware of?
4. Could the dynamic programming approach for geodesic estimation be further optimized to handle larger-scale networks efficiently?
This paper is a valuable contribution to the field, and addressing the above points would make it even stronger.