Review of the Paper
Summary of Contributions
The paper provides a theoretical convergence analysis of two-layer neural networks with ReLU nonlinearity, focusing on the teacher-student setting under Gaussian input assumptions. It claims to derive novel gradient update rules for ReLU networks and rigorously proves global convergence for specific cases, such as single-node networks (K=1) and multi-node networks (Kâ‰¥2) with orthonormal teacher parameters. Notably, the paper argues that its analysis avoids the unrealistic assumption of independent ReLU activations, a limitation in prior works. The authors also explore symmetry-breaking initializations and their role in avoiding saddle points, presenting both theoretical results and simulations to validate their findings.
Decision: Reject
The paper is rejected due to poor writing quality, unclear presentation of key insights, and debatable novelty of its assumptions. While the theoretical analysis appears correct, the lack of clarity and motivation for the contributions significantly diminishes its impact.
Supporting Arguments
1. Writing Quality and Presentation: The paper suffers from numerous grammatical errors, typos, and convoluted sentences, making it difficult to follow. For example, the abstract and introduction are overly dense, with key contributions buried in technical details. The main results are not clearly highlighted or intuitively explained, and the organization of the paper is suboptimal. Critical insights are scattered, and the appendices contain proofs that could have been summarized in the main text for better readability.
2. Novelty and Motivation: While the paper claims to avoid assumptions of independent ReLU activations, it is unclear how this significantly advances the state of the art. The assumption of Gaussian inputs, while common, is not necessarily more realistic than prior assumptions. The novelty of the symmetry-breaking initialization is also underexplored, as it is presented as a technical result without sufficient discussion of its practical implications.
3. Scientific Rigor: The theoretical results appear correct and are supported by simulations. However, the lack of clear exposition and intuitive explanations makes it challenging to assess the broader significance of the findings. The paper does not sufficiently compare its results to prior works, leaving the reader uncertain about its relative contributions.
Suggestions for Improvement
1. Improve Writing and Organization: The authors should thoroughly revise the paper for grammatical correctness and clarity. Key contributions should be clearly stated in the abstract and introduction, with intuitive explanations provided for the main results. Theoretical results should be summarized in the main text, with detailed proofs relegated to the appendices.
2. Highlight Novelty and Motivation: The authors should better justify the novelty of their assumptions and results. For example, why is the Gaussian input assumption more realistic than prior assumptions? How does the symmetry-breaking initialization contribute to practical training scenarios? A more thorough comparison to prior works is needed to contextualize the contributions.
3. Clarify Key Insights: The paper should include diagrams or visualizations to illustrate key concepts, such as the role of symmetry-breaking in convergence. Additionally, the authors should provide more intuitive explanations for their theoretical results, making the findings accessible to a broader audience.
Questions for the Authors
1. How does the assumption of Gaussian inputs compare to prior assumptions in terms of realism and practical applicability?
2. Can the symmetry-breaking initialization be generalized to other network architectures or input distributions?
3. How does the proposed analysis scale to deeper networks beyond two layers, and what are the limitations of the current approach?
While the paper demonstrates technical rigor, its poor presentation and unclear motivation prevent it from making a significant impact. Addressing these issues could substantially improve its quality and relevance.