Review of the Paper
Summary of Contributions:
This paper explores the use of Sum-Product Networks (SPNs) and Max-Product Networks (MPNs) for representation learning and structured prediction tasks. The authors propose a novel approach where SPNs are used to generate embeddings for input/output variables, and MPNs decode these embeddings back into the original space. The paper highlights several contributions:
1. A decoding procedure leveraging MPE inference to interpret MPNs as generative autoencoders.
2. An analysis of conditions under which MPNs can act as perfect encoder-decoders.
3. Extensive experiments on multi-label classification (MLC) datasets, demonstrating that SPN-based embeddings outperform RBM-based embeddings and other baselines like MADE and MANIAC.
4. The ability of MPNs to handle partial embeddings through imputation mechanisms, showcasing their robustness to missing data.
Decision: Accept
The paper presents a well-motivated and innovative approach to representation learning using SPNs and MPNs, with strong empirical results and theoretical insights. However, the decision to accept is contingent on addressing the concerns outlined below.
Supporting Arguments:
1. Novelty and Motivation: The idea of leveraging SPNs for hierarchical feature extraction and MPNs for decoding is novel and well-placed in the literature. The authors effectively position their work as a bridge between probabilistic models and autoencoders.
2. Empirical Validation: The experiments convincingly demonstrate the superiority of SPN/MPN embeddings over RBMs, MADEs, and MANIAC in structured prediction tasks. The robustness to missing components and competitive performance against discriminative methods like CRFs further strengthen the claims.
3. Theoretical Contributions: The characterization of conditions for perfect encoding/decoding and the analysis of SPN/MPN embeddings as hierarchical part-based representations are valuable theoretical insights.
Areas for Improvement:
1. Baselines: The paper lacks comparisons with discriminative structured prediction methods such as Conditional Random Fields (CRFs) or belief propagation. Including these baselines would provide a more comprehensive evaluation of the proposed method's performance.
2. Computational Complexity: The paper does not provide sufficient details on the computational complexity of SPNs/MPNs compared to alternatives like MADE or RBMs. A discussion on scalability and runtime would enhance the paper's practical relevance.
3. Clarity of Presentation: The dense presentation of experimental results, with excessive numbers and graphs, makes the paper difficult to follow. Summarizing key results in a concise table (e.g., comparing baselines, SPNs, and competitors using metrics like Hamming and exact match losses) would improve readability. Detailed results can be moved to the appendix.
4. Explanation of Performance: While the paper claims that SPNs outperform alternatives due to their ability to model distributions and disentangle correlations, this explanation remains qualitative. A deeper analysis of why SPNs/MPNs excel, possibly through ablation studies or visualization of learned embeddings, would strengthen the claims.
Questions for the Authors:
1. How does the computational complexity of SPNs/MPNs compare to MADEs and RBMs, especially for large datasets?
2. Can you provide more insights into why SPNs outperform MADEs and RBMs? Is it due to better modeling of dependencies, hierarchical representations, or other factors?
3. How do SPNs/MPNs perform in comparison to discriminative structured prediction methods like CRFs or belief propagation?
4. Could you elaborate on the choice of metrics (e.g., JACCARD, HAMMING, EXACT MATCH) and how they align with the goals of the proposed method?
Suggestions for Improvement:
1. Add a concise table summarizing key experimental results for clarity.
2. Include a discussion on computational complexity and scalability.
3. Provide comparisons with discriminative baselines like CRFs.
4. Conduct ablation studies to isolate the contributions of different components (e.g., SPN embeddings, MPN decoding).
In conclusion, this paper makes significant contributions to representation learning and structured prediction using SPNs and MPNs. Addressing the concerns and questions raised above will further solidify its impact and clarity.