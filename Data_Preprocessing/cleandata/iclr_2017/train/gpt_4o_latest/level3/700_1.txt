Review of the Paper
Summary of Contributions:
The paper proposes a novel deep learning framework called Marginal Deep Architectures (MDA) that combines Marginal Fisher Analysis (MFA) for layer-wise initialization with techniques like backpropagation, dropout, and denoising for fine-tuning. The authors argue that MDA is particularly suited for small and medium-scale datasets, where traditional deep learning models often struggle due to their reliance on large-scale training data. The paper claims that MDA outperforms both shallow feature learning models and state-of-the-art deep learning models across a variety of tasks, including handwritten digit recognition, speech recognition, and image classification. The authors also provide extensive experimental results and explore the effects of different architectural choices on performance.
Decision: Reject
While the paper introduces an interesting idea of combining MFA with deep learning, the work suffers from several critical issues that undermine its scientific rigor and clarity. The two main reasons for rejection are: (1) insufficient motivation and justification for key methodological choices, and (2) inadequate experimental rigor, particularly in hyperparameter tuning and comparisons.
Supporting Arguments:
1. Proposed Approach and Motivation: 
   - The choice of MFA for initialization is not sufficiently justified. While MFA is a reasonable dimensionality reduction technique, the paper does not adequately explain why it is preferable over other modern alternatives, such as PCA or autoencoders, especially given the concerns about applying linear dimensionality reduction followed by sigmoid activations.
   - The decision to use sigmoid activations instead of more modern and widely adopted activation functions like ReLU is not explained, raising questions about the methodological consistency.
2. Clarity and Presentation:
   - The paper lacks clarity in several key areas. For instance, the notation and explanation of MFA are poorly defined, making it difficult for readers unfamiliar with the method to follow the arguments. Similarly, the denoising mechanism is not clearly described.
   - The authors claim that MDA achieves state-of-the-art performance, but the results are not convincingly presented. For example, no error bars or statistical significance tests are provided to substantiate the claims.
3. Experimental Concerns:
   - The experimental setup is problematic. Hyperparameters such as learning rate, momentum, and number of epochs are fixed across all datasets, which is not a fair comparison. Early stopping, a standard practice to prevent overfitting, is also absent.
   - The datasets used are predominantly small-scale, which limits the generalizability of the results. While the authors test on CIFAR-10, the preprocessing (grayscale conversion) and poor performance suggest that the method is not competitive on larger datasets.
   - The lack of visualization (e.g., learned filters for image datasets) makes it difficult to understand the qualitative differences between MDA and other methods.
4. Lack of Convincing Case for MDA:
   - The paper does not provide sufficient insight into why MFA-based initialization is beneficial. For example, it would be helpful to compare the convergence behavior of MDA with other initialization strategies or to analyze the learned representations.
Suggestions for Improvement:
1. Provide a stronger theoretical or empirical justification for the use of MFA and sigmoid activations. Consider comparing MFA with other initialization techniques in terms of convergence speed or representation quality.
2. Clarify the explanation of MFA and other key concepts, ensuring that all notations are defined and examples are provided where necessary.
3. Improve the experimental rigor by performing proper hyperparameter tuning for all methods and including early stopping. Additionally, provide statistical significance tests to support claims of superiority.
4. Include qualitative analyses, such as visualizations of learned filters or embeddings, to provide deeper insights into the benefits of MDA.
5. Test the method on more diverse datasets, including larger-scale datasets, without preprocessing steps like grayscale conversion that may disadvantage competing methods.
Questions for the Authors:
1. Why was MFA chosen over other dimensionality reduction techniques, and how does it compare empirically to alternatives like PCA or autoencoders in the context of initialization?
2. Why were sigmoid activations used instead of ReLU or other modern activation functions? Did the authors experiment with alternative activations?
3. How does MDA perform with proper hyperparameter tuning and early stopping? Could the reported results be biased due to suboptimal settings for competing methods?
4. Can the authors provide visualizations or other qualitative analyses to demonstrate the benefits of MFA-based initialization?
In summary, while the paper presents an intriguing idea, it requires significant improvements in motivation, clarity, and experimental rigor to be considered for acceptance.