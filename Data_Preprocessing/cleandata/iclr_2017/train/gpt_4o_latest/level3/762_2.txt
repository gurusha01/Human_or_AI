Review of the Paper: "Defoveating Autoencoders (DFAE): A Framework for Studying Perception from Low-Fidelity Inputs"
Summary of Contributions
This paper introduces Defoveating Autoencoders (DFAEs), a novel framework designed to investigate how neural networks can perceive and reconstruct high-detail images from systematically degraded, low-fidelity inputs. The authors draw inspiration from the human visual system, which compensates for limited high-acuity input by leveraging global context and perceptual filling-in. DFAEs are evaluated on various types of input distortions (e.g., downsampling, scotomas, foveated inputs) across datasets like MNIST and CIFAR100. The study reveals that DFAEs can reconstruct global features such as shape and color but struggle with high-frequency details like texture. The paper also highlights the network's ability to generalize color information based on contextual cues, akin to human perception. This work is positioned as a step toward both improving neural network efficiency and understanding human perceptual mechanisms.
Decision: Reject
While the paper presents an interesting framework and provides some compelling results, it falls short in several critical areas that limit its scientific rigor and impact. Specifically, the lack of state-of-the-art comparisons, insufficient clarity in experimental conditions, and issues with presentation undermine the paper's contributions.
Supporting Arguments for the Decision
1. Lack of State-of-the-Art Comparisons:  
   The paper does not compare DFAEs against state-of-the-art methods in related fields, such as super-resolution or perceptual filling-in. This omission makes it difficult to assess the novelty and effectiveness of the proposed approach. For instance, how do DFAEs compare to convolutional neural networks (CNNs) or transformer-based models in reconstructing high-detail images from low-fidelity inputs?
2. Unclear Experimental Conditions:  
   The distinction between SCT-R (scotoma) and FOV-R (foveated) conditions is not sufficiently clarified. The impact of zeroing out versus removing parts of the image on reconstruction performance is especially critical for shallow networks but is not adequately addressed. Additionally, the paper does not calculate reconstruction error specifically in the periphery where DS-D and FOV-R share the same input, which would better quantify the contribution of the fovea.
3. Presentation Issues:  
   - Figures are poorly presented. For example, Figure 2 is blurry, making it difficult to assess the quality of reconstructed images. The authors should use "nearest" interpolation for clearer visuals.  
   - Figure 3's caption lacks clarity and should reference footnote 2 for better context.  
   - The organization of figures is suboptimal, with visuals placed far from the corresponding text, disrupting readability.
4. Limited Scope of Results:  
   While the paper demonstrates that DFAEs can reconstruct global features, it does not provide sufficient evidence to support claims about perceptual filling-in. The results are largely qualitative, and more quantitative metrics (e.g., PSNR, SSIM) are needed to substantiate the findings. Furthermore, the paper does not explore the robustness of DFAEs to more complex distortions or real-world scenarios.
Suggestions for Improvement
- State-of-the-Art Comparisons: Include benchmarks against existing methods in super-resolution, denoising, or perceptual filling-in to contextualize the performance of DFAEs.  
- Clarify Experimental Design: Provide a detailed analysis of SCT-R and FOV-R conditions, explicitly addressing how zeroing versus removing parts of the image impacts results. Calculate reconstruction error specifically in the periphery to isolate the contribution of foveated input.  
- Improve Figures and Organization: Use clearer interpolation methods for images, enhance figure captions, and reorganize visuals to align closely with the text.  
- Expand Quantitative Analysis: Incorporate additional metrics like SSIM and compare results across a broader range of input distortions.  
Questions for the Authors
1. How do DFAEs compare to state-of-the-art methods in super-resolution or perceptual filling-in tasks?  
2. Could you clarify the differences in network behavior under SCT-R and FOV-R conditions, particularly regarding the impact of zeroing versus removing image regions?  
3. Have you considered testing DFAEs on real-world datasets or distortions to evaluate their practical applicability?  
In conclusion, while the paper introduces an intriguing framework, it requires significant improvements in experimental rigor, state-of-the-art comparisons, and presentation to meet the standards of the conference.