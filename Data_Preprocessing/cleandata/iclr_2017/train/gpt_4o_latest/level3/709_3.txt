Review
Summary of Contributions
This paper proposes a method for unsupervised pretraining in sequence-to-sequence (seq2seq) models by initializing the encoder and decoder with pretrained language models (LMs) trained on large monolingual corpora. The approach is applied to machine translation (English→German) and abstractive summarization tasks, achieving state-of-the-art results on WMT'14 and WMT'15 English→German benchmarks and competitive results on the CNN/Daily Mail summarization dataset. The authors also conduct ablation studies to analyze the contributions of different components, such as pretraining the encoder versus the decoder, the use of residual connections, and the language modeling objective as a regularizer. The work highlights the benefits of pretraining for generalization and optimization, especially in low-resource settings.
Decision: Accept
The paper should be accepted because it provides strong empirical results and valuable insights into the role of pretraining in seq2seq models. While the proposed method is not novel and builds on existing techniques, it demonstrates a practical and effective application of these ideas, achieving state-of-the-art performance in machine translation and meaningful improvements in summarization. The ablation studies and detailed analysis further enhance the paper's contribution by offering a deeper understanding of the method's effectiveness.
Supporting Arguments
1. Empirical Strength: The method achieves significant performance gains, including a 1.3 BLEU improvement over the previous best models on WMT'14 and WMT'15 English→German tasks. This demonstrates the practical utility of the approach.
2. Insightful Analysis: The ablation studies provide a thorough examination of the contributions of various components, such as pretraining the encoder versus the decoder and the importance of the language modeling objective. This adds depth to the paper and makes it a valuable resource for researchers.
3. Low-Resource Applicability: The results show that pretraining is particularly beneficial in low-resource scenarios, which is a critical area of research in machine learning.
Suggestions for Improvement
1. Acknowledgment of Prior Work: The paper should explicitly acknowledge earlier work on encoder-decoder RNN models from the 1990s, as these form the foundational basis for seq2seq architectures.
2. Novelty and Motivation: While the method is effective, it is relatively simple and lacks novelty. The authors could strengthen the paper by discussing how their approach compares to other recent pretraining techniques, such as backtranslation or multitask learning.
3. Random Initialization Baseline: It would be valuable to include experiments starting from random initialization to better isolate the benefits of pretraining.
4. Minor Corrections: On page 2, Section 2.1, the phrase "can be different sizes" should be revised to "can be of different sizes."
Questions for the Authors
1. How does the proposed method compare to other unsupervised pretraining techniques, such as backtranslation, when applied to machine translation tasks?
2. Did you explore the impact of using different sizes or qualities of monolingual corpora for pretraining? If so, how does this affect the results?
3. Could the method be extended to other seq2seq tasks, such as dialogue generation or question answering? If so, what challenges might arise?
Overall, this paper makes a strong empirical contribution and provides useful insights into pretraining strategies for seq2seq models. Addressing the suggested improvements would further enhance its impact.