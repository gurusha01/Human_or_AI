Review
Summary of Contributions
This paper introduces the Equation Learner (EQL), a novel neural network architecture designed to learn analytical expressions governing physical systems. The authors claim that EQL is capable of extrapolating to unseen domains, a critical feature for modeling real-world systems. The network incorporates sparsity regularization to derive concise and interpretable equations, and the methodology is evaluated on synthetic and real-world datasets, including pendulum dynamics, robotic arm kinematics, and x-ray transition energies. The authors highlight EQL's ability to outperform traditional methods like MLPs and SVRs in extrapolation tasks, offering insights into the underlying physical relationships.
Decision: Reject
The paper addresses an important and challenging problemâ€”deriving interpretable equations for physical systems from data. However, the methodology relies on outdated techniques and lacks scalability for complex systems, limiting its contribution to the field. While the results demonstrate some promise, the approach does not incorporate modern advancements in machine learning, and the novelty of the proposed architecture is questionable given its reliance on older concepts like product units and L1 regularization.
Supporting Arguments
1. Lack of Novelty and Modernization: The proposed architecture primarily builds on techniques from the 1990s, such as product units and L1 regularization, without leveraging recent advancements in neural network design, optimization, or symbolic regression. This limits the paper's contribution to the state of the art.
2. Scalability Issues: The experiments are confined to relatively simple systems (e.g., pendulum dynamics, robotic arms with a few joints). The approach does not convincingly demonstrate scalability to higher-dimensional or more complex systems, which is critical for practical applications.
3. Limited Insights: While the paper claims to provide interpretable equations, the results often rely on approximations or fail to find the correct formula due to local minima. This undermines the claim of reliably identifying the true underlying equations.
Suggestions for Improvement
1. Incorporate Modern Techniques: The authors should explore recent advancements in symbolic regression, neural architecture search, and sparsity-inducing techniques (e.g., group Lasso, attention mechanisms). Incorporating modern activation functions and optimization strategies could improve scalability and performance.
2. Benchmark Against Contemporary Methods: The paper should compare EQL against state-of-the-art symbolic regression methods, such as those based on genetic programming or reinforcement learning, to better contextualize its contributions.
3. Expand Experimental Scope: The current experiments are limited in scope. Testing on more complex systems, such as chaotic dynamics or high-dimensional physical models, would better demonstrate the method's robustness and scalability.
4. Clarify Limitations: The paper should explicitly discuss the limitations of its architecture, particularly its inability to handle certain mathematical operations (e.g., division) and its susceptibility to local minima.
Questions for the Authors
1. How does EQL compare to modern symbolic regression techniques, such as those using evolutionary algorithms or neural-guided search?
2. What are the computational requirements of EQL, and how do they scale with the complexity of the system being modeled?
3. Could the authors elaborate on why cosine functions were excluded in some experiments and how this impacts the generality of the approach?
4. How does the proposed method handle noise in real-world datasets compared to state-of-the-art methods?
In summary, while the paper tackles an important problem, the methodology lacks novelty and scalability, and the results do not convincingly demonstrate significant advancements over existing approaches. The authors are encouraged to address these concerns and explore modern techniques to strengthen their contributions.