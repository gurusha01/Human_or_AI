Review of the Paper
Summary of Contributions
This paper introduces Exponential Machines (ExM), a novel predictor that models all interactions of every order between features by leveraging the Tensor Train (TT) format for parameter representation. The TT format enables the model to handle exponentially large tensors efficiently, offering a compact representation that regularizes the model and controls the number of parameters. To train the model, the authors propose a stochastic Riemannian optimization procedure that exploits the geometry of the tensor manifold for faster convergence. The paper demonstrates the model's theoretical and computational efficiency, achieving state-of-the-art performance on synthetic datasets with high-order interactions and competitive results on the MovieLens 100K dataset. Additionally, the authors extend the model to handle interactions between functions of features and propose an initialization strategy to address optimization challenges.
Decision: Borderline (Weak Reject)
While the paper presents an innovative approach by combining the TT format and Riemannian optimization, the experimental validation is insufficient to fully support its claims. The limited depth of experiments, lack of robust comparisons, and unclear role of key hyperparameters (e.g., dropout, rank selection) undermine the overall impact of the work.
Supporting Arguments for the Decision
1. Core Contribution and Innovation:  
   The application of the TT format to model higher-order interactions is novel and has significant potential for machine learning tasks. The use of Riemannian optimization to constrain TT-rank growth is a strong contribution, and the theoretical framework is well-articulated. The initialization strategy is also a thoughtful addition to address convergence issues.
   
2. Experimental Weaknesses:  
   - The experimental section is fragmented and lacks depth. The results on real-world datasets (e.g., MovieLens 100K) are limited, and the role of dropout and rank selection is not explicitly discussed.  
   - Comparisons with competing methods (e.g., high-order Factorization Machines, neural networks) are insufficient. For instance, the experiments do not clearly establish the superiority of ExM over these baselines across diverse datasets.  
   - The use of only one real-world dataset (MovieLens 100K) limits the generalizability of the findings. Additional datasets from different domains would strengthen the evaluation.  
3. Preliminary Nature:  
   The experiments appear exploratory rather than conclusive. For example, the sensitivity of the model to TT-rank and initialization is noted but not systematically analyzed. The lack of a clear discussion on rank selection and its trade-offs further weakens the practical applicability of the method.
Suggestions for Improvement
1. Expand Experimental Validation:  
   - Include more real-world datasets to demonstrate the model's applicability across diverse domains.  
   - Provide a detailed comparison with other state-of-the-art methods, including neural networks, high-order Factorization Machines, and kernel methods.  
   - Clarify the role of dropout and rank selection in the model's performance, and systematically analyze their impact.
2. Improve Presentation:  
   - The experimental results should be cohesively presented, with a clearer narrative linking the findings to the claims.  
   - Include a discussion on the limitations of the proposed approach, such as its sensitivity to initialization and computational challenges for large datasets.
3. Technical Clarifications:  
   - Justify the choice of TT-rank in experiments and discuss strategies for automatic rank selection.  
   - Explain the advantages of dropout in the context of ExM and its interaction with the TT format.  
   - Provide more details on the initialization strategy and its impact on convergence.
Questions for the Authors
1. How does the proposed model handle sparse datasets, given that Riemannian optimization does not currently support sparsity?  
2. What criteria were used to select the TT-rank in the experiments, and how does it affect the model's generalization and efficiency?  
3. Can you elaborate on the role of dropout in the model, and why it was included in the experiments?  
4. How does the performance of ExM scale with the number of features and interactions compared to high-order Factorization Machines?  
5. Are there plans to extend the Riemannian optimization procedure to support categorical features directly?
Conclusion
This paper presents a promising approach to modeling high-order feature interactions using the TT format and Riemannian optimization. However, the experimental validation is currently insufficient to justify strong acceptance. With additional experiments, clearer comparisons, and more cohesive presentation, this work could make a significant contribution to the field. For now, I recommend a weak rejection with encouragement to address the identified weaknesses.