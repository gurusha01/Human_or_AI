Review
The paper introduces QRAQ, a synthetic domain designed to evaluate an agent's ability to reason and interact in multi-turn conversational tasks. QRAQ builds on the bAbI framework but extends it by introducing variables that require logical reasoning and querying for clarification. The paper contributes a dataset generator for QRAQ problems, baseline and improved reinforcement learning (RL) architectures, and empirical evaluations of these models. The authors emphasize the challenges of reasoning with incomplete information and the importance of asking relevant questions, a step toward more realistic task-oriented dialogue systems.
Decision: Accept
The decision to accept is based on two key reasons:  
1. Novel Contribution: The QRAQ domain is a meaningful extension of bAbI-like tasks, introducing multi-turn reasoning and interaction, which are critical for advancing conversational AI. The dataset generator and extensible codebase promise long-term utility for the research community.  
2. Scientific Rigor: The paper provides a thorough empirical evaluation of both baseline and improved RL architectures, demonstrating their performance across varying levels of task complexity. The results are well-supported and scientifically rigorous.
Supporting Arguments
1. Problem Motivation and Placement: The paper is well-motivated and addresses a significant gap in the literature. While bAbI tasks focus on single-turn reasoning, QRAQ introduces the complexity of multi-turn interactions, making it a valuable benchmark for conversational agents. The authors effectively position their work within the context of prior research, including bAbI, task-oriented dialogue systems, and memory networks.  
2. Appropriate Baselines: The use of supervised learning baselines (baseSL and impSL) provides a clear upper bound for RL performance, and the comparison between baseRL and impRL highlights the improvements achieved by the proposed architecture.  
3. Empirical Results: The results are comprehensive, covering multiple metrics (e.g., answer accuracy, trajectory completeness) and datasets with varying complexity. The analysis is detailed, identifying key factors (e.g., depth, number of sentences) that influence performance. The findings are consistent with the claims made in the paper.
Additional Feedback
1. Task Difficulty: While the authors acknowledge concerns about QRAQ tasks being "too easy," they argue that the inclusion of variables and multi-turn reasoning increases complexity. However, the paper could benefit from a more explicit comparison of QRAQ's difficulty relative to bAbI tasks, perhaps through shared metrics or ablation studies.  
2. Scalability: The paper focuses on synthetic datasets, which are useful for controlled experiments but may not generalize to real-world dialogue. Future work could explore how QRAQ-inspired tasks perform on natural language datasets.  
3. Evaluation Metrics: The paper introduces several metrics (e.g., trajectory accuracy, query accuracy), but their relative importance is not discussed. Clarifying which metrics are most critical for evaluating agent performance would strengthen the analysis.  
4. Code and Dataset Release: The paper mentions plans to release the QRAQ dataset generator and simulator. Ensuring these resources are well-documented and accessible will maximize their impact on the research community.
Questions for the Authors
1. How does the complexity of QRAQ tasks compare quantitatively to bAbI tasks? Could you provide examples or metrics that illustrate this difference?  
2. Have you considered testing the RL agents on real-world datasets or tasks to evaluate their generalizability?  
3. The improved RL architecture (impRL) shows better performance on complex tasks. Could you elaborate on the specific design choices (e.g., soft-attention) that contribute most to this improvement?  
4. How do you envision QRAQ being extended or adapted for future research in conversational AI?  
Overall, the paper makes a valuable contribution to the field and is well-suited for acceptance at the conference. The introduction of QRAQ and the accompanying dataset generator provide a strong foundation for further exploration of reasoning and interaction in dialogue systems.