Review of the Paper
The paper presents multiple approaches to improve the performance of gradient descent in distributed deep learning systems, focusing on asynchronous layer-wise gradient descent. The authors claim that their methods maximize overlap between computation and communication, maintain equivalence to sequential gradient descent, and achieve up to 1.7x speedup compared to synchronous approaches. The implementation is evaluated on two hardware platforms (Intel Sandy Bridge cluster and NVIDIA DGX-1) using well-known neural network architectures (AlexNet and GoogLeNet) and the ImageNet dataset.
Decision: Reject
The decision to reject is based on two primary reasons: (1) limited novelty in the proposed methods, and (2) insufficient experimental rigor to convincingly demonstrate the advantages of the proposed approach.
Supporting Arguments
1. Limited Novelty: The overlap of computation and communication is a well-known optimization technique in distributed deep learning. Existing frameworks like TensorFlow and MXNet already implement similar strategies. While the paper proposes layer-wise gradient descent and delayed gradient updates, these methods are incremental rather than fundamentally novel. The dependency on manual implementation for delayed synchronization could have been replaced by a more sophisticated dependency scheduler, which would have added value to the contribution.
2. Experimental Weaknesses: The experimental evaluation lacks depth. While the authors report speedup and convergence times, they fail to provide full convergence curves for all compared networks. This omission makes it difficult to assess the impact of the proposed methods on convergence behavior and accuracy across different setups. Additionally, the results for layer-wise gradient descent are underwhelming, and the paper does not explore why this approach underperformed or how it could be improved.
Additional Feedback
1. Clarity and Positioning: The paper could better position its contributions within the broader literature. For example, a more detailed comparison with existing frameworks like TensorFlow and MXNet would clarify the novelty of the proposed methods.
2. Experimental Improvements: The authors should include full convergence curves for all networks and provide a more detailed analysis of trade-offs between speedup and accuracy. Additionally, experiments on more diverse datasets and architectures would strengthen the generalizability of the results.
3. Dependency Scheduler: The manual implementation of delayed synchronization could be replaced with a dependency scheduler to automate and optimize the process. This would enhance the practicality and scalability of the approach.
4. Layer-Wise Gradient Descent: The underperformance of layer-wise gradient descent should be investigated further. Is it due to implementation inefficiencies, hardware limitations, or fundamental issues with the approach? Addressing this could lead to more robust results.
Questions for the Authors
1. Why were full convergence curves not included for all networks? How do the proposed methods impact convergence behavior compared to synchronous approaches?
2. How does the proposed asynchronous approach compare to state-of-the-art frameworks like TensorFlow and MXNet in terms of scalability and ease of implementation?
3. Can the delayed gradient update approach be extended or automated using a dependency scheduler? If so, what would be the challenges?
In conclusion, while the paper addresses an important problem in distributed deep learning, its contributions are incremental, and the experimental evaluation lacks the rigor needed to substantiate the claims. Addressing the feedback above could significantly improve the paper's quality and impact.