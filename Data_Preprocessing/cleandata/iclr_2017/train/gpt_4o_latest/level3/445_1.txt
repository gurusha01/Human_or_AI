The paper presents a compelling exploration of online learning for conversational agents, focusing on reinforcement learning from natural human feedback. The authors introduce a system that transitions from supervised training on fixed datasets to an online learning paradigm, where the bot improves iteratively through interactions with teachers. The contributions are multifaceted, including the development of a simulator for controlled experiments, the use of real human feedback via Mechanical Turk, and the integration of textual feedback alongside numerical rewards. The results demonstrate that the system achieves near full-supervision performance in some cases, showcasing the feasibility of online learning in dialogue systems.
Decision: Accept.  
The primary reason for acceptance is the novelty and significance of the contributions. The paper addresses a critical gap in conversational AI research by moving beyond static datasets to dynamic, interactive learning. The ability to learn from noisy, natural feedback in an online setting is a notable advancement, with implications for real-world applications where labeled data is scarce or unavailable. While the work is somewhat incremental relative to Weston (2016), the extension to real human feedback and reinforcement learning is substantial and impactful.
Supporting Arguments:  
1. Novelty and Impact: The paper's focus on online learning from natural feedback is innovative and addresses a pressing challenge in conversational AI. The use of both synthetic and real human data strengthens the validity of the findings.  
2. Empirical Rigor: The experiments are thorough, with clear comparisons to baselines and prior work. The inclusion of classic supervision baselines helps contextualize the progress made.  
3. Significance of Results: The system's ability to achieve near full-supervision results in some cases demonstrates its potential for learning in less constrained, real-world settings.
Additional Feedback for Improvement:  
1. Task Formalization: The task definition of "learning dialogue" is imprecise, and the success criteria remain somewhat vague. While the authors provided a partial response, a more rigorous formalization would strengthen the paper.  
2. Baseline Quantification: While the difficulty of the machine translation (MT) setting and handcrafted baselines are discussed, clearer quantification of these baselines would enhance the interpretability of the results.  
3. Relationship to Prior Work: The connection to Weston (2016) needs better clarification. While the online learning aspect is a key novelty, the incremental nature of the work could be more explicitly addressed to highlight the unique contributions.  
4. Stability of FP: The instability of the Forward Prediction (FP) method in certain settings is noted. Further exploration of techniques to mitigate this issue, beyond data balancing and exploration, would be valuable.
Questions for the Authors:  
1. Can you provide a more precise definition of the task and success criteria for "learning dialogue"?  
2. How do you envision scaling this approach to more complex, open-domain dialogue tasks?  
3. Could you elaborate on the potential limitations of the system when deployed in real-world, noisy environments?  
4. How does the system handle ambiguous or contradictory feedback from human teachers?  
In conclusion, the paper makes a significant contribution to the field of conversational AI by advancing the state of online learning from natural feedback. While there are areas for improvement, the novelty, empirical rigor, and potential impact justify acceptance.