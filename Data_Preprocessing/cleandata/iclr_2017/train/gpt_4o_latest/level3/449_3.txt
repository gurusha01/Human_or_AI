Review of "FractalNet: Ultra-Deep Neural Networks Without Residuals"
Summary of Contributions
This paper introduces FractalNet, a novel architecture for deep convolutional networks inspired by fractal-like self-similarity. The authors propose a recursive expand-and-join design principle that generates ultra-deep networks without relying on residual connections. The paper challenges the prevailing notion that residual formulations (e.g., ResNets) are essential for training very deep networks, arguing instead that the critical factor is the ability to transition from effectively shallow to deep during training. Additionally, the authors propose drop-path regularization, a method to prevent co-adaptation of subpaths, which also imparts "anytime" properties to the network. Experimental results are presented on CIFAR, SVHN, and ImageNet datasets, with claims that FractalNet achieves competitive performance with ResNets and demonstrates robustness to overfitting, even without data augmentation.
Decision: Reject
While the paper presents an interesting and novel architectural design, it falls short in providing rigorous experimental validation and clear evidence to support its claims. The lack of proper baselines, incomplete comparisons, and inconsistent experimental setups undermine the paper's ability to convincingly demonstrate the utility of FractalNet.
Supporting Arguments for Rejection
1. Insufficient Baselines and Comparisons: 
   - The experimental evaluations do not include proper baselines, making it difficult to isolate the benefits of the fractal design. For example, comparisons to ResNets are incomplete, as Wide ResNets, which outperform FractalNet on CIFAR-100, are not adequately addressed.
   - The paper does not compare FractalNet to Inception-style architectures, which share similar design principles (e.g., shorter and longer paths). This omission is significant given the relevance of Inception networks to the proposed approach.
2. Inconsistent Experimental Setup:
   - Experiments without data augmentation are inconsistent and fail to provide a clear picture of FractalNet's performance. For instance, the 40-layer FractalNet comparison is unfair unless parameter reduction tricks are applied to other models as well.
   - The paper claims that drop-path regularization is effective, but its standalone benefit is unclear due to the simultaneous use of other regularizers (e.g., dropout).
3. Failure to Convincingly Support Claims:
   - The claim that residual connections are not essential is not convincingly demonstrated. While FractalNet matches ResNet's performance in some cases, the experiments do not establish that the fractal design is fundamentally superior or even comparable across a broader range of tasks and configurations.
   - The "anytime" property of FractalNet is mentioned but not thoroughly explored or benchmarked against existing architectures with similar properties, such as Residual/Highway networks.
Suggestions for Improvement
1. Add Proper Baselines: Include comparisons to Wide ResNets and Inception networks to provide a more comprehensive evaluation of FractalNet's performance relative to state-of-the-art architectures.
2. Clarify Drop-Path Regularization: Isolate the effect of drop-path regularization by conducting ablation studies that disentangle its contribution from other regularizers like dropout.
3. Fair Parameter Comparisons: Ensure that comparisons to other models (e.g., ResNets) are fair by applying equivalent parameter reduction tricks across all architectures.
4. Expand Anytime Analysis: Provide a more detailed analysis of the "anytime" property, including latency vs. accuracy trade-offs, and compare it to existing anytime-capable architectures.
5. Consistency in Experimental Setup: Ensure that experiments with and without data augmentation are consistent and provide clear insights into the network's behavior under different conditions.
Questions for the Authors
1. How does FractalNet perform when compared to Wide ResNets and Inception networks, which are highly relevant baselines given their design similarities?
2. Can you provide a clearer analysis of the standalone contribution of drop-path regularization? How does it compare to stochastic depth in ResNets?
3. How does the fractal design scale to tasks beyond image classification, such as object detection or segmentation? Would the recursive structure introduce computational inefficiencies in such tasks?
In conclusion, while the paper introduces an intriguing architectural concept, the lack of rigorous experimental validation and incomplete comparisons make it difficult to assess the true value of FractalNet. Addressing these issues would significantly strengthen the paper's contributions.