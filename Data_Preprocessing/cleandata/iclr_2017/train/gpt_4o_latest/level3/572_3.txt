The paper explores adversarial attacks on deep generative models, specifically Variational Autoencoders (VAEs) and VAE-GANs, presenting three novel attack methodologies. While adversarial examples have been extensively studied in classification tasks, this work extends the concept to generative models, which is an underexplored area. The authors demonstrate the attacks on MNIST, SVHN, and CelebA datasets, providing insights into the vulnerabilities of these models. The paper also discusses potential attack scenarios, such as exploiting latent space compression in communication systems, and evaluates the effectiveness of the attacks using quantitative metrics and visual reconstructions.
Decision: Reject
Key reasons for rejection:
1. Limited Novelty and Misleading Claims: While the paper addresses an interesting and underexplored problem, its novelty is constrained by concurrent work (e.g., Tabacof et al.) that is not adequately cited. Furthermore, the paper inaccurately refers to encoder-decoder networks as "generative models," which undermines the validity of its claims.
2. Suboptimal Presentation and Overlength: The paper exceeds the recommended page limit (13 pages vs. 8), and its presentation suffers from inconsistent terminology, a title-content mismatch, and a lack of clarity in motivating the attack scenarios.
Supporting Arguments:
1. Novelty: The exploration of adversarial examples for encoder-decoder generative networks is a valuable contribution. However, the concurrent work by Tabacof et al. should be cited and compared against, as it addresses similar attacks on VAEs.
2. Experimental Scope: The experiments are limited to MNIST, SVHN, and CelebA datasets. While these datasets are standard, testing on more complex datasets like CIFAR-10 or ImageNet would strengthen the paper's claims.
3. Motivation and Validation: The attack scenario described in the paper lacks convincing real-world applicability. Additionally, experiments on natural images are necessary to validate the broader relevance of the proposed attacks.
Additional Feedback for Improvement:
1. Title and Terminology: The title should be revised to reflect the focus on encoder-decoder networks rather than generative models. Misused terms like "Oracle" and "work for" should be corrected for precision.
2. Conciseness: The paper should be condensed to meet the page limit, focusing on the most critical contributions and results.
3. Motivation: The attack scenario should be better motivated with practical examples or applications. For instance, how might these attacks impact real-world systems using generative models?
4. Dataset Diversity: Expanding the experimental evaluation to include more diverse datasets, such as those involving natural images, would enhance the paper's impact.
5. Typographical Errors: Minor issues, such as the typo "confidentally" in Section 4.1, should be addressed.
Questions for the Authors:
1. How does your work compare quantitatively and qualitatively to Tabacof et al.'s concurrent work? Are there any unique contributions that differentiate your approach?
2. Can you provide more compelling real-world scenarios where these attacks would be relevant and impactful?
3. Why were datasets like CIFAR-10 or ImageNet not included in the evaluation? Do you anticipate your methods scaling effectively to these datasets?
In summary, while the paper addresses an important and novel problem, its limited originality, misleading claims, and suboptimal presentation prevent it from meeting the standards for acceptance. With significant revisions and additional experiments, it could become a stronger contribution to the field.