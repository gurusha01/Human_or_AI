The paper introduces NewsQA, a new large-scale machine comprehension dataset aimed at addressing the limitations of existing datasets like SQuAD. The authors claim that NewsQA is more challenging due to its diverse question-answer collection methodology, which separates question generation from answer annotation, encouraging reasoning-based questions. The dataset comprises over 100,000 question-answer pairs derived from CNN news articles, with a significant portion requiring synthesis and inference. Human performance on NewsQA is notably lower (74.9% F1) compared to SQuAD, highlighting its complexity. The authors also provide baseline results using two neural models, demonstrating a significant gap between human and machine performance (0.198 in F1), which they argue underscores the potential for further research.
Decision: Reject
While the paper presents an interesting and valuable dataset, it falls short in critical areas such as dataset quality assurance and methodological rigor. The concerns about noise in the QA collection process and the lack of strong evidence supporting the dataset's superiority over existing benchmarks like SQuAD weigh heavily against its acceptance.
Supporting Arguments:
1. Dataset Quality: The separation of question generation from answer annotation, while innovative, introduces noise. The paper acknowledges that human performance is lower than expected (74.9% F1), raising concerns about the dataset's reliability. The authors attempt to address this through validation, but the high proportion of questions with ambiguous or null answers (14%) suggests persistent quality issues.
2. Lack of Strong Evidence for Claims: While the authors argue that NewsQA requires more reasoning than SQuAD, the evidence provided is insufficient. The manual labeling of reasoning types is limited to 1,000 examples, which is not representative of the entire dataset. Additionally, the performance gap between humans and machines, while significant, could also stem from noise rather than increased complexity.
3. Missed Opportunities for Improvement: The paper identifies potential weaknesses, such as the lack of context during question generation leading to generic questions, but does not propose or experiment with solutions like hybrid approaches or automatic QA systems to discourage trivial questions.
Suggestions for Improvement:
1. Dataset Quality Control: The authors should provide more robust validation mechanisms to reduce noise. For example, incorporating a hybrid approach where partial context is shown during question generation could improve question quality.
2. Stronger Evidence for Complexity: The reasoning taxonomy analysis should be expanded to a larger subset of the dataset. Additionally, comparisons with SQuAD should control for factors like article length and question difficulty to isolate the effect of reasoning complexity.
3. Automatic QA System: Experimenting with an automatic QA system to filter out trivial questions, as suggested in the paper, could significantly enhance the dataset's utility.
Questions for Authors:
1. How do you plan to address the high proportion of ambiguous or null-answer questions in the dataset?
2. Could you provide a more detailed breakdown of human performance across different reasoning types to better understand the dataset's challenges?
3. Have you considered experimenting with hybrid question generation approaches to improve question specificity and reduce noise?
In summary, while NewsQA is a promising dataset with potential to advance machine comprehension research, the paper lacks the methodological rigor and evidence needed to justify its acceptance in its current form. Addressing the identified weaknesses could make it a stronger contribution in the future.