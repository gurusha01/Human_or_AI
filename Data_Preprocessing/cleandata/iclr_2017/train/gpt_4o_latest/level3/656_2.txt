Review of the Paper
Summary of Contributions
This paper introduces a hybrid deep learning architecture that combines scattering networks with convolutional neural networks (CNNs). The scattering network serves as a fixed, unsupervised initialization for the first layers, while the subsequent layers are trained in a supervised manner. The authors claim that this hybrid approach achieves competitive performance on standard benchmarks (CIFAR10, CIFAR100, STL10) while offering advantages such as stability to geometric transformations, reduced parameter count, and computational efficiency. The paper also explores the generalization capabilities of the hybrid network, particularly in small-data regimes, and provides a GPU-based implementation of scattering networks, ScatWave, for reproducibility.
Decision: Reject
While the paper presents a novel approach by integrating scattering networks with CNNs, it falls short in several critical areas. The primary reasons for rejection are: (1) lack of clarity in the comparison methodology, and (2) insufficient empirical evidence to support the claimed superiority of the hybrid model in generalization and computational efficiency.
Supporting Arguments
1. Novelty and Motivation: The exploration of fixed low-level features via scattering networks is an interesting and underexplored idea. However, the paper does not sufficiently justify why scattering features are preferable to learned CNN features, especially since the latter consistently outperform scattering-based features in the experiments.
   
2. Empirical Results: While the hybrid network performs well in small-data settings, its performance on full datasets is inferior to fully learned architectures like Wide ResNet. The claim of better generalization is questionable, as the hybrid model's advantage diminishes with larger datasets. Additionally, the experiments do not directly compare identical architectures with and without scattering layers, making it difficult to isolate the contribution of the scattering network.
3. Theoretical Claims: The paper emphasizes the stability of scattering networks but does not adequately address how the instability of the learned layers impacts the overall stability of the hybrid model. Furthermore, the claim that scattering networks save computational resources is not rigorously quantified or explored in practical low-power device scenarios.
4. Clarity and Presentation: The paper contains a typographical error in Section 3.1.2 ("learni"), and the methodology for comparing architectures is not clearly described. For example, it is unclear whether the scattering layers and the learned layers were optimized jointly or separately.
Suggestions for Improvement
1. Comparison Methodology: Conduct experiments that directly compare identical architectures with and without scattering layers to isolate the impact of scattering features. This would strengthen the empirical evidence for the hybrid model's effectiveness.
2. Generalization Claims: Provide more rigorous analysis and experiments to support the claim of better generalization, particularly on larger datasets. Consider exploring additional datasets or tasks to validate the hybrid approach.
3. Computational Efficiency: Quantify the computational savings of scattering networks in terms of runtime, memory usage, and energy consumption, particularly on low-power devices. This would make the practical benefits of the proposed approach more compelling.
4. Theoretical Analysis: Address the interplay between the stability of scattering layers and the instability of learned layers in the hybrid model. A more thorough theoretical discussion would strengthen the paper's claims.
5. Clarity: Improve the clarity of the methodology and results presentation. Ensure that all claims are backed by clear and reproducible experiments.
Questions for the Authors
1. How were the scattering layers and the learned layers optimized? Were they trained jointly or separately?
2. Why were identical architectures with and without scattering layers not directly compared in the experiments?
3. Can you provide quantitative evidence of the computational savings achieved by the hybrid model, particularly on low-power devices?
4. How does the instability of the learned layers affect the overall stability of the hybrid network?
In conclusion, while the paper introduces a novel hybrid architecture and demonstrates its potential in small-data regimes, it lacks the rigor and clarity needed to justify its broader claims. Addressing the above concerns could significantly strengthen the paper.