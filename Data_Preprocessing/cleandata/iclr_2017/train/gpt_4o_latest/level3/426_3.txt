Review
Summary
The paper addresses the task of mapping word embeddings between two languages using bilingual information, extending prior work by introducing two key contributions: the use of orthogonal transformations via Singular Value Decomposition (SVD) and a novel "inverted softmax" normalization technique. The authors demonstrate that orthogonal transformations are theoretically optimal for aligning vector spaces and empirically show that their method improves translation precision, achieving a precision @1 of 43% when translating English to Italian. Notably, the paper explores the use of pseudo-dictionaries derived from identical character strings and aligned sentence corpora, showcasing the robustness of their approach even without expert bilingual dictionaries. The inverted softmax is highlighted as a significant improvement over prior retrieval methods, addressing the hubness problem in word translation tasks. The paper also extends the method to sentence-level translations, achieving a precision @1 of 68% for retrieving sentence translations from a large corpus.
Decision: Reject
While the paper presents incremental improvements and some clever techniques, the novelty of the contributions is limited. Key components, such as orthogonal transformations and CCA-based alignment, have been explored in prior work (e.g., Xing et al., 2015; Faruqui & Dyer, 2014). The main improvement stems from the inverted softmax, which, while effective, does not constitute a sufficiently novel or transformative contribution to warrant acceptance at a top-tier conference. Additionally, the reliance on cognates for pseudo-dictionaries limits the generalizability of the method to languages with shared alphabets, further constraining its impact.
Supporting Arguments
1. Limited Novelty: The theoretical insight that the transformation should be orthogonal has been previously discussed (e.g., Xing et al., 2015). Similarly, the use of SVD for alignment is not new and has been employed in CCA-based methods (Faruqui & Dyer, 2014). The inverted softmax, while novel, is a relatively minor modification to the retrieval process.
2. Applicability Constraints: The use of cognates as a pseudo-dictionary is clever but inherently limited to language pairs with shared alphabets and significant lexical overlap. This restricts the broader applicability of the method, especially for typologically diverse languages.
3. Empirical Results: While the empirical results are solid and demonstrate improvements over baseline methods, the gains are incremental rather than groundbreaking. The precision @1 improvement from 34% to 43% is notable but not transformative.
Suggestions for Improvement
1. Clarify Novelty: The authors should explicitly differentiate their contributions from prior work, particularly Xing et al. (2015) and Faruqui & Dyer (2014). A more detailed discussion of how the inverted softmax compares to other normalization techniques would strengthen the paper.
2. Expand Applicability: Address the limitations of using cognates for pseudo-dictionaries. For example, could the method be extended to languages with different scripts by leveraging phonetic similarity or transliteration techniques?
3. Sentence-Level Analysis: The sentence-level translation results are intriguing but underexplored. A deeper analysis of why the method performs well for sentence retrieval and how it could be improved (e.g., incorporating word order) would add value.
4. Terminology: The term "cognate" is misused in the paper, as it traditionally refers to words with shared etymological origins, not merely identical written forms. The authors should adopt more precise terminology to avoid confusion.
Questions for the Authors
1. How does the inverted softmax compare to other normalization techniques, such as temperature scaling or probabilistic approaches, in terms of mitigating the hubness problem?
2. Can the proposed method be adapted for language pairs with different scripts or minimal lexical overlap? If so, how?
3. Why was dimensionality reduction applied after SVD, and how does it impact the results? Would the method perform similarly without this step?
In summary, while the paper demonstrates solid empirical results and introduces a useful normalization technique, the lack of significant novelty and limited applicability reduce its overall impact. Addressing these issues in a future submission could enhance its contribution to the field.