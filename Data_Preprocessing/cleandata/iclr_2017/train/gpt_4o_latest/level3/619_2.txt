Review of the Paper
Summary of Contributions
This paper proposes a simple method of adding annealed Gaussian noise to gradients during training to improve the optimization of deep neural networks. The authors claim that this technique helps escape saddle points and poor local minima, and can be applied to a variety of complex architectures, such as Neural GPUs, Neural Random Access Machines (NRAM), and End-to-End Memory Networks. The method is presented as complementary to existing optimization techniques like Adam and AdaGrad, and its simplicity makes it easy to implement. The authors provide experimental results across several tasks, including MNIST classification, question answering, and algorithm learning, to demonstrate the effectiveness of the proposed method.
Decision: Reject  
The primary reasons for rejection are:  
1. Incremental Contribution: The proposed method is a minor extension of existing techniques, such as Stochastic Gradient Langevin Dynamics (SGLD), and lacks novelty.  
2. Outdated Context: The paper does not engage with or compare its method to more recent advancements, such as batch normalization for RNNs (Cooijmans et al., 2016) and layer normalization. This omission weakens the relevance of the work.  
3. Empirical Evidence: While the authors provide experimental results, the evidence is insufficient to demonstrate that the proposed method consistently outperforms or is significantly distinct from existing strategies.  
Supporting Arguments
1. Lack of Novelty: The idea of adding noise to gradients is not new, as it has been explored in the context of SGLD and related techniques. The annealing schedule proposed here is similar to prior work, and the paper does not provide a strong theoretical or empirical justification for why this approach is superior to existing methods.  
2. Incorrect Claims: The authors claim that batch normalization is ineffective for RNNs, which is contradicted by prior work (e.g., Cooijmans et al., 2016). This undermines the credibility of the paper and suggests a lack of engagement with recent literature.  
3. Empirical Weaknesses: The experiments show some improvements with gradient noise, but the results are not compelling enough to establish the method as a robust alternative to existing techniques. For example, the method fails to improve performance in language modeling tasks, and its benefits are inconsistent across different architectures.  
Suggestions for Improvement
1. Engage with Recent Work: The paper should address and compare its method to recent advancements in optimization techniques, such as batch normalization for RNNs and layer normalization. This would help position the work within the current state of the field.  
2. Theoretical Insights: Providing a stronger theoretical foundation for why annealed gradient noise is effective, particularly in the context of modern optimizers like Adam, would strengthen the contribution.  
3. Broader Empirical Evaluation: The experiments should include comparisons with more recent techniques and provide stronger evidence of the method's generalizability across diverse tasks and architectures.  
4. Clarify Limitations: The authors should explicitly discuss the limitations of their method, such as its failure in language modeling tasks, and explore potential reasons for these shortcomings.  
Questions for the Authors
1. How does the proposed method compare to batch normalization or layer normalization in terms of performance and ease of implementation?  
2. Can you provide more theoretical insights into why annealed gradient noise outperforms fixed noise or other stochastic optimization techniques?  
3. Why do you believe the method fails in language modeling tasks, and how might it be adapted to address this limitation?  
In summary, while the paper introduces an interesting and simple optimization technique, its incremental nature, lack of engagement with recent literature, and insufficient empirical evidence make it unsuitable for publication in its current form.