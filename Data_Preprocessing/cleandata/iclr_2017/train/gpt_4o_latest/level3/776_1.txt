Review of the Paper
Summary of Contributions
The paper proposes an iterative machine translation method inspired by human translator behavior, where translations are refined iteratively through attention-based models. The authors present two models: a single attention model and a dual attention model, which leverage both the source sentence and the current translation output to suggest word substitutions. The iterative refinement process improves the output of a phrase-based translation system by up to 0.4 BLEU on the WMT15 German-English task, with minimal modifications (0.6 substitutions per sentence). The method is novel in its ability to revisit and correct earlier translation decisions, unlike traditional left-to-right decoding schemes.
Decision: Reject  
The paper demonstrates creativity in its approach, but it fails to provide convincing evidence of the iterative method's advantages over stronger baselines. Key concerns include the limited improvement in BLEU, insufficient comparisons with alternative methods, and the lack of clarity on how the iterative process avoids introducing new errors. Additionally, the experimental design and presentation suffer from several weaknesses, as detailed below.
Supporting Arguments for Decision
1. Lack of Strong Baselines and Comparisons: The paper benchmarks its results against a phrase-based machine translation (PBMT) system, which is not state-of-the-art. Comparisons with modern neural machine translation (NMT) systems, such as Transformer-based models, are absent. This omission weakens the claim that the iterative approach provides meaningful improvements.
   
2. Low Error Detection Accuracy: The probability of correctly identifying mistakes (62%) is low and not compared to meaningful alternatives. This undermines the reliability of the iterative refinement process, as it depends on accurate error detection to succeed.
3. Oracle Experiments Are Uninformative: The oracle experiments highlight the potential for improvement but fail to address the core challenge of real-world applicability. They merely show that known mistakes can be fixed, without demonstrating that the proposed method can reliably identify and correct errors on its own.
4. Mismatch Between Training and Testing Conditions: The dual attention model uses reference translations during training but replaces them with guess translations during testing, leading to a distribution mismatch. This issue is acknowledged but not adequately addressed, raising concerns about the robustness of the model.
5. Complex and Unclear Notation: The paper's notation is unnecessarily complex, making it difficult to follow key ideas. Simplifying the presentation would improve accessibility and clarity.
Suggestions for Improvement
1. Stronger Baselines: Compare the iterative method against modern NMT systems, such as Transformers, to better contextualize its performance.
2. Error Detection Improvements: Provide a more robust error detection mechanism and compare its accuracy with alternative approaches.
3. Clarify Iterative Process: Explain how the iterative refinement avoids introducing new errors and why it is not used outright if it is better than the initial guess.
4. Simplify Notation: Streamline the mathematical notation to make the paper more readable and accessible to a broader audience.
5. Address Training-Test Mismatch: Propose solutions to mitigate the distribution mismatch between training and testing conditions, such as fine-tuning on guess translations.
Questions for the Authors
1. Why was the iterative refinement process not directly compared to state-of-the-art NMT systems like Transformers?
2. How does the proposed method ensure that new errors are not introduced during the iterative correction process?
3. Could the low BLEU improvement (+0.4) be attributed to limitations in the error detection model? If so, how might this be addressed?
4. Why was the dual attention model trained with reference translations, given that they are unavailable during testing? Would training on guess translations improve performance?
In conclusion, while the paper introduces an imaginative approach to machine translation, it falls short in demonstrating its practical utility and robustness. Addressing the above concerns and providing stronger empirical evidence could significantly strengthen the work.