Review
Summary of Contributions
This paper addresses the critical problem of detecting and localizing extreme weather events in large-scale climate simulation data using deep learning. The authors propose a 3D convolutional encoder-decoder architecture for semi-supervised bounding box prediction, leveraging temporal and multichannel spatial data. The approach combines supervised learning with a reconstruction loss to utilize unlabeled data, aiming to improve detection accuracy for events like hurricanes and atmospheric rivers. The paper highlights the potential of deep learning to replace hand-engineered heuristics in climate science, offering a scalable solution for analyzing massive datasets. The inclusion of open-source code and data on GitHub is commendable and supports reproducibility.
Decision: Reject
While the paper tackles an important problem and presents a promising approach, it falls short in experimental rigor and clarity. The lack of robust experimental validation, unclear metrics, and insufficient ablation studies undermine the paper's claims. Additionally, the semi-supervised loss terminology and the use of objectness loss require further justification and refinement.
Supporting Arguments
1. Experimental Results: The results are sparse and lack sufficient depth. Metrics for evaluation are unclear, and the patterns in the reported results are inconsistent. For example, while the 3D semi-supervised model shows promise for rough localization (IOU=0.1), it performs poorly on stricter metrics (IOU=0.5). The lack of ablation studies to isolate the contributions of different components (e.g., supervised vs. semi-supervised loss) weakens the claims.
2. Clarity and Notation: While the paper is generally easy to follow, the notation is sloppy, with unclear definitions and inconsistent references. The semi-supervised loss terminology is particularly confusing, as it uses all supervised labels in addition to reconstruction loss, making it stronger than the supervised loss. This should be clarified and better justified.
3. Reproducibility: Although the authors provide GitHub links for code and data, the paper does not detail the experimental setup sufficiently. For example, hyperparameter choices and training protocols are not described in enough detail to ensure reproducibility.
4. Novelty and Value: The paper's novelty is limited, as it primarily adapts existing computer vision techniques to climate data. While this application is valuable, the lack of strong experimental validation diminishes its impact.
Suggestions for Improvement
1. Experimental Rigor: Provide more comprehensive results, including detailed metrics, confidence intervals, and comparisons with baseline methods. Conduct ablation studies to isolate the contributions of different components (e.g., 2D vs. 3D models, supervised vs. semi-supervised approaches).
2. Clarity: Refine the notation and terminology, particularly for the semi-supervised loss. Clearly define all terms and ensure consistency throughout the paper.
3. Objectness Loss: Justify the use of both classification and objectness losses, which is non-standard. Provide experimental validation to demonstrate their necessity and impact.
4. Reproducibility: Include detailed descriptions of the experimental setup, such as hyperparameters, training schedules, and computational resources used.
5. Broader Validation: Extend the evaluation to additional datasets or scenarios to demonstrate generalizability. For example, applying the model to observational or reanalysis data would strengthen its practical relevance.
Questions for the Authors
1. Can you clarify the rationale behind the semi-supervised loss formulation? Why does it include all supervised labels, and how does this differ from standard semi-supervised approaches?
2. How were the hyperparameters (e.g., λ, α, β, γ) selected, and how sensitive are the results to these choices?
3. Can you provide more details on the evaluation metrics used? For example, how were IOU thresholds chosen, and why are they appropriate for this task?
4. Have you considered using anchor boxes of varying sizes/shapes to address the poor performance on IOU=0.5? If so, what were the results?
In summary, while the paper addresses a meaningful problem and proposes a potentially impactful approach, it requires significant improvements in experimental validation, clarity, and justification of design choices before it can be accepted.