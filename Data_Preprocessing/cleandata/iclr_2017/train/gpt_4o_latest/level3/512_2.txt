Review
Summary of Contributions
This paper introduces a novel framework for nonparametrically learning activation functions in deep neural networks, expanding the flexibility of neural network architectures. The authors propose a two-stage optimization process that integrates the learning of activation functions into the backpropagation framework. The use of Fourier basis expansion for activation functions is highlighted, and the theoretical justification for this approach is provided, including generalization bounds using algorithmic stability. Empirical results on MNIST and CIFAR-10 datasets demonstrate up to a 15% relative improvement in test performance over baseline methods. The paper also explores the impact of nonparametric activation functions in convolutional and fully connected layers, showing significant improvements when combined with dropout and a two-stage training procedure.
Decision: Reject
While the paper presents an interesting and promising idea, it falls short in a few critical areas that prevent acceptance at this stage. The primary reasons for rejection are the lack of clarity in theoretical analysis and insufficient experimental comparisons. These gaps hinder the paper's ability to fully substantiate its claims and demonstrate the robustness of its proposed method.
Supporting Arguments for Decision
1. Theoretical Analysis: While the theoretical framework is detailed, it is not sufficiently informative or focused. The extensive mathematical derivations, while rigorous, are difficult to follow and do not clearly connect to the practical implications of the proposed method. This distracts from the central contributions and makes it challenging to assess the theoretical soundness of the approach.
   
2. Experimental Comparisons: The experiments are limited in scope. The paper primarily uses Fourier basis expansion but does not explore other basis functions (e.g., polynomial basis) in sufficient depth. Additionally, comparisons with wider networks or alternative methods for learning activation functions, such as piecewise linear functions or other nonparametric approaches, are missing. This limits the ability to evaluate the generality and superiority of the proposed method.
3. Clarity on Nonlinearity Weights: The paper does not clarify whether the nonlinearity weights are shared across all units and layers or are unique to each unit. This is a critical design choice that can significantly impact the method's scalability and performance.
Suggestions for Improvement
1. Focus Theoretical Analysis: Simplify and focus the theoretical contributions to highlight their practical relevance. For example, emphasize the implications of the generalization bounds and how they support the empirical results.
   
2. Expand Experimental Scope: Include experiments with alternative basis functions and compare the proposed method against wider networks and other state-of-the-art approaches for learning activation functions. This will strengthen the empirical support for the method's effectiveness.
3. Clarify Design Choices: Clearly state whether nonlinearity weights are shared across units and layers or are unique. Additionally, investigate the existence of optimal nonlinearities when weights are tied across units and layers.
4. Normalization Impact: Explore the effect of hidden unit normalization techniques (e.g., batch normalization) on the learned nonlinearities. This could provide insights into the interaction between normalization and the proposed method.
5. Polynomial Basis Analysis: The paper concludes that polynomial bases fail but does not provide sufficient analysis to support this claim. A deeper investigation into why Fourier basis works while polynomial basis does not would be valuable.
Questions for Authors
1. Are the nonlinearity weights shared across all units and layers, or are they unique to each unit? How does this choice affect scalability and performance?
2. Why does the Fourier basis succeed where polynomial basis fails? Can you provide more detailed analysis or experiments to support this conclusion?
3. How does the proposed method interact with normalization techniques like batch normalization? Does normalization affect the learned activation functions or the two-stage training process?
By addressing these issues, the paper could significantly improve its clarity, rigor, and impact.