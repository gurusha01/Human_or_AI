Review of the Paper
The paper introduces a novel framework for dynamically adapting the size of neural networks during training, termed nonparametric neural networks. This approach eliminates the need for prior knowledge about the optimal network size, a significant challenge in model selection. The authors propose a theoretical foundation for their method, demonstrating that under fan-in or fan-out regularization, the error function achieves its minimum with a finite number of parameters, thus preventing unbounded network growth. Additionally, the paper introduces a new optimization algorithm, Adaptive Radial-Angular Gradient Descent (AdaRad), and a modified normalization layer, CapNorm, to support the training process.
Decision: Reject
While the paper presents an interesting and potentially impactful idea, it falls short in several critical areas that undermine its overall contribution. The primary reasons for rejection are the insufficient discussion of the theoretical results and the lack of clarity and rigor in the experimental validation.
Supporting Arguments for the Decision
1. Theoretical Results: Theorem 1 is central to the paper but lacks sufficient discussion about its implications, practical insights, and intuition. The connection to traditional sparse regularization techniques like Lasso or Elastic Net is mentioned but not explored in depth, leaving the reader without a clear understanding of the broader significance of the result.
2. Experimental Validation: The empirical results are mixed, with nonparametric networks outperforming parametric ones in some cases but underperforming in others (e.g., the convex dataset). The paper does not adequately explain why the proposed method works better in some scenarios and worse in others. Furthermore, the claim that nonparametric networks outperform parametric networks of the same size is not convincingly validated. A direct experiment comparing a dynamically grown network to a fixed-size network with the same final structure is missing.
3. Clarity and Completeness: The definition of "zero units" and their impact on the regularizer value is unclear. The paper also does not address the sensitivity of the final network structure to initialization or the potential effects of combining fan-in and fan-out regularizers. These omissions weaken the overall rigor of the work.
Suggestions for Improvement
1. Theoretical Discussion: Expand the discussion around Theorem 1, providing intuition, practical implications, and a comparison to existing sparse regularization techniques. This would help readers better understand the novelty and importance of the result.
2. Experimental Design: Include an experiment comparing the proposed dynamic model to a fixed-size network with the same final number of nodes. This would provide stronger evidence for the efficiency and effectiveness of the method.
3. Clarifications: Clearly define "zero units" and explain why adding them changes the regularizer value. Address the role of initialization and the interaction between fan-in and fan-out regularizers.
4. Regularizer Parameter: Explore whether the regularization parameter (λ) can be predicted based on data properties, as this could provide a more systematic approach to tuning the model.
5. Mixed Results: Investigate and explain the conditions under which nonparametric networks outperform or underperform parametric networks. This analysis could guide practitioners in applying the method effectively.
Questions for the Authors
1. Why does the nonparametric approach sometimes outperform a parametric network trained from scratch when the final structure is known? Can you provide more intuition or analysis to explain this phenomenon?
2. How sensitive is the final network structure to the choice of initialization? Have you conducted experiments to evaluate this?
3. Can you clarify the definition of "zero units" and why their addition changes the regularizer value? Shouldn't both fan-in and fan-out weights be zero for a unit to be considered redundant?
4. Have you explored the potential benefits or drawbacks of combining fan-in and fan-out regularizers? How does this affect the theoretical guarantees?
5. Could the regularization parameter (λ) be dynamically adjusted or predicted based on data characteristics? If so, how might this impact the results?
While the paper has potential, addressing these issues would significantly strengthen its contribution and make it more suitable for acceptance in future iterations.