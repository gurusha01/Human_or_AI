Review of the Paper
The authors present a novel, language-agnostic approach for generating semantically similar clusters and outliers using Wikidata as a graph, culminating in the release of the WikiSem500 dataset. This dataset is positioned as a resource for intrinsic evaluation of word embeddings, particularly in the outlier detection task. The authors also demonstrate correlations between performance on WikiSem500 and downstream sentiment analysis tasks, suggesting its utility for evaluating semantic similarity in embeddings. The multilingual nature of the dataset is a notable contribution, as it spans five languages, offering diversity and scalability. The authors are commended for releasing the dataset to the community, which has potential for advancing research in semantic evaluation.
Decision: Reject
The primary reasons for rejection are the limited scale and quality of the dataset, as well as insufficient alignment with the conference's focus. While the dataset is socially useful, its size is relatively small (comparable to SemEval datasets), which limits its impact given the scalability of the proposed algorithm. Furthermore, the paper lacks rigorous comparisons to existing datasets or tasks, failing to demonstrate how WikiSem500 is better suited for semantic similarity evaluation. The misalignment with ICLR's focus is also evident, as the paper provides limited insights into the underlying algorithms or theoretical contributions, such as why GloVe performs poorly in this context.
Supporting Arguments
1. Dataset Scale and Quality: Although the authors employ heuristics to refine the dataset, manual checks reveal issues in cluster quality. The use of Mechanical Turk or similar crowdsourcing methods for post-facto filtering, akin to ImageNet, could have improved scalability and quality. The dataset's limited size (13,314 test cases across five languages) is a missed opportunity given the scalability of the algorithm.
2. Lack of Comparisons: The paper does not adequately compare WikiSem500 with existing datasets like SimLex-999 or Camacho-Collados & Navigli's outlier detection dataset. Without such comparisons, it is unclear how WikiSem500 advances the state of semantic similarity evaluation.
3. Conference Misalignment: The paper lacks theoretical depth and insights into the performance of embeddings. For instance, the poor performance of GloVe is noted but not analyzed, leaving a gap in understanding. This limits the paper's relevance to ICLR's focus on learning representations.
Additional Feedback
1. Citations: The authors should cite Chang et al. (2009) for the "word-in-cluster" task, as it closely aligns with their methodology. This would better situate the work within the literature.
2. Downstream Correlations: While the correlation with sentiment analysis is promising, the authors should explore additional downstream tasks to strengthen the dataset's utility. For example, correlations with syntactic tasks could provide a more comprehensive evaluation.
3. Dataset Expansion: The authors should consider scaling the dataset significantly, leveraging the algorithm's inherent scalability. This would make WikiSem500 more impactful and competitive with larger datasets.
4. Human Baseline: The human evaluation results highlight domain knowledge challenges. The authors could provide more detailed error analysis to identify systemic issues in the dataset.
Questions for the Authors
1. How does WikiSem500 compare quantitatively and qualitatively to existing datasets like SimLex-999 or the dataset by Camacho-Collados & Navigli (2016)?
2. Why does GloVe perform poorly on WikiSem500, and what insights can be drawn from this result?
3. Could you elaborate on the scalability of the dataset generation process? How large could the dataset realistically become with additional resources?
By addressing these concerns and expanding the dataset, the authors could significantly improve the paper's impact and relevance.