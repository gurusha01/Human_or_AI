Review
Summary of Contributions
This paper introduces a novel, domain-agnostic data augmentation technique that operates in the embedding (feature) space rather than the input space. The authors propose perturbing, interpolating, and extrapolating between context vectors generated by sequence autoencoders (SA) to create synthetic data. The technique is evaluated across diverse datasets, including spoken Arabic digits, handwritten characters, sign language, motion capture, and images, demonstrating its generalizability. Notably, extrapolation in feature space consistently improves model performance, rivaling state-of-the-art results on some benchmarks. The paper argues that this approach complements domain-specific input space augmentation and is particularly effective for complex decision boundaries.
Decision: Reject
While the concept of feature space augmentation is intriguing and well-motivated, the paper has several critical limitations that need to be addressed before acceptance. The key reasons for rejection are: (1) the limited exploration of more complex baseline models, which raises concerns about the generality of the reported gains, and (2) insufficient clarity in the comparison between using augmented context vectors for classification versus reconstructed inputs.
Supporting Arguments
1. Novelty and Motivation: The idea of performing data augmentation in feature space is well-motivated and timely, given the resurgence of unsupervised representation learning. The authors provide a strong theoretical basis for their approach, citing prior work on manifold learning and feature space representations. The domain-agnostic nature of the method is a significant advantage, as it avoids the need for domain-specific transformations.
2. Empirical Results: The experiments demonstrate the effectiveness of feature space extrapolation across diverse datasets. The results on MNIST and CIFAR-10 are particularly compelling, showing that feature space augmentation can outperform traditional input space transformations. However, the reliance on a simple 2-layer MLP as the baseline model limits the broader applicability of these findings. It is unclear whether the reported improvements would hold for more complex architectures, such as convolutional or transformer-based models.
3. Clarity of Comparisons: The paper does not sufficiently clarify the trade-offs between using augmented context vectors directly for classification versus reconstructing inputs from these vectors. This distinction is critical for understanding the practical utility of the proposed method.
4. Complementarity of Techniques: While the authors hypothesize that input and feature space augmentations may be complementary, this claim is not rigorously tested. A more thorough exploration of their interaction could strengthen the paper's contributions.
Suggestions for Improvement
1. Baseline Models: Evaluate the proposed technique using more complex models, such as CNNs or transformers, to ensure the reported gains are not specific to simple architectures.
2. Comparison of Augmentation Strategies: Provide a clearer comparison between using augmented context vectors directly versus reconstructing inputs. Include experiments to quantify the trade-offs in terms of model performance and computational cost.
3. Complementarity Analysis: Conduct experiments to explicitly test whether input and feature space augmentations are complementary. For example, combine traditional input space transformations with feature space extrapolation and analyze the results.
4. Ablation Studies: Include more detailed ablation studies to isolate the effects of noise, interpolation, and extrapolation. This would help validate the claim that extrapolation is particularly effective for complex decision boundaries.
Questions for the Authors
1. How does the performance of feature space augmentation compare when using more complex models, such as CNNs or transformers, instead of a 2-layer MLP?
2. Could you provide additional insights into the computational cost of augmenting data in feature space versus input space, particularly for large-scale datasets like CIFAR-10?
3. Have you considered using other representation learning models (e.g., variational autoencoders or contrastive learning frameworks) to construct the feature space? How might this affect the results?
In summary, while the paper presents an interesting and promising idea, its current limitations in experimental rigor and clarity prevent it from being ready for acceptance. Addressing these issues would significantly strengthen the work.