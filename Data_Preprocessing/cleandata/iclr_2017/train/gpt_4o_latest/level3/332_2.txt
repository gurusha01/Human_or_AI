Review of the Paper
Summary of Contributions
This paper introduces a novel approach to perceptual similarity judgment by fine-tuning a deep convolutional neural network (DCNN) with object persistence constraints, resulting in the Object Persistence Net (OPnet). The authors demonstrate that OPnet modifies the view-manifold of object representations, enabling better discrimination of objects within the same category while maintaining inter-categorical distinctions. The paper highlights OPnet's ability to generalize similarity judgments to novel objects and categories, including synthetic datasets, and shows improved alignment with human perceptual similarity judgments compared to AlexNet. The use of a synthetic dataset for training is an innovative way to avoid the cost of large-scale human-labeled data. The authors also provide detailed experimental results, including comparisons with existing models and analyses of the view-manifold structure.
Decision: Reject  
While the paper presents an interesting idea and demonstrates promising results, it falls short in several critical areas that limit its scientific rigor and broader applicability. The key reasons for rejection are:  
1. Overstated Claims: The claim that OPnet "remarkably" outperforms AlexNet in similarity judgments is not fully substantiated, particularly given the limited experiments on real-world datasets.  
2. Insufficient Real-World Validation: The reliance on synthetic datasets without sufficient testing on real-world data undermines the generalizability of the approach.  
3. Conceptual Overreach: The connection to human vision and cognitive science is overstated and lacks sufficient grounding in neuroscience or psychology.
Supporting Arguments
1. Overstated Claims: The use of the term "remarkably" to describe OPnet's performance over AlexNet is not justified. While the results on synthetic datasets are promising, the improvement on real-world datasets is marginal, as acknowledged by the authors themselves. This weakens the claim of significant superiority.  
2. Synthetic vs. Real Data: Although the synthetic dataset is well-constructed, the lack of extensive experiments on real-world datasets limits the practical applicability of the findings. The authors note that OPnet's advantage diminishes when tested on real-world data, which raises concerns about overfitting to synthetic biases.  
3. Human Vision Analogies: The paper's narrative overreaches by drawing strong parallels between OPnet's behavior and human perceptual similarity judgment. Terms like "object persistence" are used in ways that may mislead readers, as they omit related concepts like occlusion and continuity. The connection to human vision is speculative and not empirically validated.  
Suggestions for Improvement
1. Expand Real-World Experiments: Conduct more experiments on real-world datasets with diverse variations (e.g., lighting, occlusion, and scale) to better demonstrate the model's generalizability.  
2. Refine the Introduction: Simplify the narrative and avoid overreaching into cognitive science or neuroscience unless supported by empirical evidence. Focus on the technical contributions and their implications for machine learning and computer vision.  
3. Improve Methodological Rigor: In Section 3.1, use the entire matrix of instance-to-instance similarity assessments rather than tree-to-tree distance comparisons to better align with human perceptual data.  
4. Clarify Terminology: Avoid using terms like "object persistence" without fully addressing related concepts, such as occlusion, to prevent misinterpretation.  
5. Calibrate Claims: Refrain from using terms like "remarkably" unless the results are consistently strong across all datasets, including real-world scenarios.  
Questions for the Authors
1. How does OPnet perform on datasets with more complex real-world variations, such as lighting, occlusion, and cluttered backgrounds?  
2. Could you provide more quantitative evidence to support the claim that OPnet better aligns with human perceptual similarity judgments?  
3. Why was the tree-to-tree distance comparison used in Section 3.1 instead of the full similarity matrix, and how might this choice affect the results?  
4. Could the synthetic dataset's biases be influencing OPnet's performance? How do you plan to address this in future work?  
In conclusion, while the paper presents an innovative idea, it requires stronger empirical validation and a more grounded narrative to make a compelling case for acceptance.