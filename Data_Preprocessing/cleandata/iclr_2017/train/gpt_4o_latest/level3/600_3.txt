The paper presents a novel approach to question classification by leveraging answer data through Group Sparse Autoencoders (GSA) and integrating them into a Group Sparse Convolutional Neural Network (GSCNN). The authors argue that traditional question classification methods fail to utilize the rich semantic information in corresponding answer sets, which could enhance question representations. The proposed GSA framework introduces inter- and intra-group sparse constraints, enabling more structured and meaningful representations. By embedding GSA into CNNs, the authors achieve significant performance improvements on four datasets, particularly in multi-label classification tasks with hierarchical categories.
Decision: Reject
While the paper introduces an innovative method and demonstrates its potential, the decision to reject is based on two key reasons:
1. Outdated Baselines: The paper compares its method primarily against older baselines, such as vanilla CNNs, without benchmarking against more recent and competitive CNN- and RNN-based models. This limits the ability to assess the true efficacy of the proposed approach.
2. Lack of Component Analysis: The paper does not provide a detailed ablation study to isolate and evaluate the contributions of individual components, such as the GSA module, to the overall performance.
Supporting Arguments:
1. Novelty and Contribution: The introduction of GSA as a neural network-based alternative to Sparse Group Lasso is a noteworthy contribution. The visualization of group sparsity and the integration of GSA into CNNs are compelling ideas that could inspire further research in structured representation learning.
2. Empirical Results: The proposed method shows significant improvements on datasets like Insurance and DMV, which involve multi-label classification and hierarchical categories. However, the marginal gains on the TREC and Yahoo datasets suggest that the method's advantages may be limited to specific scenarios.
3. Comparison with Recent Methods: The absence of comparisons with state-of-the-art models, such as attention-based transformers or advanced RNN variants, weakens the empirical validation. Without these comparisons, it is unclear whether the improvements are due to the novelty of the method or simply the choice of weaker baselines.
Suggestions for Improvement:
1. Benchmarking Against Recent Methods: Include comparisons with state-of-the-art CNN, RNN, and transformer-based models to provide a more comprehensive evaluation.
2. Ablation Study: Conduct an ablation study to quantify the individual contributions of GSA and other components in the GSCNN framework.
3. Broader Dataset Evaluation: Extend experiments to more diverse datasets, including those with longer question-answer pairs, to assess the generalizability of the method.
4. Clarity in Initialization Methods: The initialization strategies for the projection matrix (random, question-based, answer-based) are described but not evaluated in detail. A comparison of these strategies could provide insights into their impact on performance.
Questions for the Authors:
1. How does the proposed method compare against transformer-based models, such as BERT or GPT, which have shown strong performance in text classification tasks?
2. Can you provide a detailed breakdown of the performance gains attributed specifically to the GSA module versus the CNN backbone?
3. How does the model handle cases where the answer data is noisy or incomplete? Have you tested its robustness in such scenarios?
In summary, while the paper introduces a promising approach, the lack of rigorous benchmarking and component analysis prevents a confident assessment of its contributions. Addressing these issues could significantly strengthen the paper for future submissions.