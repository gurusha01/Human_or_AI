Review of the Paper
Summary of Contributions
This paper introduces a proximal (quasi-) Newton's method for learning binary deep neural networks (DNNs). The primary contribution lies in combining pre-conditioning with binarization within a proximal framework, offering a novel perspective on existing DNN binarization schemes. The authors propose a loss-aware binarization (LAB) algorithm that utilizes second-order information from the Adam optimizer to enhance the binarization process. The proximal step is shown to have an efficient closed-form solution, and the method is extended to recurrent neural networks (RNNs). Empirical results demonstrate that LAB outperforms existing binarization schemes on various datasets and architectures, including feedforward and recurrent networks. The paper also highlights LAB's robustness for wide and deep networks, addressing challenges like exploding gradients in RNNs.
Decision: Reject
While the paper presents an interesting idea with potential, it suffers from significant theoretical and practical shortcomings. The key reasons for rejection are the unconvincing theoretical analysis and the lack of clarity in convergence guarantees for the proposed method. Additionally, the empirical results, though promising, do not fully substantiate the claims due to limited exploration of certain critical aspects.
Supporting Arguments
1. Theoretical Weaknesses:  
   - The convergence guarantee relies on an assumption in Theorem 3.1 (`[dt^t]_k > β`), which is difficult to verify in practice, especially given the highly non-convex nature of DNN loss surfaces. This undermines the theoretical rigor of the proposed method.
   - The optimization problem is essentially a mixed-integer programming problem, and the paper does not convincingly address how the proximal Newton framework ensures convergence in such a challenging setting.
2. Empirical Validation:  
   - While the experiments show improvements over existing methods, the results are not sufficiently comprehensive. For example, the paper does not explore the sensitivity of the method to hyperparameters like the choice of β or the diagonal Hessian approximation.
   - The datasets and architectures used in the experiments are standard but do not fully test the scalability of the method to more complex real-world tasks.
3. Positioning in Literature:  
   - The paper provides a new interpretation of existing binarization schemes but does not adequately compare LAB to other second-order optimization methods or alternative approaches to loss-aware binarization.
   - The novelty of combining pre-conditioning with binarization is interesting but not sufficiently differentiated from prior work.
Suggestions for Improvement
1. Theoretical Analysis:  
   - Provide a more robust theoretical foundation for the convergence guarantees, possibly by relaxing the assumptions in Theorem 3.1 or offering empirical evidence to support their validity.
   - Discuss the implications of the mixed-integer nature of the optimization problem and how it affects the applicability of the proximal Newton framework.
2. Empirical Validation:  
   - Include experiments on larger and more diverse datasets to demonstrate the scalability and generalizability of the method.
   - Perform ablation studies to isolate the contributions of different components, such as the use of second-order information and the specific choice of the diagonal Hessian approximation.
3. Clarity and Accessibility:  
   - Simplify the mathematical exposition to make the paper more accessible to a broader audience.
   - Provide more intuitive explanations of the proximal Newton framework and its role in binarization.
Questions for the Authors
1. How sensitive is the performance of LAB to the choice of β and the diagonal Hessian approximation? Have you explored alternatives to the Adam-based second moment estimator?
2. Can you provide empirical evidence or practical guidelines for verifying the assumption `[dt^t]_k > β` in Theorem 3.1?
3. How does LAB perform on tasks with significantly larger datasets or more complex architectures, such as transformer-based models or large-scale image datasets like ImageNet?
In conclusion, while the paper introduces an intriguing approach to DNN binarization, the theoretical and empirical gaps need to be addressed before it can be considered for acceptance.