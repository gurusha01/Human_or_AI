The paper explores instance-level image retrieval using pre-trained CNNs as feature extractors and evaluates the impact of various factors such as feature extraction layers, aggregation, normalization, image resizing, and multi-scale combinations. The authors propose a new multi-scale image representation method and claim that it outperforms state-of-the-art methods on four datasets. The study aims to provide insights into the design choices for unsupervised image retrieval tasks, especially in scenarios with limited training data.
Decision: Reject
The primary reasons for rejection are the lack of novelty and scientific rigor in the paper. Many of the findings are outdated or have already been addressed in prior works. Additionally, the paper fails to cite two major recent works ([a] Gordo et al. and [b] Radenović et al.), which contradict several of its claims, including the necessity of unsupervised methods for limited data and the state-of-the-art performance of the proposed approach. The experimental design is limited to a single dataset (Oxford) and a single network (VGG-19), raising concerns about the generalizability of the results. Furthermore, the aggressive tuning of hyperparameters suggests potential overfitting to the test set, undermining the validity of the reported improvements.
Supporting Arguments:
1. Lack of Novelty: The paper largely revisits well-established techniques, such as multi-scale representations and PCA whitening, without offering significant new insights. Many of the proposed methods, such as max-pooling and multi-scale aggregation, have already been extensively explored in prior works.
   
2. Misleading Claims: The authors overstate the performance of their method as state-of-the-art, despite failing to benchmark against more recent and relevant methods. The omission of key citations ([a] and [b]) further weakens the credibility of their claims.
3. Limited Experimental Scope: The use of only one dataset (Oxford) and one network (VGG-19) is insufficient to demonstrate the generalizability of the proposed approach. Modern benchmarks typically evaluate on diverse datasets and architectures.
4. Overfitting Concerns: The paper's aggressive tuning of parameters, coupled with the lack of cross-validation or evaluation on unseen datasets, raises suspicions of overfitting to the test set.
Additional Feedback:
1. Expand Dataset Coverage: To improve the paper, the authors should evaluate their approach on a broader range of datasets (e.g., Paris6k, Google Landmarks) and networks (e.g., ResNet, EfficientNet) to demonstrate generalizability.
2. Address Missing Citations: The authors must include and discuss the works of Gordo et al. and Radenović et al., as these are highly relevant to the topic and challenge some of the paper's claims.
3. Clarify Novelty: The authors should explicitly highlight what is novel about their approach compared to prior work. If the contribution is incremental, it should be framed as such.
4. Mitigate Overfitting: The authors should adopt cross-validation or evaluate on a separate test set to ensure the robustness of their results.
Questions for the Authors:
1. Why were Gordo et al. and Radenović et al. not cited, and how does your method compare to theirs?
2. How do you address concerns about overfitting given the aggressive tuning and limited dataset usage?
3. Can you provide results on additional datasets and networks to demonstrate generalizability?
In summary, while the paper provides a detailed evaluation of various factors in image retrieval, its lack of novelty, limited experimental scope, and misleading claims make it unsuitable for acceptance in its current form.