The paper presents a novel approach to computational creativity by redefining creativity as the ability to generate out-of-distribution (OOD) novelty and proposing a framework for evaluating generative models under this paradigm. The authors critique traditional likelihood-based generative modeling for its limitations in generating OOD novelty and introduce an experimental setup that uses held-out object classes to evaluate the generative capacity of models. They also propose new metrics, such as out-of-class objectness and out-of-class count, to assess the quality of generated samples. Through extensive experiments with autoencoders and GANs, the paper identifies architectures and hyperparameter configurations that enable OOD novelty generation, making a significant contribution to the intersection of machine learning and computational creativity.
Decision: Accept  
The paper should be accepted for its innovative framing of computational creativity within machine learning, its rigorous evaluation framework, and its potential to inspire further research in generative modeling. The key reasons for this decision are:  
1. Novelty and Relevance: The paper addresses a critical gap in generative modeling by focusing on OOD novelty, a concept central to creativity but underexplored in machine learning.  
2. Scientific Rigor: The proposed evaluation framework and metrics are well-motivated and empirically validated, providing a robust foundation for future studies.  
Supporting Arguments  
The paper's strength lies in its ability to formalize computational creativity in a machine learning context. By holding out entire classes during training and evaluation, the authors simulate the challenge of generating truly novel objects. The proposed metrics, such as out-of-class objectness and out-of-class count, effectively capture the quality of OOD novelty, as demonstrated by the experiments. The large-scale study of autoencoders and GANs provides valuable insights into the architectural and hyperparameter choices that influence novelty generation. Furthermore, the authors' critique of likelihood-based methods is compelling, highlighting the need for alternative frameworks in generative modeling.
Additional Feedback  
While the paper is a strong contribution, there are areas for improvement:  
1. Clarity in Definitions: The paper could benefit from a clearer explanation of the distinction between in-distribution and OOD novelty, particularly for readers less familiar with computational creativity.  
2. Broader Context: A deeper engagement with the computational creativity literature would strengthen the paper's interdisciplinary relevance. For instance, discussing how the proposed framework aligns with or diverges from existing theories of creativity could provide additional context.  
3. Human Evaluation: While the authors acknowledge the importance of human evaluation, the paper could explore this aspect more thoroughly. For example, integrating human feedback into the evaluation pipeline could provide richer insights into the perceived creativity of the generated objects.  
Questions for the Authors  
1. How do you ensure that the held-out classes used for model selection do not overlap with those used for evaluation? Could you elaborate on the safeguards in place to prevent data leakage?  
2. Have you considered applying the proposed framework to more complex datasets or domains (e.g., images, text, or music)? If so, what challenges do you anticipate?  
3. Could you provide additional examples or visualizations of the generated OOD objects to illustrate their novelty and quality?  
Overall, this paper makes a significant and timely contribution to the field, and its acceptance would enrich the conference's discourse on generative modeling and computational creativity.