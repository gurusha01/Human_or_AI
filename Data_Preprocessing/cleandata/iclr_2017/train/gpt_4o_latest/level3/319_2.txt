Review of the Paper
Summary of Contributions
This paper introduces a modified knowledge distillation framework that leverages attention maps to transfer knowledge from a teacher convolutional neural network (CNN) to a student CNN. The authors propose two types of attention maps—activation-based and gradient-based—and demonstrate their utility in improving the performance of student networks. The paper provides experimental evidence of the proposed method's effectiveness across various datasets and architectures, including CIFAR, ImageNet, and fine-grained classification tasks. The authors also claim that their approach outperforms FitNet-style baselines and can be combined with traditional knowledge distillation for further gains. The paper is generally well-written, and the authors provide code and models for reproducibility.
Decision: Reject
While the paper demonstrates empirical improvements and introduces an interesting perspective on attention-based knowledge transfer, several critical issues prevent it from being ready for acceptance. These include the limited applicability to convolutional architectures, misleading terminology, unclear computational details, and organizational shortcomings.
Supporting Arguments for the Decision
1. Limited Applicability: The proposed method is explicitly designed for convolutional architectures, which restricts its generalizability to other types of networks, such as transformers or recurrent neural networks. This limitation is not adequately addressed in the paper.
2. Misleading Terminology: The use of the term "attention" is inconsistent with its established meaning in the broader deep learning literature, particularly in the context of transformers and self-attention mechanisms. This could confuse readers and detract from the paper's clarity.
3. Unclear Computational Details: The computation of the induced 2-norm in Equation (2) is not well-explained and appears computationally expensive. This raises concerns about the scalability of the method, especially for large datasets like ImageNet.
4. Organizational and Notational Issues: The paper suffers from unclear notation and poor organization in some sections, making it difficult to follow the methodology and experimental setup. For example, the distinction between activation-based and gradient-based attention transfer could be more clearly delineated.
Suggestions for Improvement
1. Clarify Terminology: Avoid using "attention" unless it aligns with its established meaning in the literature. Consider alternative terms like "feature map alignment" or "spatial focus transfer."
2. Expand Applicability: Discuss how the proposed method could be extended to non-convolutional architectures or justify its focus on CNNs more explicitly.
3. Improve Computational Clarity: Provide a detailed explanation of the computational cost of Equation (2) and discuss strategies to mitigate its potential inefficiencies.
4. Enhance Organization: Reorganize the paper to clearly separate the methodology, experimental setup, and results. Use consistent and intuitive notation throughout.
5. Integrate Gradient-Based Attention: The gradient-based attention transfer appears disconnected from the main method. Either integrate it more cohesively or provide a stronger justification for its inclusion.
Questions for the Authors
1. How does the computational cost of the proposed method compare to FitNet and other baselines, particularly for large datasets like ImageNet?
2. Can the method be adapted for architectures beyond CNNs, such as transformers or graph neural networks? If not, what are the key challenges?
3. Why was the term "attention" chosen, given its potential for confusion with self-attention mechanisms in the literature?
4. How does the method perform when applied to tasks beyond image classification, such as object detection or segmentation?
In summary, while the paper presents an interesting approach to knowledge distillation, it requires significant revisions to address its limitations and improve clarity.