The paper proposes "Snapshot Ensembling," a novel method for creating ensembles of neural networks without incurring additional training costs. By leveraging a cyclic learning rate schedule, the method captures multiple local minima during a single training run, saving model snapshots at these points. These snapshots are then combined at test time to form an ensemble, which improves predictive performance. The authors demonstrate the effectiveness of this approach across several datasets (CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and ImageNet) and architectures (ResNet, Wide ResNet, DenseNet). The method is simple to implement, computationally efficient, and produces results competitive with traditional ensembles.
Decision: Accept.  
Key reasons:  
1. The proposed method is innovative, practical, and addresses a significant limitation of traditional ensembles—high computational cost—while maintaining competitive performance.  
2. The paper includes comprehensive experiments and ablation studies that validate the method's effectiveness across diverse datasets and architectures.  
Supporting Arguments:  
The paper is well-motivated, addressing the computational inefficiency of traditional ensembles, which limits their widespread adoption. The authors position their work effectively within the existing literature, differentiating it from implicit ensembling methods like Dropout and related approaches that use intermediate training stages. The cyclic learning rate schedule is a clever mechanism to induce diversity in the snapshots, and the experiments convincingly show that these snapshots are both accurate and diverse, making them suitable for ensembling. The results demonstrate consistent performance improvements over single models, and in some cases, the method approaches the performance of true ensembles, albeit at a fraction of the training cost.
However, the claim in the abstract that Snapshot Ensembles "almost match" true ensembles is overstated. For example, on CIFAR-100 with DenseNet-40, the method lags behind true ensembles. Additionally, while the paper compares Snapshot Ensembles to single models and implicit ensembles, it lacks thorough comparisons with true ensembles across all datasets. Table 4, in particular, should include such comparisons for a more balanced evaluation.
Additional Feedback:  
1. The diversity of the snapshots is a key strength of the method. However, a more quantitative analysis of diversity (e.g., pairwise disagreement or correlation) compared to true ensembles and dropout-based ensembles would strengthen the paper.  
2. The choice of cyclic learning rate parameters (e.g., number of cycles, restart learning rate) is critical to the method's success. While some analysis is provided, a more detailed discussion of how to generalize these choices across datasets and architectures would be valuable.  
3. The paper could benefit from a clearer explanation of the trade-offs between Snapshot Ensembles and true ensembles, particularly in terms of computational cost versus performance.
Questions for the Authors:  
1. How does the diversity of Snapshot Ensembles compare quantitatively to true ensembles and dropout-based ensembles?  
2. Can the method be extended to tasks beyond image classification, such as natural language processing or reinforcement learning?  
3. How sensitive is the method to the choice of cyclic learning rate parameters, and are there heuristics to guide their selection?  
In summary, the paper introduces a promising and practical method for ensembling neural networks, with strong experimental support. Addressing the noted limitations and providing additional analyses would further enhance its impact.