Review of the Paper
Summary of Contributions
This paper investigates the use of sinusoidal activation functions in neural networks, a topic that has received limited attention in the literature. The authors provide a theoretical analysis of the loss surface associated with sinusoidal activations, highlighting the challenges posed by local minima and periodicity. They demonstrate that for standard tasks like MNIST classification, the periodicity of the sine function is often underutilized, with the network behaving similarly to one using tanh. However, for algorithmic tasks where periodicity is inherently beneficial, networks with sinusoidal activations outperform those with monotonic activations. The paper also includes insightful closed-form derivations of the loss surface, which are educational and could benefit the research community. While the results are promising, the study is preliminary and lacks sufficient empirical evidence to draw significant conclusions.
Decision: Reject
The primary reasons for this decision are:
1. Preliminary Nature of the Study: The paper presents interesting theoretical insights and some empirical results, but the experiments are limited in scope and fail to provide strong, generalizable conclusions.
2. Lack of Rigorous Evidence: The findings, particularly for algorithmic tasks, are not robustly supported. The experiments on MNIST and Reuters datasets suggest that sinusoidal activations are largely interchangeable with tanh, which undermines the novelty of the approach for general tasks.
Supporting Arguments
1. Strengths:
   - The theoretical analysis of the loss surface is thorough and provides valuable insights into the challenges of training networks with sinusoidal activations.
   - The paper identifies specific scenarios, such as algorithmic tasks, where sinusoidal activations may offer advantages.
   - The closed-form derivations are a strong contribution and could serve as a foundation for future work.
2. Weaknesses:
   - The empirical results on MNIST and Reuters datasets show that the periodicity of the sine function is not crucial for effective learning, making the proposed activation function less compelling for general-purpose tasks.
   - The experiments on algorithmic tasks, while promising, are limited to toy problems and do not establish the practical utility of sinusoidal activations in real-world scenarios.
   - The study lacks comparisons with other non-monotonic activation functions, which could provide a broader context for the findings.
Suggestions for Improvement
1. Expand Experimental Scope: Include more diverse datasets and tasks, particularly real-world problems where periodicity might be beneficial. For example, time-series forecasting or signal processing tasks could provide stronger evidence for the utility of sinusoidal activations.
2. Compare with Other Non-Monotonic Functions: Evaluate the performance of sinusoidal activations against other non-monotonic functions to better contextualize the findings.
3. Analyze Deeper Architectures: Extend the analysis to deeper networks and more complex architectures, as the current focus on shallow networks limits the generalizability of the results.
4. Clarify Practical Implications: Provide a clearer discussion of when and why practitioners should consider using sinusoidal activations over established alternatives like tanh or ReLU.
Questions for the Authors
1. How do sinusoidal activations compare to other non-monotonic functions, such as Gaussian or oscillatory functions, in terms of training dynamics and performance?
2. Can the authors provide more evidence or theoretical justification for why sinusoidal activations outperform tanh in algorithmic tasks? Are there specific properties of the tasks that make them particularly suited for periodic functions?
3. Have the authors considered the impact of sinusoidal activations in tasks involving temporal or frequency-domain data, where periodicity might naturally align with the problem structure?
In conclusion, while the paper provides valuable theoretical insights and raises interesting questions, it falls short in providing compelling empirical evidence to justify the adoption of sinusoidal activations in practical applications. Further work is needed to strengthen the claims and broaden the scope of the study.