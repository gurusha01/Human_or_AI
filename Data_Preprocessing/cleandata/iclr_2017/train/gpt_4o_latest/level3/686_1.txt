Review of the Paper
The paper proposes a novel compression and reconstruction method for deep neural networks (DNNs) called Homologically Functional Hashing (HFH). HFH introduces a shared homological compression space and employs multiple hash functions alongside a small reconstruction network to recover weights. The authors claim that HFH achieves higher compression ratios with minimal accuracy loss compared to the simpler HashedNets approach. The method is evaluated on multiple datasets, demonstrating its effectiveness in reducing memory footprints while maintaining competitive performance.
Decision: Reject
While the paper presents an innovative approach to DNN compression, the decision to reject is based on two primary concerns: (1) insufficient clarity and rigor in evaluating the computational overhead introduced by the reconstruction network, and (2) lack of a comprehensive running time analysis, which is critical for practical deployment.
Supporting Arguments:
1. Memory Comparison Concerns: The paper does not explicitly clarify whether the additional parameters of the reconstruction network are included in the memory footprint comparison. This omission raises doubts about the claimed memory savings, as the reconstruction network could significantly offset the compression benefits.
   
2. Running Time Analysis: The computational cost of HFH, particularly due to the hashing and reconstruction steps, is not rigorously analyzed. While the authors argue that the reconstruction network is lightweight, the lack of empirical evidence or detailed benchmarks on running time makes it difficult to assess the method's practicality, especially for real-time or resource-constrained applications.
3. Lightweight Nature Questioned: The introduction of multiple hash functions and a reconstruction network adds complexity. This raises concerns about the method's scalability and suitability for mobile or embedded devices, where simplicity and efficiency are paramount.
Additional Feedback:
1. Empirical Validation: The paper should include a detailed breakdown of memory usage, explicitly accounting for the reconstruction network's parameters. This would provide a clearer comparison with HashedNets and other baseline methods.
   
2. Running Time Benchmarks: A thorough evaluation of the running time, including feed-forward and back-propagation costs, is essential. Comparisons with HashedNets and standard DNNs would strengthen the claims of efficiency.
3. Broader Comparisons: The paper primarily compares HFH with HashedNets. Including comparisons with other state-of-the-art compression techniques, such as pruning or quantization, would provide a more comprehensive evaluation of HFH's effectiveness.
4. Clarity on Lightweight Design: The authors should provide more evidence to justify the claim that the reconstruction network is lightweight. For example, analyzing the trade-off between compression ratio and computational overhead would help clarify the method's practicality.
Questions for the Authors:
1. Are the additional parameters of the reconstruction network included in the memory footprint comparison? If not, how significant is their impact on the overall memory usage?
2. What is the computational cost of HFH compared to HashedNets and standard DNNs during both training and inference?
3. How does HFH perform in terms of energy efficiency, particularly on mobile or embedded devices?
4. Could the authors provide more insights into the scalability of HFH for larger networks or datasets, such as ImageNet?
In summary, while HFH is an interesting and promising approach, the lack of clarity on memory and computational costs, as well as the absence of a detailed running time analysis, limits its practical applicability. Addressing these issues would significantly strengthen the paper.