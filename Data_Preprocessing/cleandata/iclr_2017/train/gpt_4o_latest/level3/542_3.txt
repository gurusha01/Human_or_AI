Review
Summary of Contributions
This paper introduces a novel reinforcement learning (RL) framework for multitask learning, where policies are guided by high-level symbolic sketches. The proposed method employs a hierarchical structure, associating each subtask with a modular subpolicy that is shared across tasks. A decoupled actor-critic algorithm is extended to accommodate this modularity, and curriculum learning is incorporated to handle complex tasks with sparse rewards. The approach is evaluated on two environments: a maze navigation game and a crafting game, demonstrating superior performance over baseline methods in terms of learning efficiency and generalization. The paper also highlights the ability of the learned subpolicies to generalize to unseen tasks in zero-shot and adaptation settings. The modular design is presented as a step toward interpretable and reusable policy components.
Decision: Reject
While the paper proposes an interesting and original framework, the overall contribution is limited by the simplified nature of the tasks and the lack of strong real-world applications. The impact on the broader RL community is constrained, as the problem addressed is a simplified version of option-learning with richer supervision. Additionally, the paper does not provide sufficient motivation for why this approach is necessary or how it could be extended to more complex, real-world scenarios.
Supporting Arguments
1. Strengths:
   - The paper is well-written and easy to follow, with clear explanations of the proposed framework and experimental setup.
   - The modular subpolicy design is an elegant adaptation of the actor-critic model, and the use of curriculum learning is effective for handling sparse rewards.
   - Experimental results are thorough, showing clear advantages over baseline methods in multitask learning and generalization.
2. Weaknesses:
   - The tasks used for evaluation (maze navigation and crafting) are relatively simple and lack the complexity of real-world RL problems. This limits the generalizability and practical relevance of the approach.
   - The paper does not provide a strong motivation for the use of high-level sketches or discuss concrete applications where this framework would be essential.
   - While the modular design is novel, it is not a significant departure from existing hierarchical RL methods, and the overall novelty is limited.
Suggestions for Improvement
1. Motivation and Applications: Provide stronger motivation for the use of policy sketches and discuss potential real-world applications where this framework could be impactful. For example, how might this approach scale to robotics, autonomous driving, or other domains with complex hierarchies?
2. Task Complexity: Consider evaluating the method on more challenging and realistic environments to demonstrate its scalability and robustness. Tasks with continuous action spaces or dynamic, partially observable environments could strengthen the paper's contributions.
3. Comparison with Hierarchical RL: Provide a more detailed comparison with existing hierarchical RL methods, such as options frameworks or hierarchical abstract machines. Highlight the specific advantages of the proposed approach in terms of scalability, efficiency, or interpretability.
4. Ablation Studies: While the paper includes ablations for the critic and curriculum learning, additional studies on the impact of sketch quality (e.g., noisy or incomplete sketches) would provide insights into the robustness of the method.
5. Broader Impact: Discuss how the modular subpolicies could be reused or transferred across domains, and whether the approach could be extended to unsupervised or self-supervised settings.
Questions for Authors
1. How does the framework handle noisy or incomplete sketches? Would the performance degrade significantly, and if so, how could this be mitigated?
2. Can the proposed method scale to tasks with continuous action spaces or high-dimensional state spaces? If not, what modifications would be required?
3. How does the approach compare to other hierarchical RL methods in terms of computational efficiency and sample complexity?
4. Is there a way to relax the assumption of having predefined sketches and instead infer them automatically from data?
In conclusion, while the paper presents an interesting framework with promising results, its limited novelty, simplified tasks, and lack of strong real-world motivation reduce its overall impact. Addressing these issues could significantly strengthen the paper for future submissions.