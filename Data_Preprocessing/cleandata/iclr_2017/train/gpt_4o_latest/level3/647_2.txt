The paper introduces Recurrent Inference Machines (RIMs), a framework that integrates model and inference into a single trainable recurrent neural network (RNN) for solving inverse problems. The authors argue that traditional approaches, which separate model and inference, become inseparable after training, making the dichotomy unnecessary. RIMs aim to directly map observations to reconstructed signals while learning both prior information and inference procedures jointly. The paper evaluates RIMs on tasks such as image denoising and super-resolution, claiming competitive or superior performance compared to state-of-the-art methods.
Decision: Reject
The primary reasons for rejection are the lack of sufficient novelty and the unconvincing experimental evidence supporting the claims. While the paper formalizes existing ideas, it does not present a unique or groundbreaking contribution specific to inverse problems.
Supporting Arguments:
1. Lack of Novelty: The idea of replacing traditional iterative algorithms with discriminatively trained recurrent networks is well-established in the literature. The paper does not introduce a fundamentally new concept but rather formalizes existing approaches.
2. Weak Experimental Evidence: Claims about benefits such as parameter sharing, handling different noise levels, and scalability are not convincingly supported by experiments. For example, the experiments focus on a narrow set of tasks (denoising and super-resolution) without demonstrating broader applicability to diverse inverse problems like in-painting or deconvolution.
3. Contradictory Claims: The paper's motivation to train end-to-end inference architectures contradicts its claims of generalization across tasks. The results suggest that task-specific training is still required for optimal performance.
4. Practical Benefits Unclear: The claimed advantages, such as faster inference and better convergence, are not convincingly tied to the unrolling of iterations, which itself is not a novel concept.
Additional Feedback:
1. Broader Applicability: To substantiate claims of generalization to inverse problems, the paper should include experiments on a wider range of tasks, such as in-painting, deconvolution, and handling multiple observation models.
2. Detailed Analysis: The paper should provide a more rigorous analysis of why RIMs outperform other methods. For example, it could compare the learned priors or convergence behavior in greater detail.
3. Experimental Design: The experimental results should include ablation studies to isolate the contributions of specific components (e.g., state variables, parameter sharing). Additionally, comparisons with a broader set of baseline methods would strengthen the claims.
4. Clarity of Motivation: The motivation for abandoning the separation between model and inference should be better justified. The paper could discuss scenarios where this integration is particularly advantageous and provide theoretical insights into why this approach is superior.
Questions for the Authors:
1. How does the proposed framework handle inverse problems with non-linear forward models, and what are the limitations in such cases?
2. Can the authors provide evidence that the claimed benefits (e.g., parameter sharing, handling multiple noise levels) are unique to RIMs and not achievable with existing methods?
3. How does the proposed method scale to larger datasets or higher-dimensional problems, such as 3D medical imaging tasks?
In summary, while the paper provides an interesting formalization of RNN-based approaches for inverse problems, it lacks sufficient novelty and rigorous experimental validation to justify acceptance. A significant rewrite addressing the above concerns is recommended.