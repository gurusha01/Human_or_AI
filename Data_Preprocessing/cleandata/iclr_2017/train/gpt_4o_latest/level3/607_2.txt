Review
Summary of the Paper
This paper proposes a memory-based attention model for video description, termed Hierarchical Attention/Memory (HAM). The model is inspired by the central executive system in human cognition and aims to improve video captioning by leveraging memories of past attention and reasoning over the entire sequence of video frames. The authors claim that HAM enables more effective temporal reasoning and hierarchical attention, which are critical for generating accurate video descriptions. The model is evaluated on two datasets, MSVD and Charades, and is reported to achieve state-of-the-art results on Charades while performing comparably on MSVD. The paper also highlights the generalizability of the proposed architecture to other sequence learning tasks.
Decision: Reject
Key reasons for rejection:
1. Limited Novelty: The proposed HAM model lacks significant differentiation from prior work (e.g., Xu et al., Yao et al.). The claims of novelty, particularly around memorizing past attention and multi-layer attention, are not convincingly supported.
2. Unconvincing Results: The performance improvements are marginal and not statistically significant. Claims of state-of-the-art performance are incorrect for MSVD and overstated for Charades.
3. Clarity and Consistency Issues: The paper suffers from inconsistent notation, unclear explanations (e.g., Equation 11 vs. Figure 1), and insufficient qualitative results to validate the attention mechanisms.
Supporting Arguments
1. Novelty and Contribution: While the paper claims to introduce a novel memory-based attention mechanism, the differences between HAM and prior work are insufficiently clarified. The hierarchical attention and memory components appear to be incremental extensions rather than groundbreaking innovations.
2. Evaluation Concerns: The lack of human evaluation, which is feasible for video captioning tasks, weakens the empirical validation. Additionally, hyperparameter inconsistencies between ablation studies and performance comparisons raise concerns about the robustness of the results.
3. Conceptual Limitations: The independent spatial and temporal attention mechanisms limit the model's ability to focus on specific aspects of frames for generating words. This is a critical drawback for tasks requiring fine-grained temporal reasoning.
4. Clarity Issues: The paper's explanations are often unclear, particularly in describing the model architecture and equations. Figure 1 lacks sufficient detail, and additional visualizations of the attention mechanisms would improve interpretability.
Suggestions for Improvement
1. Clarify Novelty: Clearly articulate how HAM differs from prior work, particularly in terms of its hierarchical attention and memory components. Provide stronger theoretical or empirical evidence to support claims of novelty.
2. Improve Evaluation: Include human evaluations and more comprehensive qualitative results to validate the model's effectiveness. Address the hyperparameter inconsistencies to ensure fair comparisons.
3. Enhance Clarity: Revise the paper for consistent notation and clearer explanations of equations and figures. Add detailed visualizations of the attention mechanisms to improve interpretability.
4. Address Motion Modeling: The paper claims to address motion modeling but does not convincingly demonstrate this. Incorporating low-level motion features (e.g., optical flow) could strengthen the model's ability to capture temporal dynamics.
Questions for the Authors
1. How does HAM fundamentally differ from prior attention models (e.g., Xu et al., Yao et al.) beyond incremental modifications?
2. Can you provide qualitative visualizations of the attention mechanisms to demonstrate how the model attends to different parts of the video while generating captions?
3. Why were hyperparameters inconsistent between ablation studies and performance comparisons? How does this impact the validity of the results?
4. Have you considered incorporating low-level motion features (e.g., optical flow) to improve temporal modeling?
In summary, while the paper addresses an important problem in video description and proposes an interesting architecture, the lack of significant novelty, unconvincing results, and clarity issues make it unsuitable for acceptance in its current form. Addressing the above concerns would significantly strengthen the paper.