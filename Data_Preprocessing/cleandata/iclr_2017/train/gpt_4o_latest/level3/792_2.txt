Review
Summary of Contributions
The paper proposes a novel regularization method, SoftTarget regularization, which aims to mitigate overfitting in deep neural networks without reducing model capacity. The method leverages an exponential moving average of past soft-labels combined with hard labels to preserve co-label similarities throughout training. The authors claim that this approach provides regularization comparable to Dropout while simplifying the learning problem. The paper includes experiments on MNIST, CIFAR-10, and SVHN datasets, demonstrating that SoftTarget regularization achieves lower test losses and higher accuracy compared to baseline methods. Additionally, the paper introduces the concept of co-label similarities as a measure of overfitting and provides empirical evidence to support its claims.
Decision: Reject
The primary reasons for rejection are the lack of rigorous empirical validation and insufficient novelty in the proposed method. While the idea of preserving co-label similarities is interesting, the paper fails to convincingly demonstrate its superiority over existing methods due to methodological shortcomings.
Supporting Arguments
1. Empirical Validation Issues: 
   - The experiments lack proper baselines. The architectures used are outdated, and no data augmentation is applied, which is standard practice in modern deep learning experiments. This undermines the empirical significance of the results.
   - The comparison to Dropout is unconvincing, as the Dropout parameters are not properly tuned or cross-validated. This raises doubts about the claim that SoftTarget outperforms Dropout.
   - The choice of 100 epochs without convergence tests is arbitrary and may not reflect the true performance of the method.
2. Novelty and Theoretical Rigor:
   - The idea of using soft-labels for regularization is not novel and has been explored in prior work, such as distillation and minimum entropy regularization. The paper does not sufficiently differentiate its approach from these methods.
   - The reasoning behind update rules (3) and (4) is unclear, and the authors do not justify why these specific formulations are optimal. Alternatives like totally corrective updates are not explored.
3. Incomplete Results:
   - The paper does not report test misclassification error, making it difficult to assess the practical impact of the proposed method.
   - Hyperparameter tuning is inadequate, with no mention of advanced techniques like random search or Bayesian optimization. This undermines the reliability of the reported results.
Suggestions for Improvement
1. Experimental Design:
   - Use modern architectures (e.g., ResNet, EfficientNet) and include data augmentation to ensure the results are empirically significant.
   - Perform proper hyperparameter tuning for all baseline methods, including Dropout, to provide a fair comparison.
   - Include convergence tests to ensure the number of epochs is sufficient for all methods.
2. Theoretical Justification:
   - Provide a stronger theoretical foundation for the update rules (3) and (4). Explore alternative formulations and justify the chosen approach.
   - Clearly differentiate the proposed method from related work, such as distillation and minimum entropy regularization.
3. Reporting and Analysis:
   - Include test misclassification error in the results to provide a more comprehensive evaluation.
   - Calculate co-label similarities using softmax outputs instead of predicted labels for better accuracy.
Questions for the Authors
1. Why were outdated architectures and no data augmentation used in the experiments? How do you justify the empirical significance of your results in this context?
2. Can you provide a theoretical explanation for the specific update rules (3) and (4)? Have you considered alternatives like totally corrective updates?
3. How does the proposed method compare to a two-step distillation approach, both in terms of performance and computational cost?
While the paper introduces an interesting perspective on co-label similarities, the lack of rigorous empirical validation and insufficient novelty prevent it from making a strong contribution to the field. Addressing the above concerns could significantly improve the quality and impact of the work.