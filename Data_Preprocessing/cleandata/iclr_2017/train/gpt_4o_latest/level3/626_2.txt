Review of the Paper
Summary:
This paper investigates the concept of preimages in fully connected feedforward neural networks with ReLU activation functions. Specifically, it provides a procedure to compute the preimages of node activities at arbitrary levels of the network, disregarding pooling effects. The authors argue that preimages, which are piecewise linear manifolds in the input space, are critical for understanding how deep networks approximate class distributions and achieve classification. The paper also discusses implications for modeling input manifolds, separating classes, and designing efficient classifiers. The authors suggest that preimages could provide insights into network training and adversarial examples.
Recommendation:
Reject for the main Conference track; Accept for the Workshop track.  
The paper introduces an interesting idea of using preimages to analyze and understand deep networks. However, it lacks polish and sufficient contributions to merit acceptance in the main Conference track. The work is better suited for the Workshop track, where preliminary ideas and exploratory research are welcomed.
Supporting Arguments:
1. Strengths:
   - The paper introduces a novel perspective on preimages as building blocks for understanding class manifolds in input space.
   - The idea of using preimages to analyze class mixing and separation is intriguing and could have practical implications for training and adversarial robustness.
   - The authors provide a step-by-step procedure for computing preimages, which is a valuable contribution.
2. Weaknesses:
   - The paper is unpolished, with numerous typos, unclear definitions, and inconsistent notation. For example, parentheses are missing in equations, and terms like "kernel" and "hyperplanes" are not rigorously defined.
   - The figures (e.g., Figures 1 and 3) are poorly explained, making it difficult to follow the visualizations of preimages and their implications.
   - The mathematical exposition is incomplete and occasionally confusing. The authors should use more elementary linear algebra to clarify their arguments.
   - The paper does not adequately discuss related work, particularly the connection to [Montufar et al., NIPS 2014], which also explores local linear maps of ReLU networks.
   - Practical challenges in computing preimages are not addressed, despite the straightforward definitions provided.
Suggestions for Improvement:
1. Clarity and Presentation:
   - Revise the paper to eliminate typos, improve notation consistency, and clarify ambiguous terms.
   - Provide detailed explanations for figures, especially Figures 1 and 3, to make them more accessible to readers.
   - Use simpler and more intuitive mathematical language to describe the procedure for computing preimages.
2. Related Work:
   - Include a discussion of [Montufar et al., NIPS 2014] and other relevant literature to better situate the work in the context of existing research.
3. Practical Considerations:
   - Address the computational challenges of preimage calculation, particularly for high-dimensional input spaces and deep networks with many layers.
4. Broader Impact:
   - Provide empirical results or theoretical insights to support claims about the utility of preimages in training efficiency and adversarial robustness.
Questions for the Authors:
1. How does the proposed method scale to deeper networks with higher-dimensional input spaces? Are there computational bottlenecks in calculating preimages?
2. Can you provide empirical evidence to demonstrate the practical utility of preimages in tasks like classification or adversarial defense?
3. How does your work compare to [Montufar et al., NIPS 2014] in terms of methodology and implications for understanding ReLU networks?
In conclusion, while the paper presents an interesting idea, it requires significant revisions and additional contributions to be suitable for the main Conference track. It is better suited for the Workshop track, where the authors can refine their ideas and receive feedback from the community.