Review of "L-SR1: A New Second Order Method to Train Deep Neural Networks"
Summary of Contributions
The paper introduces L-SR1, a novel second-order optimization method designed to address challenges in training deep neural networks, such as saddle points and poor Hessian conditioning. The authors propose leveraging the Symmetric Rank-One (SR1) update within a trust-region framework, which allows indefinite Hessian approximations, potentially overcoming saddle points more effectively than traditional quasi-Newton methods like L-BFGS. The paper highlights the method's potential for distributed training due to its insensitivity to mini-batch sizes. Experimental results on MNIST and CIFAR-10 datasets demonstrate that L-SR1 performs comparably to first-order methods like NAG and AdaDelta and significantly outperforms L-BFGS. The authors also explore the sensitivity of L-SR1 to hyperparameters and mini-batch sizes, claiming robustness and minimal tuning requirements.
Decision: Reject
While the paper presents an interesting and potentially impactful approach, it falls short in several critical areas that undermine its claims and contributions. The lack of rigorous comparisons with state-of-the-art second-order methods and insufficient experimental validation on larger-scale tasks are the primary reasons for this decision.
Supporting Arguments for Decision
1. Limited Comparisons with Related Work:  
   The paper does not compare L-SR1 to recent second-order methods, such as Hessian-free optimization (Martens, 2012) or Pearlmutter's fast Hessian-vector multiplication. These methods are well-established in the literature and are directly relevant to the problem being addressed. Without such comparisons, it is difficult to assess whether L-SR1 provides a meaningful improvement over existing approaches.
2. Unconvincing Experimental Results:  
   While the MNIST and CIFAR-10 experiments show that L-SR1 performs comparably to first-order methods like NAG and AdaDelta, the results do not clearly demonstrate a significant advantage. For example, the test loss and error rates are not substantially better than those of first-order methods, despite the additional computational overhead of a second-order method. Furthermore, the experiments are limited to small-scale datasets and architectures, leaving the scalability and effectiveness of L-SR1 on larger, more complex tasks unproven.
3. Insufficient Evidence for Mini-Batch Insensitivity:  
   The claim that L-SR1 is insensitive to mini-batch sizes is intriguing but not convincingly substantiated. While the authors provide some evidence of robustness to mini-batch size variations, the experiments are limited in scope and do not compare this property against other second-order methods or modern first-order optimizers like Adam.
4. Lack of Error Rates on Larger-Scale Tasks:  
   The paper does not include results on larger-scale datasets or tasks, such as ImageNet, which are critical for demonstrating the practical utility of the proposed method. Without such validation, the applicability of L-SR1 to real-world deep learning problems remains unclear.
Suggestions for Improvement
1. Expand Comparisons:  
   Include comparisons with state-of-the-art second-order methods, such as Hessian-free optimization and stochastic quasi-Newton methods. This would provide a clearer picture of where L-SR1 stands in the broader optimization landscape.
2. Evaluate on Larger Datasets:  
   Test L-SR1 on larger-scale tasks, such as ImageNet or other challenging benchmarks, to demonstrate its scalability and practical utility.
3. Clarify Mini-Batch Insensitivity:  
   Provide more extensive experimentation to substantiate claims about mini-batch insensitivity. Compare this property against other optimizers, including Adam and Hessian-free methods.
4. Address Skipped Updates:  
   Investigate the high rate of skipped updates observed in some experiments and provide a theoretical or empirical explanation for this phenomenon. This would strengthen the robustness claims of the method.
5. Improve Presentation of Results:  
   Clearly highlight where L-SR1 outperforms existing methods and provide more detailed error rates and convergence plots. This would make the experimental results more compelling.
Questions for the Authors
1. How does L-SR1 compare to Hessian-free optimization or Pearlmutter's fast Hessian-vector multiplication in terms of convergence speed, computational cost, and scalability?  
2. Can you provide error rates and convergence results for larger-scale tasks, such as ImageNet?  
3. What is the theoretical explanation for the high rate of skipped updates in some experiments? Does this affect the stability or convergence of L-SR1?  
4. How does the computational overhead of L-SR1 compare to first-order methods like Adam or AdaDelta, especially for larger models?  
In conclusion, while the idea of using SR1 updates to address saddle points is promising, the paper requires significant additional work to establish its contributions and validate its claims.