Review of "Wild Variational Inference Using Stein's Operator"
Summary
The paper introduces methods for "wild variational inference," which relaxes the requirement of tractable density functions in inference networks, enabling broader applicability in Bayesian inference. Two approaches are proposed: one based on amortized Stein Variational Gradient Descent (SVGD) and another on Kernelized Stein Discrepancy (KSD). These methods are applied to adaptively optimize step sizes in stochastic gradient Langevin dynamics (SGLD), demonstrating improvements over hand-designed schemes. The authors also explore general Langevin inference networks as black-box samplers for complex distributions. The paper claims that these methods simplify the design of variational inference procedures and improve computational efficiency.
Decision: Reject  
The decision to reject is based on two primary reasons: (1) the lack of rigorous evaluation on high-dimensional or complex posterior distributions, which limits the significance of the proposed methods, and (2) issues with clarity and originality, as parts of the work overlap with prior research (e.g., Wang & Liu, 2016) without sufficiently distinguishing its contributions.
Supporting Arguments
1. Proposed Method: The use of Stein's operator for wild variational inference is technically sound and aligns with recent advancements in variational inference. However, the novelty is undermined by significant overlap with prior work, particularly in the amortized SVGD approach. The second method, based on KSD, is more distinct but lacks sufficient exploration of its advantages over existing techniques.
   
2. Quality: While the methods are well-motivated, the empirical evaluation is limited to low-dimensional problems (e.g., 1D Gaussian mixture models and Bayesian logistic regression). The paper does not convincingly demonstrate scalability to high-dimensional or more complex distributions, which is critical for assessing the practical utility of the proposed methods.
3. Clarity: The paper is dense and difficult to follow due to the inclusion of multiple techniques without clearly delineating the main contributions. The writing lacks focus, and key concepts are not adequately explained, making it challenging for readers to grasp the novelty and significance of the work.
4. Originality and Significance: The originality of the work is questionable due to its overlap with Wang & Liu (2016). Additionally, the significance is unclear, as the experiments do not compare the proposed methods against simpler variational approaches or state-of-the-art alternatives in high-dimensional settings.
Suggestions for Improvement
1. Expand Empirical Evaluation: Include experiments on high-dimensional and complex posterior distributions to demonstrate the scalability and robustness of the methods. Comparisons with simpler variational approaches and other state-of-the-art methods are necessary to establish significance.
2. Clarify Contributions: Clearly distinguish the proposed methods from prior work, particularly Wang & Liu (2016). Highlight the unique aspects of the KSD-based approach and its advantages over existing techniques.
3. Improve Writing and Focus: Streamline the presentation by focusing on the core contributions and minimizing extraneous details. Clearly define terms and concepts, and provide intuitive explanations for key ideas.
4. Address Practicality: Discuss the computational overhead of the proposed methods, especially in high-dimensional settings, and provide practical guidelines for their implementation.
Questions for the Authors
1. How do the proposed methods perform on high-dimensional posterior distributions or more complex models, such as deep Bayesian neural networks?
2. Can you provide a detailed comparison of computational costs between your methods and traditional variational inference techniques?
3. How does the KSD-based approach compare to the amortized SVGD method in terms of convergence speed and approximation quality?
4. What specific advancements or modifications distinguish your work from Wang & Liu (2016)?
In summary, while the paper introduces interesting ideas, it falls short in terms of originality, clarity, and empirical rigor, which are critical for acceptance at this stage. Addressing these issues could significantly strengthen the work for future submissions.