Review
Summary of Contributions
This paper aims to provide a theoretical explanation for the success of ResNet by analyzing deep linear networks and their nonlinear variants. The authors focus on the role of shortcut connections, particularly 2-shortcuts, in achieving depth-invariant condition numbers of the Hessian at the zero initial point. They argue that this property facilitates the training of deep models. The paper also compares different initialization schemes, concluding that zero initialization with 2-shortcuts outperforms Xavier and orthogonal initialization. Extensive experiments are conducted to validate the theoretical findings, including analyses of learning dynamics, loss surfaces, and final losses.
Decision: Reject  
The primary reasons for rejection are the lack of clarity in the write-up and the misleading nature of the results. Additionally, the paper underestimates the role of nonlinearities in the learning dynamics, and the behavior of the model with zero weights does not generalize to its overall functioning.
Supporting Arguments
1. Write-up and Clarity: The paper is dense and difficult to follow, with many theoretical results buried in technical jargon. Key concepts, such as the role of nonlinearities and the practical implications of the findings, are not clearly articulated. The lack of clear organization makes it challenging to assess the novelty and significance of the contributions.
2. Misleading Results: The emphasis on the behavior of the Hessian at the zero initial point is problematic. While the authors argue that this point provides a "decent characterization" of the optimization landscape, this assumption is not well-justified. The results may not generalize to the broader training dynamics, as the optimization process quickly moves away from the zero initial point.
3. Underestimation of Nonlinearities: The paper downplays the impact of nonlinearities, which are critical to the success of deep learning models. While the authors claim that their findings are consistent across linear and nonlinear networks, the experiments on nonlinear networks are limited and do not convincingly support this claim.
Additional Feedback
1. Improving Clarity: The paper would benefit from a clearer structure and more intuitive explanations of the theoretical results. For example, the role of 2-shortcuts could be explained with simpler diagrams or analogies before delving into mathematical details.
2. Experimental Validation: The experiments should include a broader range of nonlinear networks and real-world datasets to demonstrate the practical relevance of the findings. The current focus on whitened MNIST and linear networks limits the generalizability of the results.
3. Role of Nonlinearities: The authors should provide a more detailed analysis of how nonlinearities influence the optimization process. This could include experiments that isolate the effects of activation functions and compare them across different shortcut depths.
4. Generalization Beyond Zero Weights: The paper should address how the findings at the zero initial point translate to the broader optimization landscape. This could involve analyzing the Hessian at different stages of training or exploring alternative initialization schemes.
Questions for Authors
1. How do the results at the zero initial point generalize to the broader optimization landscape, given that training quickly moves away from this point?
2. Can you provide more evidence to support the claim that the findings are consistent across linear and nonlinear networks?
3. How do the results change when using more complex datasets or architectures, such as convolutional networks with batch normalization?
In summary, while the paper addresses an important problem and provides some interesting theoretical insights, significant improvements in clarity, experimental validation, and generalization are needed before it can be considered for acceptance.