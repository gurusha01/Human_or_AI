Review
The paper critiques the limitations of standard ancestral sampling in generative autoencoders, particularly its reliance on overly restrictive priors, and proposes a Markov Chain Monte Carlo (MCMC) sampling process as an alternative. The authors argue that their approach enables sampling from the learned latent distribution \( P̂(Z) \), which better aligns with the data distribution compared to the prior \( P(Z) \). They also extend their method to denoising autoencoders and demonstrate its utility in improving sample quality and interpolations.
Decision: Reject
The primary reasons for rejection are: (1) the paper suffers from unclear writing and inconsistent notation, which significantly hampers comprehension and interpretation of the proposed method, and (2) the empirical results are limited and fail to demonstrate competitive performance against state-of-the-art generative models.
Supporting Arguments
1. Clarity and Notation Issues: The paper is poorly written, with unclear explanations and inconsistent notation, particularly in its treatment of probability distributions. For example, the distinction between \( P(Z) \), \( P̂(Z) \), and \( Q_\phi(Z|X) \) is not consistently maintained, leading to confusion. Additionally, the authors' use of standard VAE notation appears flawed, suggesting a misunderstanding of key concepts like reparameterization and prior regularization.
2. Limited Empirical Validation: The experimental results are restricted to qualitative evaluations on a single dataset (CelebA), with no quantitative metrics or comparisons to state-of-the-art models. The generated samples and interpolations are visually inferior to those produced by modern generative models, such as StyleGAN or diffusion models. Furthermore, the interpolations resemble pixel-space transitions rather than meaningful latent-space transitions, undermining the claim that MCMC sampling improves latent space traversal.
3. Misrepresentation of Concepts: The authors' critique of ancestral sampling and their assumptions about the relationship between \( P(Z) \) and \( P̂(Z) \) lack theoretical rigor. The paper does not adequately justify why MCMC sampling is necessary or how it addresses the supposed limitations of prior-based sampling. Additionally, the extension to denoising autoencoders is not well-motivated or convincingly demonstrated.
Additional Feedback
1. Clarity and Structure: The authors should focus on improving the clarity of their writing and ensuring consistent use of notation. Key concepts, such as the distinction between \( P(Z) \) and \( P̂(Z) \), should be clearly defined and contextualized.
2. Empirical Rigor: The paper would benefit from quantitative evaluations, such as FID or inception scores, and comparisons to state-of-the-art methods. Including results on additional datasets would also strengthen the empirical claims.
3. Theoretical Justification: The authors should provide a more rigorous theoretical analysis of why MCMC sampling is necessary and how it improves over existing methods. A clearer explanation of the convergence properties and computational trade-offs of the proposed approach would also be valuable.
Questions for the Authors
1. Can you provide quantitative metrics (e.g., FID, IS) to evaluate the quality of your generated samples and compare them to state-of-the-art models?
2. How does the computational cost of MCMC sampling compare to standard ancestral sampling? Is the improvement in sample quality worth the additional complexity?
3. Can you clarify the theoretical assumptions underlying your critique of ancestral sampling, particularly regarding the relationship between \( P(Z) \) and \( P̂(Z) \)?
In summary, while the paper introduces an interesting idea of using MCMC sampling in generative autoencoders, the lack of clarity, theoretical rigor, and strong empirical results prevents it from being a meaningful contribution at this stage.