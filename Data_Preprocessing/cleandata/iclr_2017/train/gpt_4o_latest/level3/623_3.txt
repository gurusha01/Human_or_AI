Review of the Paper
The paper investigates the eigenvalue distribution of the Hessian in deep learning loss functions, revealing a two-phase structure: a bulk concentrated around zero and discrete non-zero eigenvalues tied to input data complexity. It provides empirical evidence suggesting that the bulk eigenvalues are influenced by model architecture, while the discrete eigenvalues depend on the data. The authors further explore the implications of this degeneracy on optimization, convergence, and energy landscapes in deep learning. The work is positioned as part of a broader effort to understand the interplay between data, models, and algorithms.
Decision: Reject
While the paper offers interesting insights into the eigenvalue distribution of the Hessian and its implications, it falls short in several critical areas. The primary reasons for rejection are: (1) insufficient experimental rigor to substantiate claims, and (2) lack of clarity in presentation, particularly in figures and their integration into the text.
Supporting Arguments for Rejection
1. Experimental Rigor:  
   - The experiments, while intriguing, are not comprehensive enough to support the paper's claims fully. For instance, the conclusion suggests further experiments (e.g., exploring low-energy paths between solutions), but these are not included in the paper.  
   - The analysis would benefit significantly from comparative eigenvalue distribution plots for other machine learning methods, which could contextualize the findings within the broader literature.  
   - Normalizing weights before calculating the Hessian is essential to avoid misleading results, but this step is not explicitly addressed.  
   - A metric to quantify Hessian singularity is needed, as visual inspection of plots is insufficient for rigorous scientific analysis.  
2. Clarity and Presentation:  
   - Figures are poorly integrated into the text, with no clear references explaining their relevance to the narrative. This makes it difficult to follow the experimental results and their implications.  
   - The text in the figures is too small, making them hard to interpret. This significantly detracts from the paper's readability and accessibility.  
Additional Feedback for Improvement
1. Hessian Dynamics During Optimization: The paper focuses primarily on post-convergence Hessian analysis. Including plots of the Hessian's eigenvalue distribution during optimization would provide more actionable insights into the training dynamics.  
2. Comparative Analysis: Adding experiments comparing the Hessian eigenvalue distribution across different architectures, datasets, and optimization methods would strengthen the paper's claims and broaden its impact.  
3. Theoretical Context: While the paper references related work, it could better position its contributions within the existing literature. For example, a deeper discussion of how the findings relate to second-order optimization methods or recent work on flat minima would enhance its theoretical grounding.  
4. Figures and Metrics: Improve figure quality and ensure they are adequately referenced in the main text. Introduce a quantitative metric for Hessian singularity to replace reliance on visual interpretation.  
Questions for the Authors
1. How do you ensure that the observed eigenvalue distribution is not an artifact of weight initialization or lack of normalization?  
2. Can you provide comparative results for other machine learning methods or architectures to contextualize your findings?  
3. Why did you choose to focus on post-convergence Hessian analysis rather than tracking its evolution during training?  
In summary, while the paper addresses an important and underexplored topic, it requires significant improvements in experimental rigor, clarity, and presentation to meet the standards of the conference.