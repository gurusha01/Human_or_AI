Review
Summary of Contributions
The paper explores the potential of near-data processing (NDP) for machine learning (ML) by introducing a novel in-storage processing (ISP) platform, ISP-ML, which simulates ML workloads on NAND flash-based SSDs. The authors focus on stochastic gradient descent (SGD) as a case study, implementing three variants (synchronous, Downpour, and elastic averaging SGD) and comparing ISP-based optimization with conventional in-host processing (IHP). The work highlights the advantages of ISP, such as reduced data transfer overhead and faster convergence in memory-constrained environments. The authors also propose a methodology for fair performance comparisons between ISP and IHP and identify future research directions for ISP-based ML.
Decision: Reject
While the paper addresses an important and timely problem, it does not provide sufficient depth or clarity in its technical contributions and practical implications. The lack of a clear comparison with state-of-the-art hardware (e.g., GPUs, TPUs) and insufficient evidence of practical improvements limit its impact.
Supporting Arguments
1. Strengths:  
   - The paper tackles a valuable research area, addressing data transfer bottlenecks in ML through NDP.  
   - The ISP-ML platform is a significant engineering effort, and the evaluation of parallel SGD variants is interesting.  
   - The proposed methodology for IHP-ISP comparison is thoughtful and could be useful for future studies.  
2. Weaknesses:  
   - The practical improvements of ISP over existing specialized hardware (e.g., GPUs, TPUs) are unclear. The paper does not convincingly demonstrate that ISP offers a competitive advantage in real-world ML tasks.  
   - The evaluation is limited to a single dataset (MNIST) and a basic ML algorithm (logistic regression). This narrow scope reduces the generalizability of the findings.  
   - While the paper is well-written, it lacks depth in critical areas, such as hardware design trade-offs and the scalability of the proposed approach.  
   - The discussion of related work is insufficient, particularly regarding prior efforts in NDP and their limitations.  
Suggestions for Improvement
1. Expand the Evaluation:  
   - Include comparisons with state-of-the-art GPUs and TPUs to contextualize the performance of ISP.  
   - Test the platform on more complex ML models (e.g., deep neural networks) and larger datasets to demonstrate scalability and practical relevance.  
2. Clarify Practical Impact:  
   - Provide a detailed analysis of power consumption, cost, and latency improvements compared to existing hardware.  
   - Discuss the feasibility of deploying ISP-based SSDs in commercial systems and their potential market impact.  
3. Enhance Technical Depth:  
   - Elaborate on the hardware design choices, such as the trade-offs between embedded processors and dedicated hardware logic.  
   - Discuss the limitations of the current ISP-ML platform and how these could be addressed in future iterations.  
4. Improve Related Work Discussion:  
   - Provide a more comprehensive review of prior NDP research and clearly articulate how this work advances the state of the art.  
Questions for the Authors
1. How does the performance of ISP-ML compare to GPUs and TPUs in terms of throughput, power efficiency, and cost?  
2. What are the specific hardware limitations of ISP-ML, and how do they impact its scalability to larger datasets and more complex models?  
3. Could you provide more details on the communication overheads and bottlenecks in ISP, especially as the number of channels increases?  
4. How does the proposed methodology for IHP-ISP comparison account for differences in hardware configurations and workloads?  
In conclusion, while the paper presents an interesting concept and a well-executed simulation platform, it falls short in demonstrating the practical significance and broader applicability of ISP for ML. A more detailed and comprehensive evaluation, along with a clearer articulation of the contributions relative to existing hardware, would significantly strengthen the work.