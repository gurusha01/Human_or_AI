The paper introduces the Generative Multi-Adversarial Network (GMAN), an extension of the GAN framework that incorporates multiple discriminators to improve optimization and training stability. The authors argue that this approach addresses key challenges in GAN training, such as nonconvexity and overly harsh discriminator feedback, by providing diverse gradient signals to the generator. The proposed framework is theoretically grounded and empirically validated on image generation tasks, demonstrating faster convergence and improved sample quality compared to standard GANs. The authors also introduce the GMAM metric for evaluating GMAN performance, though its effectiveness in addressing mode coverage and probability mass misallocation remains questionable.
Decision: Accept (Rating: 7)  
The paper presents a novel and practical extension to GANs that is straightforward to implement and demonstrates clear improvements in training stability and sample quality. However, concerns about the GMAM metric and the limited scope of the evaluation warrant further analysis before broader claims can be made.
Supporting Arguments:  
1. Novelty and Contribution: The introduction of multiple discriminators is a well-motivated and innovative approach to addressing GAN training challenges. By leveraging random restarts and ensemble-like diversity, GMAN effectively mitigates issues of nonconvexity and harsh discriminator feedback. The ability to train with the original minimax objective without modifications is a notable achievement.  
2. Empirical Results: The experiments on MNIST, CelebA, and CIFAR-10 convincingly demonstrate that GMAN accelerates convergence and reduces variance in training dynamics. The qualitative results show sharper and more realistic images, while the GMAM metric indicates superior performance compared to baseline GANs.  
3. Practicality: The framework is simple to implement and compatible with existing GAN architectures, making it a valuable addition to the GAN training toolkit.
Areas for Improvement:  
1. Evaluation Metric: The GMAM metric, while intuitive, lacks evidence of its ability to capture GAN game convergence, mode coverage, or probability mass misallocation. The authors should provide a more rigorous justification for its use or supplement it with additional metrics, such as Inception scores or Fr√©chet Inception Distance (FID).  
2. Dataset Diversity: The evaluation is limited to a small set of datasets. Testing GMAN on more diverse and complex datasets, such as ImageNet, would strengthen the claims of generalizability and robustness.  
3. Necessity of Multiple Discriminators: While the learning curves suggest improved stability, the paper does not fully address whether the benefits of multiple discriminators outweigh the added computational cost, especially when the generator objective remains unmodified. A comparative analysis of computational efficiency would be beneficial.
Questions for the Authors:  
1. How does GMAN perform in terms of mode coverage and avoiding mode collapse compared to standard GANs? Can you provide quantitative evidence?  
2. Could you clarify how the GMAM metric addresses probability mass misallocation or provide additional metrics to evaluate this aspect?  
3. Have you explored the impact of varying the number of discriminators (e.g., beyond N=5) on performance and computational cost?  
In conclusion, the paper makes a significant contribution to GAN research by introducing a practical and effective multi-discriminator framework. Addressing the outlined concerns would further solidify its impact and applicability.