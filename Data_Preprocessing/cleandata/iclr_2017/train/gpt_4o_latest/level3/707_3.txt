Review
The paper introduces Pointer-Network Neural Networks (PNNs) for modeling referring expressions (REs) in dialogue, recipe generation, and news article tasks. The authors propose a novel framework that explicitly incorporates reference decisions into language models, outperforming standard sequence-to-sequence models with attention when conditioned on co-reference chains. The key contributions include architectural modifications to pointer networks with copy mechanisms, application to 2D arrays, and supervised alignment training. The empirical evaluation demonstrates improved performance on three tasks, albeit with limitations in dataset size and generalizability.
Decision: Reject
The decision to reject is based on two primary reasons: (1) the empirical results are inconclusive due to the small scale of datasets and lack of robust baselines, and (2) the paper contains significant clarity issues, including mathematical errors, unclear definitions, and inaccuracies in related work. While the proposed framework is conceptually interesting, the execution and presentation fall short of the standards required for acceptance.
Supporting Arguments for Rejection:
1. Empirical Limitations: The datasets used for evaluation (DSTC2 for dialogue, AllRecipes for recipes, and Gigaword for news articles) are either too small or non-standard, making it difficult to draw meaningful conclusions. The dialogue and recipe datasets, in particular, are insufficiently large to demonstrate the generalizability of the approach. Furthermore, perplexity, while a common metric, is not a reliable proxy for evaluating dialogue or recipe generation tasks.
2. Architectural Contributions: While the paper introduces modifications to pointer networks, these innovations are incremental rather than groundbreaking. The copy mechanism and application to 2D arrays are minor extensions of existing work, and the advantage of the "Entity state update" rule over standard pointer or copy networks is not well elaborated.
3. Clarity and Errors: The paper contains numerous mathematical, grammatical, and typographical errors, such as missing summations and unclear definitions. Additionally, the attention mechanism is not clearly explained, leaving ambiguity about whether it operates on word embeddings or hidden states and whether it applies to the entire dialogue history or just the previous turn.
4. Inaccuracy in Related Work: The claim that task-oriented dialogue models do not query databases during natural language generation is inaccurate, as prior work (e.g., Wen et al.) has demonstrated such capabilities. This oversight undermines the positioning of the paper within the broader literature.
Suggestions for Improvement:
1. Expand Datasets: Use larger and more diverse datasets to validate the proposed approach. For dialogue modeling, consider datasets like MultiWOZ or ConvAI2, which are more comprehensive and widely used.
2. Clarify Methodology: Provide a clearer explanation of the attention mechanism, including its inputs and scope. Elaborate on the "Entity state update" rule and its advantages over existing methods.
3. Address Errors: Carefully proofread the paper to eliminate mathematical, grammatical, and typographical errors. Ensure that all equations are complete and terms are well-defined.
4. Improve Empirical Evaluation: Incorporate additional evaluation metrics, such as human evaluation for dialogue tasks or BLEU scores for recipe generation. Compare against stronger baselines to demonstrate the robustness of the approach.
5. Positioning in Literature: Correct inaccuracies in the related work section and provide a more comprehensive discussion of how the proposed approach builds on and differs from prior work.
Questions for the Authors:
1. How does the attention mechanism operate in the proposed model? Does it use word embeddings or hidden states, and does it consider the entire dialogue history or only the previous turn?
2. Can you provide more details on the "Entity state update" rule and its specific advantages over standard pointer or copy networks?
3. Why were the chosen datasets deemed sufficient for evaluating the generalizability of the proposed approach? Would larger datasets yield different results?
4. How does the model handle rare or unseen entities in the test set, particularly in the dialogue and recipe tasks?
In summary, while the paper presents an interesting idea, the limitations in empirical evaluation, clarity, and positioning within the literature prevent it from meeting the standards for acceptance. Addressing these issues could significantly strengthen the work for future submission.