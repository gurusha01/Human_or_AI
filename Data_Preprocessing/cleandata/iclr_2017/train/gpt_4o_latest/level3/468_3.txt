Review of the Paper
Summary of Contributions
This paper addresses the problem of network quantization for deep neural networks, aiming to minimize performance loss under a compression ratio constraint. The authors propose two key contributions: (1) the use of uniform quantization combined with variable-length (Huffman) coding, and (2) the introduction of Hessian-weighted k-means clustering for fixed-length quantization. The latter eliminates the need for per-layer compression rate tuning and improves performance by accounting for the varying importance of network parameters. The paper also demonstrates an efficient approximation of Hessian weighting using the second moment estimates from the Adam optimizer, which incurs no additional computational cost. Experimental results on LeNet, ResNet, and AlexNet show strong compression ratios with minimal performance degradation, validating the proposed methods.
Decision: Accept
The paper is well-motivated, scientifically rigorous, and makes meaningful contributions to the field of network compression. While contribution (1) is not novel, it is valuable as it has not been explored in this specific context, and contribution (2) is innovative and impactful. The experimental results are strong and convincingly support the claims. However, the paper could benefit from minor clarifications and a reduction in length to improve readability.
Supporting Arguments
1. Problem Relevance and Novelty: The paper tackles a critical problem in deploying deep neural networks on resource-constrained devices. While uniform quantization with Huffman coding is not novel, its application to network quantization is unexplored and practical. The Hessian-weighted k-means clustering is a novel and well-justified approach that addresses the limitations of conventional k-means clustering by incorporating parameter importance.
   
2. Scientific Rigor: The derivation of the Hessian-weighted distortion measure is thorough and grounded in theory. The connection to entropy-constrained scalar quantization (ECSQ) is insightful and demonstrates a deep understanding of information theory. The experimental results are comprehensive, covering multiple models and scenarios, and provide strong evidence for the efficacy of the proposed methods.
3. Clarity and Presentation: The paper is clear in its explanations, though slightly verbose. The inclusion of alternative metrics (e.g., using the Adam optimizer's second moment estimates) and the discussion of practical implementation details (e.g., quantizing all layers together) enhance its utility for practitioners.
Suggestions for Improvement
1. Length: The paper is overly detailed in some sections, which could be condensed to improve readability without losing critical information. For example, the derivation of the Hessian-weighted distortion measure could be summarized more succinctly, with detailed proofs moved to an appendix.
2. Clarification: The discussion on "additional bits for layer indication" in layer-by-layer quantization needs further explanation. While this does not impact the core contributions, a brief clarification would enhance the reader's understanding.
3. Experiments: While the experiments are robust, additional comparisons with other state-of-the-art quantization methods (e.g., recent advances in mixed-precision quantization) would strengthen the paper further.
Questions for the Authors
1. Could you clarify the role of "additional bits for layer indication" in layer-by-layer quantization? How significant is this overhead in practice?
2. Have you considered the impact of retraining on the performance of the quantized models? If so, how does it compare to the results without retraining?
3. How does the proposed Hessian-weighted k-means clustering scale to extremely large models, such as GPT or ViT, where Hessian computation may become infeasible?
In conclusion, this paper makes a significant contribution to network quantization and is well-suited for acceptance, provided the authors address the minor clarifications and consider condensing the presentation.