The paper introduces an innovative approach to neural network design by employing hypernetworks to generate the weights of a main network, specifically focusing on recurrent neural networks (RNNs) such as LSTMs. This method challenges the traditional weight-sharing paradigm by enabling non-shared, adaptive weights, which are dynamically generated at each time step. The authors demonstrate the efficacy of their HyperLSTM model across several tasks, including language modeling, handwriting generation, and machine translation, achieving competitive or state-of-the-art results in many cases. The experiments are robust, covering both small-scale datasets like Penn Treebank and large-scale tasks like WMT'14 Enâ†’Fr translation.
Decision: Reject.  
While the paper presents an interesting idea and provides thorough experimental validation, the key advantages of the hypernetwork approach remain unclear. The improvements observed in HyperLSTM seem to primarily stem from an increase in model parameters rather than the hypernetwork mechanism itself. Additionally, the computational complexity of the proposed method is equal to or higher than that of standard networks, which undermines its practicality for large-scale tasks. The lack of focus in the paper further detracts from its impact, as it does not convincingly establish why hypernetworks are a superior alternative to existing methods.
Supporting Arguments:  
1. Strengths:  
   - The concept of using one network to generate the parameters of another is novel and well-motivated by related work in evolutionary computing and fast weights.  
   - The experimental results are comprehensive and demonstrate the model's ability to achieve competitive performance across diverse tasks.  
   - The paper explores interesting combinations, such as integrating layer normalization with HyperLSTM, which enhances performance.  
2. Weaknesses:  
   - The paper does not clearly articulate the unique benefits of hypernetworks beyond parameter generation. The observed performance gains could be attributed to the increased parameter count rather than the hypernetwork architecture itself.  
   - Computational efficiency is a concern, especially for tasks with large softmax layers, where the hypernetwork could significantly slow down training. This scalability issue is not adequately addressed.  
   - The paper lacks focus, attempting to cover too many tasks without delving deeply into the specific advantages or trade-offs of the proposed approach.  
Suggestions for Improvement:  
1. Clarify the core contribution of the hypernetwork approach. Is it primarily about enabling non-shared weights, improving generalization, or something else?  
2. Provide a more detailed analysis of the computational trade-offs, particularly for large-scale tasks. Include comparisons of training time and memory usage.  
3. Investigate whether the performance gains are due to the hypernetwork mechanism or simply the increased parameter count. A controlled experiment with matched parameter budgets would be helpful.  
4. Address scalability concerns, especially for tasks with large output spaces, and propose potential solutions to mitigate training slowdowns.  
Questions for the Authors:  
1. How does the performance of HyperLSTM compare to standard LSTMs when the total number of parameters is matched?  
2. Can the hypernetwork approach be made more computationally efficient, particularly for tasks with large softmax layers?  
3. What specific insights or advantages does the hypernetwork mechanism provide over simpler approaches like layer normalization or multiplicative integration?  
In summary, while the paper presents an intriguing idea and solid experimental results, it falls short in convincingly demonstrating the practical advantages of hypernetworks. Addressing the outlined concerns could significantly strengthen the paper's contribution.