Review of the Paper
Summary of Contributions
The paper investigates the expressive power of deep neural networks, focusing on the exponential growth of trajectory length with network depth and its implications for expressivity. The authors analyze three expressivity measures—neuron transitions, activation patterns, and dichotomies—and establish their exponential dependence on depth. A key contribution is the identification of trajectory length as a unifying factor underlying these measures. The paper also explores the influence of early-layer weights on network expressivity, both theoretically and experimentally, and provides insights into how training affects the input-output map, balancing stability and expressivity. The authors connect their findings to prior work on expressivity, particularly in feedforward networks with piecewise linear activations, and propose potential implications for network initialization and training strategies.
Decision: Reject
While the paper provides a detailed analysis of expressivity and trajectory length, the decision to reject is based on two primary reasons: (1) limited novelty in the theoretical contributions, as the results largely extend existing work without introducing fundamentally new insights, and (2) significant clarity issues in the presentation of results, particularly in distinguishing between theoretical and experimental findings and in defining key terms and assumptions.
Supporting Arguments
1. Limited Novelty: The paper builds on prior work, such as Montufar et al. (2014), but does not offer groundbreaking theoretical advancements. The exponential growth of trajectory length with depth is an interesting observation, but it is not sufficiently novel to justify acceptance at a top-tier conference. The connection between trajectory length and expressivity measures, while useful, is incremental.
   
2. Clarity Issues: The paper suffers from unclear explanations of key results and assumptions. For example, Theorem 1 lacks specificity in defining "random neural networks" and the properties of trajectories. The notation for asymptotic bounds is ambiguous, and the distinction between theoretical and experimental findings is often blurred. These issues hinder the accessibility and reproducibility of the work.
Suggestions for Improvement
1. Clarify Assumptions and Definitions: The authors should provide precise definitions of terms such as "random neural networks" and "trajectory length." The assumptions underlying theoretical results should be explicitly stated, and the notation for asymptotic bounds should be standardized.
2. Distinguish Theoretical and Experimental Results: The paper should clearly separate theoretical derivations from experimental validations. For example, the relationship between trajectory length and expressivity measures should be explicitly tested and validated in experiments.
3. Strengthen Connections to Prior Work: The paper's connection to prior work, particularly Montufar et al. (2014), should be better articulated. Highlighting how this work extends or complements existing results would strengthen its contribution.
4. Improve Presentation of Results: The results should be presented more clearly, with a focus on making the implications of the findings accessible to a broader audience. For example, the discussion of training effects on input-output stability and expressivity could be expanded and better contextualized.
Questions for the Authors
1. How does the proposed analysis of trajectory length differ fundamentally from prior work, and what are the key new insights?
2. Could you clarify the assumptions about the input trajectories and their generality? For example, how do the results extend to non-smooth or high-dimensional trajectories?
3. How robust are the experimental findings across different architectures and datasets? Have you tested the hypotheses on architectures beyond feedforward networks, such as convolutional or recurrent networks?
4. Theorem 1 relies on specific assumptions about weight initialization. How sensitive are the results to deviations from these assumptions, such as non-Gaussian weight distributions or non-zero biases?
In conclusion, while the paper provides a detailed exploration of expressivity and trajectory length, the limited novelty and clarity issues prevent its acceptance in its current form. Addressing these concerns could significantly improve the paper's impact and accessibility.