Review of the Paper
Summary of Contributions
This paper introduces a novel, domain-independent data augmentation technique that operates in a learned feature space rather than the input space. By leveraging sequence autoencoders (SA) to construct feature spaces, the authors propose adding noise, interpolating, or extrapolating between context vectors to generate synthetic data. The method is particularly relevant for sequence-to-sequence (seq2seq) models and is tested on a variety of datasets, including MNIST, CIFAR-10, and several time-series datasets. The authors argue that extrapolation in feature space is particularly effective, demonstrating improved performance over baselines and, in some cases, domain-specific input-space augmentation. The paper highlights the potential of feature-space augmentation as a general-purpose tool for improving supervised learning models, especially in scenarios with limited labeled data.
Decision: Reject  
While the paper proposes an interesting and potentially impactful method, it falls short in several critical areas, including experimental scope, comparisons to prior work, and clarity in positioning within the literature. These shortcomings hinder the paper's ability to convincingly demonstrate the generality and effectiveness of the proposed approach.
Supporting Arguments for Decision
1. Experimental Scope and Applicability:  
   The experiments are limited to toy datasets and small-scale benchmarks (e.g., MNIST, CIFAR-10). While the results show marginal improvements, the lack of evaluation on more challenging and foundational seq2seq tasks, such as machine translation (MT) or sentiment analysis, undermines the claim of general applicability. MT, in particular, is a canonical application of seq2seq models, and its omission is a significant oversight.
2. Comparison to Prior Work:  
   The paper does not compare its results to stronger baselines, such as Dai et al. (2015) for CIFAR-10, which would provide a more rigorous evaluation. Additionally, the proposed method is not directly compared to dropout applied to the context vector, a natural baseline for regularization in feature space. Without these comparisons, it is difficult to assess the true value of the proposed augmentation technique.
3. Positioning in the Literature:  
   The paper does not adequately situate itself within the broader context of related work. Key foundational papers, such as Bahdanau et al.'s ICASSP paper on attention mechanisms, are missing or improperly cited. This weakens the paper's motivation and its connection to the seq2seq literature.
Suggestions for Improvement
1. Expand Experimental Scope:  
   Include experiments on more complex and widely studied tasks, such as MT and sentiment analysis, to better demonstrate the method's generality and relevance. Additionally, evaluate the method on larger datasets to test its scalability.
2. Stronger Baselines:  
   Compare the proposed method to dropout applied to the context vector and to stronger baselines from prior work, such as Dai et al. (2015). This would provide a more comprehensive evaluation of the method's effectiveness.
3. Citations and Formatting:  
   Address the formatting issues in the references and ensure that all key related works are properly cited. This includes foundational seq2seq and attention mechanism papers, as well as other relevant works on data augmentation.
4. Clarify Applicability Beyond Seq2Seq:  
   While the method is primarily applied to seq2seq models, it would be valuable to discuss its potential applicability to non-seq2seq tasks. This could broaden the paper's impact and relevance.
Questions for the Authors
1. How does the proposed method compare to applying dropout directly to the context vector? Can you provide empirical results for this comparison?  
2. Why were machine translation and sentiment analysis tasks omitted from the experiments, given their importance in seq2seq research?  
3. Can the proposed augmentation method be applied to non-seq2seq tasks? If so, what modifications would be required?  
4. How does the method perform on larger and more complex datasets, where scalability might become an issue?
By addressing these issues, the paper could significantly strengthen its contributions and better demonstrate the utility of feature-space augmentation.