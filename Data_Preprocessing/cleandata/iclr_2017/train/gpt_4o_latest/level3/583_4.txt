The paper presents a novel framework for evaluating the creative capacity of generative models through the lens of out-of-distribution (OOD) novelty generation. It introduces a quantitative metric for assessing OOD novelty and validates it through extensive experimentation on over 1,000 models, complemented by a human subject study. The authors argue that traditional likelihood-based metrics are insufficient for evaluating novelty and propose alternative metrics, such as out-of-class objectness and out-of-class count, to better capture the creative potential of generative models. The paper also highlights the importance of human evaluation in assessing the quality of generated samples and provides a structured experimental setup to simulate novelty generation by holding out entire classes during training.
Decision: Reject
While the paper addresses an important and underexplored problem in machine learning, it falls short in several critical areas. The primary reasons for rejection are the lack of clarity and rigor in the human subject study and the limited scope of experiments, which undermine the generality and applicability of the proposed metric.
Supporting Arguments:
1. Human Subject Study: The paper mentions a human evaluation component but provides insufficient details about the study's design, including the number and diversity of participants, the questions asked, and the priming methods used. This lack of transparency raises concerns about the validity and reproducibility of the results.
2. Limited Experimental Scope: The experiments are restricted to Arabic digits and English letters, which significantly limits the generality of the proposed metric. The definition of "novelty" as transitioning between these two domains is narrow and lacks broader real-world relevance.
3. Debatable Definition of Novelty: The paper's definition of novelty as "out-of-class" generation is conceptually interesting but not well-justified. It remains unclear how this definition aligns with broader notions of creativity or practical applications.
4. Validation of Metrics: While the paper introduces several metrics, their utility and robustness are not convincingly demonstrated. For instance, the anti-correlation between in-class and out-of-class measures suggests potential conflicts in their objectives, which the paper does not adequately address.
Suggestions for Improvement:
1. Human Study Details: Provide a detailed description of the human subject study, including participant demographics, sample size, and the specific evaluation criteria used. This would strengthen the paper's claims about the utility of human evaluation.
2. Broader Experiments: Extend the experimental setup to include more diverse datasets and domains to demonstrate the generality of the proposed metric. For example, exploring novelty generation in more complex visual or textual datasets could add significant value.
3. Refine the Definition of Novelty: Offer a more nuanced and practical definition of novelty that aligns with real-world applications. Consider incorporating domain-specific novelty measures or tasks to ground the work in practical use cases.
4. Large-Scale Human Studies: Conduct large-scale human studies with meaningful tasks to validate the relevance and utility of the proposed metrics. This would provide stronger evidence for their applicability.
Questions for the Authors:
1. How were participants for the human study selected, and what steps were taken to ensure diversity and representativeness?
2. Why were Arabic digits and English letters chosen as the primary domains for experimentation, and how do you justify the generality of your metric based on these limited domains?
3. How do you address the anti-correlation between in-class and out-of-class measures? Could this indicate a fundamental issue with the proposed metrics?
In summary, while the paper explores an intriguing and important topic, it requires significant refinement in its experimental design, validation methods, and scope to make a stronger contribution to the field.