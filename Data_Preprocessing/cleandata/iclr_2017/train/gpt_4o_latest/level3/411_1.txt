Review of the Paper
Summary of Contributions
This paper presents a novel approach to code superoptimization by learning the proposal distribution in a stochastic search framework. Building on the STOKE superoptimization engine, the authors leverage reinforcement learning (using REINFORCE) to train a neural network that conditions the proposal distribution on program features. The proposed method aims to improve upon the uniform and non-conditioned proposal distributions used in prior work. The authors demonstrate the efficacy of their approach on two datasets: the Hacker's Delight corpus and a set of automatically generated programs. Experimental results indicate slight performance improvements over baselines, with the learned proposal distribution achieving faster and higher-quality optimization.
Decision: Reject
The paper is not suitable for acceptance at ICLR for two primary reasons: (1) its limited relevance to the core themes of representation learning and deep learning, and (2) the lack of significant technical novelty or impact in the context of the conference's audience.
Supporting Arguments
1. Relevance to ICLR: While the paper employs neural networks and reinforcement learning, the application is narrowly focused on code superoptimizationâ€”a niche domain with limited appeal to the broader ICLR community. The task does not align well with the conference's focus on advances in representation learning or generalizable machine learning techniques. The work might be better suited for venues like AAAI or UAI, where stochastic optimization and program synthesis are more central topics.
2. Technical Contributions: The proposed method, while novel in its application to superoptimization, is a straightforward adaptation of existing techniques (e.g., REINFORCE and neural networks). The use of Bag-of-Words (BoW) features to represent programs is simplistic and fails to capture deeper program semantics, limiting the model's potential. Employing more sophisticated architectures, such as Tree-LSTMs or graph neural networks, would have been a more impactful contribution.
3. Experimental Results: The reported improvements over baselines are modest and do not convincingly demonstrate the superiority of the proposed method. The lack of experiments on broader synthesis tasks or other MCMC applications further limits the generalizability and significance of the work.
Additional Feedback for Improvement
1. Representation of Program Semantics: The use of BoW features is a significant limitation. Future work should explore models that can better capture the structural and semantic properties of programs, such as Tree-LSTMs, graph neural networks, or other hierarchical representations.
2. Broader Applications: To increase the impact and generalizability of the method, the authors should evaluate their approach on other synthesis tasks or stochastic search problems beyond code superoptimization.
3. Clarity and Depth: The paper could benefit from a more detailed discussion of the limitations of the current approach, particularly regarding the choice of features and the scalability of the method. Additionally, a more thorough comparison with related work in program synthesis and stochastic optimization would strengthen the paper.
4. Relevance to ICLR: To better align with the ICLR audience, the authors could emphasize the representation learning aspects of their work, such as exploring how learned embeddings of programs could generalize across tasks or domains.
Questions for the Authors
1. Why were BoW features chosen over more expressive representations like Tree-LSTMs or graph-based models? Were such models considered, and if so, what challenges were encountered?
2. How does the proposed method scale with program length and complexity? Are there any performance bottlenecks when applying the approach to larger or more complex programs?
3. Could the method be applied to other stochastic search problems? If so, what modifications would be necessary to adapt it to these tasks?
In summary, while the paper presents an interesting application of reinforcement learning to code superoptimization, its limited relevance to ICLR's themes, modest technical contributions, and narrow scope make it a weak candidate for acceptance. The authors are encouraged to address these limitations and consider submitting to a more appropriate venue.