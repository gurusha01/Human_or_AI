Review of "Boosted Residual Networks"
Summary of Contributions
The paper introduces a new ensemble method, Boosted Residual Networks (BRN), which adapts the Deep Incremental Boosting (DIB) framework to Residual Networks (ResNets). The proposed method incrementally grows the ResNet by adding residual blocks at specific injection points during each boosting round. The authors argue that this approach leverages the architectural benefits of ResNets, such as skip connections, to improve training efficiency and accuracy. The paper provides experimental results on MNIST, CIFAR-10, and CIFAR-100 datasets, comparing BRN with DIB, AdaBoost, and single ResNets. Additionally, the authors explore distilled and bagged versions of the method to reduce test-time costs and evaluate its potential for approximation.
Decision: Reject
The paper is rejected due to insufficient novelty and inadequate empirical validation. While the idea of combining boosting with ResNets is interesting, the modifications to DIB are incremental and lack significant conceptual innovation. Furthermore, the experimental analysis is limited, raising concerns about the robustness and generalizability of the proposed method.
Supporting Arguments
1. Lack of Novelty: The primary contribution of the paper is the modification of DIB to allow the addition of residual blocks at intermediate positions rather than at the end of the network. This adjustment, while practical, is a minor extension of existing methods and does not introduce a fundamentally new approach to boosting or ResNet training.
   
2. Empirical Weaknesses: 
   - The experiments do not include data augmentation, which is a standard practice for evaluating deep learning models. This omission raises doubts about whether the reported improvements would hold under more realistic training conditions.
   - The comparisons are limited to relatively simple baselines (DIB, AdaBoost, single ResNets) and do not include state-of-the-art ResNet variants or DenseNets, which undermines the claim of superior performance.
   - The computational feasibility of the method on large-scale datasets like ImageNet is not demonstrated, and the potential computational overhead of the ensemble approach is not adequately addressed.
3. Unclear Sensitivity Analysis: The method relies on selecting an injection point (p_t) for adding residual blocks, but the sensitivity of the results to this choice is not explored. This is a critical gap, as it affects the reproducibility and practical applicability of the method.
Suggestions for Improvement
1. Strengthen Novelty: The authors should aim to provide a more substantial theoretical or methodological contribution. For example, exploring new ways to integrate boosting with ResNet architectures or proposing a novel ensemble training paradigm could enhance the paper's impact.
   
2. Expand Empirical Analysis:
   - Include experiments with data augmentation to validate the robustness of the method.
   - Compare BRN with state-of-the-art ResNet variants and DenseNets to provide a fair and comprehensive evaluation.
   - Test the method on larger datasets like ImageNet to assess scalability and computational efficiency.
3. Clarify Experimental Details: Provide more information about the experimental setup, including hyperparameters, training algorithms, and architecture configurations. Additionally, analyze the training time and computational cost of BRN compared to baselines.
4. Investigate Sensitivity: Conduct a thorough sensitivity analysis of the injection point (p_t) to understand its impact on performance and provide guidelines for its selection.
Questions for the Authors
1. How does the method perform when data augmentation is applied during training? Would the reported improvements persist under such conditions?
2. Why were state-of-the-art ResNet variants and DenseNets excluded from the baselines? How would BRN compare to these architectures?
3. Can the authors provide a detailed analysis of the computational cost of BRN, particularly for large-scale datasets like ImageNet?
4. How sensitive is the method to the choice of the injection point (p_t)? Would a suboptimal choice significantly degrade performance?
In conclusion, while the paper provides an interesting adaptation of DIB for ResNets, the lack of novelty and insufficient empirical rigor make it unsuitable for acceptance in its current form. Addressing the outlined concerns could significantly improve the quality and impact of the work.