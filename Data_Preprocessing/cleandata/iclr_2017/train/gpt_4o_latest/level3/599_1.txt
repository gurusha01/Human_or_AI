Review of the Paper
Summary of Contributions
This paper introduces GRU-D, a novel recurrent neural network (RNN) architecture designed to handle missing values in multivariate time series classification tasks. GRU-D extends the Gated Recurrent Unit (GRU) by incorporating two key mechanisms: (1) a learned convex combination for imputing missing values, and (2) a dampening mechanism that accounts for the elapsed time since the last observation. The authors argue that these mechanisms allow GRU-D to exploit the informative missingness patterns often present in real-world datasets, particularly in healthcare. The paper evaluates GRU-D on synthetic and real-world datasets (MIMIC-III, PhysioNet) and demonstrates its effectiveness compared to baseline models, including GRU-simple and traditional machine learning methods. The authors also highlight GRU-D's ability to provide insights into the temporal impact of missingness and its scalability with larger datasets.
Decision: Reject
While the paper introduces an interesting approach to handling missing values in time series data, it falls short in several critical areas. The primary reasons for rejection are: (1) the lack of convincing empirical evidence that GRU-D outperforms simpler baselines like GRU-simple, and (2) unsubstantiated or disproven claims regarding the model's ability to exploit temporal structure and scale with data size.
Supporting Arguments
1. Strengths:
   - The task definition is clear and addresses an important problem in time series analysis.
   - The proposed model introduces novel mechanisms (trainable decay and convex imputation) that are conceptually appealing.
   - The paper includes extensive experiments on diverse datasets, providing a comprehensive evaluation of GRU-D.
   - The discussion of informative missingness is insightful and highlights an underexplored area in time series modeling.
2. Weaknesses:
   - Empirical Results: GRU-D does not consistently outperform GRU-simple, particularly in scenarios where missingness is less informative. This raises questions about the utility of the additional parameters introduced by GRU-D.
   - Claims vs. Evidence: Several claims, such as GRU-D's ability to exploit temporal structure and improve with larger datasets, are either unsubstantiated or contradicted by the results. For example, the scalability experiments show only marginal improvements over baselines.
   - Presentation Issues: Important related work is relegated to the appendix, making it difficult to assess the novelty of GRU-D in the context of existing methods. Additionally, the paper contains numerous typos and could benefit from clearer writing.
   - Lack of Statistical Methods: The paper does not reference or compare GRU-D to statistical imputation methods, which are widely used for handling missing data.
Suggestions for Improvement
1. Reframe the Contribution: The paper should focus on the novel imputation parameterization as the main contribution, rather than overstating GRU-D's overall superiority.
2. Strengthen Baseline Comparisons: Include statistical methods for imputation and provide a more rigorous comparison to GRU-simple. Ensure fairness in hyperparameter tuning across models.
3. Substantiate Claims: Provide stronger empirical evidence to support claims about temporal structure exploitation and scalability. For example, include ablation studies to isolate the impact of the decay mechanism.
4. Improve Presentation: Move critical related work from the appendix to the main text and address the typos and clarity issues.
Questions for the Authors
1. Can you provide additional evidence or ablation studies to demonstrate the utility of the trainable decay mechanism in GRU-D compared to GRU-simple?
2. How does GRU-D perform when compared to statistical imputation methods (e.g., multiple imputation or EM algorithm)?
3. Could you clarify how the fairness of comparisons between GRU-D and GRU-simple was ensured, particularly in terms of parameter count and training settings?
4. Why do you believe GRU-D fails to consistently outperform GRU-simple in some scenarios? Are there specific datasets or conditions where GRU-D is expected to excel?
While the paper addresses an important problem and proposes a novel approach, it requires significant refinement and stronger empirical validation to justify its claims.