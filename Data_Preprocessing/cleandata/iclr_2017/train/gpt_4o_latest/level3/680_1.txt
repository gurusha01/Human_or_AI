The paper proposes a character-to-character neural machine translation (NMT) model that incorporates a hierarchical decoder, achieving strong results on bilingual corpora such as English-French and English-Czech. The authors address the limitations of word-level NMT models, particularly the large vocabulary bottleneck, by introducing a deep character-level architecture consisting of six recurrent neural networks. This approach eliminates the need for large vocabularies, learns morphology effectively, and demonstrates competitive BLEU scores compared to state-of-the-art models. The authors also commendably make their code publicly available, enabling reproducibility.
Decision: Accept (Conditional)  
Key reasons for this decision include the paper's strong experimental results and its well-written presentation. However, the decision is conditional on addressing the limited novelty of the hierarchical decoder and providing additional details on model size and failure cases.
Supporting Arguments:  
1. Strengths:  
   - The paper is well-structured, with clear explanations and thorough experimental analysis.  
   - The proposed model demonstrates strong empirical performance, outperforming subword-based baselines after just one epoch and achieving competitive results with other character-level models.  
   - The ability to handle out-of-vocabulary (OOV) and misspelled words is a notable advantage, showcasing the practical utility of the model.  
   - The authors' decision to release their code enhances the paper's impact and replicability.  
2. Weaknesses:  
   - The hierarchical decoder, while effective, is not novel, as similar architectures have been explored in prior work (e.g., Serban et al., 2015). The paper would benefit from a more comprehensive discussion of related work to better position its contributions.  
   - The model's complexity, involving six recurrent networks, raises concerns about computational efficiency. Character-level processing is inherently resource-intensive, and this aspect is not adequately addressed.  
   - The contributions are primarily in applying existing techniques rather than introducing fundamentally new methodologies.  
Additional Feedback for Improvement:  
1. Citations: The paper should include references to prior work on hierarchical decoders and character-level NMT models to contextualize its contributions.  
2. Model Size: Table 1 should include the model size (number of parameters) for a fair comparison with other models. This would clarify the trade-offs between performance and computational cost.  
3. Failure Cases: A discussion of failure cases would provide insights into the model's limitations and guide future improvements. For example, does the model struggle with specific linguistic phenomena or longer sequences?  
4. Efficiency: While the authors highlight training efficiency compared to other character-level models, further details on runtime and memory usage would strengthen the paper.  
5. Broader Applicability: The authors briefly mention potential applications in speech recognition and text summarization. Expanding on these ideas would enhance the paper's generalizability.  
Questions for the Authors:  
1. How does the model perform on longer sequences or highly morphologically complex languages beyond Czech?  
2. Could the authors provide more details on the computational efficiency of their approach compared to word-level and subword-level models?  
3. How does the model handle ambiguous or polysemous words at the character level?  
In conclusion, the paper presents a strong application of character-level NMT with promising results. Addressing the above points would significantly enhance its impact and clarity.