Review of the Paper
Summary of Contributions
This paper proposes an open-vocabulary neural language model (NLM) that combines character-level embeddings with convolutional networks to address challenges in handling large vocabularies, particularly for morphologically rich languages like Czech. The authors explore three embedding types—word embeddings (WE), character embeddings (CE), and combined embeddings (CWE)—and employ Noise Contrastive Estimation (NCE) to mitigate the computational cost of the output layer during training. The model is evaluated in a machine translation (MT) reranking task, achieving a modest BLEU score improvement of up to 0.7. The paper highlights the challenges of training character-based models and the instability of using character-level representations for output words.
Decision: Reject
Key reasons for rejection:
1. Limited Contribution Over Baselines: The reported BLEU improvement of 0.7 is primarily attributed to the CWE model, but the gain over a vanilla NLM is only 0.1 BLEU, which is not statistically significant. The CWE-CWE model, which is the paper's primary innovation, underperforms compared to baseline models.
2. Lack of Clarity and Rigor: The testing process for the open-vocabulary NLM is unclear, particularly regarding normalization during reranking. Perplexity results in Table 2 and Figures 2 and 3 lack sufficient explanation of how normalization was performed with a 250k output vocabulary.
3. Insufficient Discussion of Computational Complexity: The paper does not adequately address the computational trade-offs during training and inference, which is critical for evaluating the practicality of the proposed approach.
Supporting Arguments
1. Weak Empirical Results: While the CWE model shows some promise, the CWE-CWE model, which uses character-level embeddings for both input and output, performs poorly. This contradicts prior work that demonstrated the effectiveness of character-level representations in other tasks. The authors acknowledge contamination issues in character n-grams but fail to provide a robust solution.
2. Unclear Methodology: The paper does not clearly explain how normalization is handled during testing, particularly for reranking n-best lists. This lack of clarity undermines confidence in the reported results.
3. Minor but Distracting Errors: Mislabeled figures (e.g., "Figure 4" instead of "Figure 2 and 3") and unusual citation formatting detract from the paper's professionalism.
Suggestions for Improvement
1. Clarify Testing and Normalization: Provide a detailed explanation of how normalization is performed during testing, especially when reranking n-best lists with a large output vocabulary.
2. Address Computational Complexity: Include a thorough discussion of the computational trade-offs during training and inference, particularly for the CWE-CWE model.
3. Improve Empirical Rigor: Conduct statistical significance tests to validate the BLEU score improvements. Additionally, explore alternative noise distributions for NCE to address the "rich get richer" issue in character n-gram representations.
4. Enhance Presentation: Correct figure labeling and citation formatting to improve readability and professionalism.
5. Expand Analysis: Investigate why character-level output representations underperform and provide a more detailed analysis of learned representations. This could include visualizations or case studies to better understand the model's behavior.
Questions for the Authors
1. How is normalization handled during testing, particularly when reranking n-best lists with a 250k output vocabulary?
2. Can you provide statistical significance tests for the BLEU score improvements? How confident are you that the observed gains are meaningful?
3. Why do character-level output representations perform so poorly compared to prior work? Could alternative architectures, such as LSTMs or transformers, mitigate these issues?
4. What is the computational cost of the proposed models during training and inference, and how does it compare to baseline models?
In summary, while the paper addresses an important problem and introduces an interesting approach, the lack of significant empirical gains, unclear methodology, and insufficient discussion of computational complexity make it unsuitable for acceptance in its current form.