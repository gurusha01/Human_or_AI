Review of the Paper
Summary of Contributions
This paper builds upon the gradient regularizer proposed by Hariharan and Girshick (2016) and provides a deeper theoretical and empirical analysis of feature penalty regularization in low-shot learning. The authors demonstrate that the gradient regularizer is equivalent to a feature magnitude penalty weighted per example and analyze its effects through two case studies: the XOR problem and a two-layer linear network. They also propose an improved cost function by adding a weight penalty term, which they argue avoids degenerate solutions. The paper draws an intriguing connection between feature regularization and batch normalization, interpreting the regularizer as a Gaussian prior. Experimental results show modest improvements on synthetic XOR datasets, competitive performance on Omniglot, and effective low-shot learning on ImageNet. The work is notable for its simplicity, multi-angle analysis, and potential broader implications for understanding batch normalization.
Decision: Reject
While the paper offers interesting theoretical insights and experimental validation, it falls short in several critical areas. The primary reasons for rejection are the lack of clear motivation for why feature regularization aids generalization and the insufficient clarity in theoretical derivations and experimental setups.
Supporting Arguments for Decision
1. Strengths:
   - The paper provides a novel theoretical perspective on feature regularization, particularly its connection to batch normalization.
   - The empirical results on Omniglot and ImageNet demonstrate the potential of the proposed method in low-shot learning tasks.
   - The analysis of the XOR problem and the two-layer linear network offers valuable insights into the mechanics of feature regularization.
2. Weaknesses:
   - The paper does not provide a compelling explanation for why feature regularization improves generalization. The connection to batch normalization is interesting but not explored deeply enough to justify its practical significance.
   - The XOR example uses a non-standard architecture, and its relevance to real-world low-shot learning tasks is unclear.
   - The theoretical analysis relies on specific assumptions (e.g., L2 or cross-entropy loss) that limit its generalizability. Some derivations lack clear intuition, making it difficult to follow the arguments.
   - The experimental results are difficult to interpret in isolation due to the inclusion of batch normalization in the models. This makes it challenging to disentangle the effects of feature regularization from batch normalization.
   - The ImageNet experiment setup deviates from Hariharan and Girshick (2016), making direct comparisons less meaningful.
3. Presentation Issues:
   - The paper is densely written, and key ideas are buried in technical details. A more focused presentation would improve readability.
   - Missing baseline results (e.g., Matching Networks in Table 1) and unclear convergence explanations detract from the experimental rigor.
Suggestions for Improvement
1. Motivation and Intuition: Provide a stronger, more intuitive explanation for why feature regularization aids generalization. For example, discuss its impact on the geometry of the feature space or its role in reducing overfitting.
2. Case Studies: Clearly justify the choice of the XOR problem and explain its relevance to low-shot learning. Consider using more standard benchmarks or architectures for theoretical analysis.
3. Theoretical Analysis: Generalize the analysis beyond specific loss functions and provide clearer intuition for key derivations.
4. Experimental Design: Design experiments that isolate the effects of feature regularization from batch normalization. For instance, compare models with and without batch normalization while keeping other factors constant.
5. Presentation: Streamline the paper to highlight the key contributions and results. Include all relevant baselines and ensure that tables and figures are self-contained and easy to interpret.
Questions for the Authors
1. Can you provide a more intuitive explanation for why feature regularization improves generalization, particularly in low-shot learning scenarios?
2. How does the proposed method perform when batch normalization is removed entirely from the models? Can you isolate the effects of feature regularization more clearly?
3. Why was the XOR problem chosen as a case study? How does it generalize to more complex, real-world tasks?
4. Could you clarify the assumptions made in your theoretical analysis and discuss their implications for generalization to other loss functions or architectures?
While the paper has potential, addressing these concerns would significantly strengthen its contributions and impact.