Review
Summary of Contributions
This paper introduces the Pointer Sentinel Mixture Model, a novel architecture that combines the strengths of pointer networks and standard softmax-based recurrent neural networks (RNNs) to improve language modeling. The proposed model addresses challenges in predicting rare or unseen words by enabling the network to either reproduce words from recent context (via the pointer mechanism) or generate words from a standard softmax classifier. The authors apply this architecture to LSTMs, demonstrating state-of-the-art performance on the Penn Treebank dataset with fewer parameters compared to existing models. Additionally, the paper introduces WikiText, a new dataset designed to evaluate language models on long-range dependencies and rare word prediction, addressing limitations of the commonly used Penn Treebank dataset. The paper is well-written, with clear motivation, rigorous experiments, and promising results.
Decision: Accept
The paper makes a significant contribution to language modeling by proposing a novel mixture model that effectively combines pointer and softmax outputs. The results are compelling, with improvements in perplexity on established benchmarks and a strong focus on rare word prediction. The introduction of the WikiText dataset is another valuable contribution that will likely benefit the broader research community. However, minor issues with notation and clarity should be addressed.
Supporting Arguments
1. Novelty and Motivation: The idea of combining pointer networks with softmax classifiers in a mixture model is novel and well-motivated. The authors clearly articulate the limitations of existing models and how their approach addresses these issues, particularly for rare word prediction and long-range dependencies.
2. Experimental Rigor: The experiments are thorough, with comparisons to strong baselines on both the Penn Treebank and the newly introduced WikiText dataset. The results convincingly demonstrate the advantages of the proposed model, particularly in terms of perplexity and parameter efficiency.
3. Impact: The introduction of the WikiText dataset is a significant contribution that fills a gap in existing benchmarks, providing a more realistic evaluation setting for language models.
Suggestions for Improvement
1. Notation Consistency: The inconsistent use of the symbol \( p{\text{ptr}} \) in Equations 3 and 5 is confusing. In Equation 3, \( p{\text{ptr}} \) refers to a probability distribution over a set, while in Equation 5 it refers to a normalized distribution over a list. The authors should clarify this distinction and ensure consistent notation throughout the paper.
2. Ablation Studies: While the results are strong, additional ablation studies could provide deeper insights into the contributions of individual components, such as the sentinel mechanism and the gating function.
3. Qualitative Analysis: The qualitative analysis of pointer usage is interesting but could be expanded with more examples and visualizations to better illustrate the model's behavior in different scenarios.
Questions for the Authors
1. How sensitive is the model to the choice of the pointer window size \( L \)? Would a dynamic window size based on context improve performance?
2. Could the proposed mixture model be applied to other tasks beyond language modeling, such as machine translation or summarization? If so, what modifications would be required?
3. The WikiText dataset is an excellent contribution. Have you evaluated how well existing models (e.g., transformers) perform on this dataset compared to the proposed pointer sentinel-LSTM?
In conclusion, this paper makes a strong contribution to the field of language modeling by proposing a novel architecture and introducing a valuable new dataset. While there are minor issues with notation and additional analysis could strengthen the paper, these do not detract from its overall quality and impact. I recommend acceptance.