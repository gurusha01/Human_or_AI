Review
Summary of Contributions
This paper introduces a novel method for training sparse Recurrent Neural Networks (RNNs) using a threshold-based pruning approach with a scheduled increase during training. The authors demonstrate that this technique can significantly reduce the number of parameters in RNNs—achieving up to 90% sparsity—while maintaining competitive accuracy compared to dense models. The method is shown to improve computational efficiency and reduce memory requirements, making it particularly suitable for deployment on resource-constrained devices like mobile phones and embedded systems. The experimental results highlight that sparse models can outperform smaller dense models and, in some cases, even achieve better accuracy than larger dense baselines. Additionally, the authors provide timing experiments using cuSPARSE, showcasing potential speedups in inference time, though they acknowledge that the current implementation is suboptimal. The paper is well-written, addresses a practical problem in deep learning, and provides a clear comparison to prior work, particularly Yu et al. (2012).
Decision: Accept
The paper presents a well-motivated and impactful contribution to the field of efficient deep learning. The proposed pruning method is simple, effective, and practical for real-world deployment. The experimental results are compelling and support the claims made by the authors. However, there are areas where the paper could be improved, particularly in terms of clarity and additional baselines.
Supporting Arguments
1. Well-Motivated Problem: The paper tackles a critical issue in deep learning—scaling large RNNs for deployment on memory- and compute-constrained devices. The motivation is clear and aligns with current trends in efficient AI.
2. Empirical Rigor: The experimental results are thorough, comparing sparse models to dense baselines and alternative pruning methods (e.g., Yu et al., 2012). The results convincingly demonstrate the efficacy of the proposed approach.
3. Practical Relevance: The method is easy to implement, does not increase training time, and is compatible with existing frameworks, making it highly practical for real-world applications.
Suggestions for Improvement
1. Clarity in Results: While the comparison to Yu et al. (2012) is appreciated, the connection between Tables 3 and 4 could be made more explicit. For instance, the authors should clearly state how the results in Table 4 validate the superiority of gradual pruning over hard pruning.
2. Additional Baseline: The paper would benefit from including a comparison to "distillation" approaches, which are another popular method for reducing model size. This would provide a more comprehensive evaluation of the proposed method.
3. Timing Experiments: The timing results using cuSPARSE are promising, but the authors should discuss the limitations of the current implementation in more detail and propose concrete steps for improvement. For example, how might state-of-the-art sparse matrix-vector multiplication libraries impact the observed speedups?
4. Generalization: While the focus on speech recognition tasks is understandable, it would be helpful to discuss whether the proposed method generalizes to other domains, such as language modeling or machine translation.
Questions for the Authors
1. Could you clarify the relationship between the results in Tables 3 and 4? How do they jointly support the claim that gradual pruning outperforms hard pruning?
2. Have you considered comparing your method to distillation-based approaches? If so, what are the key differences in terms of accuracy and computational efficiency?
3. Can you provide more insights into the limitations of the cuSPARSE implementation? Are there specific optimizations that could bridge the gap between the observed and theoretical speedups?
Overall, this paper makes a valuable contribution to the field of efficient deep learning and is well-suited for acceptance at the conference. Addressing the above suggestions would further strengthen the work.