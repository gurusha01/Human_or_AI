Review of the Paper: Hypernetworks for Generating Adaptive Weights in Recurrent Networks
Summary of Contributions:
This paper introduces a novel neural network architecture, termed "hypernetworks," where a smaller network generates the weights for a larger main network. The focus is on applying hypernetworks to recurrent neural networks (RNNs), particularly LSTMs, to relax the weight-sharing paradigm. The authors demonstrate the efficacy of this approach across multiple sequence modeling tasks, including language modeling, handwriting generation, and machine translation, achieving state-of-the-art results in some cases. The paper also explores the scalability and efficiency of hypernetworks, presenting them as a competitive alternative to traditional RNN architectures.
Decision: Reject
While the paper presents an interesting idea and achieves impressive results, it suffers from significant issues in focus, clarity, and positioning within the literature. These shortcomings undermine its overall impact and suitability for acceptance in its current form.
Supporting Arguments:
1. Strengths: 
   - The paper successfully revives interest in multiplicative RNNs and second-order networks, showcasing their potential for language modeling and machine translation.
   - The empirical results are promising, with hypernetworks achieving competitive or state-of-the-art performance in multiple tasks.
   - The approach is well-motivated by the need for adaptive weight-sharing in RNNs, and the experiments demonstrate its practical applicability.
2. Weaknesses:
   - Lack of Focus: The paper attempts to address both RNN and CNN use cases, leading to a diluted narrative. The CNN results are underdeveloped and lack clear motivation or comparisons with established methods like model compression or hashed networks.
   - Related Work Gap: The paper fails to adequately discuss its connections to prior work on multiplicative RNNs, second-order networks, and tensor product-based architectures. This omission weakens its positioning within the broader literature.
   - Unclear Goals: It is unclear whether the primary goal is improved performance (as seen in the RNN results) or more compact networks (as suggested by the CNN experiments). This ambiguity creates a mixed message.
   - Experimental Clarity: Key figures (e.g., Figures 2, 4, and 5) and saturation statistics lack clarity and meaningful comparisons. Additionally, the handwriting generation results are difficult to interpret due to the absence of precision/recall metrics.
   - Writing Issues: The paper is too long for the conference format, with redundant details and insufficient focus on the most impactful contributions.
Suggestions for Improvement:
1. Refocus the Paper: Concentrate on the RNN results, as they are the most compelling and well-supported aspect of the work. The CNN experiments could be deferred to a future publication.
2. Clarify Related Work: Strengthen the discussion of connections to prior work, particularly multiplicative RNNs, second-order networks, and tensor product-based architectures. This will better contextualize the contribution.
3. Improve Experimental Reporting: Provide detailed discussions of training time, inference time, and memory requirements for RNNs, as well as clear motivations and comparisons for CNN results.
4. Streamline the Writing: Shorten the paper by removing redundant details and focusing on the core contributions. Ensure that figures and tables are clear and self-explanatory.
5. Handwriting Generation Metrics: Include precision/recall metrics or other quantitative evaluations to make the handwriting generation results more interpretable.
Questions for the Authors:
1. How does the proposed method compare to other weight-sharing relaxation techniques, such as those used in model compression or hashed networks, particularly for CNNs?
2. Can you provide more details on the computational trade-offs (e.g., training/inference time, memory usage) of hypernetworks compared to standard RNNs and LSTMs?
3. What is the significance of the observed weight changes in handwriting generation, and how do they correlate with qualitative results?
In conclusion, while the paper presents a novel and promising idea, it requires significant revisions to address issues of focus, clarity, and positioning within the literature. A more concise and focused version of the paper, emphasizing the RNN results and providing clearer experimental analysis, would be much stronger.