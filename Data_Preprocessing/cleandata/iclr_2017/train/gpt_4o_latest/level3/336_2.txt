Review of the Paper
This paper proposes five key modifications to the original PixelCNN architecture, collectively termed PixelCNN++, and demonstrates their effectiveness through empirical validation. The authors achieve state-of-the-art results on CIFAR-10, supported by extensive ablation experiments. Key contributions include replacing the 256-way softmax with a discretized mixture of logistic distributions, simplifying the architecture by conditioning on whole pixels rather than sub-pixels, introducing downsampling for multi-resolution processing, adding short-cut connections for information recovery, and employing dropout for regularization. These modifications improve training speed, optimize convergence, and enhance generative performance, as evidenced by both quantitative log-likelihood results and qualitative image samples.
Decision: Accept
The paper is well-motivated, methodologically sound, and demonstrates clear improvements over the original PixelCNN. The modifications are thoughtfully designed, rigorously tested through ablation studies, and yield state-of-the-art results. However, minor clarity issues in the predictive distribution and figure descriptions should be addressed to improve the paper's overall presentation.
Supporting Arguments for Acceptance
1. Novel Contributions and Empirical Validation: The proposed discretized mixture of logistic distributions is a significant improvement over the 256-way softmax, addressing issues of memory inefficiency and sparse gradients. The ablation studies convincingly demonstrate the necessity of each modification, such as the critical role of short-cut connections and dropout in achieving high performance.
2. Simplified and Efficient Architecture: Conditioning on whole pixels rather than sub-pixels reduces architectural complexity while maintaining performance. The use of downsampling instead of dilated convolutions is computationally efficient and achieves comparable results.
3. State-of-the-Art Results: The paper achieves state-of-the-art log-likelihood scores on CIFAR-10, demonstrating the practical utility of the proposed modifications.
Suggestions for Improvement
1. Clarity on Predictive Distribution: The description of how the mixture component means for green and blue channels depend on the red sub-pixel is unclear. Explicit equations or visual aids would help clarify this dependency.
2. Figure 2 Discrepancy: The layer descriptions in Figure 2 are inconsistent with the text. For example, the representation of "green square -> blue square" is ambiguous and should be clarified.
3. Shared Mixture Indicators: The rationale for sharing mixture indicators across channels is not well-explained. Additional discussion or experimental evidence supporting this design choice would strengthen the paper.
Questions for the Authors
1. How does the shared mixture indicator across channels impact performance compared to independent indicators? Was this design choice empirically validated?
2. Could you elaborate on the dependency modeling for green and blue channels, particularly the role of the red sub-pixel? Are there alternative formulations that were considered?
3. How does PixelCNN++ perform on datasets other than CIFAR-10? Are the proposed modifications generalizable to other domains, such as audio or text?
Conclusion
This paper makes meaningful contributions to generative modeling by improving the PixelCNN architecture. While minor clarity issues exist, they do not detract from the overall quality of the work. The paper is well-positioned to advance the state of the art and is a strong candidate for acceptance.