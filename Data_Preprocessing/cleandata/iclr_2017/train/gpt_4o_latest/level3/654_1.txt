Review of the Paper
Summary of Contributions
This paper introduces a novel method called Variational Walkback, which trains probabilistic models by maximizing a stochastic variational lower bound. The approach focuses on transition-based inference, allowing the model to learn a raw transition operator directly, rather than relying solely on energy-based models. This is a significant departure from traditional methods like Contrastive Divergence (CD) and Generative Stochastic Networks (GSNs). The proposed method innovatively uses a walkback strategy, where the model is trained to "walk back" from noise to the data manifold, leveraging a temperature annealing schedule to improve the tightness of the variational bound. The authors argue that this approach avoids the need for symmetric weights, making it more biologically plausible, and provides a framework for learning transition operators directly. Experimental results on datasets such as MNIST, CIFAR-10, and CelebA demonstrate the potential of the method, though some limitations are acknowledged.
Decision: Reject
The paper presents a promising idea but is not yet ready for publication due to several critical shortcomings. The primary reasons for this decision are the lack of rigorous evaluation of sample quality and the absence of a mathematical demonstration of the tightness of the variational bound for learned transition operators.
Supporting Arguments
1. Strengths:
   - The idea of parametrizing a transition operator directly, rather than relying on an energy function, is innovative and has potential for simplifying the training of undirected graphical models.
   - The proposed method introduces a principled variational bound, which is conceptually appealing and addresses some limitations of CD and GSNs.
   - The walkback strategy, combined with temperature annealing, is a compelling approach to eliminate spurious modes and improve convergence.
2. Weaknesses:
   - The paper lacks a clear evaluation of sample quality. While qualitative results are presented, quantitative metrics (e.g., held-out likelihoods using annealed importance sampling) are missing, making it difficult to assess the effectiveness of the method.
   - The mathematical analysis does not convincingly demonstrate the tightness of the variational bound for the learned transition operators. This is a critical gap, as the validity of the proposed method hinges on this property.
   - The experimental results, while suggestive, are not comprehensive. The blurring effect observed in CIFAR-10 and CelebA samples raises concerns about the model's ability to capture high-frequency details.
   - The comparison with energy-based models is insufficient. The authors should provide a more thorough analysis of the advantages and trade-offs of learning transition operators directly.
Additional Feedback for Improvement
- Evaluation: Include quantitative evaluations of sample quality using metrics like held-out likelihoods or precision/recall for generative models. Incorporate annealed importance sampling to validate the variational bound.
- Mathematical Analysis: Strengthen the theoretical analysis by providing a rigorous proof or empirical evidence of the tightness of the variational bound for the learned transition operators.
- Comparison: Expand the comparison with energy-based models and other generative frameworks (e.g., GANs, VAEs) to highlight the unique benefits of the proposed approach.
- Experiments: Address the blurring effect in generated samples by exploring alternative architectures or loss functions. Additionally, include experiments on more challenging datasets to demonstrate scalability.
Questions for the Authors
1. How does the proposed method compare quantitatively with existing methods like CD, GSNs, and VAEs in terms of sample quality and log-likelihood?
2. Can you provide a more detailed analysis of the tightness of the variational bound, either theoretically or empirically?
3. How sensitive is the method to the choice of temperature annealing schedules and the number of walkback steps during training and sampling?
The paper has potential and introduces an interesting direction for generative modeling. Addressing the above concerns could significantly strengthen the work for future submission.