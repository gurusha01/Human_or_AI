Review
Summary of Contributions
This paper investigates strategies to reduce the output vocabulary size in Neural Machine Translation (NMT) systems to improve decoding and training efficiency. It extends prior work by Mi et al. (2016) by experimenting with additional vocabulary selection methods, such as bilingual embeddings, word alignments, phrase pairs, and Support Vector Machines (SVMs), and analyzing the trade-offs between speed and accuracy in greater detail. The authors demonstrate that decoding time on CPUs can be reduced by up to 90% and training time on GPUs by 25%, with negligible degradation in translation accuracy. The study is validated on two language pairs (English-German and English-Romanian) and multiple test sets, achieving significant speed-ups while maintaining state-of-the-art BLEU scores.
Decision: Reject
While the paper provides valuable insights into vocabulary reduction techniques for NMT, it does not align closely with the core themes of representation learning and neural network innovation, which are central to ICLR. Additionally, some methodological clarifications and broader contextual explorations are missing, limiting the paper's impact.
Supporting Arguments
1. Relevance to ICLR: The paper focuses on practical efficiency improvements in NMT rather than advancing fundamental neural network architectures or representation learning techniques. This raises questions about whether ICLR is the most appropriate venue for this work, as its contributions are more aligned with applied machine translation or computational efficiency research.
   
2. Methodological Clarity: The paper does not explicitly clarify whether the reported decoding times include the vocabulary reduction step, which is critical for assessing the true efficiency gains. This omission weakens the empirical rigor of the results.
3. Scope of Applications: The paper narrowly focuses on NMT and does not explore potential applications of the proposed techniques in other domains, such as language modeling or text generation. Broadening the scope could significantly enhance the paper's relevance and impact.
Suggestions for Improvement
1. Clarify Methodology: The authors should explicitly state whether the vocabulary reduction step is included in the reported decoding times. If not, they should provide a breakdown of the time taken for this step to ensure transparency.
2. Explore Broader Applications: The techniques discussed, particularly vocabulary selection strategies, could be relevant to other NLP tasks like language modeling or summarization. The authors should explore or at least discuss these possibilities to broaden the paper's appeal.
3. Alternative Baselines: The reviewer suggests starting from character-level or subword-level models (e.g., byte pair encoding) as a baseline for addressing the large vocabulary problem. These approaches are well-established in NMT and could serve as a meaningful point of comparison.
4. Venue Consideration: The authors may consider submitting this work to a conference or journal more focused on applied machine translation or computational efficiency, such as ACL or EMNLP, where the contributions would be more directly appreciated.
Questions for Authors
1. Does the reported decoding time include the vocabulary reduction step? If not, how much additional time does this step require?
2. Have you considered character-level or subword-level approaches as an alternative to vocabulary reduction? How do they compare in terms of speed and accuracy?
3. Could the proposed techniques be applied to tasks beyond NMT, such as language modeling or text generation? If so, what challenges might arise?
In summary, while the paper offers a well-executed analysis of vocabulary reduction techniques for NMT, its limited scope, lack of methodological clarity, and misalignment with ICLR's focus lead to the decision to reject. Addressing the outlined concerns and exploring broader applications could significantly enhance the paper's impact.