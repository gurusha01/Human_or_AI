Review
Summary of Contributions
The paper proposes a simple yet intriguing optimization technique: adding annealed Gaussian noise to gradients during training. This method is tested on complex neural architectures such as End-to-End Memory Networks, Neural Programmers, Neural Random Access Machines, and Neural GPUs, which are designed for tasks like question answering and algorithm learning. The authors argue that these architectures present unique optimization challenges that are less studied compared to standard networks. The results demonstrate that gradient noise consistently improves optimization, particularly in escaping poor initializations and achieving better training loss. The method is easy to implement, requiring minimal hyperparameter tuning, and is complementary to advanced optimizers like Adam. However, the paper also highlights cases where the method is less effective, such as in language modeling tasks.
Decision: Reject
Key Reasons:
1. Lack of Rigorous Benchmarking Against Literature: While the results show improvements over baseline models without gradient noise, the paper does not compare its method to state-of-the-art results from existing literature. This omission makes it difficult to assess the broader impact and competitiveness of the proposed approach.
2. Limited Insight into Model Quality: The paper focuses on optimization improvements but does not sufficiently address whether the resulting models generalize well or are merely better optimized compared to noiseless baselines. For example, the MNIST accuracy of 92% is not competitive with simpler models like linear classifiers, raising concerns about the practical utility of the method.
Supporting Arguments
The paper makes a compelling case for the utility of gradient noise in training complex architectures, particularly in overcoming poor initialization and improving robustness across random restarts. The experiments are thorough, covering a range of architectures and tasks, and the method is shown to be easy to implement with minimal hyperparameter tuning. However, the lack of comparisons to existing methods and the limited exploration of scenarios where depth provides a clear advantage weaken the paper's overall contribution. The MNIST results, in particular, highlight a gap between optimization improvements and meaningful performance gains.
Suggestions for Improvement
1. Benchmarking Against Literature: Include comparisons with state-of-the-art methods for the tested architectures and tasks. This would provide a clearer picture of the method's competitiveness and practical utility.
2. Generalization Analysis: Investigate whether the models trained with gradient noise generalize better or simply achieve lower training loss. For example, include experiments on test set performance for tasks like MNIST and question answering.
3. Broader Applicability: Explore scenarios where depth provides a clear advantage, as the current experiments focus on architectures that are intentionally difficult to optimize. This would help demonstrate the method's relevance to real-world applications.
4. Ablation Studies: Provide more detailed ablation studies to isolate the effects of gradient noise from other factors, such as initialization and optimizer choice. This would strengthen the claims about the method's effectiveness.
Questions for the Authors
1. How does the proposed method compare to state-of-the-art results for the tested architectures and tasks, particularly in terms of generalization performance?
2. Can you provide more insight into why the method achieves only 92% accuracy on MNIST, which is not competitive with simpler models? Does this suggest limitations in the method's applicability to certain tasks?
3. Have you explored the impact of gradient noise on architectures where depth provides a clear advantage, such as ResNets or Transformers? If not, why were these omitted?
In summary, while the paper introduces a simple and potentially useful optimization technique, its lack of rigorous benchmarking and limited exploration of generalization and broader applicability prevent it from making a strong contribution to the field at this stage.