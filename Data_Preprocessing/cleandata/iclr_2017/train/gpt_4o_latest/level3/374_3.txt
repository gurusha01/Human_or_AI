Review
The paper introduces a novel fine-grained gating mechanism to dynamically combine word-level and character-level word representations based on token-specific features. This mechanism is further extended to model interactions between documents and queries in reading comprehension tasks. The proposed approach is evaluated on cloze-style reading comprehension datasets (CBT, Who Did What) and the SQuAD dataset, as well as a Twitter hashtag prediction task. The results demonstrate state-of-the-art performance on CBT and Who Did What datasets and competitive results on SQuAD, highlighting the generality and effectiveness of the method.
Decision: Accept
Key reasons for acceptance:
1. Novelty and Contribution: The paper introduces a fine-grained gating mechanism that improves upon existing methods like concatenation and scalar gating. The use of token-specific features (e.g., POS tags, NER tags, frequency) to condition the gating mechanism is innovative and well-motivated.
2. Empirical Validation: The proposed approach achieves state-of-the-art results on multiple datasets, including CBT and Who Did What, and performs comparably on SQuAD. The ablation study convincingly demonstrates the impact of individual components, reinforcing the validity of the claims.
Supporting Arguments
1. Well-Motivated Approach: The paper identifies limitations in prior methods (e.g., concatenation and scalar gating) and provides a clear rationale for the proposed fine-grained gating mechanism. The use of linguistic features to condition the gate is both intuitive and effective, as evidenced by the results.
2. Comprehensive Evaluation: The method is evaluated on diverse tasks, including reading comprehension and social media tag prediction, demonstrating its generality. The inclusion of ablation studies and comparisons with strong baselines strengthens the empirical evidence.
3. Clarity and Organization: The paper is well-written, with a thorough review of related work and clear explanations of the methodology. Visualizations and analyses (e.g., gate parameter visualization) provide additional insights into the model's behavior.
Suggestions for Improvement
1. Exploration of Syntactic Features: While the use of POS and NER tags is appreciated, exploring additional syntactic features (e.g., dependency relations) could further enhance the gating mechanism. This could be an interesting direction for future work.
2. Generality Across Tasks: While the method performs well on the evaluated tasks, its applicability to other NLP tasks (e.g., machine translation, sentiment analysis) remains unexplored. Extending the evaluation to such tasks could strengthen the claims of generality.
3. End-to-End Learning: The paper mentions the use of pre-computed linguistic features (e.g., POS, NER). Exploring an end-to-end approach where these features are learned jointly with the model could simplify the pipeline and potentially improve performance.
Questions for the Authors
1. How sensitive is the model to the choice of linguistic features (e.g., POS, NER)? Have you experimented with alternative feature sets or feature selection methods?
2. The SQuAD results show a gap compared to leaderboard scores. Could you elaborate on how the architecture could be adapted to better handle span-based answers?
3. Have you considered applying the fine-grained gating mechanism to other levels of representation, such as phrases or sentences, as mentioned in the future work section? If so, what challenges do you foresee?
In summary, the paper presents a significant contribution to the field with its innovative gating mechanism and strong empirical results. While there is room for further exploration and generalization, the current work is robust and impactful, warranting acceptance.