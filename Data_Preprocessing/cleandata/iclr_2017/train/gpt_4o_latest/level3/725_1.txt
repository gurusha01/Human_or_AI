Review of the Paper
Summary of Contributions
This paper introduces a novel intermediate stochastic model, Simplified-SFNN, to bridge deterministic deep neural networks (DNNs) and stochastic feedforward neural networks (SFNNs). The authors establish a connection between DNN → Simplified-SFNN → SFNN, enabling efficient training of stochastic models by leveraging pre-trained DNN parameters. The proposed Simplified-SFNN simplifies the stochastic nature of SFNNs, making them easier to train while preserving their regularization benefits. The paper demonstrates the effectiveness of this approach on tasks such as multi-modal learning and classification using datasets like MNIST, CIFAR-10, CIFAR-100, and SVHN. The experimental results show that Simplified-SFNN consistently outperforms baseline DNNs due to its stochastic regularization effect.
Decision: Reject
Key reasons for rejection:
1. Limited Scope of Experiments: The paper primarily focuses on small-scale tasks (e.g., MNIST) and does not provide sufficient evidence of the scalability of the proposed methods to large-scale datasets or real-world applications.
2. Lack of Theoretical Depth: While the multi-stage training methods are practical and simple to implement, the paper lacks rigorous theoretical analysis to justify the proposed approach, particularly in the context of stochastic training methods.
Supporting Arguments
1. Strengths:
   - The paper explores an interesting connection between ReLU-based DNNs and SFNNs through the Simplified-SFNN model.
   - The proposed multi-stage training method is simple and practical, making it accessible to practitioners.
   - Experimental results on MNIST and other datasets demonstrate the potential of the approach in improving regularization and multi-modal learning.
2. Weaknesses:
   - The experiments are limited to small-scale datasets, with no results reported on large-scale tasks or datasets with millions of samples. This raises concerns about the scalability and generalizability of the approach.
   - The paper does not address the computational overhead introduced by Simplified-SFNN and SFNN, especially for deeper networks.
   - There is insufficient discussion on how the proposed method relates to uncertainty representation in stochastic layers, as seen in Bayesian networks or generative models.
   - The paper misses connections to variational autoencoders (VAEs) and other stochastic training methods, which could provide a stronger theoretical foundation for the proposed approach.
Suggestions for Improvement
1. Expand Experimental Scope: Include results on larger datasets (e.g., ImageNet) to demonstrate the scalability and robustness of the proposed methods.
2. Theoretical Analysis: Provide a more rigorous theoretical justification for the multi-stage training procedure and its advantages over existing stochastic training methods.
3. Connections to Existing Literature: Discuss how the proposed Simplified-SFNN relates to uncertainty modeling in Bayesian networks and stochastic training methods in VAEs.
4. Computational Complexity: Analyze the computational cost of training Simplified-SFNN and SFNN, especially for deeper architectures, and compare it to baseline DNNs.
5. Ablation Studies: Conduct ablation studies to isolate the contributions of different components of the proposed method, such as the impact of stochastic layers and the choice of activation functions.
Questions for the Authors
1. How does the proposed Simplified-SFNN scale to larger datasets and deeper architectures? Have you tested it on datasets like ImageNet or tasks with real-world complexity?
2. Can you provide a more detailed comparison of your method with variational autoencoders and other stochastic training techniques?
3. What is the computational overhead introduced by Simplified-SFNN compared to baseline DNNs, especially for large-scale tasks?
4. How does the choice of activation functions (e.g., ReLU vs. sigmoid) affect the performance and regularization benefits of Simplified-SFNN?
While the paper presents an interesting direction for training stochastic neural networks, its limited scope and lack of theoretical rigor make it unsuitable for acceptance in its current form. Addressing the above concerns could significantly strengthen the contribution.