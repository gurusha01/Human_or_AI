Review of the Paper
Summary of Contributions
The paper introduces a Joint Many-Task (JMT) model that integrates multiple NLP tasks—POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment—into a single end-to-end trainable deep learning framework. The authors propose a hierarchical task ordering based on linguistic complexity, with each task being handled at successively deeper layers of a multi-layer bi-LSTM. The model incorporates shortcut connections, label embeddings, and a novel successive regularization strategy to mitigate catastrophic interference between tasks. The JMT model achieves state-of-the-art results on four tasks (chunking, dependency parsing, semantic relatedness, and textual entailment) and competitive performance on POS tagging, demonstrating the benefits of joint learning across tasks. The paper also provides extensive analysis of architectural choices, including the importance of shortcut connections, label embeddings, and character n-gram embeddings.
Decision: Reject
While the paper presents an innovative approach to multi-task learning and achieves strong empirical results, there are critical issues in the evaluation and methodology that undermine the validity of its claims. Specifically:
1. The evaluation lacks a simple multi-task baseline without task hierarchy, making it unclear whether the performance gains are due to the proposed hierarchical setup or simply the multi-task learning itself.
2. The inclusion of the chunking test set in the dependency parsing training data compromises the validity of the chunking results, as the model may have indirectly seen the test data during training.
3. The dependency parsing model does not ensure well-formed dependency trees, which raises concerns about the fairness of the results in Table 4.
Supporting Arguments
1. Evaluation Gaps: The absence of a baseline multi-task model without task hierarchy is a major oversight. Without this comparison, the paper does not convincingly demonstrate that the hierarchical ordering of tasks is crucial to the observed performance improvements. This undermines the central hypothesis of the paper.
2. Data Contamination: The overlap between the chunking test set and the dependency parsing training data renders the chunking results with the "JMT_all" model uninformative. This issue significantly weakens the claim of state-of-the-art performance on chunking.
3. Dependency Tree Validity: The lack of a mechanism to ensure well-formed dependency trees (e.g., single-rooted, acyclic structures) means the reported results for dependency parsing may not be directly comparable to prior work. This is a fundamental flaw in the evaluation of this task.
Suggestions for Improvement
1. Add a Multi-Task Baseline: Include a baseline model that performs multi-task learning without hierarchical task ordering to isolate the contribution of the proposed hierarchy.
2. Resolve Data Contamination: Ensure that the chunking test set is excluded from the dependency parsing training data. If this is not feasible, clearly disclose the contamination and provide adjusted results.
3. Well-Formed Dependency Trees: Modify the dependency parsing layer to guarantee well-formed trees, or explicitly analyze the impact of this limitation on the reported results.
4. Clarify Chunking Task Definition: The paper refers to chunking as a word-level task, but this is misleading. Chunking is a structured prediction task that involves identifying phrase spans, not just tagging individual words. This should be corrected for clarity.
Questions for the Authors
1. How does the performance of the JMT model compare to a simple multi-task baseline without task hierarchy? Can you provide results for such a baseline?
2. What is the extent of the overlap between the chunking test set and the dependency parsing training data? How does this overlap affect the reported results?
3. How often does the dependency parsing model produce invalid trees (e.g., multiple roots or cycles), and how do these errors impact downstream tasks?
4. Have you considered incorporating explicit mechanisms for handling antonyms and synonyms (e.g., external lexical resources) to address the semantic limitations observed in the entailment task?
In summary, while the paper presents a promising approach to joint multi-task learning, the evaluation shortcomings and methodological issues must be addressed before the work can be considered for acceptance.