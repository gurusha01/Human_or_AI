Review of the Paper
Summary of Contributions
This paper introduces a novel "density-diversity penalty" regularizer aimed at compressing fully-connected neural networks by promoting both high sparsity and low diversity in weight matrices. The proposed method simultaneously reduces the number of parameters and distinct weight values, enabling significant compression without substantial loss in accuracy. A key innovation is the "sorting trick," which efficiently computes gradients for the penalty, reducing computational overhead. The authors demonstrate the effectiveness of their approach on MNIST and TIMIT datasets, achieving compression rates of up to 200x for fully-connected layers while maintaining comparable performance to uncompressed models. The method is presented as a unified alternative to multi-stage compression pipelines like "deep compression," offering simplicity and competitive results.
Decision: Reject
While the paper presents an interesting and potentially impactful idea, it suffers from several critical shortcomings that prevent acceptance in its current form. The primary reasons for rejection are the lack of independent evaluation of the method's components and insufficient empirical analysis to support its claims.
Supporting Arguments for Decision
1. Component Analysis: The paper does not independently evaluate the contributions of its key componentsâ€”sparse initialization, weight tying, and penalty scheduling. This makes it difficult to assess the importance of each component and whether the proposed method's success is due to the penalty itself or the interplay of these elements.
   
2. Compression Metric Ambiguity: The compression metric combines sparsity and diversity, but the individual contributions of these factors to the overall compression rate are unclear. This lack of granularity limits the interpretability of the results.
3. Scalability Concerns: While the sorting trick reduces computational overhead, the probabilistic application of the penalty (1-5% per mini-batch) raises questions about scalability to larger models and datasets. The paper does not provide sufficient analysis of this trade-off.
4. Empirical Evaluation: The empirical results are limited to two datasets (MNIST and TIMIT), and the experiments lack diversity in model architectures and tasks. This restricts the generalizability of the findings.
Suggestions for Improvement
1. Component Ablation Study: Conduct experiments to isolate the effects of sparse initialization, weight tying, and penalty scheduling. This would clarify the individual contributions of these components to the overall method.
2. Metric Decomposition: Provide separate evaluations of sparsity and diversity to better understand their respective roles in compression and performance retention.
3. Scalability Analysis: Include experiments on larger datasets and models to demonstrate the method's scalability. Address the computational cost of weight sorting and probabilistic penalty application in more detail.
4. Clarify Probabilistic Penalty: The alternating training phases and probabilistic application of the penalty require further explanation. How does the penalty probability impact convergence and final performance?
5. Broader Empirical Validation: Test the method on additional datasets and tasks, such as natural language processing or larger-scale vision datasets, to establish its robustness and versatility.
6. Minor Issue: Resize Equation 4 to fit within the margins using appropriate LaTeX commands for better readability.
Questions for the Authors
1. How does the sparsity initialization interact with the density-diversity penalty during training? Would the penalty still work effectively without this initialization?
2. What is the computational overhead of the sorting trick for larger models, and how does it compare to other compression methods like "deep compression"?
3. Can the method be extended to convolutional layers or recurrent architectures, as suggested in the discussion? If so, what challenges do you anticipate?
In conclusion, the paper introduces a promising approach to neural network compression, but it requires more rigorous analysis and broader validation to substantiate its claims and demonstrate its general applicability.