Review
Summary of Contributions
This paper introduces the Stick-Breaking Variational Autoencoder (SB-VAE), which extends Stochastic Gradient Variational Bayes (SGVB) to perform posterior inference for the weights of Stick-Breaking processes. The authors propose using the Kumaraswamy distribution as an approximate posterior, enabling a differentiable non-centered parameterization (DNCP) for stick-breaking weights. This allows the SB-VAE to have a latent representation with stochastic dimensionality, making it a Bayesian nonparametric alternative to traditional VAEs. The paper also extends this approach to semi-supervised learning, demonstrating that the SB-VAE outperforms Gaussian VAEs in semi-supervised settings, particularly in preserving class structure in latent representations. The authors should be commended for their scientific rigor in reporting that stick-breaking priors do not outperform Gaussian priors in unsupervised density estimation tasks. The writing quality is excellent, with clear explanations of the technical background, methodology, and experimental results.
Decision: Accept
The paper makes a novel and significant contribution by being the first to explore stick-breaking priors in VAEs, providing a well-motivated approach that is scientifically rigorous and supported by experiments. The semi-supervised results are particularly compelling, and the proposed methodology opens up new avenues for integrating Bayesian nonparametrics with deep generative models. While there are minor issues, they do not detract from the overall quality and impact of the work.
Supporting Arguments
1. Novelty and Motivation: The paper addresses a clear gap in the literature by extending SGVB to stick-breaking processes, which are underexplored in the context of VAEs. The use of the Kumaraswamy distribution is an elegant solution to the challenges of differentiable sampling.
2. Scientific Rigor: The authors report both positive and negative results, demonstrating a commitment to transparency. The semi-supervised experiments convincingly show the advantages of stick-breaking priors in learning discriminative latent representations.
3. Writing Quality: The paper is exceptionally well-written, with clear explanations of the background, methodology, and experimental results. The last paragraph of Section 4 is particularly well-articulated, summarizing the contributions effectively.
Suggestions for Improvement
1. Acknowledgment of Related Work: Section 2.1 should acknowledge prior work on non-Gaussian priors, such as DRAW, generative ResNets, and Ladder VAEs, to better situate the contributions in the broader literature.
2. Clarity and Grammar: In Section 2.2, clarify that "v's are sampled" refers to the posterior, not the prior. Additionally, address the minor grammatical issue with two commas in this section.
3. Equation Reference: Equation 6 should include a reference to the appendix for the closed-form KL divergence, improving accessibility for readers.
4. Terminology Update: In Section 7.1, update "Density estimation" to include "mass estimation" for conceptual accuracy.
5. Experimental Details: The use of 100 importance sampling (IS) samples in Section 7.1 is relatively low. Increasing this number could provide more robust likelihood estimates.
6. Figure 3(f): Highlight the surprising effectiveness of k-NN on raw pixel data more explicitly, as this is an interesting and unexpected result.
Questions for the Authors
1. Could you elaborate on why the SB-VAE learns at a slower pace in unsupervised settings? Are there specific architectural or optimization adjustments that could mitigate this?
2. In the semi-supervised experiments, did you observe any patterns in how the SB-VAE utilizes its stochastic dimensionality across different datasets or levels of supervision?
3. Have you considered extending the SB-VAE to other types of nonparametric priors, such as the Pitman-Yor process, and what challenges might arise in doing so?
Overall, this paper represents a significant advancement in the field of deep generative models and Bayesian nonparametrics, and I recommend its acceptance.