Review
Summary of Contributions
This paper explores the use of unsupervised learning for next-frame video prediction to enhance the generalization of a supervised model for tower stability prediction. The authors propose a two-stage architecture: a generative model predicts future video frames, and a supervised model uses these predictions to classify tower stability. The paper demonstrates that incorporating predicted frames improves generalization to unseen tower configurations, particularly when the number of blocks differs between training and testing. The authors also introduce a new dataset with video sequences of collapsing towers, extending prior datasets that only included static images.
Decision: Reject
The primary reasons for rejection are the limited novelty of the approach and insufficient experimental rigor. While the paper attempts to address an interesting problem, it does not adequately distinguish itself from prior work in unsupervised video feature learning. Additionally, the experimental design fails to convincingly demonstrate the utility of the proposed method over simpler or alternative approaches.
Supporting Arguments
1. Limited Novelty: The use of predictive models for pretraining or feature extraction in video tasks is well-established. Previous works, such as Srivastava et al. (2015) and Vondrick et al. (2016), have explored similar ideas in video and text domains. The paper does not sufficiently differentiate its approach or provide significant advancements over these prior methods.
   
2. Weak Motivation for the End Task: Predicting tower stability based on motion is a task that can often be trivially inferred from the data. The paper does not convincingly argue why this task necessitates unsupervised learning or why the proposed approach is preferable to simpler methods, such as directly using still images or treating motion as privileged information.
3. Lack of Comparative Baselines: The paper does not compare its approach to alternative semi-supervised or unsupervised strategies, such as treating next frames as latent variables or using other pretraining techniques. This omission makes it difficult to assess the relative effectiveness of the proposed method.
4. Experimental Weaknesses: The experiments focus on a task where solving the unsupervised problem (predicting frames) may directly solve the supervised task (predicting motion). This trivial overlap undermines the claim that the method improves generalization. Furthermore, the lack of results on the Lerer et al. (2016) dataset limits the paper's relevance and comparability to prior work.
5. Incomplete Related Work: The related work section omits key references, such as Fathi et al. (2008), Mobahi et al. (2009), and Wang & Gupta (2015), which have explored unsupervised video features for classification tasks. This omission weakens the paper's contextualization within the broader literature.
Suggestions for Improvement
1. Clarify Novelty: Clearly articulate how the proposed method advances beyond existing work in unsupervised video feature learning. Highlight unique contributions or insights that differentiate this work.
   
2. Stronger End Task Justification: Choose an end task where the unsupervised component provides non-trivial benefits. For example, tasks where motion prediction does not directly solve the supervised task would better demonstrate the utility of the approach.
3. Comparative Baselines: Include comparisons with other semi-supervised or unsupervised methods, such as treating next frames as latent variables or using alternative pretraining strategies.
4. Expanded Related Work: Discuss prior research that has used unsupervised video features for classification tasks, including the omitted works mentioned above.
5. Dataset and Task Relevance: Report results on the Lerer et al. (2016) dataset to facilitate comparison with prior work. Additionally, consider tasks where still images are insufficient, making motion prediction genuinely necessary.
Questions for the Authors
1. How does the proposed method compare to alternative semi-supervised strategies, such as treating next frames as latent variables or using them as privileged information?
2. Why was the Lerer et al. (2016) dataset not used for evaluation? Can the authors provide results on this dataset for better comparability?
3. Could the supervised stability prediction model be unnecessary if motion can be directly inferred from the predicted frames? How would the method perform without the supervised component?
In conclusion, while the paper addresses an interesting problem, its limited novelty, weak experimental design, and insufficient comparisons with prior work prevent it from making a significant contribution to the field.