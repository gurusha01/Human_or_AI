Review
Summary of Contributions
The paper investigates the Hessian eigenspectrum of loss functions in neural networks to analyze optimization difficulty and local convexity. It provides empirical evidence that the eigenvalue distribution consists of two components: a bulk concentrated around zero (dependent on network architecture) and isolated edges (dependent on data). The authors highlight the degeneracy of the Hessian, suggesting implications for optimization landscapes, such as local flatness and the potential for low-energy paths between solutions. The paper also explores the effects of training and data complexity on the Hessian spectrum, offering insights into the relationship between data distribution and eigenvalue concentration. These findings could inform future work on optimization methods and theoretical frameworks for deep learning.
Decision: Reject  
Key Reasons:
1. Lack of Generalization and Rigor: The study is limited to specific settings without sufficient control experiments or mathematical analysis to validate the claims. This undermines the generalizability of the findings.
2. Presentation Issues: Several figures (e.g., Figures 1, 2, 3, and 6) are unreadable or inconsistent in printed format, which significantly hinders the interpretability of the results.
Supporting Arguments
1. Limited Scope and Missing Analysis: While the paper provides interesting empirical observations, it lacks rigorous theoretical backing or broader experimental validation. For instance, the relationship between eigenvalue concentration and initial parameter scaling (Figure 6) remains ambiguous, and no control experiments are provided to isolate the effects of training. Similarly, the notion of data complexity is loosely defined, making it difficult to interpret results related to overlapping distributions (Figure 8).
2. Presentation Issues: The poor rendering of key figures, including the Hessian matrix in Figure 1, detracts from the clarity of the results. Unreadable plots and inconsistent labels make it challenging to assess the validity of the claims. This is a critical issue for a paper that relies heavily on visual evidence.
3. Unclear Practical Implications: While the paper hints at potential applications, such as leveraging flat regions in optimization, these ideas are not explored in depth. The lack of concrete experiments or methodologies to demonstrate these implications weakens the impact of the work.
Suggestions for Improvement
1. Expand Experimental Scope: Include control experiments to isolate the effects of training, initialization, and data distribution on the Hessian spectrum. Provide mathematical analysis to support empirical observations.
2. Clarify Data Complexity: Define data complexity more rigorously and explore its relationship with the Hessian spectrum in a broader range of scenarios.
3. Improve Presentation: Ensure all figures are legible and consistent in both digital and printed formats. Provide detailed captions and labels to enhance interpretability.
4. Strengthen Practical Contributions: Explore concrete applications of the findings, such as optimization methods tailored to the observed Hessian properties.
Questions for the Authors
1. How do you account for the ambiguity in Figure 6 regarding the role of training versus initial parameter scaling in eigenvalue concentration?
2. Could you provide a more rigorous definition of data complexity and explain how it influences the Hessian spectrum?
3. Are the observed trends in the Hessian spectrum consistent across different architectures and datasets? If not, what factors might influence these variations?
4. How do you propose to address the practical challenges of leveraging the observed flatness in optimization methods?
While the paper offers intriguing insights, addressing these limitations is crucial to enhance its scientific rigor and practical relevance.