Review of the Paper
Summary of Contributions
This paper investigates the generation of adversarial examples for deep generative models, specifically Variational Autoencoders (VAEs) and VAE-GANs. While adversarial attacks have been extensively studied in the context of classifiers, this work extends the discussion to generative models, which is a novel and valuable direction. The authors propose three attack methods—classifier-based, VAE loss-based (LVAE), and latent space attacks—and evaluate their effectiveness on datasets like MNIST, SVHN, and CelebA. The paper also motivates potential real-world attack scenarios, such as compromising latent-space-based compression systems. The results demonstrate that generative models are indeed susceptible to adversarial attacks, albeit with varying levels of difficulty. This work contributes to the understanding of adversarial robustness in generative models and lays the groundwork for future defenses.
Decision: Reject  
The primary reasons for rejection are the lack of clarity in motivation and framing, and the paper's significant shortcomings in presentation and rigor. While the topic is intriguing, the paper does not sufficiently support its claims and suffers from structural and methodological issues.
Supporting Arguments for Decision
1. Validity of Claims: The claim that adversarial attacks reveal "telltale noise" in generative models is not convincingly demonstrated. The presented adversarial examples appear noisier than prior work, which raises questions about whether the attacks are genuinely effective or if the models are inherently more robust to adversarial perturbations. The paper does not adequately explore or explain this discrepancy.
   
2. Motivation and Scope: The paper's motivation is not clearly articulated in the introduction. While the idea of attacking generative models is novel, the practical implications of such attacks are not convincingly argued. Additionally, the title's focus on "generative models" is misleading, as the work primarily targets autoencoder-based models, excluding broader classes of generative models like diffusion models or transformers.
3. Scientific Rigor: The evaluation metrics (e.g., ASignore-target and AStarget) are not well-justified, and the results lack statistical rigor. The experiments are limited to specific datasets (MNIST, SVHN, CelebA), which may not generalize to more complex datasets like ImageNet. Furthermore, the paper does not systematically compare its attacks to existing methods in terms of computational efficiency or perceptual quality.
4. Presentation Issues: The paper exceeds the page limit (13 pages), and the introduction contains excessive background information that could be condensed. The writing is verbose, and key points are buried in lengthy descriptions, making the paper difficult to follow. Additionally, the figures and tables, while informative, are not well-integrated into the narrative.
Suggestions for Improvement
1. Clarify Motivation and Scope: Clearly articulate why attacking generative models is important and what practical scenarios these attacks could impact. Broaden the discussion to include other types of generative models or justify the focus on VAEs and VAE-GANs.
2. Improve Experimental Rigor: Provide a more thorough analysis of the results, including statistical significance and comparisons to baseline methods. Address why the adversarial examples appear noisier than in prior work and discuss the implications for model robustness.
3. Streamline Presentation: Reduce the length of the introduction and background sections, focusing on the novel contributions. Ensure that the title accurately reflects the scope of the work. Adhere to the page limit and improve the clarity of the figures and tables.
4. Address Methodological Gaps: Explore why certain attack methods (e.g., LVAE) are slower or less effective and provide insights into how these limitations could be mitigated. Discuss the trade-offs between attack success and perceptual quality.
Questions for the Authors
1. Why do the adversarial examples generated for generative models appear noisier than those for classifiers? Does this suggest that generative models are inherently more robust, or are the attacks less optimized?
2. How do the proposed attacks generalize to other generative architectures, such as GANs or diffusion models, beyond VAEs and VAE-GANs?
3. Could you provide a clearer justification for the practical relevance of attacking generative models? For example, are there real-world systems currently vulnerable to such attacks?
This paper addresses an important and underexplored topic, but significant revisions are needed to improve its clarity, rigor, and impact.