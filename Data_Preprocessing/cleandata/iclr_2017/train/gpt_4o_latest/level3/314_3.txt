Review of the Paper
This paper introduces an innovative on-policy method for predicting future intrinsic measurements in the vizDoom game environment, enabling guided exploration and goal-directed behavior. The model predicts sequences of key measurements—health, ammunition, and frags—weighted by goal triplets, which allows it to dynamically adapt to different objectives. At test time, the agent acts by maximizing long-term goals, showcasing a novel supervised-learning-inspired approach to reinforcement learning (RL). The method demonstrates significant empirical success, outperforming state-of-the-art RL baselines such as A3C, DQN, and DSR. Notably, the model's efficacy is validated by its victory in the 2016 Visual Doom AI Competition.
Decision: Accept
The decision to accept this paper is based on two primary reasons: (1) the introduction of a novel and well-motivated approach that bridges supervised learning and RL, and (2) strong empirical evidence supporting the model's superiority over existing methods. The paper's sound methodology, reproducibility, and milestone achievement in the Doom AI competition further strengthen its case for acceptance.
Supporting Arguments
1. Novelty and Motivation: The paper departs from traditional RL paradigms by leveraging dense, vector-valued feedback instead of sparse scalar rewards. This approach is well-motivated and grounded in prior work on intrinsic motivation, auxiliary variables, and forward modeling. The authors clearly articulate the advantages of their method, such as stabilizing training and enabling goal-agnostic learning.
2. Empirical Rigor: The model's performance is thoroughly evaluated across multiple challenging scenarios in the vizDoom environment, demonstrating clear advantages over strong baselines. The ablation study convincingly shows the importance of the model's added complexity, such as multivariate feedback and multi-temporal predictions.
3. Reproducibility and Impact: The detailed experimental setup and use of publicly available benchmarks (vizDoom) ensure reproducibility. Winning the Doom AI competition highlights the practical impact and robustness of the approach.
Suggestions for Improvement
1. Clarification of Notation: Some of the notation in the equations is unclear or inconsistent. For example, the use of vectors and their dimensionalities could be better explained to avoid confusion.
2. Inaccuracies in Descriptions: The paper inaccurately describes Doom as a "3D" environment, which could mislead readers. Clarifying that Doom is a pseudo-3D environment would improve accuracy.
3. Results Presentation: The results tables use inconsistent units, which makes comparisons harder. Standardizing units and providing more detailed statistical analyses (e.g., confidence intervals) would enhance the presentation.
4. Memory and Temporal Abstraction: While the paper acknowledges the lack of memory and temporal abstraction as limitations, a brief discussion of potential solutions (e.g., integrating recurrent architectures) would be valuable.
Questions for the Authors
1. How sensitive is the model's performance to the choice of temporal offsets (e.g., τ1, τ2, etc.)? Could this choice limit generalizability to other environments?
2. Can the proposed method be extended to continuous action spaces? If so, what challenges might arise, and how could they be addressed?
3. How does the model handle scenarios with conflicting goals (e.g., maximizing health while minimizing frags)? Are there any observed trade-offs in such cases?
Overall, this paper represents a significant contribution to the field of reinforcement learning and sensorimotor control. Its novel approach, strong empirical results, and practical success make it a valuable addition to the conference.