Review
Summary of Contributions
This paper provides a novel analysis of continuous-time gradient descent dynamics for two-layer ReLU networks, focusing on the teacher-student setting. It avoids common but unrealistic assumptions such as input-activation independence or noise injection, making the results more applicable to practical scenarios. The key contribution is the reduction of high-dimensional gradient descent dynamics to a bivariate system under symmetry assumptions, enabling rigorous analysis of convergence behaviors. The authors prove that for networks with orthonormal teacher parameters, symmetric initialization leads to saddle points, while symmetry-breaking initialization ensures global convergence to the teacher parameters. The work also verifies its theoretical findings through simulations, which align well with the derived dynamics. This is a significant step forward in understanding nonlinear neural network optimization without relying on oversimplified assumptions.
Decision: Accept
The paper is recommended for acceptance due to its originality, rigorous theoretical contributions, and practical relevance. The reduction of high-dimensional dynamics to a bivariate system is particularly noteworthy, as it simplifies the analysis while retaining critical nonlinear behaviors. Additionally, the avoidance of unrealistic assumptions enhances the applicability of the results to real-world scenarios.
Supporting Arguments
1. Problem Tackled: The paper addresses the challenging problem of analyzing the nonlinear dynamics of gradient descent in two-layer ReLU networks, a topic of significant theoretical and practical importance. By avoiding assumptions like input-activation independence, the analysis is more realistic and relevant.
   
2. Motivation and Placement in Literature: The work is well-motivated and positioned within the existing literature. It builds on prior studies of linear networks and extends them to nonlinear ReLU networks, filling a critical gap. The authors also compare their results with related works, such as those using spin-glass models or noise injection, highlighting the novelty of their approach.
3. Scientific Rigor: The theoretical claims are supported by detailed proofs and simulations. The reduction to a bivariate system is mathematically sound, and the convergence results are derived rigorously. The simulations further validate the theoretical findings, demonstrating consistency between theory and practice.
Suggestions for Improvement
1. Clarification on Initialization: The inconsistency in initialization mentioned on page 2 requires clarification. A more detailed explanation of how the initialization aligns with practical settings would strengthen the paper.
   
2. Typographical Errors: Minor issues such as the typo in Section 1 ("zero-mean" Gaussian clarification), repeated misspelling of "standard deviation," and a citation error on page 6 should be corrected.
3. Broader Applicability: While the focus on orthonormal teacher parameters is justified, discussing how the results might generalize to non-orthonormal cases would enhance the paper's impact.
4. Empirical Validation: Although the simulations are consistent with the theory, additional experiments with real-world datasets or more complex architectures could further demonstrate the practical relevance of the findings.
Questions for the Authors
1. Can you provide more details on how the initialization strategy aligns with common practices in deep learning, beyond the theoretical guarantees?
2. How sensitive are the results to deviations from the assumption of orthonormal teacher parameters? Could the analysis be extended to handle non-orthonormal cases?
3. Have you considered the impact of noise or other perturbations on the convergence dynamics? If so, how robust are the findings under such conditions?
In conclusion, this paper makes a significant theoretical contribution to understanding gradient descent dynamics in nonlinear neural networks. With minor revisions and clarifications, it will be a valuable addition to the conference.