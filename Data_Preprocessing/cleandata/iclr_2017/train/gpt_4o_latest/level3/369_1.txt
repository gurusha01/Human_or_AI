Review of "Trained Ternary Quantization (TTQ)" Paper
Summary
The paper proposes a novel method, Trained Ternary Quantization (TTQ), to reduce the precision of neural network weights to ternary values while maintaining or even improving model accuracy. The authors introduce trainable scaling coefficients for positive and negative weights, which are optimized during backpropagation. This approach achieves a 16Ã— reduction in model size and demonstrates competitive or superior accuracy compared to full-precision models on CIFAR-10 and ImageNet datasets. The method is particularly relevant for deploying deep neural networks on resource-constrained devices, such as mobile phones, by improving energy efficiency and reducing memory requirements.
Decision: Reject  
While the paper presents an incremental improvement in ternary quantization methods, it lacks significant novelty and broader applicability. The following reasons support this decision:
1. Incremental Contribution: The proposed method builds on existing ternary quantization techniques, such as Ternary Weight Networks (TWN), by introducing trainable scaling coefficients. While this is a logical extension, it does not represent a groundbreaking innovation in the field.
2. Narrow Audience: The paper assumes prior knowledge of ternary quantization, limiting its accessibility to a niche audience. It does not sufficiently contextualize its contributions for the broader deep learning community.
3. Lack of Practical Motivation: Although the paper discusses potential applications for mobile and edge devices, it does not provide concrete real-world use cases or demonstrate practical deployment scenarios.
4. Absence of Code Release: The lack of publicly available code hinders reproducibility and limits the immediate impact of the work.
Supporting Arguments
The strengths of the paper lie in its empirical results, which show performance improvements over prior ternary quantization methods. The method achieves higher accuracy on CIFAR-10 and ImageNet datasets compared to TWN and even outperforms full-precision models in some cases. This aligns with the current trend of learning quantization parameters rather than relying on predefined heuristics, which is a positive step forward.
However, the paper does not sufficiently differentiate itself from existing methods. The introduction of trainable scaling coefficients, while effective, is a relatively minor modification to the existing framework of ternary quantization. Furthermore, the lack of a broader discussion on the implications of this work for diverse machine learning applications limits its relevance for a general audience.
Suggestions for Improvement
1. Highlight Broader Applications: The authors should provide concrete examples of real-world scenarios where TTQ could be deployed, such as in autonomous vehicles, IoT devices, or other edge-computing environments.
2. Improve Accessibility: Include a more detailed introduction to ternary quantization for readers unfamiliar with the topic. This would make the paper more accessible to a wider audience.
3. Release Code: Sharing the implementation would enhance reproducibility and encourage adoption by the community.
4. Compare with More Baselines: The paper could benefit from comparisons with a broader range of quantization techniques, including more recent methods beyond TWN and DoReFa-Net.
5. Analyze Hardware Implications: The authors should provide a more detailed analysis of the hardware acceleration potential of TTQ, including simulations or experiments on custom circuits.
Questions for the Authors
1. How does the proposed method generalize to other architectures beyond ResNet and AlexNet? Have you tested TTQ on transformer-based models or other modern architectures?
2. Can you provide more insights into the computational overhead during training due to the introduction of trainable scaling coefficients? How does this compare to other quantization methods?
3. Are there plans to release the code for this work? If not, what are the barriers to doing so?
In summary, while the paper demonstrates promising results, its incremental contribution, narrow scope, and lack of practical motivation make it unsuitable for acceptance at a diverse conference like ICLR. Addressing the above concerns could significantly strengthen the work.