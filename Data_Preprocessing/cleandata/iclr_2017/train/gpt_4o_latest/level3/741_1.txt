Review of the Paper
Summary of Contributions
This paper explores the use of convolutional neural networks (CNNs) to learn game concepts in tic-tac-toe from visual board renderings under weak supervision. It employs Class Activation Mapping (CAM) to visualize the network's attention and claims that the CNN implicitly learns game rules, such as the winning rule, the concept of two sides, and the chessboard grid. The authors also investigate the applicability of cross-modal supervision for learning higher-level semantics and argue that CAM can activate in non-salient regions, showcasing the hierarchical information-gathering ability of CNNs. The study includes experiments with variations in board and piece appearances, as well as a temporal analysis of the model's learning behavior.
Decision: Reject
The paper presents an interesting exploration of CNNs for learning game concepts, but it falls short in several critical areas. The primary reasons for rejection are: (1) the claims made by the authors are not sufficiently substantiated by rigorous experimental evidence, and (2) the paper lacks clarity in defining key concepts and situating its contributions within the existing literature.
Supporting Arguments
1. Unsubstantiated Claims: The authors claim that the CNN has implicitly learned game rules, but this is not convincingly demonstrated. The experiments do not test generalization to unseen states, which is a standard way to validate whether a model has truly learned underlying rules rather than memorizing patterns. Without such evidence, the claim remains speculative.
2. Lack of Clarity in Cross-Modal Supervision: The definition of cross-modal supervision in this context is unclear. The authors do not adequately differentiate their approach from prior works, such as DQN and DDPG, which have already demonstrated mappings from images to actions. The novelty of their contribution is therefore questionable.
3. Experimental Design Issues: The distinction between "what will happen" and "what to do" is not clearly addressed in the experimental setup. Both tasks appear identical in the context of tic-tac-toe, making it difficult to assess the significance of the proposed methodology.
4. Insufficient Analysis: While CAM visualizations are provided, the authors do not analyze CAM for incorrect predictions, which would offer insights into the model's failure modes. Additionally, the absence of a train/validation split raises concerns about overfitting and the generalizability of the learned representations.
5. Overstated Contributions: The claim that CAM can activate in non-salient regions is not novel or surprising, given that CNNs inherently gather information from receptive fields. The paper does not provide compelling evidence to support this as a significant contribution.
Suggestions for Improvement
1. Generalization Testing: Include experiments that test the model's ability to generalize to unseen board states or configurations. This would provide stronger evidence for the claim that the CNN has learned game rules.
2. Clarify Cross-Modal Supervision: Clearly define how the proposed approach differs from existing methods and articulate its novelty. Provide a more thorough comparison with related works.
3. Improve Experimental Design: Clearly distinguish between "what will happen" and "what to do" in the experimental setup. Use separate tasks or datasets to evaluate these concepts.
4. Analyze CAM for Incorrect Predictions: Visualize and analyze CAM results for misclassified samples to better understand the model's limitations and biases.
5. Train/Validation Split: Incorporate a train/validation split to evaluate the generalizability of the learned representations and avoid overfitting.
6. Quantitative Metrics: Expand the quantitative evaluation to include metrics such as generalization accuracy on unseen states or robustness to noise in the input data.
Questions for the Authors
1. How does the model perform on unseen board states or configurations? Can it generalize beyond the training set?
2. How does your definition of cross-modal supervision differ from existing works like DQN or DDPG? What is the specific novelty in your approach?
3. Can you provide CAM visualizations for incorrect predictions? What do these reveal about the model's understanding of the game?
4. How do you ensure that the network is not simply memorizing patterns in the training data, given the lack of a train/validation split?
In conclusion, while the paper addresses an interesting problem, it lacks the rigor and clarity required to substantiate its claims and distinguish its contributions from prior work. Further analysis and experimental validation are necessary to strengthen the paper.