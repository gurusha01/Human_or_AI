Review of the Paper
Summary
The paper addresses the critical problem of detecting misclassified and out-of-distribution (OOD) examples in machine learning systems. It introduces a simple yet effective baseline that leverages the statistics of softmax outputs, contrasting this with the direct use of softmax probabilities, which are often misleading as confidence estimates. The authors evaluate their method across diverse domains, including computer vision, natural language processing, and automatic speech recognition, demonstrating its utility and limitations. Additionally, the paper proposes an auxiliary decoder-based abnormality detection module, which outperforms the baseline in certain cases, highlighting room for further research. The authors also provide standardized tasks and metrics for future work in this area, making a valuable contribution to the field.
Decision: Accept
The paper makes a meaningful contribution to the field of error and OOD detection by establishing a strong baseline, proposing an improved method, and outlining future research directions. The work is well-motivated, scientifically rigorous, and has broad applicability across multiple domains. However, some areas require further clarification and elaboration, as detailed below.
Supporting Arguments
1. Problem Significance and Motivation: The problem of detecting misclassifications and OOD examples is crucial for AI safety and reliability, especially in high-stakes applications like healthcare. The paper is well-placed in the literature, referencing foundational works and highlighting the limitations of existing approaches.
2. Scientific Rigor: The authors employ robust evaluation metrics (AUROC, AUPR) and test their methods on a wide range of datasets and tasks. The results consistently demonstrate the effectiveness of the proposed baseline and the auxiliary decoder module.
3. Contributions: The paper not only provides a strong baseline but also introduces a novel auxiliary decoder-based method, showing its potential to surpass the baseline in challenging scenarios. The standardization of tasks and metrics is a valuable resource for the community.
Additional Feedback for Improvement
1. Elaboration on Ignoring the Blank Symbol: The paper briefly mentions ignoring the logit of the blank symbol in the speech recognition task but does not provide sufficient explanation or justification. A more detailed discussion of this choice and its impact on the results would strengthen the paper.
2. Evaluation in Confusable Settings: While the paper demonstrates the effectiveness of the proposed methods, it would be interesting to evaluate their performance in more confusable settings, such as when in-domain and OOD examples are highly similar. This would provide a more comprehensive assessment of the methods' robustness.
3. Auxiliary Decoder Setup: Section 4 lacks sufficient detail about the auxiliary decoder's architecture and training process. Providing more specifics would enhance reproducibility and clarity.
4. Related Work: While the paper references recent work on performance monitoring and accuracy prediction, a more detailed comparison with these methods would contextualize the contributions better.
Questions for the Authors
1. Could you provide more details on the rationale and implementation of ignoring the blank symbol's logit in the speech recognition task? How does this affect the detection performance?
2. Have you considered testing the proposed methods in scenarios where in-domain and OOD examples are intentionally made more confusable? If so, what were the results?
3. Can you elaborate on the auxiliary decoder's architecture and training process? How sensitive is the abnormality module to the choice of noise types used during training?
In conclusion, the paper makes a significant contribution to the field of error and OOD detection. Addressing the above points would further strengthen its impact and clarity.