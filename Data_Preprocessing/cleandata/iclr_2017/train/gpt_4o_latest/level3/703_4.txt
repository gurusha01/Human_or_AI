Review of "Tartan TRT: A Hardware Accelerator for Inference with Deep Neural Networks"
This paper introduces Tartan (TRT), a hardware accelerator designed to improve the execution time and energy efficiency of Deep Neural Networks (DNNs) by exploiting layer-specific precision requirements. The authors claim that TRT achieves significant speedups and energy efficiency improvements over state-of-the-art accelerators like DaDianNao (DaDN) without sacrificing accuracy, while also enabling trade-offs between accuracy and performance. The paper provides detailed architectural descriptions, experimental results, and comparisons with prior work.
Decision: Reject  
Key Reasons:  
1. Limited Reviewer Expertise: I lack sufficient expertise in hardware accelerator design and DNN-specific hardware optimizations to confidently evaluate the technical rigor and novelty of this work.  
2. Venue Mismatch: While the work appears valuable, it may be better suited for a hardware-focused conference or journal rather than this venue, which may prioritize broader AI and machine learning contributions.
Supporting Arguments:  
1. Problem Tackled: The paper addresses the challenge of improving inference performance and energy efficiency in DNNs by leveraging variable precision requirements across layers. This is a relevant and timely problem given the increasing computational demands of DNNs.  
2. Motivation and Literature Placement: The authors provide a thorough review of related work, situating TRT within the context of existing accelerators like DaDN, Stripes, and EIE. The motivation for supporting precision scaling and addressing underutilization in fully connected layers is well-articulated.  
3. Claims and Evidence: The experimental results demonstrate that TRT achieves up to 1.90× speedup and 1.17× energy efficiency improvements over DaDN. However, the paper's technical depth and hardware-specific focus make it challenging for me to assess the validity of the claims or the rigor of the methodology.
Additional Feedback:  
1. Clarity for Broader Audience: While the paper is well-written for a hardware-specialist audience, it could benefit from a higher-level summary of key ideas and results for readers less familiar with hardware design. For example, a simplified explanation of how TRT compares to DaDN in practical terms would improve accessibility.  
2. Evaluation Scope: The evaluation focuses on CNNs for image classification, with limited discussion of other network architectures or tasks. Expanding the evaluation to include diverse DNN workloads (e.g., transformers, reinforcement learning) would strengthen the paper's impact.  
3. Future Directions: The authors briefly mention potential extensions to training and other machine learning algorithms. Expanding this discussion could highlight the broader applicability of TRT and its relevance to the AI community.
Questions for the Authors:  
1. How does TRT handle dynamic precision adjustments during runtime, and what overheads are associated with this process?  
2. Could TRT's approach to precision scaling be applied to other types of neural networks, such as transformers or recurrent networks?  
3. How does TRT compare to accelerators like EIE in scenarios where network pruning and retraining are feasible?  
In conclusion, while the paper presents a promising contribution to DNN hardware acceleration, I am not qualified to assess its technical depth or novelty. I recommend that the authors seek feedback from hardware experts or submit the work to a hardware-focused venue where it may receive a more informed evaluation.