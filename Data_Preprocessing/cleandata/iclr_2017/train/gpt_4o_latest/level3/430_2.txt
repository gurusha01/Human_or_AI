Review of the Paper
The paper introduces the Latent Sequence Decompositions (LSD) framework, which jointly learns word-to-subword segmentation and acoustic models for sequence-to-sequence tasks, particularly in speech recognition. Unlike traditional approaches that rely on fixed token decompositions, LSD dynamically learns decompositions as a function of both input (e.g., acoustics) and output sequences. The authors demonstrate the efficacy of this approach on the Wall Street Journal Automatic Speech Recognition (ASR) task, achieving a Word Error Rate (WER) of 12.9% compared to a character-based baseline of 14.8%. When combined with a convolutional encoder, the WER further improves to 9.6%. The paper argues that LSD's ability to use longer acoustic units reduces search confusion and improves performance.
Decision: Reject
While the paper proposes an innovative framework and shows empirical improvements, it lacks critical comparisons and clarity in positioning within the literature. Specifically, the absence of a comparison with text-based segmentation methods independent of acoustics, which could simplify the task, weakens the paper's claims. Additionally, the authors do not address whether the subword model outperforms a full-word model with no segmentation, leaving a significant gap in understanding the framework's true advantages.
Supporting Arguments
1. Strengths:
   - The paper tackles an important problem in sequence-to-sequence modeling by addressing the limitations of fixed token decompositions.
   - The LSD framework is well-motivated, and the empirical results show clear improvements over character-based baselines.
   - The use of longer acoustic units is a novel approach that reduces search confusion and improves decoding efficiency.
2. Weaknesses:
   - The lack of comparison with text-based segmentation methods independent of acoustics is a major oversight. Such methods are well-established and could provide a baseline for evaluating LSD's effectiveness.
   - The paper does not cite existing tools or methods for text-based segmentation, missing an opportunity to situate its contributions within the broader literature.
   - The authors do not explore whether the model would use word fragments if all segmentations were possible, leaving questions about the model's behavior and generalizability.
   - The claim that LSD outperforms character-based models is not fully substantiated without a comparison to a full-word model with no segmentation.
Additional Feedback
1. The paper would benefit from a more thorough discussion of related work, particularly text-based segmentation methods and tools. This would help clarify the novelty of the LSD framework.
2. The authors should include experiments comparing LSD to text-based segmentation methods independent of acoustics. This would strengthen the claim that LSD's joint learning approach is superior.
3. It would be valuable to analyze the model's behavior under different segmentation scenarios, such as allowing all possible segmentations or using a full-word model. This would provide deeper insights into the framework's strengths and limitations.
4. The paper could explore the impact of vocabulary size and token granularity more systematically, as these factors likely influence the model's performance.
Questions for the Authors
1. How does LSD compare to text-based segmentation methods independent of acoustics? Could such methods serve as a simpler alternative?
2. Would the model still prefer subword units over word fragments if all segmentations were possible? Have you explored this scenario?
3. How does LSD perform compared to a full-word model with no segmentation? Would such a model simplify the task while maintaining competitive performance?
In summary, while the paper presents a promising approach, the lack of critical comparisons and gaps in experimental analysis prevent it from making a strong case for acceptance. Addressing these issues would significantly improve the paper's impact and clarity.