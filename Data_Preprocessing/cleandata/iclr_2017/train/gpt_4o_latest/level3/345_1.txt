The paper presents a novel empirical Bayesian approach to neural network compression by learning both the network parameters and their priors through a unified iterative optimization process. The key contribution lies in utilizing a mixture model prior to induce weight clustering, enabling simultaneous quantization and pruning. This approach simplifies the compression pipeline compared to earlier multi-stage methods like Han et al. (2015), while achieving comparable compression rates and predictive accuracy. The authors demonstrate the effectiveness of their method on LeNet-300-100, achieving a compression rate of 64.2Ã— with minimal accuracy loss. The paper also explores hyper-prior tuning using Bayesian optimization and evaluates the scalability of the method on larger architectures like ResNet and VGG-19.
Decision: Reject
The decision to reject is based on two primary reasons: (1) the lack of detailed algorithmic descriptions and scaling analysis, which limits the reproducibility and practical applicability of the proposed method, and (2) insufficient interpretation of experimental results, particularly in Section 6.2, where the hyper-parameter optimization lacks clarity and fails to establish meaningful insights.
Supporting Arguments:
1. Strengths: 
   - The proposed method is innovative in combining empirical Bayesian principles with neural network compression. The use of a mixture model prior for weight clustering is well-motivated and aligns with the minimum description length (MDL) principle.
   - The results on LeNet-300-100 and LeNet-5-Caffe are competitive with state-of-the-art methods, demonstrating the potential of the approach for lightweight models.
   - The single-stage optimization framework is a significant improvement over the multi-stage pipelines in prior work, offering a more streamlined and theoretically grounded approach.
2. Weaknesses:
   - The paper lacks a detailed explanation of the algorithm's computational complexity and its scalability to larger models like VGG-19. The claim that the method is "too slow" for such models is not substantiated with quantitative analysis or proposed solutions.
   - Section 6.2 on hyper-parameter optimization using Spearmint is underdeveloped. The results are presented without sufficient interpretation, and the authors fail to establish a clear relationship between hyper-parameters and performance metrics.
   - The reference error in Section 6.1 ("figure C" instead of "figure 1") reflects a lack of attention to detail, which undermines the paper's overall quality.
Additional Feedback:
1. Clarity and Reproducibility: The paper should provide a more detailed description of the optimization process, including the initialization of mixture model components and the handling of collapsed components. This would enhance reproducibility and allow readers to better understand the method's practical implementation.
2. Scalability: The authors should include a quantitative analysis of the method's computational cost for larger models and propose concrete strategies to address scalability challenges.
3. Hyper-parameter Optimization: The discussion in Section 6.2 should be expanded to include insights into the optimization process. For example, how do the chosen hyper-priors influence the clustering behavior and compression rates? Are there specific trade-offs between accuracy and compression that practitioners should be aware of?
4. Accuracy-Compression Tradeoff: A more thorough evaluation of the accuracy-compression tradeoff across different models and datasets would strengthen the paper's experimental contributions.
Questions for the Authors:
1. Could you provide a more detailed explanation of how the mixture model parameters (means, variances, and mixing proportions) are updated during training? Are there any stability issues with this process?
2. How does the proposed method compare to Han et al. (2015) in terms of computational efficiency during training and inference?
3. What specific modifications or approximations would you suggest to make the method scalable to larger models like VGG-19?
While the paper introduces a promising approach to neural network compression, addressing the above concerns is essential to improve its scientific rigor and practical impact.