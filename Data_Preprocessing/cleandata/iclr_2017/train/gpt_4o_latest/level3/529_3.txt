Review of the Paper
Summary of Contributions
This paper proposes a novel framework, RL Tuner, which combines maximum likelihood (ML) and reinforcement learning (RL) for training sequence models, specifically applied to music generation. The approach refines a pre-trained Recurrent Neural Network (RNN) by incorporating music-theory-based rewards while preserving the probabilistic information learned from data. The paper makes several contributions: (1) introducing a method for combining ML and RL training, (2) connecting the approach to stochastic optimal control (SOC) and KL control, (3) empirically comparing Q-learning, Ψ-learning, and G-learning with log prior augmentation, and (4) demonstrating the method's effectiveness in generating more musically pleasing melodies compared to baseline RNNs. The authors also provide a user study to validate the subjective quality of the generated melodies.
Decision: Accept
The paper is well-motivated, scientifically rigorous, and makes meaningful contributions to both the sequence modeling and reinforcement learning literature. The key reasons for acceptance are: (1) the novel combination of ML and RL training for sequence models, which addresses known failure modes in music generation, and (2) the empirical and subjective evidence supporting the method's effectiveness. Additionally, the paper is well-placed in the literature, connecting its approach to SOC principles and prior work on KL-regularized RL.
Supporting Arguments for the Decision
1. Novelty and Motivation: The paper tackles a well-defined problem—improving the global structure and coherence of sequences generated by RNNs, particularly in music. The use of RL to impose structural constraints while retaining probabilistic information from data is a compelling and novel contribution.
2. Scientific Rigor: The authors provide a thorough theoretical foundation, connecting their approach to SOC and KL control. They derive and compare three RL methods (Q-learning, Ψ-learning, and G-learning) and empirically validate their effectiveness.
3. Empirical Results: The results demonstrate significant improvements in adherence to music-theory rules and subjective user ratings of melody quality. The inclusion of a user study strengthens the paper's claims about subjective improvements.
4. Broader Impact: The proposed framework has potential applications beyond music, such as text generation and question answering, where sequence models often suffer from similar failure modes.
Suggestions for Improvement
1. Clarification of Objective (3): The motivation for using DCN instead of variational SOC could be better clarified, particularly in the context of the unification argument. This would help readers understand the design choices more clearly.
2. Reward Augmentation in SOC: While the discussion of SOC is solid, elaborating on how reward augmentation aligns with SOC principles and comparing directly with Deep Q-Networks (DQNs) would strengthen the theoretical grounding.
3. Comparisons with Alternative Techniques: The paper lacks comparisons with alternative learning techniques for the same reward signal, such as SeqGAN or other adversarial approaches. Including these would provide a more comprehensive evaluation.
4. Differentiable Reward Models: A key question is whether the music-theory reward could be approximated by a differentiable model, potentially eliminating the need for RL. Addressing this could provide insights into the necessity of the RL component.
5. Underfitting and Random Behavior: The choice of \( E\pi \log p(at|st) \) requires the policy to "cover" the model, but underfitting in unsupported state spaces can lead to random behavior. A discussion of how this issue is mitigated would be valuable.
Questions for the Authors
1. Could you provide more details on why DCN was chosen over variational SOC for objective (3)? How does this choice impact the generalizability of the approach?
2. Have you considered using differentiable approximations of the music-theory reward? If so, how would this affect the need for reinforcement learning in your framework?
3. How does the balance parameter \( c \) affect the trade-off between adherence to music theory and retention of data probabilities? Could you provide more insights into tuning this parameter?
4. Could you compare your approach to SeqGAN or other adversarial methods for sequence generation? How does RL Tuner perform relative to these techniques?
In summary, this paper makes a strong contribution to the field of sequence modeling and reinforcement learning. While there are areas for improvement and further clarification, the novelty, rigor, and empirical results justify its acceptance.