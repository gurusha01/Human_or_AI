Review of the Paper
Summary of Contributions
The paper presents a theoretical framework for analyzing the convergence of training error in modern convolutional networks (convnets), which are nonsmooth and nonconvex. Its main contribution is Theorem 2, which provides a convergence guarantee for neural networks using a novel "neural Taylor approximation." This approximation simplifies the analysis by leveraging results from online convex optimization and Taylor expansions. The authors also empirically validate the theoretical findings and explore the role of adaptive optimizers in navigating shattered gradient landscapes. The paper is well-motivated, addressing a critical gap in the literature by providing convergence guarantees for architectures widely used in practice. The use of Taylor approximations is novel and provides a fresh perspective on understanding neural network optimization.
Decision: Reject
While the paper is ambitious and addresses an important problem, it is not ready for publication due to significant theoretical and empirical issues. The two primary reasons for rejection are: (1) Theorem 2 has critical flaws, including an unclear convergence of the Taylor optimum and a buggy proof in Appendix 3 that fails to guarantee convergence. (2) The lefthand side of Equation (3) (L3) is not equivalent to the training error, and its convergence does not imply the convergence of the actual training error. These issues undermine the core claims of the paper.
Supporting Arguments
1. Theoretical Issues: Theorem 2, the centerpiece of the paper, has two major flaws:
   - The convergence of the Taylor optimum is not well-justified. The proof in Appendix 3 is incomplete and does not establish the necessary guarantees.
   - The equivalence between the Taylor loss and the actual training error is not rigorously established. This weakens the claim that the bound on the Taylor loss implies convergence of the training error.
2. Empirical Inconsistencies: The experiments fail to adhere to the theoretical conditions required for achieving the \(O(1/\sqrt{n})\) convergence rate in Theorem 2. Specifically, the learning rate used in the experiments does not satisfy the assumptions of the theorem, making the empirical validation less credible.
3. Conceptual Disconnect: The lefthand side of Equation (3) (L3) is not equivalent to the training error. This disconnect raises questions about the practical relevance of the theoretical results.
Suggestions for Improvement
1. Clarify Theorem 2: Provide a rigorous and complete proof of Theorem 2, ensuring that the convergence of the Taylor optimum is well-justified. Address the equivalence (or lack thereof) between the Taylor loss and the training error explicitly.
2. Empirical Alignment: Re-run the experiments with learning rates that satisfy the theoretical assumptions. This alignment is critical for validating the claims of the paper.
3. Improve Exposition: Clearly explain the implications of the Taylor approximation and its limitations. For example, discuss explicitly why the Taylor loss may not fully capture the behavior of the training error.
4. Address Equation (3): Either establish the equivalence between the Taylor loss and the training error or revise the claims to reflect the limitations of the current analysis.
Questions for the Authors
1. Can you provide a more detailed justification for the convergence of the Taylor optimum in Theorem 2? How do you address the issues in Appendix 3?
2. Why does the paper claim that the convergence of the Taylor loss implies convergence of the training error, given that Equation (3) (L3) does not directly represent the training error?
3. How do the experimental results change when the learning rate is adjusted to satisfy the conditions of Theorem 2?
In summary, while the paper has strong motivation and introduces a novel analytical tool, the theoretical and empirical shortcomings must be addressed before it can be considered for publication.