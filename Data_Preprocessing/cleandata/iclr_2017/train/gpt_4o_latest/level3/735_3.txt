Review of the Paper
Summary of the Paper
The paper proposes a novel non-linear kernel dimensionality reduction approach that incorporates a trace norm regularizer into an iterative energy minimization framework. The authors claim their method provides a closed-form solution for robust Kernel PCA (KPCA) and can handle missing data and noise. The approach is applied to two tasks: matrix completion and non-rigid structure from motion (NRSfM), with reported state-of-the-art performance on the oil flow dataset and promising results on the CMU mocap dataset. The paper positions itself as a significant advancement over existing KPCA and latent variable models (LVMs), emphasizing its ability to solve ill-posed problems without pre-training.
Decision: Reject
The paper is rejected due to unconvincing experimental evaluation and unsupported claims of state-of-the-art performance. While the theoretical contributions are interesting, the empirical results fail to substantiate the claims, and several critical issues remain unaddressed.
Supporting Arguments for the Decision
1. Experimental Evaluation: The experimental results are inadequate and rely on outdated techniques and toy datasets. The use of the oil flow dataset as a benchmark is problematic, as it is not widely recognized as a robust standard for evaluating dimensionality reduction methods. The comparisons are made against older methods, and no effort is made to benchmark against modern techniques.
   
2. Robustness to Noise and Outliers: Despite the stated motivation of handling complex noise and outliers, the experiments fail to convincingly demonstrate robustness. The evaluation on the CMU mocap dataset shows improvements, but the results are limited to a sub-sampled dataset, and the setup does not rigorously test the method's robustness.
3. Out-of-Sample Problem: The paper does not address the out-of-sample extension problem, which is a critical limitation of kernel-based methods. This omission undermines the practical applicability of the proposed approach.
4. Misleading Claims: The claim of a "closed-form solution" for robust KPCA is misleading, as the method involves iterative optimization. Similarly, the assertion that the approach can be "trivially generalized" to other cost functions is overstated, as such generalizations would require solving more complex optimization problems.
5. Theoretical Missteps: The paper incorrectly describes its energy minimization framework, solving a doubly-relaxed problem that deviates from the original formulation. Additionally, the critique of LVMs like GPLVM is unclear and fails to acknowledge the flexibility of Gaussian process frameworks in handling different noise models.
Suggestions for Improvement
1. Experimental Rigor: Use modern, widely accepted benchmarks and compare against state-of-the-art methods. For example, datasets like MNIST or CIFAR-10 for dimensionality reduction or more challenging NRSfM datasets would provide a stronger empirical foundation.
2. Address Out-of-Sample Problem: Extend the method to handle out-of-sample data, as this is a critical limitation of kernel methods. This could involve exploring pre-image estimation or other techniques.
3. Clarify Claims: Revise the claims about closed-form solutions and generalizability to accurately reflect the iterative nature of the method and the challenges of extending it to other cost functions.
4. Theoretical Corrections: Ensure the energy minimization framework aligns with the original problem formulation and provide accurate descriptions of related work, such as Geiger et al.'s method for latent space dimensionality.
5. Terminology Consistency: Clarify ambiguous terms like "pre-training" and "absence of a training phase," particularly in the context of KPCA.
Questions for the Authors
1. How does the method perform on larger, more complex datasets with significant noise and outliers? Can you provide results on modern benchmarks?
2. How do you address the out-of-sample extension problem for kernel-based methods in practical applications?
3. Can you clarify the claim of a closed-form solution for robust KPCA, given that the method involves iterative optimization?
4. How does the proposed method compare to recent advancements in kernel-based dimensionality reduction and LVMs, such as deep kernel learning or variational autoencoders?
By addressing these issues, the paper could significantly improve its scientific rigor and practical relevance.