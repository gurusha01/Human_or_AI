Review
The paper presents a novel hierarchical reinforcement learning (HRL) framework that addresses the challenges of sparse rewards and long horizons by pre-training a diverse set of skills using Stochastic Neural Networks (SNNs) and leveraging these skills to improve exploration and efficiency in downstream tasks. The proposed approach combines intrinsic motivation with hierarchical policy learning, using a proxy reward with minimal domain knowledge during pre-training and an information-theoretic regularizer to encourage skill diversity. The experimental results demonstrate the effectiveness of the approach in solving challenging tasks such as mazes and gathering tasks, outperforming baseline methods.
Decision: Reject
While the paper introduces an interesting framework and demonstrates promising results, there are significant concerns regarding the failure model, scalability, and claims of minimal domain knowledge. These issues undermine the generality and rigor of the proposed approach, making it unsuitable for acceptance in its current form.
Supporting Arguments
1. Failure Model Concern: The paper does not adequately address the limitations of the failure model, particularly in scenarios where skill diversity is critical for complex behaviors. For instance, the instability observed with the "Ant" robot highlights the difficulty of switching between skills in unstable environments. This raises concerns about the robustness and applicability of the approach to real-world settings.
2. Generality and Scalability: While the framework performs well on simpler embodiments like the "Swimmer" and "Snake," its scalability to more complex systems, such as a 5-link swimmer or higher-dimensional robots, is not convincingly demonstrated. The failure modes observed with the "Ant" robot suggest that the approach may struggle with more complex embodiments, limiting its generality.
3. Intrinsic Rewards Critique: The claim that the method requires minimal domain knowledge is overstated. The proxy rewards and mutual information regularizer are still hand-crafted and task-specific, contradicting the claim of generality. This reliance on domain-specific design undermines the broader applicability of the proposed framework.
Additional Feedback
1. Space Discretization: The paper does not clarify whether the space is discretized during the estimation of mutual information. If discretization is used, the authors should discuss its implications for scalability to higher-dimensional state spaces. If not, alternative methods for estimating mutual information should be detailed.
2. Skill Transferability: The paper could benefit from a more thorough evaluation of skill transferability across diverse tasks. For example, how well do the learned skills generalize to tasks with significantly different dynamics or objectives?
3. Switching Mechanism: The instability observed during skill switching for the "Ant" robot suggests a need for a more robust transition mechanism. The authors could explore learning a transition policy or incorporating a termination policy to improve stability.
4. Evaluation Metrics: While the paper reports visitation plots and learning curves, additional quantitative metrics (e.g., sample efficiency, success rate, and robustness) would provide a more comprehensive evaluation of the proposed approach.
Questions for Authors
1. How does the discretization of space for mutual information estimation scale with higher-dimensional state spaces? Are there alternative methods considered for continuous spaces?
2. Can the proposed framework handle tasks with dynamic objectives or environments that change over time? If so, how does it adapt?
3. What strategies could be employed to address the instability observed during skill switching for complex robots like the "Ant"?
4. How does the choice of proxy reward impact the diversity and quality of learned skills? Are there guidelines for designing effective proxy rewards?
By addressing these concerns and providing additional evidence for the generality and robustness of the approach, the paper could be significantly improved.