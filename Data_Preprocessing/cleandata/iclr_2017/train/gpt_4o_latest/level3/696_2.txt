The paper provides a comprehensive survey of recent advancements in reading comprehension models, focusing on the emergence of "predication structure" in neural readers. By rephrasing "logical structure" as "predication," the authors effectively clarify the conceptual framework, which is a commendable effort. The paper organizes the literature into two distinct themes: "aggregation readers" and "explicit reference models," offering a structured perspective on an otherwise chaotic field. Additionally, the writing quality is exceptional, with Section 3 standing out for its clarity and depth.
The authors' empirical evidence for the emergence of predication structure in neural readers is intriguing and well-presented. However, the reliance on the CNN/DailyMail dataset raises concerns about its suitability for investigating logical structures, given its anonymized nature and limited semantic richness. While the paper proposes new models and demonstrates their effectiveness on the Who-did-What dataset, it falls short in providing actionable insights for dataset or model design to address broader challenges in reading comprehension. A more precise analysis of logical structure gaps in existing models and datasets, along with concrete recommendations for the research community, would significantly enhance the paper's impact.
Decision: Reject
Key Reasons:
1. Dataset Suitability: The reliance on the CNN/DailyMail dataset undermines the investigation of logical structure, as the dataset's anonymization strips away critical semantic information.
2. Lack of Practical Insights: The paper does not sufficiently address how its findings can inform future dataset or model design, limiting its practical utility for the community.
Supporting Arguments:
- The paper's theoretical contributions, particularly the notion of predication structure, are valuable. However, the empirical validation is constrained by the limitations of the datasets used, particularly CNN/DailyMail.
- While the paper organizes the literature effectively, it does not provide actionable directions for addressing the identified gaps in logical structure within models and datasets.
Additional Feedback:
1. Dataset Limitations: The authors should explore alternative datasets that better capture logical structure and semantic richness, such as SQuAD or other non-anonymized corpora.
2. Practical Implications: The paper would benefit from a dedicated section offering concrete recommendations for dataset creation or model architecture improvements to address the challenges in reading comprehension.
3. Analysis of Gaps: A more detailed analysis of where current models fail in capturing logical structure would provide valuable insights for the community.
4. Generalization: The findings should be tested across a broader range of datasets to ensure the generalizability of the proposed predication structure framework.
Questions for the Authors:
1. How do you address the limitations of the CNN/DailyMail dataset in evaluating logical structure? Have you considered alternative datasets?
2. Can you provide more detailed insights into how the proposed predication structure can guide future model or dataset design?
3. What specific gaps in logical structure do you observe in existing models, and how can these be addressed systematically?
While the paper makes a strong theoretical contribution, its practical and empirical shortcomings prevent it from meeting the standards for acceptance at this stage. Addressing these issues in a future revision could significantly enhance its impact.