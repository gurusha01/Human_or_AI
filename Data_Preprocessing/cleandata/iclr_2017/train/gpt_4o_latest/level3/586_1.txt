Review of the Paper
Summary of Contributions
This paper explores improvements to the Neural GPU, a model capable of learning algorithmic tasks such as multi-digit arithmetic operations. The authors propose two enhancements: curriculum learning and increasing model size, which they claim extend the Neural GPU's ability to generalize to longer inputs and solve more complex algorithmic problems. The paper also investigates the model's failure modes, particularly its struggles with highly structured inputs and adversarial cases. Additionally, the authors provide insights into the sensitivity of the model to random seeds and the challenges of training large models due to memory constraints. The work contributes to the understanding of generalization in algorithmic learning and highlights the limitations of current approaches.
Decision: Reject  
While the paper provides interesting observations and insights, it falls short in several critical areas. The primary reasons for rejection are:  
1. Lack of Novelty and Depth: The proposed improvements—curriculum learning and increasing model size—are well-known techniques in the field and do not represent significant advancements. The claims about memory efficiency are overstated, as the techniques used are standard practices.  
2. Insufficient Rigor in Claims: The paper does not adequately support its claims about the Neural GPU's generalization capabilities. The reliance on multiple random seeds to achieve convergence raises concerns about the robustness and computational cost of the approach.  
Supporting Arguments
1. General Impression: The paper reads more like a status update than a cohesive research contribution. While the observations are clear, the remarks are scattered and do not significantly advance the understanding or performance of Neural GPUs.  
2. Table 1 Confusion: The use of the term "RNN" in Table 1 is misleading, as it includes models that are technically RNNs but differ significantly in architecture and computational properties. This highlights the need for better characterization of input/output/memory dimensions.  
3. Model Input/Output Handling: The paper lacks clarity on how inputs (e.g., 1-hot encoding) and outputs are processed with 512 filters. It remains unclear whether intermediate layers implement specific algorithms or merely approximate them.  
4. Memory Efficiency Claims: The techniques described for reducing memory usage are standard (e.g., symbolic loops, reduced batch sizes) and do not constitute novel contributions. Moreover, these techniques may slow down training and inference, undermining the claimed efficiency.  
5. Convergence Issues: The need for a large number of random seeds to achieve convergence raises questions about the practicality and scalability of the approach. This undermines the claim that the Neural GPU is a reliable tool for learning algorithms.  
Additional Feedback for Improvement
1. Clarify Input/Output Handling: Provide a detailed explanation of how inputs and outputs are encoded and processed, particularly for models with 512 filters. This would help assess whether the model is learning interpretable algorithmic representations.  
2. Address Generalization Failures: The paper identifies failure modes (e.g., structured inputs, long carries) but does not propose solutions. Future work should explore architectural modifications or training strategies to address these issues.  
3. Improve Experimental Rigor: The paper would benefit from a more systematic evaluation of the proposed improvements. For example, compare the Neural GPU's performance with other state-of-the-art models on the same tasks.  
4. Refine Terminology: Avoid ambiguous terms like "RNN" in Table 1 and provide precise definitions of the models and tasks under consideration.  
Questions for the Authors
1. How do the intermediate layers of the Neural GPU relate to specific algorithmic steps? Can they be interpreted as implementing parts of the algorithm?  
2. What is the computational trade-off between memory efficiency and training/inference speed when using the described techniques?  
3. Could the authors provide more details on the curriculum strategies used? How were the intermediate tasks chosen, and how do they contribute to the final performance?  
4. Have the authors considered alternative architectures or mechanisms (e.g., attention) to address the model's difficulty with global data movement?  
In summary, while the paper provides some interesting observations, it lacks the novelty, rigor, and clarity required for acceptance. Addressing the above concerns could significantly improve the quality and impact of the work.