Review of the Paper
Summary of Contributions
This paper introduces the Gaussian Error Linear Unit (GELU), a novel neural network activation function derived as the expected transformation of a stochastic regularizer called the Stochastic 0-I (SOI) map. The authors claim that GELU bridges the gap between traditional nonlinearities and stochastic regularizers like dropout, offering a probabilistic interpretation of activation functions. Empirical results are presented to demonstrate that GELU outperforms ReLU and ELU across tasks in computer vision, natural language processing, and speech recognition. The paper also explores the SOI map's ability to train networks without traditional nonlinearities, suggesting that stochastic regularizers alone can achieve competitive performance.
Decision: Reject
While the paper presents a novel idea with potential, it lacks sufficient justification for its claims and rigorous empirical validation. The following key issues underpin this decision:
1. Lack of Justification for Novelty and Preference: The paper does not adequately explain why GELU is preferable to other parameterizations or why the SOI map is a superior alternative to existing stochastic regularizers. The probabilistic interpretation is interesting but not convincingly demonstrated as practically impactful.
   
2. Weak Empirical Results: While CIFAR results are acceptable, the MNIST results are poor compared to benchmarks from a decade ago, undermining the claim of GELU's superiority. TIMIT results are also inconclusive without evaluating word error rate in a speech pipeline, which is a standard metric for significance in speech recognition tasks.
3. Misleading Claims: The assertion that SOI map networks are comparable to linear functions is misleading, as the SOI map introduces nonlinearity in expectation. This undermines the claim that traditional nonlinearities are unnecessary.
Supporting Arguments
- The empirical results are inconsistent and fail to establish a clear advantage for GELU across all tasks. For example, the MNIST results are particularly weak, and the TIMIT evaluation lacks critical metrics like word error rate.
- The theoretical discussion is insufficiently grounded in the broader literature. The paper does not adequately compare GELU to other probabilistic or smoothed activation functions, such as SiLU, nor does it explore the implications of its curvature and non-monotonicity in depth.
- The grayscale plots are difficult to interpret, which detracts from the clarity of the results.
Suggestions for Improvement
1. Stronger Theoretical Justification: Provide a deeper theoretical analysis of why GELU is preferable over existing activation functions. Clarify the practical implications of its probabilistic interpretation and curvature.
2. Comprehensive Empirical Evaluation: Improve the experimental setup by including more robust benchmarks and metrics. For example, evaluate word error rate for TIMIT and compare MNIST results to state-of-the-art models.
3. Clarify Claims: Avoid misleading statements about the linearity of SOI map networks. Clearly articulate the role of nonlinearity in the SOI map and its relationship to GELU.
4. Improved Visualizations: Use color plots or more accessible grayscale visualizations to enhance readability and interpretability.
Questions for the Authors
1. Why is GELU preferable to other smoothed or probabilistic activation functions, such as SiLU? How does it compare theoretically and empirically?
2. Can you provide a more detailed explanation of the practical benefits of the probabilistic interpretation of GELU? How does this interpretation translate into improved performance?
3. Why are the MNIST results so weak compared to benchmarks? Could this be due to suboptimal hyperparameter tuning or experimental design?
4. Why was word error rate not evaluated for TIMIT? Would this metric provide stronger evidence for GELU's effectiveness in speech recognition tasks?
In conclusion, while the paper presents an intriguing idea, it requires significant improvements in both theoretical justification and empirical rigor to make a compelling case for acceptance.