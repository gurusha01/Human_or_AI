Review
Summary of Contributions
This paper investigates the role of initialization in deep networks and challenges the prevailing notion that modern techniques eliminate optimization issues such as local minima and saddle points. By constructing theoretical and empirical counterexamples, the authors demonstrate that poor initialization can lead to suboptimal solutions, even for finite datasets and finite-sized models. The paper highlights the susceptibility of rectified MLPs (ReLU-based networks) to bad initialization and explores how specific data structures and initialization schemes can hinder learning dynamics. The authors also provide a critique of existing theoretical assumptions, arguing that they fail to generalize to real-world scenarios. While the paper does not propose new solutions, it raises awareness of potential pitfalls in deep learning optimization, particularly in the context of initialization and data structure.
Decision: Reject
The primary reasons for rejection are: (1) the counterexamples feel artificially constructed and lack practical relevance, and (2) the paper does not provide new solutions, workarounds, or a systematic analysis of existing heuristics to address the identified issues.
Supporting Arguments
1. Problem Definition and Motivation: The paper tackles an important question about the error surface of deep networks and the role of initialization. However, the examples provided, while theoretically valid, appear contrived and do not convincingly demonstrate real-world implications. The lack of practical relevance limits the broader applicability of the findings.
2. Literature Placement: The paper is well-situated in the literature, referencing foundational works on error surfaces and initialization strategies. However, it overlooks key heuristics like non-saturating activation functions, batch normalization, and skip connections, which are widely used to mitigate initialization issues. This omission weakens the paper's argument, as it does not account for how these techniques might address the problems it highlights.
3. Scientific Rigor: The theoretical proofs and empirical results are rigorous and well-documented. However, the focus on edge cases (e.g., "Jellyfish" datasets) and artificially constructed scenarios detracts from the paper's practical significance. The experiments on MNIST and other datasets are limited in scope and fail to provide actionable insights for practitioners.
Suggestions for Improvement
1. Broader Analysis: Incorporate a systematic analysis of existing heuristics, such as batch normalization and skip connections, to evaluate their effectiveness in mitigating the identified issues. This would make the findings more relevant to real-world applications.
2. Practical Relevance: Extend the empirical analysis to more diverse and realistic datasets beyond MNIST. Demonstrating the impact of poor initialization on modern architectures (e.g., ResNets, Transformers) would enhance the paper's practical value.
3. Proposed Solutions: While identifying problems is valuable, the paper would benefit significantly from proposing new initialization schemes, training strategies, or architectural modifications to address the highlighted issues.
4. Clarify Contributions: Clearly delineate the scope of the paper's contributions. For example, emphasize whether the goal is to critique existing theoretical assumptions or to provide actionable insights for practitioners.
Questions for the Authors
1. How do the identified issues with initialization manifest in modern architectures like ResNets or Transformers, which incorporate techniques like skip connections and layer normalization?
2. Can the counterexamples presented in the paper be generalized to higher-dimensional, real-world datasets, or are they limited to low-dimensional, synthetic cases?
3. Have you considered the impact of alternative optimization algorithms (e.g., second-order methods) on the bad initialization scenarios described in the paper?
In summary, while the paper raises an important theoretical question and provides rigorous proofs, its practical relevance and contribution to actionable solutions are limited. Addressing these gaps could make the work more impactful for both researchers and practitioners.