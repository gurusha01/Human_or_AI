Review
Summary of the Paper
The paper investigates the impact of different context types—specifically dependency trees versus standard window contexts—on word vector learning. It claims to provide the first systematic analysis of context representations for learning word embeddings, with experiments conducted across four tasks and 21 datasets. The authors aim to offer insights into context selection and provide published code as a resource for the community. The stated goal is to guide researchers in choosing the appropriate context for word embedding models.
Decision: Reject  
The paper does not meet the standards required for acceptance due to insufficient depth in analysis, lack of comprehensive comparisons, and minimal contributions to advancing the state of the art.
Supporting Arguments
1. Lack of Depth in Analysis: While the paper claims to systematically investigate context types, the analysis is superficial and fails to explore the nuances of the problem. For example, the paper does not delve into why certain contexts perform better or worse, nor does it provide theoretical insights to explain the observed results.
   
2. Omission of Key Comparisons: The paper does not analyze Glove-like objective functions, which are known to often outperform the algorithms used in the study. This omission undermines the comprehensiveness of the analysis and leaves a significant gap in the evaluation of context types.
3. No Absolute Comparisons: The paper does not compare its results to other published word vectors or models, making it difficult to assess the significance of its findings. Without such comparisons, the results lack context and fail to establish the relevance of the proposed analysis.
4. Minimal Contribution to the Field: The paper does not provide any groundbreaking insights or resources that could influence or modify existing work. It neither advances the state of the art nor offers new tools or datasets for the community.
Additional Feedback
1. Expand the Analysis: The authors should provide a deeper exploration of why certain context types perform better or worse. This could include theoretical justifications or more detailed empirical analyses.
   
2. Include Glove-like Objectives: The omission of Glove-like objective functions is a critical gap. Future iterations of the paper should include these models to provide a more comprehensive evaluation.
3. Provide Absolute Comparisons: The paper would benefit from direct comparisons to other published word vectors or models, such as Word2Vec, Glove, or FastText, to contextualize its findings.
4. Clarify Insights: The insights presented in the paper are vague and do not provide actionable guidance for the community. The authors should aim to offer more concrete takeaways.
Questions for the Authors
1. Why were Glove-like objective functions excluded from the analysis? How do you justify this omission given their strong performance in prior work?
2. How do your results compare to established word embedding models like Word2Vec, Glove, or FastText? Can you provide absolute performance metrics for these comparisons?
3. What specific insights about context selection can practitioners take away from your findings? Could you elaborate on how these insights can influence future work?
In its current form, the paper lacks the rigor and depth necessary to make a meaningful contribution to the field. Addressing the above concerns could significantly improve the quality and impact of the work.