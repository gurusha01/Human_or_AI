Review of the Paper
Summary of Contributions
This paper introduces a hierarchical attention-based method (TS-ATT) for document classification, leveraging two bidirectional LSTMs to capture both global and local context representations. The authors also propose a simpler variant, SS-ATT, which removes the first LSTM and uses the output of the second LSTM as a global context vector. The paper draws inspiration from human reading comprehension, where a rough global understanding informs attention to specific local contexts. Experimental results on three datasets (Amazon, IMDB, and Yelp 2013) are presented, and attention visualization is used to demonstrate the interpretability of the model. The authors claim their approach achieves competitive results without requiring pretrained embeddings, which they argue simplifies the training process.
Decision: Reject
While the paper presents an interesting hierarchical attention mechanism, the experimental results and methodological rigor do not justify acceptance into the main conference. The primary reasons for rejection are: (1) the lack of state-of-the-art performance on the benchmark datasets, and (2) insufficient evidence of scalability and robustness due to the use of smaller dataset sizes compared to prior work.
Supporting Arguments
1. Experimental Results: The proposed models (TS-ATT and SS-ATT) fail to achieve state-of-the-art performance on the datasets. Although the authors claim competitive results, the reported accuracy on Yelp 2013 is lower than that of existing baselines (e.g., Tang et al., 2015). The absence of pretrained embeddings is downplayed as a significant advantage, especially when the results do not surpass those of models that use such embeddings.
   
2. Dataset Size and Scalability: The datasets used are smaller than those in prior work, raising concerns about the scalability of the proposed methods. The paper does not address whether the models would perform well on larger datasets or in real-world scenarios.
3. Baseline Comparisons: The paper lacks comparisons with key baselines, such as Tang et al. (2015), which are critical for contextualizing the performance of the proposed models. Including these baselines would strengthen the empirical evaluation.
Suggestions for Improvement
1. Baseline Inclusion: Include results from more competitive baselines, such as Tang et al. (2015), to provide a stronger empirical comparison.
2. Scalability Analysis: Conduct experiments on larger datasets to demonstrate the scalability and robustness of the proposed methods.
3. Terminology: Avoid ambiguous phrases like "Learning to Understand," which may overstate the model's capabilities.
4. Typographical Errors: Correct minor issues, such as the typo "gloal" â†’ "global."
5. Pretrained Embeddings: While the authors emphasize the lack of pretrained embeddings, they should provide a more balanced discussion of the trade-offs, especially since pretrained embeddings are standard in NLP tasks.
Questions for the Authors
1. How does the model perform on larger datasets or in low-resource settings? Have you tested scalability beyond the datasets presented?
2. Could you clarify why pretrained embeddings were not used, and whether their inclusion might improve results?
3. How does the proposed method compare to Tang et al. (2015) on the Yelp 2013 dataset, and why was this baseline omitted?
Final Remarks
The hierarchical attention mechanism proposed in this paper is conceptually interesting and has potential for further development. However, the lack of competitive results, insufficient baseline comparisons, and questions about scalability make this submission more suitable for a workshop than the main conference. Addressing these issues in future work could significantly strengthen the contribution.