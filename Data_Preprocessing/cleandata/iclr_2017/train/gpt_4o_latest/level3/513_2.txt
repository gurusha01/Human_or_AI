Review
This paper explores the use of unsupervised learning to improve the generalization of models predicting the stability of block towers. By leveraging a synthetic dataset and video sequences generated from a physics engine, the authors train three types of models: a supervised model (S), an unsupervised model predicting the final frame (CD), and an unsupervised model predicting intermediate frames (CLD). The results demonstrate that unsupervised models (CD and CLD) outperform the supervised model (S) when tested on towers with unseen heights, highlighting the potential of unsupervised learning for generalization in physical reasoning tasks.
Decision: Accept (with revisions)  
The paper presents a well-written and focused contribution that demonstrates the benefits of unsupervised learning in a specific and well-defined generalization problem. The key strength lies in the empirical results showing significant accuracy gains for unsupervised models, which could inspire further exploration of unsupervised learning for physical reasoning tasks. However, the paper's novelty is moderate, and deeper analysis is required to strengthen its contribution.
Supporting Arguments for Decision  
1. Strengths:  
   - The paper addresses an important problem: improving generalization in physical reasoning tasks, which aligns with broader goals in AI research.  
   - The experimental results are compelling, showing clear benefits of unsupervised learning in generalization to unseen scenarios.  
   - The writing is clear, and the methodology is well-documented, making the work reproducible.  
2. Weaknesses:  
   - The analysis lacks depth. For example, the paper does not explore the limitations of the rendering process, LSTM sub-sampling, or the challenges of generalization in detail.  
   - There is insufficient discussion on whether the observed generalization improvements are due to model capacity, task specification, or the training procedure.  
   - The absence of comparable baselines (e.g., other unsupervised learning techniques) limits the ability to contextualize the results within the broader literature.  
Additional Feedback for Improvement  
1. Analysis: The authors should provide a more detailed analysis of the factors influencing generalization. For instance, how does the quality of generated frames (e.g., noise in ConvLSTMDeconv) affect stability prediction? Could alternative architectures or loss functions improve performance?  
2. Baselines: Including comparisons with other unsupervised learning approaches or state-of-the-art methods for physical reasoning would strengthen the paper's claims.  
3. Motivation for Auxiliary Experiments: The motivation for using intermediate frame prediction (CLD) is unclear. Why is this approach expected to improve generalization compared to final frame prediction (CD)?  
4. Dataset and Code: The paper does not clarify whether the dataset and code will be released. Sharing these resources would significantly enhance the paper's impact and reproducibility.  
5. Future Work: The authors mention extending their work to robot manipulation tasks but do not provide concrete details. Including preliminary results or a clear roadmap for this extension would make the paper more forward-looking.  
Questions for Authors  
1. Could you elaborate on why ConvDeconv outperforms ConvLSTMDeconv? Is it purely due to noise in the generated frames, or are there other factors at play?  
2. How sensitive are the results to the choice of dataset size and augmentation techniques? Could the generalization improvements diminish with a larger dataset?  
3. Could you clarify the motivation for intermediate frame prediction (CLD)? How does it conceptually differ from final frame prediction (CD) in terms of aiding generalization?  
4. Are there plans to release the dataset and code? If not, could you provide more details on how others might replicate your experiments?  
In conclusion, while the paper presents valuable results, addressing the above concerns would significantly enhance its contribution and impact.