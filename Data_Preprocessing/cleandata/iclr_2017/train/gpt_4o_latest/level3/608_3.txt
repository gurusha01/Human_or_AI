The paper investigates the use of sinusoidal activation functions in neural networks, a largely overlooked area in the field. It provides a theoretical analysis of the challenges associated with training such networks, particularly the emergence of infinitely many shallow local minima and deep local minima, which complicate optimization. The authors demonstrate that sinusoidal activations can achieve comparable performance to established monotonic functions like tanh on standard classification tasks (e.g., MNIST, Reuters) while scarcely relying on the periodicity of the sine function. Furthermore, they show that sinusoidal activations outperform monotonic functions on algorithmic tasks where periodicity is advantageous, such as arithmetic operations. The paper concludes by encouraging further exploration of periodic activation functions in neural network architectures.
Decision: Reject
The primary reasons for this decision are the limited scope of the tasks and the insufficient exploration of the broader applicability of sinusoidal activation functions. While the paper makes a compelling case for the potential of sinusoidal activations, its impact is constrained by the narrow range of tasks considered. Additionally, the experiments, while rigorous, do not convincingly establish that sinusoidal activations are broadly superior or uniquely beneficial compared to existing activation functions.
Supporting Arguments:
1. Theoretical Contribution: The paper provides a thorough theoretical analysis of the challenges in training networks with sinusoidal activations, which is a valuable contribution. However, this analysis is primarily limited to simple architectures and does not extend to more complex or practical scenarios, such as deeper networks or real-world tasks.
2. Empirical Results: The experiments on MNIST and Reuters datasets show that sinusoidal activations perform similarly to tanh but do not leverage their periodicity. While the algorithmic tasks demonstrate the potential of sinusoidal activations, these tasks are synthetic and do not generalize to broader applications. The lack of experiments on more complex datasets or tasks reduces the paper's overall impact.
3. Positioning in Literature: The paper is well-placed in the literature and addresses a gap in the understanding of periodic activation functions. However, it does not sufficiently explore how sinusoidal activations compare to other non-monotonic or hybrid activation functions in diverse settings.
Suggestions for Improvement:
1. Broader Task Scope: Apply the proposed sinusoidal activation functions to more complex and diverse tasks, such as natural language processing, computer vision, or reinforcement learning, to better demonstrate their generalizability and practical utility.
2. Comparative Analysis: Include comparisons with other non-monotonic or hybrid activation functions, such as those combining periodic and monotonic components, to contextualize the findings.
3. Deeper Architectures: Extend the theoretical analysis to deeper networks and explore how sinusoidal activations interact with modern architectural components like attention mechanisms or residual connections.
4. Explainability: Provide more insights into why sinusoidal activations outperform monotonic functions on algorithmic tasks and whether this advantage can be leveraged in other domains.
Questions for the Authors:
1. How do sinusoidal activations perform on tasks with real-world periodicity, such as time-series forecasting or signal processing?
2. Could the initialization strategies or optimization techniques proposed for sinusoidal activations be generalized to other non-monotonic activation functions?
3. Have you considered combining sinusoidal activations with other activation functions in hybrid architectures? If so, what were the results?
While the paper offers interesting insights, its limited scope and lack of broader applicability prevent it from making a significant impact at this stage. Addressing the above concerns could substantially strengthen the work.