Review of the Paper
Summary of Contributions
This paper investigates strategies for vocabulary selection in Neural Machine Translation (NMT) to enhance computational efficiency during decoding and training. It extends prior work by exploring a diverse range of selection techniques, including word alignments, bilingual embeddings, phrase pairs, and Support Vector Machines (SVMs). The authors demonstrate that word alignment-based selection is particularly effective, achieving significant speed-ups (up to 90% in decoding and 25% in training) with negligible loss in translation quality. The experiments are conducted on two language pairs (English-German and English-Romanian), and the results are validated across multiple test sets. The paper provides a detailed analysis of the trade-offs between speed and accuracy, making it a useful contribution to the field.
Decision: Accept
The paper is a minor but valuable contribution to the NMT literature. Its strength lies in the comprehensive evaluation of vocabulary selection techniques and the practical insights it offers for improving efficiency in NMT systems. However, the decision to accept is contingent on addressing several areas for improvement, particularly in justifying the focus on word-level vocabulary over subword units and providing additional related work.
Supporting Arguments
1. Specific Problem Tackled: The paper addresses the computational inefficiency of NMT systems caused by large target vocabularies, a well-known bottleneck in both training and decoding. The problem is relevant and timely, given the increasing deployment of NMT systems in resource-constrained environments.
2. Motivation and Placement in Literature: The paper builds on prior work (e.g., Mi et al., 2016) and extends it by evaluating additional selection techniques and analyzing speed-accuracy trade-offs. However, it lacks a strong justification for prioritizing word-level vocabularies over subword units like Byte Pair Encoding (BPE) or character-level models, which are common in modern NMT systems.
3. Scientific Rigor: The experimental results are robust, with clear metrics (BLEU scores, decoding speed, and training time) and comparisons across multiple methods. The findings are well-supported and reproducible based on the detailed methodology.
Suggestions for Improvement
1. Justification for Word-Level Focus: The authors should clarify why word-level vocabulary selection is prioritized over subword or character-based approaches, which are known to handle rare words and morphological variations more effectively.
2. Coverage Reporting: Instead of using a 100k "full vocabulary" as the baseline, the authors should report the coverage rate of the actual full vocabulary to provide a more accurate comparison.
3. Related Work: The paper would benefit from citing and discussing additional related works, such as Mauser et al. (2009) and Ha et al. (2014), which explore lexical selection and efficiency in translation systems.
4. Comparison with Mini-Batch Techniques: The probabilistic semantics of Mi et al. (2016)'s mini-batch vocabulary reduction technique are critiqued, but the paper does not provide a direct empirical comparison with single-sentence vocabulary approaches. Including such a comparison would strengthen the argument.
Questions for the Authors
1. How does the proposed word alignment-based approach compare to subword-level methods like BPE in terms of both efficiency and accuracy?
2. Could the authors provide a breakdown of the computational overhead introduced by the SVM-based selection method compared to simpler techniques like word alignments?
3. What is the rationale for using a fixed vocabulary size (e.g., 100k) as the "full vocabulary" baseline, and how does this choice impact the reported results?
In conclusion, the paper offers practical insights into vocabulary selection for NMT and provides a solid foundation for future work. Addressing the suggested improvements would enhance its impact and relevance.