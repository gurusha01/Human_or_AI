Review of the Paper
The paper introduces the Parametric Exponential Linear Unit (PELU), a parameterized modification of the ELU activation function, aimed at addressing vanishing gradients and improving training dynamics in Convolutional Neural Networks (CNNs). The authors provide a theoretical analysis of PELU, suggesting that its flexibility may help mitigate vanishing gradients and bias shift. They also present empirical results on CIFAR-10/100 and ImageNet datasets, demonstrating performance improvements across various architectures. The claimed contributions are promising, and the paper is well-written with a clear structure.
Decision: Reject
While the paper introduces an interesting idea, the theoretical claims about PELU's benefits are not strongly supported by the experiments. The experimental results focus on generalization improvements, but these could stem from better inductive bias rather than optimization advantages, which the theory primarily emphasizes. Additionally, concerns about hyperparameter selection and the lack of a convincing alignment between theory and empirical results weaken the paper's overall impact.
Supporting Arguments for the Decision
1. Theoretical Claims vs. Experimental Evidence: The theoretical analysis suggests that PELU improves optimization by addressing vanishing gradients. However, the experiments primarily highlight generalization improvements, which may arise from better inductive bias rather than optimization advantages. This disconnect between theory and results undermines the paper's central claims.
2. Hyperparameter Concerns: The paper does not adequately address whether ELU could outperform PELU under different hyperparameter settings. For instance, the observed improvements in ResNet110 required increasing weight decay, raising questions about whether the results are specific to the chosen hyperparameters rather than the intrinsic benefits of PELU.
3. Experimental Setup: While the experiments demonstrate performance gains, they lack rigorous ablation studies to isolate the effects of PELU's parameterization. For example, the impact of individual parameters (a, b) on optimization and generalization is not thoroughly analyzed. This makes it difficult to attribute the observed improvements solely to PELU.
Suggestions for Improvement
1. Stronger Theoretical-Experimental Alignment: The paper should include experiments specifically designed to validate the theoretical claims about optimization benefits. For example, analyzing the gradient dynamics during training or comparing convergence rates with ELU would strengthen the connection between theory and results.
2. Hyperparameter Robustness: The authors should explore a broader range of hyperparameter settings to demonstrate that PELU's benefits are not overly dependent on specific configurations. This would address concerns about whether ELU could achieve similar performance under different conditions.
3. Ablation Studies: Conducting ablation studies to evaluate the individual contributions of parameters a and b would provide deeper insights into how PELU influences training dynamics and generalization.
4. Comparison with Other Activation Functions: While the paper compares PELU with ELU and ReLU, it would be valuable to include comparisons with other parametric activation functions, such as PReLU or SReLU, to contextualize PELU's performance within the broader literature.
Questions for the Authors
1. Can you provide evidence that PELU improves optimization (e.g., faster convergence, better gradient flow) rather than just generalization?  
2. How sensitive are PELU's performance gains to the choice of hyperparameters, such as weight decay and learning rate?  
3. Have you considered testing PELU in tasks beyond image classification, such as object detection or recurrent neural networks, to assess its broader applicability?  
In summary, while the paper introduces a novel idea with potential, the lack of strong evidence supporting the theoretical claims and the limited experimental rigor make it difficult to recommend acceptance at this stage. Addressing the above concerns could significantly strengthen the paper for future submissions.