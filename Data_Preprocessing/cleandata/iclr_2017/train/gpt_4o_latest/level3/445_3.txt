Review of the Paper
Summary of the Work:
This paper investigates reinforcement learning (RL) techniques for training dialogue agents using a memory network architecture, focusing on online learning from teacher feedback. The work extends Weston (2016) by incorporating real human feedback via Amazon Mechanical Turk and exploring online policy learning in both simulated and real-world settings. The authors evaluate the performance of various RL algorithms, including REINFORCE, Reward-Based Imitation (RBI), and Forward Prediction (FP), on synthetic datasets (bAbI and WikiMovies) as well as real human-generated data. The paper's key contributions include demonstrating the feasibility of online learning from human feedback and highlighting the potential of combining textual feedback with numerical rewards for dialogue agent training.
Decision: Reject
The primary reasons for this decision are the lack of novelty in the proposed approach and insufficient direct comparisons with Weston (2016). While the experiments are comprehensive, the work primarily serves as an incremental extension of Weston (2016) rather than introducing significant new insights or methodologies.
Supporting Arguments for the Decision:
1. Lack of Novelty: The paper builds on Weston (2016) but does not introduce substantial new machine learning techniques. The exploration policy changes and RL formulations are minor and have been well-studied in prior work. The distinction between RL and non-RL conditions is overstated, as Weston (2016)'s methods can be framed within an RL paradigm.
   
2. Insufficient Comparisons: The paper does not adequately compare its results with Weston (2016)'s static exploration policy experiments. Such comparisons would provide a clearer evaluation of the proposed approach's improvements. The claim that the iterative batch learning outperforms Weston (2016) is not rigorously substantiated.
3. Exciting but Underexplored Experiments: The fine-tuning experiments with human workers are promising but are not explored in sufficient depth. This aspect could have been a major contribution but remains underdeveloped.
4. Clarity Issues: The paper would benefit from explicitly delineating its contributions relative to Weston (2016). For instance, the framing of prior work as "not RL" is misleading and detracts from the clarity of the paper.
Additional Feedback for Improvement:
1. Explicit Comparisons: Include direct comparisons with Weston (2016)'s static exploration policy experiments to better contextualize the performance improvements claimed in this work.
   
2. Discussion on Vanilla Policy Gradient Success: The observation that vanilla policy gradient methods work well in some experiments is intriguing. A deeper analysis of why this occurs, particularly in the context of batch size and off-policy learning, would strengthen the paper.
3. Human Feedback Experiments: The experiments with Mechanical Turk are the most novel aspect of the paper. Expanding on these experiments, including a second iteration of feedback collection and analysis of the impact of human variability, would significantly enhance the paper's contributions.
4. Clarify Batch Size Claims: The claim linking batch size to off-policy learning is confusing, as many on-policy algorithms also require large batches. This needs clarification or rephrasing.
5. Novelty in Exploration Policies: The exploration policy changes are incremental. Introducing more innovative exploration strategies or demonstrating their necessity in this context would improve the paper's impact.
Questions for the Authors:
1. Can you provide more detailed comparisons with Weston (2016)'s static exploration policy experiments, particularly in terms of performance metrics?
2. Why do vanilla policy gradient methods perform well in your experiments? Is this due to specific characteristics of the datasets or the memory network architecture?
3. How do you address the variability in human feedback during the Mechanical Turk experiments? Did you analyze the quality or consistency of the feedback?
4. Could you elaborate on why FP becomes unstable in certain settings and how balancing or exploration mitigates this issue?
In conclusion, while the paper provides a thorough experimental evaluation and explores an important direction in dialogue agent training, the lack of novelty and insufficient contextualization of its contributions relative to prior work make it unsuitable for acceptance in its current form. Further development of the human feedback experiments and clearer articulation of the paper's unique contributions could significantly improve its impact.