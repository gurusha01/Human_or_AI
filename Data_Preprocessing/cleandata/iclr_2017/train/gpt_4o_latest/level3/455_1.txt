Review of "Energy-based Generative Adversarial Networks" (EBGAN)
The paper introduces the Energy-based Generative Adversarial Network (EBGAN), a novel framework that reinterprets the discriminator in GANs as an energy function. The discriminator assigns low energy to real data and high energy to generated samples, with the generator trained to minimize the energy of its outputs. A key contribution is the use of an autoencoder as the discriminator, where reconstruction error serves as the energy metric. This approach is shown to improve training stability and scalability, enabling the generation of high-resolution images without requiring multi-scale architectures. The paper also provides theoretical guarantees, demonstrating that the generator converges to the data distribution under a hinge loss. Empirical results on datasets like ImageNet and CelebA highlight the model's ability to generate high-quality images.
Decision: Accept
The decision to accept this paper is based on two key reasons: (1) the theoretical contributions are significant and well-supported, providing a rigorous foundation for the proposed energy-based framework; and (2) the empirical results are compelling, particularly the ability to generate high-resolution images, which is a notable advancement in GAN research. The reinterpretation of the discriminator as an energy function and the integration of autoencoders are innovative and well-motivated.
Supporting Arguments:
1. Problem Tackled: The paper addresses the instability in GAN training and the challenge of generating high-resolution images. By framing the discriminator as an energy function, the authors provide a flexible and theoretically grounded alternative to the probabilistic discriminator in traditional GANs.
   
2. Motivation and Placement in Literature: The approach is well-motivated, bridging GANs and energy-based models. However, the paper should explicitly cite "Improving Generative Adversarial Networks with Denoising Feature Matching" as related work, given the conceptual overlap in using reconstruction-based objectives for stabilization.
3. Scientific Rigor: The theoretical results are robust, with clear proofs demonstrating convergence properties. Empirical results are extensive, covering multiple datasets and configurations. The inclusion of high-resolution ImageNet results is particularly impactful, as few GAN models achieve this level of scalability.
Additional Feedback for Improvement:
1. Related Work: While the paper mentions relevant literature, the omission of "Improving Generative Adversarial Networks with Denoising Feature Matching" is a gap. This work shares similarities in using reconstruction-based objectives and should be discussed to contextualize EBGAN's contributions.
2. Ablation Studies: The paper could benefit from more detailed ablation studies to isolate the contributions of the autoencoder architecture and the hinge loss. For example, how does EBGAN perform with a standard binary classifier as the discriminator?
3. Clarity on Hyperparameters: The choice of the energy margin \(m\) is critical for training stability, as noted in the appendices. Including a more detailed discussion in the main text about how \(m\) affects performance would improve accessibility for readers.
4. Comparison with State-of-the-Art: While the paper compares EBGAN with DCGAN, additional comparisons with more recent GAN variants (e.g., StyleGAN or BigGAN) would strengthen its empirical claims.
Questions for the Authors:
1. How does the performance of EBGAN compare to state-of-the-art GANs on metrics like FID or IS for high-resolution datasets?
2. Could the authors elaborate on the computational trade-offs of using an autoencoder as the discriminator compared to a binary classifier?
3. The paper mentions the use of a "repelling regularizer" (PT). Could the authors provide more quantitative results on how this term impacts mode collapse and diversity in generated samples?
Overall, this paper makes a strong theoretical and empirical contribution to GAN research, and I recommend its acceptance with minor revisions to improve clarity and contextualization.