Review
Summary of Contributions
The paper presents a semi-supervised learning framework for variational autoencoders (VAEs) that integrates graphical models into the recognition network. The authors propose a novel training procedure that allows for disentangled and interpretable latent representations by embedding structural constraints into the recognition model. The framework is evaluated on several datasets, including MNIST, SVHN, Yale B faces, and a custom multi-MNIST dataset, demonstrating its ability to disentangle latent variables with minimal supervision. The paper also introduces a stochastic computation graph implementation in Torch, facilitating the specification of graphical models for a wide variety of tasks.
Decision: Reject
While the paper is well-written and presents interesting results, it lacks significant methodological novelty. The proposed approach is heavily inspired by Kingma et al.'s semi-supervised VAE framework, with only minor extensions, such as the auxiliary variable trick and the separation of generative model learning from inference. The experimental results, while strong, are limited to relatively simple datasets, which restricts the broader applicability of the method. The lack of substantial theoretical or methodological contributions makes it difficult to justify acceptance at a competitive conference.
Supporting Arguments
1. Lack of Novelty: The core methodology closely resembles existing work by Kingma et al. (2014), with only incremental modifications. While the auxiliary variable trick and the separation of generative model learning from inference are useful, they do not constitute a significant advancement in the field.
2. Limited Scope of Experiments: The experiments are restricted to simple datasets such as MNIST, SVHN, and Yale B faces. While these datasets are standard benchmarks, they do not adequately demonstrate the scalability or robustness of the proposed framework for more complex, real-world tasks.
3. Overstated Claims: The paper overstates the contrast between deep generative models and graphical models. Additionally, the use of the term "structure" is debatable, as the focus is more on disentangling and semanticizing latent representations rather than imposing explicit structural constraints.
Suggestions for Improvement
1. Clarify Novelty: Clearly articulate the methodological contributions beyond existing semi-supervised VAE frameworks. Highlight how the proposed approach differs from and improves upon prior work.
2. Expand Experiments: Evaluate the framework on more complex datasets or tasks to demonstrate its scalability and generalizability. For example, experiments on large-scale datasets or applications in natural language processing or reinforcement learning could strengthen the paper.
3. Refine Terminology: Avoid overstating the contrast between deep generative models and graphical models. Consider using more precise terminology to describe the focus on disentangling and semanticizing latent representations.
4. Title Revision: The title is too general and does not accurately reflect the specific contributions of the paper. A more descriptive title could better capture the essence of the work.
Questions for the Authors
1. How does the proposed framework perform on more complex datasets or real-world applications? Have you considered evaluating it on large-scale datasets like ImageNet or tasks like video generation?
2. Can you provide a more detailed comparison of your method with Kingma et al. (2014) and other related works? What specific advantages does your approach offer?
3. How sensitive is the framework to the choice of supervision rate (r) and the dimensionality of the latent space? Are there any guidelines for selecting these hyperparameters?
In conclusion, while the paper presents an interesting application of semi-supervised VAEs with graphical models, the lack of methodological novelty and limited experimental scope make it unsuitable for acceptance in its current form. The authors are encouraged to address these issues and resubmit to a future venue.