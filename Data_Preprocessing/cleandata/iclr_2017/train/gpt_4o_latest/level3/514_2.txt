Review
Summary of Contributions
This paper introduces a permutation-equivariant layer for deep learning with set-structured data, achieved through parameter-sharing. The authors propose a simple and computationally efficient method to handle permutation invariance and equivariance in neural networks, with linear complexity in set size. The paper demonstrates the utility of this approach in supervised and semi-supervised tasks, including MNIST digit summation, point cloud classification, anomaly detection, and red-shift estimation. The authors claim that their method generalizes well to various applications and provides insights into learned features in set-structured data.
Decision: Reject
While the paper addresses an important problem and proposes a novel approach, it falls short in several critical areas, including clarity of exposition, experimental rigor, and justification of claims. The lack of state-of-the-art results and insufficient connection to existing literature further weaken the paper's contributions.
Supporting Arguments
1. Clarity and Motivation: The concept of invariance is well-motivated, but the exposition is overly abstract and lacks a concrete running example to guide the reader. The definition of "set invariance" is unclear and does not explicitly relate to permutations of input/output dimensions. Additionally, the relationship between invariance and function composition is not addressed, leaving a gap in theoretical understanding.
   
2. Experimental Rigor: The experiments lack strong baselines for anomaly detection, and the results for point cloud classification are not state-of-the-art. The desirability of set invariance for point cloud classification is not well-justified, and the paper does not explore alternative methods such as classical set kernels or data augmentation techniques.
3. Theoretical Gaps: The paper does not clarify whether invariance at the layer level guarantees invariance at the network level. This omission raises questions about the robustness of the proposed approach.
4. Minor Issues: There are unclear definitions (e.g., "parameters shared within a relation"), naming inconsistencies (e.g., "set convolution"), and missing connections to symmetric function theory. Additionally, Example 2.2 contains a potential error (|S|=5 for left-right and up-down symmetry), which undermines the theoretical rigor.
Suggestions for Improvement
1. Clarity and Accessibility: Provide a concrete running example throughout the paper to illustrate key concepts. Clearly define "set invariance" and explicitly relate it to permutations of input/output dimensions. Address the relationship between invariance and function composition.
2. Experimental Design: Include strong baselines for anomaly detection and compare against state-of-the-art methods for point cloud classification. Justify the desirability of set invariance for specific tasks and explore alternative approaches, such as classical set kernels or data augmentation.
3. Theoretical Depth: Clarify whether invariance at the layer level guarantees invariance at the network level. Strengthen the connection to symmetric function theory and address compositionality of structures.
4. Minor Revisions: Correct errors in Example 2.2 and ensure consistency in terminology (e.g., "set convolution"). Improve the exposition of technical details to make the paper more accessible to a broader audience.
Questions for the Authors
1. Can you provide a concrete example to clarify the definition of "set invariance" and its relationship to input/output permutations?
2. Does invariance at the layer level guarantee invariance at the network level? If so, please provide a formal proof or justification.
3. Why is set invariance desirable for point cloud classification, and how does it compare to alternative approaches such as data augmentation or classical set kernels?
4. Can you include stronger baselines for anomaly detection and compare your results against state-of-the-art methods for point cloud classification?
By addressing these issues, the paper could significantly improve its clarity, rigor, and impact.