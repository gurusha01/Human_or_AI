The paper presents a novel approach to unsupervised video segmentation for identifying intermediate steps in real-world tasks and defining interpretable reward functions using feature selection. This method leverages pre-trained deep visual features to extract dense, incremental reward functions from a small number of demonstrations. The authors demonstrate the efficacy of their approach on complex robotic tasks, such as door opening and liquid pouring, showing that the learned reward functions can be used to train reinforcement learning (RL) agents without retraining the visual representations. The paper is commendable for addressing the challenging problem of learning from limited visual demonstrations and for proposing interpretable reward functions that align with subtask transitions.
Decision: Reject
While the paper introduces a promising direction, it lacks sufficient experimental rigor and comparative analysis to support its claims. The primary reasons for rejection are:  
1. Insufficient placement in the literature: The paper does not reference or compare its method to prior works on video segmentation (e.g., shot detection) or other unsupervised segmentation techniques, which limits its contextualization and novelty assessment.  
2. Limited baselines and experimental validation: The comparison is restricted to a simple random baseline, which is insufficient to evaluate the significance of the proposed method. Additionally, the hypothesis regarding sparse independent features is not experimentally validated, and comparisons to simpler models are missing.
Supporting Arguments:  
The paper's strengths lie in its innovative use of pre-trained visual features for unsupervised segmentation and reward function learning. The method achieves comparable performance to fully instrumented approaches for complex tasks, which is a significant achievement given the constraints of limited demonstrations. The interpretability of the reward functions and their alignment with subtask transitions are valuable contributions to the field of RL. However, the lack of references to prior video segmentation methods and the absence of robust baselines undermine the paper's claims of novelty and generalizability. Furthermore, the experimental results, while promising, are limited in scope and fail to validate key hypotheses, such as the utility of sparse feature subsets.
Additional Feedback for Improvement:  
1. Comparative Analysis: Include comparisons to state-of-the-art video segmentation methods and other reward learning approaches to better contextualize the contributions.  
2. Experimental Validation: Validate the hypothesis on sparse independent features by comparing the proposed method to simpler models and analyzing the impact of feature selection on performance.  
3. Baselines: Incorporate stronger baselines, such as supervised segmentation methods or alternative unsupervised approaches, to provide a more robust evaluation of the method's effectiveness.  
4. Broader Demonstrations: Extend the scope of experiments to include more diverse tasks and unconstrained demonstrations to evaluate the generalizability of the approach.  
5. Ablation Studies: Conduct ablation studies to assess the contribution of individual components, such as the segmentation algorithm and feature selection method, to the overall performance.
Questions for the Authors:  
1. How does the proposed segmentation method compare to existing unsupervised video segmentation techniques in terms of accuracy and computational efficiency?  
2. Can the method handle demonstrations with significant variations in task execution or environment (e.g., different lighting conditions, object appearances)?  
3. What is the rationale for choosing the specific pre-trained network and feature layers, and how sensitive is the method to this choice?  
4. How does the method perform when the number of demonstrations is further reduced or when demonstrations are noisy?  
5. Could the approach be extended to multi-modal inputs (e.g., combining visual and tactile data)?
In conclusion, while the paper introduces a creative and potentially impactful approach, it requires more rigorous analysis, broader comparisons, and experimental validation to substantiate its claims and demonstrate its significance in the field.