The paper presents a novel reparameterization of LSTMs that incorporates batch normalization (BN) into both input-to-hidden and hidden-to-hidden transitions, addressing prior challenges in applying BN to recurrent neural networks (RNNs). This work demonstrates that batch normalization can mitigate internal covariate shift in RNNs, leading to faster convergence and improved generalization across a variety of sequential tasks. The authors provide empirical evidence of the effectiveness of their approach on diverse datasets, including sequential MNIST, character-level language modeling, and question-answering tasks. Notably, the proposed BN-LSTM outperforms standard LSTMs in terms of both training efficiency and task performance, particularly for long-term dependency modeling, as shown in the permuted MNIST (pMNIST) experiment.
Decision: Accept
The paper makes a significant contribution to the field of RNN optimization by successfully extending batch normalization to recurrent settings, a problem that has historically been fraught with challenges. The key reasons for acceptance are:
1. Novelty and Impact: The proposed method addresses a longstanding limitation in RNN training, offering a practical and theoretically grounded solution that improves both convergence speed and generalization.
2. Empirical Rigor: The experiments are comprehensive, spanning multiple datasets and tasks, and the results consistently support the claims made by the authors.
Supporting Arguments
1. Clear Problem Definition and Motivation: The paper identifies the challenges of applying BN to RNNs, particularly the hidden-to-hidden transitions, and provides a well-motivated solution. The authors also situate their work effectively within the existing literature, contrasting their findings with prior studies that reported negative results for recurrent BN.
   
2. Empirical Validation: The experiments convincingly demonstrate the advantages of BN-LSTM over standard LSTMs. For instance, the pMNIST results highlight the model's ability to capture long-term dependencies, a critical challenge in sequential tasks. Training curves further illustrate significant reductions in convergence time, an important practical benefit.
3. Theoretical Insights: The analysis of gradient flow and the impact of unit variance on tanh derivatives is a valuable addition, providing a deeper understanding of why the proposed approach works. The inclusion of a "toy task" (Figure 1b) enhances interpretability.
Suggestions for Improvement
1. Ablation Studies: While the paper discusses the importance of proper initialization for BN parameters, an ablation study quantifying the impact of initialization choices (e.g., varying Î³) would strengthen the argument.
2. Comparison with Contemporary Methods: Although the authors reference related works (e.g., Ba et al., 2016), a more detailed experimental comparison with these methods would provide additional context for the improvements achieved.
3. Clarity in Generalization: The discussion on generalizing BN statistics for sequences longer than those seen during training could be expanded with more empirical evidence or visualizations.
Questions for the Authors
1. How does the proposed BN-LSTM perform on tasks with extremely long sequences, such as video processing or document-level NLP tasks? Are there any scalability concerns?
2. Could the authors elaborate on the computational overhead introduced by BN in recurrent settings? How does this trade off with the observed speedup in convergence?
3. Have the authors explored alternative normalization techniques (e.g., layer normalization) in the same recurrent setting for comparison?
In conclusion, this paper is a meaningful contribution to the field of RNN optimization, offering both theoretical insights and practical benefits. With minor clarifications and additional comparisons, it could further solidify its impact.