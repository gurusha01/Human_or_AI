The paper presents a novel quantitative evaluation framework for decoder-based generative models using Annealed Importance Sampling (AIS) to estimate log-likelihoods. This approach is validated using Bidirectional Monte Carlo (BDMC), which bounds estimation errors and provides a rigorous method for evaluating generative models. The authors demonstrate that AIS outperforms Kernel Density Estimation (KDE) in accuracy and enables fine-grained comparisons between models such as GANs, GMMNs, and VAEs. Notably, empirical results reveal that IWAE underestimates log-likelihoods on binarized MNIST by at least 1 nat, highlighting the utility of AIS. The publicly available evaluation framework contributes to the research community and provides insights into GAN behavior, including evidence that GANs do not memorize training data but fail to capture important data distribution modes.
Decision: Accept
The paper makes a significant contribution by addressing a critical challenge in generative modeling: the reliable evaluation of log-likelihoods. The use of AIS, validated by BDMC, represents a scientifically rigorous and well-motivated approach that is superior to existing methods like KDE. The empirical results are robust, and the insights into the behavior of different generative models are valuable for advancing the field. The public release of the evaluation framework further enhances the paper's impact.
Supporting Arguments:
1. The problem tackled—accurate log-likelihood evaluation for decoder-based generative models—is both relevant and timely, given the growing interest in generative modeling.
2. The methodology is well-placed in the literature, building on established techniques like AIS and BDMC while addressing their application to decoder-based models.
3. The claims are supported by rigorous empirical evidence. The validation of AIS using BDMC ensures the reliability of the results, and the comparisons between models are insightful and scientifically grounded.
Additional Feedback:
1. The paper could improve clarity regarding the choice of varying numbers of examples (100, 1000, 10,000) from different data sources (train, validation, test, or generated data). A more consistent experimental setup would strengthen the results.
2. The observation that AIS is slower than AIS+encoder in Figure 2c requires further explanation. Clarifying whether the number of intermediate distributions is consistent across experiments would help.
3. The use of only 16 AIS chains is lower than in prior literature. The authors should discuss the impact of this choice on confidence intervals, particularly in Table 2.
4. The BDMC gap of 10 nats for GAN-50 is significantly larger than for other models. Providing an intuition for this discrepancy would enhance understanding.
5. Minor issues include the lack of references to Table 1 in the text, unclear dataset details in Figures 2(a) and 2(c), and a typo in Figure 3 caption ("GMMN-10" should be "GMMN-50").
Questions for Authors:
1. Why were different numbers of examples used across experiments, and how does this variability affect the results?
2. Can you elaborate on why AIS+encoder is faster than AIS alone in Figure 2c? Is this related to the choice of intermediate distributions?
3. How does the choice of 16 AIS chains compare to prior work, and what is its impact on the confidence intervals reported in Table 2?
4. What factors contribute to the unusually large BDMC gap for GAN-50, and how might this affect the interpretation of the results?
Overall, the paper is a strong contribution to the field and merits acceptance, provided the authors address the noted concerns to improve clarity and rigor.