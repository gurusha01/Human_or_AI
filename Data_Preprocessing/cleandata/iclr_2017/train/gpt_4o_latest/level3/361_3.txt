Review of "Bayesian Neural Networks for Learning Curve Prediction"
Summary of Contributions
This paper proposes a Bayesian neural network (BNN) architecture to predict learning curve values during model training, with the aim of improving hyperparameter optimization. The key novelty lies in leveraging information across all tested hyperparameter settings, unlike prior work (e.g., Domhan et al., 2015), which extrapolated from individual learning curves. The authors introduce a specialized "learning curve layer" to improve predictions and evaluate two MCMC methods for inference: stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian Monte Carlo (SGHMC). The method demonstrates strong performance in predicting early learning curve behavior and is applied to extend Hyperband, showing faster convergence to optimal hyperparameter configurations compared to baseline methods.
Decision: Accept
The paper presents a well-motivated and novel contribution to the field of hyperparameter optimization. Its approach is scientifically rigorous, with empirical results that support the claims. However, some areas require clarification and additional exploration to strengthen the work further.
Supporting Arguments
1. Novelty and Motivation: The paper addresses a clear limitation in existing learning curve prediction methods by modeling dependencies across hyperparameter configurations. This is a significant step forward, as it enables better predictions for unobserved configurations and enhances resource allocation during optimization.
2. Empirical Validation: The proposed method outperforms baselines in multiple tasks, particularly in early-stage learning curve prediction. The extension of Hyperband with the BNN model shows practical utility in hyperparameter optimization, achieving faster convergence to optimal configurations.
3. Scientific Rigor: The paper provides a thorough comparison with baselines, including Domhan et al. (2015), Gaussian processes, and random forests. The use of multiple datasets and cross-validation strengthens the reliability of the results.
Areas for Improvement
1. Unexplored Regime: The paper does not evaluate scenarios where some training set curves are fully observed. Testing this regime could further highlight the advantages of information sharing across curves.
2. Timing Concerns: Training the BNN takes 20-60 seconds per epoch, which can amount to over 1.5 hours for 100 epochs. While the authors argue that this overhead does not harm performance, a more detailed analysis of the trade-off between computational cost and prediction accuracy would be valuable.
3. Prediction Accuracy: The paper does not explicitly evaluate the accuracy of individual components, such as bounding the asymptotic value of learning curves. Clarifying this would provide deeper insights into the model's strengths and limitations.
4. Minor Issues: 
   - Figure 1 axes should specify "validation accuracy."
   - Clarify the term "LastSeenValue" in Figure 6 and its absence as a baseline elsewhere.
   - Justify the choice of using only 5 basis functions and discuss whether increasing this number would improve flexibility.
Questions for the Authors
1. How does the model perform when some training set curves are fully observed? Does this further demonstrate the benefits of shared information across curves?
2. Could the computational overhead of training the BNN be reduced, for example, by using fewer epochs or alternative inference methods?
3. How sensitive is the model to the choice of the number of basis functions? Would increasing this number improve prediction flexibility, especially for more complex learning curves?
Conclusion
Overall, this paper makes a strong contribution to the field of learning curve prediction and hyperparameter optimization. While there are areas for improvement, the novelty, empirical rigor, and practical utility of the proposed method justify its acceptance. Addressing the raised concerns and questions would further strengthen the impact of the work.