Review of the Paper
Summary of Contributions
This paper introduces a principled framework for nonparametrically learning activation functions in deep neural networks, addressing a relatively unexplored area in deep learning. By allowing activation functions to be learned during training rather than being predefined as hyperparameters, the proposed method expands the functional capacity of neural networks. The authors provide a theoretical justification for their approach, including generalization bounds derived using algorithmic stability. Empirically, the framework demonstrates up to a 15% relative improvement in test performance on image recognition datasets, such as MNIST and CIFAR-10, compared to baseline methods. The paper also introduces a two-stage training process to stabilize training for convolutional networks and leverages Fourier basis expansion for activation function modeling. The work is well-written, employs novel theoretical techniques, and makes a significant contribution to the field.
Decision: Accept
The paper should be accepted due to its strong theoretical foundation, meaningful empirical results, and novel contribution to the field of deep learning. The ability to learn activation functions nonparametrically is a promising direction that could have broad implications for neural network design.
Supporting Arguments
1. Novelty and Impact: The paper tackles the underexplored problem of learning activation functions during training, which could significantly enhance the adaptability and generalization of neural networks. The use of Fourier basis expansion for activation functions is innovative and well-justified.
2. Theoretical Rigor: The authors provide a solid theoretical framework, including provable generalization bounds for networks with nonparametric activation functions. The use of algorithmic stability to derive these bounds is a notable strength.
3. Empirical Validation: The experimental results are compelling, showing consistent improvements across multiple datasets and architectures. The 15% relative improvement in test performance on MNIST and CIFAR-10 is particularly noteworthy.
4. Clarity and Writing: The paper is well-structured and clearly written, making it accessible to a broad audience. The Appendix provides detailed proofs and additional clarifications, addressing potential gaps in the main text.
Suggestions for Improvement
1. Intuition for Theorem 4.7: While the Appendix provides a detailed proof, the main text could benefit from a more intuitive explanation of Theorem 4.7. This would make the theoretical contributions more accessible to readers who may not delve into the Appendix.
2. Comparison with Alternatives: The paper briefly mentions polynomial basis expansion as an alternative to Fourier basis expansion but does not provide empirical comparisons. Including such comparisons would strengthen the justification for the chosen approach.
3. Ablation Studies: While the two-stage training process is shown to improve performance, an ablation study quantifying its impact on different datasets and architectures would provide deeper insights.
4. Scalability: The paper does not discuss the computational overhead introduced by learning nonparametric activation functions. A discussion of scalability and potential trade-offs in training time would be valuable.
Questions for the Authors
1. How does the choice of Fourier basis expansion compare empirically to other basis functions, such as polynomial or wavelet expansions, in terms of performance and training stability?
2. Could the two-stage training process be eliminated or simplified in future work? Are there specific conditions under which single-stage training might suffice?
3. How does the computational cost of learning nonparametric activation functions scale with network size and dataset complexity? Are there practical limitations to applying this method to very large-scale datasets or architectures?
In conclusion, this paper makes a significant contribution to the field of deep learning by introducing a novel framework for learning activation functions. Its theoretical and empirical strengths outweigh the minor areas for improvement, making it a strong candidate for acceptance.