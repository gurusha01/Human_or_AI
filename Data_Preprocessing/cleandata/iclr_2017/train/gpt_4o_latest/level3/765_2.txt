Review of the Paper
Summary
This paper proposes an extension to video frame prediction methods by incorporating reward prediction, enabling the joint learning of system dynamics and reward structures in reinforcement learning (RL). The authors evaluate their approach on five Atari games, demonstrating accurate cumulative reward prediction up to 200 frames. The key contribution lies in showing that a single latent representation can support both state and reward prediction, which could pave the way for more data-efficient RL in high-dimensional environments. The paper is well-written, focused, and provides clear explanations of its methodology, results, and implications.
Decision: Reject
While the paper is technically sound and well-executed, the contribution is incremental and lacks novelty. The deterministic nature of reward-state links in Atari games makes reward prediction relatively straightforward, and the results, while positive, are unsurprising. Furthermore, the paper does not explore more impactful directions, such as using artificial samples for training or integrating planning methods like Monte-Carlo tree search, which could significantly enhance the utility of the proposed approach.
Supporting Arguments
1. Strengths:
   - The paper is well-motivated and clearly positioned within the literature, building on prior work in video frame prediction and model-based RL.
   - The methodology is rigorous, with a solid experimental setup and detailed quantitative and qualitative analyses.
   - The results demonstrate that the proposed approach achieves accurate reward prediction within 50 steps, which is a promising step for model-based RL.
2. Weaknesses:
   - The contribution is limited by the deterministic reward dynamics in Atari games, making the results less generalizable to more complex, stochastic environments.
   - Reward decoding from latent state representations is relatively straightforward and does not represent a significant advancement in the field.
   - The paper misses opportunities to explore impactful extensions, such as augmenting training with artificial samples (e.g., Dyna-style methods) or integrating planning techniques like Monte-Carlo tree search, which could reduce real-world sample requirements and improve performance.
Additional Feedback
To improve the paper and its impact, the authors should consider the following:
1. Broader Evaluation: Test the approach in environments with stochastic dynamics or sparse rewards to demonstrate its robustness and generalizability.
2. Augmented Training: Incorporate artificial sample generation during training to explore the potential for reducing real-world data requirements.
3. Planning Integration: Investigate the use of the learned model for planning, such as through Monte-Carlo tree search, to highlight its utility in decision-making tasks.
4. Comparison with Baselines: Provide a more comprehensive comparison with other model-based and model-free RL approaches to contextualize the results better.
Questions for the Authors
1. How does the proposed method perform in environments with stochastic state transitions or sparse rewards? Would the deterministic nature of Atari games limit its applicability?
2. Have you considered integrating planning methods, such as Monte-Carlo tree search, to leverage the learned model for decision-making? If not, what are the challenges in doing so?
3. Could the model be extended to generate artificial samples for training, akin to the Dyna framework? If so, what would be the expected impact on data efficiency?
In conclusion, while the paper presents a technically sound approach, its contribution is incremental, and the results are limited by the deterministic nature of the test environments. Addressing the suggested improvements could significantly enhance the paper's impact and relevance.