Review
Summary of Contributions
This paper addresses the theoretical question of why deep neural networks are often preferred over shallow networks by providing rigorous bounds on the number of neurons required for function approximation. Specifically, it demonstrates that for a large class of piecewise smooth functions, shallow networks require exponentially more neurons than deep networks to achieve the same approximation error. The authors establish these results for networks using ReLU, binary step, or a combination of these activation functions. The paper is well-written, with clear arguments, detailed proofs, and a logical structure. The theoretical contributions are significant, with tight upper and lower bounds provided for both univariate and multivariate cases, and the results are extended to compositions, multiplications, and linear combinations of functions. This work advances our understanding of the expressive power of deep networks and provides a solid theoretical foundation for their empirical success.
Decision: Accept
The paper makes a significant theoretical contribution to the field of deep learning by rigorously quantifying the advantages of depth in neural networks. The proofs are well-constructed, and the results are scientifically rigorous. The clarity of the writing and the breadth of the results further strengthen the case for acceptance. However, some points could be improved or clarified, as outlined below.
Supporting Arguments
1. Problem Significance: The paper tackles a fundamental question in deep learning and provides strong theoretical evidence for the superiority of deep networks in terms of neuron efficiency for function approximation. This is a highly relevant and timely topic in the field.
2. Scientific Rigor: The results are derived with mathematical precision, and the proofs are detailed and easy to follow. The authors provide both upper and lower bounds, ensuring the results are tight.
3. Clarity and Organization: The paper is well-organized, with a clear progression from definitions and problem statements to results and proofs. The inclusion of corollaries and remarks helps contextualize the results.
Suggestions for Improvement
1. Necessity of Binary Step Units: While the authors use binary step units in their constructions, it is unclear whether they are strictly necessary for the results. The reviewer suggests exploring whether similar results can be achieved using only ReLU activations. This would simplify the architecture and align better with modern deep learning practices.
2. Example of Function Necessitating Polynomial Hidden Units: The paper would benefit from a concrete example of a piecewise smooth function that requires polynomially many hidden units in shallow networks. This would provide additional intuition and practical relevance to the theoretical results.
3. Empirical Validation: While the paper is theoretical, a small empirical demonstration comparing shallow and deep networks for specific functions could enhance the impact and accessibility of the work.
4. Discussion of Related Work: The authors briefly mention related work but could provide a more detailed comparison, particularly with Yarotsky (2016), to highlight the novelty and differences in their approach.
Questions for the Authors
1. Could the results be extended to other activation functions, such as sigmoid or tanh, which are commonly used in practice?
2. Are there practical implications for network design, such as guidelines for choosing depth and width based on the properties of the target function?
3. Can the necessity of binary step units be relaxed, and if so, how would this affect the proofs and results?
In conclusion, this paper makes a strong theoretical contribution to the understanding of deep neural networks and is well-suited for acceptance at the conference. Addressing the above suggestions would further strengthen the work.