Review of "Generative Adversarial Parallelization (GAP)"
Summary of Contributions
This paper proposes a novel framework, Generative Adversarial Parallelization (GAP), to address mode collapse and training instability in GANs. By training multiple GANs simultaneously and periodically swapping their discriminators (or generators), the method aims to reduce overfitting between specific generator-discriminator pairs and improve mode coverage. The authors introduce an updated Generative Adversarial Metric (GAM-II) to evaluate the performance of GAP-trained models and provide experimental results on synthetic datasets, MNIST, CIFAR-10, and LSUN. The paper claims that GAP enhances mode coverage, generalization, and model quality, while being adaptable to various GAN architectures.
Decision: Reject  
Key Reasons:
1. Insufficient Experimental Validation: While the paper addresses an important problem, the experimental results lack rigor and fail to convincingly demonstrate the superiority of GAP over existing methods. Key metrics like inception scores and human evaluations are missing, and comparisons with prior work are limited.
2. Unclear Motivation for GAM-II: The proposed GAM-II metric lacks clear justification as a reliable measure of generative model quality. Its correlation with established metrics like inception scores or human evaluations is unexplored.
Supporting Arguments
1. Limited Comparisons with Baselines: The paper does not adequately compare GAP with simpler alternatives, such as training multiple GANs independently or using regularization techniques like dropout. It remains unclear whether the observed improvements are unique to GAP or could be achieved through less complex methods.
2. Unclear Necessity of Swapping: The necessity of swapping discriminators and its impact on training dynamics are not well-justified. The sensitivity analysis on swapping frequency is limited, and the paper does not explore whether simpler approaches (e.g., random initialization of multiple GANs) could yield similar results.
3. Ambiguity in Results: The interpretation of Figure 6 regarding generalization and training costs is unclear. Additionally, the qualitative results (e.g., t-SNE visualizations) are insufficient to draw strong conclusions about mode coverage improvements.
Suggestions for Improvement
1. Stronger Experimental Validation: The authors should provide quantitative comparisons with prior methods using established metrics like inception scores and human evaluations. Exploring correlations between GAM-II, inception scores, and human evaluations would strengthen the case for GAM-II as a valid metric.
2. Baseline Comparisons: Include experiments comparing GAP with simpler baselines, such as training multiple GANs independently or applying regularization techniques like dropout. This would help clarify the necessity of swapping and the unique contributions of GAP.
3. Clarify Swapping Mechanism: Provide a more detailed analysis of the impact of swapping on training dynamics. For instance, how does swapping affect convergence speed, stability, and computational cost? Is swapping necessary, or could similar results be achieved through alternative mechanisms?
4. Improve Presentation of Results: Clearly explain the interpretation of key figures (e.g., Figure 6) and provide additional visualizations or quantitative metrics to support claims about mode coverage and generalization.
Questions for the Authors
1. How does GAM-II correlate with established metrics like inception scores or human evaluations? Can you provide evidence to justify its reliability?
2. Is swapping discriminators/generators the only way to achieve the observed improvements? Could simpler approaches, such as dropout or independent training of multiple GANs, achieve similar results?
3. Why does GAPC4 (hybrid GAP) not outperform GAPD4 or GAPG4? Could this indicate limitations in the proposed framework?
4. How does the computational cost of GAP compare to training a single GAN or multiple GANs independently? Is the additional complexity justified by the observed improvements?
Final Remarks
The paper addresses an important problem in GAN training and proposes a creative solution. However, the lack of compelling experimental validation, insufficient comparisons with prior methods, and unclear motivation for key design choices make it difficult to assess the significance of the proposed approach. With stronger empirical evidence and clearer justifications, this work could make a valuable contribution to the field.