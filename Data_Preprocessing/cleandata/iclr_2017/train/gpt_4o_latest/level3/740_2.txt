Review of "ParMAC: A Distributed Model for Training Nested, Nonconvex Models"
Summary of Contributions
This paper introduces ParMAC, a distributed computation model for the Method of Auxiliary Coordinates (MAC), designed to optimize nested, nonconvex machine learning models. ParMAC leverages data and model parallelism to train on large datasets across distributed systems while minimizing communication overhead. The authors implement ParMAC using MPI to train binary autoencoders and demonstrate its scalability and speedup on large datasets, achieving nearly perfect parallel efficiency for up to 128 processors. The paper also provides a theoretical analysis of parallel speedup and convergence guarantees, along with extensions for fault tolerance, data shuffling, and streaming data. The experimental results highlight ParMAC's potential for large-scale machine learning tasks, particularly in binary hashing for image retrieval.
Decision: Reject
While the paper addresses an important problem and proposes a promising distributed optimization framework, it suffers from several critical issues that hinder its clarity, rigor, and impact. The primary reasons for rejection are: (1) lack of focus and unclear novelty, and (2) insufficient empirical and theoretical support for its claims.
Supporting Arguments
1. Lack of Focus: The paper attempts to address multiple aspects (e.g., ParMAC's generality, speedup analysis, binary autoencoder implementation) but fails to deliver a single, clear message. The ParMAC algorithm itself is novel, but the theoretical remarks and extensions dilute the focus and should be moved to the appendix. A more concise narrative centered on ParMAC's core contributions would improve the paper's impact.
2. Related Work: The paper omits key references like COCOA and AIDE, which are more aligned with its practical objectives than some of the cited works. This weakens the positioning of ParMAC within the broader literature on distributed optimization.
3. Clarity Issues: Section 3 is particularly unclear, with undefined terms (e.g., "submodel") and confusing notation. The explanation of initialization, outputs, and data distribution is insufficient, making it difficult to understand the algorithm's implementation details.
4. Experimental Limitations: The experiments focus solely on binary autoencoders, with no comparisons to alternative distributed optimization methods. This limits the generalizability of the results to other models. Additionally, the claims of "nearly perfect speedups" are not rigorously supported, as the theoretical analysis relies on fudge factors rather than precise measurements.
5. Misleading Claims: The conclusion makes strong claims about parallel speedup, convergence analysis, and unchanged MAC properties, but these are not adequately justified. For instance, the convergence guarantees are stated to hold for differentiable layers but are not well-supported for the nondifferentiable layers used in binary autoencoders.
Suggestions for Improvement
1. Focus on a Clear Message: The paper should focus on the ParMAC algorithm and its practical contributions, moving theoretical extensions and less relevant discussions (e.g., the MAC-EM analogy) to the appendix.
2. Strengthen Related Work: Include comparisons to relevant distributed optimization frameworks like COCOA and AIDE to better position ParMAC within the literature.
3. Improve Clarity: Define all terms and notations clearly, particularly in Section 3. Provide a more detailed explanation of the algorithm's initialization, outputs, and data distribution.
4. Expand Experiments: Compare ParMAC to alternative distributed optimization methods and evaluate its performance on a broader range of models beyond binary autoencoders.
5. Justify Claims: Provide stronger empirical and theoretical evidence to support claims about speedup, convergence, and general applicability.
Questions for the Authors
1. How does ParMAC compare to other distributed optimization frameworks like COCOA or AIDE in terms of scalability and communication overhead?
2. Can you clarify the definition and role of "submodels" in Section 3? How are they initialized and updated?
3. What specific assumptions are required for the convergence guarantees to hold for nondifferentiable layers?
4. Why were binary autoencoders chosen as the primary application, and how generalizable are the results to other nested models?
In summary, while ParMAC has potential, the paper requires significant revisions to improve its focus, clarity, and empirical rigor.