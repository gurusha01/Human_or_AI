Review of the Paper
Summary of Contributions
This paper proposes a novel recurrent neural network architecture, the Rotation Plane Doubly Orthogonal Recurrent Neural Network (RP-DORNN), which aims to address the vanishing and exploding gradient problems in training RNNs on long sequences. The authors leverage orthogonal transformations to ensure forward and backward norm preservation, thus theoretically eliminating these gradient issues. The architecture introduces a fully multiplicative transition, parameterized by rotation planes, to achieve this. The paper validates the proposed model on a simplified memory copy task, demonstrating its ability to handle dependencies up to 5,000 timesteps, outperforming prior approaches. The work also highlights the potential for future extensions, such as combining orthogonal transitions with nonlinearities or LSTM-like mechanisms.
Decision: Reject
While the paper presents an interesting and theoretically grounded approach, it falls short in providing sufficient experimental evidence and comparative analysis to support its claims. The limited scope of the experiments and the lack of clarity on the model's expressiveness and universality are significant weaknesses.
Supporting Arguments for the Decision
1. Insufficient Experimental Validation: The experimental section is underwhelming and limited to a simplified version of the memory copy task. While the results for sequences up to 5,000 timesteps are promising, the task itself is overly simplistic and does not demonstrate the model's applicability to real-world or more complex sequence modeling problems. The authors acknowledge the need for expanded experiments but have not yet addressed this critical gap.
2. Expressiveness and Universality Concerns: The paper does not adequately address whether the proposed architecture remains a universal approximator when scaled with sufficient hidden dimensions and layers. Additionally, there is no direct comparison of the expressiveness of RP-DORNN to equivalent models without orthogonal matrices, given the same parameter count. This leaves open questions about the practical utility of the proposed approach.
3. Training Instability: The authors report instability during training for longer sequences, which undermines the robustness of the proposed method. While they provide a possible explanation, no concrete solutions or mitigations are proposed.
Suggestions for Improvement
1. Expand Experimental Validation: The paper would benefit from evaluating the model on a broader range of tasks, such as language modeling, speech recognition, or other real-world sequence modeling benchmarks. Comparative experiments with state-of-the-art models like LSTMs, GRUs, and uRNNs are essential to demonstrate the practical advantages of RP-DORNN.
2. Address Expressiveness and Universality: The authors should provide theoretical or empirical evidence to confirm whether the proposed architecture is a universal approximator. Additionally, a direct comparison of the expressiveness of RP-DORNN against non-orthogonal models with the same parameter count is necessary.
3. Clarify Success Criteria: The success criteria for the experiments are unclear. The authors should define specific metrics (e.g., accuracy, loss reduction, or convergence time) and provide a detailed analysis of the results.
4. Mitigate Training Instability: The paper should explore strategies to address the reported training instability for longer sequences. For example, regularization techniques, adaptive learning rates, or alternative parameterizations of orthogonal matrices could be investigated.
Questions for the Authors
1. How does the proposed model compare to existing architectures (e.g., LSTMs, GRUs, uRNNs) in terms of expressiveness and parameter efficiency for the same task?
2. Can the authors provide theoretical guarantees or empirical evidence that RP-DORNN is a universal approximator for sequence modeling tasks?
3. What specific metrics were used to evaluate the success of the experiments, and how do these metrics generalize to more complex tasks?
4. Have the authors explored alternative parameterizations of orthogonal matrices to mitigate training instability for longer sequences?
In conclusion, while the paper introduces an innovative approach to addressing vanishing and exploding gradients, the lack of experimental rigor and clarity on key theoretical aspects limits its impact. Addressing these issues could significantly strengthen the paper for future submissions.