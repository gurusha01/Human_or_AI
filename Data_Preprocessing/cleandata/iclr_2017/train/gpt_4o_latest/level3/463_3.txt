Review
Summary of Contributions
This paper addresses the critical problem of supervised learning with noisy labels, a practical challenge in applied machine learning. The authors propose a novel approach that replaces the traditional Expectation-Maximization (EM) algorithm with a noise adaptation layer implemented as an additional softmax layer. This method optimizes the same likelihood function as EM but integrates it directly into the neural network framework, enabling simultaneous optimization of the noise model and classifier. The proposed method is evaluated on the MNIST dataset, demonstrating improved performance over baseline methods and prior approaches, including Reed et al.'s noise-robust training strategies. The paper also introduces a sparse variant of the noise adaptation layer to address scalability issues for large class sets, with promising results on CIFAR-100.
Decision: Reject
While the paper introduces an interesting and practical method for handling noisy labels, several critical limitations prevent its acceptance at this stage. The primary reasons for rejection are the limited evaluation scope and concerns about the generalizability of the proposed architecture.
Supporting Arguments
1. Limited Dataset Evaluation: The experiments are primarily conducted on MNIST, a simple and unrealistic dataset for modern machine learning tasks. While CIFAR-100 is briefly mentioned, the results lack sufficient depth and analysis. The absence of evaluations on more complex, real-world datasets raises concerns about the method's applicability to practical scenarios.
   
2. Architectural Simplicity and Scalability: The proposed method relies on a two-hidden-layer architecture, which may not generalize well to deeper networks commonly used in practice. Additionally, the use of two softmax layers increases training complexity, and while a sparse variant is proposed, its effectiveness is not thoroughly validated for large-scale problems.
3. Unclear Rationale for Serialized Softmax Layers: The decision to use serialized softmax layers instead of alternative architectures, such as parallel losses, is not well-justified. This design choice warrants further theoretical or experimental support.
Suggestions for Improvement
1. Expand Dataset Evaluation: The authors should evaluate their method on more complex and diverse datasets, such as ImageNet or real-world noisy datasets, to demonstrate its scalability and robustness in practical scenarios.
2. Address Generalization to Deeper Architectures: The paper should explore the performance of the proposed method on deeper neural networks and provide insights into its scalability and compatibility with state-of-the-art architectures.
3. Clarify Design Choices: The rationale for using serialized softmax layers should be better explained, potentially with comparisons to alternative architectures, such as parallel loss functions or other noise-robust designs.
4. Theoretical Analysis of Scalability: While the sparse variant is a step in the right direction, a more detailed analysis of its computational complexity and performance trade-offs is necessary, especially for large-scale datasets with high class cardinality.
Questions for the Authors
1. Why was MNIST chosen as the primary dataset for evaluation, given its simplicity and limited relevance to real-world noisy label scenarios?
2. How does the proposed method perform on deeper architectures, such as ResNet or Transformer-based models?
3. Can the authors provide a comparison of serialized softmax layers with alternative architectures, such as parallel loss functions, in terms of both performance and computational efficiency?
4. How does the sparse variant of the noise adaptation layer scale with increasing class cardinality, and what are the trade-offs in terms of accuracy?
In conclusion, while the paper presents a promising approach to handling noisy labels, its limited evaluation scope and lack of generalization to practical scenarios hinder its acceptance. Addressing these issues in a revised submission would significantly strengthen the paper.