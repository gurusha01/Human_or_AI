Review of the Paper
Summary of Contributions
This paper addresses the problem of adversarial robustness in machine learning models, particularly focusing on scaling adversarial training to large datasets like ImageNet. The authors make several contributions: (1) they provide practical recommendations for scaling adversarial training to large models, (2) they empirically demonstrate that adversarial training improves robustness against single-step attacks but is less effective against iterative attacks, (3) they identify the novel "label leaking" phenomenon, and (4) they explore the transferability of adversarial examples and the impact of model capacity on robustness. The paper is well-written and organized into two parts: adversarial training on ImageNet and an empirical study of various aspects of adversarial training.
Decision: Accept
The paper is recommended for acceptance due to its valuable empirical contributions and practical insights into adversarial training. While it lacks theoretical depth in some areas, the observations and findings are significant enough to guide future research in adversarial robustness.
Supporting Arguments
1. Problem Relevance and Novelty: The paper tackles the critical problem of adversarial robustness in large-scale models, which is underexplored compared to smaller datasets like MNIST and CIFAR-10. The novel observation of "label leaking" is particularly noteworthy and adds to the understanding of adversarial training dynamics.
   
2. Empirical Contributions: The extensive experiments on ImageNet and Inception-V3 provide actionable insights, such as the trade-offs between clean accuracy and adversarial robustness, the effects of model capacity, and the transferability of adversarial examples. These findings are valuable for practitioners and researchers working on adversarial defenses.
3. Clarity and Structure: The paper is well-structured, making it easy to follow the methodology and results. The division into two parts—scaling adversarial training and empirical studies—ensures a logical flow.
Suggestions for Improvement
1. Experiments Without Clean Samples: The paper does not explore adversarial training without clean samples, which would provide a more comprehensive understanding of its effectiveness. Including such experiments would strengthen the empirical contributions.
   
2. Theoretical Depth: While the empirical results are strong, the paper lacks theoretical explanations for some observations, such as the relationship between model capacity and robustness or the inverse relationship between transferability and attack success. Adding theoretical insights or hypotheses would enhance the paper's impact.
3. Iterative Attack Robustness: The paper acknowledges that adversarial training is less effective against iterative attacks but does not propose solutions or alternatives. Exploring methods to address this limitation would make the work more complete.
4. Label Leaking Analysis: While the label leaking phenomenon is well-documented, the paper could delve deeper into its implications for adversarial training and propose mitigation strategies.
Questions for the Authors
1. Can you provide insights into why adversarial training without clean samples was not included? Do you anticipate any significant differences in robustness?
2. Have you considered integrating theoretical frameworks to explain the observed trends, such as the capacity-robustness relationship or transferability dynamics?
3. Given the limitations of adversarial training against iterative attacks, do you see potential in combining adversarial training with other defense mechanisms (e.g., ensemble methods or input preprocessing)?
4. Could the label leaking phenomenon be mitigated by modifying the adversarial example generation process, such as using pseudo-labels instead of true labels?
Overall, this paper makes a strong empirical contribution to the field of adversarial robustness and provides a solid foundation for future work. Addressing the suggested improvements would further enhance its impact.