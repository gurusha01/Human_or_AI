The paper proposes a novel optimization method for training neural networks by leveraging mollified objective functions and continuation methods. The authors introduce the concept of mollifiers, which smooth the loss surface by injecting noise during training, and gradually anneal the noise to transition from simpler to more complex objectives. This approach is particularly relevant for highly non-convex optimization problems, such as training deep neural networks with challenging architectures like deep ReLU nets, LSTMs, and GRUs. The paper demonstrates improvements in optimization and generalization on tasks such as MNIST, CIFAR-10, and language modeling, and establishes connections between mollification, curriculum learning, and stochastic noise injection.
Decision: Reject
While the paper introduces an original conceptual contribution and demonstrates promising results, it suffers from significant clarity and methodological issues that hinder its acceptance. The main reasons for rejection are:
1. Lack of Clarity in Key Equations: Equations 4-7, which are central to understanding the weak gradient \( g \) and its relationship to mollification, are poorly explained. This lack of clarity makes it difficult to fully grasp the theoretical underpinnings of the approach, despite the equivalence established in Equation 8 and Section 2.3.
   
2. Overly Complex Practical Implementation: The proposed annealing scheme (Algorithm 1) involves nine successive steps for unit activation, which appears unnecessarily complicated. This complexity may limit the practical applicability of the method, especially in large-scale tasks.
Supporting Arguments:
- The introduction of generalized mollifiers to enable sophisticated annealing effects is innovative, but the paper does not adequately explore simpler models or basic mollifiers to highlight clear optimization advantages over heuristic perturbation schemes. This omission weakens the empirical validation of the proposed approach.
- The counterintuitive annealing behavior described in Section 4, where noise is added initially, is not sufficiently justified. While this behavior may have theoretical merit, the paper does not provide enough evidence or discussion to address potential skepticism from practitioners.
- The experimental results are promising but lack sufficient comparison with simpler baselines or alternative noise injection methods. For example, the paper could have included experiments with basic mollifiers to isolate the benefits of the proposed method.
Suggestions for Improvement:
1. Clarify Theoretical Foundations: Provide a more detailed explanation of Equations 4-7, particularly the interpretation of the weak gradient \( g \). Include illustrative examples or visualizations to aid understanding.
   
2. Simplify Practical Implementation: Consider streamlining Algorithm 1 to make it more accessible for practitioners. Reducing the number of steps or providing a simplified version for common use cases would enhance usability.
3. Analyze Simpler Models: Include experiments with basic mollifiers and simpler architectures to demonstrate the optimization advantages more clearly. This would strengthen the empirical validation of the proposed method.
4. Address Counterintuitive Behavior: Provide a more thorough analysis of the counterintuitive annealing behavior observed in Section 4. Explain why this behavior occurs and how it benefits optimization.
Questions for the Authors:
1. Can you provide additional clarification or intuition behind Equations 4-7, particularly the role of the weak gradient \( g \)?
2. How does the proposed method compare to simpler noise injection techniques in terms of computational efficiency and performance?
3. Have you considered alternative annealing schedules or simpler implementations of Algorithm 1? If so, how do they perform relative to the current approach?
In summary, while the paper presents an interesting and original idea, its lack of clarity, overly complex implementation, and insufficient empirical validation prevent it from being ready for acceptance. Addressing these issues would significantly improve the paper's quality and impact.