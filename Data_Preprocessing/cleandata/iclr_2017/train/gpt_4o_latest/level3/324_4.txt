The paper presents a method for pruning low-weight filters from convolutional neural networks (CNNs) to reduce computational costs (FLOPs) and memory usage while maintaining accuracy. The proposed approach introduces structured sparsity by removing entire filters and their associated feature maps, which avoids the need for specialized sparse convolution libraries and allows the use of standard dense BLAS libraries. The authors demonstrate the effectiveness of their method on VGG-16 and ResNet architectures using CIFAR-10 and ImageNet datasets, achieving up to 34% FLOP reduction for VGG-16 and 38% for ResNet-110 without significant accuracy loss. The simplicity of the approach and its ease of implementation are highlighted as key strengths.
Decision: Accept.  
The paper is well-motivated, novel, and provides solid experimental results. The authors address a significant problem in deep learning—reducing computational overhead in CNNs—using a straightforward yet effective method. The results are rigorously validated on widely used datasets and architectures, making the contribution relevant and impactful.
Supporting Arguments:  
1. Novelty and Practicality: The method introduces structured sparsity by pruning filters rather than individual weights, which simplifies implementation and avoids the challenges of irregular sparsity. This makes the approach practical for real-world applications where efficient inference is critical.  
2. Experimental Rigor: The authors conduct extensive experiments on both simple (VGG-16) and complex (ResNet) architectures, demonstrating consistent performance improvements. The sensitivity analysis of layers and the comparison with alternative pruning strategies (e.g., random and activation-based pruning) further strengthen the validity of the results.  
3. Simplicity: The one-shot pruning and retraining strategy is computationally efficient and avoids the complexity of iterative fine-tuning, making the method accessible to practitioners.
Additional Feedback:  
1. Impact on Transfer Learning: While the paper focuses on reducing FLOPs and memory usage, it does not evaluate the effect of pruning on transfer learning tasks. Given the widespread use of pre-trained models in real-world applications, this is a critical omission. Future work should explore whether pruned models retain their generalization ability when fine-tuned on new tasks.  
2. Clarity in Figures: Figure 2 has inconsistent model naming (e.g., "VGG-16" vs. "VGG_BN"), which could confuse readers. Ensure consistent terminology throughout the paper.  
3. Broader Applicability: The paper could benefit from a discussion on how the proposed method might integrate with other compression techniques, such as quantization or knowledge distillation, to achieve further efficiency gains.
Questions for the Authors:  
1. How does the proposed pruning method affect the transferability of pruned models to downstream tasks?  
2. Could the pruning strategy be extended to other architectures, such as transformers or hybrid CNN-transformer models?  
3. How does the method perform when combined with other compression techniques, such as quantization or low-rank approximations?
Overall, the paper makes a valuable contribution to the field of model compression and is well-suited for acceptance. Addressing the feedback and questions could further enhance its impact.