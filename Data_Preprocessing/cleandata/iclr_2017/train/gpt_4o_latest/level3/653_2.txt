Review of the Paper
Summary of Contributions
The paper investigates the impact of mini-batch size on the convergence of Stochastic Gradient Descent (SGD) and its asynchronous variant (ASGD) for non-convex optimization problems. It provides theoretical analysis showing that larger mini-batches and increasing the number of learners in ASGD result in slower convergence, even when the total number of training samples processed remains constant. The authors extend prior work on convex optimization to the non-convex setting and provide experimental results to validate their theoretical findings. The paper also highlights the inherent inefficiencies in exploiting parallelism in gradient-based optimization methods, offering insights into the trade-offs between convergence speed and parallelization.
Decision: Reject
The primary reasons for rejection are:
1. Limited Novelty: While the paper extends known results to the non-convex case, these findings are already well-documented in existing literature, including deep learning textbooks. The theoretical contributions lack sufficient originality to warrant acceptance.
2. Inconsistencies in Analysis: Equation (4) uses the same \( K \) for different mini-batch sizes, which is inconsistent with the definition of \( S = MK \). This undermines the rigor of the theoretical framework.
Supporting Arguments
1. Limited Novelty: The extension of convergence guarantees from convex to non-convex settings is incremental. The results align with prior work, such as Ghadimi & Lan (2013), and do not provide significant new insights. The discussion on the inefficiency of large mini-batches and parallelism is well-known and lacks groundbreaking contributions.
2. Theoretical Flaws: The inconsistency in Equation (4) raises concerns about the validity of the theoretical results. Using the same \( K \) for different mini-batch sizes contradicts the definition of \( S = MK \) and could lead to misleading conclusions.
3. Experimental Presentation: Figures 1 and 2 focus on test error rather than the training objective, which makes it difficult to directly assess convergence speed. Highlighting the training objective would better align the experiments with the theoretical claims.
Suggestions for Improvement
1. Clarify Theoretical Assumptions: Address the inconsistency in Equation (4) and ensure that all assumptions and definitions are rigorously followed. This will strengthen the theoretical contributions.
2. Improve Experimental Design: Modify Figures 1 and 2 to focus on the training objective rather than test error. This will provide a clearer comparison of convergence speed across different mini-batch sizes and ASGD configurations.
3. Broaden Scope: Explore more advanced optimization techniques, such as momentum-based methods or variance reduction, to determine whether the observed inefficiencies persist. This could make the findings more impactful and relevant.
4. Highlight Practical Implications: Provide a more detailed discussion of how the theoretical results translate to practical settings, such as recommendations for mini-batch sizes or distributed training configurations.
Questions for the Authors
1. How do you justify using the same \( K \) for different mini-batch sizes in Equation (4), given the definition \( S = MK \)?
2. Could you provide additional experiments or analysis to validate whether the observed inefficiencies hold for optimization methods beyond vanilla SGD, such as Adam or SGD with momentum?
3. Why did you choose to focus on test error in Figures 1 and 2 instead of the training objective? Would re-plotting these figures with the training objective change the conclusions?
While the paper tackles an important problem, the limited novelty, theoretical inconsistencies, and presentation issues prevent it from making a significant contribution to the field. Addressing these concerns could improve the paper's impact and clarity.