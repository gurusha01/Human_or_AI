Review of the Paper
Summary of Contributions
This paper provides an empirical study of the invariance, equivariance, and equivalence properties of convolutional neural network (CNN) representations under various data augmentation strategies. The authors examine how data augmentation influences these properties across 70 CNNs trained on two datasets, using nine types of transformations. The study highlights that CNNs trained with data augmentation exhibit increased invariance and equivariance, even for transformations outside the training range. Additionally, the paper proposes a novel loss function to enhance equivariance, which balances classification performance and representation robustness. The authors also measure pairwise representation distances between CNNs trained with different transformations, revealing that similar augmentations yield more similar representations. While the study is comprehensive in its scope, the novelty and practical utility of the findings are limited.
Decision: Reject
The primary reasons for rejection are (1) the lack of surprising or impactful insights from the empirical results, and (2) insufficient evidence that the proposed method for improving equivariance leads to meaningful performance gains. Additionally, the computational overhead introduced by the proposed loss function is significant, with limited justification for its practical utility.
Supporting Arguments
1. Novelty and Utility: While the systematic study of data augmentation's effects on invariance and equivariance is novel, the results largely confirm well-understood intuitions. For example, the observation that data augmentation increases invariance and that similar augmentations lead to more similar representations is expected. The findings do not provide actionable insights or significant advancements in understanding CNN representations.
   
2. Proposed Method: The proposed loss function to improve equivariance is interesting but lacks compelling evidence of substantial performance improvements. For instance, the improvements on the RVL-CDIP dataset are marginal (~0.5%), and results on ILSVRC are mixed. Furthermore, the additional computational cost introduced by the method is not justified by the modest gains.
3. Scientific Rigor: While the experiments are extensive, there are inaccuracies in the paper's claims. For example, the assertion that equivariance linearizes view manifolds is incorrect, as linear representations can generate nonlinear manifolds. Additionally, the use of the term "equivariance" in Equation 2 is misleading and should be clarified as "non-equivariance."
Suggestions for Improvement
1. Clarify Terminology: The term "equivariance" in Equation 2 should be revised to "non-equivariance" to avoid confusion, as low values indicate higher equivariance. Similarly, the use of "paradigm" in the context of Equation 2 is unclear and should be rephrased.
   
2. Mathematical Notation: Address errors in mathematical notation, such as the potential need for inversion in the definition of \( x'_{ij} \) to avoid double application of transformations.
3. Theoretical Claims: Revisit and correct inaccuracies in claims about equivariance and data augmentation. For example, the claim about linearizing view manifolds should be revised or removed.
4. Computational Efficiency: Provide a more detailed analysis of the computational overhead introduced by the proposed loss function and explore ways to reduce it.
5. Practical Relevance: Strengthen the practical implications of the findings. For example, demonstrate how the proposed method can improve performance in real-world applications or provide insights that are not already well understood.
Questions for the Authors
1. Can you provide additional evidence or experiments to demonstrate the practical utility of the proposed loss function? For example, how does it perform on tasks beyond classification, such as object detection or segmentation?
   
2. How sensitive are the results to the choice of datasets? Would the findings generalize to other domains, such as medical imaging or time-series data?
3. Can you quantify the computational cost of the proposed loss function relative to standard training procedures? How does this trade-off compare to the observed performance improvements?
4. How do you address the claim that equivariance linearizes view manifolds, given that linear representations can still generate nonlinear manifolds?
By addressing these issues, the paper could provide more impactful contributions to the understanding and practical utility of invariance and equivariance in CNN representations.