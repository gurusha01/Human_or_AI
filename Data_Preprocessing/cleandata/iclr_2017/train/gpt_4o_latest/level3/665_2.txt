Review of "MS MARCO: A Large-Scale Dataset for Machine Reading Comprehension"
This paper introduces MS MARCO, a large-scale dataset designed for machine reading comprehension (RC) and question answering (QA). The dataset is unique in that it is derived from real anonymized user queries issued to the Bing search engine, with answers curated by human judges. Unlike existing datasets, MS MARCO emphasizes real-world complexity by including noisy, unstructured web data and requiring synthesis or reasoning across multiple passages for some answers. The authors aim to address shortcomings in prior datasets, such as synthetic or overly simplistic questions, and provide a resource to advance research in RC and QA.
Decision: Reject
While the paper makes a valuable contribution by introducing a dataset grounded in real-world queries, it lacks sufficient analytic rigor and fails to provide detailed comparisons with existing datasets. These shortcomings limit its utility for the research community in its current form.
Supporting Arguments:
1. Strengths:
   - The use of real user query logs offers a significant advantage over synthetic datasets, as it captures authentic user intent and the variability of natural language.
   - The dataset's scale and inclusion of human-generated answers make it a valuable resource for training and benchmarking QA models.
   - The authors highlight the dataset's potential to inspire research in reasoning and synthesis, which are critical challenges in RC and QA.
2. Weaknesses:
   - The paper does not provide a detailed quantitative comparison between MS MARCO and existing datasets like SQuAD or WikiQA. For example, metrics such as question complexity, answer diversity, or reasoning requirements are not analyzed.
   - Concerns about the dataset's reliance on current search engine capabilities are not addressed. Many questions may be answerable through simple text matching, reducing the need for deeper reasoning.
   - The paper lacks a statistical breakdown of question types (e.g., exact matches, paraphrasing, synthesis) and their distribution, which would help assess the dataset's complexity.
   - The experimental results are limited and do not convincingly demonstrate the dataset's ability to advance state-of-the-art QA models.
   - The paper appears rushed, with insufficient formal analysis of the dataset's challenges and opportunities.
Suggestions for Improvement:
1. Conduct a detailed statistical analysis of the dataset, including the complexity of questions and the reasoning required to answer them. Categories such as exact matches, paraphrasing, synthesis, and external knowledge requirements should be quantified.
2. Provide a comprehensive comparison with existing datasets, highlighting where MS MARCO excels and where it falls short.
3. Address the potential bias introduced by current search engine capabilities, such as the prevalence of simplified questions.
4. Include more robust experimental results, demonstrating how MS MARCO enables advancements in QA models compared to other datasets.
5. Clarify how the dataset's challenges (e.g., noisy data, reasoning across passages) are expected to push the boundaries of current QA systems.
Questions for the Authors:
1. What percentage of questions in MS MARCO require reasoning or synthesis across multiple passages, as opposed to simple text matching?
2. How does the dataset handle ambiguous or subjective questions, and how are multiple valid answers evaluated?
3. Can you provide more details on the feedback and auditing process used to ensure the quality of human-generated answers?
In conclusion, while MS MARCO has the potential to be a valuable dataset for RC and QA, the paper does not adequately support its claims or provide the necessary analysis to justify its impact. A more rigorous evaluation and comparison with existing datasets are essential for acceptance.