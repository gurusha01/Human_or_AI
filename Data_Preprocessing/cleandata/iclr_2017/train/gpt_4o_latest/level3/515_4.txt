Review of the Paper
Summary of Contributions
The paper introduces Exponential Machines (ExM), a novel machine learning model that uses the Tensor Train (TT) format to model all feature interactions of every order in a scalable way. The TT format enables the representation of an exponentially large tensor of parameters in a compact form, allowing for better generalization and computational efficiency. A key contribution is the development of a stochastic Riemannian optimization algorithm, which exploits the geometric structure of the TT factorization to train the model effectively. The authors demonstrate the potential of ExM on synthetic data and small datasets, showing that it achieves state-of-the-art performance for high-order interactions and performs comparably to high-order Factorization Machines on the MovieLens 100K dataset. The paper also proposes an initialization strategy based on linear models, which addresses convergence issues during training.
Decision: Reject
While the paper introduces an innovative approach with strong theoretical underpinnings, the experimental evaluation is insufficient to support the claims of general applicability and scalability. The lack of structured experiments, comprehensive performance metrics, and comparisons with established baselines limits the paper's impact and confidence in its conclusions.
Supporting Arguments for Decision
1. Strengths:
   - The use of the TT format to model high-order interactions is novel and well-motivated. The ability to control model complexity via TT-rank is a significant advantage.
   - The proposed Riemannian optimization algorithm is a valuable contribution, potentially applicable to other tensor-based models.
   - The theoretical analysis of the model's inference complexity and initialization strategy is rigorous and insightful.
2. Weaknesses:
   - Experimental Section: The experiments lack structure and fail to provide sufficient details about the datasets and evaluation metrics. For example, the UCI datasets and synthetic data are described briefly, but the experimental methodology and data preparation steps are not clearly outlined.
   - Comparisons with Baselines: The paper does not comprehensively compare ExM with relevant baselines such as Factorization Machines (FM), kernel SVMs, and neural networks across all datasets. The absence of test set generalization curves and performance metrics like AUC or accuracy for most experiments is a significant drawback.
   - Scalability: While the authors claim that the model can scale to large datasets, the experiments are restricted to small-scale problems. The scalability of the Riemannian optimization algorithm on real-world, large-scale datasets remains unproven.
   - Representativeness: The reliance on synthetic data and small datasets reduces the generalizability of the results. The MovieLens 100K dataset is the only real-world dataset used, and even there, the performance comparison with baselines is limited.
Suggestions for Improvement
1. Experimental Design:
   - Provide a structured experimental section with clear descriptions of datasets, preprocessing steps, and evaluation metrics.
   - Include test set generalization curves and compare ExM with established baselines (e.g., Factorization Machines, kernel SVMs, neural networks) across all datasets.
   - Evaluate the model on larger, real-world datasets to demonstrate scalability.
2. Performance Metrics:
   - Report standard metrics such as accuracy, AUC, and F1-score to facilitate comparison with other methods.
   - Include ablation studies to analyze the impact of TT-rank and initialization strategies on performance.
3. Scalability:
   - Conduct experiments on larger datasets to validate the claim that the Riemannian optimization algorithm scales to real-world problems.
   - Discuss the computational trade-offs of using the TT format compared to other tensor decompositions like CP-format.
4. Clarity:
   - Improve the organization of the experimental section to make it easier for readers to follow the methodology and results.
   - Provide more detailed explanations of the datasets and their relevance to the problem being addressed.
Questions for the Authors
1. How does the proposed model compare to Factorization Machines and kernel SVMs in terms of training time and memory usage on larger datasets?
2. Can the Riemannian optimization algorithm handle sparse data efficiently, or are there plans to extend it for such cases?
3. How sensitive is the model's performance to the choice of TT-rank, and what guidelines can be provided for selecting this hyperparameter in practice?
4. Why were test set generalization curves and comparisons with baselines omitted for some datasets, and can these be included in a future revision?
In conclusion, while the paper presents an innovative approach with promising theoretical contributions, the lack of robust experimental validation and comprehensive comparisons with baselines prevents it from meeting the standards for acceptance at this time. Addressing the outlined weaknesses could significantly strengthen the paper.