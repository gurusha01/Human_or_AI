The paper proposes replacing the widely used tanh activation function with sinusoidal (sin) activation functions in neural networks, challenging conventional wisdom that periodic activation functions are difficult to train. The authors provide a theoretical analysis of why sinusoidal activations can lead to challenging optimization landscapes, including shallow local minima and flat gradients. They also present experimental results showing that sinusoidal activations perform comparably to tanh on standard tasks like MNIST classification and outperform tanh on algorithmic tasks where periodicity is beneficial. The paper claims that sinusoidal activations can accelerate learning and achieve higher accuracy in certain contexts, encouraging further exploration of periodic functions in neural networks.
Decision: Reject
The primary reasons for rejection are:  
1. Insufficient Evidence for Generalization: The experiments are limited to two tasks (MNIST and algorithmic tasks), and the paper does not provide compelling evidence that sinusoidal activations generalize well to broader, real-world applications.  
2. Marginal Gains on Standard Tasks: The slight improvement on MNIST (98.0% to 98.1%) is not significant enough to justify replacing well-established activation functions like tanh, especially given the theoretical challenges of training with sinusoidal activations.
Supporting Arguments:
1. Limited Scope of Experiments: While the algorithmic tasks highlight the potential of sinusoidal activations for periodic problems, these tasks are toy examples and do not represent the diversity of real-world machine learning problems. The MNIST results, though positive, show that the periodicity of the activation function is largely unused, raising questions about its practical utility.  
2. Theoretical Challenges: The paper acknowledges that sinusoidal activations introduce optimization difficulties, such as shallow local minima and flat gradients, which could hinder training in more complex tasks. While the authors propose initialization strategies to mitigate these issues, the broader applicability of these strategies remains unexplored.  
3. Lack of Rigorous Comparison: The paper does not compare sinusoidal activations against other modern activation functions like ReLU or Swish, which are widely used in state-of-the-art models. This limits the ability to assess the practical competitiveness of the proposed approach.
Suggestions for Improvement:
1. Broader Experimental Validation: Extend the experiments to more diverse datasets and tasks, including real-world problems like image recognition (e.g., CIFAR-10, ImageNet) or natural language processing (e.g., sentiment analysis, machine translation).  
2. Comparison with Modern Activations: Include comparisons with other activation functions beyond tanh, such as ReLU, Leaky ReLU, and Swish, to better position sinusoidal activations in the broader activation function landscape.  
3. Analysis of Computational Overhead: Investigate the computational cost of training networks with sinusoidal activations compared to standard activations, as this could impact their practical adoption.  
4. Clarify Use Cases: Provide a clearer discussion of the specific types of problems where sinusoidal activations are expected to excel, supported by empirical evidence.  
Questions for the Authors:
1. How do sinusoidal activations perform on more complex, real-world datasets and tasks?  
2. Have you explored hybrid architectures that combine sinusoidal and monotonic activation functions to mitigate the optimization challenges?  
3. Can you provide a more detailed analysis of the computational cost and training time for networks using sinusoidal activations?  
In summary, while the paper introduces an intriguing idea and provides some promising results, the limited scope of experiments and lack of strong evidence for generalization prevent it from making a compelling case for acceptance. Further work addressing these limitations could significantly strengthen the contribution.