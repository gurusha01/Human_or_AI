Review
Summary of Contributions
This paper introduces a novel framework for unsupervised representation learning based on the infomax principle, specifically designed for large-scale neural populations. The authors propose a hierarchical infomax approach to find a good initialization point for optimizing mutual information (MI) between inputs and neural responses. The method is robust across complete, overcomplete, and undercomplete bases and demonstrates significant improvements in training speed and robustness compared to existing methods like ICA, sparse RBM, and dictionary learning. The paper also highlights the biological plausibility of the model and its potential extensions to deep learning architectures.
Decision: Reject
While the paper demonstrates a promising framework and provides extensive experimental results, it suffers from significant issues that hinder its suitability for acceptance in a conference proceeding. The two main reasons for this decision are: (1) the paper's excessive length (21 pages excluding the appendix), which makes it challenging to follow and unsuitable for a conference format, and (2) a lack of clarity in the theoretical reasoning, particularly regarding the use of \(I(X; \hat{Y})\) as an upper bound for \(I(X; R)\), which is counterintuitive given the typical use of lower bounds in optimization problems.
Supporting Arguments
1. Excessive Length: The paper is overly detailed, with 21 pages excluding the appendix. While the technical depth is commendable, the verbosity detracts from the core contributions and makes it difficult to identify the key insights. A condensed version focusing on the main contributions, methodology, and results would be more appropriate for a conference audience.
2. Unclear Theoretical Justification: The use of \(I(X; \hat{Y})\) as an upper bound for \(I(X; R)\) is not well-justified and appears counterintuitive. Typically, lower bounds are used in information-theoretic optimization to ensure tractability and guarantee improvement. The authors need to provide a clearer explanation or empirical validation for this choice.
3. Experimental Results: While the experimental results are extensive and demonstrate the method's efficacy, the presentation is cluttered and could benefit from better organization. For instance, the comparisons with other methods like ICA and sparse RBM are insightful but buried under dense technical details.
Suggestions for Improvement
1. Condense the Paper: Reduce the paper's length by focusing on the main contributions and moving detailed derivations and proofs to the appendix. This will make the paper more accessible to readers and suitable for a conference format.
2. Clarify Theoretical Reasoning: Provide a more detailed and intuitive explanation for the use of \(I(X; \hat{Y})\) as an upper bound for \(I(X; R)\). If this choice is critical to the framework, include empirical evidence to support its validity.
3. Notation and Readability: Simplify the notation where possible and ensure consistency throughout the paper. For example, clarify why \(K_1\) is used instead of \(N\) in Eq. (2.11) and verify if \(H(X)\) should disappear in Eq. (2.12).
4. Improve Organization: Divide Section 3 into subsections to improve readability and better highlight the experimental results. This will help readers quickly locate key findings.
5. Minor Corrections: Use "â‰ˆ" in Eq. (2.11) if it approximates Eq. (2.8), as suggested. This will improve the mathematical precision of the paper.
Questions for the Authors
1. Can you provide a more intuitive explanation for the use of \(I(X; \hat{Y})\) as an upper bound for \(I(X; R)\)? How does this choice impact the optimization process and final results?
2. Why is \(K_1\) used instead of \(N\) in Eq. (2.11)? Does this reflect a specific biological or computational constraint?
3. Should \(H(X)\) indeed disappear in Eq. (2.12), or is this an oversight?
4. How does the proposed method scale with larger datasets or more complex neural architectures? Are there any limitations in terms of computational cost or convergence?
In summary, while the paper presents a novel and promising approach, its excessive length, unclear theoretical justifications, and organizational issues make it unsuitable for acceptance in its current form. Addressing these concerns would significantly enhance the paper's clarity and impact.