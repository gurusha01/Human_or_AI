Review
Summary of Contributions
This paper proposes a novel Markov Chain Monte Carlo (MCMC) sampling process for generative autoencoders, addressing the mismatch between the learned latent distribution \( P̂(Z) \) and the prior \( P(Z) \). The authors demonstrate that MCMC sampling can improve the quality of generated samples by iteratively refining latent representations, particularly when the learned latent distribution deviates from the prior. Additionally, the paper extends this approach to denoising generative autoencoders, revealing the benefits of the denoising criterion through MCMC sampling. The proposed method is straightforward and can be applied to existing generative autoencoders without modifying their architectures. The authors validate their claims qualitatively through visual comparisons of generated samples and interpolations.
Decision: Reject  
The primary reasons for rejection are the lack of quantitative evaluation and the unclear description of the proposed model and methodology. These issues significantly hinder the ability to assess the scientific rigor and practical impact of the work.
Supporting Arguments for Decision
1. Lack of Quantitative Evaluation:  
   The paper relies heavily on qualitative results (e.g., visual comparisons of generated samples) without providing quantitative metrics to evaluate the performance of the proposed MCMC sampling process. Metrics such as Fréchet Inception Distance (FID) or Inception Score (IS) are standard in generative modeling and would have provided a more objective basis for comparison. Without such metrics, it is difficult to ascertain whether the proposed method offers meaningful improvements over existing approaches.
2. Unclear Model Description:  
   The description of the MCMC sampling process and its integration with generative autoencoders is overly dense and lacks sufficient clarity. Key aspects, such as the practical implementation of the transition operator and the convergence properties of the Markov chain, are not explained in a manner accessible to a broader audience. This lack of clarity makes it challenging to reproduce or build upon the work.
3. Unjustified Design Choices:  
   The use of additive noise in the input for denoising generative autoencoders is not adequately justified. The authors do not explain why this specific corruption process was chosen or how it impacts the performance of the proposed MCMC sampling method.
Additional Feedback for Improvement
1. Quantitative Benchmarks:  
   The paper would benefit greatly from quantitative evaluations on standard semi-supervised learning or generative modeling benchmarks, such as CIFAR-10 or CelebA. Metrics like FID, IS, or reconstruction error should be included to provide a more rigorous assessment of the method's performance.
2. Clarity and Accessibility:  
   The authors should aim to simplify and clarify the description of the MCMC sampling process. Including pseudocode or a flow diagram could help readers better understand the methodology. Additionally, the theoretical claims (e.g., convergence properties) should be supported by more intuitive explanations or practical examples.
3. Broader Context and Motivation:  
   While the paper discusses related work, it does not sufficiently motivate the need for MCMC sampling in generative autoencoders compared to alternative techniques. A discussion of the trade-offs between the proposed method and other sampling or regularization approaches would strengthen the paper.
4. Justification of Design Choices:  
   The authors should provide a rationale for the use of additive Gaussian noise in the denoising criterion and discuss its impact on the learned latent distribution. Exploring alternative noise models or corruption processes could also be valuable.
Questions for the Authors
1. How does the proposed MCMC sampling process compare quantitatively to existing sampling techniques in terms of sample quality and diversity?  
2. What is the computational overhead introduced by the MCMC sampling process, and how does it scale with the number of iterations?  
3. Could the authors provide more details on the choice of the corruption process for denoising generative autoencoders? How does it affect the learned latent distribution and the quality of generated samples?  
4. Have the authors considered testing the method on semi-supervised learning benchmarks to demonstrate its broader applicability?  
By addressing these issues, the paper could significantly improve its clarity, rigor, and impact.