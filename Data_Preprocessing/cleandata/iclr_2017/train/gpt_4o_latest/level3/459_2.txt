The paper introduces a novel tensor factorization framework for deep multi-task learning (MTL), addressing the limitations of traditional shallow MTL approaches and user-defined sharing strategies in deep neural networks (DNNs). By leveraging tensor decomposition techniques such as Tucker and Tensor Train, the proposed method enables automatic, data-driven sharing of parameters across tasks at every layer of a DNN, achieving a balance between shared and task-specific representations. The framework is validated on diverse datasets, including MNIST, AdienceFaces, and Omniglot, demonstrating superior performance over single-task learning (STL) and user-defined MTL (UD-MTL), particularly in low-data regimes.
Decision: Accept
Key reasons for acceptance include:  
1. Novelty and Contribution: The paper generalizes matrix-based MTL to tensor-based approaches, enabling end-to-end knowledge sharing in DNNs. This is a significant advancement over existing methods, which either rely on shallow models or require manual architecture design.  
2. Empirical Rigor: The experimental results convincingly demonstrate the efficacy of the proposed methods across multiple datasets and problem settings (homogeneous and heterogeneous MTL). The framework consistently outperforms STL and UD-MTL, particularly in scenarios with limited training data, showcasing its robustness and practical utility.
Supporting Arguments:  
The paper is well-motivated, addressing a critical gap in the MTL literature by automating the sharing structure in DNNs. The use of tensor factorization is both innovative and scientifically grounded, with clear connections to prior work in shallow MTL and tensor decomposition. The experiments are thorough, covering a range of datasets and tasks, and the results are presented with appropriate baselines and ablation studies. The authors also provide insights into the learned sharing structures, further strengthening the paper's contributions.
Additional Feedback for Improvement:  
1. Model Size vs. Performance: While the paper briefly discusses model capacity, a more detailed analysis of the trade-off between model size and performance would enhance the discussion. This is particularly relevant for practical applications where computational resources are constrained.  
2. Unbalanced Data: The paper could address the model's performance on unbalanced datasets more explicitly. For example, how does the proposed framework handle tasks with significantly fewer examples?  
3. Pretraining vs. Random Initialization: The reliance on pretraining DNNs for rank selection raises questions about scalability and generalizability. A deeper exploration of whether random initialization suffices, and under what conditions, would be valuable.  
4. Applications in Related Fields: Including a discussion of potential applications in related domains, such as natural language processing or reinforcement learning, could broaden the paper's impact.
Questions for the Authors:  
1. How does the proposed method perform when tasks are highly unbalanced in terms of data availability? Are there mechanisms to mitigate potential biases?  
2. Could you elaborate on the computational overhead introduced by tensor factorization, particularly for large-scale datasets or architectures?  
3. Is the framework sensitive to the choice of tensor decomposition method (e.g., Tucker vs. Tensor Train), and how should practitioners select between them?  
4. Can the method be extended to unsupervised or semi-supervised multi-task learning scenarios?
In conclusion, the paper makes a strong contribution to the field of MTL with its innovative tensor factorization approach and robust experimental validation. Addressing the suggested improvements would further strengthen its impact and applicability.