The paper introduces a JavaScript-based framework with WebCL components for training and deploying deep neural networks (DNNs). It claims to enable deep learning on ordinary personal computers and smartphones without specialized software installation, leveraging GPGPU capabilities for competitive performance. The framework supports large-scale CNNs like VGGNet and ResNet, with distributed training demonstrated using web browsers as clients. The authors highlight the framework's accessibility and cross-platform compatibility as key contributions, alongside its open-source availability.
Decision: Accept (with minor revisions)  
The paper presents a novel and practical contribution to the field of deep learning by enabling training and deployment in environments not traditionally suited for such tasks. While the framework's performance is slower than high-performance compiled software like Caffe on NVIDIA GPUs, its ease of deployment and accessibility make it a valuable addition to the ecosystem of deep learning tools. However, the paper has some methodological and presentation issues that need to be addressed.
Supporting Arguments:  
1. Novelty and Practicality: The use of JavaScript and WebCL for deep learning is innovative, especially given the framework's ability to run on web browsers and smartphones. This lowers the barrier to entry for deep learning, making it accessible to a broader audience.  
2. Empirical Validation: The experiments demonstrate the feasibility of training large-scale CNNs like VGGNet and ResNet, achieving competitive speeds on AMD GPUs. The distributed training results, while preliminary, show promise for scaling computations across multiple clients.  
3. Clarity and Writing: The paper is well-written and provides sufficient background to understand the contributions. The open-source release of the framework further enhances its impact.
Additional Feedback for Improvement:  
1. Unfair Batch Size Comparison (Table 4): The use of different batch sizes across frameworks undermines the validity of the performance comparison. The authors should rerun experiments with consistent batch sizes to ensure fairness.  
2. Incomplete Information in Figure 6: The figure lacks sufficient details, such as node.js values and consistent inclusion of Firefox, Chrome, and node.js across subfigures. This omission makes it difficult to interpret the results comprehensively.  
3. Performance Bottlenecks: While the authors acknowledge that the framework is slower than Caffe on NVIDIA GPUs, a deeper analysis of the bottlenecks (e.g., convolution implementation) and potential solutions would strengthen the paper.  
4. WebCL Dependency: The reliance on WebCL, which is not a built-in feature of most web browsers, limits the framework's practical adoption. Exploring alternatives like WebGL or wasm.js could improve usability.
Questions for the Authors:  
1. Can you clarify why different batch sizes were used in Table 4, and how this might have affected the results?  
2. Have you considered alternative GPGPU interfaces (e.g., WebGL) to address the limitations of WebCL's browser support?  
3. What specific strategies do you plan to explore for reducing communication overhead in distributed training?  
Overall, the paper makes a meaningful contribution to the field and is suitable for acceptance after addressing the outlined issues.