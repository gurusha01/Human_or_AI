Review of the Paper
Summary of Contributions
This paper investigates the impact of mini-batch size on the convergence of Stochastic Gradient Descent (SGD) in non-convex optimization settings and extends this analysis to Asynchronous Stochastic Gradient Descent (ASGD) with multiple learners. The authors provide a theoretical framework that demonstrates how larger mini-batch sizes and an increased number of learners in ASGD can lead to slower convergence prior to the asymptotic regime. The paper also highlights the inherent inefficiency in exploiting parallelism in SGD and ASGD. The theoretical results are supported by limited experimental validation using the CIFAR-10 dataset and a standard convolutional neural network architecture. The authors conclude by suggesting that the inefficiencies observed may extend to more advanced optimization methods.
Decision: Reject
The decision to reject is based on two main reasons:
1. Insufficient Experimental Validation: The experimental evaluation is limited to a single dataset (CIFAR-10) and a single architecture, which does not provide sufficient evidence to generalize the theoretical findings. Additionally, the experimental setup (e.g., N=16 learners) is relatively small-scale and does not fully explore the practical implications of the theoretical results.
2. Lack of Comparisons with Related Methods: The paper does not include comparisons with other optimization techniques, such as momentum-based SGD or Elastic Averaging SGD (EASGD), which are relevant baselines for assessing the practical significance of the findings.
Supporting Arguments
1. The theoretical analysis is rigorous and addresses a relevant problem in distributed machine learning. The mathematical derivations are well-presented and provide valuable insights into the trade-offs between mini-batch size, parallelism, and convergence efficiency.
2. However, the experimental results are insufficient to validate the theoretical claims comprehensively. The use of only one dataset and one architecture limits the scope of the conclusions. Furthermore, the experiments do not explore larger-scale setups, which are critical for understanding the practical implications of ASGD inefficiencies.
3. The lack of comparisons with other optimization methods weakens the paper's contribution. For example, momentum-based SGD and EASGD are widely used in practice and could provide a more complete picture of the trade-offs discussed in the paper.
Suggestions for Improvement
1. Expand Experimental Evaluation: Include experiments on additional datasets (e.g., ImageNet) and architectures (e.g., ResNet, Transformers) to demonstrate the generalizability of the theoretical findings. Larger-scale experiments with more learners (e.g., N > 16) would also strengthen the paper.
2. Include Baseline Comparisons: Compare the performance of vanilla SGD and ASGD with other optimization methods, such as momentum-based SGD and EASGD, to contextualize the findings within the broader literature.
3. Explore Broader Implications: While the paper focuses on vanilla SGD and ASGD, extending the analysis to more advanced optimization techniques (e.g., Adam, RMSProp) would increase the paper's relevance and impact.
4. Clarify Practical Implications: Provide a more detailed discussion of how the theoretical results translate to practical scenarios, including recommendations for practitioners on choosing mini-batch sizes or the number of learners.
Questions for the Authors
1. How do the theoretical results extend to optimization methods that incorporate momentum or adaptive learning rates, such as Adam or RMSProp?
2. Have you considered the impact of communication overheads in distributed settings, which could further exacerbate the inefficiencies observed in ASGD?
3. Can you provide more details on why N=16 was chosen as the upper limit for the number of learners in the experiments? Would larger setups yield different insights?
4. How sensitive are the theoretical findings to the assumptions made (e.g., Lipschitz continuity, bounded variance)? Would relaxing these assumptions change the conclusions?
While the paper makes a valuable theoretical contribution, addressing the above concerns would significantly improve its quality and impact.