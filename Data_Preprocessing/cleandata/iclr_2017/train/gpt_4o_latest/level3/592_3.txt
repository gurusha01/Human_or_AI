Review of "Epitomic Variational Autoencoder (eVAE)"
Summary of Contributions
This paper addresses the critical issue of over-regularization in Variational Autoencoders (VAEs), where latent dimensions are underutilized, leading to poor generalization and degraded generation quality. The authors propose a novel model, the Epitomic Variational Autoencoder (eVAE), which introduces structured sparsity in the latent space by activating only a subset of latent dimensions (epitomes) for each input. This approach effectively mitigates over-pruning and enables better utilization of model capacity. The paper demonstrates that eVAE outperforms standard VAEs and related methods like Dropout VAE and Mixture VAE on MNIST and TFD datasets, both qualitatively and quantitatively. The authors also provide a principled explanation of how eVAE avoids over-pruning and show that their model generalizes better without relying on heuristic fixes like KL annealing or "free bits."
Decision: Accept
The paper offers a novel, well-motivated solution to a fundamental problem in VAEs, and the proposed approach is supported by adequate experimental results. The introduction of structured sparsity in the latent space is a significant contribution that could inspire further research in generative modeling.
Supporting Arguments
1. Problem Significance: The issue of over-regularization in VAEs is well-known and limits their practical utility. Existing solutions like KL annealing are heuristic and lack theoretical grounding. The authors' model-based approach is a more principled alternative.
2. Novelty and Motivation: The idea of dividing the latent space into shared subspaces (epitomes) is novel and well-motivated. The authors convincingly argue that different data points require different subspaces, and their structured sparsity approach is a natural way to address this.
3. Experimental Validation: The experiments on MNIST and TFD datasets are modest but sufficient to validate the claims. The results show that eVAE achieves better generation quality and avoids over-pruning compared to baseline methods. The use of Parzen log-density as an evaluation metric is appropriate for the generation task.
Suggestions for Improvement
1. Clarify the Role of Topology in Latent Representations: The paper implicitly assumes that the latent space benefits from a structured topology (e.g., contiguous subspaces). However, it would be helpful to discuss whether priors over arbitrary subsets of latents could offer greater flexibility and representational power. The authors should justify their choice of structured sparsity over alternative approaches.
2. Exponential Potential of Latent Combinations: While the epitome-based approach limits the combinations of active latent dimensions, this could reduce the model's representational capacity. The authors should discuss this trade-off more explicitly and explore whether overlapping or dynamically chosen epitomes could mitigate this limitation.
3. Unclear Explanation on Overfitting: The explanation on page 7 regarding how under-utilization of latent dimensions leads to overfitting is unclear. The authors should elaborate on this connection, possibly with additional theoretical or empirical evidence.
4. Scalability to Complex Datasets: The experiments are limited to relatively simple datasets (MNIST and TFD). It would strengthen the paper if the authors could discuss or provide preliminary results on more complex datasets, such as CIFAR-10 or CelebA.
Questions for the Authors
1. How does the choice of epitome size (K) and stride (s) affect the model's performance on datasets with higher complexity or diversity? Could these hyperparameters be learned rather than fixed?
2. Have you considered using priors over arbitrary subsets of latents instead of contiguous subspaces? If so, how does this compare to your approach in terms of generative quality and computational efficiency?
3. Can you provide more theoretical insights or empirical evidence to clarify the connection between latent under-utilization and overfitting mentioned on page 7?
In conclusion, this paper provides a novel and well-motivated solution to a fundamental problem in VAEs. While there are areas for improvement and clarification, the contributions are significant, and the paper merits acceptance.