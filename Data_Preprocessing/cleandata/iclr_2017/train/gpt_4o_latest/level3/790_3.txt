Review of the Paper
Summary of Contributions
This paper explores the potential for neural networks to autonomously learn encryption and decryption mechanisms in a multi-agent communication scenario, specifically focusing on confidentiality against adversarial neural networks. The authors propose a novel end-to-end adversarial training framework where two neural networks ("Alice" and "Bob") learn to communicate securely, while a third neural network ("Eve") attempts to eavesdrop. The paper also extends this idea to selective protection of data fields, demonstrating how neural networks can learn what to encrypt to achieve privacy goals while maintaining utility. The authors emphasize the simplicity and flexibility of their approach, contrasting it with classical cryptographic methods that rely on predefined algorithms and rigorous mathematical guarantees.
Decision: Reject
The paper is rejected primarily due to its lack of practical value and scientific rigor. The proposed neural network-based cryptographic system is demonstrably inferior to established cryptographic methods in terms of both security guarantees and efficiency. Additionally, the selective protection mechanism in Section 3 is less effective than existing techniques that decorrelate data and apply proven cryptographic methods to sensitive fields.
---
Supporting Arguments for the Decision
1. Lack of Practical Value in Section 2  
   The proposed 3-part neural network system for symmetric encryption is fundamentally flawed when compared to provable cryptographic systems. Classical cryptography provides rigorous security guarantees (e.g., indistinguishability under chosen plaintext attacks) that are absent in this work. The adversarial training framework fails to account for all possible attackers, as it only optimizes against a specific neural network adversary. This leaves the system vulnerable to other attack vectors, making it unsuitable for real-world applications where robust security is critical.
2. Ineffectiveness of Selective Protection in Section 3  
   The task of selectively hiding correlated data fields is addressed in a suboptimal manner. Instead of decorrelating the data and encrypting sensitive fields with proven cryptographic methods, the authors rely on adversarial training to achieve privacy. This approach is neither efficient nor reliable, as evidenced by the limited robustness of the trained models and the reliance on arbitrary loss functions. The results do not convincingly demonstrate that the proposed method outperforms simpler, more established techniques.
3. Insufficient Scientific Rigor  
   The claims made in the paper are not adequately supported by the results. For example, the success of the neural network-based encryption system is inconsistent, with training failures reported in 30-50% of cases. The authors attribute these failures to instability in adversarial training but do not provide a robust solution. Furthermore, the results for asymmetric encryption are inconclusive and suggest reliance on "security by obscurity," which is widely regarded as a poor cryptographic practice.
---
Additional Feedback for Improvement
1. Clarify the Scope and Limitations  
   The paper should explicitly acknowledge the limitations of neural network-based cryptography compared to classical methods. A discussion of scenarios where the proposed approach might be preferable (e.g., resource-constrained environments or applications requiring differentiability) would help contextualize the work.
2. Improve Experimental Rigor  
   The experimental setup should include comparisons with baseline cryptographic methods to demonstrate the practical utility of the proposed approach. Additionally, the robustness of the system should be tested against a wider range of adversaries, including non-neural network-based attackers.
3. Address Training Instability  
   The high failure rate in training Alice and Bob to achieve secure communication undermines the reliability of the proposed system. The authors should explore techniques to stabilize adversarial training, such as improved loss functions or alternative optimization strategies.
4. Expand the Discussion on Selective Protection  
   The selective protection mechanism in Section 3 could benefit from a more detailed analysis of its utility and limitations. For example, how does the approach scale to real-world datasets with complex correlations? How does it compare to existing methods in terms of computational cost and privacy guarantees?
---
Questions for the Authors
1. How does the proposed system handle adversaries that are not neural networks? Would the security guarantees hold against classical cryptographic attacks?
2. Can the authors provide a theoretical justification for the choice of loss functions used in adversarial training? How sensitive are the results to these choices?
3. Have you considered integrating classical cryptographic primitives (e.g., homomorphic encryption) into the neural network framework to improve security guarantees?
4. What specific advantages does the proposed approach offer over existing methods for selective protection, beyond its end-to-end nature?
---
In conclusion, while the paper presents an interesting exploration of neural networks in cryptographic tasks, its practical utility and scientific contributions are limited. Significant improvements in methodology, experimental rigor, and positioning within the broader cryptographic literature are needed to make the work more impactful.