Review of the Paper
Summary of Contributions
This paper introduces a spatiotemporal saliency network, termed Recurrent Mixture Density Network (RMDN), designed to mimic human fixation patterns for video analysis. The model combines 3D convolutional features with an LSTM and a Gaussian Mixture Model to predict saliency maps directly from human fixation data. The authors claim that the proposed approach achieves state-of-the-art results in saliency prediction on the Hollywood2 dataset and generalizes well to UCF101. Furthermore, they demonstrate that the predicted saliency maps can improve action recognition when used to re-weight video features. The paper highlights the efficiency of the model, its ability to train without hand-engineered spatiotemporal features, and its potential for real-time applications.
Decision: Reject
While the paper presents an interesting approach and achieves notable results in saliency prediction, it suffers from several critical issues that undermine its overall contribution. The primary reasons for rejection are: (1) insufficient justification for key design choices, particularly the concatenation of features for action recognition, which contradicts the hypothesis of down-weighting irrelevant pixels, and (2) a lack of fair comparisons in action recognition experiments, where the increased model complexity due to concatenation inflates performance. Additionally, several claims in the paper are either unclear or contradicted by the methodology, such as the assertion of training without spatiotemporal feature engineering while relying on human fixation data.
Supporting Arguments
1. Unclear Role of Context in Attention: The paper does not adequately discuss why context is not automatically incorporated into the model, leaving a gap in the theoretical motivation for the proposed approach. This omission weakens the argument for the necessity of the concatenation step.
   
2. Contradictory Hypothesis: The hypothesis that down-weighting irrelevant pixels improves performance is contradicted by the reliance on feature concatenation, which increases model complexity. Alternative methods, such as directly integrating saliency maps into the classification pipeline, should have been explored.
3. Unfair Comparisons in Action Recognition: The comparison of action recognition results is biased, as the concatenation approach effectively doubles the feature dimensionality, making it difficult to isolate the contribution of saliency maps. A more rigorous comparison, such as dimensionality-matched baselines, is necessary.
4. Clarity and Consistency Issues: The abstract contains unclear statements and a typo ("we added them trained central bias"), which detracts from the paper's readability. Additionally, the claim about training without spatiotemporal feature engineering is inconsistent with the reliance on human fixation data.
Additional Feedback for Improvement
- Clarify Fixation Control: Section 3.1 requires clarification on how fixation points are controlled per frame. This is critical for understanding the training process.
- Gradient Flow in C3D Layers: Freezing the C3D layers raises questions about whether allowing gradients to flow back would improve task-specific tuning. This should be explored in future work.
- Feature Concatenation Details: Section 3.4 lacks sufficient detail on how features are concatenated. Providing a more thorough explanation would improve reproducibility.
- State-of-the-Art Comparisons: Table 4 should include current state-of-the-art results for action recognition to provide a more comprehensive evaluation.
- Typographical Errors: Minor typos, such as "we added them trained central bias," should be corrected for better presentation.
Questions for the Authors
1. Why was context not automatically incorporated into the attention mechanism, and how does this omission affect the model's performance?
2. Could alternative methods, such as directly integrating saliency maps into the classification pipeline, be more effective than feature concatenation?
3. Did you explore the impact of allowing gradients to flow back into the C3D layers during training? If not, why was this design choice made?
In summary, while the paper presents an innovative approach to saliency prediction, the issues outlined above need to be addressed to strengthen its contributions and ensure scientific rigor.