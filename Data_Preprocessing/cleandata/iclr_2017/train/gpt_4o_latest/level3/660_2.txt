Review of the Paper
The paper introduces "Eve," a semi-automatic learning rate schedule for the Adam optimizer, which adaptively adjusts the learning rate based on feedback from the objective function. The authors claim that this approach improves optimization performance in training deep learning models. The paper demonstrates the method's effectiveness on tasks involving convolutional neural networks (CNNs) and recurrent neural networks (RNNs), showing that Eve outperforms Adam and other state-of-the-art optimizers on benchmark datasets like CIFAR-10, CIFAR-100, and Penn Treebank.
Decision: Reject
While the paper presents a potentially impactful idea, it lacks sufficient originality and scientific rigor in key areas to warrant acceptance. The primary reasons for rejection are: (1) limited novelty, as the proposed method is a relatively straightforward modification of Adam, and (2) insufficient analysis and conclusive evidence to support the claims, particularly regarding the momentum term and broader applicability of the method.
Supporting Arguments
1. Limited Originality: The proposed method, while more sophisticated than simple decay techniques, is a minor extension of Adam. The idea of using feedback from the objective function to adjust the learning rate is not entirely novel and lacks a strong theoretical foundation to distinguish it from existing approaches.
2. Inconclusive Results: The experimental results, though promising, are limited in scope. The evaluation primarily focuses on small-scale models (e.g., small CNNs on CIFAR-10/100) and does not convincingly demonstrate the method's effectiveness on larger or more complex tasks. Additionally, the comparison to Adam is not fully conclusive, as the experiments do not explore a wide range of hyperparameter configurations for Adam or other baseline methods.
3. Lack of Analysis: The paper does not adequately analyze the role of the momentum term in the proposed method. This omission weakens the scientific rigor of the work, as momentum is a critical component of Adam and its variants. Furthermore, the behavior of the feedback mechanism is discussed qualitatively but lacks deeper theoretical insights.
Suggestions for Improvement
1. Theoretical Analysis: Provide a more rigorous theoretical analysis of the proposed feedback mechanism and its impact on optimization dynamics. This could help establish the novelty and robustness of the approach.
2. Broader Experiments: Evaluate Eve on larger-scale models and datasets (e.g., ImageNet or large language models) to demonstrate its scalability and generalizability. Additionally, include more comprehensive comparisons with Adam and other optimizers, exploring a wider range of hyperparameters.
3. Momentum Term: Analyze the effect of the momentum term in Eve, as it is a key component of Adam. This could provide insights into how the feedback mechanism interacts with momentum.
4. Citations: Replace the arXiv citation for Adam with the corresponding conference publication to adhere to academic standards.
Questions for the Authors
1. How does the feedback mechanism interact with the momentum term in Adam? Could this interaction lead to unintended consequences during training?
2. Have you tested Eve on larger-scale problems or more complex architectures? If so, what were the results?
3. How sensitive is Eve to its additional hyperparameters (e.g., Î²3, k, K)? Could this sensitivity limit its practical applicability?
While the paper has potential, addressing these concerns would significantly strengthen its contributions and impact.