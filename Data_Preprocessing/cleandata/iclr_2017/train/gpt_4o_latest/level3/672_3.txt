Review of "Joint Multimodal Variational Autoencoder (JMVAE)"
Summary of Contributions
This paper introduces the Joint Multimodal Variational Autoencoder (JMVAE), a novel generative model designed to handle multimodal datasets by learning a joint representation that enables bi-directional generation between modalities, such as images and text. The authors further propose JMVAE-kl, an extension that mitigates the issue of collapsing samples when modalities are missing by introducing additional KL divergence penalties. The paper demonstrates the model's ability to generate and reconstruct modalities more effectively than conventional VAEs, as well as its potential to extract meaningful joint representations. Experiments on MNIST and CelebA datasets showcase the model's performance, with JMVAE-kl addressing the challenges of missing modalities. The authors also highlight the model's ability to generate multiple modalities bi-directionally, a feature lacking in many existing approaches.
Decision: Reject
While the paper tackles an interesting and relevant problem in multimodal learning, the execution and methodological rigor fall short in several key areas. Specifically, there are concerns about scalability, fairness in comparisons, and the interpretability of results, which undermine the paper's contributions.
Supporting Arguments
1. Scalability Concerns: JMVAE requires a separate encoder for each subset of missing modalities, which does not scale well beyond two modalities. This limitation is not addressed adequately, making the approach impractical for real-world multimodal datasets with numerous modalities.
   
2. Unclear Justification for Methodology: While the authors connect JMVAE to the Variation of Information (VI), they do not justify why JMVAE was chosen over a direct VI-based approach. This weakens the theoretical grounding of the proposed method.
3. Counterintuitive Results: The log-likelihood values in Table 1 show that multiple modalities sometimes result in lower log-likelihoods than single modalities. This contradicts the expected benefits of joint representation learning and requires further explanation.
4. Fairness of Comparisons: The comparison with CVAE may be biased, as CVAE uses conditional labels during training, which could give it an advantage in certain tasks. Additionally, comparing log-likelihoods with GAN-based models on CelebA is questionable, as GANs do not optimize for log-likelihood.
Suggestions for Improvement
1. Scalability: Address the scalability issue by proposing a more efficient architecture that does not require separate encoders for every subset of missing modalities. This would make the method more applicable to datasets with more than two modalities.
2. Theoretical Justification: Provide a stronger theoretical rationale for choosing JMVAE over a direct VI-based approach. This could include a discussion of computational advantages, flexibility, or other benefits.
3. Clarify Results: Explain why multiple modalities sometimes result in lower log-likelihoods than single modalities. This could involve additional experiments or ablation studies to identify the cause.
4. Fair Comparisons: Ensure that comparisons with CVAE and GAN-based models are fair and meaningful. For example, highlight the differences in training objectives and metrics to contextualize the results.
5. Broader Evaluation: Extend the evaluation to datasets with more than two modalities to demonstrate the generalizability of the approach. This would also help address concerns about scalability.
Questions for the Authors
1. Why was JMVAE chosen over a direct Variation of Information approach? What specific advantages does JMVAE offer?
2. Can you clarify why the log-likelihood values for multiple modalities are sometimes lower than those for single modalities? Is this due to interference between modalities, or another factor?
3. How would the proposed method handle datasets with more than two modalities? Are there plans to address scalability in future work?
4. Could you provide additional qualitative examples to illustrate the bi-directional generation capabilities of JMVAE on CelebA?
In conclusion, while the paper presents an interesting idea and makes notable contributions to multimodal learning, significant issues with scalability, methodological justification, and result interpretation prevent it from being ready for acceptance. Addressing these concerns could make the work more impactful and robust.