The paper introduces a novel extension to Neural Turing Machines (NTMs), termed the Dynamic Neural Turing Machine (D-NTM), which incorporates a learnable key-value memory addressing mechanism. This innovation allows for more flexible location-based addressing, supporting both continuous and discrete attention mechanisms. The authors evaluate the model on a variety of tasks, including the Facebook bAbI episodic question-answering dataset, permuted sequential MNIST (pMNIST), and algorithmic toy tasks, demonstrating modest improvements over vanilla NTMs and LSTMs. The paper also proposes a curriculum learning strategy to address challenges in training discrete attention mechanisms and introduces several regularization techniques to improve model performance.
Decision: Reject
The primary reasons for rejection are the lack of clarity in the paper and insufficient empirical justification for the model's complexity. While the proposed D-NTM is an interesting extension to NTMs, the paper suffers from significant issues that hinder its scientific rigor and reproducibility.
Supporting Arguments:
1. Model Complexity and Justification: The D-NTM introduces a highly complex architecture, requiring over 10 cost function terms, multiple regularization techniques, and hacks such as curriculum learning. However, the paper does not adequately justify these design choices or provide ablation studies to demonstrate their necessity. This undermines confidence in the model's generalizability and robustness.
2. Evaluation and Results: While the D-NTM shows slight improvements over LSTMs on bAbI tasks, it performs worse than other memory-augmented models like Memory Networks and Dynamic Memory Networks. The results on pMNIST and toy tasks are more promising but do not sufficiently demonstrate the model's advantages in real-world, large-scale applications.
3. Reproducibility Issues: The absence of code and the lack of detailed implementation specifics make it challenging to reproduce the results. Ambiguities in notation (e.g., \( wt \), \( \gammat \)) and unclear descriptions of components like the "shallow MLP" further exacerbate this issue.
4. Writing Clarity: The paper is difficult to follow due to inconsistent and non-standard notation, as well as contradictory statements (e.g., feasibility of training discrete attention with REINFORCE). This significantly impairs comprehension and evaluation.
Additional Feedback:
1. Notation and Definitions: The authors should revise the paper to use consistent and standard notation. For instance, clarify whether \( \gammat \) is a scalar, vector, or function, and ensure variables like \( wt \) and \( b \) are consistently defined.
2. Cost Function Summary: Provide a clear and concise summary of the overall cost function, including the role of each term and its contribution to the training process.
3. Comparison to Simpler Baselines: The curriculum learning strategy for discrete attention should be compared to simpler alternatives, such as rounding continuous attention weights. This would help isolate the benefits of the proposed approach.
4. Code Release: The authors should release code or provide detailed pseudocode to improve reproducibility and facilitate further research.
Questions for the Authors:
1. Can you provide an ablation study to quantify the contributions of each cost function term and regularization technique to the model's performance?
2. Why was REINFORCE chosen for training discrete attention, given its known challenges with high variance? Did you experiment with alternative methods, such as Gumbel-Softmax?
3. How does the proposed D-NTM scale to larger datasets and tasks beyond toy problems and bAbI? Have you considered evaluating it on more complex benchmarks like visual question answering or machine translation?
While the paper introduces an intriguing idea, the issues outlined above need to be addressed to ensure the work meets the standards of scientific rigor and reproducibility.