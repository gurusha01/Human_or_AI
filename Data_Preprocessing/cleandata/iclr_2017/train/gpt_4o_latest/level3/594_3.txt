The paper investigates the use of low-rank approximations in recurrent neural networks (RNNs) to reduce the number of parameters while maintaining competitive performance. Specifically, the authors propose low-rank and low-rank plus diagonal matrix parametrizations for "Passthrough Networks," which include architectures like LSTMs and GRUs. The study demonstrates that these parametrizations can achieve results comparable to fully-parametrized networks across several tasks, including sequential MNIST classification and language modeling. The authors highlight the potential of these methods to reduce memory and computational costs without sacrificing accuracy, and they provide extensive experimental evidence to support their claims.
Decision: Accept with Minor Revisions
The primary reason for acceptance is the paper's successful demonstration of achieving similar performance to fully-parametrized networks with significantly fewer parameters. This is a commendable contribution, as parameter efficiency is a critical area of research in deep learning, especially for resource-constrained applications. The experimental results are well-documented and scientifically rigorous, showcasing the effectiveness of the proposed methods across diverse tasks. However, the claim of "conceptual unification" in the conclusion is overstated, as the low-rank framework is already well-known in the community. This misrepresentation should be addressed in the final version.
Supporting Arguments:
1. Problem Tackled: The paper addresses the challenge of reducing the parameter count in RNNs while preserving their performance, a relevant and impactful problem in the field.
2. Motivation and Literature Placement: The approach is well-motivated, building on prior work on low-rank approximations and extending it to passthrough networks. The authors position their work appropriately within the existing literature, citing relevant studies.
3. Scientific Rigor: The experimental results are thorough, covering multiple tasks and providing comparisons to state-of-the-art methods. The inclusion of both low-rank and low-rank plus diagonal approaches adds depth to the analysis.
Suggestions for Improvement:
1. Clarify Claims: The claim of conceptual unification in the conclusion should be revised to reflect the fact that low-rank approximations are a well-established concept. The novelty lies in their application to passthrough networks, not in the framework itself.
2. Comparative Analysis: While the results are competitive, they are not significantly better than fully-parametrized networks. The authors should provide a more nuanced discussion of the trade-offs between parameter reduction and performance.
3. Numerical Stability: The paper briefly mentions numerical stability issues in the memory task. A deeper analysis of these challenges and potential solutions would strengthen the contribution.
4. Broader Implications: The authors could elaborate on the practical implications of their work, such as deployment in edge devices or real-time applications.
Questions for the Authors:
1. How does the low-rank plus diagonal approach compare to other parameter-efficient methods, such as pruning or quantization, in terms of computational overhead and performance?
2. Can the proposed methods generalize to other neural architectures beyond passthrough networks, such as transformers or convolutional networks?
3. What are the limitations of the low-rank parametrization in terms of scalability to very large models or datasets?
In conclusion, the paper makes a valuable contribution to the field by demonstrating the feasibility of low-rank parametrizations in RNNs. With minor revisions to address the overstated claims and provide additional clarity, the paper will be a strong addition to the conference.