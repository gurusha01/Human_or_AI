Review
Summary of Contributions
This paper introduces Dynamic Steerable Frame Networks (DSFNs), a novel approach that generalizes convolutional neural networks (CNNs) by leveraging steerable frames instead of the traditional pixel basis. The authors argue that steerable frames, which allow continuous transformations under Lie groups, provide a more expressive and interpretable feature representation. The proposed DSFNs combine the strengths of Dynamic Filter Networks (DFNs) and Spatial Transformer Networks (STNs) by enabling locally adaptive filtering with explicit pose separation. The paper demonstrates the utility of DSFNs on two tasks: edge detection and small-scale video classification. Notably, DSFNs outperform DFNs in edge detection by regressing steerable filter parameters and improve video classification performance without increasing network capacity. The authors also show that using overcomplete frame bases as a preprocessing step enhances performance in highly optimized architectures like ResNet and DenseNet.
Decision: Accept
The paper is theoretically innovative and provides promising empirical results. The primary reasons for acceptance are:
1. Novelty and Theoretical Contribution: The introduction of steerable frames into CNNs and the development of DSFNs represent a significant advancement in the field of geometric deep learning.
2. Empirical Validation: The experiments convincingly demonstrate the advantages of DSFNs in edge detection and video classification, highlighting their ability to separate pose and canonical appearance.
Supporting Arguments
1. Well-Motivated Approach: The paper is well-placed in the literature, building on prior work in steerable filters, DFNs, and STNs. The authors clearly articulate the limitations of existing methods (e.g., global invariance in STNs and the lack of interpretability in DFNs) and justify the need for DSFNs.
2. Scientific Rigor: The theoretical foundation of steerable frames and their integration into CNNs is robust. The authors provide proofs of equivariance and detailed derivations of steering equations, ensuring the scientific soundness of their claims.
3. Empirical Results: The results are compelling. DSFNs outperform strong baselines in edge detection and video classification, demonstrating their practical utility. The experiments also highlight the benefits of using overcomplete frame bases in standard CNNs.
Suggestions for Improvement
1. Parameter Reporting: The paper should report the total number of parameters for the LSTM experiments to clarify whether performance improvements are due to architectural efficiency or increased capacity.
2. Backpropagation Details: The authors mention that backpropagation through steerable filters will be added to the final manuscript. Including this is crucial for reproducibility and understanding the computational complexity of DSFNs.
3. Baseline Comparisons: For edge detection, the paper compares DSFNs with DFNs but does not include results from a standard CNN baseline. Adding this comparison would strengthen the empirical validation.
4. Minor Issues: Address the following:
   - Add the missing verb in the abstract.
   - Correct the hyphenation of "ConvNet."
   - Remove unnecessary text in the second paragraph of page 1.
Questions for the Authors
1. How sensitive are DSFNs to the choice of frame basis? Do certain tasks require specific frames, or is the performance robust across different bases?
2. Could you elaborate on the computational overhead introduced by DSFNs compared to DFNs and STNs? Are there trade-offs in runtime efficiency?
3. How scalable is the proposed method to larger datasets and more complex tasks? Do you anticipate any challenges in applying DSFNs to real-world problems like medical imaging or robotics?
Conclusion
This paper makes a strong theoretical and practical contribution by introducing DSFNs, a method that bridges the gap between geometric deep learning and dynamic filtering. While some minor improvements are needed, the novelty, rigor, and promising results make this work a valuable addition to the field. Accept.