Review of the Paper
Summary of Contributions
This paper addresses the challenge of training stochastic feedforward neural networks (SFNNs), which are known for their expressive power and regularization benefits but are notoriously difficult to train. The authors propose a novel intermediate model, the Simplified-SFNN, which bridges deterministic deep neural networks (DNNs) and SFNNs. The main contribution is a three-step training pipeline: (1) pretraining a DNN, (2) transferring its weights to a Simplified-SFNN, and (3) optionally transferring the weights to a full SFNN or a modified deterministic model (DNN*). The approach is validated on various datasets, including MNIST, CIFAR-10, CIFAR-100, and SVHN, demonstrating improved performance in both classification and generative tasks. The authors also provide theoretical guarantees for parameter transfer between models and highlight the regularization benefits of stochastic training.
Decision: Accept
The paper makes a significant contribution to the field of stochastic neural networks by proposing a practical and theoretically grounded training method. The results are promising, particularly the improved performance of DNN* over standard DNNs and the demonstrated regularization benefits of stochastic training. However, some issues with clarity, terminology, and experimental baselines need to be addressed.
Supporting Arguments for Acceptance
1. Novelty and Practicality: The introduction of Simplified-SFNN as an intermediate model is innovative and provides a practical solution to the challenges of training SFNNs. The proposed method leverages pre-trained DNNs, making it accessible and easy to implement.
2. Theoretical Rigor: The paper provides theoretical guarantees for the parameter transfer process, ensuring that Simplified-SFNN can approximate DNNs with bounded errors. This adds credibility to the proposed approach.
3. Empirical Validation: The experimental results are compelling, showing that stochastic training improves both generative and classification tasks. The use of well-known datasets and architectures (e.g., WRN) strengthens the paper's impact.
4. Scalability: The method is demonstrated on large-scale models (e.g., WRN with 28 layers and 36 million parameters), showcasing its scalability.
Suggestions for Improvement
1. Terminology: The term "multi-modal tasks" is misleading and should be replaced with "generative tasks with a multimodal target distribution" for clarity.
2. Notation Issues: The superscript notation for layer indices (e.g., N²) is unclear and should be revised for better readability.
3. Experimental Baselines: The results on CIFAR-10, CIFAR-100, and SVHN would be more convincing if the baselines included dropout and batch normalization, as these are standard techniques for improving DNN performance.
4. Weight Transfer Details: The paper should clarify whether rescaling is required when transferring weights back from Simplified-SFNN to DNN*.
5. Acronym Clarification: The meaning of "NCSFNN" in the supplementary material is unclear and should be explicitly defined.
6. Presentation: The paper has several grammatical and stylistic issues, as well as inconsistencies in citation formatting. These should be corrected to improve readability.
Questions for the Authors
1. How does the performance of Simplified-SFNN compare to SFNN when more stochastic layers are introduced? Does the added complexity justify the performance gains?
2. Can the proposed method be extended to other types of stochastic neural networks, such as those with continuous latent variables?
3. How sensitive is the method to the choice of hyperparameters, particularly the scaling factors (e.g., γ₁, γ₂)?
4. Are there specific tasks or datasets where the proposed method does not perform well? If so, what are the limitations?
In conclusion, this paper provides a meaningful contribution to the field of stochastic neural networks and is suitable for acceptance after addressing the minor issues outlined above.