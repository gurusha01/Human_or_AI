Review
The paper investigates the impact of different context types on the quality of word embeddings across a variety of benchmarks. It aims to decouple parameters in embedding algorithms, focusing specifically on the role of context, and provides a systematic evaluation of context types across four tasks and 21 datasets. The authors claim that their work offers insights into context selection and provides a guideline for the community, supported by publicly available code.
Decision: Reject
The primary reasons for this decision are: (1) the lack of clear or consistent findings that demonstrate a significant advantage of any specific context type, and (2) insufficient exploration of whether the benchmarks used are sensitive enough to detect subtle differences in embedding quality. While the paper addresses an important and underexplored question, the results do not provide actionable conclusions or strong evidence to support the claims made.
Supporting Arguments
1. Motivation and Literature Placement: The paper is well-motivated, as the question of what constitutes the "best" context for word embeddings is both relevant and underexplored. The authors position their work appropriately within the literature, acknowledging the growing number of embedding models and the need for systematic evaluation.
2. Scientific Rigor: While the experimental setup is comprehensive, spanning multiple tasks and datasets, the results fail to reveal a clear or consistent advantage for any specific context type. This raises concerns about whether the benchmarks used are sufficiently sensitive to detect subtle differences. The authors do not address this limitation in depth, leaving a gap in the interpretation of their findings.
3. Contribution: The paper's contribution is limited by the lack of actionable insights. While the authors provide a systematic investigation, the absence of strong conclusions diminishes the practical utility of the work.
Suggestions for Improvement
1. Addressing Benchmark Sensitivity: The authors should critically evaluate whether the benchmarks used are capable of detecting subtle differences in embedding quality. If not, alternative or more sensitive evaluation methods should be considered.
2. Expanding Context Types: The paper could benefit from exploring additional context types, such as those proposed in "Open IE as an Intermediate Structure for Semantic Tasks" (Stanovsky et al., ACL 2015). This would provide a broader and more diverse basis for comparison.
3. Fundamental Questions: The paper should delve deeper into fundamental questions about context types. For example, what theoretical properties make a context type effective? How do different context types align with linguistic or cognitive theories of meaning?
Questions for the Authors
1. How do you ensure that the benchmarks used are sensitive enough to detect subtle differences in embedding quality? Have alternative evaluation methods been considered?
2. Why were the specific context types chosen for this study? Could additional context types, such as those in Stanovsky et al. (2015), provide further insights?
3. Can you provide more detailed analysis or theoretical reasoning to explain why no clear advantage was observed for any specific context type?
Overall, while the paper addresses an important question and is well-motivated, the lack of actionable insights and limited exploration of benchmark sensitivity prevent it from making a strong contribution to the field.