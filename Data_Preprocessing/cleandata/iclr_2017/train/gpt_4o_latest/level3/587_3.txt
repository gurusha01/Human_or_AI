Review
Summary of Contributions
The paper proposes DeepRebirth, a novel framework for accelerating deep neural networks by merging non-tensor layers (e.g., pooling, normalization) with neighboring tensor layers (e.g., convolution) into a single dense layer, termed the "rebirth layer." The authors claim that this approach significantly reduces computational latency and memory usage on mobile devices while maintaining minimal accuracy loss. The method is evaluated on popular architectures like GoogLeNet, AlexNet, and ResNet, achieving 3x-5x speed-ups and 2.5x runtime memory savings on mobile CPUs. The paper also demonstrates compatibility with existing model compression techniques, such as Tucker decomposition, for further optimization.
Decision: Reject
While the paper addresses an important problem—efficient deployment of deep learning models on mobile devices—and presents interesting experimental results, it has several critical shortcomings that prevent it from being ready for publication. The primary reasons for rejection are the lack of clarity in the proposed method and insufficient comparisons with existing techniques.
Supporting Arguments for Rejection
1. Clarity Issues with the DeepRebirth Layer: The introduction of the "DeepRebirth layer" is confusing. It is not a novel architectural component but rather a retrained fusion of existing layers. This terminology obfuscates the contribution and makes it harder to understand the novelty of the approach.
   
2. Lack of Comparisons with Operator Fusion Techniques: The paper does not compare its method with operator fusion techniques that do not require retraining, such as those implemented in frameworks like Theano or TensorFlow XLA. Without these comparisons, it is unclear whether the proposed method offers significant advantages over existing approaches.
3. Reproducibility Concerns: The paper lacks sufficient details for reproducibility. For example, the relationship between the depth of the fused layers and the original network is unclear, and there is no information on the retraining process, such as the number of epochs or whether distillation techniques were used.
4. Baseline Ambiguities: It is unclear whether BatchNorm layers were folded into the baseline figures in Table 7 without retraining. This omission raises questions about the validity of the reported speed-up and memory savings.
5. Open-Sourcing: While not mandatory, open-sourcing the implementation would significantly enhance the utility and credibility of the work, especially given the reproducibility issues.
Suggestions for Improvement
1. Clarify the Contribution: Clearly define the "DeepRebirth layer" and its novelty. Avoid using misleading terminology that suggests a new architectural component when it is a retraining-based fusion technique.
2. Expand Comparisons: Include a detailed comparison with operator fusion techniques that do not require retraining. This will help position the work within the broader literature and demonstrate its advantages.
3. Provide Reproducibility Details: Include more details about the retraining process, such as the number of epochs, learning rates, and whether distillation techniques were used. Additionally, clarify the depth relationship between the fused and original layers.
4. Address Baseline Ambiguities: Explicitly state whether BatchNorm layers were folded into the baseline figures without retraining. This will ensure the reported results are interpretable and fair.
5. Open-Source the Code: While not mandatory, releasing the implementation would greatly improve the paper's impact and allow the community to validate and build upon the work.
Questions for the Authors
1. How does DeepRebirth compare to operator fusion techniques like TensorFlow XLA in terms of speed-up and memory savings?
2. Were BatchNorm layers folded into the baseline figures in Table 7 without retraining? If so, how does this affect the reported results?
3. How many epochs were required for retraining the rebirth layers, and were any distillation techniques used to preserve accuracy?
4. Can you provide more details on the depth relationship between the fused layers and the original network?
In conclusion, while the paper presents an interesting idea, it requires significant revisions to clarify its contributions, provide fair comparisons, and address reproducibility concerns.