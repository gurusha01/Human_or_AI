Review
The paper introduces a novel framework for nonparametric neural networks that dynamically adjust their size during training using sparse regularization. This approach eliminates the need for expensive global searches over network architectures, offering a theoretically sound and practical alternative. The authors propose a new optimization algorithm, Adaptive Radial-Angular Gradient Descent (AdaRad), and demonstrate its effectiveness on benchmark datasets. The paper also provides theoretical guarantees for the framework and explores its performance across various datasets.
Decision: Reject
While the paper presents an innovative idea with strong theoretical foundations, it falls short in addressing key practical concerns and empirical limitations. The primary reasons for rejection are: (1) insufficient discussion and empirical evidence regarding the computational overhead introduced by the proposed method, and (2) the poor performance of nonparametric networks on convex datasets, which raises questions about the generalizability of the approach.
Supporting Arguments
1. Computational Complexity: The proposed framework introduces additional computational steps, such as weight decomposition, coordinate transformations, and the periodic addition/removal of units. While these are integral to the method, the paper does not provide sufficient analysis of the computational trade-offs. Running time experiments comparing the proposed method with traditional approaches are notably absent. This omission makes it difficult to assess the practicality of the framework for large-scale applications.
2. Performance on Convex Datasets: The empirical results reveal that nonparametric networks produce smaller and less effective models on convex datasets compared to parametric networks. The authors do not adequately explain this behavior or propose solutions to mitigate it. This limitation undermines the claim of general applicability and raises concerns about the robustness of the method.
Additional Feedback
1. Scalability: While the paper includes experiments on a large dataset (poker), it would benefit from a broader evaluation on datasets with varying characteristics, such as high-dimensional data or imbalanced classes. This would provide a more comprehensive understanding of the framework's scalability and versatility.
2. Ablation Studies: The paper introduces multiple novel components, such as CapNorm and AdaRad. However, it does not conduct ablation studies to isolate the contribution of each component to the overall performance. This would strengthen the empirical validation of the proposed method.
3. Clarity of Presentation: The paper is dense and includes a significant amount of theoretical and experimental detail. While this demonstrates rigor, it also makes the paper challenging to follow. A more concise presentation, particularly in the experimental section, would improve readability.
Questions for the Authors
1. Can you provide running time comparisons between your method and traditional approaches (e.g., grid search or random search)?
2. Why do nonparametric networks underperform on convex datasets? Are there specific characteristics of these datasets that make them unsuitable for your framework?
3. How does the performance of AdaRad compare to other optimization algorithms (e.g., Adam or RMSprop) in terms of convergence speed and computational cost?
Addressing these concerns and questions would significantly strengthen the paper and its contribution to the field.