Review of the Paper
Summary of Contributions
This paper addresses the challenge of defining and evaluating creativity in machine learning, focusing on the generation of out-of-distribution novelty. The authors propose an actionable definition of creativity as the ability to generate objects from unknown classes, leveraging training data from known classes. They introduce a novel experimental framework based on holding out entire classes during training to evaluate generative models on their capacity to produce meaningful novelty. The paper also reviews existing evaluation metrics, critiques likelihood-based methods, and proposes alternative metrics such as out-of-class objectness and out-of-class count. Through extensive experiments on autoencoders and GANs, the authors identify architectures and hyperparameter configurations that foster out-of-distribution novelty generation. This work contributes to computational creativity by providing a systematic framework for studying and evaluating novelty generation, thereby opening new avenues for research in this domain.
Decision: Accept
The paper should be accepted due to its well-motivated approach, clear contributions to the field of computational creativity, and rigorous experimental evaluation. The proposed framework and metrics address a critical gap in the literature, enabling systematic exploration of out-of-distribution novelty generation. The experimental results are thorough and provide valuable insights into the design of generative models for creativity.
Supporting Arguments
1. Problem Definition and Motivation: The paper tackles the underexplored problem of out-of-distribution novelty generation, which is crucial for advancing computational creativity. The motivation is well-grounded in both machine learning and creativity research, and the authors effectively position their work within the existing literature.
   
2. Novel Framework and Metrics: The proposed hold-out class framework is a significant contribution, providing a practical and systematic way to evaluate generative models on their creative potential. The introduction of out-of-class objectness and count metrics is innovative and addresses the limitations of traditional likelihood-based evaluation methods.
3. Experimental Rigor: The authors conduct extensive experiments on a diverse set of generative models, including autoencoders and GANs, and analyze their performance using the proposed metrics. The results are well-documented and demonstrate the effectiveness of the framework in identifying models capable of generating meaningful novelty.
4. Impact on the Field: By bridging the gap between machine learning and computational creativity, this work has the potential to inspire further research and innovation in both fields. The framework and metrics could become standard tools for evaluating creativity in generative models.
Suggestions for Improvement
1. Clarity in Definitions: While the paper provides an actionable definition of creativity, it would benefit from a more detailed discussion of what constitutes "meaningful" novelty. For instance, how do the authors distinguish between noise and truly creative outputs in a principled way?
2. Human Evaluation: The paper acknowledges the importance of human evaluation but does not fully integrate it into the experimental pipeline. Incorporating a more systematic human evaluation component, such as a well-designed visual Turing test or novelty assessment, could strengthen the results.
3. Broader Applicability: The experiments focus primarily on MNIST and letter datasets. Extending the framework to more complex datasets (e.g., natural images or 3D objects) would enhance the generalizability of the findings.
4. Algorithmic Insights: While the paper identifies architectures and hyperparameters conducive to novelty generation, it could delve deeper into why certain configurations work better. This would provide more actionable insights for designing creative generative models.
Questions for the Authors
1. How do you ensure that the generated out-of-distribution objects are not merely random noise but exhibit meaningful novelty? Could you provide more examples or a clearer definition of "valuable novelty"?
2. Have you considered applying the proposed framework to more complex datasets or real-world tasks? If so, what challenges do you anticipate?
3. Can the proposed metrics (e.g., out-of-class objectness) be directly optimized during training, or are they limited to post-hoc evaluation?
4. How do you envision integrating human evaluation into the pipeline in a scalable and reproducible manner?
In conclusion, this paper makes a significant contribution to the study of computational creativity and novelty generation. While there is room for improvement in terms of clarity, broader applicability, and human evaluation, the proposed framework and metrics are valuable tools for advancing the field.