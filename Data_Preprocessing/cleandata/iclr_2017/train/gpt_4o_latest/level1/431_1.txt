The paper introduces PALEO, an analytical performance model designed to estimate the scalability and performance of deep learning systems. The authors address the challenge of efficiently leveraging parallel and distributed computing infrastructure for training and deploying neural networks, a task complicated by the variability in performance across architectures, datasets, and hardware setups. PALEO extracts computational requirements from a neural network's architecture and maps them to a design space of algorithms, hardware, and communication strategies to estimate execution times. The paper demonstrates PALEO's robustness across different architectures (e.g., AlexNet, Inception, GANs), hardware configurations, and parallelization strategies, showing its ability to closely approximate empirical results from existing systems like TensorFlow and FireCaffe. The authors also use PALEO to explore hypothetical setups, providing insights into scalability under various conditions.
Decision: Accept
The paper is well-motivated, addresses a significant problem in scalable deep learning, and provides a novel contribution in the form of an analytical model. The methodology is rigorous, and the results convincingly support the claims. The ability to predict performance without exhaustive benchmarking is a valuable contribution to the field.
Supporting Arguments:
1. Problem Significance: The problem of optimizing deep learning systems for scalability is critical as models grow larger and more complex. PALEO provides a practical solution by enabling performance prediction without requiring costly empirical testing.
2. Novelty and Rigor: The model is novel in its approach to decomposing execution time into computation and communication components, accounting for hardware and software inefficiencies through a "Platform Percent of Peak" parameter. The experiments validate PALEO's accuracy across diverse scenarios, including real-world and hypothetical setups.
3. Results and Validation: The paper demonstrates that PALEO's predictions align closely with empirical results from established systems, such as TensorFlow and FireCaffe. The case studies and hypothetical analyses further highlight its utility in exploring design trade-offs.
Suggestions for Improvement:
1. Clarity on Assumptions: While the paper outlines assumptions (e.g., hardware parameters, communication schemes), it would benefit from a clearer discussion of their limitations. For instance, how does PALEO handle non-deterministic or asynchronous parameter updates, which are increasingly common in distributed training?
2. Broader Applicability: The focus is primarily on CNNs and GANs. Expanding the discussion to other architectures (e.g., transformers) would improve the paper's generalizability.
3. Comparison with Alternatives: While the paper mentions existing benchmarking efforts, a more explicit comparison with alternative performance modeling approaches would strengthen the case for PALEO's uniqueness and advantages.
4. Usability for Practitioners: The paper could provide more details on how practitioners can integrate PALEO into their workflows. For example, are there tools or APIs available for easy adoption?
Questions for the Authors:
1. How does PALEO handle dynamic neural network architectures, such as those with conditional execution paths or recurrent structures?
2. The paper assumes a linear scaling of GPUs on the same host. Have you validated this assumption across different hardware configurations, and how might deviations affect PALEO's accuracy?
3. Can PALEO be extended to model energy consumption or cost efficiency, which are increasingly important metrics in large-scale training?
In conclusion, the paper makes a strong contribution to the field of scalable deep learning and is well-suited for acceptance. Addressing the suggested improvements would further enhance its impact and applicability.