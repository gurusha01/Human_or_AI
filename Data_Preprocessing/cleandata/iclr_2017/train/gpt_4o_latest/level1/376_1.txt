Review of the Paper
Summary of Contributions
This paper investigates the expressiveness and capacity bottlenecks of recurrent neural networks (RNNs), focusing on two key dimensions: the ability to store task-related information in parameters and the ability to remember input history in hidden units. Through extensive experiments, the authors demonstrate that common RNN architectures (vanilla RNN, LSTM, GRU, and novel variants UGRNN and +RNN) achieve similar capacity bounds when trained optimally, with task capacity scaling linearly with the number of parameters (approximately 5 bits per parameter). The study further reveals that differences in performance across architectures are primarily due to trainability rather than inherent capacity differences. The authors propose two novel architectures, UGRNN and +RNN, which exhibit improved trainability in certain scenarios, especially for deeper networks. The work provides a valuable empirical framework for understanding RNN capacity and trainability, offering practical insights into architecture selection based on task complexity and resource constraints.
Decision: Accept
The paper should be accepted for its significant empirical contributions to understanding RNN capacity and trainability. The key reasons for this decision are:
1. Novel Insights: The study provides a rigorous and novel empirical analysis of RNN capacity, challenging common assumptions about the superiority of gated architectures.
2. Practical Relevance: The findings offer actionable guidance for practitioners, such as when to prefer vanilla RNNs versus gated architectures based on task difficulty and resource budgets.
3. Thorough Methodology: The experiments are comprehensive, involving multiple architectures, tasks, and hyperparameter optimization, ensuring robustness and reproducibility of results.
Supporting Arguments
1. Well-Motivated Problem: The paper addresses a critical and underexplored question in the literatureâ€”whether differences in RNN performance stem from capacity or trainability. This is a timely and important topic given the widespread use of RNNs in various applications.
2. Empirical Rigor: The authors employ a Gaussian Process-based hyperparameter tuner to ensure fair comparisons across architectures, running thousands of experiments for each configuration. This level of rigor is commendable and strengthens the validity of the conclusions.
3. Novel Architectures: The introduction of UGRNN and +RNN adds value, as these architectures address specific trainability challenges in deep networks, which is a practical concern for many applications.
Suggestions for Improvement
1. Theoretical Context: While the empirical results are robust, the paper could benefit from a stronger theoretical grounding to explain why capacity saturates at approximately 5 bits per parameter or why gating reduces capacity slightly.
2. Clarity in Results: Some results, such as the comparison of trainability across architectures, could be presented more clearly. For instance, a summary table highlighting key findings across tasks and architectures would improve readability.
3. Broader Evaluation: The paper focuses on synthetic and controlled tasks. Including real-world tasks, such as machine translation or speech recognition, would strengthen the practical relevance of the findings.
4. Computational Cost: The experiments required "CPU-millennia" worth of computation. While this ensures robustness, it raises concerns about accessibility for researchers with limited resources. A discussion on how to approximate these results with fewer resources would be helpful.
Questions for the Authors
1. Could you elaborate on why gating mechanisms reduce capacity slightly? Is this due to the additional parameters used for gates, or does it stem from the gating dynamics themselves?
2. How do the findings generalize to real-world tasks with noisy or non-stationary data? Have you tested the architectures on such tasks?
3. The paper suggests that vanilla RNNs are preferable for tasks with large training budgets. Could you provide more guidance on how to balance training and inference costs in practice?
4. How sensitive are the results to the choice of hyperparameter tuning method? Would simpler tuning methods yield similar conclusions?
In conclusion, this paper makes a significant contribution to the understanding of RNN capacity and trainability, and its findings are both scientifically rigorous and practically relevant. Addressing the above suggestions would further enhance the impact of this work.