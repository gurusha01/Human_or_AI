The paper explores the concept of sample importance in training deep neural networks using stochastic gradient descent (SGD). It introduces a novel metric to quantify the contribution of individual samples to parameter updates during training and provides empirical insights into how "easy" and "hard" samples affect different layers of the network at different training stages. The authors challenge the conventional wisdom of curriculum learning by demonstrating that mixing easy and hard samples in training batches leads to better performance than segregating them. The paper also provides a detailed empirical analysis on MNIST and CIFAR-10 datasets, showing that easy samples primarily shape the top layers early in training, while hard samples influence the bottom layers later. Additionally, the authors propose extensions to calculate the cumulative impact of samples on the final model.
Decision: Accept
The paper makes a significant contribution to understanding sample importance in deep learning, providing both theoretical insights and practical implications for training strategies. The key reasons for acceptance are:  
1. Novelty and Relevance: The concept of sample importance and its decomposition across layers and training stages is a novel and well-motivated contribution to the field. The findings challenge existing paradigms like curriculum learning, making the work relevant and impactful.  
2. Scientific Rigor: The empirical analysis is thorough, using standard datasets and well-defined experimental setups. The results are consistent and scientifically rigorous, supporting the claims made by the authors.
Supporting Arguments
1. Well-Motivated Approach: The paper is well-situated in the literature, referencing prior work on curriculum learning, self-paced learning, and sample leverage in statistics. The authors clearly articulate the gap in understanding sample importance in deep neural networks and provide a compelling hypothesis.  
2. Empirical Validation: The experiments are comprehensive, covering multiple datasets, architectures, and batch construction methods. The results are consistent across different random initializations, lending credibility to the findings.  
3. Practical Implications: The insights into batch construction and the importance of mixing easy and hard samples have direct implications for improving training efficiency and performance.
Suggestions for Improvement
1. Clarity in Definitions: While the concept of sample importance is well-defined, the mathematical notation (e.g., φ, β, α) can be overwhelming for readers unfamiliar with the topic. Simplifying or providing intuitive explanations alongside the equations would improve accessibility.  
2. Broader Applicability: The experiments are limited to fully connected networks on MNIST and CIFAR-10. Extending the analysis to more complex architectures like CNNs or RNNs would strengthen the paper's generalizability.  
3. Computational Feasibility: The proposed method for calculating sample importance is computationally intensive. Discussing potential approximations or scalable alternatives would enhance the practical utility of the work.  
4. Visualization: While the paper includes several figures, additional visualizations (e.g., heatmaps of sample importance across layers) could make the results more interpretable.
Questions for the Authors
1. How does the definition of sample importance generalize to architectures like CNNs or RNNs, where layers have different roles (e.g., feature extraction vs. temporal modeling)?  
2. Did you observe any class-specific patterns in sample importance (e.g., certain classes contributing more to specific layers)?  
3. Can the proposed metric be used to identify and remove redundant samples from the training set without compromising model performance?  
4. How does the choice of hyperparameters (e.g., learning rate, batch size) affect the observed patterns of sample importance?
In conclusion, the paper provides a novel and impactful contribution to understanding sample importance in deep learning. With minor improvements and extensions, it has the potential to influence both theoretical research and practical training strategies.