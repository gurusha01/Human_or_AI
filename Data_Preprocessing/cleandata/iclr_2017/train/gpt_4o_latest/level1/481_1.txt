The paper addresses the challenge of scaling adversarial training to large datasets and models, specifically ImageNet and Inception v3, to improve robustness against adversarial examples. Key contributions include recommendations for scaling adversarial training, insights into the transferability of adversarial examples, the resolution of the "label leaking" effect, and observations on the relationship between model capacity and adversarial robustness. The authors also provide a detailed evaluation of adversarial training methods and their effectiveness against various attack strategies, offering practical guidance for improving model robustness.
Decision: Accept.  
The paper makes significant contributions to the field of adversarial robustness by extending adversarial training to large-scale datasets and models, a critical step for real-world applications. The findings are well-supported by rigorous experiments, and the insights into transferability, label leaking, and model capacity are novel and valuable for both researchers and practitioners.
Supporting Arguments:  
1. Well-Motivated Problem and Contributions: The paper tackles a pressing issue in adversarial machine learningâ€”scaling adversarial training to large datasets and models. This is a natural progression from prior work on smaller datasets like MNIST and CIFAR-10. The contributions are clearly articulated and address practical challenges, such as computational efficiency and the trade-off between clean and adversarial accuracy.  
2. Scientific Rigor: The experiments are thorough, covering a wide range of adversarial methods, model configurations, and training strategies. The results are presented with sufficient detail, including ablation studies, transferability analysis, and the impact of model capacity, which strengthens the validity of the claims.  
3. Novel Insights: The discovery of the label leaking effect and its mitigation strategies is particularly noteworthy. Additionally, the observation that larger models are more robust to adversarial examples provides actionable guidance for designing robust architectures.
Suggestions for Improvement:  
1. Iterative Attack Robustness: While the paper acknowledges that adversarial training with one-step methods does not confer robustness to iterative attacks, it would be helpful to explore potential solutions or discuss why iterative methods remain a challenge.  
2. Computational Trade-offs: The paper mentions the high computational cost of adversarial training but does not quantify it. Including a comparison of training times or resource requirements would provide a clearer picture of the practicality of the proposed methods.  
3. Broader Applicability: The focus on Inception v3 and ImageNet is valuable, but it would be beneficial to discuss how the findings generalize to other architectures or domains, such as natural language processing or reinforcement learning.  
Questions for the Authors:  
1. Can the proposed adversarial training methods be adapted to iterative attacks without significant computational overhead?  
2. How do the findings on model capacity and robustness generalize to architectures beyond Inception v3?  
3. Could the label leaking effect have implications for other adversarial defense strategies, such as defensive distillation or randomized smoothing?  
In summary, the paper makes a strong case for acceptance due to its impactful contributions, rigorous methodology, and practical relevance. Addressing the suggested improvements would further enhance its value to the community.