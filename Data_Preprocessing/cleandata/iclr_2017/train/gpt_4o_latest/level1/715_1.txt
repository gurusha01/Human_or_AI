Review of the Paper
The paper proposes a novel approach for reducing the computational complexity of deep convolutional neural networks (CNNs) through feature map and kernel pruning. The authors introduce a simple and generic strategy to select pruning masks, which involves randomly generating masks and selecting the least adversarial one based on validation set performance. The proposed method is efficient, employing one-shot pruning followed by retraining, and is evaluated across multiple datasets (CIFAR-10, CIFAR-100, SVHN, and MNIST). The results demonstrate that the method achieves significant sparsity (60-70%) with minimal performance degradation (less than 1% increase in misclassification rate). The authors also highlight the scalability of their approach and its suitability for GPU and VLSI implementations due to the coarse-grained pruning granularity.
Decision: Accept
The paper is recommended for acceptance due to its well-motivated problem, scientifically rigorous methodology, and strong empirical results. The key reasons for this decision are:  
1. Novelty and Practical Relevance: The proposed pruning strategy addresses a critical challenge in deploying deep learning models on resource-constrained devices by balancing computational efficiency and model performance.  
2. Comprehensive Evaluation: The method is extensively validated on multiple datasets and architectures, demonstrating its generalizability and scalability.  
Supporting Arguments
1. Problem Motivation and Placement in Literature: The paper is well-grounded in the existing literature on network pruning, clearly identifying the limitations of fine-grained pruning methods (e.g., irregular sparsity) and positioning its coarse-grained approach as a practical alternative. The discussion of related work is thorough, and the proposed method fills a gap by offering a computationally efficient and hardware-friendly pruning strategy.  
2. Scientific Rigor and Results: The methodology is sound, with detailed explanations of the pruning mask selection process and its computational cost. The results are compelling, showing that the proposed method achieves high sparsity with minimal accuracy loss. The comparison with existing methods (e.g., weight sum criterion) further strengthens the claims.  
3. Practical Implications: The paper emphasizes the practical benefits of the approach, particularly for VLSI and GPU-based implementations, making it highly relevant for real-world applications.
Suggestions for Improvement
1. Clarity on Mask Selection: While the random generation of pruning masks is explained, the rationale behind the choice of the number of masks (N) could be elaborated further. For instance, how does N scale with network size or pruning ratio?  
2. Hardware Evaluation: The paper discusses the suitability of the approach for GPUs and VLSI but does not provide experimental results on actual hardware. Including such evaluations would strengthen the practical claims.  
3. Ablation Studies: Additional experiments analyzing the impact of different pruning granularities (e.g., feature map vs. kernel pruning) on various architectures would provide deeper insights into the method's effectiveness.  
4. Comparison with Iterative Pruning: While the paper advocates for one-shot pruning, a more detailed comparison with iterative pruning methods in terms of accuracy and retraining time would be valuable.
Questions for the Authors
1. How does the proposed method perform on larger and more complex datasets, such as ImageNet?  
2. Can the approach be extended to other types of neural networks, such as transformers or recurrent networks?  
3. How sensitive is the method to the choice of the validation set used for selecting pruning masks?  
Overall, the paper makes a meaningful contribution to the field of network pruning and is a strong candidate for acceptance. Addressing the suggested improvements would further enhance its impact.