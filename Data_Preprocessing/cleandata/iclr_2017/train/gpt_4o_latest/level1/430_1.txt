Review of "Latent Sequence Decompositions (LSD): Learning Tokenization for Sequence-to-Sequence Models"
The paper introduces the Latent Sequence Decompositions (LSD) framework, which addresses the problem of fixed, static token decompositions in sequence-to-sequence (seq2seq) models. Unlike traditional approaches that rely on pre-defined tokenization schemes (e.g., characters, words, or word-pieces), LSD learns the decomposition of sequences during training, making it adaptable to both input and output domains. The framework marginalizes over possible decompositions during training and uses beam search for inference. The authors demonstrate the effectiveness of LSD on the Wall Street Journal (WSJ) Automatic Speech Recognition (ASR) task, achieving a 12.9% Word Error Rate (WER), outperforming a character-based baseline (14.8% WER). When combined with a convolutional encoder, LSD achieves a state-of-the-art WER of 9.6%.
Decision: Accept
Key reasons for acceptance:
1. Novel Contribution: The LSD framework is a significant advancement in seq2seq modeling, introducing a principled way to learn token decompositions dynamically. This addresses limitations of fixed tokenization schemes and has broad applicability across domains.
2. Empirical Validation: The paper demonstrates substantial improvements in WER on the WSJ ASR benchmark, with rigorous comparisons to baselines and prior work. The results are both meaningful and competitive.
Supporting Arguments:
1. Well-Motivated Approach: The paper is well-situated in the literature, clearly identifying the limitations of fixed tokenization in seq2seq models. The authors provide a strong theoretical foundation for LSD and contrast it with related work, such as CTC-based models and fixed word-piece approaches.
2. Scientific Rigor: The experimental setup is robust, with detailed descriptions of the architecture, training procedures, and hyperparameters. The results are reproducible and supported by ablation studies (e.g., varying n-gram sizes and vocabulary sizes). The use of beam search for inference and the exploration of token distributions further validate the claims.
3. Impact: The ability to learn token decompositions dynamically is a generalizable contribution that could benefit other seq2seq tasks, such as machine translation or text generation.
Suggestions for Improvement:
1. Clarity of Sampling Strategy: The training algorithm relies on an ε-greedy sampling strategy to avoid local minima. While this is described, more details on how ε is scheduled during training would improve reproducibility.
2. Computational Overheads: The paper does not discuss the computational cost of marginalizing over decompositions during training. A comparison of training time and resource requirements between LSD and baseline models would be helpful.
3. Broader Applicability: While the focus is on ASR, the authors could provide additional experiments or discussions on how LSD generalizes to other tasks, such as machine translation or summarization.
4. Error Analysis: An analysis of failure cases or qualitative examples of incorrect decompositions would provide insights into the limitations of the model.
Questions for the Authors:
1. How sensitive is the LSD framework to the choice of the initial token vocabulary (e.g., n-gram size or frequency thresholds)?
2. During inference, does the beam search introduce any biases or limitations in finding the optimal decomposition? Could alternative search strategies improve performance?
3. Could the ε-greedy exploration strategy be replaced with a more principled approach, such as reinforcement learning or variational inference?
In conclusion, the paper makes a strong theoretical and empirical contribution to seq2seq modeling. While there are areas for further exploration, the novelty and impact of the LSD framework justify acceptance.