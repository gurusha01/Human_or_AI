Review
The paper proposes a novel framework for boosting generative models (BGMs) by ensembling weak generative and discriminative models to improve density estimation, sample generation, and unsupervised feature learning. Inspired by supervised boosting, the authors extend the concept to unsupervised settings, introducing a meta-algorithm that iteratively trains models to correct the errors of previous ones. The framework is flexible, allowing the use of various generative models (e.g., VAEs, RBMs) and discriminative classifiers (e.g., CNNs). Theoretical guarantees are provided for the improvement of the ensemble's fit to the data distribution, and empirical results demonstrate the effectiveness of BGMs across multiple tasks on real and synthetic datasets.
Decision: Accept
The paper makes a significant contribution to the field of generative modeling by introducing a general-purpose boosting framework for unsupervised learning. The key reasons for acceptance are:  
1. Novelty and Generality: The proposed framework is innovative, extending boosting to generative models in a principled manner. It provides a flexible approach that can incorporate a wide range of existing models.  
2. Theoretical and Empirical Rigor: The paper offers strong theoretical guarantees for the improvement of the ensemble and demonstrates its effectiveness through comprehensive experiments on density estimation, sample generation, and feature learning.
Supporting Arguments
1. Well-Motivated Problem: The paper addresses a critical challenge in generative modeling—improving the accuracy of imperfect models in fitting complex distributions. The motivation is clearly articulated, and the approach is well-situated in the literature, building on concepts from boosting and recent advances in generative models.  
2. Theoretical Contributions: The authors derive sufficient and necessary conditions for the improvement of the ensemble and provide proofs for key results. This adds a strong theoretical foundation to the proposed method.  
3. Empirical Validation: The experiments are thorough, covering multiple tasks and datasets. Results show that BGMs outperform baseline models in density estimation, generate higher-quality samples, and learn better feature representations for downstream tasks. The computational efficiency of the approach is also highlighted.  
Suggestions for Improvement
1. Clarity in Experimental Setup: While the experiments are comprehensive, some details (e.g., the choice of weights for intermediate models and stopping criteria) are heuristic and could benefit from further justification. Providing more insights into these design choices would strengthen the paper.  
2. Comparison with GANs: Although the paper briefly discusses GANs, a direct empirical comparison would provide a clearer understanding of the relative strengths and weaknesses of BGMs.  
3. Scalability: The paper mentions that MCMC sampling for discriminative models can be computationally expensive. Exploring alternative sampling strategies or discussing their feasibility for large-scale datasets would be valuable.  
4. Intermediate Model Selection: The paper does not explore how to optimally select intermediate models or their architectures. This could be an interesting direction for future work.  
Questions for Authors
1. How sensitive is the performance of BGMs to the choice of weights (α) for intermediate models? Could an adaptive weighting scheme improve results?  
2. Can the framework handle more complex datasets, such as high-resolution natural images? If so, what challenges might arise?  
3. How does the proposed method compare to state-of-the-art GANs or diffusion models in terms of sample quality and training stability?  
Overall, the paper presents a compelling and well-executed contribution to generative modeling, and I recommend acceptance. With minor clarifications and extensions, it has the potential to significantly impact the field.