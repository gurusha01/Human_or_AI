Review
Summary
The paper presents a novel JavaScript-based deep learning framework that enables training of large-scale convolutional neural networks (CNNs) such as VGGNet and ResNet directly on web browsers. By leveraging the WebCL framework for GPGPU acceleration, the authors demonstrate the feasibility of distributed training using ordinary personal computers and smartphones without requiring specialized software installations. The contributions include the development of a high-performance matrix library (Sushi2) and a deep learning library (Sukiyaki2), both of which are open-source. The experiments validate the practicality of the framework, showcasing its ability to train large-scale CNNs and achieve distributed training speedups using multiple clients.
Decision: Reject
While the paper addresses an interesting and relevant problem, the submission falls short in terms of scientific rigor and clarity in key areas. The primary reasons for rejection are: (1) insufficient empirical evidence to support claims of scalability and performance compared to state-of-the-art frameworks, and (2) reliance on WebCL, which is not widely supported in modern web browsers, limiting the practical applicability of the proposed approach.
Supporting Arguments
1. Strengths:
   - The paper tackles an important problem of democratizing deep learning by enabling training on widely accessible devices like personal computers and smartphones.
   - The open-source implementation of Sushi2 and Sukiyaki2 is a valuable contribution to the community, offering a starting point for further exploration of JavaScript-based deep learning.
   - The experimental results demonstrate the feasibility of training large-scale CNNs like VGGNet and ResNet using JavaScript, which is a significant technical achievement.
2. Weaknesses:
   - Performance Limitations: While the framework achieves comparable performance to OpenCL-based Caffe, it remains significantly slower than CUDA-based frameworks like Caffe with cuDNN. The paper does not provide sufficient analysis of how these performance gaps could be addressed in future iterations.
   - Reliance on WebCL: WebCL is not a standard feature in most modern web browsers, which severely limits the practical applicability of the framework. The authors acknowledge this limitation but do not propose concrete alternatives or solutions.
   - Scalability: The distributed training experiments are limited in scope, with a maximum of four clients. The scalability of the framework to larger numbers of clients or more complex networks is not convincingly demonstrated.
   - Communication Overhead: The paper identifies communication overhead as a bottleneck in distributed training but does not explore advanced techniques (e.g., gradient compression or asynchronous updates) to mitigate this issue.
Suggestions for Improvement
1. Empirical Validation: Provide more comprehensive experiments comparing the framework's performance against state-of-the-art deep learning libraries (e.g., TensorFlow.js, PyTorch) across a wider range of tasks and hardware configurations.
2. Broader Applicability: Explore alternatives to WebCL, such as WebGPU or WebGL, which are more widely supported in modern browsers. This would significantly enhance the framework's usability and adoption potential.
3. Scalability Analysis: Conduct experiments with a larger number of distributed clients and more complex networks to better demonstrate the scalability of the approach.
4. Communication Optimization: Investigate techniques to reduce communication overhead in distributed training, such as gradient sparsification, quantization, or parameter server architectures.
5. Clarity and Structure: Improve the clarity of the paper by reducing redundancy and providing a more concise explanation of the technical implementation. For example, the description of Sushi2 could be streamlined to focus on its unique features and advantages.
Questions for the Authors
1. How does the performance of Sushi2 and Sukiyaki2 compare to TensorFlow.js, which also supports JavaScript-based deep learning?
2. Given the limited support for WebCL in modern browsers, have you considered alternative frameworks like WebGPU or WebGL for GPGPU acceleration?
3. Can you provide more details on the communication efficiency in distributed training? For instance, how does the use of 8-bit gradient representation impact model accuracy?
4. What are the specific challenges in implementing convolution operations that achieve performance parity with CUDA-based frameworks, and how do you plan to address them?
In summary, while the paper makes a commendable effort to address an important problem, it requires significant improvements in experimental rigor, scalability analysis, and practical applicability to warrant acceptance.