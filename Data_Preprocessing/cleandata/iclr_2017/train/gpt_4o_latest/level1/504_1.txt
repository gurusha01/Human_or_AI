Review of the Paper
Summary of Contributions
This paper addresses the critical challenge of reward function design and exploration time in reinforcement learning (RL), particularly in real-world robotic tasks. The authors propose a novel method that leverages intermediate visual representations learned by pre-trained deep models to infer dense and smooth perceptual reward functions from a small number of human demonstrations. The method identifies key intermediate steps of a task without requiring explicit sub-goal supervision, enabling RL agents to learn complex robotic manipulation skills. Notably, the approach demonstrates its efficacy in real-world tasks such as door opening and liquid pouring, achieving generalization across different visual scenes and embodiments (e.g., human vs. robot demonstrations). The paper makes significant contributions, including: (1) a scalable method for perceptual reward learning, (2) the first demonstration of vision-based reward learning for complex robotic tasks from human videos, and (3) empirical validation of the learned reward functions in real-world robotic experiments.
Decision: Accept
The paper is recommended for acceptance due to its innovative approach to reward learning, strong empirical results, and potential to advance real-world RL applications. The key reasons for this decision are: (1) the method addresses a critical bottleneck in RL by reducing the dependency on manually engineered reward functions, and (2) the experimental results convincingly demonstrate the feasibility and robustness of the proposed approach in real-world scenarios.
Supporting Arguments
1. Problem Significance and Novelty: The paper tackles a well-recognized challenge in RLâ€”designing reward functions for real-world tasks. The proposed approach of leveraging pre-trained visual features to infer dense reward functions is both novel and impactful, as it eliminates the need for additional sensors or explicit sub-goal annotations.
   
2. Empirical Rigor: The experiments are well-designed and include both qualitative and quantitative evaluations. The method is validated on two distinct tasks (door opening and liquid pouring), with results showing that the learned reward functions are effective and generalizable. The real-world robotic experiments further strengthen the paper's contributions by demonstrating practical applicability.
3. Positioning in Literature: The paper is well-situated in the context of prior work, addressing limitations of existing methods (e.g., reliance on kinesthetic demonstrations or low-dimensional state spaces). The authors provide a clear comparison with related approaches, highlighting the advantages of their method.
Suggestions for Improvement
While the paper is strong overall, the following points could enhance its clarity and impact:
1. Failure Case Analysis: The paper mentions some failure cases (e.g., noisy intermediate rewards for door opening and challenges with transparent liquid in pouring tasks). A more detailed analysis of these cases, along with potential solutions or future directions, would strengthen the discussion.
   
2. Scalability and Generalization: While the method works well for the tasks presented, it would be helpful to discuss its scalability to more complex tasks or environments with higher variability. For instance, how does the method handle tasks with more ambiguous intermediate steps or noisy demonstrations?
3. Ablation Studies: The paper briefly discusses the role of feature selection and linear classifiers but could benefit from more detailed ablation studies to isolate the contributions of different components (e.g., feature selection, segmentation algorithm, and reward combination strategy).
4. Unsupervised Segmentation: The segmentation algorithm requires the number of steps to be manually specified. Exploring automated methods for determining the number of segments could make the approach more robust and user-friendly.
Questions for the Authors
1. How sensitive is the method to the quality and variability of the demonstrations? For example, would the approach work with suboptimal or noisy demonstrations?
2. Can the proposed reward functions be used in tasks with continuous, rather than discrete, intermediate steps? If so, how would the segmentation and reward combination methods adapt?
3. Have you considered extending the approach to multi-modal inputs (e.g., combining visual, tactile, and audio features)? If not, what challenges do you foresee in doing so?
In conclusion, the paper makes a significant contribution to the field of RL and robotic learning, with a well-motivated approach and strong empirical validation. Addressing the suggested improvements and questions could further enhance the paper's impact and clarity.