Review
Summary of Contributions
This paper introduces the multi-modal variational encoder-decoder framework, which addresses the limitations of simple, unimodal priors (e.g., multivariate Gaussian) in variational autoencoders (VAEs). The authors propose a novel piecewise constant prior that can efficiently model complex, multi-modal distributions, enabling richer latent representations. The framework is evaluated on two natural language processing tasks: document modeling and dialogue modeling. The proposed approach achieves state-of-the-art results on several document modeling benchmarks and demonstrates its ability to capture nuanced aspects of dialogue (e.g., time and events). The authors also provide a detailed analysis of how the piecewise prior complements Gaussian priors, offering insights into the learned latent representations.
Decision: Accept
The paper should be accepted because it makes a substantial contribution to the field of neural variational inference by introducing a novel prior that improves expressivity in latent variable models. The proposed framework is well-motivated, rigorously evaluated, and demonstrates clear empirical benefits over existing methods.
Supporting Arguments
1. Well-Motivated Approach: The authors clearly identify the limitations of unimodal priors in capturing complex data distributions, particularly in natural language tasks. The proposed piecewise constant prior is a natural and theoretically sound extension that addresses these limitations. The paper is well-situated in the literature, building on foundational work in VAEs and neural variational inference.
   
2. Strong Empirical Results: The framework achieves state-of-the-art results on document modeling tasks (e.g., 20 News-Groups, Reuters, CADE12) and demonstrates its utility in dialogue modeling. The analysis of latent variables (e.g., capturing time-related aspects in dialogue) further validates the effectiveness of the piecewise prior.
3. Scientific Rigor: The experiments are thorough, with comparisons to strong baselines (e.g., G-VHRED, NVDM) and ablation studies (e.g., varying the number of pieces in the prior). The authors also provide detailed insights into the learned latent representations, enhancing the interpretability of their approach.
Suggestions for Improvement
1. Clarity of Mathematical Derivations: While the derivations of the piecewise prior and its integration into the VAE framework are comprehensive, they are dense and could benefit from additional visual aids (e.g., diagrams illustrating the piecewise prior or its sampling process).
2. Human Evaluation in Dialogue Modeling: The human evaluation results for dialogue modeling show only marginal improvements over the baseline (G-VHRED). It would be helpful to include a more detailed qualitative analysis of the generated responses to better illustrate the advantages of the proposed model.
3. Computational Efficiency: The paper briefly mentions that iterative inference is computationally expensive. A more detailed discussion of the computational trade-offs (e.g., training time, scalability) would strengthen the paper.
4. Broader Applicability: While the paper focuses on natural language processing tasks, it would be valuable to discuss potential applications in other domains (e.g., vision, audio) where multi-modal distributions are prevalent.
Questions for the Authors
1. How sensitive is the performance of the piecewise prior to the choice of the number of components (e.g., 3 vs. 5 pieces)? Could this hyperparameter be learned during training?
2. The human evaluation results for dialogue modeling are relatively close between G-VHRED and H-VHRED. Could you provide examples of dialogues where H-VHRED clearly outperforms G-VHRED?
3. Have you considered combining the piecewise prior with other advanced priors (e.g., normalizing flows) to further enhance expressivity?
Conclusion
This paper makes a significant contribution to the field by addressing a critical limitation of VAEs and demonstrating the utility of the proposed framework on challenging NLP tasks. While there are areas for improvement, the novelty, rigor, and empirical results justify acceptance.