Review of the Paper
The paper presents a novel extension of PixelCNN for text-to-image synthesis with controllable object locations, incorporating conditioning on part keypoints and segmentation masks. The authors propose a gated conditional PixelCNN model that combines unstructured text and spatial constraints, trained end-to-end via maximum likelihood. The paper establishes quantitative baselines for three datasets (CUB, MHP, MS-COCO) and demonstrates the model's ability to generate diverse, interpretable, and controllable images. Compared to GAN-based approaches, the proposed method offers simpler and more stable training, principled likelihood evaluation, and improved results for human and bird image synthesis.
Decision: Accept
The paper is well-motivated, addresses an important problem in controlled image synthesis, and provides a significant contribution by extending PixelCNN to handle both text and spatial constraints. The results are scientifically rigorous, demonstrating qualitative and quantitative improvements over prior work. The ability to disentangle location and appearance, as well as the interpretability of the generated images, is a notable advancement.
Supporting Arguments
1. Problem and Motivation: The paper tackles the challenging problem of generating images conditioned on both textual descriptions and spatial constraints. This is a well-motivated research area with implications for controllable and interpretable generative modeling. The authors position their work effectively within the literature, contrasting their approach with GANs and highlighting the benefits of autoregressive models.
2. Scientific Rigor: The proposed method is thoroughly evaluated on three datasets, with both qualitative (diverse and interpretable samples) and quantitative (negative log-likelihood) results. The experiments demonstrate the model's ability to adhere to spatial constraints, generate plausible object appearances, and disentangle location and appearance to some extent. Comparisons with prior work, such as GAN-based methods, further validate the approach.
3. Novelty and Contribution: The extension of PixelCNN to incorporate segmentation masks and keypoints as spatial constraints is a meaningful contribution. The use of maximum likelihood training and principled likelihood evaluation offers a robust alternative to GAN-based methods, addressing issues like training instability and lack of interpretability.
Additional Feedback
1. Clarity: While the paper is generally clear, some sections (e.g., the technical details of the PixelCNN extension) could benefit from more concise explanations. Simplifying the description of the model architecture and training process would improve accessibility for a broader audience.
2. Limitations: The paper acknowledges limitations, such as noisy human faces and less accurate color constraints. However, a deeper discussion of failure cases and potential remedies (e.g., higher-capacity models or better disentanglement techniques) would strengthen the paper.
3. Evaluation: While the qualitative results are compelling, additional metrics (e.g., FID or IS scores) could provide a more comprehensive evaluation of image quality. Moreover, user studies to assess interpretability and controllability would add further validation.
Questions for the Authors
1. How does the model handle cases where the text and spatial constraints conflict (e.g., a caption describing a red bird but keypoints corresponding to a blue bird)?
2. Can the model generalize to unseen combinations of captions and spatial constraints, or does it rely heavily on training data distributions?
3. Have you considered scaling the model to higher-resolution images, and if so, what challenges do you anticipate?
In conclusion, this paper makes a strong contribution to the field of controllable image synthesis and is recommended for acceptance with minor revisions to improve clarity and evaluation.