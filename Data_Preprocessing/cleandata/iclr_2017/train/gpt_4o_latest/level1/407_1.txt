Review of the Paper: "A2T: Attend, Adapt and Transfer - An Attentive Deep Architecture for Adaptive Transfer in Reinforcement Learning"
Summary of Contributions
The paper introduces A2T (Attend, Adapt, and Transfer), a novel deep neural network architecture designed to address two critical challenges in transfer learning for reinforcement learning (RL): negative transfer and selective transfer. Negative transfer occurs when knowledge transfer from source tasks degrades performance on the target task, while selective transfer involves leveraging different source tasks for different parts of the target task's state space. A2T employs a soft attention mechanism to dynamically weigh the contributions of multiple source task solutions and a base network trained from scratch, enabling adaptive transfer. The architecture is versatile, supporting both policy and value function transfer. The authors demonstrate the efficacy of A2T through empirical evaluations on simulated environments (Chain World, Puddle World) and Atari games (Pong, Breakout), showcasing its ability to avoid negative transfer, selectively transfer from relevant source tasks, and adapt to imperfect or partially helpful source tasks.
Decision: Accept
The paper makes a significant contribution to the field of transfer learning in RL by proposing a well-motivated, generalizable framework that addresses long-standing challenges. The empirical results convincingly support the claims, and the methodology is scientifically rigorous. The novelty and practical utility of A2T make it a valuable addition to the literature.
Supporting Arguments for Decision
1. Clear Problem Definition and Motivation: The paper identifies two critical challenges in transfer learning—negative transfer and selective transfer—and positions A2T as a solution. The motivation is well-grounded in existing literature, with a comprehensive review of related work highlighting the gaps that A2T addresses.
2. Novelty and Generality of the Approach: The use of a soft attention mechanism to enable selective transfer at the granularity of individual states is a novel contribution. The architecture's flexibility to handle both policy and value function transfer, as well as its applicability across diverse RL algorithms, underscores its generality.
3. Empirical Validation: The experiments are thorough and demonstrate the effectiveness of A2T in various scenarios:
   - Selective Transfer: The attention mechanism successfully identifies and leverages relevant source tasks for different parts of the state space.
   - Negative Transfer Avoidance: A2T consistently avoids performance degradation when unfavorable source tasks are present.
   - Adaptation to Imperfect Experts: The framework adapts to partially helpful source tasks, outperforming baselines and demonstrating its robustness.
   - Visualization of Attention Weights: The visualizations provide intuitive insights into the mechanism's operation, reinforcing the empirical results.
4. Scientific Rigor: The methodology is sound, with detailed descriptions of the architecture, training procedures, and experimental setups. The use of baselines and ablation studies strengthens the validity of the results.
Suggestions for Improvement
1. Clarity and Conciseness: The paper is dense, and certain sections (e.g., experimental details) could be streamlined for readability. For instance, the description of the blurring experiments could be summarized, with detailed explanations moved to an appendix.
2. Broader Evaluation: While the experiments convincingly demonstrate A2T's effectiveness, additional evaluations on more complex, real-world RL tasks (e.g., robotics or continuous control) would further validate its generality.
3. Comparison with State-of-the-Art: The paper could include a more direct comparison with recent transfer learning methods, such as Progressive Neural Networks or other meta-learning approaches, to contextualize A2T's performance relative to the state-of-the-art.
4. Theoretical Insights: While the empirical results are strong, a theoretical analysis of A2T's convergence properties or its ability to avoid negative transfer would enhance the paper's depth.
Questions for the Authors
1. How does A2T handle scenarios where the state-action spaces of the source and target tasks differ significantly? Could the proposed architecture be extended to address such cases?
2. What are the computational overheads of training the attention network, particularly in large-scale RL tasks? How does this compare to other transfer learning methods?
3. Could the authors elaborate on the limitations of A2T? For instance, are there cases where the attention mechanism fails to identify relevant source tasks or struggles with highly noisy environments?
Conclusion
The paper presents a well-motivated, novel, and empirically validated framework for transfer learning in RL. While there is room for improvement in terms of clarity and broader evaluation, the contributions are significant, and the results are compelling. I recommend acceptance.