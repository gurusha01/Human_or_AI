Review
Summary of Contributions
The paper introduces a novel framework for adaptive computation in reinforcement learning (RL) through the use of a metacontroller. The metacontroller learns to optimize a sequence of internal simulations over predictive models (experts) to solve decision-making tasks efficiently. Unlike traditional fixed-policy approaches, the metacontroller dynamically decides how many computational iterations to perform and which experts to consult, balancing task difficulty and computational cost. The authors demonstrate the effectiveness of their approach on a challenging physics-based task, showing that the metacontroller outperforms reactive and iterative agents by adapting its computational strategy to the complexity of individual episodes. The paper also highlights the metacontroller's ability to leverage multiple experts with varying reliability and cost, showcasing its flexibility and potential for broader applications in planning and control.
Decision: Accept
Key reasons for acceptance are:
1. Novelty and Relevance: The paper addresses an important and underexplored problem in RL—adaptive computation—drawing inspiration from human cognition and bounded rationality. The proposed framework is innovative and well-motivated.
2. Strong Empirical Results: The experiments convincingly demonstrate the metacontroller's ability to achieve better performance and lower computational costs than baseline approaches. The results are robust across different task complexities and expert configurations.
Supporting Arguments
1. Well-Motivated Approach: The paper is well-grounded in prior literature, including cognitive science, neuroscience, and RL. The connections to bounded rationality and adaptive computation time are compelling, and the authors clearly articulate how their work builds on and extends these ideas.
2. Scientific Rigor: The experimental setup is thorough, with detailed descriptions of the tasks, models, and training procedures. The use of multiple baselines (reactive and iterative agents) and ablation studies (e.g., single vs. multiple experts) strengthens the validity of the claims. The results are presented clearly, with insightful analyses of the metacontroller's behavior.
3. Practical Implications: The framework has broad applicability, as it can be extended to more complex tasks, such as trajectory optimization and planning. The discussion section outlines promising directions for future work, including integration with Monte Carlo Tree Search and model-free RL methods.
Suggestions for Improvement
1. Clarity of Presentation: While the paper is comprehensive, some sections (e.g., the mathematical formulation in Section 2) are dense and could benefit from additional visual aids or simplified explanations. For instance, a flow diagram illustrating the metacontroller's decision-making process would enhance accessibility.
2. Comparison with Gradient-Based Methods: The authors briefly mention the potential advantages of their approach over gradient-based optimization but do not provide empirical comparisons. Including such comparisons would strengthen the claims about robustness and efficiency.
3. Entropy Term in Manager Training: The authors note that the entropy regularization term sometimes leads to suboptimal behavior, particularly in high-cost scenarios. Exploring alternative regularization strategies or annealing schedules could improve the metacontroller's performance and should be discussed in more detail.
4. Scalability: While the results on the physics-based task are impressive, it would be helpful to discuss the scalability of the framework to higher-dimensional tasks or environments with more complex dynamics.
Questions for the Authors
1. How does the metacontroller's performance scale with the number of experts or the complexity of the task? Are there any computational bottlenecks in training or inference?
2. Could the framework be extended to handle partially observable environments, where the state is not fully known?
3. Have you considered alternative methods for training the manager, such as policy gradient methods with entropy annealing or actor-critic approaches tailored to meta-level decisions?
Overall, this paper makes a significant contribution to the field of adaptive computation in RL and provides a strong foundation for future research. With minor revisions to improve clarity and address scalability, it has the potential to be a highly impactful work.