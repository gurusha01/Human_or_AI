The paper presents a novel approach to sensorimotor control in immersive environments by leveraging the cotemporal structure of high-dimensional sensory streams and low-dimensional measurement streams. The authors propose a supervised learning-based model that predicts future measurements from raw sensory inputs and dynamically adjusts to changing goals at test time. Unlike traditional reinforcement learning (RL) methods, which rely on scalar rewards, this approach uses dense, multidimensional feedback, enabling faster and more stable training. The model is evaluated in the context of the first-person game Doom, where it outperforms state-of-the-art RL methods on complex tasks and demonstrates strong generalization across environments and goals. Notably, the model won the Full Deathmatch track of the Visual Doom AI Competition, showcasing its practical effectiveness.
Decision: Accept  
The paper should be accepted due to its innovative departure from traditional RL paradigms and its demonstrated empirical superiority in challenging environments. The use of supervised learning for sensorimotor control with dense feedback is a compelling contribution, and the results convincingly support the claims of improved performance and generalization.
Supporting Arguments  
1. Well-Motivated Approach: The paper is well-grounded in the literature, addressing key challenges in RL, such as sparse rewards and goal generalization. The authors provide a clear rationale for their supervised learning-based approach, supported by theoretical insights and empirical evidence.  
2. Rigorous Evaluation: The experiments are extensive, covering multiple scenarios of increasing difficulty. The comparison with strong baselines (DQN, A3C, DSR) and the model's success in the Visual Doom AI Competition validate its effectiveness.  
3. Significant Contributions: The paper introduces a novel training paradigm that eliminates the need for fixed goals during training and demonstrates the advantages of vectorial feedback over scalar rewards. These contributions are likely to inspire further research in the field.
Suggestions for Improvement  
1. Memory and Temporal Abstraction: The paper acknowledges that the model is purely reactive and lacks memory or hierarchical skill organization. Future work could explore integrating memory-based architectures or temporal abstraction to enhance behavioral sophistication.  
2. Continuous Action Spaces: Extending the approach to continuous action spaces would broaden its applicability to real-world robotics and other domains.  
3. Ablation Study Details: While the ablation study is insightful, additional analysis on the impact of specific architectural components (e.g., the normalization layer in the action stream) would strengthen the findings.  
4. Clarity in Training Regimes: The description of randomized goal training could benefit from more detail, particularly regarding how the goal distributions influence the learned policies.
Questions for the Authors  
1. How sensitive is the model's performance to the choice of temporal offsets for predicting future measurements? Could this parameter be learned dynamically?  
2. Did the authors explore alternative architectures for the perception module (e.g., transformers) or goal representation? If not, how might these impact performance?  
3. How does the model handle scenarios where the sensory stream is noisy or partially observable? Would additional mechanisms, such as attention, improve robustness?  
In conclusion, the paper offers a significant contribution to the field of sensorimotor learning, with a well-motivated approach and strong empirical results. Addressing the suggested improvements could further enhance its impact.