Review
Summary of Contributions
This paper investigates the use of output regularizers, specifically a maximum entropy-based confidence penalty and label smoothing, as a means of improving generalization in supervised learning. The authors provide a systematic evaluation of these regularizers across six diverse benchmarks: image classification (MNIST, CIFAR-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT, WSJ). The paper makes two key contributions: (1) it demonstrates that penalizing low-entropy output distributions acts as a strong regularizer, improving state-of-the-art models across tasks without requiring hyperparameter modifications, and (2) it establishes a theoretical connection between the confidence penalty and label smoothing via the direction of the KL divergence. The experiments are comprehensive, covering a wide range of tasks and datasets, and the results consistently show improvements over baselines.
Decision: Accept
The paper is well-motivated, methodologically sound, and contributes both theoretical insights and practical advancements. The key reasons for acceptance are:  
1. Wide Applicability and Strong Empirical Results: The proposed regularizers improve performance across a diverse set of benchmarks, demonstrating their general utility.  
2. Theoretical Contribution: The connection between the confidence penalty and label smoothing provides a deeper understanding of output regularization techniques, which could inspire future work.  
Supporting Arguments
1. Problem Motivation and Placement in Literature: The paper addresses a well-known issue in deep learning—overfitting—and proposes a novel approach by focusing on output regularization, which has been underexplored in supervised learning. The authors effectively position their work within the existing literature, referencing related techniques like dropout, weight decay, and label smoothing, as well as drawing inspiration from reinforcement learning.  
2. Experimental Rigor: The experiments are thorough, with careful hyperparameter tuning and comparisons to strong baselines. The inclusion of diverse tasks (vision, language, and speech) strengthens the claim of wide applicability. The results consistently show improvements, and the authors provide detailed analyses, such as gradient norm comparisons, to support their findings.  
3. Clarity and Accessibility: The paper is well-written, with clear explanations of the methods and results. The theoretical connection between the confidence penalty and label smoothing is particularly insightful and is presented in a way that is easy to follow.
Suggestions for Improvement
1. Ablation Studies: While the experiments are comprehensive, it would be helpful to include ablation studies to isolate the effects of specific components, such as annealing and thresholding for the confidence penalty.  
2. Sensitivity Analysis: Although hyperparameter tuning is discussed, a more detailed sensitivity analysis of the regularization strength (β for confidence penalty, smoothing parameter for label smoothing) could provide additional insights into the robustness of the methods.  
3. Comparison to Other Regularizers: The paper briefly mentions techniques like virtual adversarial training (VAT) but does not include direct comparisons. Including VAT or other advanced regularizers in the experiments would strengthen the empirical claims.  
4. Scalability: While the paper evaluates large models, it would be useful to discuss the computational overhead introduced by the proposed regularizers, especially for tasks with limited resources.
Questions for the Authors
1. How do the proposed regularizers perform when combined with other regularization techniques, such as VAT or data augmentation?  
2. Can the confidence penalty be extended to non-uniform target distributions, as hinted in the discussion of alternative KL divergence directions?  
3. Did the authors observe any trade-offs, such as slower convergence or increased training time, when applying the confidence penalty or label smoothing?  
In conclusion, this paper makes a strong contribution to the field of supervised learning by systematically evaluating and advancing output regularization techniques. With minor improvements, it could become a foundational reference for future work in this area.