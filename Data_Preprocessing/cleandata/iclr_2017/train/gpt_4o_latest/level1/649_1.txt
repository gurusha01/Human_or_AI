Review of the Paper
This paper addresses the important and open question of what constitutes the "best" definition of context for learning word embeddings. The authors claim to provide the first systematic investigation of different context types and representations, evaluating their effectiveness across four tasks using 21 datasets. The paper aims to offer insights into context selection for word embedding models and provides publicly available code to serve as a guideline for the research community.
Decision: Reject
While the paper tackles a relevant and underexplored problem, the decision to reject is based on two primary reasons: (1) the lack of sufficient theoretical or empirical novelty in the approach, and (2) insufficient scientific rigor in the experimental design and analysis to support the claims. Below, I provide detailed arguments for this decision and suggestions for improvement.
Supporting Arguments for the Decision
1. Motivation and Positioning in the Literature: The paper identifies an important gap in understanding context selection for word embeddings. However, the discussion of related work is limited and does not sufficiently situate the contribution within the broader literature. For example, prior work on dynamic context representations or contextualized embeddings (e.g., BERT, GPT) is not adequately discussed. This omission weakens the motivation and novelty of the study.
2. Experimental Design and Rigor: While the authors evaluate context types across four tasks and 21 datasets, the paper lacks clarity on key methodological details, such as how context types are operationalized, the criteria for dataset selection, and the statistical significance of the results. Without this information, it is difficult to assess the validity and generalizability of the findings. Additionally, the paper does not compare its results to state-of-the-art contextualized embeddings, which are highly relevant to the problem.
3. Claims vs. Evidence: The paper claims to provide insights into context selection, but the presented results are descriptive rather than prescriptive. The insights are not synthesized into actionable guidelines, leaving the primary contribution incomplete.
Suggestions for Improvement
1. Expand the related work section to include recent advances in contextualized embeddings and clarify how this work complements or contrasts with them.
2. Provide more details on the experimental setup, including how context types are defined, why specific datasets were chosen, and whether results are statistically significant.
3. Strengthen the analysis by comparing the proposed approaches to state-of-the-art models and embedding techniques.
4. Translate the insights into clear, actionable guidelines to better serve the community.
Questions for the Authors
1. How do you define and operationalize the different context types in your experiments? Are these definitions consistent across all tasks and datasets?
2. How do your findings compare to contextualized embeddings like BERT or GPT? Could these models serve as baselines for your experiments?
3. Are the results statistically significant, and what measures were taken to ensure robustness across datasets?
In summary, while the paper addresses an important problem, it requires significant improvements in positioning, experimental rigor, and actionable contributions to merit acceptance.