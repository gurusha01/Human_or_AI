Review of the Paper
Summary of Contributions:  
This paper addresses a critical gap in the theoretical understanding of modern convolutional networks (convnets) by providing the first convergence guarantee applicable to nonsmooth, nonconvex architectures such as rectifier networks. The authors introduce the concept of the neural Taylor approximation and the associated Taylor loss, which serve as convex snapshots of the nonconvex optimization landscape. Using tools from online convex optimization, the paper demonstrates that the convergence rate of these networks matches the lower bound for convex nonsmooth functions. The authors also empirically validate these theoretical results across a range of optimizers, architectures, and tasks. Additionally, the paper explores the role of adaptive optimizers (e.g., RMSProp and Adam) in navigating shattered gradients and hypothesizes that their success stems from enhanced exploration of activation configurations. This hypothesis is supported by experiments that quantify exploration in the space of activation configurations.
Decision: Accept  
Key reasons for acceptance:  
1. Novel Theoretical Contribution: The paper provides a rigorous convergence guarantee for modern convnets, which is a significant theoretical advancement. The use of Taylor approximations to bridge the gap between convex optimization theory and deep learning is both innovative and impactful.  
2. Strong Empirical Validation: The experiments are comprehensive and convincingly demonstrate the practical relevance of the theoretical results, including the exploration behavior of adaptive optimizers.
Supporting Arguments:  
- The theoretical framework is well-motivated and builds on established literature in convex optimization and neural networks. The authors clearly position their work relative to prior research, highlighting its novelty and generality.  
- The empirical results are robust, with experiments conducted on diverse tasks (e.g., autoencoders on MNIST and convnets on CIFAR-10) and optimizers. The observed regret scaling as \(1/\sqrt{N}\) aligns with the theoretical predictions, lending credibility to the analysis.  
- The exploration of activation configurations is a novel perspective on the success of adaptive optimizers, and the experimental evidence supporting this hypothesis is compelling.  
Suggestions for Improvement:  
1. Clarity of Presentation: While the theoretical contributions are significant, the paper could benefit from a more accessible explanation of the neural Taylor approximation and its implications. Simplifying the notation and providing more intuitive examples would help readers unfamiliar with convex optimization.  
2. Broader Experimental Scope: The experiments focus on relatively simple architectures and datasets. Extending the analysis to more complex models (e.g., ResNets) and tasks (e.g., NLP benchmarks) would strengthen the generality of the findings.  
3. Exploration Metrics: The metrics used to quantify exploration (e.g., Hamming distance, activation-state switches) are insightful but could be complemented by additional measures, such as entropy-based metrics or visualization of activation patterns.  
Questions for the Authors:  
1. The paper hypothesizes that RMS-normalization encourages exploration in activation configurations. Could you provide more direct evidence linking this exploration to improved generalization performance, particularly on more challenging datasets?  
2. The theoretical guarantee depends on the Taylor losses encountered during training. How sensitive is the bound to the quality of these Taylor losses, and are there practical ways to ensure they are "good"?  
3. Given the exponential growth of kinks in deeper networks, do you anticipate that the proposed framework scales effectively to very deep architectures?  
Conclusion:  
This paper makes a substantial contribution to the theoretical understanding of modern convnets and provides valuable insights into the role of adaptive optimizers. While there is room for improvement in presentation and experimental scope, the novelty and rigor of the work make it a strong candidate for acceptance.