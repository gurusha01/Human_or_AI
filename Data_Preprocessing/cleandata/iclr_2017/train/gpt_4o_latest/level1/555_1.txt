Review
The paper addresses the critical challenge of efficiently exploring and ranking deep neural network (DNN) architectures for application in new domains, particularly tabular datasets. The authors propose a meta-learning-based approach that leverages topological features and early training dynamics (weights, biases, and activation functions) to predict architecture performance. They also systematically evaluate a large number of architectures across multiple datasets, providing insights into architecture transferability, the effectiveness of parallel layers, and the relative performance of DNNs versus conventional classifiers. The work is positioned as a first step toward improving the efficiency of DNN architecture exploration and understanding.
Decision: Accept
Key reasons for acceptance:
1. Novelty and Contribution: The paper introduces a novel meta-learning-based ranking method that combines architectural topology and early training dynamics. This is a unique contribution to the field, as prior work has not explored these specific meta-features for architecture ranking.
2. Scientific Rigor: The experiments are thorough, involving 11,170 architectures evaluated across 13 diverse datasets. The use of leave-one-out cross-validation and detailed performance analyses (e.g., parent-child architecture comparisons, parallel layer effectiveness) demonstrates scientific rigor.
Supporting Arguments
The paper is well-motivated and builds on existing literature in DNN architecture exploration and hyperparameter tuning. The authors clearly articulate the challenges of architecture transferability and computational inefficiency in new domains, grounding their work in practical relevance. The proposed meta-learning approach is validated through extensive experiments, showing that it outperforms random sampling in identifying high-performing architectures. Additionally, the analysis of parallel layers and their impact on performance provides valuable insights for DNN design.
The results are scientifically sound, with appropriate evaluation metrics (e.g., precision@X) and comparisons to baseline methods like Random Forest. The findings, such as the limited transferability of architectures across datasets and the effectiveness of shallow architectures for tabular data, are both insightful and actionable.
Suggestions for Improvement
1. Expand Architecture Diversity: The authors acknowledge the limited diversity of generated architectures (e.g., fixed hyperparameters, exclusion of CNNs/RNNs). Future work should address this limitation to generalize findings further.
2. Meta-Feature Selection: While the paper identifies the most impactful meta-features, the dataset-based features contribute minimally. Exploring ways to enhance their utility or refining their selection could improve the meta-learning model.
3. Broader Comparisons: The paper compares DNNs to Random Forests but could benefit from including other conventional classifiers (e.g., Gradient Boosting Machines) for a more comprehensive evaluation.
4. Scalability: The computational cost of training 11,170 architectures per dataset is high. Discussing how the method scales to larger datasets or more complex architectures would strengthen the practical applicability of the approach.
Questions for the Authors
1. How sensitive is the meta-learning model to the choice of datasets used for training? Would including datasets from more diverse domains improve its generalizability?
2. Have you considered applying your meta-learning approach to other architecture types, such as CNNs or RNNs, to test its versatility?
3. Could the ranking model be extended to incorporate runtime or resource constraints, making it more practical for real-world applications?
Overall, this paper makes a meaningful contribution to the field of automated DNN architecture exploration and provides a solid foundation for future research.