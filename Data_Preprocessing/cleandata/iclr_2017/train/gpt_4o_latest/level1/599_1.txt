The paper introduces GRU-D, a novel deep learning model that extends the Gated Recurrent Unit (GRU) architecture to handle missing data in multivariate time series by incorporating informative missingness patterns. The authors propose trainable decay mechanisms for both input variables and hidden states, enabling the model to exploit masking and time intervals to improve prediction performance. Empirical results on synthetic and real-world clinical datasets (MIMIC-III, PhysioNet) demonstrate that GRU-D outperforms existing baselines in time series classification tasks, providing state-of-the-art performance and valuable insights into the role of missingness in prediction.
Decision: Accept
Key reasons:  
1. The paper addresses an important and underexplored problem—leveraging informative missingness in time series data—by proposing a well-motivated and novel extension of GRU.  
2. The empirical results are robust, demonstrating significant improvements over strong baselines across multiple datasets and tasks, and the methodology is scientifically rigorous.  
Supporting Arguments:  
1. Problem and Motivation: The paper is well-placed in the literature, highlighting the limitations of existing imputation and RNN-based methods in handling missing data. The authors convincingly argue that missing patterns are often informative and propose a principled approach to incorporate them into the model architecture.  
2. Novelty and Contribution: GRU-D introduces trainable decay mechanisms, which are novel and well-justified. The model's ability to decay inputs and hidden states based on learned parameters is a meaningful extension to GRU, addressing the shortcomings of prior approaches that either ignore missingness or handle it in a simplistic manner.  
3. Empirical Validation: The experiments are comprehensive, covering synthetic and real-world datasets with varying missingness patterns. The results consistently show that GRU-D outperforms baselines, including GRU variations and non-RNN models, in terms of AUC scores. The analysis of decay rates and their interpretability further strengthens the paper's contributions.  
Additional Feedback:  
1. Clarity: While the paper is generally well-written, some sections, particularly the mathematical formulations (e.g., decay mechanisms), could benefit from clearer explanations and more intuitive examples to aid understanding for a broader audience.  
2. Broader Applicability: The paper focuses on healthcare datasets, but it would be helpful to discuss the generalizability of GRU-D to other domains, such as finance or climate science, where missing data is also prevalent.  
3. Ablation Studies: Although the paper includes variations of GRU-D, a more detailed ablation study isolating the impact of input decay, hidden state decay, and masking would further clarify their individual contributions.  
4. Computational Efficiency: While the paper mentions scalability, a more detailed discussion of the computational overhead introduced by trainable decays compared to standard GRU would be valuable.
Questions for Authors:  
1. How sensitive is the performance of GRU-D to the choice of hyperparameters, such as the decay rate initialization or the architecture's hidden size?  
2. Could the proposed decay mechanisms be extended to other RNN variants, such as LSTM, and if so, how would the performance compare?  
3. How does GRU-D handle scenarios where the missingness is not informative or is completely random? Would it degrade gracefully in such cases?  
In conclusion, the paper makes a significant contribution to the field of time series analysis with missing data, and its methodological and empirical rigor justifies its acceptance. Addressing the feedback and questions raised could further enhance its impact.