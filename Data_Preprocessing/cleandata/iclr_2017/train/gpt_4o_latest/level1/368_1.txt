The paper proposes a novel method for evaluating log-likelihoods of decoder-based generative models using Annealed Importance Sampling (AIS) and validates its accuracy with Bidirectional Monte Carlo (BDMC). This approach addresses the challenge of evaluating generative models like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Generative Moment Matching Networks (GMMNs), where traditional methods such as Kernel Density Estimation (KDE) are inaccurate in high-dimensional spaces. The authors provide a comprehensive analysis of the performance of these models, their degree of overfitting, and their ability to capture modes in the data distribution. The paper also highlights the limitations of existing evaluation metrics and demonstrates the utility of AIS in providing fine-grained comparisons between models. The authors make their evaluation code publicly available, which further strengthens the reproducibility of their work.
Decision: Accept
The paper makes a significant contribution to the field of generative modeling by addressing a critical gap in the evaluation of decoder-based models. The proposed AIS-based method is well-motivated, rigorously validated, and provides insights that cannot be captured by existing methods like KDE. The authors' empirical results are thorough and scientifically rigorous, supporting their claims effectively. The work is also well-placed in the literature, building upon prior methods while addressing their limitations. These factors justify acceptance.
Supporting Arguments:
1. Novelty and Impact: The use of AIS for log-likelihood estimation in decoder-based models is innovative and addresses a long-standing issue in generative modeling. The validation using BDMC adds credibility to the method.
   
2. Scientific Rigor: The experiments are comprehensive, covering multiple models, architectures, and datasets. The authors validate their method against simulated data and provide detailed analyses of overfitting, mode coverage, and observation model appropriateness.
3. Reproducibility: The availability of the evaluation code ensures that the results can be independently verified and extended by the community.
Suggestions for Improvement:
1. Clarity of Presentation: While the paper is thorough, it is dense and could benefit from a more concise presentation of the methodology and results. For instance, the detailed description of AIS and BDMC could be streamlined for readability.
2. Broader Applicability: The paper focuses primarily on MNIST, a relatively simple dataset. Extending the analysis to more complex datasets (e.g., CIFAR-10 or ImageNet) would strengthen the generalizability of the findings.
3. Discussion of Limitations: While the authors acknowledge that AIS is computationally expensive, a more explicit discussion of its scalability to larger datasets and models would be valuable.
Questions for the Authors:
1. How does the computational cost of AIS compare to other evaluation methods (e.g., KDE) for larger datasets or more complex models? Are there strategies to mitigate this cost?
2. Could the proposed method be extended to other types of generative models, such as autoregressive models or flow-based models? If not, what are the key challenges?
3. How sensitive is the AIS-based evaluation to the choice of intermediate distributions and transition operators? Could suboptimal choices impact the reliability of the results?
In conclusion, this paper makes a strong and well-supported contribution to the evaluation of generative models and provides a valuable tool for the research community. With minor improvements in presentation and broader applicability, it has the potential to become a foundational reference in the field.