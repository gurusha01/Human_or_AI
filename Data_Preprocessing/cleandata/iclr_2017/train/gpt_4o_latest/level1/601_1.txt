Review of "NewsQA: A Machine Comprehension Dataset"
The paper introduces NewsQA, a large-scale dataset for machine comprehension (MC) consisting of over 100,000 question-answer pairs derived from 12,744 CNN news articles. The dataset is designed to challenge existing MC models by requiring reasoning abilities such as synthesis and inference, rather than relying on simpler word-matching techniques. The authors detail a four-stage crowdsourcing process to ensure the quality and diversity of questions, emphasizing exploratory and curiosity-driven queries. NewsQA is compared to existing datasets like SQuAD, highlighting its unique characteristics, such as the inclusion of unanswerable questions and diverse answer types. The paper also benchmarks human and machine performance, showing a significant gap (0.198 in F1) that underscores the dataset's difficulty and potential for advancing MC research. The dataset is freely available, providing a valuable resource for the community.
Decision: Accept
The paper makes a strong case for acceptance due to its significant contribution to the field of machine comprehension. The creation of a challenging, high-quality dataset like NewsQA addresses a critical gap in the literature by providing a resource that demands reasoning beyond simple word-matching. The dataset's scale, diversity, and rigorously designed collection methodology make it a valuable addition to the existing body of MC datasets. Additionally, the authors provide thorough analysis and baseline results, demonstrating the dataset's utility and difficulty.
Supporting Arguments
1. Well-Motivated Contribution: The authors clearly articulate the limitations of existing datasets and position NewsQA as a necessary advancement. The emphasis on reasoning-based questions and the inclusion of unanswerable queries align with real-world applications of MC systems.
   
2. Scientific Rigor: The paper provides detailed descriptions of the dataset creation process, including validation steps to ensure quality. The comparison of human and machine performance is methodologically sound, and the significant performance gap highlights the dataset's potential to drive future research.
3. Baseline Evaluation: The inclusion of both human and neural model baselines (e.g., mLSTM and BARB) provides a clear benchmark for future work. The analysis of reasoning types and answer types further demonstrates the dataset's complexity.
Suggestions for Improvement
1. Clarify Reasoning Taxonomy: While the paper categorizes reasoning types (e.g., word matching, inference, synthesis), more examples or detailed explanations of these categories would enhance clarity and reproducibility.
2. Expand on Model Limitations: The paper could delve deeper into why existing models struggle with NewsQA, particularly in synthesis and inference tasks. This would provide more actionable insights for researchers.
3. Address Dataset Bias: The authors should discuss potential biases introduced by the use of CNN articles as the sole source material, as this may limit the dataset's generalizability.
Questions for Authors
1. How does the dataset handle potential ambiguities in questions or answers, especially in cases where multiple interpretations are possible?
2. Have you considered extending the dataset to include articles from other domains or sources to improve generalizability?
3. Could you provide more details on the human evaluation process, particularly how disagreements among annotators were resolved?
In conclusion, NewsQA is a well-executed and impactful contribution to the field of machine comprehension. Addressing the suggested improvements would further strengthen the paper, but they do not detract from its overall merit.