The paper presents a novel approach to constructing a phylogenetic tree (Tree of Life) using the deep representations learned by convolutional neural networks (CNNs) trained for image classification. The authors propose that the hierarchical features learned by CNNs can be leveraged to quantify the visual similarity between species, enabling the construction of a tree of life. They demonstrate this by applying their method to ImageNet categories and extending it to species outside the training set using cosine similarity of activation vectors. The paper also explores different methods (Approximation Central Point, Minimum Spanning Tree, and Multidimensional Scaling) for tree construction and validates the results against WordNet hierarchies. The work provides insights into deep representations and suggests potential applications in bioinformatics.
Decision: Accept
Key reasons for acceptance:
1. Novel Contribution: The paper introduces a creative and interdisciplinary application of deep learning to evolutionary biology, bridging computer vision and phylogenetics.
2. Rigorous Evaluation: The authors conduct extensive experiments at varying levels of granularity (fine-grained, coarse-grained, and non-biological categories), demonstrating the robustness and generalizability of their approach.
Supporting Arguments:
1. Well-Motivated Approach: The paper is well-placed in the literature, drawing connections between deep learning's hierarchical feature extraction and its potential to model evolutionary relationships. The authors provide a clear rationale for using CNNs and justify their choice of networks (AlexNet, VGG, ResNet) based on classification performance.
2. Empirical Validation: The results are scientifically rigorous, with comparisons to WordNet hierarchies and evaluations across multiple datasets. The use of multiple tree construction methods and their relative performance is a strength.
3. Interdisciplinary Impact: The work has implications beyond computer vision, offering a new perspective on bioinformatics and evolutionary biology.
Suggestions for Improvement:
1. Biological Validation: While the visual similarity-based trees align with WordNet hierarchies, it would strengthen the paper to compare the results with phylogenetic trees derived from genetic data. This would provide a more biologically grounded validation.
2. Clarity in Methodology: The description of the tree construction methods (ACP, MST, MDS) could be more concise and intuitive. Including visual illustrations of these methods might help readers unfamiliar with the techniques.
3. Broader Dataset: The experiments rely heavily on ImageNet categories. Exploring datasets with more explicit evolutionary labels or genetic information could enhance the biological relevance of the findings.
4. Limitations and Future Work: The paper could benefit from a more explicit discussion of its limitations, such as the reliance on visual similarity, which may not always correlate with evolutionary relationships. Suggestions for integrating genetic data or addressing these limitations in future work would be valuable.
Questions for the Authors:
1. How does the proposed method handle cases where visual similarity does not align with genetic similarity (e.g., convergent evolution)?
2. Have you considered incorporating additional modalities, such as textual descriptions or genetic data, to complement the visual features?
3. Can the approach be extended to construct trees for datasets with fewer images per category, where the robustness of statistics might be compromised?
Overall, this paper makes a compelling case for leveraging deep learning in evolutionary biology and provides a strong foundation for future interdisciplinary research.