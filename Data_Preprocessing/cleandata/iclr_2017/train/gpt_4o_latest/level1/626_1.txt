Review of the Paper
Summary of Contributions
This paper presents a novel framework for understanding the preimages of node activities in fully connected multilayer rectifier networks (ReLU networks). The authors propose a method to compute the preimages of activities at arbitrary levels, disregarding the effects of max-pooling, and demonstrate that these preimages form piecewise linear manifolds in the input space. The paper argues that these preimages are fundamental building blocks for modeling class-specific input manifolds and ensuring efficient classification. The authors also explore the implications of preimages for convolutional networks, suggesting that preimages are "well-behaved" and semantically consistent with the input image. The paper concludes with a discussion on how preimages can inform efficient training and potentially address adversarial examples. Overall, the work aims to provide a theoretical foundation for understanding how deep networks model input manifolds and achieve classification efficiency.
Decision: Reject
While the paper addresses an important and underexplored problem, the decision to reject is based on two key reasons: (1) insufficient empirical validation of the proposed theoretical framework, and (2) lack of clarity and accessibility in the presentation, which hinders comprehension and reproducibility.
Supporting Arguments
1. Lack of Empirical Validation: The paper primarily relies on theoretical derivations and heuristic arguments to support its claims. While the mathematical framework for computing preimages is well-developed, there is no empirical evidence to demonstrate its practical utility. For instance, the paper does not provide experimental results showing how preimages can be used to improve classification performance, enhance training efficiency, or address adversarial examples. Without empirical validation, the claims remain speculative.
2. Clarity and Accessibility: The paper's presentation is dense and overly technical, making it difficult to follow for a broader audience. Key concepts, such as the construction of preimages and their implications for classification, are not explained intuitively. Additionally, the notation and mathematical derivations are not sufficiently contextualized, which may alienate readers unfamiliar with the topic. This lack of clarity undermines the paper's potential impact.
Suggestions for Improvement
1. Empirical Validation: Include experiments to validate the theoretical claims. For example, demonstrate how preimages can be used to separate mixed classes, improve classification accuracy, or mitigate adversarial attacks. Comparing the proposed method with existing approaches (e.g., Mahendran & Vedaldi, 2015; 2016) would strengthen the paper.
2. Clarity in Presentation: Simplify the exposition and provide intuitive explanations for key concepts. Use diagrams and examples to illustrate how preimages are computed and how they relate to input manifolds. Clearly state the assumptions and limitations of the proposed framework.
3. Address Pooling Effects: The paper explicitly disregards the effects of max-pooling, which is a critical component of many deep networks. Future work should incorporate pooling into the analysis to make the framework more comprehensive and applicable to real-world networks.
4. Adversarial Examples: The discussion on adversarial examples is intriguing but underdeveloped. Provide concrete evidence or experiments to explore the relationship between preimages and adversarial inputs.
Questions for the Authors
1. How do you plan to empirically validate the proposed framework? Are there specific datasets or tasks where the computation of preimages has demonstrated practical benefits?
2. How does the exclusion of max-pooling affect the generalizability of your results to standard convolutional networks?
3. Can you provide more intuitive examples or visualizations of preimages to help readers understand their significance?
4. Have you considered the computational complexity of calculating preimages for large-scale networks? How feasible is this approach for modern deep learning architectures?
In conclusion, while the paper addresses a significant problem and provides a solid theoretical foundation, the lack of empirical validation and clarity in presentation limits its current impact. Addressing these issues in future iterations could make this work a valuable contribution to the field.