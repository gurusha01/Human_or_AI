The paper presents a novel method for pruning filters in Convolutional Neural Networks (CNNs) to reduce computational costs without introducing irregular sparsity. Unlike traditional weight pruning approaches that often result in sparse connectivity and require specialized libraries, this method removes entire filters and their associated feature maps, enabling the use of efficient dense matrix operations. The authors demonstrate that their approach achieves significant reductions in FLOP (up to 34% for VGG-16 and 38% for ResNet-110 on CIFAR-10) while maintaining accuracy after retraining. They also provide insights into layer sensitivity to pruning and propose a one-shot pruning and retraining strategy for simplicity and efficiency.
Decision: Accept
Key reasons for acceptance:
1. Significant Contribution: The paper addresses a critical problem in CNN efficiency and provides a structured pruning method that avoids the challenges of sparse connectivity, making it highly practical for real-world applications.
2. Rigorous Evaluation: The authors conduct extensive experiments on multiple architectures (VGG-16, ResNet-56/110/34) and datasets (CIFAR-10, ImageNet), demonstrating the effectiveness of their method with both theoretical and empirical results.
Supporting Arguments:
- The paper is well-motivated, situating its contributions within the broader literature on model compression and computational efficiency. It highlights the limitations of existing weight pruning methods and positions filter pruning as a more structured and hardware-friendly alternative.
- The methodology is clearly described, with detailed explanations of the pruning criteria (`l1-norm`), sensitivity analysis, and retraining strategies. The results are robust, showing consistent performance improvements across different architectures and datasets.
- The comparison with alternative pruning methods (e.g., random pruning, activation-based pruning, and `l2-norm` pruning) strengthens the validity of the proposed approach. The authors also address practical considerations, such as FLOP reduction translating to real-world inference time savings.
Suggestions for Improvement:
1. Clarity on Retraining Overheads: While the paper emphasizes the simplicity of one-shot pruning and retraining, a more detailed comparison of retraining time versus iterative approaches would provide additional clarity on the trade-offs.
2. Broader Applicability: The paper focuses on CIFAR-10 and ImageNet datasets. Including results on other domains (e.g., object detection or segmentation tasks) would demonstrate the generalizability of the method.
3. Ablation Studies: While the paper compares `l1-norm` and `l2-norm` pruning, further ablation studies on the impact of pruning ratios across different layers could provide deeper insights into the method's robustness.
Questions for the Authors:
1. How does the method perform on tasks beyond classification, such as object detection or semantic segmentation, where feature map quality may have a more significant impact?
2. Could the proposed pruning strategy be combined with other compression techniques, such as quantization or knowledge distillation, to achieve further efficiency gains?
3. How sensitive is the method to the choice of pruning ratio, and what guidelines can be provided for selecting this parameter in practice?
In conclusion, the paper makes a strong contribution to the field of CNN acceleration and provides a practical, well-validated approach to filter pruning. With minor clarifications and additional experiments, it could have even broader impact.