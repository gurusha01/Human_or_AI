Review
Summary of Contributions
This paper addresses the problem of domain-invariant representation learning for unsupervised domain adaptation using neural networks. The authors propose a novel regularization method, Central Moment Discrepancy (CMD), which explicitly matches higher-order central moments of domain-specific latent feature distributions. CMD is computationally efficient, avoiding expensive kernel matrix computations required by methods like Maximum Mean Discrepancy (MMD). The authors provide theoretical guarantees, proving that CMD is a metric and that convergence in CMD implies convergence in distribution. Empirical results demonstrate that CMD achieves state-of-the-art performance on the Office and Amazon reviews datasets, outperforming several baselines, including MMD, Domain-Adversarial Neural Networks (DANN), and Variational Fair Autoencoders (VFAE). The paper also includes a parameter sensitivity analysis, showing that CMD is robust to hyperparameter changes.
Decision: Accept
The paper is well-motivated, theoretically sound, and empirically validated. The key reasons for acceptance are:
1. Novelty and Impact: CMD introduces a new perspective on domain adaptation by explicitly matching higher-order moments, which is a meaningful contribution to the field.
2. Theoretical Rigor: The authors provide strong theoretical guarantees, including proofs that CMD is a metric and ensures convergence in distribution.
3. Empirical Strength: The method achieves state-of-the-art results on multiple benchmarks, demonstrating its practical utility.
Supporting Arguments
1. Problem Relevance: The paper tackles a critical challenge in domain adaptationâ€”learning domain-invariant representations without requiring labeled target data. This is a well-recognized and impactful problem in machine learning.
2. Positioning in Literature: The authors provide a thorough review of related work, clearly identifying limitations of existing methods like MMD and KL divergence. CMD is well-motivated as an improvement over these approaches.
3. Scientific Rigor: The theoretical analysis is robust, with clear proofs for CMD's properties. The empirical evaluation is comprehensive, covering multiple datasets, baselines, and sensitivity analyses.
4. Practical Implications: CMD's computational efficiency and robustness to hyperparameter changes make it a practical choice for real-world applications.
Suggestions for Improvement
1. Clarity of Theoretical Results: While the proofs are rigorous, they could benefit from more intuitive explanations to help readers understand the implications of CMD being a metric and its connection to convergence in distribution.
2. Ablation Studies: An ablation study on the contribution of each moment order (e.g., first, second, third) would provide deeper insights into CMD's effectiveness.
3. Comparison with Linear-Time MMD: The paper compares CMD with quadratic-time MMD but does not evaluate against linear-time MMD estimators. Including this comparison would strengthen the empirical claims.
4. Scalability Analysis: While CMD is computationally efficient, a detailed analysis of its scalability to larger datasets or higher-dimensional feature spaces would be valuable.
Questions for the Authors
1. How does CMD perform when the number of hidden nodes or layers in the neural network is significantly increased? Does it remain robust in deeper architectures?
2. Can CMD be extended to handle dependent marginal distributions explicitly, given the current limitation of matching only independent marginals?
3. Have you considered applying CMD to other tasks beyond domain adaptation, such as generative modeling or fairness in machine learning?
Overall, this paper makes a significant contribution to the domain adaptation literature and is a strong candidate for acceptance. The suggestions above are intended to further enhance the clarity and impact of the work.