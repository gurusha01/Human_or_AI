The paper presents a novel framework for bioacoustic signal representation and deep learning enhancement using a Fast Chirplet Transform (FCT). The authors propose FCT as a computationally efficient, bioinspired auditory representation that bridges the gap between scattering transforms and convolutional neural networks (CNNs). The paper claims four main contributions: (1) the introduction and implementation of FCT, (2) its computational validation on large bioacoustic datasets, (3) its effectiveness in pretraining CNNs for bioacoustic classification tasks, and (4) its potential for interspecies bioacoustic transfer learning. The results demonstrate that FCT pretraining accelerates CNN training by 26-28% and improves classification performance by 2.3-7.8% across datasets.
Decision: Accept
The paper is well-motivated, methodologically sound, and provides significant contributions to bioacoustic signal processing and deep learning. The key reasons for acceptance are: (1) the introduction of FCT as a novel, computationally efficient representation with clear biological and mathematical grounding, and (2) the demonstrated empirical improvements in CNN training efficiency and accuracy, which are relevant to the AI and bioacoustics communities.
Supporting Arguments:
1. Problem Relevance: The paper addresses a critical challenge in bioacoustic classification—handling weak signal-to-noise ratios and limited data—by proposing a biologically inspired representation that aligns with neurophysiological evidence.
2. Methodological Rigor: The authors provide a clear mathematical formulation of FCT, describe its implementation in detail, and validate its computational efficiency on large datasets (e.g., Orca recordings, LifeClef birds, and TIMIT vowels). The results are robust and reproducible, with open-source code provided.
3. Empirical Validation: The experimental results convincingly demonstrate the benefits of FCT in accelerating CNN training and improving classification performance. The comparisons with raw audio and Mel spectrograms are fair and well-documented.
Suggestions for Improvement:
1. Clarity and Accessibility: The paper is dense with technical details, which may hinder accessibility for a broader audience. Simplifying the explanation of FCT and its parameters, particularly in Section 2, would improve readability.
2. Comparative Analysis: While the results are promising, a comparison with other state-of-the-art bioacoustic representations (e.g., scattering transforms or other wavelet-based methods) would strengthen the claims.
3. Generalization: The experiments focus on specific datasets (birds and vowels). Expanding the validation to additional bioacoustic domains (e.g., insect sounds or marine mammals) would enhance the generalizability of the approach.
4. Ablation Studies: Including ablation studies to isolate the contributions of different components of FCT (e.g., chirp rate, low-pass filtering) would provide deeper insights into its effectiveness.
Questions for the Authors:
1. How does FCT perform in scenarios with extremely low signal-to-noise ratios or highly imbalanced datasets? Are there any limitations in such cases?
2. Could the authors elaborate on the hyperparameter selection process for FCT (e.g., chirp rate, number of octaves) and its sensitivity to these parameters?
3. How does the proposed FCT-based pretraining compare with transfer learning from pretrained CNNs on large-scale audio datasets (e.g., AudioSet)?
In conclusion, the paper makes a strong contribution to the field and is recommended for acceptance, with minor revisions to enhance clarity and broaden validation.