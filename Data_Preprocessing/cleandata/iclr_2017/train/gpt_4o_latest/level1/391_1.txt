Review of the Paper
Summary of Contributions
This paper addresses the challenge of deploying large Recurrent Neural Networks (RNNs) on resource-constrained devices such as mobile phones and embedded systems. The authors propose a novel pruning technique that progressively sets weights to zero during training, resulting in sparse networks with significantly reduced model size and inference time. The method achieves up to 90% sparsity, reducing model size by 8× and inference time by 2× to 7×, while maintaining accuracy close to or better than dense baselines. The technique is computationally efficient, does not increase training time, and is applicable to both vanilla RNNs and Gated Recurrent Units (GRUs). Experimental results demonstrate the effectiveness of the approach on speech recognition tasks, showing that pruned models outperform dense models of equivalent size. The authors also discuss practical implications for deployment and provide insights into pruning schedules and sparsity patterns.
Decision: Accept
The paper makes a strong case for acceptance due to its practical significance, rigorous experimentation, and clear contributions to the field. The proposed pruning method is well-motivated, achieves impressive results, and addresses a critical problem in deploying RNNs on constrained hardware. The work is also well-placed in the literature, building on and improving existing pruning techniques while offering new insights into sparsity and performance trade-offs.
Supporting Arguments
1. Well-Motivated Problem and Approach: The paper identifies a pressing issue in deploying large RNNs on mobile and embedded devices, where memory and computational constraints are significant. The proposed pruning technique is simple, efficient, and directly addresses these challenges. The authors provide a thorough comparison with related work, highlighting the novelty and advantages of their approach.
   
2. Rigorous Experimental Validation: The experiments are comprehensive, covering both vanilla RNNs and GRUs across multiple configurations. The results are convincing, demonstrating that the proposed method achieves significant compression and speed-up without substantial loss in accuracy. The comparison with hard pruning and dense baselines further strengthens the claims.
3. Practical Impact: The method is easy to implement in existing training frameworks and has clear implications for real-world deployment. The discussion on hardware-specific optimizations and potential future work adds depth to the paper.
Suggestions for Improvement
1. Clarity on Hyperparameter Selection: While the authors provide heuristics for selecting pruning hyperparameters, the process could be elaborated further. For example, how sensitive are the results to these hyperparameters? Could an automated or adaptive approach be explored?
2. Sparse Matrix Multiplication Performance: The paper notes that current sparse matrix libraries underperform compared to theoretical expectations. It would be helpful to discuss potential improvements or alternative libraries that could better exploit the sparsity.
3. Generalization to Other Tasks: The experiments focus primarily on speech recognition. While the authors mention plans to extend the technique to language modeling and embeddings, preliminary results or a discussion of potential challenges would strengthen the paper.
4. Comparison with Quantization: Since quantization is a complementary technique, a brief experimental comparison or discussion of how the two methods interact would be valuable.
Questions for the Authors
1. How does the proposed pruning technique generalize to other architectures, such as Transformer models or convolutional networks? Are there any limitations specific to RNNs?
2. Could the pruning schedule be further optimized using reinforcement learning or other adaptive methods?
3. Have you explored the impact of pruning on robustness to adversarial attacks or out-of-distribution data?
In conclusion, this paper makes a significant contribution to the field of neural network compression and deployment. Its practical relevance, strong experimental results, and clear presentation make it a valuable addition to the conference. With minor clarifications and extensions, the work could have even broader impact.