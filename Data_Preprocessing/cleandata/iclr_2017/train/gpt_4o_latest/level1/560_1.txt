Review
Summary of Contributions
The paper addresses the challenge of training deep and recurrent neural networks (RNNs) for tasks with long-term dependencies, particularly focusing on the vanishing and exploding gradient problem. The authors propose a novel weight matrix factorization and parameterization strategy that allows for bounding matrix norms, thereby controlling the degree of expansivity during backpropagation. By introducing a configurable margin around the Stiefel manifold, the paper explores the trade-off between orthogonality constraints and optimization convergence. The authors provide both theoretical insights and extensive empirical evaluations on synthetic tasks (e.g., copy and adding tasks) and real-world datasets (e.g., sequential MNIST and Penn Treebank). The results demonstrate that loosening strict orthogonality constraints can improve convergence rates and model performance, while overly relaxed constraints may lead to instability. The work contributes to the understanding of how orthogonality impacts gradient stability, optimization, and representational power in RNNs.
Decision: Accept
The paper is well-motivated, methodologically sound, and provides significant insights into the trade-offs between orthogonality constraints and optimization performance in RNNs. The proposed approach is novel, scientifically rigorous, and well-situated within the existing literature. The empirical results are thorough and convincingly support the claims made in the paper. 
Supporting Arguments
1. Well-Motivated Problem and Novel Approach: The paper tackles a critical issue in training deep and recurrent networks, which is well-known but not fully resolved. The proposed parameterization strategy, which balances orthogonality and optimization flexibility, is novel and addresses a gap in the literature.
2. Comprehensive Empirical Validation: The authors conduct experiments on both synthetic and real-world tasks, demonstrating the effectiveness of their approach across diverse settings. The results are consistent with the theoretical claims and provide actionable insights for practitioners.
3. Scientific Rigor: The paper provides a clear theoretical foundation for the proposed method, including detailed derivations and discussions on gradient stability, spectral norms, and optimization dynamics. The experiments are reproducible, with well-documented hyperparameters and baselines.
Suggestions for Improvement
1. Clarify the Impact of Margins: While the authors demonstrate that loosening orthogonality constraints improves convergence, the exact relationship between the spectral margin and task-specific performance could be better quantified. For example, how does the optimal margin vary across tasks or datasets?
2. Discussion on Computational Overhead: The proposed method involves matrix factorization and geodesic gradient descent, which may introduce computational overhead. While the authors briefly discuss running times, a more detailed comparison with baseline methods (e.g., standard RNNs or LSTMs) would strengthen the practical relevance of the approach.
3. Broader Applicability: The experiments focus primarily on RNNs. It would be helpful to discuss whether the proposed method could generalize to other architectures, such as transformers or feedforward networks, where gradient stability is also a concern.
4. Ablation Studies: While the paper explores various margins and constraints, additional ablation studies on other hyperparameters (e.g., learning rates for different components) could provide deeper insights into the robustness of the method.
Questions for the Authors
1. How sensitive is the proposed method to the choice of the margin hyperparameter? Could an adaptive margin strategy be explored to dynamically adjust the constraints during training?
2. The results suggest that orthogonal initialization is beneficial, even when orthogonality constraints are later relaxed. Could the authors elaborate on why this initialization is critical, and whether it is task-dependent?
3. For tasks like Penn Treebank, where long-term dependencies are crucial, how does the proposed method compare to more complex architectures like LSTMs or GRUs in terms of both performance and computational cost?
Overall, this paper makes a meaningful contribution to the field and provides a solid foundation for future work on improving gradient stability in neural networks.