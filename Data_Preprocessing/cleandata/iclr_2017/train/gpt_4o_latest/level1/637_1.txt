The paper introduces a novel method for mining logical theories directly from relational embeddings, addressing limitations of prior approaches by enabling the discovery of more expressive rules, including logical disjunctions and negations. The authors frame theory learning as a sparse recovery problem in the embedding space, leveraging compressed sensing techniques like Orthogonal Matching Pursuit (OMP) to extract diverse and interpretable rules. Empirical results demonstrate the method's superiority over existing approaches, particularly in terms of F-score and per-rule recall, across multiple knowledge bases.
Decision: Accept
The paper makes a significant contribution to the field of rule mining by extending the capabilities of relational embeddings and addressing key limitations of prior work. The proposed method is innovative, well-motivated, and empirically validated, making it a valuable addition to the literature.
Supporting Arguments:
1. Clear Problem Definition and Motivation: The paper identifies the limitations of existing methods, such as their inability to handle logical disjunctions and negations and their lack of rule diversity. The proposed approach is well-placed in the literature, building on prior work by Yang et al. (2015) and addressing its shortcomings.
   
2. Scientific Rigor: The authors provide a thorough theoretical foundation for their method, including a detailed derivation of the optimization problem and its connection to sparse recovery. The empirical evaluation is robust, with experiments conducted on multiple datasets and comparisons against strong baselines.
3. Empirical Validation: The results convincingly demonstrate the advantages of the proposed method, particularly in terms of F-score and per-rule recall. The inclusion of diverse and interpretable rules further highlights the practical utility of the approach.
Suggestions for Improvement:
1. Scalability Concerns: While the authors acknowledge the computational challenges of enumerating candidate rule bodies, they could provide more details on how online recovery algorithms like Online Search OMP might address this issue in future work. A preliminary analysis of scalability on larger knowledge bases would strengthen the paper.
2. Error Analysis: The empirical results focus on aggregate metrics like F-score and recall, but a deeper error analysis (e.g., types of errors made, robustness to noise) would provide additional insights into the method's strengths and weaknesses.
3. Clarity in Presentation: Some sections, particularly the mathematical formulations, could benefit from additional explanations or visual aids to improve accessibility for readers less familiar with compressed sensing or relational embeddings.
4. Broader Applicability: While the method is designed for binary relations, a discussion on how it might be extended to handle n-ary relations or more complex knowledge bases would be valuable.
Questions for the Authors:
1. How does the method perform on larger, more complex knowledge bases like YAGO or DBpedia? Are there any scalability bottlenecks that need to be addressed?
2. How sensitive is the method to the choice of hyperparameters, such as the embedding size (d) or the coefficient threshold (Ï„)?
3. Could the proposed approach be integrated with other relational embedding techniques, such as those based on deep learning, to further improve performance?
In conclusion, the paper presents a well-motivated and rigorously evaluated method that advances the state of the art in rule mining from relational embeddings. Addressing the suggestions above would further enhance the paper's impact and clarity.