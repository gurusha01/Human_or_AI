Review of the Paper: "Interior Gradients for Feature Importance in Deep Networks"
The paper introduces a novel method, Interior Gradients, to address the limitations of standard gradient-based feature attribution in deep neural networks. The authors identify a critical issue with gradients in nonlinear networks, where saturation can lead to misleadingly small gradients for important features. To overcome this, the paper proposes examining gradients of counterfactual inputs, specifically scaled versions of the original input. The method is computationally efficient, requires no modifications to the network, and satisfies desirable theoretical properties such as sensitivity and implementation invariance. The authors demonstrate the utility of Interior Gradients on multiple architectures, including GoogleNet, molecular graph convolutional networks, and LSTMs, and provide both qualitative visualizations and quantitative evaluations.
Decision: Accept
The paper makes a significant contribution to the field of explainable AI by proposing a simple yet effective method for feature attribution that is theoretically grounded and empirically validated. The key reasons for acceptance are:  
1. Novelty and Practicality: The method addresses a well-known limitation of gradient-based attribution (saturation) in a novel and computationally efficient way, making it accessible to practitioners.  
2. Theoretical Rigor: The method satisfies key axioms (sensitivity, implementation invariance) and is shown to be the unique solution under an extended set of axioms.  
3. Empirical Validation: The paper provides strong empirical evidence across diverse tasks and architectures, demonstrating the method's generalizability and effectiveness.
Supporting Arguments
1. Well-Motivated Approach: The paper clearly identifies the problem of saturation and its impact on feature attribution. The proposed solution is intuitive and builds on the familiar concept of gradients, making it easy to adopt in practice.  
2. Thorough Evaluation: The authors validate the method using multiple evaluation techniques, including pixel ablation, localization, and qualitative visualizations. The results consistently show that Interior Gradients outperform standard gradients in capturing feature importance.  
3. Broad Applicability: The method is demonstrated on vision, molecular, and language modeling tasks, highlighting its versatility.  
Suggestions for Improvement
1. Comparison with Other Methods: While the authors compare Interior Gradients to standard gradients, a comparison with other state-of-the-art attribution methods (e.g., DeepLIFT, LRP) would strengthen the empirical evaluation. The authors mention these methods but do not provide quantitative results.  
2. Scalability Analysis: It would be helpful to include a discussion on the computational cost of the method, especially for large-scale datasets or real-time applications.  
3. Limitations: The paper briefly mentions limitations (e.g., inability to capture feature interactions), but a more detailed discussion would provide a balanced perspective.  
Questions for the Authors
1. How does the choice of the scaling path (e.g., linear scaling) affect the attributions? Could alternative paths yield better results in certain cases?  
2. Have you considered extending the method to capture feature interactions, such as through pairwise attributions?  
3. Can the method be adapted for use in unsupervised learning tasks or generative models?  
In conclusion, the paper presents a compelling and impactful contribution to the field of explainable AI. With minor improvements, it has the potential to become a widely adopted technique for feature attribution in deep learning.