Review of "Multiplicative LSTM (mLSTM): A Novel Recurrent Neural Network Architecture for Sequence Modelling"
The paper introduces the multiplicative LSTM (mLSTM), a hybrid recurrent neural network architecture that combines the strengths of the long short-term memory (LSTM) and multiplicative recurrent neural network (mRNN) architectures. The authors argue that mLSTM's input-dependent recurrent transition functions make it more expressive and robust for autoregressive density estimation. Through a series of character-level language modeling experiments, the paper demonstrates that mLSTM outperforms standard LSTM and its deep variants, particularly as task complexity increases. Notably, mLSTM achieves a test error of 1.19 bits/character on the Hutter Prize dataset when combined with dynamic evaluation, setting a new state-of-the-art benchmark.
Decision: Accept
The paper makes a compelling case for acceptance due to its novel architectural contribution and strong empirical results. The introduction of mLSTM addresses a well-motivated problem in sequence modeling—improving input-dependent transitions while maintaining long-term information retention. The empirical evaluation is thorough, demonstrating consistent improvements over LSTM across multiple datasets and task complexities. The results on the Hutter Prize dataset, in particular, highlight the practical significance of the proposed approach.
Supporting Arguments:
1. Well-Motivated Approach: The paper is grounded in a clear problem—improving the flexibility of input-dependent transitions in RNNs without sacrificing the ability to retain long-term dependencies. The authors effectively situate their work within the literature, contrasting mLSTM with existing architectures like LSTM, mRNN, and multiplicative integration RNNs.
2. Rigorous Evaluation: The experimental design is robust, covering a range of datasets (e.g., Penn Treebank, Text8, Hutter Prize, and multilingual tasks) and comparing mLSTM against strong baselines. The use of dynamic evaluation further strengthens the results, showcasing mLSTM's ability to adapt to recent sequences.
3. Significant Results: The paper demonstrates that mLSTM achieves state-of-the-art performance on the Hutter Prize dataset and competitive results on other benchmarks. The analysis of mLSTM's robustness to surprising inputs adds depth to the evaluation.
Suggestions for Improvement:
1. Clarity in Motivation: While the paper provides a strong technical rationale for mLSTM, the motivation could be made more accessible to a broader audience. A high-level explanation of why input-dependent transitions are critical for sequence modeling would help readers unfamiliar with the intricacies of RNN architectures.
2. Ablation Studies: An ablation study isolating the contributions of the mRNN-inspired factorized weights and the LSTM gating mechanisms would provide deeper insights into the architecture's performance gains.
3. Generalization to Other Tasks: The paper acknowledges that mLSTM's applicability to tasks beyond character-level modeling remains unexplored. Preliminary experiments on word-level language modeling or tasks with continuous inputs could strengthen the paper's broader impact.
Questions for the Authors:
1. How does mLSTM perform on tasks with continuous or non-sparse inputs, such as speech modeling or time-series forecasting? Are there plans to extend the architecture to these domains?
2. Could the authors elaborate on the computational trade-offs of mLSTM compared to standard LSTM? For instance, how does the increased parameter count affect training time and memory usage?
3. The paper mentions that mLSTM is less prone to overfitting compared to other architectures. Could the authors provide more details on why this might be the case?
Overall, this paper makes a significant contribution to the field of sequence modeling, and its novel architectural insights are likely to inspire further research. With minor clarifications and extensions, the work could have even broader applicability.