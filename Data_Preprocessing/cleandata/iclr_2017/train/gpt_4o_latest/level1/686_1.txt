Review of "Homologically Functional Hashing for Neural Network Compression"
Summary of Contributions
This paper introduces Homologically Functional Hashing (HFH), a novel approach to compress deep neural networks (DNNs) by leveraging multiple low-cost hash functions and a small reconstruction network. HFH improves upon the existing HashedNets framework by addressing its limitations, such as high collision risks and lack of joint training. The proposed method introduces a shared "homological" compression space across all layers and incorporates a lightweight reconstruction network to map hashed values back to the original parameter space. HFH achieves higher compression ratios with minimal accuracy loss compared to HashedNets, as demonstrated across multiple datasets, including MNIST, CIFAR-10, and ImageNet. Additionally, HFH simplifies the process of determining compression ratios by reducing it to a single scalar parameter, making it computationally efficient. The paper positions HFH as a general-purpose tool for DNN compression, compatible with other compression techniques.
Decision: Accept
The paper makes a significant contribution to the field of DNN compression by addressing key limitations of existing methods and demonstrating superior performance. The novelty of the homological compression space and the reconstruction network is well-motivated, and the experimental results convincingly support the claims.
Supporting Arguments
1. Well-Motivated Approach: The paper identifies clear limitations in HashedNets (e.g., single hash function collisions, disjoint training, and layer-specific compression ratios) and proposes HFH as a principled solution. The use of multiple hash functions and a shared compression space is grounded in prior work on hashing and redundancy in DNNs.
   
2. Strong Empirical Results: The experiments are thorough, covering a variety of datasets and network architectures. HFH consistently outperforms HashedNets in terms of accuracy at comparable compression ratios. Notably, HFH achieves competitive results even on large-scale datasets like ImageNet and real-world applications like semantic ranking.
3. Scientific Rigor: The paper provides detailed theoretical analysis, including properties of HFH (e.g., reduced collision risk, improved reconstruction) and comparisons with HashedNets. The training procedure and computational complexity are clearly explained, and the results are reproducible.
Suggestions for Improvement
1. Ablation Studies: While the paper explores variations in the number of hash functions and reconstruction network layers, further ablation studies could clarify the relative importance of each component (e.g., the homological space vs. the reconstruction network).
2. Comparison with Other Compression Methods: The paper primarily compares HFH to HashedNets. Including results against other state-of-the-art methods, such as pruning-based approaches (e.g., dynamic network surgery), would strengthen the evaluation.
3. Memory Overhead Analysis: While the reconstruction network is described as lightweight, a more detailed analysis of its memory and computational overhead would be helpful, particularly for resource-constrained devices.
4. Clarity in Notation: The mathematical notation is dense and could be streamlined for better readability. For example, the description of the reconstruction network (Eq. 4) could be simplified with illustrative diagrams or pseudocode.
Questions for the Authors
1. How does HFH perform when combined with other compression techniques, such as pruning or quantization? Can it further enhance compression ratios without significant accuracy loss?
2. Have you tested HFH on transformer-based architectures or other non-CNN models? If not, do you anticipate any challenges in applying HFH to such models?
3. How sensitive is HFH to the choice of hash functions? Would alternative hashing schemes (e.g., locality-sensitive hashing) improve performance?
In conclusion, the paper presents a well-motivated and impactful contribution to DNN compression. With minor clarifications and additional comparisons, it has the potential to become a foundational work in the field.