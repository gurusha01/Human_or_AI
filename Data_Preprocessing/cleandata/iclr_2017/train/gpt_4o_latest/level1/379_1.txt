Review of the Paper
Summary of Contributions
The paper introduces a novel technique called dynamic batching to enable efficient batching for neural networks with dynamic computation graphs (DCGs), which are commonly used in domains like natural language processing (e.g., parse trees) and cheminformatics (e.g., molecular graphs). The authors address a significant limitation of DCGs: their incompatibility with static data-flow graph-based deep learning libraries like TensorFlow. Dynamic batching allows operations across different input graphs and nodes to be batched together, enabling the use of static computation graphs to emulate dynamic ones. The paper also presents TensorFlow Fold, a high-level library that simplifies the implementation of DCG models by providing compositional blocks and abstractions. The authors demonstrate the effectiveness of their approach through empirical benchmarks, achieving substantial speedups over unbatched implementations, and showcase the library's utility by implementing state-of-the-art models like Tree-LSTMs and graph convolution networks.
Decision: Accept
The paper is recommended for acceptance due to its strong technical contribution in solving a practical and widely acknowledged problem in deep learning, as well as its rigorous empirical validation and the introduction of a useful open-source library. The work is well-motivated, scientifically rigorous, and has the potential to significantly impact the development of DCG-based models.
Supporting Arguments
1. Problem Significance: The inability to efficiently batch DCGs has been a major bottleneck for practitioners, limiting the adoption of such models despite their success in various domains. The paper addresses this gap with a general and elegant solution.
2. Technical Novelty: The dynamic batching algorithm is a well-thought-out contribution, enabling efficient batching by rewriting computation graphs and leveraging static graph constructs like `tf.while_loop`. This innovation is both novel and practical.
3. Empirical Rigor: The experimental results convincingly demonstrate the speedups achieved by dynamic batching, especially on GPUs, where the method achieves up to 120x improvements over unbatched implementations. The benchmarks are thorough and include comparisons against manual batching.
4. Tooling Contribution: TensorFlow Fold is a valuable addition to the deep learning ecosystem, offering a high-level API that simplifies the implementation of complex DCG models. The library's utility is demonstrated through concise reimplementations of existing models and novel extensions.
Suggestions for Improvement
1. Clarity on Limitations: While the paper briefly mentions the overhead introduced by additional `concat` and `gather` operations, it would be helpful to provide a more detailed discussion of scenarios where dynamic batching might not be advantageous (e.g., very large batch sizes or specific hardware configurations).
2. Broader Comparisons: The paper could benefit from comparisons with other approaches to handling DCGs, such as the SPINN architecture, beyond the brief mention in Section 4. A more detailed discussion of trade-offs would strengthen the paper.
3. Library Usability: While TensorFlow Fold appears promising, the paper could include a user study or anecdotal feedback from practitioners to better demonstrate its ease of use and adoption potential.
4. Scalability: The scalability of dynamic batching for extremely large graphs or datasets could be explored further. Are there memory or computational bottlenecks that arise in such cases?
Questions for the Authors
1. How does the performance of dynamic batching compare to manual batching in real-world tasks where input graphs have highly irregular structures? Are there specific domains where the overhead of dynamic batching might outweigh its benefits?
2. Could the authors elaborate on the types of models or applications where TensorFlow Fold might struggle to provide significant advantages over existing approaches?
3. Are there plans to extend TensorFlow Fold to support other deep learning frameworks (e.g., PyTorch), given its growing popularity?
In conclusion, the paper makes a significant contribution to the field by addressing a long-standing challenge in training DCG-based models. The proposed dynamic batching algorithm and TensorFlow Fold library are both innovative and impactful, making this work a strong candidate for acceptance.