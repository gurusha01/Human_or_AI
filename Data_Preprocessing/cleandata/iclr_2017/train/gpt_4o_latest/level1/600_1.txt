Review
Summary of Contributions
The paper introduces a novel approach to question classification by leveraging answer information to enhance question representation. The authors propose two key contributions: (1) Group Sparse Autoencoders (GSA), a neural network-based framework that imposes group sparsity constraints to refine representations, and (2) Group Sparse Convolutional Neural Networks (GSCNNs), which integrate GSA into CNNs to jointly learn question representations with group sparse constraints. The paper demonstrates significant improvements over strong CNN baselines on four datasets, including private and public datasets, and provides compelling evidence for the utility of group sparsity in question classification tasks. The authors also explore the impact of different initialization strategies for the projection matrix and evaluate the model's performance in both conventional and unseen-label classification tasks.
Decision: Accept
The paper is recommended for acceptance due to its innovative approach to incorporating answer information into question classification, the strong empirical results, and the clear motivation for the proposed methods. The integration of GSA into CNNs is novel and well-justified, and the experiments demonstrate consistent improvements across diverse datasets. The work is also well-placed in the literature, addressing gaps in existing question classification techniques.
Supporting Arguments
1. Well-Motivated Problem and Approach: The paper identifies a clear gap in existing question classification methods, which often neglect the rich information available in answer sets. The use of group sparse constraints is a creative and well-motivated solution to this problem, and the authors effectively position their work in the context of prior research on sparse coding and CNNs.
2. Empirical Rigor: The experiments are thorough, covering multiple datasets with varying characteristics (e.g., single-label vs. multi-label, hierarchical vs. flat categories). The results consistently show that the proposed GSCNNs outperform strong baselines, including vanilla CNNs, particularly in datasets where answer information is available.
3. Technical Novelty: The introduction of GSA as a neural network-based alternative to traditional sparse coding is a significant contribution. The visualization of learned representations further validates the interpretability and effectiveness of the proposed method.
Suggestions for Improvement
1. Clarity of Presentation: While the technical details are comprehensive, the paper could benefit from a more concise explanation of GSA and GSCNNs. For instance, the mathematical formulations in Sections 2 and 3 could be streamlined to improve readability.
2. Comparison with Other Methods: Although the authors justify focusing on CNNs, it would strengthen the paper to include comparisons with other state-of-the-art methods in question classification, such as transformer-based models (e.g., BERT or RoBERTa).
3. Ablation Studies: The paper could include more detailed ablation studies to isolate the contributions of different components, such as the impact of group sparsity constraints versus other architectural choices.
4. Generalization to Other Tasks: While the focus on question classification is well-justified, a brief discussion of how the proposed methods could generalize to other NLP tasks (e.g., sentiment analysis or topic classification) would broaden the paper's impact.
Questions for the Authors
1. How does the model handle cases where the answer sets are noisy or incomplete? Have you tested the robustness of the proposed methods in such scenarios?
2. Could you provide more details on the computational efficiency of GSCNNs compared to vanilla CNNs? Does the additional layer significantly increase training time?
3. Have you considered evaluating the model on transformer-based embeddings (e.g., BERT) as input features instead of traditional word embeddings?
Overall, this paper makes a strong contribution to the field of question classification and introduces techniques that could have broader implications for NLP tasks involving hierarchical or multi-label categories. With minor improvements, it could be even more impactful.