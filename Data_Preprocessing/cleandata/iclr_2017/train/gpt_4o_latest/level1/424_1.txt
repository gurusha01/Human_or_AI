Review of the Submitted Paper
Summary of Contributions
This paper addresses the challenge of optimizing deep neural networks with highly non-convex loss functions by introducing a novel approach based on mollified objective functions. Inspired by continuation and curriculum learning methods, the authors propose a training procedure that starts with a smoothed, simpler objective and gradually transitions to the original, more complex objective. The paper introduces the concept of mollifiers, including noisy mollifiers, and demonstrates how they can be applied to neural networks to improve optimization. The proposed method is shown to enhance convergence and generalization performance on challenging tasks, such as training deep MLPs, LSTMs, and convolutional networks. The authors also provide theoretical insights, experimental results, and comparisons with existing techniques like residual connections and batch normalization.
Decision: Accept
The paper presents a well-motivated and novel approach to a fundamental problem in deep learning. The proposed mollification framework is theoretically grounded, experimentally validated, and shows promise for improving optimization and generalization in deep networks. The key reasons for acceptance are:  
1. Novelty and Relevance: The idea of mollifying objective functions and gradually increasing complexity during training is innovative and addresses a critical challenge in deep learning optimization.  
2. Experimental Validation: The authors provide extensive empirical evidence across diverse tasks (e.g., MNIST, CIFAR-10, parity problems, and language modeling), demonstrating the effectiveness of their method.  
Supporting Arguments
1. Problem and Motivation: The paper clearly identifies the difficulty of optimizing non-convex loss landscapes in deep learning and situates its approach within the context of continuation methods and curriculum learning. The connection to existing literature is well-established, and the proposed method builds on and extends prior work in a meaningful way.  
2. Theoretical Rigor: The mathematical formulation of mollifiers and their properties is thorough and well-supported. The derivations, including those for noisy mollifiers and their gradients, are detailed and align with the proposed methodology.  
3. Empirical Results: The experiments are comprehensive, covering a variety of architectures and tasks. The results consistently show that the mollified networks converge faster and achieve better generalization compared to baseline methods. The comparisons with residual connections and batch normalization are particularly compelling.  
Suggestions for Improvement
While the paper is strong overall, there are areas where it could be improved:  
1. Clarity of Presentation: The paper is dense with mathematical details, which may be challenging for readers unfamiliar with mollifiers or continuation methods. Simplifying some explanations or providing more intuitive visualizations (e.g., the effect of mollification on loss landscapes) could enhance accessibility.  
2. Hyperparameter Sensitivity: The annealing schedules and noise parameters play a crucial role in the method's success. A more detailed discussion of how these hyperparameters were selected and their sensitivity across tasks would strengthen the paper.  
3. Comparison with Additional Baselines: While the paper compares its method to residual connections and batch normalization, it would be valuable to include comparisons with other optimization techniques, such as learning rate schedules or adaptive optimizers like AdamW.  
4. Scalability: The paper focuses on relatively small datasets (e.g., MNIST, CIFAR-10). Testing the method on larger-scale tasks, such as ImageNet or large-scale language modeling, would better demonstrate its scalability and practical applicability.  
Questions for the Authors
1. How sensitive is the method to the choice of mollifier (e.g., Gaussian vs. other kernels)? Have you explored alternative mollifiers, and if so, how do they compare?  
2. Can the proposed method be combined with other optimization techniques, such as learning rate schedules or gradient clipping, to further improve performance?  
3. The experiments suggest that mollified networks generalize better. Can you provide more insights into why this is the case, particularly in terms of the loss landscape or the learned representations?  
4. How computationally expensive is the mollification procedure compared to standard training methods?  
Conclusion
This paper makes a significant contribution to the field of deep learning optimization by introducing a novel and effective training procedure. While there are areas for improvement, the strengths of the paper outweigh its weaknesses, and it has the potential to inspire further research in this domain. I recommend acceptance.