The paper presents an unsupervised method for learning feature representations of graph-structured data using an encoder-decoder model inspired by the skip-thought model in natural language processing. The key contribution is the adaptation of random walk sequences on graphs as input to the encoder-decoder model, enabling the generation of graph embeddings that capture both structural and functional similarities. The proposed method, termed "Skip-Graph," is evaluated on real-world datasets for graph classification tasks and demonstrates competitive performance compared to state-of-the-art techniques. The authors also explore various aggregation strategies for generating graph-level embeddings and provide insights into parameter tuning and visualization of learned embeddings.
Decision: Accept
The paper is well-motivated, presents a novel and general-purpose approach to graph representation learning, and demonstrates strong empirical results. The key reasons for acceptance are:
1. Novelty and Contribution: The adaptation of the skip-thought model to graph data and the use of random walks for unsupervised graph representation learning is innovative and fills a gap in the literature.
2. Empirical Validation: The method achieves competitive or superior performance on multiple datasets compared to established baselines, demonstrating its effectiveness.
Supporting Arguments
1. Problem Definition and Motivation: The paper clearly identifies the limitations of existing task-specific and supervised graph representation methods and positions its approach as a general-purpose, unsupervised alternative. The use of random walks to capture graph substructures is well-justified and builds on prior work in node representation learning.
2. Methodology: The encoder-decoder framework is rigorously described, and the analogy to skip-thoughts is compelling. The inclusion of multiple aggregation strategies for graph-level embeddings adds flexibility to the approach.
3. Experimental Results: The results are robust, with the proposed method outperforming baselines on three out of four datasets. The parameter studies and visualization further strengthen the claims about the method's effectiveness and interpretability.
Suggestions for Improvement
1. Clarity on Functional Similarity: While the paper mentions that the method captures "functionally similar" subgraphs, it would benefit from a more precise definition and examples of what constitutes functional similarity in the context of the datasets used.
2. Comparison with More Methods: Although the paper compares against relevant baselines, including additional recent graph neural network approaches (e.g., GraphSAGE, GIN) could provide a more comprehensive evaluation.
3. Scalability Analysis: The paper does not discuss the computational complexity of the method, particularly for large graphs. Including a runtime comparison with baselines would strengthen the evaluation.
4. Ablation Studies: An ablation study on the impact of random walk parameters (e.g., walk length, number of walks) on performance would provide deeper insights into the method's behavior.
Questions for the Authors
1. How does the method handle graphs with highly imbalanced class distributions in real-world datasets? Have you tested it on unbalanced datasets without subsampling?
2. Can the proposed method be extended to dynamic or evolving graphs? If so, what modifications would be required?
3. How sensitive is the model to the choice of random walk parameters (e.g., minimum and maximum walk lengths)? Could this introduce variability in the learned embeddings?
In conclusion, the paper makes a significant contribution to unsupervised graph representation learning and is well-suited for acceptance. Addressing the suggested improvements and questions could further enhance the paper's impact and clarity.