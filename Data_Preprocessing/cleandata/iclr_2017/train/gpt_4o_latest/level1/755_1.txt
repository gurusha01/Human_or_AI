Review of the Paper
Summary of Contributions
This paper provides a theoretical explanation for the success of ResNet, focusing on the role of shortcut connections, particularly 2-shortcuts, in easing the optimization of deep networks. The authors analyze deep linear networks and some nonlinear variants to demonstrate that 2-shortcuts lead to depth-invariant condition numbers of the Hessian at the zero initial point, making training deep models as feasible as training shallow ones. They also show that shortcuts of other depths either fail to improve training or introduce optimization challenges, such as higher-order stationary points or exploding condition numbers. The paper further highlights the advantages of initializing 2-shortcut networks with small weights over traditional initialization methods like Xavier or orthogonal initialization. Extensive experiments are presented to validate the theoretical claims, including analyses of learning dynamics, condition numbers, and final losses.
Decision: Accept
The paper makes a significant theoretical contribution to understanding the optimization dynamics of ResNet, particularly the role of 2-shortcuts. The combination of rigorous theoretical analysis and empirical validation provides a compelling explanation for ResNet's success. The insights into initialization strategies and the role of Hessians are novel and impactful, with potential implications for designing and training deep networks beyond ResNet.
Supporting Arguments
1. Well-Motivated Problem: The paper addresses a critical question in deep learningâ€”why ResNet is easier to train than other architectures. The authors situate their work well within the existing literature, building on prior empirical observations and theoretical insights.
2. Theoretical Rigor: The theoretical results are robust, with clear mathematical formulations and proofs. The derivation of depth-invariant condition numbers for 2-shortcuts is particularly compelling and aligns with empirical observations.
3. Empirical Validation: The experiments are thorough and well-designed, supporting the theoretical claims. The comparison of different initialization methods and shortcut depths provides practical insights.
4. Novelty: The focus on the Hessian's condition number and its role in optimization dynamics is a fresh perspective that advances our understanding of ResNet and deep learning optimization.
Suggestions for Improvement
1. Clarity of Presentation: While the theoretical results are rigorous, the paper's presentation is dense and could benefit from clearer explanations, particularly for readers less familiar with optimization theory. For example, the role of strict saddle points and their implications for optimization could be elaborated further.
2. Broader Applicability: The paper focuses on linear and simplified nonlinear networks. It would be valuable to discuss how the insights generalize to more complex architectures, such as convolutional or recurrent networks, beyond the brief mention in the future directions.
3. Experimental Scope: While the experiments are convincing, they are limited to MNIST and whitened datasets. Extending the analysis to more complex datasets and tasks would strengthen the empirical claims.
4. Practical Implications: The paper could better highlight the practical implications of its findings, such as how to design initialization schemes or shortcut structures for architectures other than ResNet.
Questions for the Authors
1. How do the theoretical results extend to convolutional networks, where the structure of the Hessian may differ due to weight sharing?
2. Could the authors provide more intuition or visualizations for the role of strict saddle points in optimization and how they differ from higher-order stationary points?
3. Have the authors tested the proposed initialization strategy (small weights with 2-shortcuts) on tasks beyond MNIST, such as ImageNet or other large-scale datasets?
4. How sensitive are the results to the choice of activation functions? For example, would the findings hold for more modern activations like Swish or GELU?
In summary, this paper makes a strong theoretical and empirical contribution to understanding ResNet's optimization dynamics. While there is room for improvement in presentation and broader applicability, the core insights are novel and impactful, warranting acceptance.