Review of the Submitted Paper
Summary of Contributions
The paper introduces a novel class of probabilistic models called discrete variational autoencoders (discrete VAEs), which effectively integrate discrete latent variables into the variational autoencoder (VAE) framework. This is achieved through a combination of an undirected graphical model (a restricted Boltzmann machine, RBM) for discrete latent variables and directed hierarchical continuous latent variables. The authors propose a method to backpropagate through discrete latent variables by augmenting them with continuous variables, enabling efficient training. The model is shown to outperform state-of-the-art methods on permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets. The paper also introduces a hierarchical approximating posterior to address correlations in the posterior distribution and demonstrates the effectiveness of the approach through rigorous experiments.
Decision: Accept
The paper is recommended for acceptance due to its significant contributions to the field of unsupervised learning with discrete latent variables. The proposed discrete VAE framework addresses a long-standing challenge in training models with discrete latent variables and demonstrates state-of-the-art performance on multiple benchmarks. The methodology is well-motivated, scientifically rigorous, and supported by strong empirical results.
Supporting Arguments
1. Problem Relevance and Innovation: The paper tackles the critical problem of incorporating discrete latent variables into the VAE framework, which has been a challenging area due to the non-differentiability of discrete variables. The proposed solution of augmenting discrete variables with continuous ones is both innovative and practical, making it a valuable contribution to the literature.
2. Empirical Validation: The experiments are thorough and demonstrate clear improvements over existing methods. The use of datasets like MNIST, Omniglot, and Caltech-101 Silhouettes provides a robust evaluation of the model's capabilities. The results are reproducible and statistically significant, with detailed analysis provided for various architectural choices.
3. Scientific Rigor: The paper is grounded in a solid theoretical framework. The derivations, particularly for backpropagation through discrete variables and the hierarchical approximating posterior, are detailed and correct. The authors also address potential pitfalls, such as variance in gradient estimates, and propose effective solutions.
Suggestions for Improvement
1. Clarity of Presentation: While the theoretical sections are comprehensive, they are dense and may be challenging for readers unfamiliar with the topic. Simplifying some of the explanations or providing additional visual aids (e.g., diagrams of the hierarchical posterior) could improve accessibility.
2. Ablation Studies: Although the paper includes some analysis of simplified models, a more systematic ablation study would strengthen the claims. For example, isolating the contributions of the RBM, the hierarchical posterior, and the continuous latent variables would provide deeper insights into the model's performance.
3. Scalability: The paper briefly mentions the potential need for larger RBMs for datasets like ImageNet. A discussion on the computational scalability of the proposed method, particularly for high-dimensional data, would be valuable.
4. Comparison with Alternatives: While the paper compares discrete VAEs to several state-of-the-art methods, it would benefit from a more detailed discussion of how it differs from or improves upon closely related approaches, such as structured VAEs or models using REINFORCE.
Questions for the Authors
1. How does the choice of the smoothing transformation (e.g., spike-and-exponential) affect the model's performance? Would alternative transformations lead to significant differences in results?
2. Can the proposed method handle datasets with a large number of discrete classes (e.g., ImageNet) without significant modifications? If not, what adaptations would be necessary?
3. How sensitive is the model to the choice of hyperparameters, such as the size of the RBM or the number of layers in the hierarchical posterior?
The paper makes a substantial contribution to the field and is a strong candidate for acceptance. Addressing the above suggestions and questions would further enhance its impact.