Review of the Paper
Summary
This paper presents a novel framework for training and evaluating the ability of artificial agents to seek information efficiently in partially observable environments. The authors introduce a collection of tasks requiring agents to iteratively search for and synthesize information fragments to achieve specific goals. They propose a reinforcement learning-based approach that combines deep neural architectures with intrinsic and extrinsic reward mechanisms to encourage intelligent information-seeking behavior. The paper makes several contributions, including a shift in perspective on attention mechanisms, the development of task-agnostic heuristics for information gain, and empirical demonstrations of their approach on tasks such as cluttered MNIST, BlockWorld, CelebA, and Hangman. The results show that the proposed models achieve competitive performance while consuming significantly less information compared to baseline methods.
Decision: Accept
The paper is well-motivated, demonstrates scientific rigor, and provides meaningful contributions to the field of reinforcement learning and information-seeking agents. The key reasons for acceptance are:  
1. Novelty and Relevance: The paper introduces a general problem setting for information-seeking, which is a critical capability for intelligent agents. The proposed approach is innovative and addresses a gap in the literature.  
2. Empirical Validation: The authors provide extensive experimental results across diverse tasks, demonstrating the effectiveness and generalizability of their approach.  
Supporting Arguments
1. Problem Definition and Motivation: The paper is well-placed in the literature, drawing connections to prior work on attention, curiosity-driven exploration, and reinforcement learning. The authors clearly articulate the importance of efficient information-seeking for artificial agents and provide a compelling motivation for their work.  
2. Methodology: The proposed framework is rigorously defined, with detailed descriptions of the models, reward functions, and training procedures. The use of intrinsic rewards to encourage information-seeking behavior is particularly noteworthy.  
3. Experimental Results: The empirical results are robust and demonstrate the superiority of the proposed approach in terms of information efficiency and task performance. The experiments are diverse, covering both synthetic and real-world datasets, and include comparisons with baseline methods.  
Suggestions for Improvement
1. Clarity of Presentation: While the paper is comprehensive, certain sections, such as the mathematical formulation of the objective function (Eqn. 1), could benefit from additional explanation or examples to aid reader understanding.  
2. Baseline Comparisons: The authors acknowledge that some baselines (e.g., RAM and DRAW) were not optimized for information efficiency. It would strengthen the paper to include additional baselines or ablations to isolate the impact of specific design choices.  
3. Task Diversity: While the tasks are varied, they primarily focus on visual and language domains. Exploring applications in other domains, such as robotics or decision-making under uncertainty, could further demonstrate the generalizability of the approach.  
4. Discussion of Limitations: The paper could benefit from a more explicit discussion of the limitations of the proposed approach, such as scalability to larger environments or computational costs.  
Questions for the Authors
1. How does the proposed framework scale to environments with significantly larger state spaces or more complex tasks?  
2. Can the intrinsic reward mechanism be adapted dynamically during training to better balance exploration and exploitation?  
3. How sensitive is the model's performance to the choice of hyperparameters, such as the number of questions allowed or the reward function design?  
Overall, this paper makes a strong contribution to the field and is likely to stimulate further research on information-seeking agents. With minor revisions for clarity and additional baselines, it would be an excellent addition to the conference.