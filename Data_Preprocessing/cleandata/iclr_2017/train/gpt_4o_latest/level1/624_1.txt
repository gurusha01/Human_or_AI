Review of the Paper
Summary of Contributions
This paper addresses the theoretical underpinnings of why deep neural networks trained with gradient-based methods are not significantly affected by bad local minima, despite their non-convex nature. The authors challenge the widely accepted "no bad local minima" hypothesis by constructing counterexamples that demonstrate the susceptibility of deep networks to suboptimal local minima under specific conditions. They focus on finite-sized datasets and models, showing that bad learning dynamics can arise due to poor initialization, data structure, or architectural choices. The paper provides both theoretical proofs and empirical evidence, including experiments on MNIST and synthetic datasets, to support its claims. The authors also propose that the well-behaved learning dynamics observed in practice are conditional on factors such as initialization and data structure, rather than being globally guaranteed.
Decision: Accept
The paper makes a significant theoretical contribution by challenging a foundational assumption in deep learning and providing concrete counterexamples to support its claims. The combination of rigorous theoretical analysis and empirical validation strengthens the paper's impact. Additionally, the work opens up new avenues for understanding the error surfaces of neural networks and the role of initialization and data structure in successful training.
Supporting Arguments for the Decision
1. Novelty and Importance: The paper addresses a critical gap in the literature by questioning the generalizability of the "no bad local minima" hypothesis to finite-sized datasets and models. This is a valuable contribution to the theoretical understanding of deep learning.
2. Scientific Rigor: The theoretical results are well-supported with detailed proofs, and the empirical experiments are carefully designed to validate the claims. The use of both synthetic and real-world datasets (e.g., MNIST) adds credibility to the findings.
3. Clarity and Organization: The paper is well-structured, with a clear progression from theoretical results to empirical validation. The authors provide sufficient background and context, making the work accessible to a broad audience.
Suggestions for Improvement
1. Broader Empirical Validation: While the paper provides compelling examples, the empirical results are limited to specific datasets and architectures. Extending the experiments to more diverse datasets (e.g., CIFAR-10, ImageNet) and architectures (e.g., convolutional networks) would strengthen the generalizability of the findings.
2. Practical Implications: The paper focuses heavily on theoretical insights but does not sufficiently discuss the practical implications of its findings. For example, how can practitioners mitigate the risks of bad local minima in real-world scenarios? Including recommendations or guidelines would enhance the paper's utility.
3. Comparison with Related Work: While the literature review is thorough, the paper could benefit from a more explicit comparison of its findings with prior work. For instance, how do the proposed counterexamples differ from those in earlier studies, and what new insights do they provide?
4. Visualization of Results: The paper includes some figures, but additional visualizations (e.g., error surface plots, training dynamics) could help illustrate the phenomena described in the text more intuitively.
Questions for the Authors
1. How do the constructed counterexamples scale to high-dimensional datasets and more complex architectures, such as convolutional or transformer-based networks?
2. Can the authors provide more insights into the practical relevance of their findings? For example, are there specific initialization schemes or data preprocessing techniques that can mitigate the issues identified in the paper?
3. The paper focuses on the ReLU activation function. Do the findings generalize to other popular activation functions, such as GELU or Swish, which are increasingly used in practice?
Overall, this paper makes a valuable contribution to the theoretical understanding of deep learning and raises important questions about the assumptions underlying current practices. With some additional empirical validation and practical insights, it could have an even broader impact.