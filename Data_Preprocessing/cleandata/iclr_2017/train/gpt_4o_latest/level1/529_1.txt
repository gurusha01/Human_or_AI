Review of the Paper
Summary of Contributions
This paper addresses the challenges of training sequence models, particularly Recurrent Neural Networks (RNNs), for tasks like music generation, where supervised learning often leads to failure modes such as incoherent global structure and excessive repetition. The authors propose RL Tuner, a novel framework that combines Reinforcement Learning (RL) with Maximum Likelihood (ML) training to refine pre-trained RNNs by imposing structural constraints through a reward function. The paper contributes to the literature by:  
1. Introducing a method that integrates RL with pre-trained RNNs to balance learned data distributions and heuristic rewards.  
2. Demonstrating connections between this approach and KL-regularized RL methods like Ψ-learning and G-learning.  
3. Applying the framework to music generation, showing that the RL Tuner produces more musically pleasing melodies compared to baseline RNNs.  
4. Providing empirical comparisons of Q-learning, Ψ-learning, and G-learning in this context.  
The experimental results, including user studies and quantitative metrics, convincingly show that the RL Tuner improves adherence to music theory rules while retaining information from the training data. This framework has potential applications beyond music generation, such as text generation and sequence modeling tasks where structural constraints are desirable.
---
Decision: Accept  
The paper makes a significant contribution to the intersection of RL and sequence modeling, presenting a novel and well-motivated approach that is empirically validated. The key reasons for acceptance are:  
1. Novelty and Impact: The proposed method effectively addresses a critical limitation of sequence models by combining ML and RL in a principled way, with clear potential for broader applications.  
2. Scientific Rigor: The theoretical grounding in KL-regularized RL and the empirical comparisons among Q-learning, Ψ-learning, and G-learning are thorough and well-executed.  
---
Supporting Arguments
1. Problem Motivation and Placement in Literature: The paper is well-situated in the literature, clearly identifying the limitations of existing RNN-based generative models and contrasting its approach with related work like SeqGAN and MIXER. The authors also highlight the novelty of retaining data-driven probabilities while imposing structural constraints, which distinguishes their method from prior RL-based approaches.  
2. Experimental Validation: The experiments are comprehensive, including quantitative metrics (e.g., adherence to music theory rules, log probabilities) and subjective evaluations (user studies). The results convincingly demonstrate that the RL Tuner improves both the musicality and structural coherence of generated melodies.  
3. Clarity and Reproducibility: The paper provides detailed descriptions of the methodology, including mathematical derivations, implementation details, and open-source code, ensuring reproducibility.  
---
Suggestions for Improvement
While the paper is strong overall, the following points could enhance its clarity and impact:  
1. Reward Design: The music-theory-based reward function is well-motivated but somewhat heuristic. The authors could discuss how these heuristics might generalize to other domains or how alternative reward functions could be designed.  
2. Broader Applications: While the paper mentions potential applications in text generation and question answering, a concrete example or preliminary experiment in another domain would strengthen the claim of generalizability.  
3. User Study Details: The user study results are compelling, but additional details about the study design (e.g., participant demographics, instructions) would improve transparency.  
4. Limitations: The authors acknowledge the simplicity of the melodies and the heuristic nature of the rules but could further discuss how these limitations might be addressed in future work (e.g., incorporating richer musical representations or more complex reward functions).  
---
Questions for the Authors
1. How sensitive is the RL Tuner to the choice of the constant \( c \) that balances data-driven probabilities and heuristic rewards? Could this parameter be learned or dynamically adjusted?  
2. Could the proposed framework handle polyphonic music generation, or would significant modifications be required?  
3. How does the RL Tuner perform when the training data contains conflicting or noisy patterns (e.g., melodies that violate music theory rules)?  
4. Could the approach be extended to unsupervised pre-training, where no labeled data is available?  
Overall, this paper makes a valuable contribution to sequence modeling and RL, and I recommend its acceptance with minor revisions to address the above suggestions.