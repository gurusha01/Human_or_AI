Review
This paper introduces a novel, domain-agnostic approach to dataset augmentation by performing transformations in a learned feature space rather than the input space. The authors propose using sequence autoencoders to construct feature spaces and apply transformations such as noise addition, interpolation, and extrapolation to generate synthetic data. The paper demonstrates that extrapolation in feature space improves the performance of supervised learning models across a variety of domains, including speech, motion capture, and image classification. Notably, the authors achieve near state-of-the-art results on two datasets and show that feature space augmentation can complement traditional input space augmentation.
Decision: Accept
Key reasons for acceptance:
1. Novelty and Generality: The proposed method is simple yet innovative, offering a domain-independent solution to dataset augmentation, which is a significant contribution to the field.
2. Strong Empirical Validation: The paper provides extensive experimental results across diverse datasets, demonstrating the effectiveness of the approach and its potential to generalize across domains.
Supporting Arguments
1. Well-Motivated and Timely: The paper is well-placed in the literature, addressing the limitations of domain-specific augmentation techniques. The use of learned feature spaces aligns with recent advancements in unsupervised representation learning, such as autoencoders and generative models.
   
2. Rigorous Evaluation: The authors conduct experiments on five datasets from different domains, providing both qualitative visualizations and quantitative results. The consistent improvement in model performance, particularly with extrapolation, supports the claims made in the paper.
3. Practical Implications: The method's domain-agnostic nature makes it broadly applicable, and the authors provide insights into when extrapolation is most effective (e.g., datasets with complex decision boundaries). This guidance enhances the practical utility of the work.
Suggestions for Improvement
1. Theoretical Insights: While the empirical results are strong, the paper could benefit from a deeper theoretical analysis of why extrapolation in feature space is effective, particularly in high-dimensional settings. This would strengthen the scientific rigor of the work.
2. Comparison with Other Feature Space Techniques: The paper does not compare its method to other feature space augmentation techniques, such as those using GANs or VAEs. Including such comparisons would provide a more comprehensive evaluation.
3. Scalability: The paper does not discuss the computational cost of finding nearest neighbors in feature space, particularly for large datasets. Addressing this limitation or proposing scalable alternatives would enhance the method's practicality.
4. Ablation Studies: While the paper evaluates different augmentation techniques (noise, interpolation, extrapolation), it would be helpful to include ablation studies on the architecture of the sequence autoencoder to understand its impact on the results.
Questions for the Authors
1. How does the choice of the representation learning model (e.g., sequence autoencoder) affect the quality of the feature space and the effectiveness of the augmentation? Have you explored alternative architectures like VAEs or contrastive learning-based models?
2. In high-dimensional feature spaces, how do you ensure that extrapolated samples remain within the manifold of plausible data points? Could this lead to unrealistic or noisy samples in some cases?
3. Could the proposed method be combined with adversarial training or other regularization techniques to further improve robustness? If so, have you explored such combinations?
4. How does the method perform on larger-scale datasets, such as ImageNet, where computational efficiency and scalability become critical?
Overall, this paper makes a valuable contribution to the field of data augmentation and representation learning. Addressing the above points would further strengthen the work and broaden its impact.