The paper introduces "wild variational inference," a novel approach to variational inference that eliminates the need for tractable density functions in inference networks. This innovation allows the application of variational methods to more complex and challenging probabilistic models. The authors propose two methods: (1) amortized Stein variational gradient descent (SVGD), which trains inference networks to mimic SVGD dynamics, and (2) kernelized Stein discrepancy (KSD) minimization, which directly optimizes a discrepancy measure without requiring explicit density calculations. As an application, the paper demonstrates how these methods can adaptively optimize step sizes in stochastic gradient Langevin dynamics (SGLD), achieving significant performance improvements over traditional hand-designed step size schemes. Empirical results on Gaussian mixture models and Bayesian logistic regression validate the effectiveness of the proposed methods.
Decision: Accept
Key reasons:  
1. Novelty and Contribution: The paper addresses a significant limitation in variational inference by enabling the use of general inference networks without tractability constraints. This is a meaningful contribution to the field of Bayesian inference and probabilistic modeling.  
2. Empirical Validation: The proposed methods are rigorously evaluated on both synthetic and real-world tasks, demonstrating clear improvements over baseline approaches. The results are compelling and suggest practical utility.  
Supporting Arguments:  
- The paper is well-motivated and builds on a solid foundation of prior work, including Stein discrepancy and SVGD. The authors provide a clear explanation of how their methods extend existing approaches, making the contribution both novel and well-grounded in the literature.  
- The empirical results are thorough, covering multiple tasks and comparing against strong baselines. The adaptive step size optimization for SGLD is particularly impactful, as it addresses a common challenge in Bayesian inference.  
- The theoretical framework is sound, with detailed derivations and connections to established concepts like Stein operators and kernel methods.  
Additional Feedback for Improvement:  
1. Clarity of Presentation: While the paper is technically rigorous, some sections (e.g., the derivations of KSD and SVGD) are dense and could benefit from additional intuition or visual aids to improve accessibility for a broader audience.  
2. Broader Impact Discussion: The paper could elaborate on potential applications of wild variational inference beyond the examples provided, such as in deep generative models or reinforcement learning.  
3. Computational Complexity: A more detailed discussion of the computational trade-offs of the proposed methods, particularly in high-dimensional settings, would strengthen the paper. For instance, how does the complexity of KSD minimization scale with the number of particles or dimensions?  
Questions for the Authors:  
1. How sensitive are the proposed methods to the choice of kernel in KSD? Would different kernels significantly impact performance?  
2. Can the methods be extended to handle non-differentiable target distributions, or are they inherently limited to smooth densities?  
3. How do the proposed approaches compare to other recent advances in adaptive MCMC methods, such as Hamiltonian Monte Carlo with adaptive step sizes?  
Overall, the paper makes a strong contribution to the field of variational inference and probabilistic modeling. With minor improvements to clarity and additional discussion of broader implications, it could have an even greater impact.