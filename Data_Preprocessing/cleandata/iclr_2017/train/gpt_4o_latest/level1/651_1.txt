The paper introduces the Gaussian Error Linear Unit (GELU), a novel neural network activation function that bridges the gap between stochastic regularizers and traditional nonlinearities. The authors propose that GELU, derived as the expected transformation of a stochastic regularizer, offers a probabilistic perspective on neuron outputs. They empirically demonstrate that GELU outperforms widely used activation functions like ReLU and ELU across diverse tasks, including computer vision, natural language processing, and speech recognition. Furthermore, the paper explores the SOI (Stochastic 0-I) map, a stochastic regularizer that can replace traditional nonlinearities, challenging the necessity of deterministic activation functions in neural networks. The authors provide theoretical insights, rigorous experimental results, and practical recommendations for implementing GELU.
Decision: Accept
The paper is well-motivated, scientifically rigorous, and makes a significant contribution to the field of neural network design. The key reasons for acceptance are:  
1. Novelty and Impact: The GELU activation function is a meaningful innovation that combines stochastic regularization with deterministic nonlinearities, offering both theoretical insights and practical advantages.  
2. Empirical Validation: The authors present extensive experimental results across multiple domains, consistently demonstrating GELU's superiority over ReLU and ELU.  
Supporting Arguments:  
1. The paper is well-placed in the literature, addressing the limitations of existing activation functions (e.g., ReLU's hard gating and ELU's gradient issues) and building on prior work in stochastic regularizers like dropout and zoneout. The connection between GELU and the SOI map is particularly compelling, as it provides a probabilistic interpretation of activation functions.  
2. The experimental methodology is robust, with evaluations on diverse datasets (e.g., MNIST, CIFAR-10/100, TIMIT) and architectures (e.g., fully connected networks, convolutional networks, residual networks). The results are consistent and statistically supported, showcasing GELU's versatility and effectiveness.  
3. The practical recommendations for implementing GELU, such as using a specific approximation for the Gaussian CDF, enhance the paper's utility for practitioners.  
Suggestions for Improvement:  
1. Clarity of Theoretical Insights: While the connection between GELU and the SOI map is intriguing, the explanation of how the SOI map probabilistically replaces traditional nonlinearities could be further clarified. For instance, a more detailed comparison with adaptive dropout would strengthen the theoretical narrative.  
2. Broader Comparisons: The experiments focus primarily on ReLU and ELU. Including comparisons with other modern activation functions, such as Leaky ReLU, Swish, or Mish, would provide a more comprehensive evaluation.  
3. Ablation Studies: It would be helpful to include ablation studies to isolate the contributions of GELU's curvature, non-monotonicity, and probabilistic weighting to its performance gains.  
Questions for the Authors:  
1. How does GELU perform in scenarios with limited computational resources, given its reliance on the Gaussian CDF approximation?  
2. Have you explored the impact of GELU on tasks involving sparse or highly imbalanced data?  
3. Could the SOI map be extended to other domains or architectures, such as graph neural networks or transformers?  
Overall, the paper presents a significant advancement in activation function design, and its acceptance would benefit both theoretical research and practical applications in deep learning.