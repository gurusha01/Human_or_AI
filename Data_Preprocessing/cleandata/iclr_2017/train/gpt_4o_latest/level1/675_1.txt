Review of "Incremental Sequence Learning for Sequence Prediction and Classification"
Summary of Contributions  
This paper introduces a novel approach to sequence learning called Incremental Sequence Learning (ISL), which incrementally trains a model by first focusing on shorter prefixes of sequences and progressively increasing the sequence length as performance improves. The authors demonstrate the effectiveness of ISL using a new dataset derived from MNIST, where handwritten digits are represented as pen stroke sequences. The paper claims that ISL significantly accelerates training (20x faster to reach baseline performance), reduces test error by 74%, and improves generalization compared to regular sequence learning and other curriculum learning methods. Additionally, the paper explores the transfer learning potential of ISL for sequence classification tasks, showing that pretraining with ISL improves classification accuracy. The work is well-supported by experiments and provides open-source code and datasets, making it a valuable contribution to the field of sequence learning.
Decision: Accept  
Key reasons for acceptance are:  
1. Significant Contribution: The paper addresses an important problem in sequence learning and provides a simple yet effective method that demonstrates substantial improvements in both computational efficiency and generalization performance.  
2. Scientific Rigor: The claims are well-supported by extensive experiments, including ablation studies and comparisons with baseline and alternative curriculum learning methods. The analysis of the underlying mechanisms (e.g., the role of RNNs in building internal representations) further strengthens the findings.  
3. Reproducibility: The authors provide open-source code and datasets, ensuring that the work can be easily reproduced and extended by the community.
Supporting Arguments  
- The experimental results clearly demonstrate the superiority of ISL over regular sequence learning and other curriculum learning methods. The reduction in test error and the ability to generalize better are particularly noteworthy.  
- The ablation studies effectively isolate the factors contributing to ISL's success, such as the role of RNNs in learning internal representations and the impact of adaptive batch sizes.  
- The introduction of the MNIST pen stroke sequence dataset is a valuable contribution in itself, providing a new benchmark for sequence learning tasks.  
- The transfer learning experiments show that ISL-trained models can be effectively repurposed for sequence classification, highlighting the versatility of the approach.
Suggestions for Improvement  
1. Clarify Dataset Limitations: While the MNIST pen stroke dataset is novel, the paper notes that information is lost during the thinning process. It would be helpful to quantify the extent of this loss and discuss its potential impact on the generalizability of the results to other datasets.  
2. Comparison with More Baselines: The paper compares ISL with regular sequence learning and two curriculum learning methods. Including additional baselines, such as attention-based models or transformer architectures, could strengthen the evaluation.  
3. Scalability Analysis: While ISL performs well on the MNIST-derived dataset, it would be valuable to test its scalability on larger and more complex sequence datasets, such as natural language or time-series data.  
4. Transfer Learning Details: The transfer learning experiments are promising but could benefit from a more detailed analysis of how ISL-trained representations differ from those learned by models trained from scratch.
Questions for the Authors  
1. How does ISL perform on sequence datasets with different characteristics, such as variable-length sequences or sequences with high noise levels?  
2. Could the observed benefits of ISL be further enhanced by combining it with other techniques, such as attention mechanisms or data augmentation?  
3. Have you considered applying ISL to real-world sequence learning tasks, such as speech recognition or financial time-series forecasting?  
In conclusion, this paper makes a strong contribution to the field of sequence learning by introducing a simple yet effective approach that is well-supported by experimental results. With minor clarifications and additional evaluations, this work has the potential to influence a wide range of sequence learning applications.