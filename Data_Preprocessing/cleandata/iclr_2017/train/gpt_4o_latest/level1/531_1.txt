Review of the Paper
Summary of Contributions  
This paper introduces a novel neural network architecture, termed Perception Updating Networks (PUN), for video modeling and frame prediction. The approach is inspired by computer graphics pipelines and aims to explicitly decouple "what" (content) and "where" (movement) in video frames. The authors propose a statistical framework based on variational auto-encoding Bayes and derive a variational lower bound to optimize the model. The architecture incorporates recurrent neural networks (RNNs) augmented with task-specific modules, such as sprite memory and transformation layers, to achieve interpretable and efficient video generation. The paper demonstrates the effectiveness of the proposed method on synthetic datasets (Bouncing Shapes) and the Moving MNIST benchmark, showing promising results in interpretability and long-term video prediction compared to baseline RNNs.
Decision: Accept  
The paper is well-motivated, introduces a novel and interpretable approach to video modeling, and provides empirical evidence supporting its claims. The key reasons for acceptance are:  
1. The proposed framework offers a fresh perspective on video modeling by integrating principles from computer graphics and machine learning, which is a meaningful contribution to the field.  
2. The experiments demonstrate the utility of the approach, particularly in generating interpretable representations and achieving longer-term video predictions compared to baseline methods.
Supporting Arguments  
1. Well-Motivated Approach: The paper is grounded in prior work on vision as inverse graphics and builds on the limitations of existing methods. By explicitly decoupling "what" and "where," the authors address a key challenge in video modeling and provide a principled statistical framework for their approach. This is a significant step forward in the field.  
2. Empirical Validation: The experiments on synthetic datasets and Moving MNIST effectively demonstrate the advantages of PUN over baseline RNNs. The results show that the proposed method generates smoother and more interpretable video frames, and it performs better in long-term video prediction.  
3. Scientific Rigor: The paper provides a detailed derivation of the variational lower bound and explains the architectural choices clearly. The use of both convolution-based and spatial transformer-based implementations adds robustness to the evaluation.
Suggestions for Improvement  
1. Clarity of Presentation: While the paper is thorough, it is dense and difficult to follow in places. Simplifying the mathematical derivations or moving some details to an appendix could improve readability.  
2. Comparison with State-of-the-Art: The paper acknowledges that the results on Moving MNIST are not as strong as those of Video Pixel Networks. A more detailed discussion of how PUN could be combined with such approaches would strengthen the paper.  
3. Additional Experiments: The experiments focus on synthetic datasets and Moving MNIST, which are relatively simple. Testing the method on more complex real-world datasets would provide stronger evidence of its generalizability.  
4. Ablation Studies: While the paper discusses the impact of certain architectural choices (e.g., softmax for Î´xy), a more systematic ablation study would help clarify the importance of each component in the proposed architecture.
Questions for the Authors  
1. How does the proposed method perform on real-world video datasets with more complex dynamics and occlusions?  
2. Can the architecture handle scenarios where multiple sprites interact or occlude each other?  
3. Have you considered combining PUN with more advanced decoders (e.g., PixelCNN) to improve the likelihood scores on benchmarks like Moving MNIST?  
4. How sensitive is the model to hyperparameter choices, such as the size of the sprite memory or the latent variable dimensions?  
Overall, this paper makes a meaningful contribution to the field of video modeling and is a strong candidate for acceptance, provided the authors address the above suggestions to further strengthen their work.