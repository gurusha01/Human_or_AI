Review
Summary of Contributions
The paper proposes a novel method for improving the performance of convolutional neural networks (CNNs) by transferring attention maps from a teacher network to a student network. The authors define attention in CNNs as spatial maps derived from activation-based or gradient-based information and propose mechanisms to transfer this attention to guide the learning of a student network. The paper introduces several attention transfer methods, demonstrates their effectiveness across various datasets (e.g., CIFAR, ImageNet, CUB, and Scenes), and compares them with traditional knowledge distillation techniques. The results show consistent improvements in student network performance, particularly when combining attention transfer with knowledge distillation. The authors also provide open-source code and models, which enhances reproducibility.
Decision: Accept
Key Reasons:
1. Novelty and Contribution: The paper introduces a well-motivated and novel approach to leveraging attention maps for knowledge transfer, which is a meaningful extension to existing knowledge distillation techniques.
2. Empirical Validation: The proposed methods are rigorously evaluated across multiple datasets and architectures, demonstrating consistent improvements in performance. The experiments are thorough and scientifically rigorous.
Supporting Arguments
1. Motivation and Literature Placement: The paper is well-grounded in prior work on attention mechanisms and knowledge distillation. It builds on foundational research while addressing a specific gap: the use of attention maps for improving student networks. The authors provide a clear explanation of how their approach differs from and extends existing methods.
2. Experimental Rigor: The experiments are comprehensive, covering both small-scale (CIFAR) and large-scale (ImageNet) datasets, as well as fine-grained classification tasks. The results consistently show that attention transfer improves performance, and the paper provides detailed comparisons with baseline methods like full activation transfer and knowledge distillation.
3. Practical Relevance: The proposed methods are computationally efficient, as attention maps can be computed during forward propagation, and they are compatible with existing architectures. The availability of code further enhances the practical utility of the work.
Suggestions for Improvement
1. Clarify Hyperparameter Tuning: While the paper mentions that hyperparameters (e.g., Î²) were not extensively tuned for ImageNet experiments, it would be helpful to provide more details on how these were selected for other datasets. This would aid reproducibility and understanding of the method's sensitivity to hyperparameters.
2. Gradient-Based Attention Transfer: The experiments on gradient-based attention transfer are limited to CIFAR and a single architecture. Expanding these experiments to larger datasets and more complex architectures would strengthen the claims about its effectiveness.
3. Visualization of Attention Maps: While the paper includes some visualizations, additional qualitative comparisons of attention maps (e.g., before and after transfer) across datasets and architectures could provide deeper insights into how attention transfer impacts learning.
4. Discussion of Limitations: The paper could benefit from a more explicit discussion of potential limitations, such as the computational cost of second backpropagation in gradient-based attention transfer or challenges in applying the method to tasks beyond classification (e.g., object detection).
Questions for the Authors
1. How does the choice of attention mapping function (e.g., Fsum vs. Fmax) affect performance across different datasets and architectures? Are there specific scenarios where one function is preferable?
2. Have you explored the impact of attention transfer on tasks beyond classification, such as object detection or segmentation? If not, do you anticipate any challenges in extending the method to these tasks?
3. For gradient-based attention transfer, how does batch normalization affect the second backpropagation step, and what strategies could be used to address this?
Overall, the paper makes a significant contribution to the field of knowledge transfer in neural networks, and the proposed methods have strong potential for practical applications. With minor clarifications and additional experiments, the work could be further strengthened.