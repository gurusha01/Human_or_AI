Review of the Paper
Summary of Contributions
This paper addresses the challenge of efficiently training large-scale stochastic feedforward neural networks (SFNNs), which are known for their expressive power and regularization benefits but are notoriously difficult to train. The authors propose a novel intermediate model, Simplified-SFNN, which bridges the gap between deterministic deep neural networks (DNNs) and SFNNs. The key contribution lies in establishing a connection between DNN → Simplified-SFNN → SFNN, enabling the transfer of pre-trained DNN parameters to SFNNs via Simplified-SFNN. This approach leverages the computational efficiency of DNNs while preserving the stochastic regularization benefits of SFNNs. The paper demonstrates the effectiveness of this method on multiple datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and tasks (classification and multi-modal learning), showing consistent improvements in performance over baseline DNNs.
Decision: Accept
The paper is well-motivated, methodologically sound, and makes a significant contribution to the field of stochastic neural networks. The proposed Simplified-SFNN model and the associated training procedure are novel and address a critical bottleneck in training SFNNs. The experimental results are comprehensive and convincingly demonstrate the utility of the approach.
Supporting Arguments
1. Problem Tackled: The paper effectively addresses the long-standing issue of training SFNNs, which are computationally expensive due to the intractability of probabilistic inference and gradient estimation. By introducing Simplified-SFNN, the authors provide a practical solution that leverages the strengths of both DNNs and SFNNs.
   
2. Motivation and Placement in Literature: The paper is well-situated within the existing literature on stochastic neural networks. It builds on prior work on parameter transfer and stochastic regularization while addressing the limitations of earlier methods. The authors provide a clear theoretical foundation for their approach, supported by rigorous proofs and derivations.
3. Empirical Validation: The experimental results are robust and scientifically rigorous. The authors evaluate their method on diverse datasets and tasks, demonstrating consistent improvements in both classification accuracy and multi-modal learning. The use of state-of-the-art architectures like Wide Residual Networks (WRNs) further strengthens the empirical contributions.
Suggestions for Improvement
1. Clarity of Presentation: While the theoretical derivations are thorough, the paper could benefit from a more concise explanation of key equations and proofs. For example, the connection between Simplified-SFNN and SFNN could be summarized more intuitively for readers unfamiliar with the technical details.
2. Ablation Studies: It would be helpful to include ablation studies to isolate the contributions of different components of the proposed method (e.g., the impact of the Simplified-SFNN layer versus direct parameter transfer).
3. Scalability Analysis: While the paper demonstrates improvements in training efficiency, a more detailed analysis of the computational cost (e.g., training time or memory usage) compared to baseline SFNNs would provide additional insights into the practicality of the method.
4. Broader Applicability: The paper primarily focuses on image datasets and tasks. It would be interesting to explore the applicability of the proposed method to other domains, such as natural language processing or time-series data.
Questions for the Authors
1. How sensitive is the performance of Simplified-SFNN to the choice of hyperparameters (e.g., γ values)? Could you provide guidelines for selecting these parameters in practice?
2. Have you considered extending the method to architectures with consecutive stochastic layers? If so, what challenges arise, and how might they be addressed?
3. Can the proposed approach be integrated with other regularization techniques, such as adversarial training or data augmentation, to further enhance performance?
In conclusion, this paper makes a strong contribution to the field of stochastic neural networks by proposing a novel and practical training method. With minor improvements in clarity and additional experiments, it has the potential to significantly impact both research and applications.