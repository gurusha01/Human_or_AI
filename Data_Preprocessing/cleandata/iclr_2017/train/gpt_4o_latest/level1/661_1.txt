Review of "Neural Graph Machines: Semi-Supervised Learning on Graphs with Neural Networks"
Summary of Contributions
This paper proposes a novel training objective, termed Neural Graph Machines (NGM), that integrates the strengths of label propagation and neural networks for semi-supervised learning on graphs. The authors introduce a regularization term inspired by label propagation, which biases neural networks to learn similar hidden representations for neighboring nodes in a graph. The proposed method is scalable, leveraging stochastic gradient descent, and is validated across diverse tasks, including multilabel classification on social graphs, text categorization, and semantic intent classification. The paper demonstrates that NGM outperforms baseline approaches, including two-stage methods like node2vec, and achieves significant improvements in low-data regimes. The authors also explore the use of graph adjacency matrices as direct inputs to neural networks, further simplifying the pipeline. Overall, the work contributes a flexible, efficient, and effective framework for graph-based semi-supervised learning.
Decision: Accept
The paper presents a well-motivated and novel approach to semi-supervised learning on graphs, supported by rigorous experimentation and clear improvements over existing methods. The key reasons for acceptance are:
1. Novelty and Significance: The integration of label propagation principles into neural network training is innovative and addresses a critical gap in leveraging unlabeled data in graph-based tasks.
2. Empirical Validation: The experimental results convincingly demonstrate the efficacy of the proposed method across multiple datasets and neural architectures, with clear improvements over baselines.
Supporting Arguments
1. Well-Motivated Approach: The paper builds on established methods like label propagation and neural networks, clearly identifying their limitations and proposing a unified framework that addresses these shortcomings. The connections to prior work are well-articulated, and the proposed method is positioned as a natural extension of existing techniques.
2. Scientific Rigor: The experiments are thorough, covering diverse tasks and datasets. The comparisons to baseline methods are fair, and the results are consistently favorable for NGM. The scalability of the method is demonstrated through its application to large graphs, and the complexity analysis supports its practical feasibility.
3. Generality: The framework is flexible, applicable to various neural architectures (e.g., CNNs, LSTMs), and capable of handling different types of graphs and input features. This generality enhances the impact of the proposed method.
Suggestions for Improvement
1. Clarity in Graph Construction: While the paper discusses graph construction methods, more details on how graphs are constructed for specific tasks (e.g., similarity thresholds, edge weights) would improve reproducibility.
2. Hyperparameter Sensitivity: The choice of hyperparameters (e.g., Î± values) is critical to the method's performance. A more detailed analysis of their impact would strengthen the paper.
3. Ablation Studies: While the paper demonstrates the overall effectiveness of NGM, ablation studies isolating the contributions of individual terms in the objective function (e.g., labeled-labeled vs. labeled-unlabeled edges) would provide deeper insights.
4. Comparison to Recent Graph Neural Networks (GNNs): The paper primarily compares NGM to label propagation and two-stage methods. Including comparisons to modern GNNs (e.g., Graph Convolutional Networks) would position the work more clearly within the current state of the art.
Questions for the Authors
1. How sensitive is the method to the quality of the input graph? For example, how does performance degrade with noisy or incomplete graphs?
2. Can the proposed method handle dynamic graphs where edges or nodes change over time? If so, how would the training objective adapt to such scenarios?
3. In the experiments, why were unlabeled-unlabeled edges found to be less helpful? Could this be task-specific, or is it a general observation?
In conclusion, this paper makes a significant contribution to semi-supervised learning on graphs and offers a promising direction for future research. With minor clarifications and additional comparisons, it could further solidify its impact.