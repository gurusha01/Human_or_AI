Review of the Paper: Fine Grained Action Repetition (FiGAR)
Summary of Contributions
The paper introduces FiGAR, a novel framework for enabling temporal abstractions in Deep Reinforcement Learning (DRL) algorithms. Unlike traditional DRL methods that execute actions at fixed time scales, FiGAR allows agents to dynamically decide both the action and the time scale of its repetition. This is achieved through a structured and factored policy representation, where the action and action-repetition policies are decoupled. The framework is generic and can be applied to both discrete and continuous action spaces, making it compatible with a wide range of DRL algorithms, including A3C, TRPO, and DDPG. The authors empirically demonstrate FiGAR's efficacy by showing significant performance improvements across diverse domains, including Atari 2600 games, MuJoCo physics tasks, and the TORCS car racing simulator. The results highlight FiGAR's ability to learn temporal abstractions, improve policy efficiency, and enhance gameplay performance.
Decision: Accept
Key reasons for acceptance:
1. Novelty and Generality: FiGAR introduces a well-motivated and generalizable framework for temporal abstraction, addressing a critical gap in DRL literature.
2. Empirical Rigor: The paper provides extensive experimental results across multiple domains, demonstrating consistent performance improvements and validating the claims.
Supporting Arguments
1. Problem Significance: The paper tackles the important problem of dynamic temporal abstraction in DRL, which has been underexplored in prior work. By enabling agents to decide both actions and their durations, FiGAR addresses limitations of static action repetition in existing algorithms.
2. Positioning in Literature: The authors provide a comprehensive review of related work, situating FiGAR as a lightweight and scalable alternative to existing methods like STRAW and Dynamic Frameskip DQN. The factored policy representation avoids the action-space explosion seen in other approaches, making FiGAR computationally efficient.
3. Empirical Validation: The experiments are thorough and well-designed, covering diverse tasks and DRL algorithms. The results demonstrate substantial improvements in performance (e.g., 900× improvement in Enduro and 35× in Atlantis) and validate the utility of temporal abstractions. The paper also explores the impact of different action-repetition sets, showcasing FiGAR's flexibility.
Suggestions for Improvement
1. Clarity of Presentation: While the paper is detailed, the explanation of the FiGAR framework (Section 4) could be streamlined for better readability. A flowchart or diagram illustrating the interaction between the action and action-repetition policies would be helpful.
2. Analysis of Failure Cases: The paper briefly mentions that FiGAR does not always outperform baselines (e.g., Demon Attack). A deeper analysis of such cases could provide insights into the limitations of the framework.
3. Stochastic Environments: The authors acknowledge that FiGAR's inability to interrupt macro-actions limits its applicability in stochastic environments. Exploring preliminary solutions, such as stop actions, would strengthen the paper.
4. Ablation Studies: While the importance of the action-repetition policy (πθx) is discussed, additional ablation studies isolating its contribution across different domains would provide further insights.
Questions for the Authors
1. How does the choice of the action-repetition set \( W \) impact the computational efficiency of FiGAR? Are there trade-offs between larger \( W \) values and training time?
2. Could FiGAR be extended to handle stochastic environments where interrupting macro-actions is critical? If so, how might this be implemented?
3. The results suggest that FiGAR performs better in deterministic environments. Have you considered testing FiGAR in partially observable or highly stochastic domains like Minecraft or StarCraft?
Overall, the paper makes a significant contribution to the field of DRL by introducing a novel and effective framework for temporal abstraction. While there are areas for improvement, the strengths of the work outweigh its limitations, and I recommend acceptance.