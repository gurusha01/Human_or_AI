The paper addresses the problem of improving code suggestion systems for dynamically-typed programming languages, specifically Python, which are less supported in modern IDEs compared to statically-typed languages. It proposes a novel neural language model incorporating a sparse pointer network to capture long-range dependencies in code. The authors release a large-scale corpus of 41M lines of Python code and demonstrate that their model significantly improves code suggestion accuracy, particularly for identifiers, compared to baseline LSTM models and n-gram approaches.
Decision: Accept  
The paper is well-motivated, presents a clear and significant contribution to the field, and provides rigorous experimental evidence to support its claims. The introduction of a sparse pointer network to address long-range dependencies is a meaningful advancement, and the release of a large Python corpus is a valuable resource for the community. The results demonstrate substantial improvements in perplexity and accuracy, particularly for identifier prediction, which is a critical aspect of code suggestion.
Supporting Arguments  
1. Problem Relevance and Novelty: The paper tackles a well-defined and important problem in the domain of dynamic programming languages, where code suggestion systems lag behind their counterparts for statically-typed languages. The use of sparse pointer networks to address long-range dependencies is novel and well-justified in the context of programming languages.  
2. Experimental Rigor: The experiments are thorough, comparing the proposed model against strong baselines (n-gram models, LSTMs with and without attention). The results are compelling, with a 5% improvement in accuracy and a 13x improvement in identifier prediction accuracy, which is critical for practical applications.  
3. Broader Impact: The release of the 41M-line Python corpus is a significant contribution that will benefit future research in this area. The paper also discusses potential extensions, such as scaling to entire projects and integrating with IDEs, which highlights its practical relevance.
Suggestions for Improvement  
1. Clarity on Corpus Quality: While the authors use GitHub stars and forks as proxies for quality, further details on how this heuristic impacts the dataset's representativeness would strengthen the paper. For example, are there biases toward certain domains or coding styles?  
2. Evaluation Metrics: While perplexity and accuracy are standard, additional metrics such as runtime efficiency or memory usage of the sparse pointer network could provide a more comprehensive evaluation, especially for real-world IDE integration.  
3. Qualitative Examples: The qualitative analysis is insightful, but more detailed examples of failure cases or limitations of the model would provide a balanced perspective and guide future work.  
4. Comparison with PCFGs: The related work mentions probabilistic context-free grammars (PCFGs) but does not include a direct comparison. Adding such an evaluation could strengthen the claim that neural models are superior for this task.
Questions for the Authors  
1. How does the model handle cases where identifiers are introduced in external libraries or modules rather than within the same file?  
2. What are the limitations of the sparse pointer network in terms of scalability to larger projects or cross-file dependencies?  
3. Could the model be extended to support other dynamic languages, such as JavaScript, and how transferable are the findings to such contexts?  
4. How sensitive is the model's performance to the size and quality of the training corpus?  
Overall, the paper makes a strong contribution to the field of code suggestion and is well-suited for acceptance. Addressing the suggested improvements would further enhance its impact and clarity.