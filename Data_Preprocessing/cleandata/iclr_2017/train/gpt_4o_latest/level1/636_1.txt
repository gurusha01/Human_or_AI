Review
Summary of Contributions
This paper addresses the issue of variance introduced by dropout in neural networks, proposing two key contributions: (1) a novel weight initialization technique that adjusts for dropout rates and the compressiveness of nonlinearities, and (2) a method to correct Batch Normalization (BN) variance estimates after training by re-estimating them with dropout turned off. The authors demonstrate that their weight initialization stabilizes variance during both feedforward and backpropagation, leading to faster and more consistent convergence. Additionally, the BN variance correction improves test-time performance, achieving state-of-the-art results on CIFAR-10 and CIFAR-100 without data augmentation. The proposed methods are computationally efficient, requiring no batch statistics or special forward passes, and are validated through extensive experiments on multiple datasets and architectures.
Decision: Accept
The paper makes a significant and well-supported contribution to the field of neural network training. The proposed methods address a practical and underexplored problem, are theoretically sound, and demonstrate strong empirical results. The simplicity and generality of the techniques make them highly applicable to a wide range of tasks, further strengthening the case for acceptance.
Supporting Arguments
1. Clear Problem Statement and Motivation: The paper identifies a specific and important problem—variance instability caused by dropout—and motivates the need for corrective techniques. The discussion is well-grounded in prior literature, connecting the proposed methods to existing weight initialization and BN techniques.
   
2. Theoretical Rigor: The derivation of the weight initialization method is mathematically sound, with clear explanations of how dropout and nonlinearities affect variance. The authors also provide empirical evidence to validate their theoretical claims, such as experiments showing consistent variance across layers during feedforward and backpropagation.
3. Strong Empirical Results: The proposed methods yield significant performance improvements, including state-of-the-art results on CIFAR-10 and CIFAR-100 without data augmentation. The experiments are thorough, comparing the new techniques against established baselines (e.g., He and Xavier initializations) across multiple architectures and optimizers.
4. Practical Impact: The methods are computationally efficient and easy to implement, making them accessible to practitioners. The BN variance correction, in particular, is a simple post-training adjustment that can be applied to existing models.
Suggestions for Improvement
1. Clarity in Presentation: While the paper is thorough, some derivations (e.g., the backpropagation variance correction) could benefit from additional explanation or visual aids to improve accessibility for a broader audience.
   
2. Broader Evaluation: The experiments focus heavily on CIFAR datasets. While these are standard benchmarks, it would strengthen the paper to include results on more diverse datasets or tasks (e.g., ImageNet, NLP tasks) to demonstrate generalizability.
3. Ablation Studies: The paper could include more detailed ablation studies to isolate the contributions of the weight initialization and BN variance correction separately. For example, how much of the performance gain is attributable to each method?
4. Impact on Small Batch Sizes: The authors briefly mention that BN struggles with small batch sizes. It would be valuable to explore whether the proposed BN variance correction mitigates this issue.
Questions for the Authors
1. How sensitive are the proposed methods to hyperparameter choices, such as the dropout rate or the corrective scalars used in weight initialization?
2. Have you tested the BN variance correction on tasks with small batch sizes or online learning settings? If so, how does it perform compared to standard BN?
3. Could the proposed weight initialization be combined with other regularization techniques (e.g., stochastic depth) to further improve performance?
Overall, this paper is a strong contribution to the field and is recommended for acceptance. The proposed methods are theoretically sound, empirically validated, and practically impactful, and they address an important gap in the literature.