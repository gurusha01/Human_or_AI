The paper proposes SampleRNN, a novel hierarchical model for unconditional audio generation that generates raw audio waveforms one sample at a time. The model combines memory-less modules (autoregressive multilayer perceptrons) and stateful recurrent neural networks (RNNs) operating at different temporal resolutions. This hierarchical structure enables the model to capture long-term dependencies in audio while maintaining computational efficiency. The authors demonstrate the model's effectiveness on three datasets (speech, music, and human vocal sounds) and show that it outperforms competing models like WaveNet in both quantitative metrics and human evaluations. Additionally, the paper provides an in-depth analysis of the contributions of individual model components to its performance.
Decision: Accept
The key reasons for this decision are:  
1. Novelty and Contribution: The hierarchical structure of SampleRNN, with modules operating at different temporal resolutions, is a significant innovation in the field of audio generation. The paper makes a strong case for its advantages over existing models, such as WaveNet.  
2. Empirical Validation: The results are robust, with both quantitative (negative log-likelihood) and qualitative (human preference tests) evaluations supporting the claims. The model consistently outperforms baselines across diverse datasets.
Supporting Arguments:  
1. The paper is well-motivated and grounded in the literature. It builds on prior work, such as WaveNet, while addressing its limitations (e.g., fixed receptive fields and lack of hierarchical modeling). The authors provide a clear rationale for their design choices, such as using RNNs for long-term dependencies and MLPs for short-term correlations.  
2. The experimental setup is rigorous, with extensive comparisons across datasets and ablation studies to evaluate the impact of individual components. The human evaluation results further strengthen the claims of superior sample quality.  
3. The paper includes practical insights, such as the benefits of quantizing audio signals and the use of truncated backpropagation through time for efficient training.
Suggestions for Improvement:  
1. Clarity in Model Description: While the hierarchical structure is well-explained, the mathematical notations in Section 2 could be simplified for better readability. For instance, equations could be accompanied by more intuitive diagrams or pseudocode.  
2. Comparison with WaveNet: The authors acknowledge that their WaveNet implementation may differ from the original. It would be helpful to clarify these differences and discuss their potential impact on the results.  
3. Generalization to Other Domains: While the focus is on audio generation, the authors briefly mention potential applications to other high-resolution sequential data. Expanding on this could broaden the paper's appeal.  
4. Ablation Study on Frame Sizes: The choice of frame sizes (e.g., FS(1) = 2, FS(3) = 8) is critical to the model's performance. A more detailed exploration of how these parameters affect results would be valuable.
Questions for the Authors:  
1. How sensitive is the model's performance to the choice of hyperparameters, such as frame sizes or the number of GRU layers?  
2. Could the hierarchical structure of SampleRNN be extended to conditional audio generation tasks, such as text-to-speech synthesis?  
3. How does the model handle highly unbalanced datasets like Onomatopoeia, and what strategies were used to mitigate potential biases?  
4. What are the computational trade-offs between SampleRNN and WaveNet in terms of training and inference time?
In conclusion, the paper makes a significant contribution to the field of audio generation and provides a strong foundation for future work. With minor clarifications and additional analysis, it could have an even greater impact.