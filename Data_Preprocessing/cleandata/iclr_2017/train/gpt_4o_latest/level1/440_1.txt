Review of the Paper
Summary of Contributions
This paper introduces a novel algorithm for policy search in stochastic dynamical systems using model-based reinforcement learning (RL). The key innovation lies in employing Bayesian Neural Networks (BNNs) with stochastic input variables to model complex transition dynamics, such as multi-modality and heteroskedasticity, which are often missed by traditional approaches like Gaussian Processes (GPs). The authors propose training BNNs using α-divergence minimization with α = 0.5, which reportedly outperforms variational Bayes (VB) in terms of accuracy and robustness. The trained BNNs are then integrated into a policy search algorithm that uses random roll-outs and stochastic optimization to find near-optimal policies. The method is validated on challenging benchmarks, including the Wet-Chicken problem, gas turbine control, and an industrial benchmark, demonstrating superior performance over competing methods. The paper makes a strong case for the applicability of this approach in real-world industrial settings where safety and exploration constraints are critical.
Decision: Accept
The paper should be accepted due to its significant contributions to model-based RL, particularly in addressing complex stochastic dynamics with BNNs and demonstrating superior empirical performance across diverse benchmarks. The method is well-motivated, scientifically rigorous, and has clear practical relevance.
Supporting Arguments
1. Clear Problem Definition and Motivation: The paper addresses a well-defined problem—policy search in stochastic systems with complex dynamics—and motivates the use of BNNs with stochastic inputs as a scalable and expressive solution. The authors highlight the limitations of existing approaches, such as GPs and VB, and position their method as a robust alternative.
   
2. Scientific Rigor and Empirical Validation: The proposed method is grounded in sound theoretical principles, particularly the use of α-divergence minimization for training BNNs. The experiments are comprehensive, covering both synthetic and real-world scenarios. The results convincingly demonstrate the superiority of the proposed approach in terms of predictive accuracy, robustness, and policy performance.
3. Practical Relevance: The focus on industrial applications, such as gas turbine control and the industrial benchmark, underscores the practical utility of the method. The ability to handle off-policy batch RL scenarios with safety constraints is particularly noteworthy.
Suggestions for Improvement
1. Clarity in Presentation: While the paper is technically sound, the presentation of certain sections, particularly the derivation of equations and the description of α-divergence minimization, could be streamlined for better readability. Including a high-level summary of the algorithm before delving into technical details would help readers unfamiliar with the topic.
2. Comparison with Model-Free Methods: The paper focuses on comparing the proposed model-based approach with other model-based methods. Including a discussion or experimental comparison with state-of-the-art model-free RL methods would provide a more comprehensive evaluation.
3. Scalability Analysis: While the paper discusses computational complexity, it would benefit from a more detailed analysis of how the method scales with increasing state-action space dimensionality and longer horizons.
4. Exploration and Safety: The authors mention future work on exploration and safety but do not provide preliminary results or insights. A brief discussion on how uncertainty estimates from BNNs could be leveraged for safe exploration would strengthen the paper.
Questions for the Authors
1. How does the performance of the proposed method compare with state-of-the-art model-free RL methods, particularly in terms of sample efficiency and policy quality?
2. Can the authors provide more insights into the choice of α = 0.5 for α-divergence minimization? How sensitive is the method to this hyperparameter?
3. How does the method handle high-dimensional state-action spaces, and are there any limitations in terms of scalability?
4. Could the authors elaborate on the robustness of the method when the training data is sparse or noisy, particularly in industrial applications?
Overall, this paper makes a significant contribution to the field of model-based RL and provides a promising approach for tackling complex stochastic systems. With minor improvements in presentation and additional comparisons, it could have an even greater impact.