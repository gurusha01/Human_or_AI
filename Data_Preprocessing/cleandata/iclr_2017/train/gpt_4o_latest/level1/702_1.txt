The paper proposes two novel Recurrent Neural Network (RNN)-based architectures, Classifier and Selector, for extractive document summarization. The Classifier processes sentences in their original order, deciding their inclusion in the summary, while the Selector picks sentences in an arbitrary order to optimize summary quality. Both architectures explicitly model salience, redundancy, and other abstract features, making them interpretable. The authors demonstrate state-of-the-art performance on the Daily Mail dataset and competitive results on the DUC 2002 dataset. They also provide insights into when each architecture is more effective, such as Selector's advantage in less-structured data scenarios. The paper emphasizes interpretability through feature visualization and highlights the potential for further improvements using beam search and domain adaptation.
Decision: Accept
Key Reasons:
1. Novelty and Contribution: The paper introduces two distinct architectures for extractive summarization, providing theoretical and empirical comparisons. The Selector framework, in particular, is a novel contribution to deep learning-based summarization.
2. Scientific Rigor: The experiments are thorough, with clear baselines, ablation studies, and evaluations on both in-domain and out-of-domain datasets. The interpretability of the models is a significant strength.
Supporting Arguments:
- The problem of extractive summarization is well-motivated, and the authors position their work effectively within the existing literature. They compare their models to state-of-the-art methods and highlight differences in architecture and training strategies.
- The results on the Daily Mail dataset demonstrate that the proposed deep Classifier model outperforms previous methods, while the Selector model shows promise in less-structured scenarios, as evidenced by experiments on shuffled data.
- The interpretability of the models, achieved through explicit feature modeling, adds practical value for end-users and researchers.
Suggestions for Improvement:
1. Ground Truth Generation: The authors use a greedy approach to generate extractive labels from abstractive summaries. Exploring more sophisticated methods, such as ILP-based optimization, could improve training data quality and model performance.
2. Domain Adaptation: The models perform worse on the DUC 2002 dataset compared to graph-based methods. Investigating techniques for domain adaptation, such as fine-tuning on smaller out-of-domain datasets, could enhance generalizability.
3. Selector Architecture: While the Selector framework is novel, its performance lags behind the Classifier in structured datasets. Further exploration of its potential in multi-document summarization or tweet summarization, as suggested, would strengthen its utility.
4. Inference Optimization: Incorporating beam search during inference could further improve the Selector model's performance and align it with its theoretical strengths.
Questions for the Authors:
1. How does the performance of the Selector model change with beam search during inference? Would this narrow the gap with the Classifier model?
2. Could the authors elaborate on the computational trade-offs between the Classifier and Selector architectures, particularly in large-scale applications?
3. Have the authors considered using pre-trained language models (e.g., BERT) for sentence representations to enhance performance?
Overall, this paper makes a strong contribution to the field of extractive summarization, and the proposed architectures offer valuable insights and practical tools for future research.