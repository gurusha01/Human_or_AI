Review
Summary of Contributions
This paper introduces a novel approach to neural network compression using a modernized version of "soft weight-sharing," originally proposed by Nowlan & Hinton (1992). The authors demonstrate that their method achieves competitive compression rates while combining pruning and quantization into a single retraining process. The paper also establishes a connection between compression and the Minimum Description Length (MDL) principle, providing a theoretical foundation for the approach. The method is evaluated on several neural network architectures, such as LeNet-300-100, LeNet-5-Caffe, and ResNet, achieving state-of-the-art compression rates without significant accuracy loss. Additionally, the authors propose hyper-prior tuning using Bayesian optimization to further improve compression results.
Decision: Accept
The paper is well-motivated, scientifically rigorous, and provides both theoretical and empirical evidence to support its claims. The key reasons for acceptance are:
1. Novelty and Simplicity: The revival and adaptation of soft weight-sharing for compression is a novel contribution, offering a simpler alternative to multi-step compression pipelines like Han et al. (2015a).
2. Strong Results: The method achieves state-of-the-art compression rates on benchmark models with minimal accuracy degradation, demonstrating its practical utility.
Supporting Arguments
1. Well-Placed in Literature: The paper situates its contributions effectively within the context of prior work on compression, pruning, quantization, and Bayesian inference. The connection to MDL principles adds a strong theoretical underpinning.
2. Scientific Rigor: The experiments are thorough, covering multiple architectures and including comparisons to prior methods. The use of hyper-priors and Bayesian optimization to tune parameters is a thoughtful addition that addresses potential pitfalls of the approach.
3. Practical Relevance: By combining pruning and quantization into a single retraining step, the method simplifies the compression process, making it more accessible for practitioners.
Suggestions for Improvement
1. Scalability: The authors acknowledge that the method struggles with very large networks like VGG due to computational cost. While a potential solution is outlined in Appendix C, experimental results on larger networks would strengthen the paper.
2. Hyper-Parameter Tuning: The paper highlights challenges in optimizing the 13-dimensional hyper-parameter space. Providing more insights into the optimization process or reducing the number of tunable parameters could make the method more practical.
3. Implementation Details: While the method is theoretically sound, its implementation appears complex. A more detailed explanation of the initialization and training process, particularly for practitioners, would be beneficial.
4. Broader Evaluation: The experiments focus on MNIST and CIFAR datasets. Extending the evaluation to larger-scale datasets like ImageNet would demonstrate the method's robustness and scalability.
Questions for the Authors
1. How does the method perform when applied to larger networks like VGG or transformers? Are there any plans to address the computational bottlenecks mentioned in Appendix C?
2. Could the authors provide more details on the sensitivity of the method to the choice of hyper-priors? For instance, how does the performance vary with different Gamma or Beta prior configurations?
3. Is the method robust to different initialization schemes for the mixture model components? Have alternative initialization strategies been explored?
Conclusion
This paper makes a significant contribution to the field of neural network compression by reviving and adapting a principled approach from the 1990s. While there are areas for improvement, the novelty, rigor, and practical relevance of the proposed method justify its acceptance. Addressing the scalability and implementation challenges in future work could further enhance its impact.