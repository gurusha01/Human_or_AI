The paper introduces the Doubly Orthogonal Recurrent Neural Network (DORNN), a novel RNN architecture designed to address the vanishing and exploding gradient problems that hinder the training of traditional RNNs on long sequences. The authors propose a fully multiplicative recurrent transition architecture that preserves both forward hidden state activation norms and backpropagated gradient norms, ensuring stability over long time horizons. The architecture employs a rotation plane parameterization of orthogonal matrices, which allows for efficient computation and expressivity. The authors validate their approach on a simplified memory copy task and demonstrate the ability to learn dependencies up to 5,000 timesteps, outperforming prior methods.
Decision: Accept
Key reasons for acceptance are: (1) The paper addresses a fundamental challenge in sequence modeling with a theoretically grounded and novel approach. (2) The results demonstrate significant improvements in handling extremely long-term dependencies, pushing the boundaries of what current RNN architectures can achieve.
Supporting Arguments
1. Problem Tackled: The paper effectively addresses the vanishing and exploding gradient problem in RNNs, a long-standing issue in deep learning. By proposing a fully orthogonal and input-modulated architecture, the authors provide a theoretically sound solution that is both innovative and practical.
   
2. Motivation and Literature Placement: The paper is well-motivated and builds on a rich body of prior work, including unitary RNNs and orthogonal matrix representations. The authors clearly position their contribution within the context of existing methods, such as uRNNs and LSTMs, and highlight the limitations of these approaches in handling extremely long dependencies.
3. Scientific Rigor: The theoretical claims about norm preservation are rigorously proven, and the experimental results support these claims. The ability of the proposed architecture to scale to 5,000 timesteps is a significant achievement, and the experiments are well-designed to isolate the impact of the proposed method.
Suggestions for Improvement
1. Broader Validation: While the memory copy task is a standard benchmark, it is highly simplified. The paper would benefit from additional experiments on more complex, real-world sequence modeling tasks (e.g., language modeling, speech recognition) to demonstrate the practical applicability of the architecture.
   
2. Training Stability: The authors note instability during training for longer sequences. Providing a more detailed analysis of this phenomenon and potential mitigation strategies would strengthen the paper.
   
3. Comparison to Baselines: While the paper compares against prior work conceptually, empirical comparisons to other state-of-the-art architectures like LSTMs, GRUs, and uRNNs on the same task would provide a clearer picture of the performance gains.
4. Parameter Efficiency: The paper does not discuss the parameter efficiency of the proposed architecture compared to baselines. Including this analysis would help assess the scalability of the method.
Questions for the Authors
1. How does the architecture perform on tasks with more complex input-output relationships, such as natural language processing or time-series forecasting?
2. Can the proposed method be combined with nonlinear transitions (e.g., LSTM-style gates) to improve expressivity while maintaining stability?
3. How sensitive is the performance to the random initialization of rotation planes? Would optimizing these planes improve results further?
In conclusion, this paper makes a strong theoretical and empirical contribution to the field of sequence modeling. Addressing the suggested improvements would further enhance its impact and applicability.