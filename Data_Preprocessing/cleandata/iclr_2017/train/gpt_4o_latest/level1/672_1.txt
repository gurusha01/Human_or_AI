Review of "Joint Multimodal Variational Autoencoder (JMVAE)"
Summary of Contributions
This paper introduces the Joint Multimodal Variational Autoencoder (JMVAE), a novel approach to multimodal learning that models a joint distribution of multiple modalities, enabling bi-directional generation (e.g., generating images from text and vice versa). The authors propose an additional method, JMVAE-kl, to address the challenge of missing modalities during testing by reducing the divergence between the multimodal encoder and modality-specific encoders. The paper demonstrates the effectiveness of JMVAE and JMVAE-kl through both qualitative and quantitative experiments on MNIST and CelebA datasets, showing improvements over existing methods like VAEs, CVAEs, and CMMAs in terms of log-likelihood and bi-directional generation capabilities. The authors also highlight the ability of JMVAE to handle modalities with vastly different dimensions and structures, such as high-dimensional images and low-dimensional binary attributes.
Decision: Accept
The paper makes a significant contribution to the field of multimodal learning by addressing a key limitation of existing methods—unidirectional generation—and introducing a robust solution for handling missing modalities. The proposed JMVAE and JMVAE-kl are well-motivated, scientifically rigorous, and empirically validated. The experimental results convincingly demonstrate the superiority of the proposed methods over baseline approaches. However, there are areas where the paper could be improved, particularly in clarity and additional ablation studies.
Supporting Arguments
1. Clear Problem Statement and Novelty: The paper identifies a critical limitation of existing multimodal VAEs (e.g., CVAEs) that enforce unidirectional generation and proposes JMVAE as a solution. The novelty lies in modeling a joint distribution of all modalities and enabling bi-directional generation, which is a meaningful advancement in the field.
   
2. Scientific Rigor: The authors provide a thorough theoretical foundation for JMVAE and JMVAE-kl, including derivations of the objective functions and their relationship to the variation of information. The experiments are well-designed, with comparisons to strong baselines (e.g., CVAEs, CMMAs) and evaluations on both toy (MNIST) and challenging (CelebA) datasets.
3. Empirical Validation: The results demonstrate that JMVAE achieves higher log-likelihoods and better bi-directional generation compared to baselines. The qualitative results, such as generating faces from attributes and vice versa, further validate the model's effectiveness.
Suggestions for Improvement
1. Clarity in Presentation: While the paper is technically sound, some sections, particularly the derivations in Section 3.3 and Appendix A, are dense and could benefit from clearer explanations or visual aids. For instance, a flowchart summarizing the training and inference processes for JMVAE and JMVAE-kl would be helpful.
2. Ablation Studies: The paper briefly discusses the impact of the regularization parameter (α) in JMVAE-kl but does not explore other hyperparameters or architectural choices. A more detailed ablation study would strengthen the claims about the robustness of the proposed method.
3. Broader Evaluation: While MNIST and CelebA are suitable for demonstrating the model's capabilities, additional experiments on more diverse multimodal datasets (e.g., audio-visual data or text-image pairs from real-world tasks) would make the results more generalizable.
4. Computational Efficiency: The paper does not discuss the computational cost of JMVAE and JMVAE-kl compared to baselines. Including training times or memory requirements would provide a more complete picture of the method's practicality.
Questions for the Authors
1. How does the performance of JMVAE scale with the number of modalities? Have you tested the model on datasets with more than two modalities?
2. Can JMVAE handle scenarios where one modality is entirely missing during training? If so, how does it compare to methods like CVAEs in such settings?
3. How sensitive is the model to the choice of α in JMVAE-kl? Would an adaptive approach to tuning α improve performance?
Conclusion
The paper presents a well-motivated and impactful contribution to multimodal learning, addressing key limitations of existing methods. While there are areas for improvement, particularly in clarity and broader evaluation, the strengths of the proposed approach outweigh these concerns. I recommend accepting this paper for publication.