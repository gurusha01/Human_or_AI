Review of the Paper
Summary of Contributions
This paper investigates the vulnerability of deep convolutional neural networks (CNNs) to adversarial attacks in a black-box setting, where the adversary has no knowledge of the network's architecture or parameters. The authors propose two novel attack methods: (1) a simple random perturbation of single or small sets of pixels, and (2) a more sophisticated greedy local-search approach that iteratively selects a small set of pixels to perturb. The paper demonstrates that these methods can generate adversarial examples with minimal perturbations, even for high-resolution datasets, and extend the attacks to k-misclassification scenarios. The authors provide extensive experimental results across multiple datasets and architectures, showing the effectiveness of their methods in generating adversarial examples while perturbing only a small fraction of pixels. The paper also highlights the correlation between perturbed pixels and saliency maps, and discusses the limitations of adversarial training as a defense against these attacks.
Decision: Accept
Key reasons for this decision are:
1. Novelty and Practicality: The proposed black-box attack methods are simple, computationally efficient, and do not rely on assumptions like transferability or access to gradients, making them widely applicable.
2. Scientific Rigor: The experimental results are thorough, spanning multiple datasets, architectures, and scenarios (e.g., k-misclassification). The methods are compared against a strong baseline (fast-gradient sign method), and the results convincingly demonstrate the effectiveness of the proposed approaches.
Supporting Arguments
1. Well-Motivated Problem: The paper addresses a critical and timely problem in the field of adversarial machine learning: the robustness of CNNs to black-box attacks. The focus on practical, gradient-free attacks makes the work highly relevant for real-world applications.
2. Strong Experimental Evidence: The authors evaluate their methods on five datasets (e.g., CIFAR10, ImageNet1000) and multiple architectures (e.g., VGG, Network-in-Network) and provide detailed metrics (e.g., success rates, perturbation size, computation time). The results consistently show that the proposed methods outperform the baseline in terms of success rates and perturbation efficiency.
3. Broader Implications: The discussion on the limitations of adversarial training and potential defenses adds depth to the paper and opens avenues for future research.
Suggestions for Improvement
1. Clarity of Presentation: While the paper is detailed, it is dense and could benefit from better organization. For example, the description of the greedy local-search algorithm could be streamlined to improve readability. Including a flowchart or diagram to illustrate the algorithm would help readers grasp the process more intuitively.
2. Comparison with Other Black-Box Attacks: The paper primarily compares its methods to the fast-gradient sign method, which assumes access to gradients. A comparison with other black-box attacks, such as those relying on transferability (e.g., substitute models), would strengthen the paper's claims.
3. Analysis of Failure Cases: The paper could include a deeper analysis of scenarios where the proposed methods fail or require larger perturbations. This would provide insights into the limitations of the approach and guide future improvements.
4. Defense Strategies: While the paper briefly mentions potential defenses, it would be valuable to include a more detailed discussion or preliminary experiments on countermeasures, such as query-based detection or input preprocessing.
Questions for the Authors
1. How does the performance of the proposed methods vary with different choices of hyperparameters (e.g., perturbation size, neighborhood size in local search)? Could you provide guidelines for selecting these parameters in practice?
2. Have you tested the proposed methods on other types of neural networks (e.g., recurrent or transformer-based models)? If not, do you anticipate similar vulnerabilities?
3. For the k-misclassification experiments, how does the success rate scale with increasing k? Are there diminishing returns as k becomes large?
4. Could the greedy local-search approach be adapted to scenarios where the adversary has limited query budgets? If so, how would this impact the success rates?
Overall, this paper makes a significant contribution to the field of adversarial machine learning and provides practical tools for evaluating the robustness of CNNs. With some refinements, it has the potential to become a foundational work in black-box adversarial attacks.