The paper introduces a novel method for finding dependent subspaces across multiple views of data, focusing on preserving neighborhood relationships rather than maximizing simple coordinate correlations as in traditional Canonical Correlation Analysis (CCA). The authors propose a framework that optimizes cross-view neighborhood similarity, which is directly tied to an information retrieval task. The method is linear, interpretable, and capable of detecting nonlinear and local dependencies. It is experimentally shown to outperform alternatives like CCA and LPCCA in preserving cross-view neighborhood similarities across artificial and real-world datasets.
Decision: Accept
The paper offers a well-motivated and innovative approach to a longstanding problem in multi-view learning. The method is theoretically sound, empirically validated, and provides a significant contribution to the literature by shifting the focus from coordinate correlations to neighborhood relationships. The experimental results convincingly demonstrate the method's superiority over existing techniques.
Supporting Arguments:
1. Clear Problem Definition and Motivation: The paper addresses a well-defined problem—finding dependent subspaces that preserve neighborhood relationships across views. The motivation is strong, as many real-world applications prioritize data relationships over individual feature correlations.
   
2. Novel Contribution: The proposed method introduces a dependency criterion based on cross-view neighborhood similarity, which is both innovative and practical for tasks like neighbor retrieval. This is a meaningful departure from traditional CCA-based approaches.
3. Empirical Rigor: The experiments are diverse, covering artificial data, image data (MNIST and video), stock prices, and biological data (Cell-Cycle). The use of precision-recall curves for evaluation is appropriate for the information retrieval context, and the results consistently show the method's advantages.
4. Theoretical Soundness: The paper provides a detailed mathematical formulation, including invariance properties, optimization techniques, and extensions to different settings (e.g., nonlinear transformations or subspaces with different dimensions).
Suggestions for Improvement:
1. Scalability: While the authors acknowledge the computational complexity of their method (O(N²) for naive implementation), they could provide more discussion or experiments on scalability for larger datasets. Incorporating acceleration techniques mentioned in the related work would strengthen the practical applicability.
2. Comparison with Nonlinear Methods: Although the paper focuses on linear transformations, a brief experimental comparison with nonlinear methods (e.g., kernel CCA) would provide additional context for the method's performance.
3. Interpretability of Results: While the method is interpretable due to its linear nature, the authors could include more detailed examples or visualizations of the learned subspaces, particularly for real-world datasets like MNIST or Cell-Cycle.
4. Hyperparameter Sensitivity: The paper does not discuss the sensitivity of the method to hyperparameters like the neighborhood size or the penalty parameter γ. A brief analysis would help practitioners better understand how to tune the method.
Questions for the Authors:
1. How sensitive is the method to the choice of neighborhood size (k) or the falloff parameter (σ)? Does this choice significantly impact performance across datasets?
2. Could the method be extended to handle missing data or unpaired views? If so, how would the optimization framework need to be adapted?
3. Have you considered integrating nonlinear transformations (e.g., neural networks) into the framework, and if so, what challenges do you anticipate in terms of optimization?
In conclusion, the paper makes a valuable contribution to multi-view learning by introducing a novel, well-motivated approach to finding dependent subspaces. While there are areas for improvement, the strengths of the method and its empirical results justify acceptance.