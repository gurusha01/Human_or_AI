Review of the Paper
Summary of Contributions
This paper provides a compelling empirical investigation into the necessity of both depth and convolutional layers in deep learning models, particularly for image classification tasks like CIFAR-10. The authors revisit and extend prior work on model distillation, demonstrating that shallow models, even when trained to mimic deep teacher models via distillation, fail to achieve comparable accuracy unless they include multiple convolutional layers. The paper's key contribution is its rigorous exploration of the architectural and parameter constraints under which shallow models can approach the performance of deep convolutional networks. By employing Bayesian hyperparameter optimization, the authors ensure that their results are not confounded by suboptimal training. The findings strongly support the claim that depth and convolution are critical for high accuracy, even when leveraging advanced training techniques like distillation.
Decision: Accept
The paper should be accepted because it addresses a significant and ongoing question in deep learningâ€”whether shallow models can replicate the performance of deep convolutional networks under similar parameter budgets. The authors provide robust empirical evidence, leveraging state-of-the-art techniques, and their results are both scientifically rigorous and practically relevant. The paper also sets a high benchmark for future research in model compression and distillation.
Supporting Arguments
1. Well-Defined Problem and Motivation: The paper tackles a clearly defined problem: determining whether shallow models can match the accuracy of deep convolutional networks when trained via distillation. The motivation is well-placed in the literature, building on prior work by Ba and Caruana (2014) and Hinton et al. (2015), and addressing gaps in their findings.
   
2. Methodological Rigor: The authors employ a thorough experimental setup, including Bayesian hyperparameter optimization, data augmentation, and a carefully designed teacher-student framework. The use of a highly accurate ensemble of deep convolutional networks as the teacher model strengthens the validity of their conclusions.
3. Empirical Results: The results are presented clearly and convincingly. The paper demonstrates that shallow models without convolutional layers perform poorly, and that adding convolutional layers significantly improves accuracy. The findings are consistent across different parameter budgets, reinforcing the claim that depth and convolution are indispensable for high performance.
Suggestions for Improvement
1. Clarity in Presentation: While the paper is thorough, it is dense and could benefit from a more concise presentation of key results. For example, summarizing the main findings in a table or figure in the introduction would help readers quickly grasp the paper's contributions.
2. Theoretical Insights: The paper is heavily empirical, and while this is a strength, it would be valuable to include more theoretical discussion on why convolutional layers and depth are critical. For instance, linking the findings to representational efficiency or generalization theory could add depth to the discussion.
3. Additional Benchmarks: While the focus on CIFAR-10 is appropriate, including preliminary results on a more challenging dataset like ImageNet (as briefly mentioned) would strengthen the paper's generalizability.
Questions for the Authors
1. Can the authors provide more insights into why dropout consistently reduced the accuracy of student models? Is this effect specific to the distillation process, or might it generalize to other training paradigms?
   
2. The paper mentions ongoing experiments with larger models (100M and 300M parameters). Can the authors share preliminary findings from these experiments to further validate their conclusions?
3. Could the authors elaborate on the computational cost of their approach, particularly the Bayesian optimization process? How feasible is this methodology for researchers with limited computational resources?
Conclusion
This paper makes a significant contribution to the understanding of model compression and the role of depth and convolution in neural networks. Its rigorous methodology and clear empirical results make it a valuable addition to the field, and I recommend its acceptance with minor revisions to improve clarity and theoretical depth.