Review
Summary of Contributions
The paper introduces a novel approach to parameterizing "Passthrough Networks" (e.g., LSTMs, GRUs, Highway Networks, and Deep Residual Networks) using low-rank and low-rank plus diagonal matrix decompositions. The authors argue that these parametrizations exploit the decoupling of network state size from the number of parameters, thereby reducing memory requirements and data complexity while preserving memory capacity. The paper provides theoretical justification for the proposed parametrizations and demonstrates their effectiveness through empirical evaluations on a variety of tasks, including memory tasks, sequential MNIST classification, and language modeling. Notably, the approach achieves competitive results, including a near state-of-the-art performance on the challenging sequential randomly-permuted MNIST task.
Decision: Accept
Key reasons for acceptance:
1. Novelty and Contribution: The paper proposes a well-motivated and novel parameterization technique that addresses a critical problem in deep learning—balancing model complexity and memory efficiency—while maintaining competitive performance.
2. Empirical Rigor: The experimental results are thorough and demonstrate the utility of the proposed approach across multiple benchmarks. The inclusion of comparisons with state-of-the-art models (e.g., uRNN) strengthens the claims.
Supporting Arguments
1. Problem Definition and Motivation: The paper clearly identifies the challenges of training deep networks with large parameter matrices and positions its approach as a principled solution. The discussion of the decoupling property of Passthrough Networks is insightful and well-grounded in prior literature.
2. Experimental Validation: The experiments are comprehensive, covering a range of tasks with varying levels of difficulty. The results consistently show that the proposed low-rank and low-rank plus diagonal parametrizations achieve competitive performance, even outperforming state-of-the-art models in some cases (e.g., the addition task). The ablation studies (e.g., comparing low-rank vs. low-rank plus diagonal) further validate the effectiveness of the approach.
3. Theoretical Soundness: The paper provides a solid theoretical foundation for the proposed parametrizations, including their universality and ability to retain the representation power of parent architectures. This adds credibility to the approach.
Suggestions for Improvement
1. Clarity of Presentation: While the paper is comprehensive, the mathematical exposition can be dense and difficult to follow in some sections (e.g., the derivation of low-rank and low-rank plus diagonal parametrizations). Simplifying or summarizing key equations could improve readability.
2. Comparison with Other Parameter-Efficient Architectures: The paper briefly mentions convolutional parametrizations but does not provide direct experimental comparisons. Including such comparisons would strengthen the argument for the proposed approach.
3. Training Stability: The authors note numerical stability issues in some tasks (e.g., memory tasks) and propose solutions like weight normalization and max-norm constraints. However, further discussion on the generalizability of these solutions across tasks would be helpful.
4. Broader Applicability: While the proposed parametrizations are shown to work well for specific tasks, it would be interesting to explore their applicability to more complex domains, such as large-scale vision or natural language processing tasks.
Questions for the Authors
1. How does the proposed approach scale to very large datasets and models, particularly in domains like computer vision or large-scale language modeling?
2. Can the low-rank plus diagonal parametrization be combined with other architectural innovations (e.g., attention mechanisms or convolutional layers) to further enhance performance?
3. What are the computational trade-offs (e.g., training time, convergence speed) of using low-rank parametrizations compared to full-rank models?
In summary, the paper makes a significant contribution to the field by proposing a novel and effective parameterization technique for Passthrough Networks. While there are areas for improvement, the strengths of the paper outweigh its weaknesses, and I recommend it for acceptance.