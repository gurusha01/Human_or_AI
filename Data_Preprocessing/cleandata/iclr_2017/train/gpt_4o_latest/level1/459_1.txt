Review
Summary of Contributions
The paper presents a novel framework for deep multi-task representation learning (DMTRL) by generalizing matrix factorization techniques to tensor factorization. Unlike existing multi-task learning (MTL) approaches that require manually defined sharing structures, this method learns cross-task sharing structures at every layer of a deep neural network (DNN) in an end-to-end manner. The proposed approach applies to both homogeneous and heterogeneous MTL settings and reduces the design complexity of DNN architectures. The authors introduce three variants of their method—DMTRL-LAF, DMTRL-Tucker, and DMTRL-TT—each leveraging different tensor factorization techniques. The paper demonstrates the efficacy of the proposed methods through extensive experiments on MNIST, AdienceFaces, and Omniglot datasets, showing superior performance compared to single-task learning (STL) and user-defined MTL (UD-MTL) baselines. The framework also provides a data-driven mechanism to interpolate between fully shared and fully task-specific layers, addressing the limitations of hard-sharing strategies.
Decision: Accept
Key reasons for acceptance:
1. Novelty and Significance: The paper introduces a novel tensor factorization-based approach to deep MTL, which is a meaningful advancement over existing methods that rely on manual sharing strategies.
2. Empirical Rigor: The experimental results convincingly demonstrate the superiority of the proposed methods across diverse datasets and problem settings, including homogeneous and heterogeneous MTL.
3. Practical Impact: By automating the sharing structure, the framework significantly reduces the trial-and-error process in DNN architecture design, making it highly relevant for practitioners.
Supporting Arguments
1. Problem Definition and Motivation: The paper clearly identifies the limitations of existing MTL approaches, particularly the reliance on manually defined sharing structures, and motivates the need for a data-driven solution. The use of tensor factorization is well-justified as a natural extension of matrix-based MTL methods.
2. Methodological Soundness: The proposed framework is grounded in established tensor factorization techniques (e.g., Tucker and Tensor Train decompositions) and integrates seamlessly with DNN training via backpropagation. The authors provide sufficient theoretical background and implementation details to support reproducibility.
3. Experimental Validation: The experiments are comprehensive, covering both small-scale (MNIST) and large-scale (AdienceFaces, Omniglot) datasets. The results consistently show that DMTRL outperforms STL and UD-MTL, particularly in low-data regimes where MTL is most beneficial. The analysis of learned sharing structures further highlights the interpretability of the approach.
Suggestions for Improvement
1. Clarity of Presentation: The paper is dense with technical details, which may overwhelm readers unfamiliar with tensor factorization. Simplifying the exposition of the methodology and providing more visual aids (e.g., diagrams of tensor factorization) would improve accessibility.
2. Ablation Studies: While the experiments are thorough, an ablation study isolating the contributions of Tucker, Tensor Train, and LAF factorization methods would provide deeper insights into their relative strengths and weaknesses.
3. Scalability Analysis: The paper could include a discussion on the computational overhead introduced by tensor factorization, particularly for large-scale tasks. Reporting training times and memory requirements would help assess the practical feasibility of the approach.
4. Broader Applications: While the paper focuses on vision tasks, it would be valuable to explore the applicability of DMTRL to other domains, such as natural language processing or reinforcement learning.
Questions for the Authors
1. How sensitive is the performance of DMTRL to the choice of tensor ranks? While the paper mentions that the framework is not highly sensitive, a more detailed analysis would be helpful.
2. Can the proposed methods handle tasks with vastly different data distributions or modalities (e.g., combining vision and text tasks)?
3. How does the framework scale with the number of tasks? For instance, would the computational cost become prohibitive for hundreds of tasks?
In conclusion, the paper makes a significant contribution to the field of multi-task learning by addressing a critical limitation in existing methods. With minor improvements in clarity and additional analyses, the work has the potential to set a new standard for deep MTL research.