The paper addresses the problem of improving both the size efficiency and inference speed of convolutional neural networks (CNNs) through pruning, a process that traditionally reduces model size but offers limited speed improvements. The authors propose a novel sparse convolution design, a performance model for predicting sparsity sweet spots, and a pruning algorithm called Guided Sparsity Learning (GSL). These contributions are validated on AlexNet and GoogLeNet, demonstrating significant speedups (3.1–7.3×) across various hardware platforms without accuracy loss.
Decision: Accept
The paper is well-motivated, presents a significant contribution to CNN optimization, and supports its claims with rigorous theoretical and empirical evidence. The key reasons for acceptance are:
1. Novelty and Impact: The proposed sparse convolution design and GSL algorithm address a critical gap in CNN pruning by achieving both size reduction and speed improvements, which is a meaningful advancement for practical deployment.
2. Scientific Rigor: The paper provides a robust performance model and extensive experimental results across multiple architectures, demonstrating the generalizability and effectiveness of the method.
Supporting Arguments
1. Problem Significance: The paper tackles a well-recognized limitation in CNN pruning—its inability to significantly accelerate inference despite reducing model size. This is a critical issue for deploying CNNs on resource-constrained devices.
2. Novel Approach: The sparse convolution design supports arbitrary sparsity patterns, overcoming limitations of existing methods that impose structured sparsity. The performance model is insightful, offering actionable guidelines for sparsity levels across layers and hardware platforms.
3. Strong Empirical Evidence: The results are compelling, showing up to 7.3× speedups on Intel Atom processors and 3.1× on Xeon Phi, with no accuracy degradation. The use of AlexNet and GoogLeNet as benchmarks enhances the credibility of the findings.
Suggestions for Improvement
1. Clarity of Presentation: While the technical depth is appreciated, certain sections, such as the performance model derivation, could benefit from clearer explanations or visual aids to improve accessibility for a broader audience.
2. Broader Applicability: The paper focuses on CPUs and Xeon Phi processors. Including results for GPUs, which are widely used for CNN inference, would strengthen the paper's impact and relevance.
3. Comparison with Alternatives: While the paper briefly mentions related work, a more detailed comparison with state-of-the-art methods, such as Winograd or FFT-based approaches, would contextualize the contributions more effectively.
Questions for the Authors
1. How does the proposed sparse convolution design compare in performance with GPU-optimized dense methods, such as cuDNN, especially for high sparsity levels?
2. Can the Guided Sparsity Learning (GSL) algorithm be extended to incorporate other FLOP-reduction techniques, such as tensor factorization or quantization, in a unified framework?
3. What are the potential trade-offs between sparsity levels and training time when using GSL, particularly for larger networks like ResNet?
In conclusion, the paper makes a strong contribution to the field of CNN optimization, addressing a critical bottleneck in inference speed. With minor clarifications and additional experiments, it could become a foundational reference for future work in this area.