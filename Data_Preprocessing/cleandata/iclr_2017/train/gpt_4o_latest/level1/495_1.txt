Review of the Paper
Summary of Contributions
This paper addresses the fundamental question of why deep neural networks outperform shallow networks in function approximation. It provides rigorous theoretical results demonstrating that, for a large class of piecewise smooth functions, deep networks require exponentially fewer neurons than shallow networks to achieve the same approximation error. The authors derive both upper and lower bounds on the network size for univariate and multivariate function approximation, focusing on networks using ReLU and binary step activation functions. The results are extended to compositions, multiplications, and linear combinations of functions. Notably, the paper establishes that deep networks achieve a size complexity of \(O(\text{polylog}(1/\epsilon))\), while shallow networks require \(\Omega(\text{poly}(1/\epsilon))\) neurons for the same error \(\epsilon\). These findings are supported by tight theoretical bounds, making a significant contribution to the literature on the expressive power of neural networks.
Decision: Accept
The paper is recommended for acceptance due to its strong theoretical contributions and rigorous analysis. The key reasons for this decision are:
1. Novelty and Impact: The paper provides a comprehensive theoretical framework that advances our understanding of the depth-efficiency tradeoff in neural networks, a topic of high relevance in the AI community.
2. Scientific Rigor: The results are well-supported by detailed proofs, and the claims are substantiated with tight bounds for both deep and shallow networks.
Supporting Arguments
1. Well-Motivated Problem: The paper is well-situated in the literature, building on prior works like Telgarsky (2016) and Eldan & Shamir (2015) while extending their results to broader classes of functions. The motivation to explore the depth advantage in function approximation is compelling and timely.
2. Theoretical Rigor: The proofs are detailed and mathematically sound, covering both upper and lower bounds. The authors convincingly demonstrate the exponential gap in neuron requirements between shallow and deep networks.
3. Generality of Results: The results apply to a wide range of functions, including piecewise smooth, strongly convex, and compositions of functions. This generality enhances the practical relevance of the findings.
Suggestions for Improvement
1. Clarity of Presentation: While the proofs are thorough, the paper's readability could be improved by summarizing key results in a more accessible manner. For instance, a table comparing the neuron requirements for deep and shallow networks across different function classes would be helpful.
2. Empirical Validation: Although the paper is theoretical, a small empirical study demonstrating the practical implications of the results would strengthen its impact. For example, comparing the performance of shallow and deep networks on specific function classes could provide additional insights.
3. Broader Context: The paper could benefit from a discussion on how these theoretical results translate to real-world applications, such as image or speech recognition, where deep networks excel.
Questions for the Authors
1. How do the results extend to other activation functions beyond ReLU and binary step units? Would similar bounds hold for sigmoid or tanh activations?
2. Can the theoretical results be generalized to architectures like convolutional or recurrent neural networks, which are widely used in practice?
3. Are there specific function classes where shallow networks might still be competitive despite the exponential gap in neuron requirements?
In conclusion, this paper makes a significant theoretical contribution to the understanding of deep versus shallow networks and is a strong candidate for acceptance. With minor improvements in presentation and additional context, it could have an even broader impact.