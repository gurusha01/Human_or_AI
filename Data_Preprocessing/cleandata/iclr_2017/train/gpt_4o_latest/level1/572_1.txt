Review of the Paper: "Adversarial Examples on Deep Generative Models"
Summary of Contributions
This paper addresses the novel and underexplored problem of generating adversarial examples for deep generative models, specifically Variational Autoencoders (VAEs) and VAE-GANs. While adversarial attacks on classification models are well-documented, this work extends the concept to generative models, which are increasingly used in tasks such as image compression and synthesis. The authors propose three distinct attack methodologies: (1) a classifier-based attack, (2) an attack leveraging the VAE loss function (LVAE), and (3) a latent-space attack. These methods are evaluated on MNIST, SVHN, and CelebA datasets, demonstrating that generative models are vulnerable to adversarial perturbations. The paper also discusses practical attack scenarios, such as compromising latent-space-based compression systems, and provides metrics for evaluating attack success. The work is a valuable contribution to understanding the robustness of generative models and lays the groundwork for future research on defenses.
Decision: Accept
The paper is well-motivated, technically sound, and provides significant insights into a relatively unexplored area. The key reasons for acceptance are:
1. Novelty and Relevance: The paper addresses an important gap in the literature by extending adversarial attacks to generative models, which have unique vulnerabilities compared to classifiers.
2. Scientific Rigor: The proposed attacks are well-defined, and the experimental results are thorough, demonstrating the effectiveness of the methods across multiple datasets and architectures.
Supporting Arguments
1. Problem Motivation and Placement in Literature: The paper convincingly argues why adversarial attacks on generative models are important, especially in scenarios involving latent-space-based compression or data sharing. The discussion of related work is comprehensive, highlighting the novelty of the proposed attacks.
2. Methodological Soundness: The three attack methodologies are clearly described, and their differences are well-justified. The latent-space attack, in particular, is shown to be both effective and efficient, outperforming the other methods in terms of reconstruction quality and computational cost.
3. Experimental Validation: The evaluation is robust, covering multiple datasets (MNIST, SVHN, CelebA) and metrics (e.g., attack success rates, perturbation magnitudes). The visual examples and quantitative results convincingly demonstrate the success of the attacks.
Suggestions for Improvement
1. Clarity in Motivation: While the paper provides a motivating attack scenario, it could benefit from a more detailed discussion of real-world implications, particularly in applications like medical imaging or autonomous systems.
2. Comparison with Concurrent Work: The authors briefly mention concurrent work (e.g., Tabacof et al., 2016) but do not provide a detailed comparison. A side-by-side evaluation would strengthen the paper's claims of novelty and effectiveness.
3. Scalability to Larger Datasets: The paper focuses on relatively small datasets. It would be helpful to discuss the scalability of the proposed methods to larger datasets like CIFAR-10 or ImageNet, even if experimental results are not provided.
4. Defense Mechanisms: While the paper focuses on attacks, a brief discussion of potential defenses or robustness strategies would enhance its impact and relevance.
Questions for the Authors
1. How do the proposed methods scale to generative models trained on larger and more complex datasets, such as CIFAR-10 or ImageNet?
2. Can the latent-space attack be extended to other types of generative models, such as diffusion models or autoregressive models?
3. Did the authors explore the impact of different latent-space dimensionalities on the success of the attacks? If so, how does this affect the trade-off between attack success and perturbation magnitude?
In conclusion, this paper makes a strong contribution to the field of adversarial machine learning by extending the scope of attacks to generative models. While there are areas for further exploration, the novelty, rigor, and relevance of the work justify its acceptance.