Review of the Paper
Summary of Contributions
This paper extends adversarial and virtual adversarial training, previously applied primarily in the image domain, to text classification tasks. The authors propose applying perturbations to word embeddings in recurrent neural networks (RNNs) rather than directly to discrete one-hot word representations, addressing the challenge of sparse, high-dimensional text inputs. The method is evaluated on multiple benchmark datasets, achieving state-of-the-art results for both supervised and semi-supervised text classification tasks. The paper also provides qualitative analyses, demonstrating improved word embedding quality and reduced overfitting. The approach is notable for its simplicity, requiring the optimization of only one additional hyperparameter, and its compatibility with pre-trained language models.
Decision: Accept
The paper should be accepted because it makes a significant and well-supported contribution to the field of text classification. The extension of adversarial and virtual adversarial training to the text domain is novel, and the results convincingly demonstrate its effectiveness across multiple datasets. Furthermore, the paper is well-motivated, scientifically rigorous, and provides valuable insights into the behavior of adversarial training in text models.
Supporting Arguments
1. Novelty and Problem Addressed: The paper tackles a clear and important problemâ€”adapting adversarial and virtual adversarial training to text classification tasks. The proposed solution of perturbing word embeddings instead of discrete inputs is both innovative and practical, addressing the limitations of existing methods in handling sparse, high-dimensional text data.
   
2. Scientific Rigor: The experimental results are robust, covering five datasets with diverse characteristics. The authors compare their method against strong baselines and state-of-the-art models, demonstrating consistent improvements. The inclusion of both supervised and semi-supervised settings strengthens the paper's contributions.
3. Motivation and Placement in Literature: The paper is well-placed in the literature, building on foundational work in adversarial training and semi-supervised learning. The authors provide a clear motivation for their approach and situate it effectively within the context of prior work.
Suggestions for Improvement
1. Clarity on Semi-Supervised Results: While the paper highlights the benefits of virtual adversarial training in semi-supervised settings, the performance drop on the Rotten Tomatoes dataset raises questions. The authors should clarify whether this issue is dataset-specific or indicative of broader limitations in their method.
2. Ablation Studies: The paper could benefit from additional ablation studies to isolate the contributions of different components, such as pretraining, adversarial training, and virtual adversarial training. This would help better understand the relative importance of each component.
3. Generalization to Other Tasks: While the paper suggests that the method could be applied to other text tasks (e.g., machine translation, question answering), no experiments are provided to support this claim. Including preliminary results or a discussion of potential challenges would strengthen this argument.
Questions for the Authors
1. How does the method scale to larger datasets or more complex tasks, such as multi-label classification or sequence-to-sequence tasks?
2. Can the authors provide more details on the computational overhead introduced by adversarial and virtual adversarial training, particularly in comparison to standard training?
3. Did the authors explore the impact of different embedding dimensions or LSTM architectures on the performance of the proposed method?
In conclusion, this paper makes a strong contribution to the field of text classification and introduces a promising direction for leveraging adversarial training in the text domain. With minor clarifications and additional experiments, it could have an even greater impact.