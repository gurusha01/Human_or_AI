Review of the Paper: "Nonparametric Learning of Activation Functions in Deep Neural Networks"
Summary
This paper introduces a novel framework for nonparametrically learning activation functions in deep neural networks, addressing the current limitation where activation functions are treated as fixed hyperparameters. The authors propose a Fourier basis expansion approach to model activation functions, integrating this into the training process via backpropagation. The paper provides theoretical justifications for the method, including generalization bounds using algorithmic stability, and demonstrates empirical improvements on benchmark datasets like MNIST and CIFAR-10. A two-stage training procedure is introduced to stabilize training, particularly for convolutional networks. The experimental results show up to a 15% relative improvement in test error rates compared to baseline models, highlighting the potential of this approach to enhance deep learning performance.
Decision: Accept
The paper makes a significant contribution to deep learning by expanding the learnable components of neural networks to include activation functions. The theoretical rigor, coupled with strong empirical results, demonstrates the feasibility and effectiveness of the proposed method. The innovation is well-motivated and addresses an underexplored area in the literature.
Supporting Arguments
1. Problem Tackled: The paper addresses the critical limitation of fixed activation functions in neural networks, proposing a principled method to learn them nonparametrically. This expands the functional capacity of neural networks and aligns with the broader goal of automating neural architecture design.
   
2. Theoretical Rigor: The authors provide a solid theoretical foundation, including provable generalization bounds using algorithmic stability. This is a significant strength, as it ensures the method is not only empirically effective but also theoretically sound.
3. Empirical Results: The experimental results are compelling, showing consistent improvements across datasets and architectures. The two-stage training procedure is a practical solution to stabilize training, particularly for convolutional networks, and demonstrates the authors' attention to implementation challenges.
4. Literature Placement: The paper situates itself well within the existing literature, contrasting its approach with related works such as piecewise linear activations and hyperparameter optimization methods. The novelty of learning activation functions directly during training is clearly articulated.
Suggestions for Improvement
1. Clarity in Methodology: While the theoretical framework is detailed, the practical implementation of the Fourier basis expansion could benefit from additional clarity. For example, a step-by-step explanation of how the coefficients are initialized and updated during training would help practitioners replicate the results.
2. Comparison with Other Basis Functions: The authors briefly mention that polynomial basis expansions were explored but found less effective. A more detailed comparison, including empirical results, would strengthen the justification for choosing Fourier basis functions.
3. Scalability Analysis: While the paper demonstrates improvements on MNIST and CIFAR-10, it would be valuable to discuss the scalability of the method to larger datasets and deeper architectures. This would provide a clearer picture of its practical applicability.
4. Ablation Studies: An ablation study on the two-stage training procedure would be helpful to quantify its impact on performance. For instance, how much of the improvement is attributable to the two-stage process versus the nonparametric activation functions themselves?
Questions for the Authors
1. How does the choice of the Fourier basis expansion compare to other potential nonparametric methods, such as Gaussian processes or splines, in terms of both theoretical guarantees and empirical performance?
2. Can the proposed method be extended to recurrent neural networks or transformers, where activation functions play a critical role in temporal modeling?
3. What are the computational overheads introduced by learning activation functions, especially in large-scale models?
Overall, this paper makes a strong contribution to the field of deep learning, and the proposed method has the potential to inspire further research into learning network components beyond weights.