The paper proposes ParMAC, a distributed computation model for the Method of Auxiliary Coordinates (MAC), aimed at optimizing nested, nonconvex machine learning models such as deep neural networks and binary autoencoders. ParMAC introduces a circular topology for distributed systems, enabling parallelism by decoupling nested models into submodels and auxiliary coordinates. The paper claims that ParMAC achieves high parallel speedups, low communication overhead, and scalability across large datasets. It presents theoretical analyses of convergence and parallel speedup, and demonstrates the framework's efficacy through experiments on large-scale datasets, achieving nearly perfect speedups on a 128-processor cluster.
Decision: Accept
Key reasons: 
1. Novelty and Practical Relevance: The paper addresses a critical challenge in distributed optimization of nonconvex models, which is underexplored compared to convex optimization. ParMAC's simplicity, scalability, and ability to handle nondifferentiable layers make it a significant contribution.
2. Strong Empirical and Theoretical Support: The authors provide rigorous theoretical analysis of parallel speedup and convergence, complemented by extensive experiments on real-world datasets, including large-scale benchmarks like SIFT-1B.
Supporting Arguments:
1. Well-Motivated Approach: The paper is well-placed in the literature, highlighting the limitations of existing distributed optimization methods (e.g., parameter-server approaches) and the challenges of nonconvex models. ParMAC's design directly addresses these gaps by minimizing communication overhead and leveraging inherent parallelism in MAC.
2. Experimental Validation: The experiments convincingly demonstrate ParMAC's scalability and efficiency. For instance, the reported 100Ã— speedup on SIFT-1M and the ability to train on SIFT-1B in under 30 hours are impressive. The theoretical predictions of speedup align well with empirical results, lending credibility to the proposed model.
3. Generality and Flexibility: ParMAC's applicability to various nested models and its support for practical considerations like data shuffling, load balancing, and fault tolerance make it a versatile framework for distributed machine learning.
Suggestions for Improvement:
1. Clarity on Convergence for Nondifferentiable Layers: While the paper acknowledges the lack of theoretical guarantees for nondifferentiable layers, it would benefit from a more detailed discussion of practical observations regarding convergence in such cases.
2. Comparison with Other Distributed Frameworks: While the paper mentions alternatives like TensorFlow and DistBelief, a direct experimental comparison (e.g., runtime, precision) would strengthen the claims of ParMAC's superiority.
3. Scalability Beyond 128 Processors: The experiments are limited to 128 processors. While theoretical predictions suggest scalability to larger clusters, empirical validation on larger systems would solidify these claims.
4. Broader Applicability: The paper focuses on binary autoencoders. Demonstrating ParMAC's applicability to other nested models, such as deep neural networks with complex architectures, would enhance its impact.
Questions for the Authors:
1. How does ParMAC handle datasets with highly imbalanced distributions across machines? Does this affect convergence or speedup?
2. Can the circular topology introduce bottlenecks in cases where certain submodels require significantly more computation than others? If so, how can this be mitigated?
3. Have you considered integrating ParMAC with popular distributed frameworks like PyTorch or TensorFlow to make it more accessible to practitioners?
In conclusion, the paper makes a strong contribution to distributed optimization for nonconvex models, with both theoretical rigor and practical relevance. While there are areas for further exploration, the work is well-suited for acceptance at the conference.