The paper presents a novel model architecture, RASOR (Recurrent Span Representations), for extractive question answering on the SQuAD dataset. The authors propose an efficient method to compute fixed-length representations for all possible answer spans in a passage using recurrent networks, allowing for global normalization during training and exact decoding during evaluation. The model incorporates both passage-aligned and passage-independent question representations to enhance performance. Experimental results demonstrate that RASOR outperforms prior state-of-the-art models, such as Wang & Jiang (2016), by 5% in exact match and 3.6% in F1 score, achieving a significant reduction in error compared to the baseline. The authors also provide detailed ablation studies and analysis to highlight the importance of their architectural choices and learning objectives.
Decision: Accept
The key reasons for this decision are:  
1. Novelty and Impact: The proposed RASOR model introduces a well-motivated and efficient approach to span-based answer extraction, addressing limitations of prior models like greedy decoding and independence assumptions. The significant performance improvement over state-of-the-art baselines demonstrates the model's impact.  
2. Scientific Rigor: The paper provides comprehensive experimental results, ablation studies, and qualitative analyses to support its claims. The methodology is clearly described, and the results are reproducible.
Supporting Arguments:  
- The paper is well-placed in the literature, building on and addressing limitations of prior work (e.g., Wang & Jiang, 2016). The motivation for explicitly representing spans and using global normalization is clearly articulated and experimentally validated.  
- The authors provide rigorous comparisons to alternative learning objectives and architectures, showing that their approach aligns better with the task requirements.  
- The inclusion of both passage-aligned and passage-independent question representations is a thoughtful design choice, and its benefits are empirically demonstrated.  
- The analysis section provides valuable insights into the model's behavior and failure cases, which could guide future improvements.
Suggestions for Improvement:  
1. Test Set Results: The lack of results on the SQuAD test set is a limitation. While the authors mention ongoing efforts to address this, including these results would strengthen the paper.  
2. Scalability: While the authors claim efficiency, additional experiments on longer passages or larger datasets would provide a more comprehensive evaluation of scalability.  
3. Error Analysis: The qualitative analysis of failure cases is insightful but could be expanded. For instance, exploring strategies to address semantic dependencies that the model struggles with would be valuable.  
4. Clarity: Some sections, such as the derivation of question-focused passage word embeddings, are dense and could benefit from additional visual aids or simplified explanations.
Questions for the Authors:  
1. How does RASOR perform on other datasets or domains beyond SQuAD? Can the model generalize effectively to different types of text?  
2. What are the computational trade-offs of the proposed architecture compared to simpler models like endpoint prediction?  
3. Have you explored alternative methods for handling semantic dependencies in failure cases, such as incorporating external knowledge or pre-trained language models?  
Overall, this paper makes a strong contribution to the field of extractive question answering and is well-suited for acceptance at the conference.