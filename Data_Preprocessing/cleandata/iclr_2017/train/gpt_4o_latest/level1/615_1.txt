Review of the Paper: "L-SR1: A New Second Order Method to Train Deep Neural Networks"
Summary
This paper introduces L-SR1, a novel second-order optimization method designed to address key challenges in training deep neural networks, such as handling saddle points and poor Hessian conditioning. The authors propose a limited-memory variant of the Symmetric Rank One (SR1) quasi-Newton method, combined with a trust-region framework and batch normalization, to make the approach practical for large-scale non-convex optimization. The paper provides empirical evidence that L-SR1 performs competitively with first-order methods like Nesterov's Accelerated Gradient Descent (NAG) and outperforms the widely used second-order method L-BFGS on standard benchmarks such as MNIST and CIFAR10. Additionally, the authors explore the hyperparameter sensitivity of L-SR1 and its potential for distributed training, highlighting its robustness and scalability.
Decision: Accept
The paper makes a meaningful contribution to the field of optimization for deep learning by proposing a practical second-order method that addresses known limitations of existing approaches. The key reasons for acceptance are:
1. Novelty and Practicality: The use of SR1 updates in a limited-memory setting, combined with a trust-region approach, is a novel and well-motivated solution to the challenges of saddle points and Hessian conditioning in deep learning.
2. Empirical Validation: The experimental results convincingly demonstrate that L-SR1 is competitive with state-of-the-art first-order methods and significantly outperforms L-BFGS, a widely used second-order method.
Supporting Arguments
1. Problem Motivation and Placement in Literature: The paper clearly identifies the limitations of existing second-order methods (e.g., L-BFGS) and motivates the use of SR1 updates to overcome these challenges. The authors provide a thorough review of related work, situating their approach within the broader context of optimization methods for deep learning.
2. Experimental Rigor: The experiments are well-designed, with comparisons across multiple datasets (MNIST, CIFAR10) and architectures (LeNet5, residual networks). The analysis of hyperparameter sensitivity and mini-batch size variation further strengthens the case for L-SR1's robustness and scalability.
3. Potential for Distributed Training: The demonstrated insensitivity of L-SR1 to large mini-batch sizes is a significant advantage for distributed training, a critical area of interest in modern deep learning.
Suggestions for Improvement
While the paper is strong overall, the following points could enhance its clarity and impact:
1. Comparison with Recent Stochastic Second-Order Methods: The paper mentions several recent stochastic quasi-Newton methods in the related work but does not include direct experimental comparisons. Adding such comparisons would strengthen the claims about L-SR1's superiority.
2. Skipped Updates: The paper notes high rates of skipped updates during MNIST training with batch normalization but does not provide a detailed explanation. A deeper investigation into this phenomenon would improve understanding and potentially guide future improvements.
3. Test Loss Variability: The authors observe higher variability in test loss for L-SR1 compared to other methods when training residual networks. Exploring the reasons for this variability and proposing mitigation strategies would be valuable.
4. Scalability to Larger Datasets: While the results on MNIST and CIFAR10 are promising, experiments on larger datasets (e.g., ImageNet) would provide stronger evidence of L-SR1's scalability and practical utility.
Questions for the Authors
1. How does L-SR1 compare with other recent stochastic second-order methods, such as RES or ADAQN, in terms of convergence speed and final test accuracy?
2. Can the authors provide more insights into the interplay between batch normalization and skipped updates? Does this issue persist for other datasets or architectures?
3. Have the authors considered extending L-SR1 to other optimization tasks, such as reinforcement learning or natural language processing? If so, what challenges might arise?
In conclusion, this paper presents a well-motivated, novel, and practical contribution to second-order optimization for deep learning. While there are areas for further exploration, the current work is robust and impactful, warranting acceptance.