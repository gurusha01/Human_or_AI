Review of the Paper: "Learning Optimization Algorithms via Reinforcement Learning"
Summary of Contributions
This paper proposes a novel approach to automating the design of optimization algorithms by framing the problem as a reinforcement learning (RL) task. The authors represent optimization algorithms as policies and use guided policy search (GPS) to learn these policies, enabling the discovery of optimization algorithms that outperform hand-engineered methods. The learned optimizers are evaluated on a variety of convex and non-convex optimization problems, including logistic regression, robust linear regression, and neural network training. The results demonstrate that the learned algorithms achieve faster convergence and/or better final objective values compared to traditional methods like gradient descent, momentum, conjugate gradient, and L-BFGS. The work is well-motivated, as it addresses the labor-intensive process of manually designing optimization algorithms, and it contributes to the growing field of meta-learning by focusing on learning optimization processes rather than task-specific regularities.
Decision: Accept
The paper makes a strong case for acceptance due to its innovative framing of optimization algorithm design as a reinforcement learning problem and its empirical results demonstrating superior performance of the learned optimizers. The contributions are significant, the methodology is scientifically rigorous, and the results are compelling.
Supporting Arguments
1. Well-Motivated Problem: The paper addresses a highly relevant problem in machine learning and optimization: the laborious and iterative process of designing optimization algorithms. By proposing an automated approach, the authors align with the broader trend of leveraging machine learning to replace manual design processes.
2. Novel Methodology: Representing optimization algorithms as policies in an RL framework is a creative and impactful idea. The use of GPS to learn these policies is well-justified, and the paper provides sufficient background to support this choice.
3. Strong Empirical Results: The experiments are comprehensive, covering both convex and non-convex settings. The learned optimizer consistently outperforms traditional methods in terms of convergence speed and/or final objective value, demonstrating its practical utility.
4. Scientific Rigor: The methodology is clearly described, and the experiments are conducted with appropriate baselines and metrics. The visualization of optimization trajectories provides additional insights into the behavior of the learned optimizer.
Suggestions for Improvement
1. Clarity on Generalization: While the paper evaluates the learned optimizer's ability to generalize to unseen objective functions, more discussion on the limits of this generalization would strengthen the work. For example, how does the optimizer perform on significantly different classes of problems (e.g., highly non-smooth objectives)?
2. Comparison to Recent Work: The paper mentions related work that appeared on ArXiv after submission, but a more detailed comparison to this concurrent work (e.g., Andrychowicz et al., 2016) would provide additional context for the novelty of the proposed approach.
3. Ablation Studies: An ablation study to assess the impact of different components of the proposed framework (e.g., the choice of neural network architecture or the use of GPS) would help clarify which aspects are most critical to the optimizer's success.
4. Scalability: The experiments focus on relatively small-scale problems. It would be helpful to discuss the scalability of the approach to larger datasets and higher-dimensional optimization problems, as this is critical for practical adoption.
Questions for the Authors
1. How sensitive is the learned optimizer to the choice of hyperparameters during training (e.g., the number of trajectories, trajectory length, or neural network architecture)?
2. Can the learned optimizer handle constraints or other problem-specific requirements (e.g., box constraints or sparsity)?
3. How does the computational cost of training and using the learned optimizer compare to traditional methods, especially for large-scale problems?
In conclusion, this paper presents a significant and well-executed contribution to the field of optimization and meta-learning. With minor clarifications and additional experiments, it has the potential to make a lasting impact.