Review of the Paper
Summary of Contributions:  
This paper introduces a Joint Many-Task (JMT) model for Natural Language Processing (NLP) that jointly trains multiple linguistic tasks—POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment—within a single end-to-end framework. The authors propose a novel architecture where tasks are arranged hierarchically, with increasingly complex tasks handled at deeper layers of a multi-layer bi-LSTM. Key innovations include the use of shortcut connections to word representations, label embeddings from lower-level tasks, and a successive regularization strategy to mitigate catastrophic interference during training. The model achieves state-of-the-art results on chunking, dependency parsing, semantic relatedness, and textual entailment, while performing competitively on POS tagging. The paper also provides extensive analysis of the model's architecture and training strategies, highlighting the benefits of joint learning and task-specific design choices.
Decision: Accept  
The paper should be accepted because it addresses a significant challenge in NLP—jointly learning multiple tasks with hierarchical dependencies—using a well-motivated and scientifically rigorous approach. The proposed JMT model is novel, achieves state-of-the-art results on multiple benchmarks, and is supported by thorough experimentation and analysis.
Supporting Arguments:  
1. Well-Motivated Approach: The paper is grounded in prior work on multi-task learning and linguistic hierarchies, and it builds on these foundations with clear innovations. The use of task-specific layers and shortcut connections is particularly compelling, as it aligns with linguistic intuition and empirical evidence.  
2. Scientific Rigor: The authors provide extensive experimental results, comparing their model to strong baselines and prior state-of-the-art methods. The results are robust across multiple datasets and tasks, and the analysis of architectural components (e.g., successive regularization, shortcut connections) is detailed and insightful.  
3. Impact and Generalizability: The JMT model has the potential to influence future research in multi-task learning, as it demonstrates how to effectively balance task-specific and shared representations. The simplicity of the proposed architecture (e.g., avoiding complex mechanisms like attention) makes it accessible and adaptable to other domains.
Suggestions for Improvement:  
1. Clarity in Presentation: While the paper is comprehensive, the dense technical details can be overwhelming. Simplifying the mathematical notation and providing more intuitive explanations (e.g., visualizing the role of successive regularization) would improve accessibility.  
2. Error Analysis: The error analysis for semantic tasks highlights limitations in handling antonyms and synonyms. Including experiments with external lexical resources (e.g., WordNet) or pre-trained embeddings designed to capture such relationships would strengthen the paper.  
3. Scalability: The paper does not discuss the computational efficiency of the JMT model in detail. Providing training time and resource requirements would help practitioners assess its feasibility for large-scale applications.  
4. Broader Applicability: While the model performs well on the selected tasks, it would be interesting to see its performance on other NLP tasks (e.g., machine translation or summarization) to evaluate its generalizability.
Questions for the Authors:  
1. How does the model handle cases where tasks have conflicting objectives or require contradictory updates to shared parameters?  
2. Can the proposed architecture be extended to handle tasks with different input modalities (e.g., combining text and images)?  
3. How does the model's performance degrade as the number of tasks increases? Are there limits to the scalability of the JMT framework?  
4. Have you explored alternative strategies for task ordering during training? How sensitive are the results to the chosen order?  
In conclusion, this paper makes a significant contribution to multi-task learning in NLP, and its acceptance will benefit both the research community and practitioners. The suggestions and questions provided aim to further refine and extend the work.