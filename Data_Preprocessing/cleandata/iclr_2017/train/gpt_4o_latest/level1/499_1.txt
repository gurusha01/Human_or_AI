The paper introduces a novel approach using hypernetworks to generate adaptive weights for recurrent neural networks (RNNs), specifically LSTMs, challenging the traditional weight-sharing paradigm. The authors propose training a smaller "hypernetwork" end-to-end to dynamically generate weights for a larger "main network," allowing for non-shared, input-dependent weights. The paper demonstrates the efficacy of this approach across diverse sequence modeling tasks, including character-level language modeling, handwriting generation, and neural machine translation, achieving state-of-the-art or competitive results.
Decision: Accept
Key Reasons:  
1. The paper addresses a significant problem in RNNs—rigid weight-sharing—by proposing a well-motivated and innovative solution using hypernetworks.  
2. The experimental results are robust, demonstrating improvements over baseline models across multiple tasks, and the methodology is scientifically rigorous.  
Supporting Arguments:  
- Problem Tackling and Motivation: The paper is well-placed in the literature, building on prior work in evolutionary computing, fast weights, and multiplicative RNNs. The authors clearly articulate the limitations of existing methods and position hypernetworks as a scalable and efficient alternative. The connection to related work, such as HyperNEAT and Layer Normalization, is thorough and contextualized.  
- Empirical Validation: The experiments are comprehensive, covering small-scale (Penn Treebank) and large-scale (Hutter Prize Wikipedia, WMT'14) datasets. The results convincingly demonstrate the advantages of hypernetworks in terms of perplexity, log loss, and BLEU scores. The inclusion of qualitative analyses, such as visualizing weight changes during handwriting generation, further strengthens the claims.  
- Scalability and Practicality: The authors address memory efficiency concerns by introducing a row-wise scaling mechanism, making the approach feasible for real-world applications. The successful application to a production-level neural machine translation system underscores the practical relevance of the method.
Suggestions for Improvement:  
1. Clarity in Mathematical Formulation: While the methodology is detailed, some equations (e.g., Eq. 2 and Eq. 5) could benefit from additional explanation or a visual diagram to aid understanding.  
2. Ablation Studies: Including ablation experiments to isolate the contributions of different components (e.g., embedding size, recurrent dropout) would provide deeper insights into the model's performance.  
3. Comparison with More Baselines: While the paper compares HyperLSTM to standard LSTMs and Layer Normalization, additional comparisons with recent advancements in RNN architectures (e.g., Transformer-based models) would strengthen the results.  
4. Error Analysis: A more detailed analysis of failure cases, particularly in machine translation, would help contextualize the limitations of the approach.
Questions for the Authors:  
1. How does the computational overhead of training hypernetworks compare to standard RNNs, particularly for large-scale tasks like machine translation?  
2. Could the proposed approach be extended to other architectures, such as Transformers or convolutional networks?  
3. How sensitive is the model to hyperparameters like embedding size and dropout rates?  
Overall, the paper makes a significant contribution to the field of sequence modeling by introducing a novel and effective approach to weight generation for RNNs. The results are compelling, and the methodology is well-grounded in theory and experimentation. With minor improvements in clarity and additional analyses, the paper would be even stronger.