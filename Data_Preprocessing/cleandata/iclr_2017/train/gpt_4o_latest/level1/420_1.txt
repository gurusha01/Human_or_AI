The paper proposes a novel neural language model that introduces a key-value-predict attention mechanism, separating the functions of output representations into three distinct roles: key, value, and next-word prediction. This approach addresses the issue of overloaded representations in conventional memory-augmented neural language models. The authors demonstrate that their model outperforms existing memory-augmented models on the Children's Book Test (CBT) and a Wikipedia corpus. Interestingly, they also find that a simpler N-gram RNN model, which uses concatenated recent output representations, achieves comparable performance, suggesting that short-term memory is sufficient for many language modeling tasks. The paper contributes by improving memory-augmented architectures, providing insights into the challenges of leveraging long-range dependencies, and offering a simpler yet effective baseline model.
Decision: Accept.  
Key reasons: (1) The paper introduces a well-motivated and novel modification to attention mechanisms in neural language models, which is both conceptually and practically significant. (2) The empirical results are robust and provide valuable insights into the limitations of current memory-augmented models, as well as the effectiveness of simpler architectures.
Supporting Arguments:  
1. Problem Tackled: The paper addresses the issue of overloaded output representations in memory-augmented neural language models, a problem that hinders the effective use of attention mechanisms for long-range dependencies. This is a well-defined and relevant problem in the field of language modeling.  
2. Novelty and Motivation: The proposed key-value-predict attention mechanism is a novel and well-motivated approach, inspired by prior work in memory networks and neural Turing machines. The authors effectively position their work within the existing literature and provide a clear rationale for their design choices.  
3. Empirical Rigor: The experiments are thorough, comparing the proposed models against strong baselines and state-of-the-art architectures. The results are consistent across two datasets, and the analysis of attention span utilization is insightful. The finding that simpler models can achieve comparable performance is particularly noteworthy and challenges assumptions about the necessity of complex memory mechanisms.  
Suggestions for Improvement:  
1. Long-Range Dependencies: While the paper highlights the difficulty of leveraging long-range dependencies, it would benefit from a more detailed discussion of potential future directions to address this limitation. For example, could alternative training objectives or architectural modifications encourage longer attention spans?  
2. Ablation Studies: An ablation study isolating the contributions of the key, value, and predict components in the attention mechanism would strengthen the claims about the effectiveness of the proposed architecture.  
3. Broader Implications: The paper could expand on the implications of its findings for other tasks beyond language modeling, such as machine translation or question answering, where long-range dependencies are critical.  
Questions for the Authors:  
1. How does the proposed key-value-predict attention mechanism scale with larger datasets or more complex corpora?  
2. Could the authors provide more details about the training stability of the proposed models compared to simpler baselines?  
3. Have the authors considered combining the N-gram RNN approach with the key-value-predict mechanism to explore hybrid architectures?  
Overall, the paper makes a meaningful contribution to the field of neural language modeling and provides a strong foundation for future work on memory-augmented architectures.