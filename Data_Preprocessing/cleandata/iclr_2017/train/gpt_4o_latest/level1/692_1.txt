Review
Summary  
The paper proposes a novel attention-based framework for sentiment analysis that combines global and local context representations using bidirectional LSTMs (Bi-LSTMs). The authors introduce two models: the Two-Scan Approach with Attention (TS-ATT) and the Single-Scan Approach with Attention (SS-ATT). The TS-ATT model first generates a rough global context representation using a Bi-LSTM and then refines it by incorporating local contexts with attention during a second scan. The SS-ATT model simplifies this process by combining global and local context learning in a single scan. The authors claim that their models outperform existing baselines, including CNNs, LSTMs, and other attention-based models, on benchmark sentiment classification datasets. Additionally, attention visualization and case studies are provided to demonstrate the interpretability of the proposed framework.
Decision: Accept  
The paper should be accepted because (1) it introduces a well-motivated and novel approach to address limitations in existing sentiment analysis models, and (2) it provides strong empirical evidence that the proposed models achieve state-of-the-art performance on multiple benchmark datasets. The attention visualization further enhances the interpretability of the model, which is a valuable contribution to the field.
Supporting Arguments  
1. Problem and Motivation: The paper addresses the challenge of capturing complex semantic compositions in sentiment analysis, which is a well-recognized problem in NLP. The use of global-local context attention is a novel and intuitive approach inspired by human reading behavior, making the motivation clear and compelling.  
2. Empirical Results: The proposed models achieve superior performance on two out of three datasets and are competitive on the third. The improvements over baselines, including CNNs, Bi-LSTMs, and prior attention-based models (e.g., NAM), are significant and consistent.  
3. Scientific Rigor: The methodology is clearly described, and the experiments are conducted rigorously with appropriate baselines and hyperparameter settings. The attention visualization and case studies provide additional evidence of the model's effectiveness and interpretability.
Suggestions for Improvement  
1. Clarity on Model Comparison: While the authors compare their models to several baselines, it would be helpful to provide more details about the specific configurations of these baselines (e.g., filter sizes for CNNs, attention mechanisms in NAM). This would ensure fair comparisons and reproducibility.  
2. Ablation Study: An ablation study to isolate the contributions of the global context, local context, and attention mechanism would strengthen the claims about the effectiveness of the proposed framework.  
3. Complexity Analysis: The paper could benefit from a more detailed discussion of the computational trade-offs between TS-ATT and SS-ATT, particularly in terms of training and inference time.  
4. Generalization to Other Tasks: While the focus is on sentiment analysis, it would be interesting to discuss how the proposed framework might generalize to other NLP tasks, such as machine translation or question answering.
Questions for the Authors  
1. How sensitive is the performance of the proposed models to the choice of hyperparameters, such as the dimensionality of word embeddings and LSTM states?  
2. Can the proposed framework handle very long texts effectively, given the limitations of LSTMs in capturing long-term dependencies?  
3. How does the model perform when pretrained embeddings (e.g., word2vec or GloVe) are used instead of random initialization? Would this further improve performance?  
4. Could the authors provide more insights into the trade-offs between TS-ATT and SS-ATT in terms of accuracy and computational efficiency?  
Overall, the paper makes a strong contribution to the field of sentiment analysis and introduces a novel, interpretable, and effective framework. With minor improvements and clarifications, it has the potential to make a significant impact.