The paper addresses the problem of modeling relational time series, where observations are correlated both within and across multiple time series, a challenge prevalent in domains such as ecology, meteorology, and social data analysis. The authors propose a novel Relational Dynamic model with Gaussian representations (RDG), which combines representation learning with a dynamic state space model to capture temporal and relational dependencies. By embedding time series as Gaussian distributions in a latent space, the model explicitly accounts for uncertainty at both the observation and dynamic levels. The paper claims contributions in three areas: (i) introducing a new dynamic model for relational time series, (ii) incorporating stochastic components to model uncertainty, and (iii) demonstrating competitive performance on four datasets.
Decision: Accept.  
The key reasons for this decision are the novelty of the approach and its strong empirical performance. The paper introduces a well-motivated and scientifically rigorous model that advances the state of the art in relational time series forecasting. Its ability to model uncertainty and provide confidence intervals is a significant contribution, particularly for applications requiring robust predictions.
Supporting Arguments:  
1. Novelty and Motivation: The paper is well-placed in the literature, addressing a gap in relational time series modeling by explicitly incorporating uncertainty through Gaussian embeddings. The authors build on prior work, such as deterministic models (e.g., Ziat et al., 2016), and extend it meaningfully. The inclusion of relational dependencies is a clear advancement over existing methods like RNNs and Kalman Filters.  
2. Empirical Rigor: The experimental results are compelling, with RDG outperforming or matching state-of-the-art baselines across four datasets. The ability to predict confidence intervals adds practical value, and the results convincingly demonstrate the model's robustness over varying prediction horizons.  
3. Clarity and Structure: The paper is well-organized, with clear explanations of the model components, loss functions, and experimental setup. The inclusion of ablation studies (e.g., with/without structural regularization) strengthens the claims.
Suggestions for Improvement:  
1. Clarity on Graph Dependencies: While the paper assumes a static graph structure as prior knowledge, it would be helpful to discuss how the model performs when the graph is noisy or partially incorrect. Could the model learn the graph structure dynamically?  
2. Scalability: The computational complexity of the model, particularly for large-scale datasets or high-dimensional latent spaces, is not discussed. Including a runtime analysis or scalability experiments would strengthen the paper.  
3. Imputation Tasks: Although the paper mentions imputation as a potential extension, no experiments are provided. Including results on imputation tasks would further validate the model's versatility.  
4. Interpretability of Latent Space: The paper could explore the interpretability of the learned latent space representations. For example, do the embeddings reflect meaningful relationships between time series?
Questions for the Authors:  
1. How sensitive is the model to the choice of hyperparameters, such as the weights for the structural regularization term (Î»R)?  
2. How does the model handle missing data in the training set, and does this affect the uncertainty estimates?  
3. Could the proposed approach be extended to handle dynamic graph structures, where relationships between time series evolve over time?  
Overall, the paper makes a significant contribution to the field of relational time series modeling and is a strong candidate for acceptance. Addressing the suggested improvements would further enhance its impact.