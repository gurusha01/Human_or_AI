Review of "Surprisal-Driven Recurrent Networks"
Summary of Contributions:  
The paper introduces a novel architecture, termed "Surprisal-Driven Recurrent Networks," which incorporates a feedback mechanism based on the discrepancy (or surprisal) between past predictions and actual observations. Unlike traditional recurrent neural networks (RNNs) that use such error signals only during training, this approach leverages them during inference to improve prediction accuracy. The authors demonstrate the effectiveness of this feedback mechanism by integrating it into Long Short-Term Memory (LSTM) networks and achieving state-of-the-art performance on the enwik8 character-level text modeling task, with a Bits-Per-Character (BPC) score of 1.37. The paper situates its contribution within the broader literature on top-down feedback in neural networks, contrasting its method with Gated-Feedback RNNs and Ladder Networks. The experimental results are compelling and suggest that the proposed feedback mechanism enhances generalization capabilities.
Decision: Accept  
Key reasons for acceptance are:  
1. Novelty and Contribution: The paper introduces a unique and well-motivated feedback mechanism that is distinct from existing approaches in the literature. The idea of using prediction errors during inference is both innovative and practically impactful.  
2. Empirical Validation: The proposed method achieves state-of-the-art results on a challenging benchmark dataset, demonstrating its efficacy. The experimental setup is rigorous, and the results are compelling.
Supporting Arguments:  
- The paper is well-placed in the literature, providing clear distinctions between its approach and related works like Gated-Feedback RNNs and Ladder Networks. The motivation for using surprisal as a feedback signal is grounded in both theoretical reasoning and empirical evidence.  
- The experiments are thorough, using a widely recognized dataset (enwik8) and a robust evaluation protocol. The performance improvement (1.37 BPC) over existing methods is significant and suggests the practical utility of the proposed architecture.  
- The mathematical formulation of the feedback mechanism is detailed and scientifically rigorous, ensuring reproducibility. The authors also provide insights into the implementation details, such as training parameters and initialization methods.
Suggestions for Improvement:  
1. Clarity in Presentation: While the technical details are comprehensive, the paper could benefit from a more intuitive explanation of the feedback mechanism for readers less familiar with the mathematical intricacies. For example, visualizations or diagrams showing how surprisal-driven feedback influences predictions over time would enhance understanding.  
2. Ablation Studies: The paper would be strengthened by including ablation studies to isolate the contribution of the feedback mechanism. For instance, how does the performance degrade if the feedback signal is removed or modified?  
3. Regularization and Sparsity: The authors mention the potential for further improvements through regularization and sparsity. Including preliminary experiments or discussions on these aspects would provide a more complete picture of the method's potential.  
4. Broader Applicability: While the paper focuses on character-level text modeling, it would be helpful to discuss the applicability of the approach to other temporal tasks (e.g., speech recognition, time-series forecasting) to highlight its generalizability.
Questions for the Authors:  
1. How sensitive is the performance of the proposed method to the choice of hyperparameters, such as the learning rate or feedback weight initialization?  
2. Did the authors observe any trade-offs, such as increased computational cost or slower convergence, when incorporating the feedback mechanism?  
3. Could the feedback mechanism be extended to multi-layer architectures, and if so, how would the feedback interact across layers?  
4. Are there any qualitative insights into how the feedback mechanism disambiguates similar patterns, as hypothesized in the introduction?  
Overall, this paper makes a significant contribution to the field of recurrent neural networks and temporal data modeling. With minor improvements in presentation and additional experiments, it has the potential to be a highly impactful work.