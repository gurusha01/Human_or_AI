Review of the Paper: Hierarchical Compositional Network (HCN)
Summary of Contributions
This paper introduces the Hierarchical Compositional Network (HCN), a directed generative model designed to discover and disentangle the hierarchical building blocks of binary images without supervision. The authors propose a novel inference and learning approach based on max-product message passing (MPMP), which eliminates the need for expectation-maximization (EM) and enables efficient feature learning. The model demonstrates flexibility by supporting tasks such as classification, inpainting, and supervised learning, with a functional forward pass resembling that of a convolutional neural network (CNN) but with qualitatively different features. The paper claims that HCN learns interpretable, composable, and reusable features, overcoming limitations of existing generative models like variational autoencoders (VAEs) and generative adversarial networks (GANs). Experimental results on synthetic and real-world datasets, including MNIST, validate the model's ability to generalize, denoise, and classify with minimal supervision.
Decision: Accept
The paper is recommended for acceptance due to its novel contributions to generative modeling, particularly in unsupervised feature learning. The proposed HCN framework addresses a challenging problem—learning hierarchical and interpretable features—while providing a rigorous and scientifically sound methodology. The experimental results convincingly demonstrate the model's strengths, including its ability to generalize to noisy and unseen data, outperforming standard CNNs in certain settings. The use of MPMP for efficient inference is a notable technical innovation.
Supporting Arguments
1. Well-Defined Problem and Motivation: The paper clearly identifies the limitations of existing generative models (e.g., lack of interpretable features, inability to handle "explaining away") and positions HCN as a solution. The hierarchical compositional approach is well-motivated, with connections to prior work on AND-OR graphs and compositional hierarchies.
   
2. Scientific Rigor: The authors provide a detailed mathematical formulation of the HCN model, including its probabilistic structure and inference mechanism. The use of MPMP is justified with theoretical insights, and the experimental results are thorough, covering synthetic, semi-supervised, and real-world scenarios.
3. Empirical Validation: The experiments demonstrate HCN's ability to learn meaningful features, outperform baselines like NOCA, and generalize to noisy and corrupted data. The results on MNIST, including its robustness to noise, highlight the model's practical utility.
Suggestions for Improvement
1. Scalability and Computational Efficiency: While the paper acknowledges the computational overhead of MPMP and the high memory requirements for large datasets, it would benefit from a more detailed discussion of potential optimizations or scalability strategies. For example, how does the online learning extension compare to batch learning in terms of performance and efficiency?
2. Comparison with Modern Generative Models: The paper briefly mentions VAEs and GANs but does not experimentally compare HCN to these models. Including such comparisons would strengthen the claims about HCN's advantages.
3. Clarity in Presentation: The mathematical exposition, while rigorous, could be streamlined for accessibility. For instance, the description of MPMP updates (Appendix D) could be summarized in the main text with key insights, deferring detailed equations to the appendix.
4. Real-World Applications: While the MNIST experiments are compelling, additional real-world datasets (e.g., natural images) would demonstrate the broader applicability of HCN. This could also highlight its ability to handle more complex, non-binary data.
Questions for the Authors
1. How does HCN perform on larger-scale datasets with more complex structures, such as CIFAR-10 or ImageNet? Are there plans to extend the model to handle grayscale or RGB images directly?
2. Can the proposed MPMP schedule be generalized to other types of generative models, or is it specific to HCN's structure?
3. How sensitive is the model to hyperparameter choices (e.g., sparsity priors, pooling sizes)? Could automated methods like Bayesian optimization be used to tune these parameters?
In conclusion, the paper makes a significant contribution to the field of generative modeling and unsupervised learning. With minor improvements in scalability, clarity, and broader comparisons, it has the potential to be a highly impactful work.