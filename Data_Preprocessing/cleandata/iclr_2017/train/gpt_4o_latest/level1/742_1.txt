Review
Summary of Contributions
The paper investigates the expressive power of deep neural networks, focusing on their behavior before and after training. It introduces three natural measures of expressivity—neuron transitions, activation patterns, and dichotomies—and demonstrates their exponential dependence on network depth. A key theoretical contribution is the identification of a unifying quantity, trajectory length, which explains this exponential growth. The authors provide both theoretical proofs and empirical validation for these claims. Additionally, the paper explores the implications of these findings for trained networks, showing that earlier layers have a greater influence on expressivity and that training trades off between stability and expressivity. Experiments on MNIST and CIFAR-10 datasets support these conclusions. Overall, the paper contributes to our understanding of how depth influences neural network expressivity and provides insights that could inform architectural design and training strategies.
Decision: Accept
The paper makes a significant theoretical and empirical contribution to the understanding of neural network expressivity. The key reasons for acceptance are:
1. Novelty and Depth of Analysis: The paper introduces trajectory length as a unifying concept for expressivity measures and rigorously proves its exponential growth with depth. This is a novel and impactful contribution to the field.
2. Strong Empirical Validation: The theoretical claims are supported by well-designed experiments on both synthetic data and real-world datasets (MNIST and CIFAR-10), lending credibility to the findings.
Supporting Arguments
1. Well-Motivated Approach: The paper is well-placed in the literature, addressing gaps in prior work on expressivity by focusing on random networks and practical architectures. It builds on foundational results while addressing their limitations, such as unrealistic assumptions about width or hardcoded weights.
2. Scientific Rigor: The theoretical results are derived rigorously, with proofs provided in the appendix. The empirical results are comprehensive, covering a range of network depths, widths, and initialization schemes.
3. Practical Implications: The findings have clear implications for neural network design, such as the importance of early layers and the trade-off between stability and expressivity during training.
Suggestions for Improvement
1. Clarity of Presentation: While the theoretical results are impressive, the presentation is dense and could benefit from simplification. For instance, the proofs in the appendix are detailed but may overwhelm readers unfamiliar with advanced mathematical techniques. A high-level summary of key steps would improve accessibility.
2. Broader Experimental Scope: The experiments focus primarily on MNIST and CIFAR-10. It would be valuable to include results on more complex datasets or tasks, such as natural language processing or reinforcement learning, to demonstrate the generality of the findings.
3. Practical Recommendations: While the paper discusses implications for architecture and training, it stops short of providing actionable guidelines. For example, how might one use the findings to design better initialization schemes or layer-wise training strategies?
Questions for Authors
1. How sensitive are the results to the choice of activation function? The paper focuses on ReLU and hard-tanh; would the findings generalize to other commonly used activations like GELU or Swish?
2. Can the observed trade-off between stability and expressivity during training be quantified more precisely? For instance, is there a way to predict the optimal balance for a given task or dataset?
3. The experiments on trained networks suggest that earlier layers have greater expressive power. How might this insight be leveraged in transfer learning scenarios, where pre-trained models are fine-tuned on new tasks?
Overall, this is a strong paper that makes a meaningful contribution to the field. Addressing the above points would further enhance its impact and clarity.