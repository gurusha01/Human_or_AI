Review of "Graph Convolutional Recurrent Network (GCRN)"
Summary of Contributions
This paper introduces the Graph Convolutional Recurrent Network (GCRN), a novel deep learning architecture that generalizes recurrent neural networks (RNNs) to handle structured sequences of data represented by arbitrary graphs. The model combines graph convolutional neural networks (CNNs) for spatial feature extraction with RNNs for temporal sequence modeling. Two architectures are proposed: (1) a stacked graph CNN and RNN, and (2) a graph convolutional LSTM (convLSTM). The paper evaluates the GCRN on two tasks: spatio-temporal sequence prediction using the moving MNIST dataset and natural language modeling on the Penn Treebank dataset. Results demonstrate that GCRN can capture spatio-temporal structures effectively, outperforming traditional CNN+RNN models in certain cases, particularly in terms of parameter efficiency and learning speed.
Decision: Accept
The paper is well-motivated, presents a novel and scientifically rigorous approach, and provides promising results for spatio-temporal sequence modeling. The key reasons for acceptance are:
1. Novelty and Generalization: The GCRN extends existing RNN and CNN architectures to graph-structured data, addressing a critical gap in modeling irregular spatial structures.
2. Empirical Validation: The experiments on moving MNIST and Penn Treebank datasets provide strong evidence of the model's effectiveness, particularly in leveraging graph structures for improved learning speed and parameter efficiency.
Supporting Arguments
1. Problem Relevance: The paper tackles an important problem—modeling spatio-temporal data on arbitrary graphs—which has applications in diverse domains such as video prediction, sensor networks, and natural language processing. The proposed GCRN is a significant step forward in this area.
2. Motivation and Placement in Literature: The authors provide a comprehensive review of related work, clearly identifying the limitations of existing methods (e.g., convLSTM for grid-structured data) and positioning GCRN as a generalization. The use of graph CNNs to handle irregular spatial structures is well-justified.
3. Scientific Rigor: The theoretical formulation of GCRN is detailed and grounded in prior work on graph CNNs and RNNs. The experiments are thorough, with appropriate baselines (e.g., FC-LSTM, convLSTM) and metrics (e.g., perplexity, prediction accuracy). The results are consistent with the claims, demonstrating the model's ability to exploit spatial and temporal correlations.
Suggestions for Improvement
1. Clarity of Model Description: While the mathematical formulation is detailed, the paper could benefit from more intuitive explanations of the two GCRN architectures, particularly for readers unfamiliar with graph CNNs.
2. Dataset Diversity: The experiments focus on two datasets, one synthetic (moving MNIST) and one textual (Penn Treebank). Including additional real-world datasets, such as sensor networks or fMRI data, would strengthen the empirical validation.
3. Model Comparison: The paper mentions that Model 1 outperforms Model 2 in language modeling due to dimensionality issues. It would be helpful to explore strategies for addressing these issues (e.g., dimensionality reduction) and compare the two models more comprehensively.
4. Computational Efficiency: While the paper highlights the parameter efficiency of graph CNNs, a more detailed analysis of computational costs (e.g., runtime, memory usage) would provide a clearer picture of the trade-offs.
Questions for the Authors
1. How does the choice of graph structure (e.g., k-nearest neighbor, adjacency matrix) impact the performance of GCRN? Have you explored alternative graph construction methods?
2. Can the proposed architectures handle dynamic graphs where the graph structure changes over time? If not, what modifications would be required?
3. In the Penn Treebank experiments, why does Model 2 perform worse than standalone LSTM? Could this be mitigated by alternative architectural designs or hyperparameter tuning?
Overall, this paper makes a significant contribution to the field of graph-based sequence modeling and is a strong candidate for acceptance. Addressing the above suggestions could further enhance its impact.