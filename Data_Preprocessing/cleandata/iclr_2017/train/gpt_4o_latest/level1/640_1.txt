The paper presents a novel multi-view Bayesian non-parametric algorithm for learning multi-sense word embeddings, leveraging multilingual corpora to improve sense disambiguation and representation. Unlike prior work, which is restricted to bilingual corpora, this approach efficiently incorporates multilingual distributional signals, allowing for a principled, data-driven determination of the number of senses per word. The authors demonstrate that their model achieves competitive performance on word sense induction tasks with significantly less data compared to monolingual models, and they provide qualitative and quantitative evidence of its efficacy. The paper also explores the impact of language family distance and crosslingual window size on performance, offering valuable insights into the benefits of multilingual training.
Decision: Accept
Key reasons for acceptance are:
1. Novelty and Contribution: The paper introduces the first multilingual approach to multi-sense word embedding learning, addressing limitations of prior bilingual methods. The use of a Bayesian non-parametric framework to infer variable senses per word is both innovative and impactful.
2. Empirical Rigor: The experimental results convincingly demonstrate the benefits of multilingual training, with improvements in word sense induction tasks and efficient use of training data. The analysis of language family effects and qualitative visualizations further strengthen the paper's contributions.
Supporting Arguments:
1. The paper is well-motivated and thoroughly grounded in the literature, addressing clear gaps in existing work on multi-sense embeddings. The authors position their approach effectively against prior monolingual and bilingual models.
2. The experimental results are robust, with comparisons across multiple datasets and metrics (e.g., Adjusted Rand Index). The multilingual model achieves performance comparable to state-of-the-art monolingual models trained on significantly larger corpora, highlighting its efficiency.
3. The qualitative analysis (e.g., PCA visualizations) provides intuitive evidence of the model's ability to cluster senses effectively, demonstrating its practical utility.
Suggestions for Improvement:
1. Clarity in Model Description: While the mathematical formulation is detailed, some parts of the model description (e.g., the role of the Ψ(ye, xf) factor) could benefit from additional explanation or illustrative examples for accessibility to a broader audience.
2. Contextual Word Similarity Results: The paper notes that the multilingual model underperforms on the SCWS dataset, attributing this to parameter tuning. It would be helpful to provide a more detailed analysis or alternative tuning strategies to address this limitation.
3. Scalability Discussion: While the paper emphasizes efficiency, a discussion of the computational complexity and scalability of the proposed approach for larger multilingual corpora would be valuable.
4. Polysemy in Foreign Languages: The authors mention extending the model to handle polysemy in foreign languages as future work. Including preliminary experiments or a discussion of potential challenges in this direction would strengthen the paper.
Questions for the Authors:
1. How sensitive is the model to the choice of hyperparameters (e.g., α and T)? Could you provide additional insights into how these were tuned and their impact on performance?
2. The paper mentions that using languages from distant families (e.g., Chinese) improves disambiguation. Could you elaborate on how this finding might generalize to other language pairs or multilingual contexts?
3. How does the model handle noise in parallel corpora, such as alignment errors or domain mismatches? Would this significantly impact performance?
In conclusion, the paper makes a significant contribution to the field of multilingual NLP and multi-sense word embeddings. With minor clarifications and additional analysis, it has the potential to be a highly impactful work.