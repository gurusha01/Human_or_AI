Review of the Paper: "Divide and Conquer with Neural Networks"
Summary of Contributions
This paper introduces a novel neural network architecture inspired by the divide-and-conquer paradigm, aimed at solving algorithmic tasks by learning from input-output pairs. The authors propose a recursive framework with two core operations: splitting an input into disjoint subsets and merging partial solutions into a final output. The architecture leverages scale invariance, enabling parameter sharing across scales and allowing the model to generalize to larger input sizes. Notably, the paper introduces a differentiable training framework that optimizes both accuracy and computational complexity. The authors demonstrate the approach on two tasks—sorting and planar convex hull—and provide empirical evidence of its ability to generalize and achieve competitive performance under weak supervision. The paper also outlines potential extensions to graph problems and hierarchical reinforcement learning, highlighting its broad applicability.
Decision: Reject
While the paper presents an innovative approach and contributes to the field of learning algorithmic tasks, it falls short in several critical areas. The primary reasons for rejection are: (1) insufficient empirical evaluation to substantiate the claims, and (2) lack of clarity in the presentation of key technical details, which hinders reproducibility and understanding.
Supporting Arguments for the Decision
1. Empirical Evaluation:  
   - The experiments are limited to two relatively simple tasks (sorting and planar convex hull). While these tasks are useful benchmarks, they do not convincingly demonstrate the scalability or generalizability of the proposed approach to more complex or real-world problems.  
   - The results lack comparison with strong baselines, such as existing neural architectures for algorithmic tasks (e.g., Pointer Networks, Neural GPUs). Without such comparisons, it is difficult to assess the true advantages of the proposed method.  
   - The paper does not provide sufficient statistical analysis or ablation studies to validate the impact of key design choices (e.g., the regularization terms, dynamic tree structures, or weak supervision framework).
2. Clarity and Reproducibility:  
   - The technical description of the split and merge operations, as well as the training procedure, is dense and difficult to follow. For instance, the mathematical formulation of the loss functions and regularization terms is not clearly explained in the context of the overall architecture.  
   - The paper does not provide enough implementation details (e.g., hyperparameter choices, training schedules) to enable reproducibility. While the authors mention that code will be made available, this is not sufficient for assessing the current submission.  
   - The theoretical claims about complexity optimization (e.g., achieving Θ(n log n) complexity) are not rigorously proven or supported by detailed empirical evidence.
3. Novelty and Scope:  
   - While the recursive divide-and-conquer framework is novel, its practical utility is not convincingly demonstrated. The paper does not explore tasks where the proposed approach would significantly outperform existing methods.  
   - The discussion of future work (e.g., joint training of split and merge, applications to graph problems) suggests that the current work is preliminary and incomplete.
Suggestions for Improvement
1. Expand Empirical Evaluation:  
   - Include experiments on more diverse and challenging tasks, such as graph-based problems or NP-hard combinatorial optimization tasks.  
   - Compare the proposed method against relevant baselines, and provide detailed quantitative and qualitative analyses.  
   - Conduct ablation studies to isolate the contributions of individual components (e.g., scale invariance, regularization terms).
2. Improve Clarity:  
   - Simplify and streamline the presentation of the architecture and training procedure. Use diagrams and pseudocode to clarify the recursive split and merge operations.  
   - Provide a detailed appendix with implementation details, including hyperparameters and training schedules.
3. Strengthen Theoretical Justification:  
   - Provide rigorous proofs or derivations for the claims about complexity optimization.  
   - Discuss the limitations of the approach, such as its reliance on scale invariance and potential challenges in tasks without this property.
Questions for the Authors
1. How does the proposed method compare to existing approaches like Pointer Networks or Neural GPUs in terms of accuracy, complexity, and scalability?  
2. Can the architecture handle tasks that do not exhibit strong scale invariance or divide-and-conquer structure?  
3. How sensitive is the model to the choice of regularization hyperparameters (e.g., βS, βM)? Have you explored automated tuning methods?  
4. What are the computational costs (e.g., training time, memory usage) of the proposed method compared to baselines?  
In summary, while the paper introduces an interesting idea, it requires more rigorous evaluation, clearer presentation, and stronger theoretical grounding to be suitable for acceptance.