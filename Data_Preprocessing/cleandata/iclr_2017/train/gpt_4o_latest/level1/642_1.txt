The paper addresses the critical problem of optimizing numerical representations and precision in deep neural network (DNN) computations to improve computational efficiency without significantly compromising inference accuracy. Specifically, it explores unconventional narrow-precision floating-point representations and proposes a novel search technique to efficiently identify optimal precision configurations. The study evaluates these configurations on production-grade DNNs like GoogLeNet and VGG, demonstrating an average speedup of 7.6Ã— with less than 1% accuracy degradation compared to single-precision floating-point baselines.
Decision: Accept.  
The paper makes a strong case for acceptance due to its significant contributions to the field of DNN hardware optimization. It rigorously evaluates a previously underexplored design space of customized precision representations on large-scale, real-world DNNs and introduces a novel, efficient search method for precision configuration. These contributions are both timely and impactful for the design of future DNN platforms.
Supporting Arguments:  
1. Well-Motivated Problem: The paper identifies a clear gap in the literature, as prior work has primarily focused on small-scale networks and fixed-point arithmetic. By addressing large-scale, production-ready DNNs, the paper provides insights that are directly applicable to real-world scenarios.  
2. Scientific Rigor: The methodology is robust, combining theoretical analysis with empirical validation. The results are comprehensive, covering multiple DNN architectures and demonstrating consistent performance improvements.  
3. Novelty and Practicality: The proposed search technique for customized precision is innovative and significantly reduces the computational cost of identifying optimal configurations, making it practical for hardware designers.  
Suggestions for Improvement:  
1. Clarity of Presentation: The paper is dense, and some sections, particularly those describing the mathematical models and hardware implications, could benefit from clearer explanations or visual aids to improve accessibility for a broader audience.  
2. Broader Applicability: While the focus on GoogLeNet and VGG is commendable, additional experiments on other types of DNNs, such as recurrent or transformer-based models, could strengthen the generalizability of the findings.  
3. Comparison with Alternatives: The paper could include a more detailed comparison with other recent approaches to DNN hardware optimization, such as spiking neural networks or redundancy-aware methods, to contextualize its contributions further.  
Questions for the Authors:  
1. How does the proposed search method scale with even larger DNNs or more complex architectures, such as transformers?  
2. Could the method be extended to dynamically adjust precision during runtime based on workload characteristics?  
3. How sensitive are the results to variations in the hardware manufacturing process (e.g., different silicon technologies)?  
In conclusion, the paper makes a significant contribution to DNN hardware optimization by addressing a critical and underexplored problem with a well-motivated, rigorous, and practical approach. While there are areas for improvement, they do not detract from the overall quality and impact of the work.