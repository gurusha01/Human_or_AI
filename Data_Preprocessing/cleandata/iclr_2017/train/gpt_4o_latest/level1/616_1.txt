Review
Summary of Contributions
The paper presents a novel weakly supervised framework for solving inverse problems in computer vision by leveraging adversarial imagination priors and memory retrieval mechanisms. The proposed approach generates "imaginations" (e.g., albedo, shading, in-painted images, and figure-ground segmentation) that, when rendered, reconstruct the original input image. The model is trained without paired annotations, relying instead on unlabelled memory repositories and adversarial losses to constrain the imagination spaces. The paper demonstrates the effectiveness of the approach on three tasks: image in-painting, intrinsic image decomposition, and figure-ground layer extraction. Key innovations include the use of fully convolutional discriminators for local statistics matching, task-specific differentiable renderers, and a memory retrieval engine to guide imaginations. The results show that the method achieves competitive performance compared to supervised baselines, even on unseen data, while requiring minimal human supervision.
Decision: Accept
The paper makes a significant contribution to weakly supervised learning for inverse problems, offering a novel and well-motivated approach that is scientifically rigorous. The results are compelling, and the method has the potential to impact a wide range of computer vision tasks. The decision to accept is based on the following key reasons:
1. Novelty and Relevance: The use of adversarial imagination priors and memory retrieval for weakly supervised inverse problems is innovative and addresses a critical gap in the literature.
2. Empirical Validation: The experiments are thorough and demonstrate the effectiveness of the approach across multiple tasks, with clear comparisons to baselines.
Supporting Arguments
1. Well-Motivated Approach: The paper is well-placed in the literature, addressing limitations of existing methods that rely on paired annotations or hand-designed priors. The use of data-driven adversarial priors and memory retrieval is a logical and impactful extension of prior work.
2. Scientific Rigor: The methodology is clearly described, and the experimental results are robust. The use of fully convolutional discriminators and task-specific renderers is well-justified and empirically validated.
3. Generality: The framework is versatile, applicable to multiple tasks, and demonstrates strong generalization to unseen data, which is a critical requirement for real-world applications.
Suggestions for Improvement
1. Clarity in Memory Retrieval: While the memory retrieval engine is central to the approach, its implementation details (e.g., how relevance is measured across tasks) could be elaborated further. For example, how does the retrieval mechanism scale with larger, more diverse memory repositories?
2. Quantitative Metrics: While qualitative results are compelling, additional quantitative metrics (e.g., PSNR, SSIM for in-painting, or IoU for segmentation) would strengthen the evaluation.
3. Ablation Studies: The paper could benefit from more detailed ablation studies to isolate the contributions of individual components, such as the memory retrieval engine, adversarial priors, and fully convolutional discriminators.
4. Limitations and Future Work: The paper briefly mentions future work on video-based tasks but does not discuss the limitations of the current approach. For example, how does the method handle highly complex or ambiguous scenes where memory retrieval may fail?
Questions for the Authors
1. How sensitive is the model to the quality and diversity of the memory repositories? Would the performance degrade significantly if the memory database is small or biased?
2. Can the proposed framework handle tasks with more complex multimodal imagination spaces, such as simultaneous depth and motion estimation in videos?
3. How does the model compare to state-of-the-art supervised methods quantitatively, beyond qualitative visualizations?
Overall, the paper is a strong contribution to the field and merits acceptance with minor revisions to improve clarity and comprehensiveness.