My primary concern with this work lies in its reliance on a hypothesis—one that is gaining traction in the literature—that solving long-term dependency problems merely requires ensuring the flow of gradients. The standard approach to achieve this is by enforcing orthogonal matrices, which, in the absence of nonlinearity, lead to unitary Jacobians. This ensures that gradients neither vanish nor explode. However, this hypothesis is largely assumed to be true without sufficient empirical validation, and aside from results on synthetic data, we lack compelling evidence to substantiate its validity.
My specific objections to this line of reasoning are as follows: a) The representational power of the model is compromised. By constraining the system to orthogonal matrices, we lose the ability to represent the same family of functions as before. For instance, we cannot model complex attractors or similar dynamics when the model is run forward without inputs, as these require eigenvalues greater than 1. Additionally, this approach makes it challenging to handle noise effectively, as it attempts to preserve every detail of the input. Ideally, given limited capacity, the model should learn to retain only the information relevant to the task. However, this framework does not allow for such selective preservation. My broader concern is that the community is overly focused on addressing the gradient preservation issue without adequately considering its potential side effects.
I would like to see one of these papers advocating for Jacobians with eigenvalues of 1 demonstrate their utility in realistic scenarios, particularly on complex, real-world datasets.