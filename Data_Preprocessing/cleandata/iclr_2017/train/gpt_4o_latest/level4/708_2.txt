The paper introduces a method for generating adversarial input images for convolutional neural networks using only black-box access (i.e., the ability to query the network for outputs corresponding to chosen inputs, without access to its internal parameters). However, the definition of adversarial examples in this context is somewhat relaxed: the goal is k-misclassification (ensuring the true label is absent from the top-k predictions) rather than misclassification to a specific target label.
A comparable black-box scenario is explored in Papernot et al. (2016c), where black-box access is leveraged to train a substitute model of the network, which is subsequently attacked. In contrast, this work employs black-box access through a local search approach. The method involves perturbing the input, analyzing the resulting changes in output scores, and retaining perturbations that move the scores closer to achieving k-misclassification.
A significant concern regarding the paper's novelty is that the proposed greedy local search strategy closely resembles gradient descent. Instead of using backpropagation, the method numerically approximates gradients by observing changes in output scores corresponding to input perturbations, as the network parameters are inaccessible. Consequently, the greedy local search algorithm, which is extensively discussed in the paper, is not particularly novel, and the work represents a relatively incremental technical contribution.