The proposed system performs on par with the bi-directional LSTM baseline for NMT, while benefiting from the inherent parallelizability of CNNs.
The core contribution lies in employing two stacked CNNs (one for encoding and one for decoding) for translation, augmented with residual connections and position embeddings. Although the use of CNNs for translation has been explored previously (as acknowledged by the authors), it appears that the competitive performance of this system relative to RNNs stems from the authors' specific combination of architectural elements (e.g., attention mechanisms, position embeddings). The paper also includes experiments analyzing the system's sensitivity to certain design choices, such as the optimal number of layers in the CNNs.
The experimental results are presented in a thorough and detailed manner.
Including one or two figures would greatly enhance the clarity of the architecture.
This work focuses less on novel representation learning techniques and more on the thoughtful integration of existing methods to achieve strong results on the reported NMT tasks. While I am reasonably confident that the paper represents solid machine learning research, I am less certain about its alignment with the scope of this particular conference.