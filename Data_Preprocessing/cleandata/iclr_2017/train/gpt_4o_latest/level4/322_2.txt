I concur with Reviewer 2 regarding the intriguing aspects of this paper. The concept of dynamically adding or removing units is indeed a compelling direction, allowing the model to adaptively grow or shrink based on the problem and data requirements, rather than relying on the user's prior knowledge.
The authors present an interesting theoretical result, demonstrating that under fan-in or fan-out regularization, the error function achieves its optimum with a finite number of parameters. This ensures that the network does not grow indefinitely and avoids overfitting the data completely. This result reminds me of traditional approaches like Lasso or Elastic Net, where regularization leads to sparse weights. However, I would have appreciated more intuition being provided for this theorem. While the result is elegant and somewhat intuitive (at least to me), it would benefit from a more thorough discussion in the paper. For instance, reducing the discussion of prior work (which is valuable but less critical than exploring the main result) could create space to better address the theoretical contributions. Please also refer to point 2 below for additional suggestions.
I have a few specific comments:
1. A valuable experiment would be to demonstrate that a model like yours, where neurons are automatically added or removed, can outperform a network of the same final size (after training) but with a fixed architecture determined at the start. This would highlight the efficiency of your approach. The key question here is whether your method optimizes the trade-off between performance and memory by reallocating resources—removing unnecessary nodes and adding needed ones.
I understand that some experiments along these lines are presented in Figure 2, though the results appear mixed. I must note that the figure is not very clear and requires careful inspection to interpret. In some cases, the non-parametric networks outperform their parametric counterparts, while in others, they underperform. Even so, I see value in your method, as it aids in discovering the network structure.
What remains unclear to me is why the non-parametric networks sometimes outperform the final fixed-size network trained from scratch. Why would the non-parametric learning process yield better results than the parametric approach when the final architecture is already known? Additional insight into this phenomenon would be helpful.
2. The discussion of Theorem 1 could be significantly improved. While the theorem is presented, its implications and meaning are not fully explored. Beyond the formal proofs in the appendix, what is the key takeaway from this result? What does it mean in plain terms? To me, the conclusion seems intuitive and almost self-evident, but I wonder if there is a deeper insight that I am missing. 
As mentioned earlier, I believe this theoretical result deserves more attention, potentially supported by additional experiments. For instance, can the regularization parameter λ be predicted based on properties of the data? Is there a data-driven way to estimate the optimal λ? My intuition is that λ plays a critical role in determining the final network structure—is this correct?
Additionally, how sensitive is the final network structure to the initialization? If you start with different random weights, do you end up with significantly different architectures? How much variability is there?
Finally, what happens when fan-in and fan-out regularizers are combined? Does the theoretical result still hold in this scenario?
I have a few further clarifications and questions:
1. You mention that adding zero units changes the value of the regularizer. Could you clarify this? For instance, does the L2 norm actually change when zero values are added?
2. Zero units are defined as having either fan-in or fan-out weights equal to zero. However, I believe what you mean is that both fan-in and fan-out weights must be zero; otherwise, removing the unit would alter the output function f. This point should be clarified more explicitly in the paper.
Based on the above comments, I have updated my rating to a 7, with the expectation that the authors will address these concerns in their revisions.