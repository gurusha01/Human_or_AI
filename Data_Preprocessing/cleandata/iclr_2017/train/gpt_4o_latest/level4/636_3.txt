The paper introduces a method to compensate for the input/activation variance caused by dropout in a network. Furthermore, it proposes a practical inference strategy involving the re-estimation of batch normalization parameters with dropout disabled prior to testing.
The authors effectively demonstrate how dropout impacts input/activation variance and adjust the initial weights to maintain unit variance, thereby preventing activation outputs from exploding or vanishing. The approach is shown to function as a robust initialization technique for deep networks, yielding performance comparable to or slightly better than existing methods. However, the limited experimental validation and the marginal improvements in accuracy relative to current techniques make it challenging to fully assess the effectiveness of the proposed method. Incorporating an analysis of output activation and gradient statistics across training epochs in multiple experiments could strengthen the argument for the network's stability when using the proposed approach.
The authors might also consider including validation related to backpropagation variance. Additionally, while comparisons are frequently made to batch normalization, it should be noted that batch normalization serves a broader purpose than merely acting as a weight initialization technique. The proposed method is a solid initialization approach, though it remains unclear if it surpasses existing alternatives.