This paper discusses the design choices behind TerpreT [1] and presents experiments focused on learning simple loop programs and list manipulation tasks. The TerpreT framework represents an important step in bridging the gap between the programming languages (PL) and machine learning (ML) communities. Unlike the recent trend in the ML community toward program induction, this work emphasizes leveraging programming language design to constrain the search space. Specifically, the authors utilize control flow structures (e.g., if-then-else, foreach, zipWithi, and foldli "templates"), immutable data (avoiding reuse of "neural" memory), and type systems (penalizing ill-typed programs and restricting the search to well-typed programs, which proves more effective). From a high-level perspective, this work lies between "making everything continuous and applying gradient descent" (ML) and "discretizing everything with structured, heuristics-guided combinatorial search" (PL).
I appreciated the inclusion of a relevant baseline (\lambda^2), but I would have liked to see a comparison with a fully neural network-based program synthesis baseline. While such a baseline might only succeed on the simplest tasks, some of the experimental tasks in this paper seem straightforward enough for "non-generating code" neural networks to perform reasonably well.
I also wish that the authors had made TerpreT and the code for reproducing their experiments publicly available.
I am curious about how the otherwise compelling recommendations for programming language design to support gradient descent-based inductive programming would generalize to more complex tasks beyond simple loops. While the tasks presented are already interesting and non-trivial, I wonder to what extent these tasks influenced the choice of constraints (e.g., those for structuring control flow) and whether these constraints would remain effective for more challenging problems.
Overall, I believe this paper meets the standards for acceptance at ICLR, though I should note that I am not a specialist in program induction or synthesis.
Writing:
- The paper can be difficult to follow at times. For example, the naming scheme for the model variants could be clarified by summarizing it in a table that includes boolean indicators for the features each variant incorporates.
- Introduction: "basis modern computing" â†’ should be "basis of modern computing."
- Page 3, training objective: The phrase "minimize the cross-entropy between the distribution in the output register rR^{(T)} and a point distribution with all probability mass on the correct output value" could be rephrased for clarity. To better engage the broader ML community, you might describe this as treating the output of rR^{(T)} as a classification problem with the correct output value as the target. You can then provide details about the specific criterion/loss function used (e.g., cross-entropy). 
[1] "TerpreT: A Probabilistic Programming Language for Program Induction," Gaunt et al., 2016