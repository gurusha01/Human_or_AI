Review:
Summary:  
This paper introduces a read-again attention-based document representation combined with a copy mechanism for the task of summarization. The proposed model processes each sentence in the input document twice, generating a hierarchical representation rather than relying on a bidirectional RNN. During decoding, the model leverages the document representation obtained through the read-again mechanism and incorporates a pointer mechanism to handle out-of-vocabulary (OOV) words from the source document. The approach focuses on abstractive summarization, and the authors demonstrate performance improvements on the DUC 2004 dataset, along with an analysis of the model under various configurations.
Contributions:  
The primary contribution of this work is the introduction of the read-again attention mechanism, where the model processes each sentence twice to derive a more refined document representation.
Writing:  
The writing in this paper requires significant improvement. There are multiple typographical errors, and the explanations of the model and its architecture lack clarity. Certain sections of the paper appear unnecessarily verbose.
Pros:  
- The proposed model is a straightforward extension of the model introduced in [2] for summarization.  
- The results outperform the baseline models.  
Cons:  
- The performance improvements are relatively modest.  
- The justifications provided for the proposed approach are insufficient.  
- The paper requires substantial improvement in its writing. Many sections lack clarity and precision, and the overall organization could be improved. Some parts of the text are written in an informal tone.  
- The paper is heavily application-oriented, with limited contributions from a machine learning perspective.  
Question:  
- How does the training speed compare to that of a standard LSTM?  
Criticisms:  
A similar concept to the read-again mechanism proposed in this paper has already been explored in [1] within the context of algorithmic learning. Applying this idea to summarization does not constitute a significant contribution. Furthermore, the justification for the read-again mechanism is weak, and it is unclear why the additional gating parameter, α_i, is necessary for the second pass.  
The pointer mechanism for handling unknown/rare words, as introduced in [2], has been adopted in this work for the read-again attention mechanism. However, the paper does not clearly identify the source of the observed performance gains—whether they stem from the read-again mechanism or the use of the pointer mechanism.  
The paper is highly application-focused, with limited contributions from a machine learning perspective. To strengthen the impact of the proposed method, it would be valuable to evaluate the read-again mechanism on tasks beyond summarization, such as neural machine translation (NMT), to determine whether the observed improvements generalize.  
Lastly, the writing quality of the paper is subpar and requires significant revision. Overall, the paper is not well-written.  
Minor Comments:  
Below are some specific corrections and suggestions:  
- On page 4: Replace "… better than a single value …" with "… scalar gating …"  
- On page 4: Replace "… single value lacks the ability to model the variances among these dimensions." with "… scalar gating couldn't capture the …"  
- On page 6: Replace "… where h0^2 and h0^'2 are initial zero vectors …" with "… h0^2 and h0^'2 are initialized to a zero vector at the beginning of each sequence …"  
There are also inconsistencies in the paper, such as references to "Tab. 1" in some sections and "Table 2" in others.  
The naming of the models in Table 1 could be improved for better clarity. Additionally, the placement of Table 1 is slightly awkward and should be adjusted.  
References:  
[1] Zaremba, Wojciech, and Ilya Sutskever. "Reinforcement learning neural Turing machines." arXiv preprint arXiv:1505.00521 362 (2015).  
[2] Gulcehre, Caglar, et al. "Pointing the Unknown Words." arXiv preprint arXiv:1603.08148 (2016).