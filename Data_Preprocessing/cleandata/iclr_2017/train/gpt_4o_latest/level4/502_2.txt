This paper introduces a novel evaluation metric for dialogue systems and demonstrates that it achieves a higher correlation with human annotations. While I agree that traditional MT-based metrics like BLEU are overly simplistic and fail to capture sufficient semantic information, the proposed metric appears overly complex and difficult to interpret.
Additionally, equation (1) could potentially be repurposed as a retrieval-based dialogue system. Essentially, the approach outlined in this paper involves training one dialogue model to evaluate another. This raises a fundamental question: why should we trust the evaluation model? This concern is closely tied to the last point in my detailed comments.
Detailed Comments:
- How can we justify what this metric captures or evaluates? For instance, with BLEU, we understand that it measures n-gram overlap. However, for the proposed model, it seems unclear what specific aspects are being captured. If this is indeed the case, it becomes challenging to address questions such as whether the metric's performance is heavily dependent on the data.
- Why not consider building the model incrementally? As described in equation (1), the metric relies on both context and reference to compute a score. Would it be possible to design a version of the score function that uses only the reference? This would ensure the metric draws from the same information sources as BLEU or ROUGE.
- Regarding equation (1), is there a way to design the metric as a nonlinear function? From what I observe, the comparison between BLEU (or ROUGE) and the proposed metric in Figure 3 seems analogous to comparing an exponential scale with a linear scale.
- The two reasons provided in section 5.2 are unconvincing when considered together. Based on these reasons, I would suggest examining the correlation with average scores. A more robust approach would involve presenting results both with and without averaging.
- In Table 6, the metric appears to favor short responses. If this observation holds, the metric essentially behaves in the opposite manner to BLEU, which penalizes short sentences. On the other hand, human annotators often assign higher scores to shorter responses, as longer sentences are more likely to include irrelevant words. Could the length factor be controlled during annotation? Otherwise, it is unsurprising that the correlation behaves as observed.