A novel sparse coding model is proposed, which learns features in conjunction with their transformations. The authors observe that performing inference over per-image transformation variables is challenging. To address this, they propose tying these variables across all data points, effectively treating them as global parameters, and incorporating multiple transformations per feature. Additionally, they introduce the concept of a transformation tree, where each path from the root to a leaf generates a feature by multiplying the root feature with the transformations along the edges of the path. The one-layer tree model achieves comparable reconstruction error to traditional sparse coding while utilizing fewer parameters.
This work represents a meaningful contribution to the fields of sparse coding and transformation learning. The authors tackle a challenging inference problem that arises in transformation models. However, I remain unconvinced about the broader utility of the proposed approach. The authors assume that "jointly learning sparse features and transformations" is a significant objective, but this assumption is neither substantiated nor supported by experimental evidence. The method does not appear to enable novel applications, enhance our understanding of the brain's what/where pathways, or improve the modeling of natural images.
The authors assert that the model captures pose information. However, while the model explicitly encodes the transformations linking features in the tree structure, test-time inference is limited to the (sparse) coefficients associated with each (feature, transformation) pair, akin to traditional sparse coding. It remains unclear what is gained by associating coefficients with transformations, especially given the existence of other models that similarly implement a "what/where" decomposition.
It would be beneficial to verify whether the x{v->b} parameters change meaningfully from their initial values. The loss surface appears problematic even with tied transformations, suggesting that these parameters might not undergo significant updates. A comparison with a baseline model where x{v->b} are fixed and sampled from a reasonable range (either randomly or uniformly spaced) would help clarify the advantages of the proposed approach.
One of the conceptually intriguing aspects of the paper is the transformation tree, but the benefits of deeper trees are not convincingly demonstrated. The results suggest that the approach has only been applied successfully to simple toy datasets with vertical and horizontal bars.
Lastly, the paper does not address how the method could be extended to multiple layers. While transformation operators T can be defined in the first layer based on the input space, it is unclear how these operators would be defined in subsequent learned feature spaces. Additionally, the hierarchical processing of pose information and the learning dynamics of a deeper version of the model remain unexplored.
In conclusion, I do not recommend this paper for publication. The problem being addressed is not clearly articulated, the method offers only moderate novelty, and the novel components are not convincingly demonstrated to provide significant benefits.