The paper presents an intriguing approach, connecting findings from neuroscience and biology to a method for sparse coding that is both adaptive and capable of autonomously generating or removing codes as new data arrives from a nonstationary distribution.
I have several observations:
1. The algorithm could benefit from a more detailed discussion to provide a clearer understanding of its contribution. While the technique is functional, it is not fundamentally novel in concept. The process of adding codes when necessary and removing them when they become redundant is a well-established idea.
2. Can the organization of the input data be tied to the behavior of the proposed method? In the paper, the authors first present buildings, followed by natural images, which are less structured and more challenging. Is this approach effectively a form of curriculum learning? What would happen if the data transitions without an inherent progression from simpler to more complex structuresâ€”for instance, moving from flowers to birds, then to fish, leaves, and trees? 
It seems intuitive that the method would perform better when the training data is structured in this way, progressing from an artificial, simpler domain to a more complex, less structured one.
Overall, the paper is engaging, and the idea holds promise with some noteworthy insights. However, I am not convinced it is ready for publication in its current form.