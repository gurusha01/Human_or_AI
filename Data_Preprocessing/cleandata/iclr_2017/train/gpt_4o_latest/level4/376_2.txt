CONTRIBUTIONS  
The paper conducts large-scale experiments to evaluate the capacity and trainability of various RNN architectures. The capacity experiments indicate that RNNs, regardless of architecture, can store between three and six bits of information per parameter, with ungated RNNs achieving the highest per-parameter capacity. Additionally, all architectures are shown to store approximately one floating-point number per hidden unit. Trainability experiments reveal that ungated architectures (RNN, IRNN) are significantly more challenging to train compared to gated architectures (GRU, LSTM, UGRNN, +RNN). The paper also introduces two new RNN architectures (UGRNN and +RNN); experimental results suggest that the UGRNN achieves a similar per-parameter capacity to the ungated RNN while being much easier to train, and that deep (8-layer) +RNN models are more trainable than existing architectures.
CLARITY  
The paper is clearly written and easy to understand.
NOVELTY  
To the best of my knowledge, this is the first paper to empirically quantify the number of bits of information that can be stored per learnable parameter in RNNs. The approach of measuring network capacity by identifying the dataset size and hyperparameters that maximize mutual information is particularly innovative.  
The proposed UGRNN bears similarities to, but is distinct from, the minimal gated unit introduced by Zhou et al. in "Minimal Gated Unit for Recurrent Neural Networks," International Journal of Automation and Computing, 2016.
SIGNIFICANCE  
I have mixed opinions regarding the significance of this work. While the experiments are interesting, they do not uncover particularly surprising or unexpected findings about recurrent networks. It is difficult to see how the results will influence either my understanding of RNNs or my practical use of them in future work. That said, the rigorous experimental validation of intuitive results about RNNs is valuable, especially since few researchers have access to the computational resources required for such large-scale experiments.
The capacity experiments (both per-parameter and per-unit) primarily involve modeling random data. However, in most practical applications of RNNs—such as machine translation, language modeling, or image captioning—the data is far from random. It is unclear whether an architecture's ability to model random data translates to improved performance on real-world tasks. Indeed, while Section 2.1 demonstrates variability in architectures' capacity to model random data, the text8 experiments in Section 3 show that these same architectures exhibit little variation in their ability to model real-world data.
The experimental results presented in the paper do not sufficiently establish the significance of the proposed UGRNN and +RNN architectures. While it is noteworthy that the UGRNN achieves comparable bits per parameter to the ungated RNN and that deep +RNNs are more trainable than other architectures, the real-world task experiments (language modeling on text8) fail to demonstrate a significant advantage over existing architectures like GRU or LSTM.
SUMMARY  
While I wish the experiments had yielded more surprising insights about RNNs, there is value in rigorously validating intuitive findings. The proposed UGRNN and +RNN architectures show promise in synthetic tasks, but their performance on real-world tasks is less convincing. Overall, I believe the strengths of the paper outweigh its weaknesses, and the ideas presented are valuable to the research community.
PROS  
- The paper is the first, to my knowledge, to explicitly measure the bits per parameter that RNNs can store.  
- The paper experimentally validates several intuitive insights about RNNs:  
  - RNNs, regardless of architecture, can store approximately one number per hidden unit from the input.  
  - Comparisons between RNN architectures should be based on parameter count rather than hidden unit count.  
  - With careful hyperparameter tuning, all RNN architectures perform similarly on text8 language modeling.  
  - Gated architectures are easier to train than ungated RNNs.  
CONS  
- The experiments do not uncover particularly surprising or unexpected findings.  
- The motivations for the UGRNN and +RNN architectures are not well-articulated.  
- The utility of the UGRNN and +RNN architectures is not convincingly demonstrated.