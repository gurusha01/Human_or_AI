This paper introduces a collection of techniques under the umbrella of "sampling generative models," with a focus on analyzing the learned latent space and generating output images with specific properties for GANs. Rather than presenting a single, cohesive idea, the paper proposes a variety of methods that appear to produce visually appealing results. While the paper contains some intriguing concepts, it also has several shortcomings.
The spherical interpolation concept is noteworthy but, upon closer examination, raises significant concerns. The proposed slerp interpolation equation (page 2) assumes that the two points, q1 and q2, lie on the same sphere, where the parameter θ represents the angle corresponding to the great arc connecting the two points. However, the latent space of a GAN—whether trained with a uniform or Gaussian distribution—is not inherently distributed on a sphere, and many points exhibit varying distances from the origin. The authors justify their approach by referencing the well-known phenomenon that, in high-dimensional spaces, most points in a uniform distribution tend to lie on a thin outer shell of the unit cube. While this is true due to the volume distribution in high dimensions, it does not imply that the density of data is higher in the outer shell compared to the inner regions. In a uniform distribution, data density remains constant throughout, meaning points on the outer shell are no more likely than those in the inner part. Conversely, under a Gaussian distribution, data density is higher near the center and decreases significantly toward the periphery. A well-trained model should generate plausible samples by focusing on the most likely points in the distribution. From this perspective, spherical interpolation should not outperform the commonly used linear interpolation. However, the authors seem to overlook this distinction, as evidenced by their responses to questions. While the results presented in the paper suggest that spherical interpolation produces better visual outcomes, these conclusions are based on only three pairs of examples, making it difficult to draw definitive insights. If spherical interpolation indeed performs better, it would challenge our current understanding of the learned model.
Beyond this, the J-diagram and nearest neighbor latent space traversal appear to be effective tools for exploring the latent space of a learned model. The section on attribute vectors, which focuses on transforming images to exhibit desired attributes, is also compelling and introduces new ways to interpret the GAN latent space.
In summary, most of the techniques proposed in this paper serve as useful visualization tools. However, the contributions are primarily centered on the design of these visualizations, with limited advancements on the technical or modeling front. The spherical interpolation is the only mathematical formulation presented, yet its validity is debatable. Additionally, the visualization tools lack quantitative evaluation, leaving the impression that these results may lean more toward artistic exploration than rigorous scientific contribution.