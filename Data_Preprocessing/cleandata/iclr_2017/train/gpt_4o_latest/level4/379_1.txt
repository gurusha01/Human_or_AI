The authors present the implementation of TensorFlow Fold, which enables the execution of various computations without requiring modifications to the computation graph. This is accomplished by designing a generic scheduler as a TensorFlow computation graph that takes a graph description as input and executes it.
They effectively demonstrate the advantages of this approach for tasks where the computation varies for each data point, such as in the case of TreeRNN.
In their experiments, the authors compare their method against two baselines: using a static batch (repeating the same graph structure multiple times) and using a batch size of 1.
The reason I assigned a score of 7 rather than a higher score is the lack of comparison to the primary alternative to their approach. Specifically, one could generate a new TensorFlow graph for each dynamic batch instead of relying on their graph as the scheduling mechanism. In other words, an alternative would be to explicitly create each non-uniform batch as a TensorFlow graph and execute it using standard TensorFlow functionality.