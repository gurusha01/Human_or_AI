The paper expands the imitation learning framework to scenarios where the demonstrator and learner operate from differing perspectives. This represents a significant contribution with a range of practical applications. The core innovation lies in leveraging adversarial training to develop a policy that can effectively handle this disparity in viewpoints. While the problem formulation is relatively novel within the imitation learning domain (which traditionally assumes a first-person perspective), it shares conceptual ties with the transfer learning literature, as discussed in Section 2.
The proposed approach is well-articulated and builds naturally on recent advancements in imitation learning and adversarial training.
However, I would recommend including comparisons to the following methods in Figure 3 to strengthen the paper's impact:
1) Standard first-person imitation learning using data from agent A, with the policy applied back on agent A. This serves as an upper bound for performance, as it assumes the correct perspective is maintained throughout.
2) Standard first-person imitation learning using data from agent A, but applying the policy on agent B. While this is expected to perform worse than third-person learning, it would be valuable to quantify the performance gap.
3) Reinforcement learning using data from agent A, with the policy applied back on agent A. This approach might outperform third-person imitation learning depending on the scenario (e.g., the balance between imitation and exploration challenges, or the degree of perspective mismatch between agents). Although this method appears to align with how expert data is generated for the demonstrator, its performance on the learner is not explicitly presented in the current results (e.g., Figure 3).
Incorporating these comparisons would, in my opinion, substantially enhance the paper's overall contribution and impact.