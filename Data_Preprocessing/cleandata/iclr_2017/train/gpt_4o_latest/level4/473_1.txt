This paper provides a theoretical rationale for reusing the input word embeddings in the output projection layer. It achieves this by introducing an additional loss term aimed at minimizing the discrepancy between the predictive distribution and an estimate of the true data distribution. This approach is appealing as it effectively smooths the labels provided as input. However, the method used to construct the estimate of the true data distribution appears tailored to justify the weight tying presented in Eqs. 3.6 and 3.7.
It is not immediately clear why the projection matrix \( L \) in Eq. 3.6 (referred to here as \( L' \)) should necessarily be identical to the one in Eq. 2.1. For instance, \( L' \) could be derived from word2vec embeddings trained on a large corpus or learned as a separate set of parameters. If \( L' \) is treated as a newly learned matrix, the result in Eq. 4.5 seems to suggest using an independent matrix for the output projection layer, which aligns with the conventional approach.
The experimental results are strong and lend credibility to the approximate derivation in Section 4, particularly the distance plots shown in Figure 1.
Minor comments:
- Third line in the abstract: "where model" → "where the model"
- Second line in Section 7: "into space" → "into the space"
- Shouldn't the RHS in Eq. 3.5 be \( \sum \tilde{y}{t,i} \left(\frac{\hat{y}t}{\tilde{y}{t,i}} - ei\right) \)?