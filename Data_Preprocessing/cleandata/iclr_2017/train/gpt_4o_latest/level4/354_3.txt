The paper introduces a method to create an ensemble of neural networks without incurring additional training costs (equivalent to training a single network) by saving snapshots of the network during training. The network is trained using a cyclic (cosine) learning rate schedule, and snapshots are captured at the lowest points of the learning rate cycles. The authors demonstrate that these snapshot ensembles outperform a single network on image classification tasks across various datasets.
Strengths:
1. The proposed method is straightforward, making it easy to implement and reproduce, especially given the detailed experimental setup provided in the paper.
2. The paper is well-written, with a clear explanation of the methodology and comprehensive experimental evaluation.
Suggestions for improvement / additional comments:
1. While comparing methods under a fixed computational budget is reasonable, the paper would benefit from a more thorough comparison with "true ensembles" (ensembles of independently trained networks). Specifically, Table 4 should be expanded to include results from "true ensembles."
2. The comparison with "true ensembles" is limited to DenseNet-40 on CIFAR100 in Figure 4. The results indicate that the proposed snapshot ensemble achieves approximately 66% of the performance improvement of a "true ensemble" over the single baseline model. However, the authors' claim in the abstract that snapshot ensembles "almost match" the performance of independently trained ensembles is not entirely accurate based on this result.
3. To better understand the diversity of snapshot ensembles, it would be helpful to compare their diversity with other ensembling techniques, such as (1) "true ensembles" and (2) ensembles generated using dropout, as described by Gal et al., 2016 ("Dropout as a Bayesian Approximation").