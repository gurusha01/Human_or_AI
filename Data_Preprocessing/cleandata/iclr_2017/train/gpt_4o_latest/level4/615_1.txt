The paper introduces a novel second-order optimization method, L-SR1, for training deep neural networks. The authors claim that the method addresses two key challenges in this context: the poor conditioning of the Hessian and the abundance of saddle points. The proposed approach can be interpreted as a combination of the SR1 algorithm described by Nocedal & Wright (2006) and limited-memory techniques outlined by Byrd et al. (1994). However, I find the lack of a more formal, theoretical justification in this work to be a significant shortcoming (providing additional intuition would also enhance the paper). In contrast, such theoretical insights are presented in prior works like Dauphin (2014) and Martens. Furthermore, the experimental results are not particularly compelling, as the performance in terms of wall-clock time is omitted, and the reported advantages over competing methods are marginal, even when measured by the number of epochs. While I understand that the authors are still refining their implementation, the question remains: given the unconvincing experimental evidence, what incentive would practitioners have to adopt L-SR1 for training deep models? In its current state, the work is not ready for publication.