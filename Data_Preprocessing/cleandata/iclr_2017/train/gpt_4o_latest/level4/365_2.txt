This paper presents a set of techniques aimed at compressing fully-connected neural networks while preserving comparable performance. These include a density-diversity penalty and a corresponding training algorithm. The central contribution of the work is the explicit penalization of both the overall magnitude of the weights and the diversity among them. This strategy leads to sparse weight matrices with relatively few unique values. Although the authors propose a more efficient method for computing the gradient of the diversity penalty, they still resort to applying the penalty with a low probability (1-5%) per mini-batch.
The proposed method achieves notable compression of fully connected layers with minimal accuracy degradation. However, I am concerned that the computational cost of sorting weights (even for only 1 or 2 out of every 100 mini-batches) might render this approach impractical for larger networks. Perhaps the induced sparsity could be leveraged to mitigate some of this overhead?
The most significant limitation of this paper is the lack of independent exploration of the various components of the method. Sparse initialization, weight tying, probabilistic application of the density-diversity penalty, setting the mode to 0, and the alternating schedule between weight-tied standard training and diversity penalty training are all introduced, but their individual contributions are not adequately discussed. Additionally, the only quantitative metric provided is the compression rate, which depends on both sparsity and diversity, making it difficult to disentangle their individual effects. I would strongly encourage the authors to analyze how each component of the algorithm impacts diversity, sparsity, and overall compression.
A clarification is needed: Section 3.1 states that the density-diversity penalty is applied with a fixed probability per batch, while Section 3.4 describes structured phases alternating between density-diversity penalty application and weight-tied standard cross-entropy training. Does the scheme in Section 3.4 apply the density-diversity penalty probabilistically only during the density-diversity phase?
Preliminary rating:  
This is an intriguing paper, but it lacks sufficient empirical analysis of its various components. Consequently, the algorithm comes across as a collection of heuristics that collectively achieve good performance without a clear explanation of why it works.
Minor notes:  
Please adjust the size of Equation 4 to fit within the margins (using `\resizebox{\columnwidth}{!}{...}` in LaTeX works well for this purpose).