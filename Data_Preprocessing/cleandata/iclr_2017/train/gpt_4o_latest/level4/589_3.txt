The authors tackle the challenge of modeling time-varying signals on a graph, where the signal at a node evolves based on the inputs and hidden states of its neighboring nodes, with the neighborhood size treated as a hyperparameter. Their approach builds upon the work of Shi et al. (2015), extending it to general graph structures instead of a fixed grid by employing the graph convolution framework of Defferrard et al. (2016). However, this is not a strict generalization, as the graph-based formulation assumes uniform treatment of all edges, whereas the convolutional kernels in Shi et al. inherently encode directionality. The authors present results on the moving MNIST dataset and the Penn Tree Bank language modeling task.
While the paper, model, and experiments are adequate, I have several concerns:
1. From a technical standpoint, the proposed model lacks significant novelty. While I am generally willing to overlook this if the paper compensates with rigorous experimental evaluation, clear exposition, and meaningful insights into the strengths and weaknesses of the approach compared to prior work, I feel this paper falls short in these areas.
2. The experimental results section is brief and lacks sufficient interpretation. Although I am not fully up to date with the latest Penn Tree Bank language modeling benchmarks, I am aware that it is a highly competitive and widely studied dataset. It is surprising that the authors compare their results only to Zaremba et al. (2014), as I would expect comparisons to multiple other recent works.
3. The paper's writing is unclear in places, and the authors do not make a strong enough effort to compare models or provide insights into why the proposed approach performs better. For instance, if I understand correctly, the word probabilities depend on the graph's neighborhood. What is the effective width of this graph? For example, if a word is sampled in one region of the graph, how does this information propagate to other regions along the edges? Additionally, it is unclear how the model achieves reasonable performance on the moving MNIST dataset without distinguishing the directionality of edges. While the authors acknowledge this limitation, they fail to provide a convincing explanation of how the model handles it. How does a pixel "know" to activate in the next frame? These aspects require deeper consideration and clearer presentation.
In conclusion, the paper offers limited technical contributions, the experimental evaluation is not sufficiently comprehensive, and the insights provided are minimal.