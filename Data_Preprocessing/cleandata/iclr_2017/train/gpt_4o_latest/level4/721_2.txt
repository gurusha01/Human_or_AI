This paper introduces a novel approach to unsupervised learning by modifying sparse coding to explicitly model transformations (e.g., shift, rotation), in contrast to the simple pooling mechanisms typically employed in convolutional networks. The authors present results on natural image datasets, illustrating that the algorithm learns both features and their associated transformations. A comparison with traditional sparse coding reveals that the proposed method represents images with fewer degrees of freedom.
While this is an intriguing and promising direction, the work feels preliminary and lacks the completeness and impact of a fully realized contribution. For instance, one of the stated motivations is to jointly represent an object's identity and pose. Although the paper makes progress toward this goal, it falls short of achieving it, leaving several conceptual and methodological gaps unaddressed.
There are also several aspects of the paper that are unclear:
- The central concept appears to be the use of a transformational sparse coding tree to facilitate inference of the Lie group parameters \(x_k\). However, the explanation of this process is vague. For example, the statement, "The main idea is to gradually marginalize over an increasing range of transformations," is suggestive but lacks clarity. What exactly is meant by "marginalization" in this context? This needs to be defined more rigorously.
- The relationship between the Lie group operator, the tree leaves, and the weights \(w_b\) is not well explained. While the learning rule provides the gradient for the Lie group operator, it is unclear how this gradient is used to train the tree's leaves. This is particularly confusing because the Lie group operator is initially introduced but later described as intractable for inference due to the presence of numerous local minima, which motivates the tree-based approach. Why, then, is the Lie group operator being learned at all?
- The statement, "Averaging over many data points smoothens the surface of the error function," is difficult to interpret. Why would averaging over multiple data points be appropriate, given that each data point presumably has its own unique transformation?
- The paper does not clearly describe the training data or its generation process. Are patches with known transformations generated, and is the algorithm evaluated on its ability to recover these transformations? This needs to be elaborated upon.
The results in Figure 4 are intriguing, but due to the aforementioned ambiguities, it is challenging to fully interpret their significance or understand their implications.
I encourage the authors to revise the paper for greater clarity and to further develop these promising ideas, which have the potential to make a significant contribution to the field.