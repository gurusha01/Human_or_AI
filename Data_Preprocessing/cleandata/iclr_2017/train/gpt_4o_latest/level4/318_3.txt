The paper introduces an enhancement to the Gated Graph Sequence Neural Network by incorporating the capability to perform complex graph transformations within the model. The core idea is to develop a method that can construct or modify a graph structure as an internal representation to address a specific problem, with a focus on question-answering tasks in this work. The author outlines five distinct differentiable transformations that are learned from a training dataset, typically in a supervised manner where the graph's state is provided at each timestep. A specific instance of the model is described, which processes a sequence as input and iteratively updates an internal graph state to produce a final prediction. This approach is demonstrated to be effective for question-answering tasks (e.g., BaBi) and yields promising results.
The proposed approach is particularly compelling as it enables the model to maintain its current state as a complex graph while remaining differentiable, making it amenable to learning through gradient-descent techniques. This can be viewed as a successful integration of continuous and symbolic representations. Furthermore, the model appears more general than recent efforts to incorporate symbolic elements into differentiable models (e.g., Memory Networks, NTM), as the state's structure here is not fixed and can evolve dynamically. However, my primary concern lies in the training methodology, which relies on providing the graph's state at each timestepâ€”a feasible approach for specific tasks like BaBi but not scalable to more complex problems. Another concern is the paper's overall content, which might be better suited for a journal format rather than a conference, as its density makes it somewhat challenging to read in its current form.