This paper introduces a model for iteratively refining translation hypotheses, which offers several advantages. These include allowing the translation model to condition on both "left context" and "right context" and potentially enabling faster and/or more accurate decoding. The authors argue that this approach mirrors the refinement process often employed by human translators and text generators in general.
This is an intriguing and important idea that is not yet widely explored in neural network models, making this paper a valuable contribution. However, while this work represents a promising first step, I believe it requires further analysis before it is ready for final publication. For instance, there are numerous connections to prior research in NLP, MT, and broader ML fields that could better situate this work (see detailed suggestions below). More critically, the model described in Section 3 could be viewed as a globally normalized, undirected (~CRF) translation model trained with a pseudo-likelihood objective. Under this interpretation, the model aligns closely with traditional discriminative translation models that employ "undirected" features, and the decoding process resembles a standard greedy hill-climbing algorithm (albeit with an additional heuristic for selecting which variable to update), which is not particularly novel.
A second concern is that the paper does not sufficiently discuss the limitations of the proposed model. For example, the editing procedure cannot easily handle word insertions or deletions in translations. While this is a reasonable simplification for tractability, it is a significant drawback since missing or extra words—particularly function words—are common issues in the baseline models being used. Additionally, the standard criticisms of absolute positional models (as opposed to relative positional models) are especially relevant here and should be addressed, as they could help justify some of the design choices made in this work.
Overall, this paper represents an initial step in an interesting direction, but it requires a more comprehensive analysis to fully demonstrate its value. Such an analysis could also reveal important model variants (e.g., is the goal truly a global translation model, or would a post-editing model capable of more complex operations be more appropriate?).
Related Work:
The paper could better contextualize its contributions by engaging more deeply with prior work in related areas. Iterative refinement has been explored in other domains with complex output spaces, such as the DRAW model by Gregor et al. and the conditional adversarial network models for image refinement proposed by Isola et al. In NLP, stochastic hill-climbing approaches have been proposed, such as Zhang and Lei et al. (2014), who use random initial guesses followed by greedy hill climbing with local refinements, and the structured prediction cascades of Weiss and Taskar (2009), as well as general coarse-to-fine strategies. In MT, Arun et al. (2009) use a Gibbs sampler to refine initial guesses for decoding with more complex models. While the explicit use of an error model is novel in the context of correction, it is conceptually similar to the discriminative word lexicon models of Mauser et al. (2009) and their neural counterparts by Ha et al. (2014). Additionally, the field of "automatic post-editing" has seen significant work, including the WMT2016 shared task, which provides standard test sets, baselines, and datasets that could be leveraged to train post-editing models using human-generated data. Incorporating these techniques could serve as a useful comparison for the models proposed in this paper.
The phrase "the target sentence is also embedded in distributional space via a lookup table" could be clearer. A better phrasing might be: "the target sentence is represented using distributed word representations via a lookup table." The term "distributional" implies that the representations are derived from word distributions in the corpus, whereas in this case, the representations are learned specifically for this task, which only indirectly relates to distributional properties.
Section 3 Model:
In Section 3, the model predicts the distribution over target word types at an absolute position \(i\) in the output sentence, conditioned on the target language context and the source language context. While the model is introduced as a mechanism for refining existing hypotheses, it is not immediately clear from this section that the training data consists of gold-standard translations. The term "training set" could be interpreted in multiple ways, and this ambiguity is only resolved later in the paper. Clarifying this earlier would improve the paper's readability.
The use of a fixed-sized window to represent the target word in context appears to make an assumption similar to Model 1, where only lexical features (and not alignment or positional features) influence attention. This should be explicitly stated, as it would clarify the model's assumptions and suggest potential refinements, such as incorporating representations of \(i\) and \(j\) into \(S^j\) and \(T^i\). This would allow the model to learn responses akin to Models 2 or 3. Interestingly, by excluding these features, the model may behave more like a relative positional model than an absolute positional model, which could be a desirable property.
The decision to use a fixed window for representing the target sentence also warrants further discussion, especially since a global context is used for the source sentence. Explaining this design choice would provide additional transparency and might suggest avenues for improvement.
The relationship between the training objective and pseudo-likelihood (PL; Besag, 1975) could also be highlighted. Since the objective appears to be a PL objective for a specific global model, this opens up possibilities for alternative decoding algorithms or, at the very least, a different analysis of the proposed decoding objective.
Section 4 Model:
The model in Section 4 conditions on the true context of a position in the reference target, the current target hypothesis, and the source. However, the rationale for this design is unclear, as only two of these variables are available at test time. Replacing \(y{\text{ref}}\) with \(yg\) seems difficult to justify and should be further explained.