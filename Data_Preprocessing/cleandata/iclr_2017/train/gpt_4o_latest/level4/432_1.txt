This paper is exceptionally well-written and succeeds in unifying certain value-based and policy-based (regularized policy gradient) methods by identifying novel connections between the value function and policy that have not been previously established. The theoretical contributions are both original and insightful, with the potential to significantly impact the RL field beyond the specific algorithm introduced in the paper. Furthermore, the authors leverage this theory to develop a unified framework combining Q-learning and policy gradient methods, which demonstrates performance that matches or surpasses state-of-the-art algorithms on the Atari benchmark suite. The empirical section is clearly articulated, detailing the optimizations that were applied.
A minor observation concerns the stationary distribution used for a policy. There are nuanced differences between employing a discounted versus non-discounted distribution, which are not critical in the tabular setting but may require careful consideration in future work involving function approximation. However, this does not pose a significant issue for the current version of the paper.
In conclusion, this paper is highly deserving of acceptance and is likely to have a broad impact on the RL community. It paves the way for further theoretical advancements and the development of new algorithms.