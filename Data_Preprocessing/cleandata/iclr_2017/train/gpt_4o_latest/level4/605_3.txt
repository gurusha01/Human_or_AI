This paper presents an innovative extension of the variational autoencoder framework to handle arbitrary tree-structured outputs. The authors evaluate the model's density estimation capabilities using two datasets: a synthetic arithmetic expression dataset and a first-order logic proof clause dataset.
Pros:
+ The manuscript is well-written and easy to follow.
+ The definition of tree structures is comprehensive enough to encompass a wide range of tree types encountered in practical applications.
+ The proposed tree generation and encoding mechanism is both elegant and clearly explained.
+ While the experimental scope is somewhat narrow, the evaluation is reasonably thorough. The inclusion of IWAE for improved log-likelihood estimation is a particularly commendable aspect.
Cons:
- The improvement in performance over a baseline sequential model is minimal.
- The experimental evaluation is limited in scope, both in terms of datasets and metrics. Specifically: (a) only one real-world dataset is tested, where the proposed model underperforms compared to the baseline, and (b) there is no analysis of the learned latent representation's utility for downstream tasks, such as classification.
- A claimed advantage of the model is its ability to generate trees in time proportional to tree depth, but this is not empirically validated in the experiments.
The tree generation and encoding procedures leverage repeated use of common operations in a clever manner. The weight-sharing and gating mechanisms appear to be critical for the model's performance, but their importance is difficult to assess without an ablation study (e.g., these components are not directly compared in Tables 1 and 2). Extending the experiments to another domain, such as source code modeling or parse trees conditioned on sentences, would strengthen the case for the model's broader applicability. While the model shows promise and could be applied to diverse data types, the limited experimental breadth raises concerns.
* Section 3.1: "We distinguish three types" should be revised to "two types."
* Section 3.6: The explanation of the variable-sized latent state is somewhat unclear, particularly regarding how the number of latent variables (z's) is determined.
* Sections 4.2-4.3: When generating the datasets, did you ensure that the test set is disjoint from the training set?
* Table 1: Why are the variable latent results missing for depth-11 trees?