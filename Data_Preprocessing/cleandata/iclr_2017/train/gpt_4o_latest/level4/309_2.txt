This paper focuses on enhancing feature learning in deep reinforcement learning by supplementing the primary policy's optimization problem with additional terms corresponding to (domain-independent) auxiliary tasks. These auxiliary tasks include control (learning alternative policies aimed at maximally altering the state space, i.e., the pixels), immediate reward prediction, and value function replay. Except for the latter, these auxiliary tasks are solely used to influence feature learning (via the shared CNN+LSTM feature extraction network). Experimental results demonstrate the advantages of this approach on Atari and Labyrinth tasks, particularly showcasing significantly improved data efficiency compared to A3C.
The paper is well-written, the ideas are robust, and the results are compelling, making this a clear acceptance in my view. At a high level, I have only a few comments, none of which are major concerns:
- It would be helpful to discuss the additional computational cost incurred by optimizing these auxiliary tasks. How much does this impact training speed? Which components are the most computationally expensive?
- If possible, clarify in the abstract/introduction that the agent learns separate policies for each task. The abstract's phrasing—"also maximises many other pseudo-reward functions simultaneously by reinforcement learning"—initially led me to believe the agent was learning a single policy to optimize all rewards together. I only realized this was incorrect upon reaching equation 1.
- The "feature control" concept is not strongly validated empirically. The preliminary results in Figure 5 are not particularly convincing, as they only show a slight initial improvement. While I appreciate the idea, I am concerned about the potential stability and convergence issues arising from the task evolving during learning, given that the extracted features are being modified.
- Since you noted that "the performance of our agents is still steadily improving," why not allow them to continue training to observe their ultimate performance (at least for the best-performing agents)?
- Why are the auxiliary task weight parameters (the lambda_*) not treated as hyperparameters to optimize? Were there any experiments to confirm that setting them to 1 was an effective choice?
- Please explicitly mention that the auxiliary tasks are not trained using "true" Q-Learning, as they are trained off-policy with more than one step of empirical rewards (as noted in the OpenReview comments).
Minor points:
- "Policy gradient algorithms adjust the policy to maximise the expected reward, L_pi = -..."—this is actually a loss to be minimized.
- In equation 1, lambda_c should be inside the summation.
- Just below equation 1, rt^(c) should be rt+k^(c).
- Figure 2 is not referenced in the text, and Figure 1(d) should be referenced in section 3.3.
- "The features discovered in this manner is shared" should be "are shared."
- The text around equation 2 refers to the loss L_PC, but this term is not defined and does not appear explicitly in equation 2.
- Please explain what "Clip" means for dueling networks in the legend of Figure 3.
- I would have appreciated seeing more ablation studies on Atari to determine whether the same patterns of individual contribution observed in Labyrinth also hold for Atari.
- In the legend of Figure 3, the percentages mentioned pertain to Labyrinth, but this is not clear from the text.
- In section 4.1.2: "Figure 3 (right) shows..."—this actually refers to the top-left plot of the figure. Additionally, "This is shown in Figure 3 Top" should be "Figure 3 Top Right."
- "Figure 5 shows the learning curves for the top 5 hyperparameter settings on three Labyrinth navigation levels"—this appears to refer to the left and middle plots of the figure, so only two levels are shown (the preceding text may also need revision).
- In section 4.2: "The left side shows the average performance curves of the top 5 agents for all three methods the right half shows..."—this sentence is missing a comma or similar punctuation after "methods."
- Appendix: "Further details are included in the supplementary materials."—please specify where these details can be found.
- What is the value of lambda_PC? (I assume it is 1, but this should be stated explicitly.)
[Edit] I am aware that some of my questions were already addressed in the Comments section, so there is no need to re-answer them.