This paper introduces a novel approach for estimating visual attention in videos. The method begins by processing the input video clip through a convolutional neural network (specifically, C3D) to extract visual features. These features are subsequently fed into an LSTM network. At each time step, the hidden state of the LSTM is utilized to generate the parameters of a Gaussian mixture model, which is then used to produce the visual attention map.
Overall, the proposed idea is sound, and the paper is well-written. While RNNs/LSTMs have been extensively applied to vision tasks involving discrete sequence outputs, there has been limited exploration of their use in problems with continuous outputs, as demonstrated in this work.
The experimental results effectively validate the proposed approach. Notably, the method surpasses other state-of-the-art techniques in saliency prediction on the Hollywood2 dataset. Additionally, it achieves performance gains over baseline methods (e.g., C3D + SVM) for the action recognition task.
However, one limitation of this paper is the absence of certain critical baseline comparisons. Specifically, the paper does not clearly demonstrate how the "recurrent" component contributes to the overall performance. Although Table 2 shows that RMDN outperforms other state-of-the-art methods, this could be attributed to the use of strong C3D features, whereas other methods in Table 2 rely on traditional handcrafted features. Since saliency prediction can be framed as a dense image labeling problem (similar to semantic segmentation), numerous methods have been proposed in recent years, such as fully convolutional networks (FCNs) or deconvolutional networks. A straightforward baseline would be to apply an FCN to each frame individually. If the proposed method still outperforms such a baseline, it would provide stronger evidence that the "recurrent" component is indeed contributing to the improved performance.