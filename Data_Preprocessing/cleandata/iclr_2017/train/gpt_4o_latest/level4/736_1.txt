This paper introduces a straightforward domain adaptation approach where batch normalization is applied independently within each domain.
Pros:
- The approach is remarkably simple, making it both easy to understand and implement.  
- Experimental results indicate that the method performs competitively with, or better than, existing techniques on standard domain adaptation benchmarks.  
- The analysis in Section 4.3.2 highlights that only a minimal number of target domain samples are required to adapt the network effectively.  
Cons:
- The novelty of the method is limited—it may be too basic to qualify as a distinct "method." Instead, it represents the most direct and intuitive approach when employing batch normalization in domain adaptation. Using source domain batch normalization statistics for target domain examples, as an alternative, seems less natural (as I assume is the case for the Inception BN results in Tables 1-2).  
- The analysis in Section 4.3.1 appears unnecessary beyond serving as a sanity check—KL divergence between distributions should naturally be zero when batch normalization shifts/scales each distribution to N(0,1).  
- Section 3.3 lacks clarity, and its purpose is not immediately evident.  
Overall:  
While the method lacks significant novelty, its simplicity is not inherently a drawback, especially given its strong performance on standard benchmarks. This aligns with the tradition of "Frustratingly Easy Domain Adaptation." If the paper is accepted, Sections 4.3.1 and 3.3 should either be removed or revised for greater clarity in the final version.