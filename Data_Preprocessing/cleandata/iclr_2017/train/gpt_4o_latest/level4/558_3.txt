This paper introduces a straightforward count-based exploration method tailored for high-dimensional reinforcement learning (RL) tasks, such as Atari games. The approach relies on state hashing, which effectively groups (or quantizes) similar states together. The hash is derived either from hand-crafted features or from features learned in an unsupervised manner using an auto-encoder. To encourage exploration, a bonus akin to the Upper Confidence Bound (UCB) is assigned to newly encountered states.
Overall, the paper is well-executed and supported by extensive experiments. However, I am curious about its generalizability to a broader range of Atari games. Montezuma's Revenge, in particular, seems well-suited for methods that implicitly or explicitly cluster states (as this approach does), given its multiple distinct scenarios with minor visual variations, which naturally exhibit clustering structures. Conversely, such methods may struggle in environments where the state space is fully continuous, such as those in RLLab experiments.
One concern is that the authors did not address my question regarding the necessity of updating the hash code during training. My understanding is that this is primarily required to ensure the hash remains adaptive to the specific game in the early training stages (to minimize reconstruction error). After this initial phase, stabilization becomes the key priority. Additionally, Section 2.3 (Learned embedding) is both critical and somewhat unclear. I recommend that the authors improve its clarity in future revisions, perhaps by including an algorithm block to better illustrate the process.