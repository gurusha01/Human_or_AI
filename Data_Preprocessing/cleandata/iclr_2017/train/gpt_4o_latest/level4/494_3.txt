The authors present a dataset of proof steps in higher-order logic, derived from a collection of proven theorems. The success of approaches like AlphaGo indicates that, for challenging combinatorial problems, having a carefully curated set of expert data (in this case, sequences of subproofs) provides a strong foundation for achieving potentially superhuman performance. Superhuman automated theorem provers (ATPs) would undoubtedly be of immense value. While this dataset is relatively smaller compared to the original Go datasets, it represents a promising initial step. However, as ATPs and higher-order logic are outside my area of expertise, I am unable to assess the quality of this aspect of the work.
It would be exciting to see future research scale up the baselines and incorporate the networks into state-of-the-art ATPs. The scalability of deep learning methods and their ability to leverage larger datasets suggest the potential for an iterative process to enhance ATPs: as ATPs improve, they could generate additional data in the form of new theorems. While this vision may still be far off, it is an intriguing and promising direction for the field.