This paper addresses the sparse coding problem with a cosine-loss function and incorporates it as a feed-forward layer within a neural network, adopting an energy-based learning framework. The bi-directional extension renders the proximal operator equivalent to a specific non-linearity (CReLU, though arguably redundant). However, the experimental results fail to demonstrate notable improvements over baseline methods.
Pros:
- Minimizing cosine-distance appears beneficial in scenarios where computing the inner product between features is essential.
- The insight that bidirectional sparse coding corresponds to a feed-forward network with CReLU non-linearity is interesting.
Cons:
- The approach of unrolling sparse coding inference into a feed-forward network is not novel.
- The class-wise encoding strategy limits practicality in multi-class scenarios, as it necessitates a separate sparse coding network for each class.
- The proposed method does not convincingly outperform baselines in real-world applications.