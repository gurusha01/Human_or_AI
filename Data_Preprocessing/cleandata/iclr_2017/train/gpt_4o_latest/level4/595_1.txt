The paper introduces a novel approach for density estimation on sparse data (specifically, text documents) using deep generative Gaussian models, namely variational auto-encoders (VAEs). Additionally, it proposes a method to derive word embeddings from the generative parameters of the model, offering a degree of interpretability comparable to Bayesian generative topic models.
To outline the contributions, I will briefly summarize the generative process described in the paper: a K-dimensional latent variable is sampled from a multivariate Gaussian distribution, followed by an MLP (parameterized by \(\theta\)) that predicts unnormalized potentials over a vocabulary of \(V\) words. These potentials are exponentiated and normalized to form the parameters of a multinomial distribution, from which word observations are repeatedly sampled to construct a document. Instead of performing intractable inference, the VAE framework is employed, where an inference network (parameterized by \(\phi\)) predicts the mean and variance of a normal distribution for each document, enabling reparameterized gradient computation.
The first contribution, albeit straightforward, involves incorporating tf-idf features to inject first-order statistics (global information) into local observations. The authors argue that this approach is particularly beneficial for sparse data like text.
The second, more compelling contribution involves a novel optimization approach for the generative parameters (\(\theta\)) and variational parameters (\(\phi\)), which draws inspiration from the original Stochastic Variational Inference (SVI) framework. Specifically, the variational parameters \(\phi\) are treated as global parameters, while the predicted mean \(\mu(x)\) and covariance \(\Sigma(x)\) for each observation \(x\) are treated as local parameters. Unlike the standard VAE, where local parameters are indirectly optimized through the shared MLP parameters, this work directly optimizes the local parameters while holding the generative parameters fixed (step 3 in Algorithm 1). These optimized local parameters are then used to update the generative parameters (step 4), followed by an update to the global variational parameters (step 5). While other works have explored optimizing local parameters, deriving this procedure from the familiar SVI framework makes the approach more interpretable and less ad hoc.
However, some aspects remain unclear. For instance, the functional form of the gradient used in step 3 of Algorithm 1 is not explicitly shown, which would have been helpful. Additionally, the gradient update for global variational parameters (step 5) uses the initial prediction of local parameters, seemingly ignoring the optimization performed in step 3. This warrants clarification—either by providing a fundamental reason for this choice or by addressing the potential oversight. The authors also suggest that this optimization approach mitigates poor initialization of the generative model \(p_\theta(x|z)\), which is particularly problematic for sparse data. It would be helpful to elaborate on why the initialization issue is exacerbated in the case of sparse data.
The final contribution is a clever procedure to derive word embeddings from the generative model parameters, enabling interpretability of the model's learned representations. Notably, these embeddings are context-sensitive, as the latent variable represents an entire document.
Regarding Figures 2a and 2b, the caption states that solid lines represent validation perplexity for \(M=1\) (no optimization of local parameters) and dashed lines represent \(M=100\) (100 iterations of local parameter optimization). However, the figure legends suggest a different interpretation. Assuming the caption is correct, the results indicate that deeper networks exposed to more data benefit from local parameter optimization. It would be helpful for the authors to confirm whether the \(M=1\) models in Figure 2b have indeed reached a plateau, as longer training might allow them to catch up with the \(M=100\) curves. Additionally, since the x-axis does not reflect comparable running times, this raises further questions about the interpretation of the results.
The analysis of singular values provides an intriguing lens to examine how the model utilizes its capacity. However, Figures 2c and 2d are difficult to interpret, and a more detailed walkthrough would greatly aid readers.
For the word embeddings, an evaluation on a predictive task is notably absent. While Table 2b is illustrative, it is not easily reproducible. The description—"we create a document comprising a subset of words in the context's Wikipedia page"—is vague, and it is unclear whether this setup requires careful design to achieve the results shown in Table 2b.
In conclusion, while the inference and embedding techniques presented in the paper appear promising, they might have benefited from being explored as separate contributions. This would allow for a deeper investigation into each technique and its implications.