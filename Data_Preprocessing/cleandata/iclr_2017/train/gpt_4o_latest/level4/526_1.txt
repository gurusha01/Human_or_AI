This paper establishes a theoretical guarantee for the convergence of the training error. The result is fairly general and applies to the training of a broad class of neural network models. The central idea of the paper is to approximate the training loss using its linear approximation. Due to the linearity (and thus convexity) in the variables, the authors leverage results from the well-established literature on online learning.
The paper demonstrates notable novelty by employing the Taylor approximation, which significantly simplifies the analysis of the model's behavior. However, there are two major issues with the main result of the paper, Theorem 2.
1. It is unclear whether the Taylor optimum converges.  
As the authors themselves observe, the upper bound is path-dependent. While Appendix 3 attempts to argue that the Taylor optimum does indeed converge, the proof provided is flawed. Specifically, in the proof of Lemma 2, it is shown that the difference between two consecutive Taylor optima approaches 0. However, this result is weaker than proving that the sequence is a Cauchy sequence, which is necessary to ensure convergence.
2. The left-hand side of Equation (3) (denoted as L3 in this review) is not equivalent to the training error.  
An upper bound on this average error does not suffice to guarantee the convergence of the training error. For instance, in the case of gradient descent (where each minibatch \( x0^n \) represents the entire training set), the convergence of the training error should be expressed as \( \lim{n \to \infty} l(f{w^n}(x0^n), y^n) \). While the convergence of L3 is necessary, it is not sufficient to imply the convergence of the training error.
A secondary concern regarding Theorem 2 (albeit less significant compared to the two primary issues above) is that achieving the \( O(1/\sqrt{n}) \) rate requires selecting a specific learning rate. Deviating from this prescribed learning rate—whether larger or smaller (in the order of \( n \))—leads to significantly worse regret. However, in the experiments presented in the paper, the learning rates are not chosen in accordance with the theorem.
In summary, this paper is well-motivated and demonstrates strong novelty. It has the potential to evolve into a solid contribution. However, due to the two critical issues and the flawed proof highlighted above, I believe it is not yet ready for publication.