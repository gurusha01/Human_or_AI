The authors introduce a novel software package for probabilistic programming, leveraging recent advancements in tools from the deep learning community. The proposed software appears highly promising and has the potential to revolutionize workflows in the probabilistic modeling field by enabling rapid prototyping and quick iteration of ideas. The use of composability principles is particularly insightful, and the extension of inference capabilities to methods like HMC, beyond the more straightforward VI inference (which can already be implemented using existing deep learning frameworks), makes the package even more compelling.
That said, the primary consideration for any probabilistic programming language (PPL) is its practicality for real-world applications. This aspect was not sufficiently demonstrated in the submission. While the paper includes numerous example code snippets, most of these are not empirically evaluated. For instance, the Dirichlet process mixture model example (Figure 12) is critical: do the proposed black-box inference tools function effectively for this case? Similarly, will the GAN example (Figure 7) converge when optimized with real-world data? To persuade the community of the package's practicality, these aspects need to be demonstrated empirically. At present, the only evaluated model is a VAE with various inference techniques, which are relatively straightforward to implement using TensorFlow alone.
Presentation:
- The paper's presentation could be improved. For instance, the authors could provide more signaling to guide the reader through the explanations. On page 5, terms like `qbeta` and `qz` are introduced without prior explanation—the authors could clarify that an example will follow shortly thereafter.
- I recommend explaining in the preface how the layers are implemented and how the KL divergence is handled in VI, for example.
- It would be helpful to discuss which values are optimized and which change during inference (even before section 4.4). This was unclear throughout much of the paper.
Experiments:
- Why is runtime not reported in Table 1?
- What are the "difficulties around convergence" mentioned in relation to analytical entropies? As inference becomes more automated, diagnosing issues becomes more challenging. Are diagnostic tools provided in the software package to address this?
- Did HMC produce reasonable results in the experiment described at the bottom of page 8? Only runtime is reported.
- How challenging is it to get inference methods like HMC to work when users lack full control over the computational graph structure and sampler?
- It would be highly beneficial to include a table comparing the performance (e.g., runtime, predictive log-likelihood) of the various inference tools across more models.
- What benchmarks are planned for the Model Zoo? Probabilistic modeling lacks standardized benchmarks for evaluating and comparing models. While the Model Zoo is a practical concept in the Caffe ecosystem due to established benchmarks like ImageNet, it is unclear what datasets will be used to evaluate models like the DPMM.
Minor Comments:
- In Table 1, I suggest comparing results to Li & Turner with α=0.5 (corresponding to the Hellinger distance), as their work concluded that this value performs best. The choice of α=-1 here is unclear.
- How are discrete distributions handled (e.g., Figure 5)?
- The variable `x_real` is not defined in Figure 7.
- Consider highlighting `M` in Figure 8 for clarity.
- Replace the period with a comma after "rized), In" on page 8.
In conclusion, the software developments presented in this work are exciting, and I commend the authors for striving toward practical and accessible "inference for all." However, in its current state, I must assign the submission a score of 5.