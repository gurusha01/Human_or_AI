This paper investigates the concept of "Sample Importance" for individual samples within a training dataset and its impact on the overall learning process.
The study presents empirical results demonstrating that different training samples produce larger gradients at varying stages of the learning process and across different network layers. The findings include intriguing results that challenge the conventional curriculum learning approach, which suggests prioritizing "easy" training samples early on. However, the paper does not provide a clear definition of what constitutes "easy" training samples.
Additionally, the experiments revealing that ordering samples based on either NLL or SI performs worse than using mixed or random batch construction are particularly insightful.
Suggestions for Improvement:
It would be beneficial to disentangle the contribution of "Sample Importance" from the gradient magnitudes. Since higher gradients (as a function of a specific weight vector) can be influenced by weight initialization, this could introduce noise into the model and obscure the true impact of sample importance.
Furthermore, it would be valuable to explore whether incorporating "Sample Importance" into the batch selection algorithm could lead to improvements that surpass the baseline performance of random batch selection.
In summary, this is a strong paper that provides a variety of experiments to analyze how individual samples in SGD influence different aspects of the training process.