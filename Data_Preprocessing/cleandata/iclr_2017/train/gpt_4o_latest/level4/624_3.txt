The paper investigates specific scenarios involving neural networks and datasets where optimization fails. However, many of the models and datasets examined are artificially constructed and do not adhere to standard hyperparameter selection or parameter initialization heuristics. This limits the practical applicability of the findings.
The experiment "bad initialization on MNIST" demonstrates that when very negative biases are used or weights are sampled from a non-centered distribution, all ReLU activations remain "off" for all data points, thereby hindering optimization. However, this situation does not arise in practice, as proper initialization heuristics effectively prevent such cases.
The "jellyfish" dataset introduced by the authors is shown to be challenging for a small model to fit. However, the model's size and depth are not well-suited for this specific problem.
Proposition 4 assumes the ability to select the mean of the weight initialization distribution. In practice, this assumption is unrealistic, as most initialization heuristics sample weights from distributions with a mean of 0.
Proposition 5 examines infinitely deep ReLU networks. However, in practice, very deep networks are more commonly implemented as ResNets.