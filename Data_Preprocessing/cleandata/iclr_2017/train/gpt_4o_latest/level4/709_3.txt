Authors explore the application of layer-wise pretraining, inspired by language models, for encoder-decoder architectures. This approach enables the utilization of separate source and target corpora in an unsupervised manner, eliminating the dependence on large parallel training datasets. The core idea is relatively straightforward, involving the initial optimization of both the encoder and decoder using LSTMs tasked with language modeling.
While the concepts themselves are not novel, the paper serves as a cohesive synthesis of several established techniques. Nonetheless, the experimental results provide valuable insights into the significance of initialization and the comparative effectiveness of various initialization strategies in the encoder-decoder framework.
The regularizer introduced on page 3 resembles a standard multi-task objective function, particularly when used in an alternating fashion. It would be interesting to investigate whether comparable performance could be achieved by starting with this objective from a random initialization.
Additionally, the authors should consider acknowledging the early work on encoder-decoder RNN models from the 1990s.
Minor comments:
- Pg. 2, Sec 2.1, 2nd paragraph: "can be different sizes" â†’ "can be of different sizes"