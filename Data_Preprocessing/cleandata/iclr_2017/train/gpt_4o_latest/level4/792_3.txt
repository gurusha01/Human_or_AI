This manuscript addresses neural network regularization by combining the target distribution with the model's own predictions. Conceptually, it aligns with methods like scheduled sampling (Bengio et al), SEARN (Daumé et al), and DAgger (Ross et al), which employ a "roll-in" mixture of target and model distributions during training. The authors clarified in the pre-review questions that these targets are generated on-the-fly rather than from a lagged distribution. However, this distinction makes the algorithm pseudocode somewhat misleading, assuming my interpretation is correct.
The work represents an incremental advancement on the concept of label softening/smoothing, which has recently regained attention, and thus the novelty is limited. While the authors highlight that this method better preserves co-label similarity, it does not necessarily imply causation with respect to regularization. A more natural baseline would involve a fixed, soft label distribution or a setup where the softening/temperature of the label distribution is gradually reduced, as one might expect as the model increasingly approximates the target distribution.
The idea is intriguing and has some appeal, but the utility of the approach is not convincingly demonstrated. For example, the dropout baselines for MNIST are significantly weaker than those reported in prior literature (e.g., Srivastava et al., 2014, who achieved 1.06% error with a 3x1024 MLP using dropout and a simple max norm constraint). In contrast, the dropout baselines here fail to achieve below 1.3%, which is relatively high by contemporary standards for the permutation-invariant task. Similarly, the CIFAR-10 results are far from the current state of the art, making it challenging to evaluate the contribution in the context of other advancements. On SVHN, the largest dataset considered, the reported accuracies are particularly poor; state-of-the-art single-network performance has been less than half the reported error rates for 3–4 years now. This raises questions about the conclusions that can be drawn regarding the method's effectiveness, especially in better-optimized settings.
I also have concerns about data hygiene, specifically the practice of reporting minimum test loss/maximum test accuracy rather than employing an unbiased model selection approach, such as using the minimum validation set error. Additionally, the potential regularization benefits of early stopping based on validation set performance are not explored. For reference, see the protocol outlined in Goodfellow et al. (2013).