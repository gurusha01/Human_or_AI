This paper builds primarily upon the methodologies of QA-biLSTM and QA-biLSTM with attention, as introduced in Tan et al. 2015 and Tan et al. 2016, through the following two contributions:
1. It develops a topic-specific word embedding by employing an approach akin to Paragraph2vec, utilizing the topic and title information available in the dataset.
2. It addresses the multiple-unit answer selection problem (e.g., selecting one sentence from the answer section and another from the supplemental section) in contrast to the single-answer selection problem explored in Tan et al. 2015 and 2016. The approach to maintaining coherence across different parts of the answers draws inspiration from the attention mechanism proposed by Tan et al. 2016.
Although the practical results presented in the paper are intriguing, the paper's core contributions remain relatively modest.