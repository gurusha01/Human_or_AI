The paper investigates the capacity of deep networks with ReLU activation functions to represent specific types of low-dimensional manifolds. In particular, it examines what the authors term "monotonic chains of linear segments," which are essentially collections of intersecting tangent planes. The authors propose a construction that effectively models such manifolds within a deep network and provide a preliminary error analysis of the proposed approach.
While the results appear novel to the best of my knowledge, they are not particularly surprising for two reasons: (1) the representational power of deep networks is already well-documented, and (2) the study selects a deep network architecture and data structure that are highly "compatible." I have three primary concerns regarding the results presented in the paper:
(1) Over the past decade, there has been considerable research on learning data representations from sets of local tangent planes. Notable examples include local tangent space analysis by Zhang & Zha (2002), manifold charting by Brand (2002), and alignment of local models by Verbeek, Roweis, and Vlassis (2003). However, none of this prior work is cited in the related work section, despite its clear relevance to the presented analysis. For example, it would be valuable to compare these earlier techniques with the deep network used to generate the embedding in Figure 6. Such a comparison could shed light on the inductive biases introduced by the deep network: does it produce superior representations due to better inductive biases, or does it yield inferior representations because the optimization problem is non-convex?
(2) It is unclear how the analysis extends to more complex datasets where local linearity assumptions about the data manifold are invalid due to the sparsity of high-dimensional data, or how it applies to deep network architectures that deviate from pure ReLU networks. For instance, most modern architectures incorporate batch normalization, which appears to conflict with the presented analysis.
(3) The error bound derived in Section 4 seems impractical for real-world scenarios, as the upper bound on the error grows exponentially with the total curvatureâ€”a quantity that is typically large in practical settings. This issue is highlighted by the analysis of the Swiss roll dataset, where the authors acknowledge that the "bound for this case is very loose." The looseness of the bound for such a relatively simple manifold raises concerns about the utility of the error analysis in understanding the representational power of deep networks.
I strongly encourage the authors to address issue (1) in their revision. While issues (2) and (3) may be more challenging to resolve, addressing them is critical for this line of research to significantly advance our understanding of deep learning.
Minor Comments:
- In the related work section, the authors only reference fully supervised Siamese network approaches, which differ from their unsupervised approach. However, the authors are not the first to explore unsupervised representation learning with deep networks. Notable prior work includes deep autoencoders (Hinton & Salakhutdinov, 2006), denoising autoencoders from Bengio's group, and parametric t-SNE (van der Maaten, 2009).
- What loss function is used in the experiments? The description of "the difference between the ground truth distance ... and the distance computed by the network" is unclear, as it seems to incentivize the network to produce infinitely large distances (resulting in a loss of negative infinity). Is the difference squared? Clarification is needed.