This paper primarily serves as a (well-written) exploration of a toy application. It demonstrates how SGVB can be utilized within the context of state-space models. The core concept involves framing a state-space model as a deterministic temporal transformation, where innovation variables function as latent variables. Notably, the prior distribution over these innovation variables is time-invariant. The paper focuses on performing approximate inference over the innovation variables rather than directly over the states. While this approach addresses a specific problem (e.g., it does not explore scenarios where priors over the beta's are time-dependent), it remains an intriguing and valuable application. However, the presentation of ideas could benefit from greater clarity and conciseness; the paper delves into technical specifics rather quickly, which may represent a missed opportunity for broader accessibility.
I commend the authors for the extensive detail provided in both the main text and the appendix.
The experiments, while limited to toy examples, show encouraging potential.
- Section 2.1: "In our notation, one would typically set betat = wt, though other variants are possible" -> It would be helpful to clarify that if Ft and Bt are excluded from beta_t, they are not treated in a Bayesian manner (e.g., they are simply optimized).
- Section 2.2, last paragraph: "A key contribution is [â€¦] forcing the latent space to fit the transition." This contribution appears somewhat trivial to achieve.
- Eq 9: "This interpretation implies the factorization of the recognition model:.."  
The factorization is not explicitly implied anywhere; in principle, one could use q(beta|x) = q(w|x,v)q(v) instead.