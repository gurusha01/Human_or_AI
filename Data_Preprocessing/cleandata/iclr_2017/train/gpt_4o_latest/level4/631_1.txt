Paper summary: This study introduces the use of RNNs within a convolutional network framework as a supplementary mechanism for propagating spatial information across images. Promising results are presented for both classification and semantic labeling tasks.
Review summary:  
The manuscript is well-written, the core idea is clearly articulated, and the experiments are thoughtfully designed without overstating their contributions. While the paper does not present groundbreaking innovation, it represents a solid contribution to incremental advancements in the field.
Pros:  
- Clear and concise explanation of the proposed approach.  
- Well-designed and carefully executed experiments.  
- A simple yet effective concept.  
- Avoids overhyping the contributions.  
- Comprehensive comparisons with related architectures.
Cons:  
- The idea is somewhat incremental and could be perceived as derivative (e.g., related to Bell 2016).  
- While the results are promising, they do not surpass the state-of-the-art benchmarks.
Quality: The proposed ideas are technically sound, and the experiments are well-executed and analyzed.  
Clarity: The paper is generally easy to follow and well-written, though a few important details are missing (see specific comments below).  
Originality: The originality is limited, as the work combines existing concepts in a novel way rather than introducing fundamentally new ideas.  
Significance: This work provides a meaningful step forward in understanding best practices for designing neural networks for tasks like semantic labeling and classification.
Specific comments:  
- Section 2.2: The statement "we introduce more nonlinearities (through the convolutional layers and ...)" is misleading, as convolutional layers are linear operators.  
- Section 2.2: Why is it claimed that RNNs cannot have pooling operators? There seems to be no inherent limitation preventing this.  
- Section 3: The phrase "into the computational block" is unclear. Which block is being referred to? This appears to be a typo and should be clarified.  
- Figures 2b and 2c are missing. Please either include these figures or remove references to them.  
- Consider adding a brief description of GRU in the appendix for completeness, as it may help readers unfamiliar with the concept.  
- Section 5.1: The final sentence is unclear. Convolutions combined with ReLU and pooling in ResNet also introduce nonlinearities "between layers." Please elaborate or rephrase.  
- Section 5.2.1 (and Appendix A): How is the learning rate adjusted (increased or decreased)? Is this done manually? This is a critical detail that should be explicitly stated. Additionally, is the learning rate schedule consistent across experiments in each table? If manual intervention is involved, what is the variance in results between different human schedulers?  
- Section 5.2.1: The claim "we certainly have a strong baseline" is overstated. For example, the Pascal VOC12 competition reports 85.4 mIoU as the best-known result, whereas the paper reports 64.4. Please revise this statement to reflect the actual performance.  
- Section 5.2.3: Typographical error—replace "Modules" with "modules."  
- The results section does not address the increased memory usage or computational cost associated with the proposed method. This is a significant omission and should be discussed.  
- Section 6: The phrase "adding multi-scale spatial" should be revised to "adding spatial," as there is nothing inherently "multi-scale" in the RNN approach.  
- Section 6: Typographical error—replace "Furthermoe" with "Furthermore."  
- Appendix C appears redundant with Figure 5. Consider consolidating or removing this content.