The paper introduces a highly intricate compression and reconstruction approach (involving additional parameters) aimed at reducing the memory footprint of deep neural networks.
The authors demonstrate that their complex method outperforms the simpler hashed net approach. However, one concern arises: Are the additional parameters required for the reconstruction network included in the memory comparison? If not, the experimental results may not be entirely fair.
Given that the hashing and reconstruction processes are likely to dominate the feed-forward and back-propagation computations, it is crucial to evaluate the two methods in terms of running time. While hashed net is relatively straightforward, it still introduces an additional bottleneck. Please provide an analysis of the impact on running time. Minor gains in memory efficiency at the expense of significant computational overhead may not be justifiable. I remain unconvinced that this method qualifies as lightweight. If complex compression and reconstruction methods are permissible, then any off-the-shelf technique could be employed, albeit at a substantial computational cost.