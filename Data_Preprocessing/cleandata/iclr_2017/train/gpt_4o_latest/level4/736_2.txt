Update: I appreciate the authors' responses to my comments. However, I maintain that the proposed method requires further experimental validation. Currently, its applicability is limited to scenarios where pre-trained ImageNet models are used, but it is equally important to demonstrate its effectiveness in cases where models are trained from scratch. A fair comparison with state-of-the-art methods on simpler datasets (e.g., as in (Bousmalis et al., 2016) and (Ganin & Lempitsky, 2015)) would significantly strengthen the paper. In my view, this is a critical limitation that prevents me from raising my rating.
This paper introduces a straightforward modification to batch normalization, repurposing it as a domain adaptation technique. The authors demonstrate that the per-batch means and variances, typically computed during the batch normalization process, are sufficient to distinguish between domains. Based on this insight, they propose adapting to the target domain by substituting the population statistics calculated on the source dataset with the corresponding statistics from the target dataset.
Overall, I believe this paper is better suited for a workshop track rather than the main conference track. My primary concerns are as follows:
1. While the core idea is straightforward, the paper seems structured in a way that obscures the simplicity of the main contribution (e.g., the idea could have been explicitly stated in the abstract, but the authors chose not to do so).
2. (As noted in the pre-review questions) The authors rely on a much stronger base CNN, which could account for a significant portion of the reported performance gains. To convincingly demonstrate the utility of the proposed method, a fair comparison with competing approaches is necessary. This comparison should also include scenarios where the model is trained from scratch, rather than relying on pre-trained ImageNet networks.