This paper was recently submitted to arXiv:
The authors introduce a novel, publicly available dataset and a set of tasks designed for goal-oriented dialogue applications. The dataset and tasks are synthetically generated using rule-based programs, enabling the evaluation of various aspects of dialogue system performance, such as issuing API calls, presenting options, and handling complete dialogues.
This contribution is a valuable addition to the dialogue systems literature and has the potential to advance research in developing and understanding dialogue systems. However, there are notable limitations to this approach. Firstly, it remains unclear how well Deep Learning models perform on these tasks compared to traditional methods (e.g., rule-based systems or shallow models). Deep Learning models typically require large amounts of training data, and observed performance differences between neural networks may primarily stem from differences in regularization techniques. Additionally, tasks 1-5 are entirely deterministic, which limits their ability to evaluate models' handling of noisy or ambiguous interactions, such as inferring user goal distributions or implementing dialogue repair strategiesâ€”critical capabilities for dialogue systems. Nonetheless, this remains an intriguing direction for further exploration.
As noted in the comments below, the paper lacks a baseline model that incorporates word order information. This is a significant shortcoming, as it may lead to an overestimation of the neural networks' performance, while simpler baselines could potentially perform comparably or even better. To ensure a fair evaluation and accurately assess the advantages of representation learning for this task, it is essential for the authors to include an additional non-neural benchmark model that accounts for word order information. For instance, the authors could experiment with a logistic regression model that uses: 1) word embeddings (similar to the Supervised Embeddings model), 2) bi-gram features, and 3) match-type features. Including such a baseline would more convincingly demonstrate the utility of Deep Learning models for this task. If this baseline is added, I would increase my rating to 8.
A minor comment: in the conclusion, the paper claims that "the existing work has no well-defined measures of performances." This statement is not entirely accurate. End-to-end trainable models for task-oriented dialogue systems do have well-defined performance metrics. For example, see "A Network-based End-to-End Trainable Task-oriented Dialogue System" by Wen et al. While non-goal-oriented dialogue systems are indeed more challenging to evaluate, they can still be assessed using human subjects, as demonstrated by Liu et al. (2016) for Twitter. See also "Strategy and Policy Learning for Non-Task-Oriented Conversational Systems" by Yu et al.
---
I have updated my score based on the new results added to the paper.