In this paper, the authors introduce a Bayesian extension of the skip-gram model to learn word embeddings. The proposed approach incorporates two key modifications compared to the original model. First, the training process leverages aligned sentences from multiple languages, allowing the context words for a target word to originate either from the same sentence or from an aligned sentence in a different language. This enables the learning of multilingual embeddings. Second, the model assigns each word multiple vectors, corresponding to its different senses. A latent variable, z, determines which sense is activated based on the given context.
Overall, I find the idea of employing a probabilistic framework to model polysemy to be compelling. The proposed model serves as an elegant generalization of the skip-gram approach in this direction. However, I found the paper somewhat difficult to follow. The formulation could potentially be simplified (e.g., why not consider a target word w and a context c, where c can belong to either the source or target language? Since all factors are independent, this modification would likely not alter the model significantly and could improve the clarity of the presentation). Additionally, the performance metrics reported in Tables 2 and 3 appear to be relatively low.
In summary, I appreciate the core idea of the paper, which is to represent word senses using latent variables within a probabilistic framework. However, I believe the clarity of the method's presentation could be significantly improved, which would enhance the overall impact of the paper. I also have reservations about the reported experimental results.
Pros:
- Innovative extension of the skip-gram model to address polysemy.
Cons:
- The paper lacks clarity in its presentation.
- The reported results appear to be suboptimal.