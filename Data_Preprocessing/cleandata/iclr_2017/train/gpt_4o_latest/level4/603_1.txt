This paper addresses the code completion problem, which involves generating a distribution over the next token or sequence of tokens given partially written source code. This is a significant and compelling problem with applications in both industry and research. The authors introduce an LSTM-based model that generates a depth-first traversal of an AST sequentially. As expected, the proposed approach demonstrates improved performance compared to prior methods that rely on more rigid conditioning mechanisms (e.g., Bielik et al. 2016). However, merely enhancing prior work with LSTM-based conditioning does not constitute a sufficiently novel contribution to warrant a full paper. To strengthen the contribution, the authors could explore additional directions, such as evaluating the impact of alternative traversal orders on predictive accuracy or proposing novel strategies for handling UNK tokens. Since the overarching goal of this work is to advance code completion, it would be valuable to go beyond simply adapting existing methods to neural architectures.
Comments:
- The last two sentences in the related work section assert that other methods are limited to "examining a subset of source code." This statement is not only vague but also inaccurate. The models presented in Bielik et al. 2016 and Maddison & Tarlow 2014 are, in principle, capable of conditioning on any part of the already generated AST. The distinction of this work lies in the LSTM's ability to flexibly learn conditioning without increasing computational complexity.
- In the denying prediction experiments, the most insightful metric is the Prediction Accuracy, defined as P(accurate | model doesn't predict UNK). It would also be interesting to report P(accurate | UNK is not the ground truth). While models trained to ignore UNK losses are expected to perform worse overall, it would be informative to evaluate their performance specifically on non-UNK tokens.