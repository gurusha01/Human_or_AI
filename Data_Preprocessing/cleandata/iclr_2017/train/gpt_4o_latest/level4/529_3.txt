This paper employs a combination of likelihood-based and reward-based learning to train sequence models for music. The integration of likelihood and reward-based learning has been well-established, stemming from the unification of inference and learning introduced in the ML literature through the EM formalism of Attias (2003) for fixed horizons. This was later extended by Toussaint and Storkey (2006) to general horizon settings, by Toussaint et al. (2011) to POMDPs, and further generalized by Kappen et al. (2012) and Rawlik et al. (2012). These foundational works laid the groundwork for unifying probabilistic and data-driven objectives with reinforcement learning signals, framing them as part of a unified reward/likelihood paradigm. Consequently, the optimal control target under this unification becomes p(b=1|\tau)Ep(A,S) \prodt \pi(at|st), representing the probability of receiving a reward alongside the probability of policy actions under the data-derived distribution, thereby incorporating the log p(at|st) term into (9) as well.
The interpretation of the secondary objective as a prior offers an alternative perspective within a stochastic optimal control (SOC) framework. However, this interpretation is somewhat unconventional, given that the core principle of SOC aligns control objectives with inference objectives. While the SOC off-policy objective retains the KL term, it still diverges from the approach presented in this paper.
Although the discussion of optimal control is solid, further elaboration on the historical context and the mechanisms by which reward augmentation operates within SOC would enhance the paper. This would facilitate a direct comparison between SOC off-policy methods and DQN on equal footing.
The motivation behind objective (3) is reasonable but could benefit from greater clarity through the unification argument outlined above. The paper then employs DCN to pursue a distinct approach from variational SOC in achieving this objective.
A noteworthy point of discussion is the choice of Epi \log p(at|s_t), which necessitates that the policy "covers" the model. However, a challenge in generative tasks arises from the fact that well-trained models are often underfit, leading to actions that, over time, push the state into regions of the space unsupported by the data. This results in the model losing confidence and behaving randomly. This approach, as opposed to a KL(p||pi) formulation—which is not straightforward to implement—struggles to address this issue without a strong signal to counteract the distribution's tails. In the context of music, with its smaller discrete alphabet, this problem is likely less pronounced than in real-valued policy densities with exponentially decaying tails. A deeper discussion of this issue would be valuable, particularly regarding the role of the balancing parameter c. As evidenced in Figure 2, the reward signal had to be significantly amplified to bring the log p signal into an appropriate range.
Overall, in the domain of music, this paper provides a reasonable demonstration of the value of augmenting a sequence model with an additional reward constraint. It shows that DQN is one viable method for learning this signal. However, the paper does not explore alternative techniques for learning the same signal. Instead, for the comparator methods, it treats p(a|s) as a "prior" term rather than a reward term, leaving some uncertainty as to whether DQN is particularly well-suited for this task.
An additional question worth exploring in the discussion is whether the music theory reward could be approximated using a differentiable model, potentially eliminating the need for an RL-based approach altogether.