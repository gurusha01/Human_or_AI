The paper introduces a novel regularizer for CNNs that penalizes positive correlations between feature weights while leaving negative correlations unaffected. Additionally, an alternative version that penalizes all correlations, regardless of sign, is explored. These are referred to as "local" and "global" regularizers, though I find these terms somewhat ambiguous, as they are quite broad and could imply a variety of meanings.
The experimental evaluation is thorough, with multiple experiments conducted on standard benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN), showing improvements in most cases. While the gains are relatively modest, the baselines are already highly competitive, as the authors note. That said, some results raise questions about statistical significance. Furthermore, additional results using the global regularizer (beyond MNIST) would have been valuable, given that the primary novelty of the paper lies in preserving negative correlations. It would be interesting to better understand the specific impact of this design choice.
One significant concern is the ambiguity caused by the paper's interchangeable use of "features" to describe both activations and filter weights. This could lead to confusion, though the authors have indicated that they plan to clarify this issue.
The paper also does not sufficiently address the interaction between the proposed regularizer and the choice of nonlinearity, which seems critical. The goal is to achieve uncorrelated feature activations, but the penalty is applied only to the weights, in a data-agnostic manner, without considering the effects of nonlinearities. The authors have mentioned in their responses to reviewers that they will address this, but I believe it is an important point that warrants detailed discussion.
Regarding the authors' response to my question about biases: while they correctly note that their method can be combined with the "multi-bias" approach, this does not fully address my concern. The "multi-bias" approach challenges the underlying assumption of this workâ€”that features should not be positively correlated or redundant. My intuition is that correlated features are acceptable as long as they do not lead to wasted model capacity. In the case of "multi-bias," the weights are shared across correlated features, which avoids such inefficiencies.
The dichotomy presented in the introduction between regularization methods that reduce capacity and those that do not feels somewhat arbitrary. For instance, weight decay is categorized as a method that reduces capacity, while the proposed method is not. However, this distinction seems to depend heavily on how one defines model capacity, as weight decay does not actually reduce the number of parameters in a model.
In summary, while the work is somewhat incremental, it is well-executed, and the results are convincing, even if they are not particularly groundbreaking.