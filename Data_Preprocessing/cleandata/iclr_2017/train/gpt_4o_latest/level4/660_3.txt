As highlighted for Figure 5 Left, it appears that simply tuning the learning rates can sometimes suffice. While I understand your reasoning for Figure 6 Right, I have the following concerns:  
1) Not all well-chosen learning rates cause Adam to fail; it seems you specifically selected a case where it did (also, note that Adam was significantly faster than Eve in the initial stages).  
2) I am not convinced by the claim that "Eve always converges," as you only demonstrate this for 0.1. Since Eve is distinct from Adam, the learning rate of 0.1 for Adam does not directly translate to 0.1 for Eve due to the role of d_t.  
From my perspective, you define dt as a function of time using three hyperparameters. Similarly, one could define dt directly. The behavior of dt that you present is not particularly unusual and could be parameterized. If Eve indeed outperforms Adam, examining dt would allow us to directly assess whether the learning rates were underestimated or overestimated. While you might argue that Eve automates this process, it is worth noting that you still tune the learning rates for each problem individually.