This paper posits that the ability to handle recursion is crucial for neural programming architectures, as it facilitates strong generalization to out-of-domain test cases and enables learning from smaller amounts of training data. The core of the paper builds upon the work of Reed & de Freitas on Neural Programmer Interpreters (NPI) from ICLR 2016, which learns from program traces. In this work, the authors extend the NPI framework by training models on traces that include recursive calls. They demonstrate how correctness can be verified by evaluating the learned program on a limited set of base cases and reduction rules. Notably, they showcase that the NPI architecture can perfectly infer solutions to problems like Bubblesort and the Tower of Hanoi.
What stands out is the simplicity of the idea â€” as the authors themselves acknowledge, the primary modification lies in the execution traces provided to the training pipeline. However, I am uncertain about the broader implications of this work. Does this suggest that the neural programming problem is effectively solved when execution traces are available? (And, if so, was the problem inherently too simple?) For instance, as another reviewer points out, a more challenging input domain could involve MNIST digits. Imagine a scenario where the NPI must learn to sort MNIST digits from highest to lowest. In such a case, having execution traces would essentially decouple the task of recognizing the digits from the task of inferring the program logic, reducing the problem to learning MNIST digit recognition and symbolic bubble sorting. What would be an example of a problem where execution traces are accessible but cannot be solved using the proposed method?