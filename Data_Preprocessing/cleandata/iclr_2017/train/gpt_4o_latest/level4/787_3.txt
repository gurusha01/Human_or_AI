This paper introduces SEM, a straightforward large-scale multilabel learning algorithm that models the probability of each label using softmax(sigmoid(W^T X) + b), effectively forming a one-layer hidden network. However, neither this formulation nor the use of adagrad for optimization is novel. Interestingly, the paper explicitly derives the gradient and proposes alternating adagrad steps instead of the more conventional approach, but it remains unclear whether this deviation has any tangible impact on performance. The primary mechanism driving the efficiency of the model is candidate label sampling, which is implemented in a fairly standard manner by sampling labels in proportion to their frequency in the dataset.
Considering that both the model and the training strategy lack novelty, it is surprising that the reported results surpass the state-of-the-art in both quality and efficiency. That said, non-asymptotic efficiency claims are often debatable, as implementation effort can significantly influence performance outcomes. Overall, I feel this paper falls short of the standard required for acceptance.