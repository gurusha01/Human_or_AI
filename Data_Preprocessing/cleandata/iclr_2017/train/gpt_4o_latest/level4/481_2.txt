This paper explores the phenomenon of adversarial examples and adversarial training using the ImageNet dataset. While the final conclusions remain somewhat ambiguous, the paper presents several noteworthy findings derived from its experiments.
The manuscript is well-written and easy to comprehend. Although I have some reservations about certain aspects of the paper (see detailed comments below), it makes valuable contributions and is worthy of publication.
Pros:  
1. This paper introduces the concept of 'label leaking' for the first time in the literature. While its impact becomes prominent only with large datasets, it is an important aspect that future research in this domain should address carefully.  
2. The use of the ratio of 'clean accuracy' to 'adversarial accuracy' as a robustness metric is more reasonable compared to the approaches adopted in prior works.  
Cons:  
1. Although the experimental results are based on the ImageNet dataset, the title of the paper may be somewhat misleading. I consider Section 4 to be the core contribution of the paper. However, Sections 4.3 and 4.4 are not specific to large-scale datasets, making the emphasis on 'large-scale' in the title and introduction seem inappropriate.  
2. The conclusions of the paper are primarily drawn from observations of experimental results. Additional tests should have been conducted to validate these hypotheses. Without such validation, the conclusions appear premature. For instance, findings based on a single dataset like ImageNet cannot be generalized to all large-scale datasets.