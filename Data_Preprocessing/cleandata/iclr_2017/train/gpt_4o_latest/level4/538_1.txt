The authors introduce a novel model for learning symbolic expression representations. They conduct a fairly comprehensive evaluation against similar methods and provide a well-reasoned justification for their approach.
As noted in the preliminary questions, I believe the authors could enhance the justification for their subexpforce loss.
On page 6, the authors state that they compare their model to a two-layer MLP without residual connections. However, I think it would be beneficial to include a direct comparison between models with and without the subexpforce loss (while retaining residual connections and normalization).
My primary concern lies with the evaluation metric, which appears to be precision calculated on a per-query basis. I suggest that a more standard metric, such as precision-recall or ROC, would offer greater insight. Specifically, the current metric seems biased toward scenarios where equivalence classes are larger, as this factor is not accounted for in the denominator. Consequently, the likelihood of a random expression matching the query increases, which could inflate the reported performance.