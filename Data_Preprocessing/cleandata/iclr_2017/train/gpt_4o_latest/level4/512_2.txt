Review - Summary:
The paper proposes a parametric class of nonlinearities for use in neural networks. It introduces a two-stage optimization process to jointly learn the network weights and the parameters of the nonlinearities.
Significance:
The paper presents an interesting and novel idea, supported by promising experimental results. However, I find the theoretical analysis to be less insightful and somewhat distracting from the paper's central contribution.  
More extensive experimentation with the proposed approach—using different basis functions and comparing it to wider networks (equivalent in size to the number of cosine basis functions used in the learned model)—would strengthen the paper's results and conclusions.
Comments:
- Are the weights of the nonlinearities shared across all units and layers, or does each unit have its own unique nonlinearity?
- If the weights are tied across units and layers, it would be interesting to explore whether there exists an optimal nonlinearity under this constraint.
- How does the learned nonlinearity differ when the hidden units are normalized versus unnormalized? Specifically, how does the use (or absence) of batch normalization influence the learned nonlinearity?
- Does normalization impact the conclusion that polynomial basis functions fail?