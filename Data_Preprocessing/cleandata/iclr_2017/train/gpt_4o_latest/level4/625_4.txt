The paper introduces a hierarchical DRL algorithm designed to tackle sequences of navigate-and-act tasks within a 2D maze environment. During both training and evaluation, the agent is provided with a list of sub-goals represented as text, and its objective is to learn to utilize pre-trained skills to accomplish these sub-goals. The authors demonstrate that their approach generalizes effectively to sequences of varying lengths and novel combinations of sub-goals (e.g., if the agent knows how to pick up a diamond and visit an apple, it can also visit the diamond).
Overall, the paper is technically sound and presents an intriguing and non-trivial integration of cutting-edge advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). Specifically, the authors propose a hierarchical DRL agent capable of learning skills and planning with them. These skills are acquired using a differentiable temporally extended memory network with an attention mechanism. Additionally, the authors introduce a novel application of analogy-making and parameter prediction.
Nevertheless, the paper does not clearly articulate why the proposed problem is significant or why it has not been addressed previously. Given that the evaluation domain is a relatively simple 2D maze, the motivation for employing deep networks is not well justified. Similar problems have been addressed using simpler models. Notably, the authors overlook a substantial body of literature on planning with skills. Since all skills are pre-trained before the hierarchical agent is evaluated, the problem being solved resembles supervised learning more than reinforcement learning (as the use of pre-trained skills minimizes delayed rewards). Furthermore, the demonstrated generalization appears to be limited to parsing a sentence (describing the sub-task) into its components (item, location, action).
The paper is challenging to follow due to frequent shifts between algorithm descriptions and technical details. It is particularly dense with implementation specifics, which detract from the overall clarity. I recommend relocating many of these details to the appendix. The paper should also be self-contained; the authors should not assume that readers are familiar with all the methods employed and should introduce all relevant notations.
Addressing the aforementioned issues would significantly improve the paper and enhance its contribution to the field, making it more suitable for a future conference.