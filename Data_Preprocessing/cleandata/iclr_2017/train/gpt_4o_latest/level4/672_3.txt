This paper introduces an alternative to Conditional Variational Auto-Encoders (CVAE) and Conditional MultiModal Auto-Encoders (CMMAE) for inferring missing modalities in datasets with multiple modalities. The proposed method, termed Joint Multimodal Variational Auto-Encoder (JMVAE), employs a Variational Auto-Encoder (VAE) across all modalities, augmented with additional KL divergence penalties. These penalties enforce consistency between the approximate posterior conditioned on all modalities and the approximate posterior conditioned on subsets of the modalities.
The authors establish a connection between their proposed JMVAE approach and the Variation of Information (VI). However, it is unclear why the authors opted for the JMVAE framework instead of directly leveraging a more elegant VI-based approach.
Another concern is the scalability of the proposed method. Based on the information provided (noting the absence of code and the lack of detailed encoder specifications), it appears that the method requires a separate encoder for each subset of missing modalities. While this may be feasible with only two modalities, it raises questions about scalability to datasets with a larger number of modalities.
An additional point of confusion arises in Table 1, where the log-likelihood log(p(x)) estimated using multiple modalities is reported to be lower than when using just one modality. This result seems counterintuitiveâ€”could the authors clarify or provide an explanation for this observation?
The comparison between the representations learned by JMVAE and CVAE may also be problematic. Specifically, the CVAE representation is learned conditionally, incorporating the label in the case of MNIST, which should ideally be excluded from the representation. Intuitively, the CVAE representation might capture "style," as illustrated in (Kingma et al., 2014) in their conditional generation figure. This discrepancy could render the comparison unfair.
For CelebA, the comparison of log-likelihood across models that incorporate GANs is likely not meaningful, as GANs do not optimize for log-likelihood directly.
In summary, while the paper addresses an interesting problem and introduces promising ideas, the execution requires further refinement and clarification.