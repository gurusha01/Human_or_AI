SUMMARY:  
This paper presents a series of experiments aimed at evaluating methods for training a dialogue agent using reinforcement learning. The authors employ a standard memory network architecture, training it on both the bAbI dataset and a variant of the WikiMovies dataset, building on the work of Weston (2016, henceforth W16). The paper conducts extensive experiments to compare the performance of various training algorithms across different experimental setups.
STRENGTHS:  
The experimental evaluation is thorough and well-executed. I concur with the authors that the results provide valuable additional insights into the performance of the model introduced in W16.
WEAKNESSES:  
This work largely functions as an extension or appendix to the earlier paper, offering no novel machine learning contributions. Furthermore, the paper conflates "training with an adaptive sampling procedure" and "training in interactive environments" in a way that is not entirely accurate. Notably, the authors fail to compare their results with the static exploration policy experiments from W16, even though such a side-by-side evaluation would be both feasible and informative. The primary differences between this work and W16 lie in minor, well-studied modifications to the exploration policy.  
My main concern is the lack of novelty: while the additional data presented here is useful, it would be better suited for a *ACL short paper or a technical report. This submission does not stand alone as a significant contribution, making it unsuitable for an ICLR submission.
"REINFORCEMENT LEARNING"  
[Update: Concerns raised in this section have been addressed by the authors.]
The paper attempts to draw a sharp distinction between the reinforcement learning (RL) approach explored here and the "non-RL" approach in W16. However, this distinction is overstated.  
As noted in W16, the RBI objective is a specific case of vanilla policy gradient with a zero baseline and off-policy samples. In this sense, the RBI approach in this paper is essentially the same as in W16, differing only in the exploration policy. Similarly, the change in FP is merely a modification of the sampling policy. The distinction between fixed dataset and online learning is not particularly meaningful when the fixed dataset consists of unlimited synthetic data.  
It is worth noting that some exploration policies in W16 provide a stronger training signal than the RL "from scratch" setup here. For example, when $\pi_{acc} = 0.5$, the training samples yield denser rewards. However, based on Figures 3 and 4, the completely random initial policy achieves average rewards of ~0.3 on bAbI and ~0.1 on WikiMovies, which are comparable to or better than the exploration policies in W16.  
The paper would benefit from framing the differences from W16 explicitly in terms of exploration policies, rather than attempting to categorize W16 as "not RL." This framing would provide a clearer and more accurate comparison.  
The lack of direct comparisons to the training conditions in W16 is problematic. By framing this work as RL and the previous work as non-RL, the authors sidestep meaningful comparisons. This is a missed opportunity. Regardless of the quality of the off-policy sample generators in W16, it is always possible to compare them fairly in this context. Evaluating all methods in an online setting and presenting side-by-side results would offer a much clearer picture of the relative performance of the training objectives.
ON-POLICY VS OFF-POLICY  
Vanilla policy gradient methods, like those used here, typically require additional techniques (e.g., importance sampling, trust region methods) to handle off-policy samples effectively. Yet, they appear to work without such modifications in some experiments, which is an intriguing result. A discussion exploring why this might be the case would add value to the paper.
OTHER NOTES:  
- The claim that "batch size is related to off-policy learning" is somewhat misleading. Many on-policy algorithms also require large batches of transitions collected from the current policy before performing updates.  
- The experiments on fine-tuning with human workers are the most compelling aspect of this work. These results deserve much more attention and detailed exploration, rather than being relegated to the penultimate paragraphs.