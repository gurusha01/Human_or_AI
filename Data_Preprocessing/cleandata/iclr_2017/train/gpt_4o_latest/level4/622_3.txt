Review - Summary:  
This paper investigates ResNets through a theoretical framework based on a spin glass model. The authors conclude that ResNets initially function as an ensemble of shallow networks during the early stages of training (analyzed by examining the magnitude of weights for paths of a specific length). However, as training progresses, the scaling parameter \( C \) (introduced in assumption A4) increases, leading ResNets to behave as ensembles of progressively deeper networks.
Clarity:  
The paper is somewhat challenging to follow due to the heavy use of notation, with potential instances of notation overloading. Including a summary of some of the proofs in the main text could have improved readability.
Specific Comments:  
- In the proof of Lemma 2, the origin of the sequence \( \beta \) is unclear (I am unsure how it directly follows from equation (11)).  
- The ResNet structure analyzed in the paper deviates slightly from the standard, as multiple layers are skipped. Could the same analysis apply if only a single layer is skipped? It seems that the skipping primarily influences the number of paths of a given length.  
- The new experiments demonstrating the scaling increase in practice are compelling! However, I am uncertain whether Theorems 3 and 4 fully establish this connection theoretically, particularly in light of the simplifying assumption introduced at the beginning of Section 4.2.