Updated Review: 18 Jan. 2017  
I appreciate the authors for including a comparison to the previously published sparsity method by Yu et al., 2012. The comparison appears reasonable, though it would be more transparent if the authors explicitly clarified that the most relevant comparison for the results in Table 4 corresponds to the "RNN Sparse 1760" result presented in Table 3.  
I have revised my review to reflect my assessment of the updated manuscript, while retaining the original review to maintain the historical context of the paper's progression.  
This paper makes three primary contributions:  
(1) It introduces a method for training sparse RNNs, where weights below a specified threshold are masked to zero. The threshold follows a schedule such that pruning is initiated only after a certain number of iterations, with the threshold progressively increasing throughout the training process.  
(2) It presents experimental results on a Baidu-internal task using the Deep Speech 2 network architecture, demonstrating that applying the proposed sparsification technique to a large model can yield a final trained model with superior performance and fewer non-zero parameters compared to a dense baseline model.  
(3) It reports timing experiments using the cuSPARSE library, indicating that sufficiently sparse models have the potential for faster evaluation. However, the findings also suggest that the current cuSPARSE implementation may not yet be fully optimized for this purpose.  
Strengths  
+ The paper is generally clear and straightforward to follow.  
+ It addresses a significant and practical challenge in deep learning: deploying models with minimal computational and memory overhead.  
Weaknesses  
- The paper should include a comparison to "distillation" approaches (e.g.,