Paraphrased Review
Paper Summary:
The authors explore identity re-parametrization in both linear and nonlinear contexts.
Detailed Comments:
— Linear Residual Network:
The paper demonstrates that for a linear residual network, every critical point corresponds to a global optimum. Despite the non-convex nature of the problem, it is intriguing that such a straightforward re-parametrization yields this result.
— Nonlinear Residual Network:
The authors propose a framework where data points are mapped to their labels using a residual network. This involves an initial random projection, followed by a residual block that clusters the data based on their labels, and a final layer that maps these clusters to the corresponding labels.
1. In Equation 3.4, there appears to be a mismatch in dimensions: \(qj \in \mathbb{R}^k\) and \(ej \in \mathbb{R}^r\). Could you clarify this discrepancy?
2. While the proposed construction seems valid, what specific role does the residual network play in this setup? Would a similar construction work without the identity mapping? Please elaborate on this point. In the linear case, the spectral perspective clearly shows how the identity mapping aids optimization. Could you provide analogous intuition for the nonlinear case?
3. The existence of a network within the residual class that overfits raises an important question: does this provide any insight into why residual networks outperform other architectures? Furthermore, what does the existence of such a network reveal about the representational capacity of residual networks? For instance, even a simple linear model, assuming points are not too close, can overfit the data and achieve fast convergence rates (see, for example, the Tsybakov noise condition).
4. What implications does the proposed construction have for the required number of layers in the network?
5. The idea of clustering activations independently of the labels is reminiscent of older pretraining techniques. For example, one could use these centroids as weights for the subsequent layer. This approach also relates to methods like the Nyström approximation (see relevant literature for further context).