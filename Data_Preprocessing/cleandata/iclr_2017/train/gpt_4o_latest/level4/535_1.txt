This paper introduces a model for video captioning that incorporates both soft and hard attention mechanisms, utilizing a C3D network as the encoder and an RNN as the decoder. The authors conduct experiments on the YouTube2Text, M-VAD, and MSR-VTT datasets. Although prior work has already explored image captioning with soft and hard attention, as well as video captioning with soft attention, the primary contribution of this paper lies in its specific architecture and the application of attention across different CNN layers.
The paper is well-written, and the experiments effectively demonstrate the advantages of employing attention over multiple layers. However, given the context of prior research in captioning, the contribution and insights provided by this work are relatively incremental for a conference paper at ICLR. Additional experiments and a deeper analysis of the main contribution could enhance the paper, but I would suggest resubmission to a more appropriate venue.