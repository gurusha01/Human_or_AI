This study investigates neural models for sentence summarization by employing a read-again attention mechanism and a copy mechanism, which enables direct copying of word representations from source sentences. The experimental results indicate that the model achieves improved performance on the DUC dataset. However, the paper is not well-written overall. Several points are unclear, some claims lack sufficient evidence, and the experimental results are incomplete.
Detailed comments:
- Read-again attention: How does this mechanism outperform standard attention? What would happen if the same sentences are read multiple times? Have you compared it against stacked LSTMs with an equivalent number of parameters? The experimental section lacks a model ablation study to address these questions.
- Reading two sentences: Why is it necessary to process two sentences? The Gigaword dataset is designed for source-to-compression tasks and does not inherently require multiple input sentences. How does the model perform when using single-sentence input compared to two-sentence input?
- Copy mechanism: What happens when multiple identical words appear in the source sentences? According to Equation (5), only one vector is copied to the decoder. This issue does not arise with a hard copy mechanism. Furthermore, there is no experimental comparison between the proposed vector copy mechanism and a hard copy mechanism.
- Vocabulary size: This section seems tangential to the main focus of the paper. Without evidence demonstrating that this is a unique property of the vector copy mechanism, its inclusion appears trivial and detracts from the paper's primary contributions.
- Experiments: On the DUC dataset, the model is compared against other recent models, but on the Gigaword dataset, it is only compared to ABS (Rush et al., 2015) and GRU (?), which are relatively weak baselines. Claiming state-of-the-art performance in summarization based on such limited comparisons is misleading and irresponsible.
- Typos: (1) "Tab. 1" should be "Table 1." (2) "Fig. 3.1.2" appears to be incorrectly labeled.