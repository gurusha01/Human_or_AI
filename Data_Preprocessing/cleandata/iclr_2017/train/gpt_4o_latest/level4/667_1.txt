This paper tackles the challenge of data sparsity in the healthcare domain by utilizing hierarchies of medical concepts structured within ontologies. The study centers on sequential prediction based on a patient's medical record (a sequence of medical codes, some of which may occur infrequently). Rather than assigning each medical code an independent embedding to input into an RNN, the proposed method assigns a "basic" embedding to each node in the medical ontology and constructs a "final" embedding for each medical code by computing a learned weighted average (via an attention mechanism) of the medical code's ancestors in the ontology. The paper is notably well-written, and the proposed approach is intuitive.
I have the following observations:  
- Why is the patient's visit represented as the sum of medical codes in the visit, rather than, for example, the average or a learned weighted average? Wouldn't this introduce a bias based on the number of codes in the visit?  
- It is unclear why the basic embeddings are not fine-tuned. Did fine-tuning negatively impact performance? Could the authors provide an explanation for this choice?  
- In Figure 2, the results appear to be very close, and the figures themselves are not very clear (the top of figure (b) is missing). Additionally, I am curious about the statistical significance of the differences, so it would be helpful if the authors could comment on this.  
In conclusion, this is an interesting application paper that applies well-established deep learning techniques to address a critical issue encountered when deploying deep learning models in data-scarce domains. The paper highlights an important problem, but I would appreciate it if the authors could elaborate on the novel insights their work contributes to the ICLR community and explain why they believe ICLR is an appropriate venue for their research.