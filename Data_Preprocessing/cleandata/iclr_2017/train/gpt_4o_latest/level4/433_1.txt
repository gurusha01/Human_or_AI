This paper introduces an innovative approach to training a generative model that enables exact inference, efficient sampling, and precise log-likelihood computation. The key idea is to structure the Jacobian matrix, arising from the change of variables formula (mapping data to latent space), as triangular. This design simplifies the determinant calculation, thereby facilitating the learning process.
The paper effectively explains this central concept and proposes a method to implement it. Specifically, it involves selecting specialized "routings" between the latent variables and data, where part of the transformation is an identity mapping and the other part is a more complex function of the input (e.g., a deep neural network). This results in a Jacobian with a computationally manageable structure. Furthermore, these routings can be stacked to enable more intricate transformations.
On the experimental front, the model is evaluated across multiple datasets, and the results are compelling, demonstrating strong performance in both sample quality and quantitative metrics. It would be exciting to explore whether this model proves beneficial for other tasks and whether the learned latent representations enhance downstream applications like classification or inference tasks, such as image restoration.
In conclusion, the paper is well-written, the results are impressive, and the proposed model is both novel and promising. I am pleased to recommend its acceptance.