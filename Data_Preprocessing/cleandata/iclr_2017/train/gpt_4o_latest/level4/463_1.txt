This paper investigates training methods in the presence of significant label noise.  
It is a solid contribution that proposes two main approaches. The first is a latent variable model, where training involves the EM algorithm, alternating between estimating the true labels and maximizing the parameters conditioned on those labels.  
The second approach bypasses explicit label estimation by directly marginalizing over the true labels and optimizing \( p(z|x) \).  
Pros: The paper addresses an important challenge in training on large datasets, where annotations are often noisy and not carefully curated.  
Cons: The results on MNIST are entirely synthetic, making it unclear whether the proposed methods would generalize effectively to real-world datasets.  
Comments:  
- Equation (11) appears computationally expensive. How would this scale when training on a large dataset like ImageNet with 1000 classes?  
- It would be valuable to evaluate how well the proposed methods can recover the parameters of the corrupting distribution, either through the EM algorithm or the integration approach.  
Overall, this is a decent paper. However, the ideas lack novelty, as prior work (cited in the paper) has also tackled the problem of label noise. The paper could be significantly improved by either demonstrating state-of-the-art performance on a dataset known to contain label noise or showing that the proposed methods can reliably estimate the probabilities of label corruption.