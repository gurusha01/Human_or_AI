Review - Summary: This paper represents the first exploration of stick-breaking priors and their associated inference methods in the context of VAEs. The authors provide a clear and thorough explanation of the background material, as well as detailed descriptions of the priors, posteriors, and their DNCP formulations. The paper is exceptionally well-written.
In their experiments, the authors observe that stick-breaking priors do not consistently outperform spherically Gaussian priors in fully unsupervised settings, as evaluated by log-likelihood. Reporting this 'negative' result demonstrates commendable scientific integrity. In semi-supervised scenarios, however, the results show notable improvement.
Comments:
- Section 2.1: There is substantial prior work involving non-Gaussian p(z), such as DRAW, the generative ResNet paper cited in the IAF paper, Ladder VAEs, etc.
- Section 2.2: There are two unnecessary commas.
- Text flow around Equation 6: Please include a reference to the appendix where the closed-form KL divergence is detailed.
- "The v's are sampled via" => "In the posterior, the v's are sampled via." It is unclear that this refers to the posterior rather than the prior.
- The final paragraph of Section 4 is excellent.
- Section 7.1: "Density estimation" => Technically, this also involves mass estimation.
- Section 7.1: Using 100 IS samples is somewhat low.
- Figure 3(f): It is intriguing that k-NN performs so well on raw pixel data.