This paper focuses on pruning entire groups of filters in CNNs, aiming to reduce computational costs without introducing sparse connectivity. This approach is significant as it accelerates and compresses neural networks while maintaining compatibility with standard fully-connected linear algebra operations. 
The reported results demonstrate a 10% improvement on ResNet-like architectures and ImageNet, though similar gains might also be achievable through improved network design. While comparisons with newly designed networks would have strengthened the paper, it is acknowledged that such experiments can be time-intensive. 
Overall, this is a solid paper with some valuable contributions.