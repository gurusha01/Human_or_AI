The submitted approach for modeling multimodal datasets involves employing a variational autoencoder (VAE) with a distinct inference network for each possible combination of missing and present modalities. The method is demonstrated on the MNIST and CelebA datasets.
However, MNIST is not inherently a multimodal dataset. The authors treat the labels as a separate modality to be modeled using a variational autoencoder, a decision the reviewer finds puzzling. Furthermore, the proposed method does not actually address scenarios where modalities are missing, raising doubts about its practical applicability.
Additionally, the reported differences in log-likelihoods across models are minimal and appear to be attributable to noise rather than meaningful improvements. 
The second experiment presents log-likelihood results for models that were not explicitly trained to optimize log-likelihood. This makes it unclear what valid conclusions, if any, can be drawn from the comparison.