This paper addresses the challenge of transferring solutions from existing tasks to solve a novel task within the reinforcement learning framework, emphasizing two critical issues: avoiding negative transfer and enabling selective transfer. The proposed method employs a convex combination of existing solutions and the solution being learned for the novel task. By assigning non-negative weights to each solution, the approach effectively disregards solutions with negative effects while allocating higher weights to more relevant solutions for each state. The paper introduces the "A2T" learning algorithm for policy and value transfer, applicable to REINFORCE and ACTOR-CRITIC algorithms, and evaluates it through experiments on synthetic Chain World and Puddle World simulations as well as the Atari 2600 game Pong.  
+The paper proposes an innovative method for transfer reinforcement learning.  
+The experiments are thoughtfully designed to showcase the effectiveness of the proposed approach.  
-An essential aspect of transfer learning is the ability of the algorithm to autonomously determine whether existing solutions to known tasks are sufficient to address the novel task, thereby avoiding the need for learning-from-scratch. This aspect is not explored in the paper, as most experiments use a learning-from-scratch solution as the base network. It would be valuable to evaluate the algorithm's performance without a base network. Furthermore, from Figures 3, 5, and 6, while the proposed algorithm appears to accelerate learning, its overall performance does not seem to surpass that of the standalone base network. Providing examples where existing solutions complement the base network would strengthen the argument.  
-Without the base network, the proposed approach can be interpreted as ensemble reinforcement learning, leveraging the expertise of previously learned agents to address the novel task.