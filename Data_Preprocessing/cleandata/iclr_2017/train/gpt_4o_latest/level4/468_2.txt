This paper introduces a novel neural network compression technique aimed at maximizing compression through parameter quantization while minimizing the impact on the expected loss. The approach assumes that network pruning has already been performed and focuses solely on quantizing the individual scalar parameters of the network. Unlike prior work (Han et al., 2015a; Gong et al., 2014), the proposed method explicitly considers the effect of weight quantization on the loss function used for training, as well as the impact on a variable-length binary encoding of the cluster centers utilized in the quantization process.
However, the submitted paper spans 20 pages, exceeding the recommended 8 pages. The length appears unjustified, as the first three sections (comprising five pages) are overly generic and redundant. These sections, including Figures 1 and 2, could be significantly condensed or omitted. While not a strict requirement per the submission guidelines, I recommend the authors reduce the paper to 8 pages to enhance its readability.
To account for the impact of quantization on the network's loss, the authors propose using a second-order approximation of the loss function's cost. For weights that initially represent a local minimum of the loss, this results in a formulation where the effect of weight quantization on the loss is expressed as a weighted k-means clustering objective. The weights in this objective are derived from the Hessian of the loss function at the original weights. The Hessian can be computed efficiently using a backpropagation-based algorithm similar to those used for gradient computation, as demonstrated in prior literature. Alternatively, the authors suggest using the second-order moment term from the Adam optimization algorithm as a loose approximation of the Hessian.
In Section 4.5, the authors argue that their approach naturally facilitates quantizing weights across all layers simultaneously, given that the Hessian weighting accounts for the varying impact of quantization errors across layers on network performance. However, the final statement in this section was unclear:  
"In such deep neural networks, quantising network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers."  
The authors could provide additional clarification on this point.
In Section 5, the authors extend their method to incorporate the code length of weight quantization into the clustering process. The first method, based on prior work, involves uniform quantization of the weight space, which is subsequently refined using their Hessian-weighted clustering procedure from Section 4. For nonuniform codeword lengths to encode cluster indices, the authors present a modified Hessian-weighted k-means algorithm that incorporates code length considerations, weighted by a factor λ. By varying λ, the authors achieve different compression-accuracy trade-offs and propose clustering weights for multiple λ values to identify the most accurate solution within a given compression budget.
Section 6 presents experimental results obtained using the proposed methods, comparing them to the layer-wise compression technique of Han et al. (2015) and to uncompressed models. The experiments were conducted on three datasets—MNIST, CIFAR-10, and ImageNet—using dataset-specific architectures from the literature. The results demonstrate a consistent and significant advantage of the proposed method over Han et al.'s approach. However, no comparisons were made to the work of Gong et al. (2014). The experimental findings highlight the benefits of the Hessian-weighted k-means clustering criterion and the variable bitrate cluster encoding.
In conclusion, this work is quite interesting, though its technical novelty appears somewhat limited (I am not an expert in quantization). Notably, the proposed techniques seem broadly applicable beyond deep convolutional networks, as they could be used for parameter quantization in any model with an associated cost function that allows for a locally quadratic approximation. It would be helpful if the authors discussed this general applicability in their paper.