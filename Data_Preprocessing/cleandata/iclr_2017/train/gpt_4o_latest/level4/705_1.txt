This paper builds on the NIPS 2016 work titled "Unsupervised learning of spoken language with visual context" and directly addresses the future work proposed in that paper: "to perform acoustic segmentation and clustering, effectively learning a lexicon of word-like units" using the embeddings generated by their system. The analysis presented is intriguing, and I appreciate the direction the authors are pursuing.
However, my primary concern lies with the novelty of the work. This feels like a relatively straightforward extension of an existing model, which is acceptable, but in that case, the analysis should be more comprehensive. At present, it seems that the authors are primarily showcasing some of the capabilities of the NIPS model (with minor enhancements). To make the analysis more compelling, I would have liked to see comparisons of various segmentation approaches (both in the audio and visual domains). For instance, if we assume access to perfect segmentation in both modalities, what insights emerge? Additionally, it would be valuable to explore what is learned through the grounded representations and evaluate the system on tasks such as multi-modal semantics.
That said, the paper is well-written, and I find this research direction highly promising. Understanding what models learn is crucial, and this work exemplifies the kinds of questions researchers should be asking. Unfortunately, I feel that the model lacks sufficient novelty, and the depth of the analysis is not quite enough to elevate this paper beyond borderline for ICLR.