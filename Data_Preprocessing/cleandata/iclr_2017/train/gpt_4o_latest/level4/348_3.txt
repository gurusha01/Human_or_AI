This paper introduces a novel inductive bias within the architecture of (convolutional) neural networks (CNNs). The mathematical foundations and derivations underlying the proposed architecture are presented with rigor and detail. The architecture aims to generate equivariant representations with steerable features while utilizing fewer parameters compared to traditional CNNs, which is particularly advantageous in small data regimes. The paper establishes intriguing and original connections between steerable filters and the concept of "steerable fibers." The design of the architecture draws heavily from the authors' prior work and is also influenced by the "capsules" framework (Hinton, 2011). Experimental comparisons on CIFAR10 demonstrate that the proposed architecture outperforms state-of-the-art designs, such as ResNets, especially in small data settings. However, the absence of empirical evaluations on large-scale datasets like ImageNet or COCO limits the contribution to a primarily theoretical one. Additionally, more empirical analysis of the equivariance properties would have strengthened the work. It is not immediately intuitive why this architecture achieves superior performance on CIFAR10, as it is unclear how capturing equivariances directly aids in distinguishing between object categories. Wouldn't a dataset focused on action recognition in videos, for instance, serve as a more illustrative benchmark?