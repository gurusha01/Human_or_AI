EDIT: Updated score. See additional comment.
I appreciate the central idea of the paper, which stems from the observation in Section 3.0â€”that the authors identify numerous predictable patterns in the independent evolution of weights during neural network training. It is promising to see that a simple neural network can be leveraged to accelerate training by directly predicting weights.
That said, the technical rigor of the current paper is lacking, and I strongly encourage the authors to conduct a more thorough analysis of their approach. Below are some specific recommendations:
- The observations in Section 3.0, which serve as the foundation of the proposed approach, should be explicitly and systematically presented in the paper. At present, they are described in an anecdotal manner.
- A major shortcoming of the paper is the lack of details regarding the training of the Introspection network (I). How effective was the training, as measured by training, validation, and test losses? Additionally, how well does the introspection network need to perform in order to meaningfully accelerate training? These are critical questions for anyone seeking to adopt this method.
- Another significant concern is the absence of baseline comparisons. Could a simpler model, such as a linear or quadratic function, achieve similar results instead of using a neural network? What about employing a straightforward heuristic for adjusting weights? Comparing against these baselines is essential to assess the complexity of the weight evolution patterns captured by the neural network.
- The use of default TensorFlow example hyperparameters, as mentioned by the authors on OpenReview, is not scientifically justified. Instead, the authors should first identify hyperparameters that yield good results within a reasonable timeframe and establish these as the baseline. The benefit of the introspection network should then be demonstrated by showing how it accelerates training while achieving comparable outcomes.
- In their OpenReview discussion, the authors mention experimenting with RNNs as the introspection network, but they claim it "didn't work" with a small state size. What exactly does "didn't work" mean in this context? Did the RNN underfit? It is difficult to believe that a large state size would be necessary for this task. Even if that were the case, memory constraints should not preclude evaluation, as the RNN could process weights in a "mini-batch" mode. However, I believe that exploring other baselines is more critical than revisiting RNNs.
- A question regarding jump points:  
The introspection network (I) is trained on SGD trajectories. When I is used to accelerate training at multiple jump points, if the input weights cross earlier jump points, I receives input data from a weight evolution that deviates from standard SGD (as it has been altered by I). This seems like a potential issue but does not appear to impact the experiments. This observation further underscores the importance of baseline comparisons. It is possible that I is performing a very simple operation that is inherently robust to this issue.
Given the intriguing nature of the main idea, I am open to revising my score if the aforementioned concerns are adequately addressed.