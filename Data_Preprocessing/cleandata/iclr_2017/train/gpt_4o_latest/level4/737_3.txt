The SqueezeNet paper, first released in February 2016, caught my attention upon reading. It presents a set of well-thought-out engineering strategies aimed at reducing parameter memory usage in CNNs for object recognition tasks (e.g., ImageNet). These strategies are logical and effective, achieving an impressive compression ratio of approximately 50x compared to AlexNet (and potentially ~500x when combined with Han, 2015). The results are strong and noteworthy, making the paper a valuable contribution worthy of publication.
Since its release on arXiv, the paper has garnered attention and inspired further extensions by others. This demonstrates its potential for significant impact and underscores the importance of providing it with a permanent, formal publication venue.
On the downside, the proposed architecture has only been evaluated on ImageNet, leaving it uncertain whether the methods generalize to other domains, such as audio or text recognition. Additionally, like many papers focused on architectural refinements, it lacks rigorous mathematical or theoretical justification for the proposed ideas, relying instead on their empirical success and practical sensibility.
Overall, I believe this paper merits acceptance at ICLR, as it represents a meaningful contribution to the ongoing development of deep learning architectures.