This paper presents a set of StarCraft micro-management tasks (focused on controlling individual units during battles). These tasks pose significant challenges for contemporary DeepRL methods due to their high-dimensional and variable action spaces (where the action space corresponds to the task assigned to each unit, and the number of units varies dynamically). In such expansive action spaces, basic exploration strategies (e.g., epsilon-greedy) tend to perform poorly.
The authors propose a novel algorithm, ZO, to address this issue. This algorithm integrates concepts from policy gradient methods, deep networks trained via backpropagation for state embeddings, and gradient-free optimization. The methodology is clearly articulated and benchmarked against several existing baselines. Thanks to the structured exploration enabled by gradient-free optimization, the proposed algorithm significantly outperforms these baselines.
Overall, this is a well-written paper that introduces a novel algorithm to tackle a highly relevant problem. Following the success of DeepRL in handling large state spaces (e.g., visual environments), there is growing interest in applying RL to more structured state and action spaces. The tasks introduced in this work serve as compelling testbeds for such challenges.
To enhance reproducibility, it would be beneficial if the authors could share the source code or detailed specifications of their tasks, enabling other researchers to benchmark against their results.
I found Section 5, which describes the raw inputs and feature encodings, somewhat challenging to follow. The authors could improve clarity in this section and might also consider releasing the source code for their algorithm or at least the encoder, which would facilitate more precise comparisons by future work.
While the paper discusses baseline comparisons, it does not include value-based approaches that attempt to improve exploration by modeling uncertainty (e.g., Bootstrapped DQN). Including such comparisons would provide valuable insights into how these alternative methods, which also aim to enhance exploration, perform in this context.
Finally, it would be interesting for the authors to explore and discuss the applicability of action embedding models, such as energy-based approaches (e.g., ...).