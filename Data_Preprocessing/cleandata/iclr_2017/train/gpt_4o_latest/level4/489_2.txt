This paper evaluates various unsupervised sentence embedding methods through a series of auxiliary prediction tasks. By assessing how well classifiers can predict attributes such as word order, word content, and sentence length, the authors aim to determine the type and extent of information captured by different embedding models. The primary comparison is between an encoder-decoder model (ED) and a permutation-invariant model, CBOW. (The paper also includes an analysis of skip-thought vectors, but since these were trained on a different corpus, direct comparisons are challenging.)
The study yields several intriguing and occasionally counter-intuitive findings, which the authors generally analyze and explain effectively. However, I found the discussion surrounding the word-order experiment somewhat lacking. In my view, the key question should have been framed as, "How well does model X perform relative to the theoretical upper bound derived from natural language statistics?" While this is partially addressed in Section 7, I would have preferred the influence of natural language statistics to be introduced earlier, rather than being presented later as an explanation for a 'surprising' result. I had a similar reaction to the discussion of the word-order experiments.
In my opinion, the most compelling insights pertain to the ED model. It is particularly interesting that the LSTM encoder does not appear to leverage natural-language ordering statistics, even though doing so would likely enhance its per-parameter expressivity. Additionally, I find it puzzling that word content accuracy declines for high-dimensional embeddings. This phenomenon might warrant further investigation, perhaps by constraining the decoder's capacity.
In summary, this is an excellent paper that explores the informational content encoded in different types of sentence embeddings. I recommend it for acceptance.