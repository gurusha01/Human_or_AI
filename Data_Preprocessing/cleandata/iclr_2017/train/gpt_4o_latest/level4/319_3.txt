This paper explores the concept of attention transfer between a teacher network and a student network.
The proposed method achieves attention transfer by minimizing the l2 distance between the attention maps of the teacher and student networks across various layers, in addition to minimizing the classification loss and, optionally, incorporating a knowledge distillation term. The authors introduce several activation-based attention mechanisms (e.g., the sum of absolute feature values raised to the power of p or the maximum of values raised to the power of p) and propose a gradient-based attention mechanism (computed as the derivative of the loss with respect to the inputs).
The approach is evaluated on multiple datasets, including CIFAR, CUB/Scene, and ImageNet, demonstrating that attention transfer improves the test performance of student networks. However, even with attention transfer, the student networks consistently underperform compared to the teacher network.
Some observations and questions:
- In Section 3, the authors claim that networks with higher accuracy exhibit stronger spatial correlation between the object and the attention map. While Figure 4 provides qualitative evidence, it would be beneficial to include quantitative results to support this claim.
- How were the hyperparameter values chosen? It would be helpful to analyze the impact of the parameter $\beta$.
- In Figure 7(b), it would be useful to report the teacher network's training and validation losses for comparison.
- The experiments do not clearly highlight the advantages and disadvantages of the different attention mechanisms proposed.
- While attention transfer improves the student network, it does not outperform the teacher. Given that the student networks have fewer parameters, it would be interesting to quantify the corresponding computational speed-up. Additionally, if the teacher and student share the same architecture, does attention transfer provide any tangible benefits?
In summary:
Strengths:
- The paper is clearly written and well-motivated.
- The proposed method consistently improves the student network's performance compared to training the student alone.
Weaknesses:
- The student networks perform worse than the teacher models.
- It is unclear which attention mechanism is most suitable for specific scenarios.
- The novelty is somewhat incremental compared to FitNet.