In this work, the authors introduce a method to pretrain the encoder and decoder of seq2seq models on a large corpus of unlabeled data using a language modeling (LM) objective. They demonstrate that this approach leads to performance improvements in tasks such as machine translation and abstractive summarization.
Although the benefits of pretraining seq2seq models have been recognized in prior research and explored in several studies (e.g., Zoph et al. 2016, Dai and Le 2015), I believe this is the first study to employ LM-based pretraining for both the encoder and decoder components. The proposed method is straightforward, yet it yields significant performance gains (e.g., +2.7 BLEU on NMT). Furthermore, the authors conduct comprehensive ablation studies to investigate the sources of these improvements. Therefore, I recommend that this paper be accepted.