This paper introduces a formalization of the problem setting where only a subset of available MDPs has an associated reward signal. The authors term this setting "semi-supervised reinforcement learning" (SSRL), drawing an analogy to semi-supervised learning, where only a subset of the dataset has labels. To address SSRL, the authors propose a method called semi-supervised skill generalization (S3G), which leverages the maximum entropy control framework. The proposed approach is conceptually straightforward and essentially implements an EM algorithm with partial labels: the method alternates between estimating a reward function (parameterized) and optimizing a control policy based on this reward function. The authors validate their approach through experiments conducted on four tasks (obstacle, 2-link reacher, 2-link reacher with vision, and half-cheetah) in the MuJoCo environment.
The paper is well-written and generally clear. While the appendix provides additional context, a few implementation details appear to be missing, which could hinder the full reproducibility of the experiments. However, the authors indicate that they will release the code.
The connection to inverse reinforcement learning is appropriately established. That said, the paper does not reference off-policy policy learning. For example, it seems that the \(\tau \in D_{samp}\) term in equation (3) might benefit from variance reduction techniques, such as TB(\(\lambda\)) [Precup et al., 2000] or Retrace(\(\lambda\)) [Munos et al., 2016].
The experimental results are compelling, but the sentence "To extensively test the generalization capabilities of the policies learned with each method, we measure performance on a wide range of settings that is a superset of the unlabeled and labeled MDPs" could benefit from additional clarification and numerical details for the different scenarios. Alternatively, if "superset" is meant to imply "union," replacing the term would improve clarity. This clarification might also shed light on the poor performance of the "oracle" method in the "obstacle" and "2-link reacher" tasks, aligning with the explanation provided in the paper: "In the obstacle task, the true reward function is not sufficiently shaped for learning in the unlabeled MDPs. Hence, the reward regression and oracle methods perform poorly."
Minor correction on page 4: "5-tuple \(M_i = (S, A, T, R)\)" should be referred to as a 4-tuple.
Overall, this is a solid and well-executed paper. However, I am not entirely confident that all relevant connections and references to prior work have been fully addressed, which is reflected in my confidence score of 3.