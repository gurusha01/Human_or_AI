I have reviewed the authors' response and stand by my initial rating.
---
This paper presents a method for incorporating the directed acyclic graph (DAG) structure of data into word/code embeddings to utilize domain knowledge and improve the training of an RNN in low-data scenarios. The approach is applied to medical visit codes, where each code belongs to an ontology that can be represented as a DAG. In this DAG, codes are leaf nodes, and different codes may share common ancestors (non-leaf nodes). Instead of embedding only the leaf nodes, the method also embeds the non-leaf nodes, combining the embeddings of a code and its ancestors using a convex sum. This convex sum functions as an attention mechanism over the representations, where the attention weights are determined by the embeddings and the parameters of an MLP. This design allows the model to decouple the learning of code embeddings from the interactions between codes. The code embeddings are initially pretrained using GloVe and subsequently fine-tuned.
The model is rigorously evaluated on two medical datasets, with various experimental setups to isolate the impact of the DAG structure (e.g., GRAM or GRAM+ vs. RNN or RandomDAG) and the effect of pretraining the embeddings (e.g., RNN+ vs. RNN, GRAM+ vs. GRAM). Both factors are demonstrated to contribute to achieving the best performance, and the evaluation methodology appears comprehensive.
The paper is well written, and the authors effectively justify the use of MLP-based attention over a simple dot product of embeddings.
I have only two comments:
1) Why is a softmax applied in equation 4, given that the loss function is a multivariate cross-entropy (where multiple codes in the predicted visit can be equal to 1) rather than a single-class cross-entropy?
2) What is the dimensionality of the embeddings (m)?