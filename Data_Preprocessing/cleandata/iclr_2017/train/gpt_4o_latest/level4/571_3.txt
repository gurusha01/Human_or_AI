The paper introduces two methods for enhancing generative models, both relying on likelihood ratio estimates. These methods are tested on synthetic data and the MNIST dataset for tasks involving sample generation and semi-supervised learning.
Although the concept of boosting generative models and the proposed techniques are intriguing, the reviewer finds the experimental evaluation lacking for the following reasons:
1. The bagging baseline described in Section 3.1 appears to involve merely refitting a model to the same dataset, raising the probability to the power of alpha, and then renormalizing. While this makes the distribution more peaked, it is unclear why this serves as a meaningful baseline. If this interpretation is incorrect, clarification would be appreciated.
2. In Section 3.2, the sample generation experiment employs a Markov chain that converges very slowly, as evidenced by the similarity between plots c and f, d and g, and e and h. This raises doubts about whether the generated samples truly originate from the stationary distribution. A qualitative evaluation using AIS seems necessary to address this concern.
3. Additionally, the choice of alphas in the same section appears somewhat arbitrary. It would be helpful to understand the outcomes when a more straightforward choice, such as alpha_i=1 for all i, is applied.
4. The semi-supervised classification results reported are difficult to interpret, as the baseline RBM seems to perform comparably to the boosted models, offering limited insight into the benefits of the proposed approach.
Overall, the work is mostly well-written and, to the best of the reviewer's knowledge, original.