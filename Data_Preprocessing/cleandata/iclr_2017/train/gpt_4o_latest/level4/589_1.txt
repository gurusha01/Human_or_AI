The paper introduces a method that integrates graph convolution with RNNs to address problems where the inputs are graphs. The two main contributions are: (i) employing a graph convolutional layer to extract features, which are subsequently processed by an RNN, and (ii) substituting matrix multiplications with graph convolution operations. Contribution (i) is demonstrated on language modeling, achieving lower perplexity on the Penn Treebank (PTB) dataset compared to LSTM. Contribution (ii) surpasses the performance of LSTM + CNN on the moving-MNIST dataset.
However, both proposed models/ideas are relatively straightforward and align with the prevailing trend of combining different architectures. For example, the concept of replacing matrix multiplications with graph convolution represents only a minor extension of the work by Shi et al.
With respect to the PTB experiment (section 5.2), I have reservations about the methodology. Specifically, instead of tuning the models using the provided development set, the authors opted to use a pre-existing configuration designed for a different model without adjustment.
Pros:
- Strong experimental results
Cons:
- Ideas lack novelty
- PTB experiment methodology is questionable