The paper introduces a modified knowledge distillation framework that reduces the discrepancy between the aggregated statistics of feature maps from the teacher and student networks. The authors provide empirical evidence showing that the proposed method outperforms the FitNet-style distillation baseline.
Pros:
+ The proposed methods are evaluated on a variety of computer vision datasets.  
+ The paper is generally well-written.
Cons:  
- The approach appears to be restricted to convolutional architectures.  
- The use of "attention" terminology in the paper is misleading. The method essentially distills the summed squared values (or other statistics such as summed Lp norms) of activations in a hidden feature map, rather than leveraging traditional attention mechanisms.  
- The gradient-based attention transfer feels disconnected from the rest of the work. The gradient-based methods are neither directly compared to nor integrated with the "attention-based" transfer, making them seem like a tangential addition with limited contribution to the overall framework.  
- The computation of the induced 2-norm in Eq. (2) is unclear. Since \( Q \) is a matrix in \( \mathbb{R}^{H \times W} \), its induced 2-norm corresponds to its largest singular value, which could be computationally expensive to calculate. It is unclear whether the authors intended to refer to the Frobenius norm instead.
In summary, while the proposed distillation method demonstrates strong empirical performance, the paper suffers from organizational issues, unclear terminology, and ambiguous notation.