This paper introduces an empirical Bayesian framework for learning both the parameters of a neural network and their priors. The proposed method employs a mixture model prior over the weights, resulting in a clustering effect in the weight posterior distributions, which are approximated using delta peaks. This clustering effect is leveraged for parameter quantization and compression of the network parameters. The authors demonstrate that their approach achieves compression rates and predictive accuracy comparable to those of related methods.
In contrast to earlier work by [Han et al., 2015], which employs a three-stage process involving pruning small-magnitude weights, clustering the remaining weights, and optimizing the cluster centers, the current work offers a more principled approach. It replaces the ad-hoc multi-stage structure with a unified, iterative optimization process.
The first experiment, detailed in Section 6.1, illustrates that the empirical Bayes approach—without incorporating hyper-priors—induces a strong clustering effect and sets many weights to zero. For example, a compression rate of 64.2 is achieved on the LeNet300-100 model. However, the text in Section 6.1 refers to Figure C, which appears to be a typographical error and should likely refer to Figure 1.
Section 6.2 explores the use of hyper-priors, with their parameters and other hyper-parameters (e.g., learning rates) optimized using Spearmint [Snoek et al., 2012]. Figure 2 visualizes the performance of various hyper-parameter configurations, with each trained network corresponding to an accuracy-compression rate point on the graph. While the authors claim that the best results align along a linear trend, this interpretation seems overly optimistic given the limited data. A brief discussion on whether such a linear relationship is theoretically expected would enhance the section, as the results currently lack sufficient interpretation.
Section 6.3 presents results for CNN models and compares them to recent work by [Han et al., 2015] and [Guo et al., 2016]. The proposed method achieves comparable compression rates and accuracy. However, the authors acknowledge that their algorithm is currently too slow for practical use on larger models like VGG-19. Although some results for VGG-19 are briefly reported, they are not compared to related methods. It would be valuable to explain the factors contributing to the slower training process compared to standard training without weight clustering and to clarify how the algorithm scales with respect to the model size and dataset characteristics.
The primary contribution of this paper is experimental, utilizing well-established concepts from empirical Bayesian learning to introduce weight clustering effects in CNN training. Nonetheless, it is noteworthy that such a straightforward approach achieves results comparable to state-of-the-art yet more ad-hoc network compression techniques. The paper could be strengthened by providing a clearer description of the training algorithm and its scalability to larger networks and datasets. Additionally, a more detailed discussion of the hyper-parameter search process (presumably not using test data) and how competing methods handle hyper-parameter optimization for the accuracy-compression tradeoff would be valuable. Ideally, methods should be evaluated across multiple points on this tradeoff curve.