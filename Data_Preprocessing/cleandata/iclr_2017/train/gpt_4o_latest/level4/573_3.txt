This paper proposes leveraging Generalized Advantage Estimation (GAE) to optimize deep neural networks (DNNs) for information-seeking tasks. The problem is formulated within a reinforcement learning (RL) framework, and the proposed approach explicitly incorporates information gain to encourage exploration.
While both GAE and DNNs have been previously applied in RL contexts, the novelty of this work appears to lie in the explicit modeling of information gain. However, the paper lacks sufficient empirical evidence to convincingly demonstrate the advantages and general applicability of the proposed method. A direct, controlled comparison with prior RL frameworks that do not model information gain is absent. For instance, in the cluttered MNIST experiment, the comparison against Mnih et al. (2014)—which is somewhat outdated—uses two different settings. However, the inputs to the two methods differ in both settings, making it unclear what factors contribute to the observed performance differences.
Additionally, the experimental section is disorganized and difficult to follow. Including a table to summarize the results would significantly improve clarity.