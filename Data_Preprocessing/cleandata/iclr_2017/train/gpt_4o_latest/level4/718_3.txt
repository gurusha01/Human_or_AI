Regrettably, the paper lacks sufficient clarity for me to fully grasp the proposed contributions. At a high level, the authors appear to introduce a generalization of the traditional layered neural network architecture (of which MLPs are a specific instance), utilizing arbitrary nodes that exchange messages. The paper subsequently attempts to demonstrate that their layer-free architecture can replicate the computations of a standard MLP. However, this reasoning seems circular. The detailed methodology is also difficult to follow: while the authors aim to depart from layer-based designs relying on matrix-vector products, Algorithm 4 still employs matrix-vector products for both the forward and backward passes. Although the implementation uses asynchronously communicating nodes, the "locking" nature of the computation renders it functionally identical to the conventional approach.