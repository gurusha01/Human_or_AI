This paper introduces a semi-supervised approach for "self-ensembling," where the model leverages a consensus prediction (derived from earlier epochs) as an additional target to regress to, alongside the standard supervised learning loss. The method is conceptually linked to the "dark knowledge" framework, and the paper demonstrates that ladder networks can be an effective technique for settings with limited labeled data (though not exclusively). Two variations of the proposed method are presented: one that is computationally intensive and exhibits high variance due to requiring two passes through the same example per step, and a temporal ensembling approach that is more stable and computationally efficient but demands greater memory usage and introduces an additional hyper-parameter.
Overall, I have a favorable impression of this work. However, I see some limitations. Specifically, the temporal ensembling method may require significant memory and involve non-trivial infrastructure or bookkeeping for large-scale datasets like ImageNet. Additionally, I found the experiments in Figure 2 and Section 3.4 regarding tolerance to noisy labels quite perplexing. It seems highly implausible that a classifier could achieve either 30% or ~78% accuracy (depending on whether temporal ensembling was applied) when 90% of the labels are randomized. I struggle to understand how this result is possible.
Minor comments:
- Please bold the best-in-category results in your tables.
- It would be helpful to discuss the ramp-up of w(t) in the main paper.
- The authors should consider including state-of-the-art results for the fully-supervised case in their tables, rather than only their own results.
- I am unclear why the authors opted not to use more SVHN examples. The justification that it would be "too easy" feels somewhat unconvincing; using all examples would also facilitate comparisons with prior work.