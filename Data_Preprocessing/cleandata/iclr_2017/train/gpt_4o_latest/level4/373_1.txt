The authors present transfer learning variants for neural network-based models, applied across several NLP tagging tasks.
While the field of multi-task learning is vast, the proposed approaches do not appear particularly novel from a machine learning perspective: components of a general NLP architecture are shared, with the extent of shared "layers" depending on the task at hand.
The originality primarily lies in the specific architecture employed within the context of NLP tagging tasks.
The experimental results indicate that the proposed approach performs effectively in scenarios with limited labeled data (Figure 2). However, Table 3 demonstrates only marginal improvements when applied at full scale.
The results in Figure 2 are somewhat questionable: the authors seem to have fixed the architecture size while varying the amount of labeled data. It is plausible that tuning the architecture for each data size could have yielded better performance.
In summary, while the paper is well-written, its novelty appears somewhat constrained, and the experimental evaluation is underwhelming.