This paper investigates a range of memory-augmented architectures (key, key-value, key-predict-value) alongside simpler, near memory-less RNN architectures. Employing an attention model with access to these various decompositions is an intriguing concept and merits further exploration, particularly in alternative tasks where such a model might demonstrate even greater potential. The results on the Wikipedia corpus are compelling and showcase a diverse set of model types, which is where the proposed models exhibit their greatest strengths. However, the same models applied to the CBT dataset yield comparable but less persuasive evidence of the distinctions between the architectures.
The authors have also made their Wikipedia corpus publicly available, which I regard as a positive and valuable contribution. Upon examining the dataset, I believe that a model capable of better capturing longer-term dependencies could achieve improved performance on this Wikipedia dataset. For instance, in the first article of train.txt, which discusses a person named "George Abbot," the token "Abbot" reappears only after a gap of 40 tokens in the subsequent sentence, and then again 15 tokens later. Most occurrences of "Abbot" are separated by dozens of timesteps. Conducting an analysis based on readily available metrics, such as the frequency of token reoccurrence or average sentence length, could provide useful insights into the optimal length of an attention window for this dataset.
Overall, this is a well-articulated paper that raises important questions about the spans utilized in current language modeling approaches and offers a promising foundation for future research directions.