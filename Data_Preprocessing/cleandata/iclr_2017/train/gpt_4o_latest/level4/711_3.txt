The authors introduce RASOR as a solution to the task of identifying the optimal answer span for a given question. The paper primarily emphasizes modeling the interaction between the question and the corresponding answer spans. While the proposed idea is reasonable and has merit, it is not particularly groundbreaking. The analysis presented is interesting and holds potential utility. However, it would be beneficial if the authors further explored and evaluated various boundary prediction models to strengthen their argument for the necessity of globally modeling the span scores.
The core concept of RASOR lies in globally normalizing and ranking the scores of potential answer spans. This is achieved by first using LSTMs to model the hidden representations of all words. The representation of a text span is then constructed by concatenating the hidden vectors of the span's start and end words. While this approach is sound, it does not represent a major leap forward. Furthermore, as shown in Table 6, the improvement over the end-point prediction baseline is relatively modest.
I appreciate the authors' efforts in conducting multiple analysis experiments, some of which are particularly insightful. For instance, the finding that question-independent representations significantly contribute to performance is noteworthy. However, I would like the authors to provide a clearer explanation of what specifically enables their model to outperform Match-LSTM. Is the improvement attributable to hyperparameter tuning, or does it stem from the incorporation of question-independent representations?
Another strength of the proposed model is its relative simplicity, which opens up the possibility of integrating the proposed techniques with other recently developed methods.