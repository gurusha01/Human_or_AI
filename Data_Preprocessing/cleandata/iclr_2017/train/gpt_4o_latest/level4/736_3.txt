Overall, I find this paper to be engaging and it demonstrates empirical performance improvements over the baselines. However, my primary concern lies in the paper's technical depth, as its main contribution can be summarized as follows: instead of maintaining batch normalization (BN) mean and bias estimations across the entire model, the authors propose estimating them on a per-domain basis. I am uncertain about the novelty of this approach, as it appears to be a straightforward extension of the original batch normalization concept. As such, I believe this work is more suitable for a short workshop presentation rather than a full conference paper.
Detailed Comments:
Section 3.1:  
I respectfully disagree with the claim that the primary purpose of BN is to align the distribution of training data. While this alignment may occur as a side effect, the main goal of BN is to regulate the scale of the gradient, enabling the training of very deep models without encountering vanishing gradient issues. The observation that intermediate features from different datasets form distinct groups in a t-SNE embedding is plausible. However, this is not a unique property of BN; similar results can be observed when visualizing intermediate features from models like AlexNet. Therefore, the premise presented in Section 3.1 appears to be inaccurate.
Section 3.3:  
I share the concern raised by another reviewer regarding this section. The content seems somewhat disconnected from the overarching idea of AdaBN. Equation 2 essentially reiterates a well-known fact: the combination of a BN layer and a fully connected layer results in a linear transformation. This holds true for both the original BN and the proposed method. Consequently, this argument does not contribute substantial theoretical depth to the paper. (More broadly, the novelty of the paper appears limited.)
Experiments:
- Section 4.3.1:  
This section does not provide an accurate assessment of the proposed method's "effectiveness." Instead, it verifies a straightforward observation: in the baseline approach, source domain features are normalized into a Gaussian distribution, while the proposed method explicitly normalizes target domain features into the same Gaussian distribution. As a result, it is unsurprising that the KL divergence between these distributions is reducedâ€”this is explicitly enforced by the method. However, this reduction in KL divergence does not directly translate to improved classification performance.
- Section 4.3.2:  
The sensitivity analysis is an intriguing aspect of the paper, as it indicates that only a small number of images are required to account for domain shift in AdaBN parameter estimation. This suggests that a single "whitening" operation may suffice to mitigate domain bias. For instance, in both cases presented, a single batch is sufficient to recover approximately 80% of the performance gain. However, the figure does not provide data for even smaller numbers of examples. It would be valuable to include a comparison of these approaches and conduct a more detailed analysis of the contributions from each layer of the model. The current analysis feels somewhat superficial.