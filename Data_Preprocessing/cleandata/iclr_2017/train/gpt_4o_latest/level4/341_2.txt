The paper introduces an intriguing new problem formulation for imitation learning: an agent aims to replicate a trajectory demonstrated by an expert, but the expert's trajectory is provided in a different state or observation space than the one available to the agent (while the underlying MDP dynamics remain consistent). The authors propose a solution that integrates recent advancements in domain confusion losses with a generative adversarial network (GAN)-based inverse reinforcement learning (IRL) method.
I find the problem to be relevant and agree with the authors that it offers a more natural and potentially broader framework for imitation learning. However, there are several issues with the current version of the paper that prevent it from being a strong exploration of this novel idea. Below, I outline my concerns in no particular order:
- The paper occasionally feels rushed (this is particularly evident in the experiments, as discussed below) and makes some bold claims in the introduction that are not adequately supported by the proposed approach. For instance, the statement: "Advancements in this class of algorithms would significantly improve the state of robotics, because it will enable anyone to easily teach robots new skills" appears overstated. The proposed method, as I understand it, shares the same challenges as standard GAN training (e.g., instability) and relies on a highly accurate simulator to perform well (since TRPO requires a large number of simulated trajectories per step). This undermines the claim. Additionally, there are instances of ungrammatical sentences or inconsistent tense usage, which make the paper harder to follow. For example, on Page 2: "we find that this simple approach has been able to solve the problems."
- The concept of third-person imitation learning is appealing, clear, and (to the best of my knowledge) novel. However, instead of exploring how existing IRL algorithms could be broadly adapted to this setting, the authors focus on a specific approach they find promising (using GANs for IRL) and extend it. A considerable portion of the paper is spent arguing why current IRL algorithms are unsuitable for the third-person setting. However, I do not see why the proposed GAN-based approach would fare any better than other existing IRL methods. For example, why couldn't behavioral cloning also be extended with a domain confusion loss in the same way? It would have been more compelling to discuss which algorithms can and cannot be adapted similarly (and to test them). One straightforward alternative might involve training two autoencoders for both domains, sharing higher layers, and applying a domain confusion loss at the highest layer. Would this not yield features that are directly usable? If not, why?
- While the argument that existing IRL algorithms would struggle in the proposed setting is plausible, it is disappointing that no empirical validation of this claim is provided. For instance, no comparisons are made to assess what happens if one performs supervised learning (behavioral cloning) using the expert observations and then transfers to the new domain. How effective would this approach be in practice? Additionally, how quickly can different IRL algorithms solve the target task in general (assuming a first-person perspective)?
- Although I appreciate the effort to frame the experiments around specific research questions, I feel that the posed questions detract somewhat from the central theme of the paper. For example, Question 2 shifts the focus to the use of additional velocity information, which seems tangential. Similarly, the experiments addressing Question 3 only evaluate two hyperparameters, while ignoring others such as TRPO parameters, the number of rollouts per iteration, the number of expert episodes provided, and the GAN design choices. While I understand that a thorough evaluation of all parameters is not feasible within the scope of a conference paper, additional experiments or at least a discussion of these aspects would have been valuable.
- The experimental evaluation does not clearly present the computational cost of TRPO training with the learned reward function. How many rollouts are required at each step?
- The experiments lack critical details. For example, how are the expert trajectories generated? In the pendulum experiment, are the domains identical apart from the pole's color? If so, it is surprising that such a minor change has such a significant impact. Figure 3 reports average performance over five trials, but what about Figure 5? If Figure 5 also represents average performance, what is the variance? Furthermore, given the known challenges of training GANs, how often does training fail? Were the hyperparameters reused across all experiments, or did they require tuning for each case?
UPDATE:  
I have revised my score. Please refer to my comments on the authors' rebuttal for further clarification.