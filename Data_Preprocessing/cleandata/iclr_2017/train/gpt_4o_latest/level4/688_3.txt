The paper introduces a compelling modification to the PoWER algorithm, which is well-justified. However, the primary shortcoming lies in the absence of comparisons with alternative methods and evaluations on more complex problems. The experimental results do not sufficiently demonstrate the claimed advantages, generality, and scalability of the approach over existing methods. Establishing this confidence does not necessarily require testing the method on all large-scale domains or conducting an exhaustive hyperparameter search, but it should extend beyond the current domains. For instance, the Cartpole task involves optimizing only five parameters, and the ad targeting task lacks comparisons with other methods. Given that this approach builds on PoWER and is closely related to RWR, it is plausible that its performance may be constrained when applied to other domains or benchmarked against alternative methods. For example, standard techniques like importance sampling-based off-policy learning are known to face challenges in high-dimensional or continuous action spaces, and similar limitations may arise for RWR/PoWER-like methods due to their connection with entropy-regularized reinforcement learning.