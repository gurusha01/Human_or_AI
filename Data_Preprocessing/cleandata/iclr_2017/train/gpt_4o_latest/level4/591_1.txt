The paper introduces a novel criterion, termed "sample importance," to analyze the influence of individual samples during the training process of deep neural networks. However, this criterion is not clearly defined. Specifically, while \(\phi^ti\) is defined, the term \(\phi^t{i,j}\) is not, leaving the definition incomplete. From the context, it appears that sample importance is the squared L2 norm of the gradient for a sample \(i\) at time \(t\), scaled by the squared learning rate. This scaling is puzzling, as the learning rate should not logically influence the importance of a sample in this context.
The paper includes experiments conducted on the widely used MNIST and CIFAR datasets, employing network architectures, hyperparameters, and initializations that are generally appropriate for these datasets. However, the size of the hidden layers is somewhat small for MNIST and extremely small for CIFAR, which likely contributes to the poor performance observed in Figure 6, where the error rate on CIFAR reaches 50%.
The analysis of how sample importance evolves during training across different layers leads to conclusions that are largely trivial:
- The observation that "the overall sample importance is different under different epochs" is expected, as the gradient norm naturally varies over time.
- The finding that "the output layer always has the largest average sample importance per parameter, and its contribution peaks early in training before declining" is also unsurprising. This is because gradients are strongest at the output layer due to backpropagation and tend to diffuse as they propagate to earlier layers, which are less stable. Additionally, as training progresses, it is expected that the output layer's gradients diminish. Furthermore, the gradient norm is influenced by the scaling of the variables, which should be taken into account.
The question posed in Figure 4, "Is Sample Importance the same as Negative log-likelihood of a sample?" is misguided. The answer is obviously no.
The results on CIFAR are notably poor, undermining the general applicability of the findings. For MNIST, the performance is difficult to interpret due to the presentation in Figure 7, where the error rate should be confined to a range of 0–10% or 0–20% for clarity.
Despite these significant issues (and others not mentioned here), the paper does highlight some intriguing points. The distinction between "easy" and "hard" samples appears to align, albeit in a very preliminary manner, with intuitive expectations: easy samples correspond to representative or canonical examples, while hard samples represent edge cases. Additionally, the experiments are well-structured and clearly presented.