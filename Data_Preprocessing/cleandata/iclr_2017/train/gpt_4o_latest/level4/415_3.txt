Encouraging orthogonality in weight features has been shown to be beneficial for deep networks in numerous prior studies. In this work, the authors propose an explicit regularization term to promote de-correlation among weight features within a layer and to encourage orthogonality. Additionally, they provide insights into why and how negative correlations should be avoided to achieve better de-correlation.
Orthogonal weight features are particularly advantageous for improving generalization when dealing with a large number of trainable parameters and limited training data, conditions that often lead to overfitting. As noted by the authors, biases can aid in de-correlating feature responses even when the weight features themselves are correlated. Regularization methods such as OrthoReg are especially useful for training deeper and more compact networks, where the representational capacity of each layer is constrained, leading to improved generalization.
While the performance improvements are modest, the research direction and the observations presented in this work are promising.