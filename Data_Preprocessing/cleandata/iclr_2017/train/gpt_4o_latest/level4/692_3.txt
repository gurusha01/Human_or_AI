This paper introduces a hierarchical attention-based approach for document classification. The core concept involves first applying a bidirectional LSTM to extract a global context vector, followed by another attention-based bidirectional LSTM that leverages the final hidden state from the initial pass to weight local context vectors (TS-ATT). Additionally, a simpler architecture is proposed (SS-ATT), which eliminates the first LSTM and directly uses the output of the second LSTM as the global context vector. Experiments are conducted on three datasets, but the results generally fall short of state-of-the-art performance.
While the proposed idea is interesting, the experimental results are not sufficiently compelling to validate the new model architecture. For instance, why is the Yelp 2013 dataset used in this work smaller than the ~300k documents reported in the original Tang et al., 2015 paper? I also observed that the other datasets are relatively small. Is this due to scalability issues with the proposed model? Furthermore, the results from Tang et al., 2015 should be included in Table 2 for comparison, as their model achieves 65.1% accuracy on Yelp 2013â€”significantly higher than the results reported here. Additionally, I recommend avoiding phrases like "Learning to Understand" when describing the model.
In conclusion, I believe this submission would be more suitable for a workshop rather than the main conference.
Minor comments:
- Correct "gloal" to "global."
- While not requiring pretrained embeddings is a nice feature, it is not particularly impactful, as many models perform well without them.