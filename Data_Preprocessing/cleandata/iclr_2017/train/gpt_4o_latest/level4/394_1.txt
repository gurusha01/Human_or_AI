The authors introduce a straightforward method for regularizing recurrent neural networks. The approach is conceptually similar to dropout, but instead of zeroing out units, it substitutes them with their respective values from the preceding time step on an element-wise basis with a certain probability.
Overall, the paper is well-written, and the method is clearly presented, aside from issues raised during the pre-review question phase. The related work section is comprehensive and likely represents the most thorough review of the literature on RNN regularization currently available.
The experimental section compares the proposed method against the current state-of-the-art on a set of NLP benchmarks and a synthetic task. All experiments are conducted on sequences involving discrete values. Additionally, one experiment demonstrates that the sequential Jacobian is significantly higher for long-term dependencies compared to the dropout baseline.
While the paper shows significant promise, I have several concerns:
1) As noted during the pre-review phase, the authors have not conducted a full hyperparameter search, which is a standard practice in the field. A proper model selection process is missing, despite the fact that the author count suggests sufficient resources to perform such an analysis. This omission is particularly concerning given that Table 2 highlights that validation error is not a reliable predictor of test error for the dataset in question. This raises serious concerns about overfitting during model selection. Furthermore, the performance improvement from Zoneout appears marginal in other tasks.
2) The mathematical investigation of Zoneout is insufficient. For instance, an analysis of the gradients from unit K at time step T to unit K' at time step T-R would have been valuable, especially since these gradients are not necessarily zero in the case of dropout. Additionally, the question of whether Zoneout has a variational interpretation, akin to Yarin Gal's work, is left unexplored. While one could speculate that Zoneout might fit within a ResNet framework with dropout applied to the incremental parts, the paper does little to address why Zoneout performs well. The existing literature offers numerous starting points for such an analysis, but these opportunities are not pursued.
3) The datasets used in the experiments are exclusively symbolic. It would have been beneficial to evaluate the method on a broader range of tasks, such as those involving continuous data from dynamical systems. It is unclear whether the proposed method would generalize to such settings.
There is currently a proliferation of "tricks" aimed at improving RNN training. How does Zoneout distinguish itself? While the idea is elegant and easy to implement, the paper falls short in several areas: the experimental results are unconvincing (see points 1 and 3), and the theoretical contributions are limited (see point 2).
In summary, the paper amounts to an "epsilon improvement" with strong writing, but it is undermined by mediocre experimental evaluation and a lack of theoretical depth.