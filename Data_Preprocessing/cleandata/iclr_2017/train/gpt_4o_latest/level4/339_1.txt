The authors propose a straightforward method to integrate a cache into neural language models, effectively enabling a copying mechanism for recently used words. In contrast to much of the related work on copying mechanisms in neural networks, this approach does not require training through long-term backpropagation, making it both efficient and scalable to significantly larger cache sizes. The authors demonstrate notable improvements in language modeling performance by incorporating this cache into RNN baselines.
The primary contribution of this paper lies in the observation that using the hidden states \( hi \) as keys for words \( xi \), and \( h_t \) as the query vector, naturally results in a lookup mechanism that functions effectively without requiring backpropagation-based tuning. While this is a straightforward observation and may already be familiar as folk knowledge to some researchers, it has valuable implications for scalability, and the experimental results presented are compelling.
The core idea of leveraging locally-learned representations for large-scale attention, where backpropagation would typically be computationally prohibitive, is intriguing and holds potential for enhancing other types of memory networks.
My primary critique of this work is its simplicity and incremental nature relative to prior literature. As a modest modification of existing NLP models, albeit with strong empirical results, simplicity, and practical utility, it may be better suited for an NLP-focused conference. Nonetheless, I believe that approaches that simplify and refine recent advancements into efficient, broadly applicable tools deserve recognition. This method is likely to benefit a substantial portion of the ICLR community, and I therefore recommend its publication.