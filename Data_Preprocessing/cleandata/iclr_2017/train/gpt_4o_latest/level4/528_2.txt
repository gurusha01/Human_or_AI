The paper explores various modeling strategies for designing differentiable programming languages. The authors present four recommendations, which are evaluated on a set of 13 algorithmic tasks involving lists, such as determining the "length of the list" or retrieving the "k-th element from the list." The solutions are learned from input/output example pairs, with 5 examples used for training and 25 for testing.
The primary distinction between this work and differentiable architectures like NTM, Neural GPU, and NRAM lies in the focus on automatically generating code to solve the specified tasks, rather than relying on a black-box neural architecture.
My main concern pertains to the experiments. It would be beneficial to include a comparison with some of the neural networks discussed in the related work. Additionally, it would be helpful to evaluate the model on standard problems commonly used to benchmark these neural architectures, such as "sorting," "merging," and "adding." I am also curious about how well this approach generalizes to other types of programs that cannot be addressed using the prefix-loop-suffix structure.
Another issue is that despite 1) the simplicity of the tasks, 2) the highly constrained structure of the solutions, and 3) the use of extensions that perform much of the heavy lifting, the proposed model still struggles to find solutions. For instance, the A+L model with the "loop" extension fails to solve the "list length" task in 84% of the runs.
Pros:
- Produces interpretable code instead of relying on a black-box neural architecture.
- Demonstrates the ability to learn from a very small number of examples.
Cons:
- Limited performance, as it only works for very simple tasks.
- Lacks comparisons with neural architectures.