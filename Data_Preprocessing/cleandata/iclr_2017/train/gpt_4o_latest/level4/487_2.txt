The paper introduces a sparsely connected network along with an efficient hardware architecture, achieving up to 90% memory savings compared to traditional fully connected neural network implementations. 
The authors achieve this by removing certain connections in the fully connected layers, demonstrating improved performance and computational efficiency across three different datasets. A notable contribution is the integration of their approach with binary and ternary connect methods, which yields additional performance gains.
However, I found the paper somewhat difficult to follow due to a potentially misleading statement: "In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs)." This phrasing initially led me to believe that LFSRs reduce connections by storing some information within the registers. In reality, LFSRs are merely employed as random binary generators. While any random generator could serve this purpose, LFSRs are chosen here for their suitability in VLSI implementation. 
A clearer explanation might be: "In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated using LFSRs, which are also utilized in the VLSI implementation to disable the connections."
Algorithm 1 essentially describes training a network via backpropagation, where each layer is equipped with a binary mask that disables certain connections. Including this clarification in the text would enhance readability.
It is worth noting that the use of random connections is not a novel concept in CNNs. A similar idea was employed between CNN layers in a 1998 paper by Yann LeCun and collaborators.