The paper introduces an approach to sequence transduction tailored for scenarios where a monotonic alignment between the input and output is plausible. The method assumes that such an alignment can be provided as part of the training data, with the Chinese Restaurant process employed in the experiments.
While the idea is reasonable, its applicability is restricted to domains where monotonic alignment is accessible. However, as highlighted during the pre-review discussions, there exists a significant body of related work with strong overlaps, including probabilistic models with hard alignment (e.g., Sequence Transduction With Recurrent Neural Network, Graves et al., 2012) and efforts to incorporate external alignments into end-to-end models (e.g., A Neural Transducer, Jaitly et al., 2015). Consequently, I find the proposed approach lacking in sufficient novelty.
Another concern pertains to the evaluation methodology. Comparing the proposed model, which relies on external alignment, with a vanilla soft-attention model that learns alignments from scratch seems unfair. A more appropriate control experiment would involve pretraining the soft-attention model to match the external alignment. This pretraining could mitigate overfitting, particularly on the small dataset where the proposed approach demonstrates the most significant gains. On larger datasets, such as SIGMORPHON, the improvements are modest and limited to a specific subset of languages.
In summary, the two primary issues with the paper are: (a) insufficient novelty and (b) an unfair comparison between a model trained with external alignment and one trained without it.