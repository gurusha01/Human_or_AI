The Neural Turing Machine (NTM) and related "external memory models" have demonstrated the capability to learn algorithmic solutions by employing differentiable analogues of traditional memory structures. Specifically, the NTM, Differentiable Neural Computer (DNC), and similar approaches provide mechanisms for shifting a memory access head to linked memory locations relative to the current read position.
The NTM, which is most pertinent to this work, employs a differentiable version of a Turing machine tape. Its controller outputs a kernel that enables "soft" shifts of the head, allowing the model to read and write sequences. However, since this soft shifting often "blurs" the head's focus, the controller additionally outputs a sharpening parameter to refocus the distribution and mitigate this issue.
The central idea of this work is to observe that while the NTM mimics a differentiable Turing tape, there is no inherent reason to adhere strictly to the topology of a Turing tape. Instead, the authors propose storing memory at a set of points on a manifold and employing shift operations that form a Lie group. This allows memory points to exhibit diverse relationships, rather than being restricted to the linear structure of Z.
This approach is mathematically elegant, and the authors empirically evaluate models with the shift group R² acting on R² and the rotation group acting on a sphere.
Overall, the paper is well-written and presents a novel idea.
The main limitation of this work lies in its limited practical impact. While the proposed approach is mathematically elegant and potentially advantageous for specific problems where the problem structure aligns with the group structure, it is unclear whether it significantly advances the development of models capable of more general program learning. On the contrary, it is likely to make already complex and computationally slow models, such as the NTM, even slower. Broadly speaking, memory topology appears to be problem-specific and would likely benefit more from being learned rather than predefined.
The baseline used for comparison is overly simplistic and lacks key features, such as sharpening (the NTM's solution to address the smearing of head distributions). Furthermore, the paper does not include comparisons with the NTM's successor, the DNC, which offers a more general mechanism for linking memories based on prior accesses.
Minor issues:
- The footnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can maintain the head's position by gating the link matrix.
- The figures on page 8 are difficult to interpret.
- Figure 2 remains unclear despite the lengthy caption. Adding clearer labels to the images themselves might improve comprehension.