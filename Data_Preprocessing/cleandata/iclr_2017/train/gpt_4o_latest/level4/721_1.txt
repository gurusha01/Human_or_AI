This paper develops a generative model for image patches, where dictionary elements are subjected to gated linear transformations before being combined. These transformations are theoretically motivated using Lie group operators, though in practice, they are implemented as a fixed set of linear transformations. The work is framed as a step toward learning a hierarchy of transformations, but the experiments only explore a single layer (except for a toy example in the appendix).
I appreciate the motivation behind this algorithm. However, its implementation feels quite similar to group or block sparse coding. I found the restriction to linear transformations limiting. Additionally, the experiments were primarily toy examples, showcasing the algorithm's ability to learn groups of Gabor- or center-surround-like features. While this might have been acceptable five years ago, the experimental scope feels insufficient by contemporary standards.
Specific comments:
- In line with common practices in machine learning literature, I tend to associate $x$ with inputs and $w$ with network weights, while latent variables are often denoted as $z$ or $a$. Depending on your target audience, you might consider revising your choice of symbols to make the model easier to interpret.
- Nitpick: Number all equations for easier reference.
- Section 2.2: It feels inconsistent that the transformation is fixed but is still expressed as a function of $x$.
- Section 2.3: The revised text here is somewhat confusing. Initially, I understood that you were using a fixed set of linear transformations, motivated by Lie groups but without actually computing matrix exponentials in the algorithm. However, the equations in the latter half of this section seem to suggest you are indeed working with matrix exponentials. It would be helpful to clarify this point to avoid ambiguity.
- As an aside, another potential solution to the local minima issue is the approach used in Sohl-Dickstein (2010). They introduced blurring operators tailored to each transformation operator, allowing gradient descent to escape local minima by transitioning through coarser (more blurred) scales.
- Section 3.2: I believe you are referring to the number of model parameters when discussing degrees of freedom, rather than the number of latent coefficients to be inferred. This distinction should be made clearer. Additionally, would it be more appropriate to compare reconstruction error while controlling for the number of model parameters or the number of latent variables?
- I am curious whether a convolutional extension of this algorithm could be feasible or would make it more suitable as a generative model for full images.
---
Post-rebuttal update:
Thank you for your detailed rebuttal! I have reviewed it, but it did not significantly alter my assessment of the paper.