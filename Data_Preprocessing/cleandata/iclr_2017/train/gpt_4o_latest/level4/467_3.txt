The authors propose an extension to GANs by introducing an inference pathway from the data space to the latent space and incorporating a discriminator that operates jointly on the latent and data spaces. They demonstrate that the theoretical properties of GANs remain valid for BiGAN and evaluate the unsupervised features learned via the inference pathway in terms of their performance on supervised tasks after retraining the deeper layers.
I observe one structural concern with this paper: As stated in the abstract, the primary objective of the work is to learn unsupervised features (rather than improving GANs), yet a significant portion of the paper is devoted to elaborating on the connection to GANs and their theoretical properties. It is unclear whether these theoretical aspects contribute meaningfully to the goal of learning high-quality features. While reading, I found myself losing track of the focus on unsupervised features until they were revisited on page 6. Aligning the narrative of the paper more closely with this central theme could enhance its clarity and impact.
That said, the BiGAN framework represents an elegant and compelling extension of GANs. However, it is not immediately clear how much the theoretical properties contribute, given that the model does not appear to have fully converged. For instance, Figure 4 suggests that G(E(x)) might primarily be performing a form of nearest-neighbor retrieval (a criticism often leveled at GANs for their potential to memorize samples). Additionally, it would be intriguing to know how well the discriminator performs after training.
Regarding the goal of learning robust features: While the method does not achieve state-of-the-art performance on most of the evaluated tasks (as shown in Tables 2 and 3), it remains competitive. It would be interesting to explore how much these results could improve with enhancements to the BiGAN training process and the convolutional architecture employed.
The paper is well-written and includes most of the necessary details. However, providing additional specifics about the training process (e.g., learning rates, initialization) would aid in reproducing the results.
In summary, the paper introduces a fascinating framework that holds promise for future research, even though the current results are somewhat underwhelming in terms of both feature evaluation and GAN performance.
Minor suggestion: Highlighting the best performance values in Tables 2 and 3 could improve readability.