The paper provides two primary contributions:
1) Demonstrates that uniform quantization performs effectively when combined with variable-length (Huffman) coding.
2) Enhances fixed-length quantization by introducing Hessian-weighted k-means as an alternative to the commonly used vanilla k-means. The motivation behind Hessian weighting is well-articulated, and the paper explains how an efficient approximation can be leveraged "for free" when using the Adam optimizer, which is a clever addition. Compared to vanilla k-means, a key advantage of this method (beyond improved performance) is that it eliminates the need for manual tuning of per-layer compression rates, as this is handled automatically.
In summary, I find the paper compelling: while (1) is not particularly novel, it appears that no prior work has explored this, so it is valuable to confirm that it works well. Contribution (2) is innovative, well-executed, and delivers strong results. The paper is well-written, easy to follow, and presents solid outcomes. My only criticism is that it feels slightly too lengthy.
Minor note â€“ I am still unclear about the explanation regarding "additional bits for each binary codeword for layer indication" in the context of layer-by-layer quantization. Why not simply maintain an array of quantized weight values for each layer, such that q[0][:] stores all quantized weights for layer 0, q[1][:] for layer 1, and so on, with each layer having its own codebook? In this case, the only overhead compared to joint quantization would be storing the codebook for each layer, which seems negligible. I don't fully grasp the necessity of the "additional bit" aspect. However, this is a minor issue and does not impact the overall quality of the paper. The authors may want to clarify this point further, as it could be unclear to others as well (or perhaps I am missing something obvious).