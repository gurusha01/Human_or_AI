Overall, the paper feels like a status update from some of the leading researchers in the field. It is well-written and clear, with intriguing observations, but the insights are somewhat disjointed and do not coalesce into a significant advancement in understanding the capabilities of the Neural GPU model.
Minor comment regarding the term RNN in Table 1: I found Table 1 somewhat confusing, as several columns pertain to models that are technically RNNs. The use of RNNs in applications like translation and word2vec underscores that RNNs can be described in terms of input sequence length, input size, and the sizes (per step) of their input, output, and working memory. This distinction could be clarified further.
Basic model question: How are inputs represented (e.g., as 1-hot vectors for each character), and how are outputs retrieved when the model employs, for instance, 512 "filters"? If both inputs and outputs are 1-hot encoded and processed using the same filters as intermediate layers, then the intermediate activation functions should be interpretable as digits. This would allow us to interpret the filters as implementing a reliable algorithm, such as multiplication-with-carry. Examining the intermediate values could provide insights into why the typically functional models fail in specific pathological cases, as identified in Table 3.
The preliminary experiment on input alignment is intriguing for two reasons: it hints at the potential for effectively utilizing an attentional mechanism, but it also suggests that the model is not currently handling general expression evaluation in the manner a correct algorithm would.
The claims in the abstract about improving the memory efficiency of the Neural GPU seem exaggerated. The paragraph at the top of page 6 describes these improvements as replacing graph unrolling with `tf.whileloop` and using `swapmemory` to offload data to host memory when GPU memory is insufficient. While these are sensible practices, they do not represent a groundbreaking enhancement to the model's efficiency. In fact, they may slow down training and inference when GPU memory is sufficient.
The observation about requiring numerous random seeds to achieve convergence raises questions about whether the Neural GPU justifies its computational expense, especially when applied to learning algorithms that are already well understood (e.g., parsing and evaluating S-expressions). It might be worth considering whether the computational effort spent training these models (with multiple seeds) could be better utilized in a traditional search through program space, such as sampling Lisp programs.
The discussion of curriculum strategies used to achieve the reported results was interesting and provides insight into the effort required to train this type of model. However, it leaves this reviewer with the impression that, despite the stated extensions to the Neural GPU model, its practical utility for real-world problems remains uncertain.