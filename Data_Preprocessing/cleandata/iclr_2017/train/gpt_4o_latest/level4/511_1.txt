This paper introduces an algorithm designed to approximate the solution of specific time-evolution partial differential equations (PDEs). The authors propose an intriguing learning-based methodology to tackle such PDEs. The approach alternates between the following steps:  
1. Sampling points in the space-time domain.  
2. Computing the PDE solution at the sampled points.  
3. Regressing a space-time function to satisfy the computed solutions at these sampled points, with the hope of generalizing beyond them.  
I find the proposed algorithm both interesting and potentially practical. Traditional grid-based methods for simulating PDEs often become computationally prohibitive due to the curse of dimensionality. Consequently, leveraging learning-based approaches to approximate PDE solutions appears to be a promising direction for real-world applications. However, as the authors correctly highlight, directly applying gradient descent to minimize the regression loss function fails in this context due to the non-differentiability of the "min" operator present in the studied PDEs.  
In this regard, I believe the proposed method represents a compelling approach to learning PDE solutions in scenarios where non-differentiability poses significant challenges for numerical solvers. This is indeed a difficult and important problem in the context of solving PDEs.  
The paper motivates the problem (time-evolution PDEs with a "min" operator acting on spatial derivatives) through applications in control theory. However, I think these types of problems hold even broader relevance for the machine learning and deep learning communities. For instance...