I appreciate the authors' detailed responses to my questions.
This paper introduces a support-regularized variant of sparse coding that incorporates the manifold structure of the data. To achieve this, the authors enhance the traditional sparse coding loss with an additional term that promotes similar active sets for nearby points. The optimization procedure is accompanied by convergence guarantees. Experimental results on clustering and semi-supervised learning tasks demonstrate the advantages of the proposed approach.
The manuscript is well-written and engaging. The primary contribution of this work lies in directly incorporating and optimizing the regularization function itself, rather than relying on approximations or surrogates. The authors develop a PGD-style iterative method and provide a convergence analysis for it.
Thank you for clarifying the assumptions outlined in Section 3. Including some of these clarifications in the manuscript would strengthen its presentation.
Additionally, the authors propose a fast encoding scheme for their method. They have included a new semi-supervised learning experiment that demonstrates an intriguing application of the method and its fast approximation. While this addition is interesting, I believe that the use of fast encoders is neither particularly novel nor central to the paper. The idea of transforming iterative optimization algorithms into feed-forward networks for faster inference has been explored previously in similar contexts. This is a natural extension and not particularly surprising. It might be worthwhile to evaluate the importance of aligning the architecture with the optimization algorithm compared to using a generic network, though some prior work has already addressed this type of analysis.