The paper proposes an extension to the GAN framework by incorporating multiple discriminators. The authors justify this approach from two perspectives:
(1) Employing multiple discriminators is analogous to optimizing the value function with random restarts, which could aid optimization given the nonconvex nature of the value function.
(2) Utilizing multiple discriminators may address optimization challenges that arise when a single discriminator acts as an overly harsh critic. By receiving feedback from multiple discriminators, the generator is less likely to encounter consistently poor gradient signals from all of them.
The central idea of the paper appears straightforward to implement in practice and represents a valuable addition to the GAN training toolkit.
However, I am not fully convinced by the GAM metric (and, by extension, the GMAM metric) used for evaluation. Without evidence that the GAN game is converging, even approximately, it is difficult to argue that the discriminators provide meaningful insights about the generators in relation to the data distribution. Specifically, these metrics do not offer information about mode coverage or the misallocation of probability mass.
The learning curves presented in Figure 3 are more compelling to me, as they offer strong evidence that increasing the number of discriminators has a stabilizing effect on the training dynamics. That said, these results, along with those in Figure 4, seem to suggest that the unmodified generator objective is inherently more stable, even with a single discriminator. This raises the question: is it necessary to use multiple discriminators when training the generator with an unmodified objective?
In summary, the ideas introduced in this paper show significant promise, but I would recommend an extended analysis along the lines of Figures 3 and 4 across additional datasets before considering it ready for publication.
UPDATE: After discussions with the authors, I have revised my rating to a 7.