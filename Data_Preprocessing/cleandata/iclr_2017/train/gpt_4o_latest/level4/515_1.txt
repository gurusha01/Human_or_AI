The manuscript introduces an innovative application of tensor factorization to linear models, enabling the consideration of higher-order interactions among variables in classification and regression tasks while maintaining computational efficiency, as the approach scales linearly with the dimensionality. The proposed factorization leverages the Tensor Train (TT) format, originally introduced by Oseledets (2011). Additionally, the authors incorporate a Riemannian optimization framework to explicitly account for the geometry of the tensor manifold, thereby accelerating convergence.
Overall, the paper is well-written and presents a compelling application of the TT tensor format to linear models, complemented by the use of Riemannian optimization. This combination is particularly intriguing due to its potential applicability across a broad spectrum of machine learning algorithms.
However, I have some reservations regarding the experimental section, which I find to be less robust compared to the theoretical contributions of the paper. Specifically, the number of experiments on real-world datasets is limited, and the role of dropout in these datasets is not thoroughly explored. Furthermore, there is a lack of comprehensive comparisons with other algorithms on real datasets. Another significant omission is the explicit discussion of how the rank is selected for the experiments. Overall, the experimental section appears to consist of preliminary tests that, while addressing various aspects, are not presented in a cohesive or systematic manner.
I would classify the paper as borderline between weak acceptance and weak rejection. It does not meet the criteria for full acceptance primarily due to the inadequacies in the experimental setup. However, with additional experiments that substantiate the effectiveness of the proposed approach, the paper could achieve a much stronger evaluation.
Minor Comments:
- Formula 2: While it is evident that the parameters of the model in (1) can be learned as described in (2), alternative methods could also be employed depending on the chosen approach.
- Before formula 9: Is the claim that the rank is bounded by 2r supported by Lubich et al., 2015?
- After formula 10: Why is the total cost of the N projections stated as O(dr²(r+N))? Shouldn't it be O(Ndr²(r+1))? Each element in the summation has rank 1, and the cost for each is O(dr²(r+TTrank(Z)²)), where TTrank(Z) = 1. Am I misunderstanding this?
- Section 6.2: Could you elaborate on why random initialization hinders convergence? This is an intriguing observation but lacks sufficient explanation. Any hypotheses?
- Section 6.3: You employ dropout—could you discuss its specific advantages in the context of exponential machines? Was dropout applied to real-world datasets?
- How was r₀ chosen in your experiments? Was a validation set used?
- Section 7: Why are variables such as x₁x₂ absent?
- Section 8: There is a typo in the word "experiments."
- Section 8.1: The sentence "We simplicity, we binarized" appears to have a grammatical issue.
- Section 8.3: The statement "we report that dropout helps" is overly general and is only demonstrated on a synthetic dataset.
- Section 8.5: Could you provide additional results for this dataset, such as training and inference times? Alternatively, could you compare the performance with other algorithms?