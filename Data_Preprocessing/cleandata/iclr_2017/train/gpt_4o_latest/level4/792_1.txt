The paper presents a regularization approach based on soft targets, which are generated by blending the true hard labels with the current model's predictions. However, a highly similar method was already proposed in Section 6 of (Hinton et al., 2016, Distilling the Knowledge in a Neural Network).
Strengths:  
+ Thorough analysis of co-label similarity.
Weaknesses:  
- The baselines are weak. It is unclear whether the authors have optimized the hyperparameters in their experiments. For instance, I trained a 5-layer fully connected MNIST model with 512 hidden units, using Adam and He initialization, and achieved 0.986 accuracy without any regularization. In contrast, the paper reports 0.981 accuracy for the same architecture.  
- The paper lacks novelty, as the proposed method closely resembles (Hinton et al., 2016). This level of contribution may not meet the standards for ICLR.