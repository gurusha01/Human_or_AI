Paper Summary 
This paper introduces a method to train a predictive model (i.e., predicting future video frames based on an input image) and leverages the predictions from this model to enhance the performance of a supervised classifier. The approach is demonstrated on a tower stability dataset.
 Review Summary 
The work appears to be in an early stage, particularly in terms of experimentation, and the idea of using forward modeling as a pretraining strategy has already been explored in video and text classification domains. The discussion of related work is inadequate. Additionally, the choice of the end task (predicting motion) may not effectively showcase the benefits of unsupervised training.
 Detailed Review 
This work appears to be in a preliminary phase. It lacks comparisons with other semi-supervised approaches. Methods that treat future frames as latent variables (or privileged information) could also be considered. Furthermore, it is unclear whether the supervised stability prediction model is necessary once the next frame has been predicted. Essentially, the task could be simplified to determining whether motion will occur in the video after the current frame (e.g., by comparing the first frame with the final predicted frame or analyzing the density of gray pixels in the upper region of the video). Training a model to detect motion solely from unsupervised data might perform equally well. I recommend avoiding tasks where the labels can be trivially derived from the unsupervised data, as this effectively transforms unlabeled videos into labeled frames.
The related work section lacks sufficient discussion on prior research that uses unsupervised feature learning from videos (via predictive models, dimensionality reduction, etc.) to aid classification tasks for still images or videos. For instance, Fathi et al. (2008), Mobahi et al. (2009), and Srivastava et al. (2015) have explored such approaches. More recently, Wang and Gupta (2015) achieved strong ImageNet results using features pretrained on unlabeled videos, while Vondrick et al. (2016) demonstrated that generative video models can serve as effective initializations for video classification tasks. In text classification, pretraining classifiers with language models, as shown by Dai and Le (2015), is another example of predictive modeling.
I also suggest reporting test results on the dataset introduced by Lerer et al. (2016). While I understand that your predictive model requires your own videos for pretraining, stability prediction itself only necessitates still images.
Overall, the experimental section feels underdeveloped. It would be more impactful to focus on a task where solving the unsupervised problem does not inherently make the supervised task trivial (or, conversely, where a simple heuristic cannot transform the unlabeled data into labeled data).
 References 
Fathi, Alireza, and Greg Mori. "Action recognition by learning mid-level motion features." Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008.  
Mobahi, Hossein, Ronan Collobert, and Jason Weston. "Deep learning from temporal coherence in video." Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009.  
Srivastava, Nitish, Elman Mansimov, and Ruslan Salakhutdinov. "Unsupervised learning of video representations using LSTMs." CoRR, abs/1502.04681 2 (2015).  
A. Dai, Q.V. Le, "Semi-supervised Sequence Learning," NIPS, 2015.  
X. Wang, A. Gupta, "Unsupervised learning of visual representations using videos," ICCV 2015.  
C. Vondrick, H. Pirsiavash, A. Torralba, "Generating videos with scene dynamics," NIPS 2016.