This paper introduces an on-policy deep reinforcement learning (RL) approach that incorporates additional auxiliary intrinsic variables.
- The proposed method represents a specific instance of a universal value function-based framework, and the authors appropriately reference the relevant prior work. Arguably, the most significant technical contribution of this paper lies in synthesizing and distilling several existing ideas to address 3D navigation tasks. However, I believe the contributions should be articulated more explicitly in the abstract and introduction.
- It would have been helpful to explore the failure modes of this approach. Under what conditions does the model struggle to generalize to dynamic or changing goals? Additionally, there are conceptual concernsâ€”being an on-policy method, the approach is susceptible to catastrophic forgetting if the agent does not periodically revisit goals encountered in the distant past.
- Given that the primary contribution of this work is the integration of multiple key concepts and the demonstration of empirical benefits, I would have appreciated results on other domains, such as Atari, potentially leveraging the ROM as intrinsic variables.
In summary, this paper provides clear empirical evidence supporting the advantages of the proposed formulations, and the experimental insights presented here could prove valuable for the development of future agents.