This paper focuses on leveraging Bayesian neural networks to model learning curves that emerge during the training of machine learning algorithms. The primary application is hyperparameter optimization: by modeling the learning curve, poor-performing runs can be terminated early, thereby saving computational resources. The work extends prior research that utilized parametric learning curves, with the key innovation being that the parameters of these learning curves are treated as the final layer of a Bayesian neural network. This approach appears entirely reasonable and well-motivated.
The paper's main strength lies in addressing a genuine and practical need. From my own experience, there is considerable demand for effective systems that enable early termination in hyperparameter optimization. One question I regret not raising during the pre-review phase is whether the authors intend to release their code. I strongly encourage them to do so, as the code would significantly enhance the paper's contribution, particularly given that the work leans more toward practical implementation than conceptual novelty.
While the experiments presented in the paper are comprehensive, the results feel somewhat underwhelming. My primary interest lies less in how well the learning curves are modeled and more in the practical impact on hyperparameter optimization. I was expecting to see substantial speedups from this method, but the results leave me uncertain about the magnitude of the improvement. Instead of the current "objective function vs. iterations" plots, I would prefer to see the inverse: the number of iterations required to achieve a fixed objective function value. This perspective would better address my core concernâ€”how much time can actually be saved. Additionally, it would be helpful to include some discussion of real-world runtime, as hyperparameter optimization methods can sometimes be so computationally intensive that their practical utility is diminished.
One figure I feel is notably absent is a histogram of termination times across different runs. Such a visualization would provide much-needed intuition, offering insights into the fraction of runs terminated early and how early they are terminated. At present, I lack a clear sense of these dynamics, aside from the fact that at least some runs must be terminated early for the proposed method to outperform others.
In summary, I believe this paper deserves acceptance as it represents a solid contribution to an important and practical problem. While the progress is incremental, it is nonetheless meaningful, and I am willing to accept that.