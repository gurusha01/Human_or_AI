An intriguing connection is established between dropout, Tishby et al.'s "information bottleneck," and VAEs. Specifically, the classification of 'y' from 'x' is decomposed into three components: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z). By optimizing the objective E{(x,y)~data} [ E{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is created, where lambda governs an upper bound on the amount of information flowing through 'z'.
This objective is equivalent to a VAE objective with a downweighted KL divergence (posterior|prior), an encoder that takes 'x' as input, and a decoder that predicts only 'x'.
- The discussion of related work (section 2) is sufficiently thorough.  
- In section 3, it would be helpful to include a reminder of the definition of mutual information.  
- The connection to VAEs in section 5 is compelling.  
- Unfortunately, the results on MNIST and CIFAR-10 are underwhelming. Given that the method appears to be more flexible than other forms of dropout, this outcome is somewhat disappointing.  
- It is unclear why the CIFAR-10 results are significantly worse than the results originally reported for the same architecture.  
- The specific version of 'beta' used in figure 3a is not clearly stated.  
Overall, while the theoretical contributions of the paper are promising, the experimental results are not sufficiently convincing. I encourage the authors to conduct additional experiments that demonstrate meaningful improvements, particularly on CIFAR-10, and potentially on larger-scale problems.