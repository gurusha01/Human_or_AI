This paper investigates the tradeoff between performance, area, and energy model accuracy in the context of designing custom number representations for deep learning inference. Using common image-based benchmarks such as VGG and GoogLeNet, the authors demonstrate that employing fewer than 16 bits in a custom floating-point representation can enhance runtime performance and energy efficiency, with only a marginal reduction in model accuracy.
Questions:
1. Does the proposed custom floating-point number representation account for support of denormalized numbers?  
2. Is the custom floating-point unit (FPU) operated at the same clock frequency as the baseline 32-bit FPU? If not, what are the specific clock frequencies used, and how does this affect the overall system design, particularly in terms of data transfer between memory and the FPUs?
Comments:
1. I suggest incorporating IEEE half-precision floating-point (1-bit sign, 5-bit exponent, and 10-bit mantissa) as a baseline for comparison. It is widely recognized in both the machine learning (ML) and hardware (HW) communities that 32-bit floating-point representations are excessive for deep neural network (DNN) inference. Moreover, major hardware vendors already provide support for IEEE half-precision floats.  
2. The claim that adopting custom floating-point representations results in a YY.ZZÃ— reduction in energy consumption is, in my view, misleading. While it is plausible that the energy consumption of the FPU itself decreases due to the smaller operand bit-width, a significant portion of the total energy is consumed in data movement between memory and compute units. Consequently, reducing the FPU's energy consumption by a certain factor does not directly translate to an equivalent reduction in overall energy consumption. Readers unfamiliar with such nuances, such as those primarily from the ML community, could be misled by this claim.  
3. Similarly, the authors should clarify that the reported speedup pertains solely to the FPU and does not necessarily reflect the speedup of the entire workload. Although the computational speedup of the FPU scales approximately quadratically with bit-width, memory bandwidth requirements scale linearly. This means that custom FPUs could become bottlenecked by memory bandwidth, necessitating a reevaluation of the speedup and energy savings claims under such scenarios.  
4. The authors should also address the complexities and overheads associated with data access and system design when using non-byte-aligned number representations. For instance, adopting a custom 14-bit representation may enhance the FPU's performance and energy efficiency, but these gains could be offset by the additional overhead required to support non-byte-aligned memory accesses. This includes challenges in designing system buses and data paths to accommodate such representations.