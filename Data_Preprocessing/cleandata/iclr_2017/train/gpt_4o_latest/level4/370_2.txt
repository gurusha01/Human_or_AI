Review - Summary:  
The paper introduces a novel model training strategy aimed at achieving higher accuracy. The challenge addressed is the trade-off between overfitting and underfitting: training an excessively large model risks overfitting and capturing noise, while pruning or downsizing a model too much may result in underfitting and the loss of critical connections. To tackle this, the proposed method involves a sequence of training steps: initially training a dense network, pruning it to create a sparse network, training the sparse network, and finally reintroducing connections to retrain the model as dense (DSD). This DSD approach is a generalizable method applicable to architectures such as CNNs, RNNs, and LSTMs. The improved accuracy achieved through DSD can be attributed to several factors: escaping saddle points, increased robustness to noise due to sparsity, and symmetry breaking, which enables richer representations.
Pros:  
The central contribution of this paper is its demonstration that models possess the capacity to achieve higher accuracy, as evidenced by the ability to compress models without sacrificing performance. This "lossless compression" highlights the significant redundancy present in models trained with conventional methods. The findings underscore an important insight: larger models can achieve better accuracy when more effective training schemes, such as the proposed DSD method, are employed.
Cons & Questions:  
One limitation is that the accuracy improvement achieved is relatively modest (approximately 2â€“3%) for most models. This raises the question of the trade-off: what is the cost of achieving this improvement? Concerns about resource and performance efficiency emerge, as training large models is computationally intensive, often requiring hours or even days on high-performance GPUs.  
Additionally, a key question remains: can the Dense-Sparse-Dense training cycle be repeated indefinitely (e.g., Dense-Sparse-Dense-Sparse-Dense...) to achieve progressively higher accuracy? If not, what are the limitations or diminishing returns of this iterative DSD approach?