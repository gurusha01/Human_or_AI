Because the authors did not address reviewer feedback, I am maintaining my original review score.
-----
This paper introduces a method to model relational (i.e., correlated) time series using a latent variable framework inspired by deep learning. The authors develop a flexible parametric (but non-generative) model with Gaussian latent factors, trained using a composite objective function. This objective includes terms for reconstruction error (to capture observed time series), smoothness in the latent state space (via a KL divergence term that encourages neighboring states to have similar distributions), and a regularizer that enforces similarity in latent state trajectories for related time series. Relationships between trajectories are predefined based on domain knowledge, such as the expectation that latent state trajectories for neighboring (wind speed) base stations should be similar. The model is trained via gradient descent. Several extensions are proposed, including a nonlinear transition function (using an MLP) and a reconstruction error term that accounts for variance. However, the model is constrained to a linear decoder. While the experimental results are positive, they are not particularly compelling.
Strengths:
- The paper tackles an important and challenging problem: combining uncertainty modeling over hidden states with the flexibility of neural network-inspired methods.
- The use of KL divergence to represent relationships between hidden states is innovative. Coupled with the Gaussian latent state distribution, this results in a regularization term that is both simple and differentiable.
- The general approach—framing the problem as a neural network-like loss function—is robust, flexible, and potentially extensible to other methods, such as variants of variational autoencoders.
Weaknesses:
- The presentation is unclear, particularly in Sec. 3.3, where the model is defined. Four model variants are introduced, differing in their choice of decoder (with or without a variance term) and transition function (linear vs. MLP). While the 2,2 variant appears to perform better overall, the improvements are inconsistent and often marginal. This makes it difficult to draw definitive conclusions about the contributions of each loss component, the utility of the nonlinear transition function, and practical guidelines for applying the model. Two suggestions for improvement: (1) focus on the 2,2 variant in Sec. 3.3 (hypothesizing it as the best-performing model) and treat the simpler variants as baselines briefly described in Sec. 4.1; (2) conduct more extensive experiments on larger datasets to provide stronger evidence for the approach's advantages.
- The description of learning is vague, with only brief references to gradient descent and ADAM. Inference is mentioned in a single subsection but is limited to one incomplete sentence.
- The purpose of the inequality in Eq. 9 is unclear.
- The experimental results are unconvincing. Given the dataset size, the differences compared to the RNN and KF baselines may not be statistically significant. Moreover, these baselines are relatively weak (especially if the RNN is not an LSTM or GRU).
- The paper's positioning relative to variational autoencoders and related models is unclear. Recurrent VAEs (e.g., Krishnan et al., 2015) seem to address similar goals in uncertainty modeling and could likely be extended to incorporate relationships between time series using the regularization strategy proposed here. Similarly, Johnson et al., 2016 (referenced in a separate question) appears relevant.
This paper explores a promising research direction, presenting some intriguing ideas and preliminary results. However, I recommend that the authors restructure the manuscript to improve clarity, particularly in the model description, and provide more detailed discussions of learning and inference. Additionally, more comprehensive experiments are needed to demonstrate the approach's strengths and limitations more convincingly.