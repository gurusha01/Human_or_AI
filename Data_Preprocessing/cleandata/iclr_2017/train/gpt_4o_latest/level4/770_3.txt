This paper tackles the practical challenge of generating rare or unseen words within the framework of language modeling. Given that language adheres to Zipf's law, most existing approaches restrict the vocabulary size for computational efficiency, often mapping rare words to a UNK token. Rare words, however, are particularly significant in applications such as question answering and machine translation. The authors propose a novel language modeling technique that integrates knowledge base (KB) facts, enabling the generation of (potentially unseen) words derived from KBs. Additionally, the paper introduces a dataset that aligns words with Freebase facts and their corresponding Wikipedia descriptions.
The proposed model operates by first selecting a KB fact based on the sequence of previously generated words and facts. Using the selected fact, the model then decides whether to generate a word from the vocabulary or to produce a symbolic word from the KB. For the latter case, the model is trained to predict the position of the word within the fact's description.
While the paper presents a compelling approach, the writing—particularly the notations in Section 3—could benefit from refinement. The experiments are well-executed and yield strong results, with the heatmaps at the end providing particularly valuable insights.
Comments
The contributions of this paper would be more impactful if the proposed method demonstrated improvements in a practical application, such as question answering, even though the paper mentions that the technique could be applied to enhance QA.  
In Section 3, the authors refer to the entity as a "topic," which is somewhat confusing. A "topic" can imply something abstract, but in this context, it always refers to a Freebase entity. Clarifying this terminology would improve readability.  
A key question arises regarding the necessity of predicting a fact at every step before generating a word. On average, how many distinct facts does the model select to generate a sentence? Intuitively, a natural language sentence would describe only a few facts about an entity. If the fact generation step could be bypassed—perhaps by introducing a latent variable to decide whether a fact should be generated—the model could potentially achieve faster performance.  
In Equation 2, the model requires a hard decision to select a fact. For end-to-end training, this necessitates annotating every word with a corresponding fact, which may not always be feasible, especially in domains like social media text.  
The approach of learning position embeddings for copying knowledge words seems somewhat counter-intuitive. Does the sequence of knowledge words adhere to a specific structure, such as the second word always being a last name (e.g., "Obama")? Clarifying this would strengthen the argument.  
Finally, it would be valuable to compare the proposed method with character-level language models, which inherently address the unknown token problem.