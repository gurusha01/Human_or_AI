This paper introduces a method for learning sequence decompositions (e.g., words) for speech recognition. The work tackles a significant problem, and I anticipate its potential utility in other areas, such as machine translation. The proposed approach is both innovative and well-justified; however, I strongly recommend including a comparison with byte pair encoding (BPE). BPE serves as a natural and critical baseline (i.e., dynamic versus fixed decomposition). The evaluation should report BPE performance across a range of vocabulary sizes.
Minor comments:
- Do the learned decompositions align with phonetically meaningful units? Based on the example provided in the appendix, it is unclear whether the model is identifying phonemes or merely capturing the most frequent character n-grams.
- Have you considered potential applications beyond speech recognition? Demonstrating effectiveness in other domains would significantly enhance the contribution, though this may be outside the current scope.