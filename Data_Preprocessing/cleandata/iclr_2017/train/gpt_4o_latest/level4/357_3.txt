Review
1) Summary  
This paper explores the effectiveness of separating appearance and motion information for the task of future frame prediction in natural videos. The proposed approach introduces a novel two-stream encoder-decoder architecture, MCNet, which comprises two distinct encoders: one is a convolutional network operating on individual frames, and the other is a convolutional network combined with an LSTM that processes sequences of temporal differences. These encoders are followed by combination layers (stacking and convolutions) and a deconvolutional decoder that also incorporates residual connections from the encoders. The entire architecture is trained end-to-end using the objective and adversarial training strategy proposed by Mathieu et al.
2) Contributions  
+ The proposed architecture is innovative and well justified. It draws some inspiration from the two-stream networks of Simonyan & Zisserman, which have proven highly effective for real-world action recognition.  
+ The qualitative results are extensive, insightful, and highly convincing (including quantitative evaluations) on the KTH and Weizmann datasets. These results highlight the advantages of decoupling content and motion for simple scenes with periodic motions, as well as the importance of residual connections.
3) Suggestions for Improvement  
Static dataset bias:  
In response to pre-review concerns regarding the static nature of the qualitative results, the authors introduced a simple baseline that copies the pixels from the last observed frame. While the updated experiments on KTH reaffirm the strong performance of the proposed method under these conditions, the observation that this baseline outperforms all other methods (including the authors') on UCF101 raises questions about the relevance of reporting average statistics on UCF101. Although the authors provide some qualitative analysis related to the amount of motion, additional quantitative analysis is necessary to validate the performance of this and other methods for future frame prediction. At a minimum, the results on UCF101 should be clarified by categorizing scenes based on motion intensity, for example, by calculating the overall motion magnitude (e.g., l2 norm of temporal differences) and reporting PSNR and SSIM for different quartiles or deciles. Ideally, the paper should also include experiments on other realistic datasets beyond UCF101. For instance, the Hollywood 2 dataset by Marszalek et al could be a strong candidate, as it features movies with complex actor, camera, and background motions, where the "pixel-copying" baseline would likely perform poorly. Additionally, experiments on non-action video datasets, such as the KITTI tracking benchmark, would significantly enhance the paper.
Additional recognition experiments:  
As raised in the pre-review questions, further experiments on action recognition tasks using UCF101, particularly by fine-tuning the model, would greatly strengthen the paper. Video classification inherently requires learning both appearance and motion features, and the two-stream encoder combined with the integration layers in the MCNet+Res architecture appears particularly well-suited for this purpose. If the architecture indeed enables unsupervised pre-training of content and motion representations, as hypothesized by the authors, such experiments would provide strong evidence. These additional experiments would also help address concerns about the static nature of the learned representations.
4) Conclusion  
In summary, this paper presents a promising architecture for an important problem but requires additional experiments to fully support the authors' claims. If the suggested experiments are conducted and yield convincing results, this paper would be highly relevant for ICLR.
5) Post-Rebuttal Final Decision  
The authors have conducted substantial additional work in response to the reviewers' suggestions, providing new and compelling experimental evidence. This makes the paper one of the most experimentally comprehensive studies on this problem. I am therefore increasing my rating and recommending acceptance of this paper. Excellent work!