Review - Paraphrased Version:
Summary:  
This paper introduces a method to transform a dense RNN into a sparse network. The proposed algorithm incrementally sets more weights to zero during the RNN training process, resulting in a model with reduced storage requirements and faster inference.
Pros:  
The paper proposes a pruning approach that eliminates the need for re-training and does not interfere with the RNN training process. The method achieves 90% sparsity, significantly reducing the number of parameters.
Cons & Questions:  
Wouldn't selecting appropriate hyperparameters for various models and applications be challenging? In Equation 1, does q represent the final model's sparsity? Is there a formula to predict the sparsity, number of parameters, and accuracy of the final model based on a given set of hyperparameters, without undergoing the training process? (Questions addressed)
In Table 3, there is a clear trade-off between the number of units and sparsity to optimize the number of parameters or accuracy, and in Table 5, to improve speed. This is promising, but where are the results for GRU Sparse Big? For instance, the accuracy should remain comparable while achieving reasonable compression and speed-up, similar to how RNN Sparse Medium compares with RNN Dense. The advantage of pruning and achieving high speed-up seems limited if it comes at the cost of significant accuracy loss. (Issue resolved with updated data)
Why is the sparsity in Table 3 different from that in Table 5? The text mentions an "average sparsity of 88%," but Table 5 reports 95%. Are the models in Table 3 distinct from those in Table 5? (Issue resolved)
In the introduction, the paper states: "... unlike previous approaches such as in Han et al. (2015). State-of-the-art results in speech recognition generally require between days and weeks of training time, so a further 3-4× increase in training time is undesirable."
However, Han et al. (2015) claims that "Huffman coding doesn't require training and is implemented offline after all the fine-tuning is finished." Both your approach and Han et al. (2015) utilize weight pruning techniques. Intuitively, the training times for LSTM models should be comparable. What accounts for the 3-4× increase in training time reported in Han et al. (2015) but not in your method?