The authors propose an online learning approach for determining the structure of sum-product networks (SPNs). Their algorithm assumes Gaussian coordinate-wise marginal distributions and simultaneously learns both the parameters and structure in an online fashion. Parameter updates are performed using a recursive procedure that reweights the network nodes contributing most to the likelihood of the current data point. For structure learning, the method either merges independent product Gaussian nodes into multivariate leaf nodes or creates a mixture over the two nodes when the resulting multivariate node would become excessively large.
The scalability of the dataset to larger sizes (in terms of the number of datapoints) is encouraging, though the number of variables remains relatively small. Current benchmarks for tractable continuous density modeling using neural networks, such as the NICE and Real-NVP families, demonstrate scalability to both large datasets and high-dimensional variables. Similarly, intractable methods like GANs, GenMMNs, and VAEs exhibit this property as well.
A key concern for the ICLR audience is the reliance on SPN-specific datasets, which are not widely used in the deep learning generative modeling community. Additionally, the use of GenMMN as a baseline is suboptimal for bridging the gap to neural network-based approaches, as its Parzen-window likelihood evaluation is not particularly meaningful. More robust evaluation methods, such as annealed importance sampling (as discussed in "On the Quantitative Analysis of Decoder-Based Generative Models" by Wu et al.), would be preferable. Incorporating a simple VAE model to establish a lower bound on the likelihood, or employing models like Real-NVP, would provide a more meaningful comparison.
While most neural network-based density models scale effectively with both the number of observations and variables, it is unclear whether this method achieves similar scalability "horizontally." Evaluating its performance on datasets like MNIST would provide valuable insights into its feasibility for larger-scale applications.
SPNs are uniquely capable of handling not only marginal queries but also a variety of conditional queries in a tractable manner. However, this strength is neither evaluated nor compared in the paper. A potential application could involve imputing missing pixels or color channels in images, where high-performing tractable models are currently lacking.
Despite its limited connection to the broader ICLR generative modeling literature, the algorithm presented here is straightforward, intuitive, and demonstrates clear improvements over prior state-of-the-art methods for online SPN structure learning. Using a VAE as a baseline for continuous data would strengthen comparisons to neural network approaches. Furthermore, integrating SPNs with deep latent variable models, such as using SPNs as observation models or posteriors, could result in a powerful hybrid approach.
While I appreciate the potential of SPNs and would like to see them gain more recognition within the ICLR probabilistic modeling community, I am uncertain whether this paper does enough to establish their relevance. Like the other reviewers, I am not an SPN expert, but the proposed algorithm appears to be a simple and effective method for online structure induction. Its focus on scalability aligns with recent trends in representation learning. Overall, I believe the paper merits publication, though I would strongly encourage the authors to address the aforementioned points to better bridge the gap with the broader deep generative modeling literature.