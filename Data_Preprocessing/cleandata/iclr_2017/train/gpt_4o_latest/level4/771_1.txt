This paper introduces a method for estimating the context sensitivity of paraphrases and leverages this to guide a word embedding learning model. The central idea and the proposed model are presented in a compelling manner and appear plausible. However, the primary weaknesses lie in the experimental evaluation and the exploration of the model itself. The evaluation does not convincingly demonstrate whether the proposed model offers a significant improvement over simpler approaches (especially those that do not rely on the paraphrase database). Similarly, the model section does not make a strong case that this formulation is the most natural or optimal one to consider. The paper would be improved by providing more thorough explanations for the model design choices or, ideally, by exploring alternative formulations.
Overall, I am inclined to recommend rejecting the paper while encouraging the authors to submit a revised and improved version in the near future.
Detailed/minor comments are as follows:
1) Although the paper is mostly grammatically correct, it would benefit from revision by a native English speaker. As it stands, some sections are difficult to follow due to unconventional sentence structures.
2) The tables require clearer and more descriptive labels.
3) The results are somewhat inconclusive. For instance, in the analogy task presented in Table 4, it is surprising that CBOW outperforms the proposed embeddings on the semantic aspect of the task, despite the latter being specifically designed to excel in this area.
4) Why was "Enriched CBOW" excluded from the analogy task?
5) In the related work section, several papers are cited that combine lexica and corpora to learn embeddings. However, the paper repeatedly claims to be the first of its kind or suggests that insufficient work has been done in this area. This comes across as somewhat misleading.