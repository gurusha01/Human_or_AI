This paper presents an implementation of the delayed synchronized SGD method for multi-GPU deep neural network training.
Comments:
1) The manual implementation of delayed synchronization and state protection described in the paper is useful. However, such functionality could have been achieved using a dependency scheduler instead of relying on manual threading.
2) The overlap of computation and communication is a well-known technique that has already been implemented in existing frameworks like TensorFlow (as discussed in Chen et al.) and MXNet. Therefore, the contribution claimed in this aspect appears to be somewhat limited.
3) The reported convergence accuracy is only provided for the initial iterations and exclusively for AlexNet. Including the full convergence curve across all compared networks would provide a more comprehensive evaluation.
Summary:  
This paper implements a variant of the delayed SyncSGD approach. However, I find the novelty of the system to be somewhat limited (as noted in comment (2)). Additionally, the experimental evaluation could be improved to better demonstrate the advantages of the proposed approach.