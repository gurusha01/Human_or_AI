The paper introduces a novel neural architecture named DRAGNN, designed for the transition-based framework. DRAGNN leverages TBRUs, which are neural units that compute hidden activations for the current state of a transition-based system. The authors demonstrate that DRAGNNs can encompass a broad spectrum of transition-based methods found in the literature. Furthermore, DRAGNNs facilitate the straightforward implementation of multitask learning systems. Experimental results indicate that DRAGNNs enabled the authors to develop (near) state-of-the-art systems for two tasks: parsing and summarization.
The paper is structured into two main components: the introduction of DRAGNN and its practical applications.
For the first component, DRAGNN is presented as an elegant tool for constructing transition-based systems. However, its novelty is somewhat unclear. The transition-based framework is already well-established, and there is a significant trend in NLP toward using neural networks for implementing such systems. In my view, the distinction between DRAGNN and the Stack-LSTM (Dyer et al., 2015) is minimal. While DRAGNN is undoubtedly a robust architecture, its primary contribution appears to lie in the domain of software engineering rather than theoretical innovation.
In the second component, the authors showcase the use of DRAGNN to develop new transition-based systems for various (multi-)tasks. These implementations are well-executed and highlight DRAGNN's strength, particularly in multitask learning. However, it is important to note that the employed solutions are largely drawn from existing literature, which makes it challenging to assess the novelty of this section in the context of the conference's focus.