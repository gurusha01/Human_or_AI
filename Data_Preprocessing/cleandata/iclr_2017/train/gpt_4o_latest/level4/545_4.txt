This paper introduces a novel learning framework termed "compositional kernel machines" (CKM), which integrates two key concepts: kernel methods and sum-product networks (SPNs). CKM begins by defining leaf kernels on the elements of the query and training examples, and subsequently constructs kernels recursively in a manner analogous to SPNs. The authors demonstrate that CKM evaluation can be performed efficiently by leveraging techniques used in SPNs.
Positive:  
The concept presented in this paper is intriguing. Instance-based learning approaches, such as SVMs with kernels, have historically been effective but have largely been supplanted in recent years by deep learning techniques like convolutional neural networks (convnets). This work explores an underexplored intersection of kernel methods and deep network principles (specifically SPNs), which is a promising direction.
Negative:  
While the idea is compelling, the work appears to be in a very preliminary stage. In its current state, the proposed framework does not seem to offer clear advantages over convnets. Below, I outline my concerns in detail:
1. One of the key claims of the paper is that CKM is faster to train than convnets. However, the reasoning behind this claim is unclear. Both CKM and convnets rely on gradient descent during training, so it is not evident why CKM would be faster.  
   Furthermore, during inference, the runtime of a convnet depends solely on its network structure, whereas CKM's runtime is influenced by both the network structure and the size of the training set. This raises concerns about CKM's scalability for large training datasets. The reliance on specialized data structures and optimization tricks, even for a relatively simple dataset like NORB, seems to underscore this limitation.
2. The role of the leaf kernel is not well-explained, making it difficult to understand what it captures. For instance, if the "elements" correspond to raw pixel intensities, the leaf kernel essentially compares pixel intensities between the query and training images. However, this approach seems prone to comparing irrelevant background pixels, which may not contribute meaningfully to recognition.  
   Section 3.1, which discusses this concept, is particularly dense and challenging to follow. A clearer explanation of this section would be beneficial.
3. The design of the sum-product function's architecture is also unclear. The example provided in Section 3.1 appears somewhat arbitrary, and it is not evident how one would systematically construct such architectures for other tasks or datasets.
4. The experimental evaluation is the weakest aspect of the paper. The NORB dataset, while historically significant, is now considered small and simplistic by modern standards. Even on this dataset, the proposed method only slightly outperforms SVMs. Additionally, it is unclear whether the "SVM" results in Table 2 refer to linear SVMs or kernel SVMs. If the latter, it is likely that kernel SVMs would yield even better performance.  
   Moreover, CKM performs significantly worse than convnets on NORB and only shows improvements over convnets on synthetic datasets (e.g., NORB compositions and NORB symmetries). This limited experimental evidence does not convincingly demonstrate the advantages of the proposed framework.
Overall:  
While the paper presents some interesting ideas, it feels too preliminary in its current form. Substantial additional work is needed to better articulate the framework's advantages and address the concerns raised above. That said, it is worth noting that many foundational ideas in machine learning initially appeared immature when first introduced, only to mature and gain significance over time. This paper may fall into that category, and its ideas could potentially inspire future developments.