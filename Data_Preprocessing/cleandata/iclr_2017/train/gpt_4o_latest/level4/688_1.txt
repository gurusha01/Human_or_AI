This paper introduces iterative PoWER, an off-policy adaptation of PoWER, a policy gradient algorithm within the reward-weighted framework.
I do not have sufficient familiarity with this type of lower bound scheme to provide detailed commentary on it. However, the approach appears to result in less conservative step sizes in the policy parameter space. Since all expectation-based algorithms (and their KL-regularized counterparts, such as TRPO) tend to take relatively small steps, this could be a reasonable method to accelerate their convergence.
The experimental details provided in Section VI are inadequate for reproducibility. For instance, is the statement "The cart moved right" meant to indicate "a positive force is applied to the cart"? How is a negative force applied in this context? What is the representation of the state? What is the distribution of initial states? Additionally, a linear policy seems insufficient for achieving both swing-up and balance of a cart-pole system. Are the experiments limited to balancing only? What is the magnitude of the policy noise, and how was it determined? How long were the episodes?
The footnote at the bottom of page 8 was confusing. If Newton's method is being used, where is the discussion of gradients and Hessians? I initially interpreted the argmax_theta operator as representing an EM-style step, which aligns with how I understood Eq. (8) in the Kober paper.