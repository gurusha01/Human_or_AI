This paper introduces a method for learning object representations by composing a set of templates derived from binary images. Specifically, it proposes a hierarchical model that combines AND, OR, and POOL operations. The learning process involves approximated inference using MAX-product belief propagation (BP), followed by a heuristic to threshold activations into binary values.
The topic of learning interpretable hierarchical representations is highly compelling, and this work provides valuable insights, particularly in the context of modern convolutional neural networks. However, I have several concerns regarding the paper:
1) The paper does not adequately cite or discuss relevant prior work and incorrectly claims to be the first to learn interpretable parts. A comparison to the following related works should be included:
   - Compositional hierarchies by Sanja Fidler
   - AND-OR graphs by Leo Zhu and Alan Yuille for object modeling
   - AND-OR templates developed by Song-Chun Zhu's group at UCLA  
   The claim of being the first to discover such parts should be removed.
2) The experimental evaluation is limited to simplistic toy datasets. The aforementioned works have been applied to real-world images (e.g., by binarizing images using contours). I would like to see how the proposed method performs on more established benchmarks and its effectiveness for classification tasks. Additionally, comparisons with other generative models, such as VAEs and GANs, would strengthen the evaluation.
3) A discussion of the relationship, differences, and advantages of the proposed approach compared to sum-product networks and grammars would be valuable.
Additional comments:
- The paper claims that inference becomes feed-forward after learning, but since message passing is employed, the resulting network should be considered recurrent.
- The technical details and algorithmic discussions currently relegated to the appendix should be moved to the main body of the paper.
- The introduction asserts that compression is evidence of understanding. I disagree with this claim, and it should be removed.
- A discussion linking the proposed approach to the Deep Rendering model would be beneficial.
- It is unclear how some constraints are enforced during message passing, especially given that constraints are known to be challenging to optimize with MAX-product inference. How is this issue addressed?
- The learning and inference algorithms appear to rely heavily on heuristics (e.g., clipping values to 1, selecting specific messages to propagate). An analysis of these design choices would be helpful.
- Performing multiple iterations of steps 5) and 2) does not constitute a single backward pass.
I will reconsider my evaluation based on the authors' responses to these concerns.