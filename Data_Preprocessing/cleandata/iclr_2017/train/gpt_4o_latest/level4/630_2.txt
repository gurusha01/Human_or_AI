This paper presents two incremental contributions to enhance the current state-of-the-art in summarization using seq2seq models with attention and copy/pointer mechanisms.
1. The authors propose a 2-pass reading approach, where the representations generated during the 1st pass are utilized to re-weight the contribution of each word to the sequential representation in the 2nd pass. They detail how this "read-again" process is implemented for both GRU and LSTM architectures.
2. On the decoder side, the authors employ a softmax mechanism to alternate between generating words from the decoder vocabulary and copying words from the source input. A novel aspect of their approach involves representing the previously decoded word \( Y_{t-1} \) in a different manner. This modification enables the use of a smaller decoder vocabulary, resulting in faster inference times without compromising summarization performance.
The paper reports achieving new state-of-the-art results on the DUC2004 dataset; however, the comparison on the Gigaword dataset appears incomplete, as it omits more recent results published after Rush et al. (2015). While the work is overall solid, there are several scientific gaps that warrant further exploration. For instance:
- What is the computational overhead introduced by the 2nd-pass reading in the end-to-end system?
- How does the decoder's reduced vocabulary mechanism perform in terms of both summarization quality and runtime efficiency when the 2nd-pass reading is not employed on the encoder side?
- There are alternative methods for enhancing sentence embeddings. How does the proposed 2nd-pass reading compare to recent advancements in self-attention and LSTMN models? For example, Cheng et al. (2016), "Long Short-Term Memory-Networks for Machine Reading," and Parikh et al. (2016), "A Decomposable Attention Model for Natural Language Inference."