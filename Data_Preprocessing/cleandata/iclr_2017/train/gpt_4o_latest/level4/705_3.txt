CONTRIBUTIONS  
This paper proposes a method for jointly learning semantic "word-like" units from audio and visual data. The approach employs a multimodal neural network architecture that takes both image and audio inputs (represented as spectrograms). Through joint training, the model maps images and spoken language captions into a shared representation space. Audio-visual groundings are established by computing the affinity between image patches and audio clips, enabling the model to associate specific visual regions with corresponding audio segments. The experiments focus on tasks such as image search (audio-to-image), annotation (image-to-audio), and acoustic word discovery.
NOVELTY + SIGNIFICANCE  
As highlighted in Section 1.2, the fields of computer vision and natural language processing have extensively explored multimodal learning, particularly for applications like image captioning and retrieval. This paper contributes incremental progress in multimodal learning by leveraging a novel combination of input modalities (audio and images).  
However, the bidirectional image/audio retrieval task has already been addressed by the authors in earlier work (Harwath et al., NIPS 2016). Aside from minor variations in the dataset and CNN architecture, the training methodology in this paper remains largely unchanged from the prior work. The primary novelty of this submission lies in the method used to associate image regions with audio subsequences using the trained model.  
The proposed approach for this association relies on relatively straightforward combinations of standard techniques, offering limited novelty. Specifically, alignment scores are computed between densely sampled image regions and audio subsequences, and a series of heuristics are then applied to cluster image regions and audio subsequences into meaningful associations.
MISSING CITATION  
The paper overlooks a significant body of related work spanning computer vision, natural language processing, and speech recognition. A particularly relevant missing reference is:  
Ngiam et al., "Multimodal deep learning," ICML 2011.
POSITIVE POINTS  
- By utilizing a larger dataset and an improved CNN architecture, the paper demonstrates improved performance over prior work in bidirectional image/audio retrieval.  
- The method effectively facilitates acoustic pattern discovery.  
- The combination of audio-visual grounding with image and acoustic cluster analysis successfully identifies meaningful audio-visual cluster pairs.
NEGATIVE POINTS  
- The paper offers limited novelty, particularly when compared to Harwath et al., NIPS 2016.  
- While the clustering approach yields good results, it is heuristic in nature and lacks significant innovation.  
- The proposed method involves numerous hyperparameters (e.g., patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc.), but the paper does not provide a discussion on how these were selected or the sensitivity of the method to these choices.