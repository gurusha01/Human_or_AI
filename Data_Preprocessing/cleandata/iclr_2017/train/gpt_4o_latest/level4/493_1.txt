The paper examines the misclassification error of discriminators and emphasizes that while assuming a uniform probability prior over the classes is reasonable at the start of optimization, the class distribution diverges significantly from this prior as the parameters evolve away from their initial values. As a result, the optimized upper bound (log-loss) becomes less tight.
To address this issue, the authors propose an optimization procedure that involves recomputing the bound. The paper is well-written, and although the primary observation made is a known fact, it is articulated in a clear and engaging manner that could make it valuable to a broad audience at this conference.
I would like to point out the strong connections between this framework and curriculum learning. For further insights, the authors may refer to [1], which is a relevant citation that should be included. Adding a discussion on this topic could enhance the paper's quality.
Additionally, there exists a substantial body of work on directly optimizing task-specific losses [2][3] and the references therein. These should be discussed and connected, particularly in relation to Section 3, which focuses on optimizing the ROC curve.
[1] Training Highly Multiclass Classifiers, Gupta et al. 2014.  
[2] Direct Loss Minimization for Structured Prediction, McAllester et al.  
[3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet.
Final comment:  
I believe the material presented in this paper will appeal to a broad audience at ICLR. The problem addressed is compelling, and the proposed approach is robust. I recommend accepting the paper and increasing my score (from 7 to 8).