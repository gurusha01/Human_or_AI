The approximation properties of neural networks have been previously explored for various classes of functions. This paper aims to establish an analog of the approximation theorem for the class of noise-stable functions. Noise-stable functions, whose outputs are not significantly influenced by individual input dimensions, represent an intriguing class, and I find the problem formulation compelling. The paper is well-structured, and the proofs and arguments are presented in a clear and accessible manner.
I have two primary concerns:
1. Presentation: My interpretation of the arguments is that noise-stability reflects the "true" dimensionality of the data, as it captures the function's dependence on different dimensions. Consequently, it should be possible to reformulate and prove an approximation theorem analog based on the "true" dimensionality of the data. However, it remains unclear under what circumstances the bounds derived from stability are tighter than those based on dimensionality, given that both grow exponentially. While these discussions are intriguing, the authors present the results as bounds that are independent of dimensionality, with a constant that grows exponentially with \(1/\epsilon\). This framing is somewhat misleading, as the stability parameter \(\epsilon\) could itself depend on the dimensionality. In most cases, I believe \(1/\epsilon\) increases with the dimension.
2. Contribution: Although the connection between noise-stability and approximation is novel and interesting, the paper's contribution feels limited. The results appear to be straightforward applications of prior work, and many of the lemmas are restatements of existing results. To elevate the significance of this work, I believe additional discussions and more substantial results are necessary to make it a more complete and impactful contribution.