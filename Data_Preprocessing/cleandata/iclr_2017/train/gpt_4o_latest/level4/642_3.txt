The paper presents an initial investigation into customized precision hardware tailored for large convolutional networks, specifically AlexNet, VGG, and GoogLeNet. It demonstrates that significant speed-ups (up to 7x) can be achieved with reduced floating-point precision, outperforming fixed-point representations in this context.
Additionally, the paper proposes a method to predict custom floating-point precision parameters directly from neural network activations, thereby bypassing exhaustive search. However, I found this aspect difficult to follow. The authors evaluate only the activations of the last layer, but it is unclear on what data this evaluation is performed. Is it conducted on the entire validation set? Furthermore, why would this approach be faster than directly computing classification accuracy?
The findings could be valuable for hardware manufacturers, though with a caveat. Most modern convolutional networks incorporate batch normalization, whereas none of the networks evaluated in this study do. This raises the possibility that the conclusions drawn here might not hold for networks with batch normalization, where fixed-point representations could potentially perform better. This is an avenue that warrants further exploration.
In general, the paper offers limited novelty, serving primarily as a practical study on numerical precision trade-offs during neural network inference. However, training time optimization is another critical area of interest, as there are far more researchers focused on accelerating the training of new networks than on speeding up the evaluation of existing ones.
While I am not an expert in digital logic design, my informed assessment is that this paper falls slightly below the acceptance threshold.