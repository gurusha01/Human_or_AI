The paper introduces an online variant of segment-to-segment transducers, enabling predictions on target sequences without requiring observation of the entire input sentence. The authors extend their prior work, incorporating the ability to leverage independent priors on target hypotheses, such as language grammar or sentence length.
Strong points:
- The paper is well-written and presents an intriguing idea of integrating multiple sources of information within a Bayesian framework for sequence-to-sequence models. Addressing problems in an online setting often adds complexity, and the authors' approach to tackling this challenge is of significant interest to the research community.
- The experimental section is robust and demonstrates strong results, although it is not entirely comprehensive (see weak points).
Weak points:
- The proposed method does not improve computational complexity compared to Tillmann's approach, which may limit its applicability in scenarios involving long input sequences. This limitation persists despite the use of a constrained model for alignment latent variables.
- The paper does not include a baseline that combines direct, language model (LM), and bias contributions without the channel component. Was there a specific (non-obvious) algorithmic constraint that prevented the inclusion of this baseline?
Additional (minor) comments:
- Regarding the first weak point: Could the authors elaborate on how their approach conceptually differs from Tillmann et al. (1997), beyond the use of connectionist discriminative models to derive specific conditional probabilities?
- How sensitive is the model to the choice of hyperparameters in Eq. (3)? Do the authors perform a naive search over the hyperparameter space, or is a more sophisticated approach employed?
- Additional details on the auxiliary direct model would be valuable for better understanding.
- How critical is the correct selection of pruning variables (K1 and K2) for the model's performance?
- Section 2 states that no Markovian assumptions are made. Does this imply the absence of a first-order Markovian assumption specifically?
Typos:
- Table 1: "chanel" â†’ "channel" (one row before the last).
Apologies for the delayed review.