This paper proposes an extension of boosting to the domain of learning generative models. The strong learner is formulated as a geometric average of "weak learners," which can either be normalized generative models (e.g., VAEs) or un-normalized ones (e.g., RBMs) under the genBGM framework, or a classifier trained to distinguish between the strong learner at iteration T-1 and the true data distribution under the discBGM framework. The latter approach shares conceptual similarities with methods like Noise Contrastive Estimation and GANs.
The method is supported by robust theoretical guarantees, with clearly defined conditions under which each boosting iteration improves the log-likelihood. However, a notable limitation is the absence of a normalization constant for the resulting strong learner, as well as the reliance on heuristics to weight the weak learners, which appears to have practical significance (as discussed in Sec. 3.2). The discriminative approach also faces a computational bottleneck, as each boosting round necessitates generating a "training set" worth of samples from the previous strong learner, with samples obtained via MCMC.
The experimental evaluation is the weakest aspect of the paper. The method is tested on a synthetic dataset and a single real-world dataset, MNIST, for both generation and feature extraction for classification. While the synthetic experiments effectively demonstrate the method, the MNIST results are less convincing due to the use of overly weak baseline models. For instance, a modestly sized VAE can achieve 90 nats within hours on a single GPU, which is clearly feasible. Additionally, despite the authors' arguments, I remain unconvinced of the practical utility of mixing base learners, as it imposes significant implementation and training overhead for K different models. A more fundamental question remains unanswered: is it preferable to train a large VAE by maximizing the ELBO, or to perform 10 iterations of boosting using VAEs that are 1/10th the size of the baseline model? Furthermore, the experimental section lacks critical details, particularly regarding the sampling procedure for generating samples from the BGM. The inclusion of likelihood estimates obtained via AIS would also strengthen the evaluation.
In terms of novelty and related work, the paper overlooks a key reference: "Self Supervised Boosting" by Welling et al [R1]. A preliminary review suggests substantial similarities to the GenBGM approach, which should be acknowledged and discussed.
In summary, I remain undecided. The concept of boosting generative models is compelling, well-justified, and has the potential for significant impact. Given the strength of the theoretical contributions, I am inclined to overlook some of the aforementioned shortcomings and hope the authors can address them during the rebuttal phase.
[R1]