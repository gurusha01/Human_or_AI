Review - Summary:  
The paper introduces a deep neural network designed for machine comprehension tasks on the SQuAD dataset. The proposed architecture builds upon two prior models: match-LSTM and Pointer Net. Match-LSTM computes attention over each word in the question for every word in the passage and sequentially aggregates this alignment across the passage. The Pointer Net is employed to generate the answer, either by predicting each word in the answer or by identifying the start and end tokens within the passage. Experimental results demonstrate that both variants of the proposed model surpass the baseline established in the original SQuAD paper. Additionally, the paper provides an analysis of the results, including performance variations based on answer lengths and question types.
Strengths:  
1. The paper proposes a novel end-to-end model for machine comprehension, avoiding reliance on hand-crafted features.  
2. The model achieves a significant improvement over the baseline reported in the SQuAD paper.  
3. The analysis of results provides useful insights, such as the model performing better for shorter answers and struggling with "why" questions.  
Weaknesses/Questions/Suggestions:  
1. The paper does not quantitatively evaluate the contribution of attention mechanisms in match-LSTM and the answer pointer layer. It would be helpful to compare model performance with and without attention in these components.  
2. The paper does not explain the large performance gap between the boundary model and the sequence model in the answer pointer layer. Providing insights into this difference would strengthen the paper.  
3. An analysis of the model's performance on questions requiring different reasoning types (as categorized in Table 3 of the SQuAD paper) would be valuable. This could highlight the model's strengths and weaknesses with respect to various reasoning demands.  
4. In Equation 2, the activations resulting from {h^p}i and {h^r}{i-1} in G_i are repeated across the dimensions of Q. The authors should clarify why this design choice was made instead of learning distinct activations for each dimension.  
5. The ensemble model (last row in Table 2) does not include Bi-Ans-Ptr, despite Bi-Ans-Ptr improving performance by 1.2% in F1. The authors should explain why it was excluded.  
6. A more detailed discussion and comparison of the DCR model (referenced in Table 2) would enhance the paper's clarity and completeness.  
Review Summary:  
The paper proposes a reasonable end-to-end model for machine comprehension on the SQuAD dataset, achieving significant improvements over the baseline. However, the paper would benefit from additional analyses, ablation studies, and insights regarding the role of attention mechanisms, the performance gap between boundary and sequence models, and the model's behavior across different reasoning types.