This paper introduces an extension of the PixelCNN method that incorporates conditioning on text and spatially-structured constraints (e.g., segmentation or keypoints). The approach is conceptually similar to Reed 2016a, but the proposed extension is built on PixelCNN rather than GAN. After reviewing the authors' response, I understand that their argument is not that conditional PixelCNN outperforms conditional GAN, but rather that it offers a different perspective. I believe the paper would benefit from a more detailed discussion of the advantages and limitations of each model. Additionally, I agree with the other reviewer that an analysis of training and generation times would strengthen the paper. While I understand that PixelCNN sampling operates at O(N) complexity compared to O(1) for GANs, I wonder if this is the primary reason why the experiments are limited to low-resolution images (32 x 32). Furthermore, given the lack of quantitative comparisons, it would be more compelling to include additional visualizations beyond the three examples provided. That said, the generated results appear reasonably good, with sufficient diversity. The issue with color mistakes is acknowledged, and the authors have offered some explanations in their response. In terms of technical novelty, the contribution is incremental, as the extension is relatively straightforward and closely related to prior work. Nevertheless, I lean toward acceptance, as the paper is highly relevant to the ICLR community and offers a valuable foundation for future research and comparative studies on deep image synthesis methods.