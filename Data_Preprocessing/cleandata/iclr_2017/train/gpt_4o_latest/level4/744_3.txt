The paper proposes a layer architecture where a single parameter is employed to gate the layer's output response, either amplifying or suppressing it. The authors demonstrate that this design can facilitate the optimization of deep networks by simplifying the learning of identity mappings within layers, thereby improving gradient propagation to lower layers and enhancing supervision.
Using the introduced SDI metric, the study shows that gated residual networks are more effective at learning identity mappings compared to other architectures.
While the theoretical reasoning is well-articulated, the experimental evidence regarding the learned k values does not appear to robustly support the theory, as the k values are predominantly very small and exhibit minimal variation across layers. Furthermore, the experimental validation of the proposed approach is relatively limited, both in terms of reported performance metrics and the scope of large-scale experiments conducted.