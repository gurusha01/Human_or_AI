This paper demonstrates that in a serial setting, employing a larger mini-batch size increases the number of samples required to achieve the same convergence guarantee. A similar phenomenon is analyzed in the context of asynchronous SGD with multiple learners. This behavior has been previously observed in convex optimization literature (e.g., "Better Mini-Batch Algorithms via Accelerated Gradient Methods", NIPS 2011), where the convergence rate follows O(1/\sqrt{bT}+1/T). Consequently, using bT samples results in only a \sqrt{b} improvement in time. The current paper extends this type of result to the nonconvex case, though the mathematical foundation largely relies on [Ghadimi & Lan, 2013]. However, this behavior is already well-documented and summarized in the deep learning textbook (chapter 8), which limits the novelty of the contribution.
The theoretical findings of this paper imply that avoiding mini-batches is optimal. However, in practice, mini-batches are often advantageous. As noted in the deep learning textbook, mini-batches enable the use of larger learning rates (whereas this paper assumes the same learning rate for different mini-batch sizes). Additionally, modern multicore architectures facilitate parallel processing of mini-batches with minimal overhead. As a result, the practical relevance of the presented results is limited.
Other comments:
- Equation (4): Given that the same total number of samples (S) is processed, where S=MK (M is the mini-batch size and K is the number of mini-batches, as stated in the first paragraph of section 2), the values of K should differ when comparing two mini-batch sizes (Ml and Mh). However, the equation uses the same K on the LHS of (4), which is inconsistent.
- Figures 1 and 2: Since the primary focus is on convergence speed, why not present the training objective instead?