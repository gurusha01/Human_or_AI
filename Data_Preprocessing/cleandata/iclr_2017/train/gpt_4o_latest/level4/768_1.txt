This paper introduces a method for learning groups of orthogonal features in a convolutional network by penalizing correlations among features within each group. The proposed approach is applied to image classification tasks using "privileged information" in the form of foreground segmentation masks. The model is trained to learn orthogonal groups of foreground and background features through a correlation penalty, augmented by an additional "background suppression" term.
---
Pros:
- The paper introduces a novel "group-wise model diversity" loss term, which, to the best of my knowledge, has not been proposed before.
- The use of foreground segmentation masks to enhance image classification performance is also a novel contribution.
- The proposed method is evaluated on two well-known and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.
---
Cons:
- The evaluation is insufficient. A baseline that excludes the background suppression term should be included to help readers discern the individual contributions of the background suppression term versus the group orthogonality term. Additionally, the role of the background suppression term is unclear to me—it appears redundant, as the group orthogonality term should inherently discourage the use of background features by the foreground feature extractor.
- The results with "Incomplete Privileged Information" should be extended to the full ImageNet dataset (rather than just 10% of it), where privileged information is available for only a subset of images. This would help confirm whether the proposed method and the use of segmentation masks remain effective in scenarios with more labeled classification data.
- The overall presentation is somewhat unclear and difficult to follow. For instance, Section 4.2, titled "A Unified Architecture: GoCNN," does not provide a high-level overview of the method but instead lists specific implementation details. This is confusing, starting from the very first sentence.
- Minor Issues:
  - Referring to Equation 3 as a "regression loss" and writing "||0 - x||" instead of simply "||x||" is unnecessary and makes the explanation harder to understand. Norm regularization terms are rarely, if ever, written in this way or described as "regression to 0."
  - In Figure 1, the FG and BG suppression labels appear to be swapped. For example, the "suppress foreground" mask has 1s in the foreground and 0s in the background, which would suppress the background rather than the foreground.
---
Additional Question:
Why do the results in Table 4 (with 100% privileged information) differ from those in Tables 1 and 2? Are these not the same experimental setting?
---
The ideas in this paper are novel and show potential, but the lack of sufficient ablation studies makes it difficult for readers to identify which aspects of the method are critical to its performance. In addition to more experiments, the paper would benefit from reorganization and revisions to improve clarity.
---
Edit (1/29/17): After reviewing the latest revisions—particularly the full ImageNet evaluation results in Table 5, which demonstrate that the background segmentation "privileged information" remains beneficial even with the full labeled ImageNet dataset—I have updated my rating from 4 to 6.
(A minor point about Figure 1 remains: I still believe the "0" and "1" labels in the top part of the figure should be swapped to align with the other labels. For example, in the topmost path of Figure 1a, labeled "suppress foreground," the current configuration has 0 in the background and 1 in the foreground, which should be reversed to suppress the foreground.)