UPDATE: The authors have successfully addressed all my concerns in the revised version of the paper. Consequently, I have increased my score and now recommend the paper for acceptance.
This work integrates recent advancements in variational autoencoders (VAE) and autoregressive density modeling within the proposed PixelVAE framework. The paper demonstrates that the PixelVAE model, incorporating a much shallower PixelCNN decoder, achieves comparable negative log-likelihood (NLL) performance to that of a standard PixelCNN. 
The concept of leveraging a VAE to capture global structure while using a PixelCNN decoder to model local structure is both intuitive and effective, as it mitigates the issue of blurry reconstructions/samples often associated with VAEs. I particularly appreciate the hierarchical image generation experiments presented in the paper.
Below are my suggestions and concerns regarding the paper:
1) Have the authors conducted any experiments to show that employing a PixelCNN as the decoder in a VAE enhances the disentanglement of high-level factors of variation in the latent code? For instance, training both a PixelVAE and a standard VAE on MNIST with a 2D latent code, visualizing the latent representations for test images, and color-coding the points by digit could demonstrate whether PixelVAE achieves better separation. Additionally, a semi-supervised classification comparison between the two models would significantly strengthen the paper.
2) A concurrent ICLR submission, "Variational Lossy Autoencoder," explores a similar idea. Including a discussion of this related work and comparing the two approaches would add valuable context to the paper.
3) While the authors' responses to the pre-review questions clarified the architectural details, I still recommend including the exact architecture specifications for all experiments in the paper and/or releasing the code. The current presentation lacks sufficient clarity, making the experiments challenging to reproduce.
4) As mentioned in my pre-review comments, it would be beneficial to include two sets of MNIST samples—perhaps in an appendix—generated using PixelCNN and PixelVAE with the same PixelCNN depth. This would help illustrate how the latent code in PixelVAE effectively captures global structure.
I am willing to further increase my score if the authors incorporate these suggestions.